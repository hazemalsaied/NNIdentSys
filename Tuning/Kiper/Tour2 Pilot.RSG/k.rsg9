INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11354.017598
validation loss after epoch 0 : 866.454756
	Epoch 1....
Epoch has taken 0:02:59.341123
Number of used sentences in train = 2811
Total loss for epoch 1: 7612.758761
validation loss after epoch 1 : 843.044730
	Epoch 2....
Epoch has taken 0:02:58.663034
Number of used sentences in train = 2811
Total loss for epoch 2: 6858.608082
validation loss after epoch 2 : 803.027593
	Epoch 3....
Epoch has taken 0:02:58.607689
Number of used sentences in train = 2811
Total loss for epoch 3: 6385.488079
validation loss after epoch 3 : 793.367005
	Epoch 4....
Epoch has taken 0:03:02.861319
Number of used sentences in train = 2811
Total loss for epoch 4: 6085.406482
validation loss after epoch 4 : 774.751086
	Epoch 5....
Epoch has taken 0:02:49.017847
Number of used sentences in train = 2811
Total loss for epoch 5: 5739.886095
validation loss after epoch 5 : 838.206632
	Epoch 6....
Epoch has taken 0:02:59.894224
Number of used sentences in train = 2811
Total loss for epoch 6: 5491.828758
validation loss after epoch 6 : 842.585631
	Epoch 7....
Epoch has taken 0:02:56.955699
Number of used sentences in train = 2811
Total loss for epoch 7: 5348.627263
validation loss after epoch 7 : 819.915338
	Epoch 8....
Epoch has taken 0:02:53.429676
Number of used sentences in train = 2811
Total loss for epoch 8: 5185.333820
validation loss after epoch 8 : 845.496372
	Epoch 9....
Epoch has taken 0:02:49.800488
Number of used sentences in train = 2811
Total loss for epoch 9: 5106.997238
validation loss after epoch 9 : 860.032624
	Epoch 10....
Epoch has taken 0:02:49.636795
Number of used sentences in train = 2811
Total loss for epoch 10: 5015.988879
validation loss after epoch 10 : 868.138370
	Epoch 11....
Epoch has taken 0:02:51.787222
Number of used sentences in train = 2811
Total loss for epoch 11: 4903.079279
validation loss after epoch 11 : 933.219282
	Epoch 12....
Epoch has taken 0:02:48.012640
Number of used sentences in train = 2811
Total loss for epoch 12: 4866.431682
validation loss after epoch 12 : 937.839239
	Epoch 13....
Epoch has taken 0:02:48.079339
Number of used sentences in train = 2811
Total loss for epoch 13: 4836.422326
validation loss after epoch 13 : 963.357719
	Epoch 14....
Epoch has taken 0:02:49.441014
Number of used sentences in train = 2811
Total loss for epoch 14: 4807.044559
validation loss after epoch 14 : 982.480989
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.015229
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1269.924790
	Epoch 1....
Epoch has taken 0:00:17.930086
Number of used sentences in train = 313
Total loss for epoch 1: 793.844377
	Epoch 2....
Epoch has taken 0:00:17.936630
Number of used sentences in train = 313
Total loss for epoch 2: 716.926477
	Epoch 3....
Epoch has taken 0:00:17.939219
Number of used sentences in train = 313
Total loss for epoch 3: 621.397684
	Epoch 4....
Epoch has taken 0:00:17.917415
Number of used sentences in train = 313
Total loss for epoch 4: 597.966245
	Epoch 5....
Epoch has taken 0:00:17.936006
Number of used sentences in train = 313
Total loss for epoch 5: 571.770360
	Epoch 6....
Epoch has taken 0:00:17.922687
Number of used sentences in train = 313
Total loss for epoch 6: 558.664540
	Epoch 7....
Epoch has taken 0:00:17.957731
Number of used sentences in train = 313
Total loss for epoch 7: 553.820126
	Epoch 8....
Epoch has taken 0:00:17.946654
Number of used sentences in train = 313
Total loss for epoch 8: 544.278940
	Epoch 9....
Epoch has taken 0:00:17.936811
Number of used sentences in train = 313
Total loss for epoch 9: 539.025276
	Epoch 10....
Epoch has taken 0:00:17.936375
Number of used sentences in train = 313
Total loss for epoch 10: 536.994028
	Epoch 11....
Epoch has taken 0:00:17.956077
Number of used sentences in train = 313
Total loss for epoch 11: 537.272140
	Epoch 12....
Epoch has taken 0:00:17.940073
Number of used sentences in train = 313
Total loss for epoch 12: 536.162639
	Epoch 13....
Epoch has taken 0:00:17.921001
Number of used sentences in train = 313
Total loss for epoch 13: 535.663985
	Epoch 14....
Epoch has taken 0:00:17.908951
Number of used sentences in train = 313
Total loss for epoch 14: 535.530124
Epoch has taken 0:00:17.913822

==================================================================================================
	Training time : 0:48:03.021681
==================================================================================================
	Identification : 0.467

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9088.608989
validation loss after epoch 0 : 654.254293
	Epoch 1....
Epoch has taken 0:01:56.206243
Number of used sentences in train = 2074
Total loss for epoch 1: 5451.230651
validation loss after epoch 1 : 649.104341
	Epoch 2....
Epoch has taken 0:01:56.233443
Number of used sentences in train = 2074
Total loss for epoch 2: 4799.612832
validation loss after epoch 2 : 650.005339
	Epoch 3....
Epoch has taken 0:01:57.419393
Number of used sentences in train = 2074
Total loss for epoch 3: 4382.826671
validation loss after epoch 3 : 655.479226
	Epoch 4....
Epoch has taken 0:02:15.331591
Number of used sentences in train = 2074
Total loss for epoch 4: 4005.620282
validation loss after epoch 4 : 688.440311
	Epoch 5....
Epoch has taken 0:01:57.334061
Number of used sentences in train = 2074
Total loss for epoch 5: 3761.346738
validation loss after epoch 5 : 675.778915
	Epoch 6....
Epoch has taken 0:01:56.381420
Number of used sentences in train = 2074
Total loss for epoch 6: 3553.358722
validation loss after epoch 6 : 748.266921
	Epoch 7....
Epoch has taken 0:01:57.378329
Number of used sentences in train = 2074
Total loss for epoch 7: 3448.443132
validation loss after epoch 7 : 767.692205
	Epoch 8....
Epoch has taken 0:01:57.375110
Number of used sentences in train = 2074
Total loss for epoch 8: 3391.126084
validation loss after epoch 8 : 849.193012
	Epoch 9....
Epoch has taken 0:02:15.222497
Number of used sentences in train = 2074
Total loss for epoch 9: 3345.618516
validation loss after epoch 9 : 779.055673
	Epoch 10....
Epoch has taken 0:01:57.531440
Number of used sentences in train = 2074
Total loss for epoch 10: 3307.953354
validation loss after epoch 10 : 811.269098
	Epoch 11....
Epoch has taken 0:01:56.261829
Number of used sentences in train = 2074
Total loss for epoch 11: 3277.707860
validation loss after epoch 11 : 793.321617
	Epoch 12....
Epoch has taken 0:01:56.344008
Number of used sentences in train = 2074
Total loss for epoch 12: 3273.514832
validation loss after epoch 12 : 820.319923
	Epoch 13....
Epoch has taken 0:01:57.411796
Number of used sentences in train = 2074
Total loss for epoch 13: 3228.031591
validation loss after epoch 13 : 864.134277
	Epoch 14....
Epoch has taken 0:01:57.432994
Number of used sentences in train = 2074
Total loss for epoch 14: 3220.951910
validation loss after epoch 14 : 977.097102
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:15.306574
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1405.465862
	Epoch 1....
Epoch has taken 0:00:11.963878
Number of used sentences in train = 231
Total loss for epoch 1: 601.413521
	Epoch 2....
Epoch has taken 0:00:11.964859
Number of used sentences in train = 231
Total loss for epoch 2: 437.031852
	Epoch 3....
Epoch has taken 0:00:11.950824
Number of used sentences in train = 231
Total loss for epoch 3: 407.062737
	Epoch 4....
Epoch has taken 0:00:11.934001
Number of used sentences in train = 231
Total loss for epoch 4: 382.370567
	Epoch 5....
Epoch has taken 0:00:11.843391
Number of used sentences in train = 231
Total loss for epoch 5: 366.484039
	Epoch 6....
Epoch has taken 0:00:11.852883
Number of used sentences in train = 231
Total loss for epoch 6: 356.915113
	Epoch 7....
Epoch has taken 0:00:11.854524
Number of used sentences in train = 231
Total loss for epoch 7: 355.793580
	Epoch 8....
Epoch has taken 0:00:11.842366
Number of used sentences in train = 231
Total loss for epoch 8: 351.996756
	Epoch 9....
Epoch has taken 0:00:11.853966
Number of used sentences in train = 231
Total loss for epoch 9: 352.513711
	Epoch 10....
Epoch has taken 0:00:11.834343
Number of used sentences in train = 231
Total loss for epoch 10: 349.887915
	Epoch 11....
Epoch has taken 0:00:11.838207
Number of used sentences in train = 231
Total loss for epoch 11: 350.468814
	Epoch 12....
Epoch has taken 0:00:11.837801
Number of used sentences in train = 231
Total loss for epoch 12: 348.969832
	Epoch 13....
Epoch has taken 0:00:11.851170
Number of used sentences in train = 231
Total loss for epoch 13: 348.274477
	Epoch 14....
Epoch has taken 0:00:11.841171
Number of used sentences in train = 231
Total loss for epoch 14: 348.907451
Epoch has taken 0:00:11.835779

==================================================================================================
	Training time : 0:33:07.602577
==================================================================================================
	Identification : 0.318

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 26022.889513
validation loss after epoch 0 : 1200.788921
	Epoch 1....
Epoch has taken 0:04:11.288719
Number of used sentences in train = 3226
Total loss for epoch 1: 10242.427515
validation loss after epoch 1 : 1095.412873
	Epoch 2....
Epoch has taken 0:04:07.348967
Number of used sentences in train = 3226
Total loss for epoch 2: 9129.637850
validation loss after epoch 2 : 1078.736054
	Epoch 3....
Epoch has taken 0:04:22.668128
Number of used sentences in train = 3226
Total loss for epoch 3: 8541.357185
validation loss after epoch 3 : 1094.847199
	Epoch 4....
Epoch has taken 0:04:09.349774
Number of used sentences in train = 3226
Total loss for epoch 4: 8120.349236
validation loss after epoch 4 : 1108.893417
	Epoch 5....
Epoch has taken 0:04:06.772006
Number of used sentences in train = 3226
Total loss for epoch 5: 7791.531427
validation loss after epoch 5 : 1114.918948
	Epoch 6....
Epoch has taken 0:03:46.174914
Number of used sentences in train = 3226
Total loss for epoch 6: 7506.829431
validation loss after epoch 6 : 1186.920447
	Epoch 7....
Epoch has taken 0:03:49.792007
Number of used sentences in train = 3226
Total loss for epoch 7: 7278.070315
validation loss after epoch 7 : 1286.173839
	Epoch 8....
Epoch has taken 0:03:48.175346
Number of used sentences in train = 3226
Total loss for epoch 8: 7126.362793
validation loss after epoch 8 : 1192.159668
	Epoch 9....
Epoch has taken 0:03:47.980776
Number of used sentences in train = 3226
Total loss for epoch 9: 6967.074001
validation loss after epoch 9 : 1284.107947
	Epoch 10....
Epoch has taken 0:03:46.027532
Number of used sentences in train = 3226
Total loss for epoch 10: 6898.397374
validation loss after epoch 10 : 1313.128130
	Epoch 11....
Epoch has taken 0:03:46.250295
Number of used sentences in train = 3226
Total loss for epoch 11: 6796.812208
validation loss after epoch 11 : 1301.751980
	Epoch 12....
Epoch has taken 0:03:48.137750
Number of used sentences in train = 3226
Total loss for epoch 12: 6689.995410
validation loss after epoch 12 : 1379.871936
	Epoch 13....
Epoch has taken 0:03:48.543119
Number of used sentences in train = 3226
Total loss for epoch 13: 6632.245554
validation loss after epoch 13 : 1442.085934
	Epoch 14....
Epoch has taken 0:03:48.506870
Number of used sentences in train = 3226
Total loss for epoch 14: 6560.946379
validation loss after epoch 14 : 1464.611978
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:17.045412
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1549.515919
	Epoch 1....
Epoch has taken 0:00:24.507877
Number of used sentences in train = 359
Total loss for epoch 1: 983.021814
	Epoch 2....
Epoch has taken 0:00:24.494660
Number of used sentences in train = 359
Total loss for epoch 2: 880.812747
	Epoch 3....
Epoch has taken 0:00:24.507703
Number of used sentences in train = 359
Total loss for epoch 3: 814.890700
	Epoch 4....
Epoch has taken 0:00:24.453117
Number of used sentences in train = 359
Total loss for epoch 4: 772.573687
	Epoch 5....
Epoch has taken 0:00:24.487974
Number of used sentences in train = 359
Total loss for epoch 5: 739.772774
	Epoch 6....
Epoch has taken 0:00:24.471825
Number of used sentences in train = 359
Total loss for epoch 6: 716.198056
	Epoch 7....
Epoch has taken 0:00:24.498951
Number of used sentences in train = 359
Total loss for epoch 7: 700.252350
	Epoch 8....
Epoch has taken 0:00:24.483495
Number of used sentences in train = 359
Total loss for epoch 8: 705.989960
	Epoch 9....
Epoch has taken 0:00:24.482467
Number of used sentences in train = 359
Total loss for epoch 9: 684.114018
	Epoch 10....
Epoch has taken 0:00:24.491338
Number of used sentences in train = 359
Total loss for epoch 10: 684.423565
	Epoch 11....
Epoch has taken 0:00:23.344474
Number of used sentences in train = 359
Total loss for epoch 11: 678.957939
	Epoch 12....
Epoch has taken 0:00:22.138736
Number of used sentences in train = 359
Total loss for epoch 12: 683.734787
	Epoch 13....
Epoch has taken 0:00:22.114980
Number of used sentences in train = 359
Total loss for epoch 13: 674.456145
	Epoch 14....
Epoch has taken 0:00:22.131014
Number of used sentences in train = 359
Total loss for epoch 14: 674.284725
Epoch has taken 0:00:22.122864

==================================================================================================
	Training time : 1:05:21.452254
==================================================================================================
	Identification : 0.023

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 17, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 71, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 38, 'lstmDropout': 0.22, 'denseActivation': 'tanh', 'wordDim': 241, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 71)
  (w_embeddings): Embedding(9264, 241)
  (lstm): LSTM(312, 38, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13720.046251
validation loss after epoch 0 : 1182.608290
	Epoch 1....
Epoch has taken 0:02:50.853879
Number of used sentences in train = 2811
Total loss for epoch 1: 9495.644826
validation loss after epoch 1 : 1101.739496
	Epoch 2....
Epoch has taken 0:02:53.309807
Number of used sentences in train = 2811
Total loss for epoch 2: 7807.868796
validation loss after epoch 2 : 1067.187123
	Epoch 3....
Epoch has taken 0:02:55.885662
Number of used sentences in train = 2811
Total loss for epoch 3: 6664.091793
validation loss after epoch 3 : 1221.532575
	Epoch 4....
Epoch has taken 0:02:51.251427
Number of used sentences in train = 2811
Total loss for epoch 4: 6041.168063
validation loss after epoch 4 : 1191.560022
	Epoch 5....
Epoch has taken 0:02:50.918265
Number of used sentences in train = 2811
Total loss for epoch 5: 5633.589352
validation loss after epoch 5 : 1223.925672
	Epoch 6....
Epoch has taken 0:02:51.332260
Number of used sentences in train = 2811
Total loss for epoch 6: 5376.293512
validation loss after epoch 6 : 1241.360098
	Epoch 7....
Epoch has taken 0:02:50.933601
Number of used sentences in train = 2811
Total loss for epoch 7: 5225.900127
validation loss after epoch 7 : 1317.134532
	Epoch 8....
Epoch has taken 0:02:49.549329
Number of used sentences in train = 2811
Total loss for epoch 8: 5104.049971
validation loss after epoch 8 : 1358.096448
	Epoch 9....
Epoch has taken 0:02:49.565479
Number of used sentences in train = 2811
Total loss for epoch 9: 4970.693834
validation loss after epoch 9 : 1387.973686
	Epoch 10....
Epoch has taken 0:02:51.181723
Number of used sentences in train = 2811
Total loss for epoch 10: 4902.277364
validation loss after epoch 10 : 1499.298446
	Epoch 11....
Epoch has taken 0:03:17.241735
Number of used sentences in train = 2811
Total loss for epoch 11: 4831.579445
validation loss after epoch 11 : 1429.634897
	Epoch 12....
Epoch has taken 0:02:50.956087
Number of used sentences in train = 2811
Total loss for epoch 12: 4802.744537
validation loss after epoch 12 : 1569.265056
	Epoch 13....
Epoch has taken 0:02:49.590131
Number of used sentences in train = 2811
Total loss for epoch 13: 4758.712792
validation loss after epoch 13 : 1439.798890
	Epoch 14....
Epoch has taken 0:02:49.568795
Number of used sentences in train = 2811
Total loss for epoch 14: 4690.509383
validation loss after epoch 14 : 1508.885368
	TransitionClassifier(
  (p_embeddings): Embedding(18, 71)
  (w_embeddings): Embedding(9264, 241)
  (lstm): LSTM(312, 38, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.337280
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1497.034384
	Epoch 1....
Epoch has taken 0:00:18.080997
Number of used sentences in train = 313
Total loss for epoch 1: 926.295507
	Epoch 2....
Epoch has taken 0:00:18.076964
Number of used sentences in train = 313
Total loss for epoch 2: 723.886223
	Epoch 3....
Epoch has taken 0:00:20.732750
Number of used sentences in train = 313
Total loss for epoch 3: 624.846018
	Epoch 4....
Epoch has taken 0:00:20.756695
Number of used sentences in train = 313
Total loss for epoch 4: 589.094595
	Epoch 5....
Epoch has taken 0:00:20.758346
Number of used sentences in train = 313
Total loss for epoch 5: 560.673640
	Epoch 6....
Epoch has taken 0:00:20.759683
Number of used sentences in train = 313
Total loss for epoch 6: 567.658790
	Epoch 7....
Epoch has taken 0:00:18.339359
Number of used sentences in train = 313
Total loss for epoch 7: 552.477700
	Epoch 8....
Epoch has taken 0:00:18.127509
Number of used sentences in train = 313
Total loss for epoch 8: 537.189521
	Epoch 9....
Epoch has taken 0:00:18.123311
Number of used sentences in train = 313
Total loss for epoch 9: 538.470044
	Epoch 10....
Epoch has taken 0:00:18.120597
Number of used sentences in train = 313
Total loss for epoch 10: 550.184886
	Epoch 11....
Epoch has taken 0:00:18.121807
Number of used sentences in train = 313
Total loss for epoch 11: 540.708810
	Epoch 12....
Epoch has taken 0:00:18.108070
Number of used sentences in train = 313
Total loss for epoch 12: 531.563949
	Epoch 13....
Epoch has taken 0:00:18.105945
Number of used sentences in train = 313
Total loss for epoch 13: 531.986621
	Epoch 14....
Epoch has taken 0:00:18.100075
Number of used sentences in train = 313
Total loss for epoch 14: 532.218122
Epoch has taken 0:00:18.098072

==================================================================================================
	Training time : 0:47:56.407141
==================================================================================================
	Identification : 0.115

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 71)
  (w_embeddings): Embedding(7068, 241)
  (lstm): LSTM(312, 38, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11508.056018
validation loss after epoch 0 : 956.728116
	Epoch 1....
Epoch has taken 0:01:56.359039
Number of used sentences in train = 2074
Total loss for epoch 1: 7134.934809
validation loss after epoch 1 : 930.120653
	Epoch 2....
Epoch has taken 0:01:56.442663
Number of used sentences in train = 2074
Total loss for epoch 2: 5727.866681
validation loss after epoch 2 : 898.699595
	Epoch 3....
Epoch has taken 0:01:56.463712
Number of used sentences in train = 2074
Total loss for epoch 3: 4891.795507
validation loss after epoch 3 : 959.925577
	Epoch 4....
Epoch has taken 0:01:57.782615
Number of used sentences in train = 2074
Total loss for epoch 4: 4494.604184
validation loss after epoch 4 : 984.403729
	Epoch 5....
Epoch has taken 0:01:57.520186
Number of used sentences in train = 2074
Total loss for epoch 5: 4098.066641
validation loss after epoch 5 : 974.317323
	Epoch 6....
Epoch has taken 0:01:56.429072
Number of used sentences in train = 2074
Total loss for epoch 6: 3881.821038
validation loss after epoch 6 : 1045.828707
	Epoch 7....
Epoch has taken 0:01:56.477501
Number of used sentences in train = 2074
Total loss for epoch 7: 3793.864404
validation loss after epoch 7 : 1030.745211
	Epoch 8....
Epoch has taken 0:01:57.230761
Number of used sentences in train = 2074
Total loss for epoch 8: 3633.162490
validation loss after epoch 8 : 1184.842520
	Epoch 9....
Epoch has taken 0:01:56.429180
Number of used sentences in train = 2074
Total loss for epoch 9: 3592.282102
validation loss after epoch 9 : 1083.444022
	Epoch 10....
Epoch has taken 0:01:57.503898
Number of used sentences in train = 2074
Total loss for epoch 10: 3492.534536
validation loss after epoch 10 : 1205.245371
	Epoch 11....
Epoch has taken 0:01:57.714506
Number of used sentences in train = 2074
Total loss for epoch 11: 3432.973346
validation loss after epoch 11 : 1280.892957
	Epoch 12....
Epoch has taken 0:02:15.420938
Number of used sentences in train = 2074
Total loss for epoch 12: 3400.687023
validation loss after epoch 12 : 1219.236030
	Epoch 13....
Epoch has taken 0:01:57.690109
Number of used sentences in train = 2074
Total loss for epoch 13: 3389.213043
validation loss after epoch 13 : 1303.990153
	Epoch 14....
Epoch has taken 0:01:56.452517
Number of used sentences in train = 2074
Total loss for epoch 14: 3361.643343
validation loss after epoch 14 : 1367.040310
	TransitionClassifier(
  (p_embeddings): Embedding(18, 71)
  (w_embeddings): Embedding(7068, 241)
  (lstm): LSTM(312, 38, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.685287
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1443.493688
	Epoch 1....
Epoch has taken 0:00:11.996117
Number of used sentences in train = 231
Total loss for epoch 1: 737.038443
	Epoch 2....
Epoch has taken 0:00:11.987968
Number of used sentences in train = 231
Total loss for epoch 2: 570.542241
	Epoch 3....
Epoch has taken 0:00:11.986921
Number of used sentences in train = 231
Total loss for epoch 3: 463.828483
	Epoch 4....
Epoch has taken 0:00:11.987654
Number of used sentences in train = 231
Total loss for epoch 4: 426.356050
	Epoch 5....
Epoch has taken 0:00:11.990086
Number of used sentences in train = 231
Total loss for epoch 5: 394.043097
	Epoch 6....
Epoch has taken 0:00:11.984093
Number of used sentences in train = 231
Total loss for epoch 6: 385.724095
	Epoch 7....
Epoch has taken 0:00:11.974352
Number of used sentences in train = 231
Total loss for epoch 7: 368.381235
	Epoch 8....
Epoch has taken 0:00:11.989016
Number of used sentences in train = 231
Total loss for epoch 8: 361.927022
	Epoch 9....
Epoch has taken 0:00:11.977358
Number of used sentences in train = 231
Total loss for epoch 9: 361.814752
	Epoch 10....
Epoch has taken 0:00:11.988779
Number of used sentences in train = 231
Total loss for epoch 10: 353.052449
	Epoch 11....
Epoch has taken 0:00:11.974968
Number of used sentences in train = 231
Total loss for epoch 11: 355.017759
	Epoch 12....
Epoch has taken 0:00:11.973239
Number of used sentences in train = 231
Total loss for epoch 12: 352.937369
	Epoch 13....
Epoch has taken 0:00:11.980460
Number of used sentences in train = 231
Total loss for epoch 13: 352.252611
	Epoch 14....
Epoch has taken 0:00:11.975083
Number of used sentences in train = 231
Total loss for epoch 14: 350.447334
Epoch has taken 0:00:11.974747

==================================================================================================
	Training time : 0:32:33.695156
==================================================================================================
	Identification : 0.186

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 71)
  (w_embeddings): Embedding(17997, 241)
  (lstm): LSTM(312, 38, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18576.522587
validation loss after epoch 0 : 1651.711063
	Epoch 1....
Epoch has taken 0:04:02.702471
Number of used sentences in train = 3226
Total loss for epoch 1: 12716.229706
validation loss after epoch 1 : 1637.826628
	Epoch 2....
Epoch has taken 0:03:51.528378
Number of used sentences in train = 3226
Total loss for epoch 2: 10347.144090
validation loss after epoch 2 : 1683.699654
	Epoch 3....
Epoch has taken 0:04:03.717436
Number of used sentences in train = 3226
Total loss for epoch 3: 8975.435014
validation loss after epoch 3 : 1833.310607
	Epoch 4....
Epoch has taken 0:03:58.463829
Number of used sentences in train = 3226
Total loss for epoch 4: 8063.888156
validation loss after epoch 4 : 2011.195951
	Epoch 5....
Epoch has taken 0:04:06.804349
Number of used sentences in train = 3226
Total loss for epoch 5: 7584.252064
validation loss after epoch 5 : 2055.280674
	Epoch 6....
Epoch has taken 0:04:03.459573
Number of used sentences in train = 3226
Total loss for epoch 6: 7207.336315
validation loss after epoch 6 : 2232.425325
	Epoch 7....
Epoch has taken 0:03:52.917917
Number of used sentences in train = 3226
Total loss for epoch 7: 6935.038451
validation loss after epoch 7 : 2345.676684
	Epoch 8....
Epoch has taken 0:03:54.118477
Number of used sentences in train = 3226
Total loss for epoch 8: 6802.548091
validation loss after epoch 8 : 2333.821495
	Epoch 9....
Epoch has taken 0:03:51.425537
Number of used sentences in train = 3226
Total loss for epoch 9: 6657.676457
validation loss after epoch 9 : 2504.868015
	Epoch 10....
Epoch has taken 0:03:51.146708
Number of used sentences in train = 3226
Total loss for epoch 10: 6616.041717
validation loss after epoch 10 : 2487.443015
	Epoch 11....
Epoch has taken 0:03:49.206670
Number of used sentences in train = 3226
Total loss for epoch 11: 6535.764863
validation loss after epoch 11 : 2677.963535
	Epoch 12....
Epoch has taken 0:03:49.156692
Number of used sentences in train = 3226
Total loss for epoch 12: 6475.007496
validation loss after epoch 12 : 2709.215678
	Epoch 13....
Epoch has taken 0:03:51.468860
Number of used sentences in train = 3226
Total loss for epoch 13: 6424.890024
validation loss after epoch 13 : 2740.029008
	Epoch 14....
Epoch has taken 0:03:49.393896
Number of used sentences in train = 3226
Total loss for epoch 14: 6399.919681
validation loss after epoch 14 : 2777.808392
	TransitionClassifier(
  (p_embeddings): Embedding(13, 71)
  (w_embeddings): Embedding(17997, 241)
  (lstm): LSTM(312, 38, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:49.162296
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2132.395082
	Epoch 1....
Epoch has taken 0:00:22.540666
Number of used sentences in train = 359
Total loss for epoch 1: 1228.722150
	Epoch 2....
Epoch has taken 0:00:22.579982
Number of used sentences in train = 359
Total loss for epoch 2: 912.787991
	Epoch 3....
Epoch has taken 0:00:22.572591
Number of used sentences in train = 359
Total loss for epoch 3: 805.712425
	Epoch 4....
Epoch has taken 0:00:22.519566
Number of used sentences in train = 359
Total loss for epoch 4: 753.872966
	Epoch 5....
Epoch has taken 0:00:22.575313
Number of used sentences in train = 359
Total loss for epoch 5: 721.203571
	Epoch 6....
Epoch has taken 0:00:22.552350
Number of used sentences in train = 359
Total loss for epoch 6: 710.825061
	Epoch 7....
Epoch has taken 0:00:22.549143
Number of used sentences in train = 359
Total loss for epoch 7: 699.760148
	Epoch 8....
Epoch has taken 0:00:22.550727
Number of used sentences in train = 359
Total loss for epoch 8: 688.914909
	Epoch 9....
Epoch has taken 0:00:22.532788
Number of used sentences in train = 359
Total loss for epoch 9: 694.470964
	Epoch 10....
Epoch has taken 0:00:22.535608
Number of used sentences in train = 359
Total loss for epoch 10: 683.880161
	Epoch 11....
Epoch has taken 0:00:22.560837
Number of used sentences in train = 359
Total loss for epoch 11: 688.218852
	Epoch 12....
Epoch has taken 0:00:22.550955
Number of used sentences in train = 359
Total loss for epoch 12: 677.790580
	Epoch 13....
Epoch has taken 0:00:22.576305
Number of used sentences in train = 359
Total loss for epoch 13: 680.573675
	Epoch 14....
Epoch has taken 0:00:22.550832
Number of used sentences in train = 359
Total loss for epoch 14: 675.847016
Epoch has taken 0:00:22.549127

==================================================================================================
	Training time : 1:04:23.729252
==================================================================================================
	Identification : 0.047

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 11, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 60, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 143, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 74, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 60)
  (w_embeddings): Embedding(5892, 74)
  (lstm): LSTM(134, 143, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2288, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 24451.130507
validation loss after epoch 0 : 1894.557636
	Epoch 1....
Epoch has taken 0:02:51.539925
Number of used sentences in train = 2811
Total loss for epoch 1: 15446.365393
validation loss after epoch 1 : 1460.467991
	Epoch 2....
Epoch has taken 0:02:51.566222
Number of used sentences in train = 2811
Total loss for epoch 2: 11921.590939
validation loss after epoch 2 : 1221.450542
	Epoch 3....
Epoch has taken 0:02:51.557142
Number of used sentences in train = 2811
Total loss for epoch 3: 10136.956630
validation loss after epoch 3 : 1181.215930
	Epoch 4....
Epoch has taken 0:02:53.471995
Number of used sentences in train = 2811
Total loss for epoch 4: 9111.455988
validation loss after epoch 4 : 1174.394758
	Epoch 5....
Epoch has taken 0:02:53.160677
Number of used sentences in train = 2811
Total loss for epoch 5: 8426.895455
validation loss after epoch 5 : 1223.240687
	Epoch 6....
Epoch has taken 0:02:53.351857
Number of used sentences in train = 2811
Total loss for epoch 6: 7933.527172
validation loss after epoch 6 : 1252.560310
	Epoch 7....
Epoch has taken 0:02:51.587343
Number of used sentences in train = 2811
Total loss for epoch 7: 7494.786624
validation loss after epoch 7 : 1291.661916
	Epoch 8....
Epoch has taken 0:02:51.723986
Number of used sentences in train = 2811
Total loss for epoch 8: 7281.289382
validation loss after epoch 8 : 1319.203093
	Epoch 9....
Epoch has taken 0:02:53.025115
Number of used sentences in train = 2811
Total loss for epoch 9: 7009.104567
validation loss after epoch 9 : 1319.424477
	Epoch 10....
Epoch has taken 0:02:55.493355
Number of used sentences in train = 2811
Total loss for epoch 10: 6857.253675
validation loss after epoch 10 : 1314.066304
	Epoch 11....
Epoch has taken 0:02:53.342056
Number of used sentences in train = 2811
Total loss for epoch 11: 6581.964983
validation loss after epoch 11 : 1436.869989
	Epoch 12....
Epoch has taken 0:02:51.555984
Number of used sentences in train = 2811
Total loss for epoch 12: 6069.372067
validation loss after epoch 12 : 1372.637901
	Epoch 13....
Epoch has taken 0:02:51.759298
Number of used sentences in train = 2811
Total loss for epoch 13: 5970.630089
validation loss after epoch 13 : 1276.707943
	Epoch 14....
Epoch has taken 0:02:53.157506
Number of used sentences in train = 2811
Total loss for epoch 14: 5705.566048
validation loss after epoch 14 : 1323.202779
	TransitionClassifier(
  (p_embeddings): Embedding(18, 60)
  (w_embeddings): Embedding(5892, 74)
  (lstm): LSTM(134, 143, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2288, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:53.464231
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 3004.710640
	Epoch 1....
Epoch has taken 0:00:18.318720
Number of used sentences in train = 313
Total loss for epoch 1: 1378.478979
	Epoch 2....
Epoch has taken 0:00:18.316530
Number of used sentences in train = 313
Total loss for epoch 2: 1094.535440
	Epoch 3....
Epoch has taken 0:00:18.297824
Number of used sentences in train = 313
Total loss for epoch 3: 920.195425
	Epoch 4....
Epoch has taken 0:00:18.303088
Number of used sentences in train = 313
Total loss for epoch 4: 806.274729
	Epoch 5....
Epoch has taken 0:00:18.305917
Number of used sentences in train = 313
Total loss for epoch 5: 725.582870
	Epoch 6....
Epoch has taken 0:00:18.308792
Number of used sentences in train = 313
Total loss for epoch 6: 690.085842
	Epoch 7....
Epoch has taken 0:00:18.307976
Number of used sentences in train = 313
Total loss for epoch 7: 662.356830
	Epoch 8....
Epoch has taken 0:00:18.323207
Number of used sentences in train = 313
Total loss for epoch 8: 638.144912
	Epoch 9....
Epoch has taken 0:00:18.323906
Number of used sentences in train = 313
Total loss for epoch 9: 623.224343
	Epoch 10....
Epoch has taken 0:00:18.325435
Number of used sentences in train = 313
Total loss for epoch 10: 618.896357
	Epoch 11....
Epoch has taken 0:00:18.323722
Number of used sentences in train = 313
Total loss for epoch 11: 627.002371
	Epoch 12....
Epoch has taken 0:00:18.319088
Number of used sentences in train = 313
Total loss for epoch 12: 619.207600
	Epoch 13....
Epoch has taken 0:00:18.322934
Number of used sentences in train = 313
Total loss for epoch 13: 605.988366
	Epoch 14....
Epoch has taken 0:00:18.315148
Number of used sentences in train = 313
Total loss for epoch 14: 599.775502
Epoch has taken 0:00:18.325873

==================================================================================================
	Training time : 0:47:44.999890
==================================================================================================
	Identification : 0.474

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 60)
  (w_embeddings): Embedding(5655, 74)
  (lstm): LSTM(134, 143, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2288, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 20525.836759
validation loss after epoch 0 : 1612.920090
	Epoch 1....
Epoch has taken 0:01:57.264998
Number of used sentences in train = 2074
Total loss for epoch 1: 12954.822622
validation loss after epoch 1 : 1179.598061
	Epoch 2....
Epoch has taken 0:01:57.220543
Number of used sentences in train = 2074
Total loss for epoch 2: 8844.899713
validation loss after epoch 2 : 988.658136
	Epoch 3....
Epoch has taken 0:01:57.203198
Number of used sentences in train = 2074
Total loss for epoch 3: 7303.483970
validation loss after epoch 3 : 865.707876
	Epoch 4....
Epoch has taken 0:01:58.517376
Number of used sentences in train = 2074
Total loss for epoch 4: 6261.815519
validation loss after epoch 4 : 832.962319
	Epoch 5....
Epoch has taken 0:01:58.205255
Number of used sentences in train = 2074
Total loss for epoch 5: 5516.168585
validation loss after epoch 5 : 851.256111
	Epoch 6....
Epoch has taken 0:01:58.451326
Number of used sentences in train = 2074
Total loss for epoch 6: 5064.532828
validation loss after epoch 6 : 848.601513
	Epoch 7....
Epoch has taken 0:01:57.003666
Number of used sentences in train = 2074
Total loss for epoch 7: 4683.681496
validation loss after epoch 7 : 897.873948
	Epoch 8....
Epoch has taken 0:01:56.932775
Number of used sentences in train = 2074
Total loss for epoch 8: 4478.611197
validation loss after epoch 8 : 872.341897
	Epoch 9....
Epoch has taken 0:01:57.955365
Number of used sentences in train = 2074
Total loss for epoch 9: 4288.486213
validation loss after epoch 9 : 879.189310
	Epoch 10....
Epoch has taken 0:01:58.146044
Number of used sentences in train = 2074
Total loss for epoch 10: 4121.020947
validation loss after epoch 10 : 951.981019
	Epoch 11....
Epoch has taken 0:02:12.735781
Number of used sentences in train = 2074
Total loss for epoch 11: 4044.778938
validation loss after epoch 11 : 908.648022
	Epoch 12....
Epoch has taken 0:01:56.909375
Number of used sentences in train = 2074
Total loss for epoch 12: 3897.530190
validation loss after epoch 12 : 936.315400
	Epoch 13....
Epoch has taken 0:01:56.968173
Number of used sentences in train = 2074
Total loss for epoch 13: 3830.988171
validation loss after epoch 13 : 959.479425
	Epoch 14....
Epoch has taken 0:01:58.019894
Number of used sentences in train = 2074
Total loss for epoch 14: 3782.770859
validation loss after epoch 14 : 982.133637
	TransitionClassifier(
  (p_embeddings): Embedding(18, 60)
  (w_embeddings): Embedding(5655, 74)
  (lstm): LSTM(134, 143, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2288, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.934610
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2648.118966
	Epoch 1....
Epoch has taken 0:00:12.047559
Number of used sentences in train = 231
Total loss for epoch 1: 1391.091124
	Epoch 2....
Epoch has taken 0:00:12.065632
Number of used sentences in train = 231
Total loss for epoch 2: 1066.272934
	Epoch 3....
Epoch has taken 0:00:12.064402
Number of used sentences in train = 231
Total loss for epoch 3: 787.851771
	Epoch 4....
Epoch has taken 0:00:12.061956
Number of used sentences in train = 231
Total loss for epoch 4: 668.895508
	Epoch 5....
Epoch has taken 0:00:12.051554
Number of used sentences in train = 231
Total loss for epoch 5: 599.335118
	Epoch 6....
Epoch has taken 0:00:12.067737
Number of used sentences in train = 231
Total loss for epoch 6: 539.897466
	Epoch 7....
Epoch has taken 0:00:12.064926
Number of used sentences in train = 231
Total loss for epoch 7: 458.302446
	Epoch 8....
Epoch has taken 0:00:12.052669
Number of used sentences in train = 231
Total loss for epoch 8: 479.652219
	Epoch 9....
Epoch has taken 0:00:12.061946
Number of used sentences in train = 231
Total loss for epoch 9: 425.773896
	Epoch 10....
Epoch has taken 0:00:12.041866
Number of used sentences in train = 231
Total loss for epoch 10: 428.846601
	Epoch 11....
Epoch has taken 0:00:12.045338
Number of used sentences in train = 231
Total loss for epoch 11: 421.114631
	Epoch 12....
Epoch has taken 0:00:12.025854
Number of used sentences in train = 231
Total loss for epoch 12: 386.589271
	Epoch 13....
Epoch has taken 0:00:12.022479
Number of used sentences in train = 231
Total loss for epoch 13: 380.922409
	Epoch 14....
Epoch has taken 0:00:12.032923
Number of used sentences in train = 231
Total loss for epoch 14: 379.009181
Epoch has taken 0:00:12.033819

==================================================================================================
	Training time : 0:32:39.558364
==================================================================================================
	Identification : 0.412

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 60)
  (w_embeddings): Embedding(6880, 74)
  (lstm): LSTM(134, 143, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2288, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 39605.658306
validation loss after epoch 0 : 3961.585019
	Epoch 1....
Epoch has taken 0:03:48.012608
Number of used sentences in train = 3226
Total loss for epoch 1: 36839.588585
validation loss after epoch 1 : 3912.657007
	Epoch 2....
Epoch has taken 0:03:49.033634
Number of used sentences in train = 3226
Total loss for epoch 2: 36533.971292
validation loss after epoch 2 : 3902.609921
	Epoch 3....
Epoch has taken 0:03:49.107275
Number of used sentences in train = 3226
Total loss for epoch 3: 36477.436940
validation loss after epoch 3 : 3930.747644
	Epoch 4....
Epoch has taken 0:03:49.176500
Number of used sentences in train = 3226
Total loss for epoch 4: 28509.933475
validation loss after epoch 4 : 2042.355476
	Epoch 5....
Epoch has taken 0:03:50.363817
Number of used sentences in train = 3226
Total loss for epoch 5: 16536.964975
validation loss after epoch 5 : 1641.692510
	Epoch 6....
Epoch has taken 0:03:50.159434
Number of used sentences in train = 3226
Total loss for epoch 6: 13918.863872
validation loss after epoch 6 : 1507.084124
	Epoch 7....
Epoch has taken 0:03:50.461999
Number of used sentences in train = 3226
Total loss for epoch 7: 12386.361361
validation loss after epoch 7 : 1516.108166
	Epoch 8....
Epoch has taken 0:03:50.332928
Number of used sentences in train = 3226
Total loss for epoch 8: 11174.256415
validation loss after epoch 8 : 1493.311478
	Epoch 9....
Epoch has taken 0:03:48.508344
Number of used sentences in train = 3226
Total loss for epoch 9: 10301.085629
validation loss after epoch 9 : 1543.352022
	Epoch 10....
Epoch has taken 0:03:50.535638
Number of used sentences in train = 3226
Total loss for epoch 10: 9727.176314
validation loss after epoch 10 : 1596.216527
	Epoch 11....
Epoch has taken 0:03:50.494141
Number of used sentences in train = 3226
Total loss for epoch 11: 9119.105683
validation loss after epoch 11 : 1700.335883
	Epoch 12....
Epoch has taken 0:03:49.919580
Number of used sentences in train = 3226
Total loss for epoch 12: 8584.282715
validation loss after epoch 12 : 1708.231136
	Epoch 13....
Epoch has taken 0:03:48.251478
Number of used sentences in train = 3226
Total loss for epoch 13: 8179.164196
validation loss after epoch 13 : 1776.488652
	Epoch 14....
Epoch has taken 0:03:50.129727
Number of used sentences in train = 3226
Total loss for epoch 14: 7813.311587
validation loss after epoch 14 : 1799.315551
Epoch has taken 0:03:53.330954
# Network optimizer = Adagrad, learning rate = 0.07
	TransitionClassifier(
  (p_embeddings): Embedding(13, 60)
  (w_embeddings): Embedding(6880, 74)
  (lstm): LSTM(134, 143, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2288, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Number of used sentences in train = 359
Total loss for epoch 0: 2285.689070
	Epoch 1....
Epoch has taken 0:00:22.543290
Number of used sentences in train = 359
Total loss for epoch 1: 1591.304953
	Epoch 2....
Epoch has taken 0:00:22.544951
Number of used sentences in train = 359
Total loss for epoch 2: 1306.658605
	Epoch 3....
Epoch has taken 0:00:22.560870
Number of used sentences in train = 359
Total loss for epoch 3: 1139.103179
	Epoch 4....
Epoch has taken 0:00:22.547095
Number of used sentences in train = 359
Total loss for epoch 4: 1011.845415
	Epoch 5....
Epoch has taken 0:00:22.523985
Number of used sentences in train = 359
Total loss for epoch 5: 928.811479
	Epoch 6....
Epoch has taken 0:00:22.557884
Number of used sentences in train = 359
Total loss for epoch 6: 866.435946
	Epoch 7....
Epoch has taken 0:00:22.558202
Number of used sentences in train = 359
Total loss for epoch 7: 807.640039
	Epoch 8....
Epoch has taken 0:00:22.386704
Number of used sentences in train = 359
Total loss for epoch 8: 795.141173
	Epoch 9....
Epoch has taken 0:00:22.366075
Number of used sentences in train = 359
Total loss for epoch 9: 750.365484
	Epoch 10....
Epoch has taken 0:00:22.360374
Number of used sentences in train = 359
Total loss for epoch 10: 719.348261
	Epoch 11....
Epoch has taken 0:00:22.355199
Number of used sentences in train = 359
Total loss for epoch 11: 715.518880
	Epoch 12....
Epoch has taken 0:00:22.368649
Number of used sentences in train = 359
Total loss for epoch 12: 716.108757
	Epoch 13....
Epoch has taken 0:00:22.378528
Number of used sentences in train = 359
Total loss for epoch 13: 698.805230
	Epoch 14....
Epoch has taken 0:00:22.405513
Number of used sentences in train = 359
Total loss for epoch 14: 696.744979
Epoch has taken 0:00:22.386830

==================================================================================================
	Training time : 1:03:05.316990
==================================================================================================
	Identification : 0.481

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 110, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 49, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 110, 'lstmDropout': 0.26, 'denseActivation': 'tanh', 'wordDim': 143, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 49)
  (w_embeddings): Embedding(1177, 143)
  (lstm): LSTM(192, 110, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1760, out_features=110, bias=True)
  (linear2): Linear(in_features=110, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14292.978515
validation loss after epoch 0 : 981.533696
	Epoch 1....
Epoch has taken 0:02:53.024711
Number of used sentences in train = 2811
Total loss for epoch 1: 8652.511139
validation loss after epoch 1 : 892.446460
	Epoch 2....
Epoch has taken 0:02:52.847813
Number of used sentences in train = 2811
Total loss for epoch 2: 7572.285908
validation loss after epoch 2 : 874.623982
	Epoch 3....
Epoch has taken 0:02:52.746460
Number of used sentences in train = 2811
Total loss for epoch 3: 7006.870192
validation loss after epoch 3 : 865.009928
	Epoch 4....
Epoch has taken 0:02:52.359901
Number of used sentences in train = 2811
Total loss for epoch 4: 6481.014635
validation loss after epoch 4 : 858.633425
	Epoch 5....
Epoch has taken 0:02:51.560570
Number of used sentences in train = 2811
Total loss for epoch 5: 6123.870592
validation loss after epoch 5 : 884.156238
	Epoch 6....
Epoch has taken 0:02:52.827036
Number of used sentences in train = 2811
Total loss for epoch 6: 5804.354222
validation loss after epoch 6 : 901.408621
	Epoch 7....
Epoch has taken 0:02:52.469609
Number of used sentences in train = 2811
Total loss for epoch 7: 5571.596025
validation loss after epoch 7 : 879.689900
	Epoch 8....
Epoch has taken 0:02:52.853492
Number of used sentences in train = 2811
Total loss for epoch 8: 5457.314895
validation loss after epoch 8 : 925.700774
	Epoch 9....
Epoch has taken 0:02:50.887621
Number of used sentences in train = 2811
Total loss for epoch 9: 5285.559939
validation loss after epoch 9 : 941.173270
	Epoch 10....
Epoch has taken 0:02:51.718810
Number of used sentences in train = 2811
Total loss for epoch 10: 5156.645722
validation loss after epoch 10 : 1004.640425
	Epoch 11....
Epoch has taken 0:02:52.395742
Number of used sentences in train = 2811
Total loss for epoch 11: 5030.255778
validation loss after epoch 11 : 978.326885
	Epoch 12....
Epoch has taken 0:02:52.775515
Number of used sentences in train = 2811
Total loss for epoch 12: 4968.659122
validation loss after epoch 12 : 986.734786
	Epoch 13....
Epoch has taken 0:02:52.438766
Number of used sentences in train = 2811
Total loss for epoch 13: 4850.302149
validation loss after epoch 13 : 984.901078
	Epoch 14....
Epoch has taken 0:02:51.168042
Number of used sentences in train = 2811
Total loss for epoch 14: 4814.367611
validation loss after epoch 14 : 1044.437473
	TransitionClassifier(
  (p_embeddings): Embedding(18, 49)
  (w_embeddings): Embedding(1177, 143)
  (lstm): LSTM(192, 110, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1760, out_features=110, bias=True)
  (linear2): Linear(in_features=110, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.184064
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1615.605374
	Epoch 1....
Epoch has taken 0:00:18.227517
Number of used sentences in train = 313
Total loss for epoch 1: 892.100137
	Epoch 2....
Epoch has taken 0:00:18.279655
Number of used sentences in train = 313
Total loss for epoch 2: 717.774855
	Epoch 3....
Epoch has taken 0:00:18.295900
Number of used sentences in train = 313
Total loss for epoch 3: 630.105040
	Epoch 4....
Epoch has taken 0:00:18.298912
Number of used sentences in train = 313
Total loss for epoch 4: 599.021462
	Epoch 5....
Epoch has taken 0:00:18.315977
Number of used sentences in train = 313
Total loss for epoch 5: 556.040850
	Epoch 6....
Epoch has taken 0:00:18.304503
Number of used sentences in train = 313
Total loss for epoch 6: 552.673424
	Epoch 7....
Epoch has taken 0:00:18.312434
Number of used sentences in train = 313
Total loss for epoch 7: 543.413260
	Epoch 8....
Epoch has taken 0:00:18.306972
Number of used sentences in train = 313
Total loss for epoch 8: 551.485484
	Epoch 9....
Epoch has taken 0:00:18.295457
Number of used sentences in train = 313
Total loss for epoch 9: 538.514552
	Epoch 10....
Epoch has taken 0:00:18.306118
Number of used sentences in train = 313
Total loss for epoch 10: 533.133153
	Epoch 11....
Epoch has taken 0:00:18.308893
Number of used sentences in train = 313
Total loss for epoch 11: 521.931219
	Epoch 12....
Epoch has taken 0:00:18.299443
Number of used sentences in train = 313
Total loss for epoch 12: 520.553561
	Epoch 13....
Epoch has taken 0:00:18.309598
Number of used sentences in train = 313
Total loss for epoch 13: 529.552195
	Epoch 14....
Epoch has taken 0:00:18.303375
Number of used sentences in train = 313
Total loss for epoch 14: 516.318021
Epoch has taken 0:00:18.312436

==================================================================================================
	Training time : 0:47:38.239027
==================================================================================================
	Identification : 0.401

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 49)
  (w_embeddings): Embedding(1133, 143)
  (lstm): LSTM(192, 110, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1760, out_features=110, bias=True)
  (linear2): Linear(in_features=110, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10703.650090
validation loss after epoch 0 : 735.234978
	Epoch 1....
Epoch has taken 0:01:57.653113
Number of used sentences in train = 2074
Total loss for epoch 1: 5954.272638
validation loss after epoch 1 : 663.580705
	Epoch 2....
Epoch has taken 0:01:56.825604
Number of used sentences in train = 2074
Total loss for epoch 2: 5083.378883
validation loss after epoch 2 : 698.213777
	Epoch 3....
Epoch has taken 0:01:56.880018
Number of used sentences in train = 2074
Total loss for epoch 3: 4548.017802
validation loss after epoch 3 : 690.593148
	Epoch 4....
Epoch has taken 0:01:59.815702
Number of used sentences in train = 2074
Total loss for epoch 4: 4176.502428
validation loss after epoch 4 : 821.358904
	Epoch 5....
Epoch has taken 0:02:07.692821
Number of used sentences in train = 2074
Total loss for epoch 5: 3929.496125
validation loss after epoch 5 : 726.270169
	Epoch 6....
Epoch has taken 0:01:57.993515
Number of used sentences in train = 2074
Total loss for epoch 6: 3819.836732
validation loss after epoch 6 : 745.191190
	Epoch 7....
Epoch has taken 0:01:56.871115
Number of used sentences in train = 2074
Total loss for epoch 7: 3628.231335
validation loss after epoch 7 : 810.829360
	Epoch 8....
Epoch has taken 0:01:57.258182
Number of used sentences in train = 2074
Total loss for epoch 8: 3549.038383
validation loss after epoch 8 : 863.076659
	Epoch 9....
Epoch has taken 0:01:57.417442
Number of used sentences in train = 2074
Total loss for epoch 9: 3430.526139
validation loss after epoch 9 : 896.902454
	Epoch 10....
Epoch has taken 0:01:58.585197
Number of used sentences in train = 2074
Total loss for epoch 10: 3387.445881
validation loss after epoch 10 : 834.390966
	Epoch 11....
Epoch has taken 0:02:06.083462
Number of used sentences in train = 2074
Total loss for epoch 11: 3333.051013
validation loss after epoch 11 : 955.075552
	Epoch 12....
Epoch has taken 0:02:00.045293
Number of used sentences in train = 2074
Total loss for epoch 12: 3308.243968
validation loss after epoch 12 : 879.612817
	Epoch 13....
Epoch has taken 0:02:09.136623
Number of used sentences in train = 2074
Total loss for epoch 13: 3285.453395
validation loss after epoch 13 : 992.598510
	Epoch 14....
Epoch has taken 0:01:57.010910
Number of used sentences in train = 2074
Total loss for epoch 14: 3264.335848
validation loss after epoch 14 : 880.779293
	TransitionClassifier(
  (p_embeddings): Embedding(18, 49)
  (w_embeddings): Embedding(1133, 143)
  (lstm): LSTM(192, 110, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1760, out_features=110, bias=True)
  (linear2): Linear(in_features=110, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:58.116814
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1650.001661
	Epoch 1....
Epoch has taken 0:00:12.049435
Number of used sentences in train = 231
Total loss for epoch 1: 595.811145
	Epoch 2....
Epoch has taken 0:00:12.046928
Number of used sentences in train = 231
Total loss for epoch 2: 497.294952
	Epoch 3....
Epoch has taken 0:00:12.053774
Number of used sentences in train = 231
Total loss for epoch 3: 436.510300
	Epoch 4....
Epoch has taken 0:00:12.052032
Number of used sentences in train = 231
Total loss for epoch 4: 390.078663
	Epoch 5....
Epoch has taken 0:00:12.047587
Number of used sentences in train = 231
Total loss for epoch 5: 372.506562
	Epoch 6....
Epoch has taken 0:00:12.046986
Number of used sentences in train = 231
Total loss for epoch 6: 355.598671
	Epoch 7....
Epoch has taken 0:00:12.053329
Number of used sentences in train = 231
Total loss for epoch 7: 352.206145
	Epoch 8....
Epoch has taken 0:00:12.052739
Number of used sentences in train = 231
Total loss for epoch 8: 360.593638
	Epoch 9....
Epoch has taken 0:00:12.054187
Number of used sentences in train = 231
Total loss for epoch 9: 351.327783
	Epoch 10....
Epoch has taken 0:00:12.045811
Number of used sentences in train = 231
Total loss for epoch 10: 350.748129
	Epoch 11....
Epoch has taken 0:00:12.054113
Number of used sentences in train = 231
Total loss for epoch 11: 350.000502
	Epoch 12....
Epoch has taken 0:00:12.040735
Number of used sentences in train = 231
Total loss for epoch 12: 348.615776
	Epoch 13....
Epoch has taken 0:00:11.918212
Number of used sentences in train = 231
Total loss for epoch 13: 351.112927
	Epoch 14....
Epoch has taken 0:00:11.947948
Number of used sentences in train = 231
Total loss for epoch 14: 347.088810
Epoch has taken 0:00:11.962808

==================================================================================================
	Training time : 0:32:58.153416
==================================================================================================
	Identification : 0.132

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 49)
  (w_embeddings): Embedding(1202, 143)
  (lstm): LSTM(192, 110, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1760, out_features=110, bias=True)
  (linear2): Linear(in_features=110, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17823.680526
validation loss after epoch 0 : 1327.399958
	Epoch 1....
Epoch has taken 0:03:48.789089
Number of used sentences in train = 3226
Total loss for epoch 1: 11491.909402
validation loss after epoch 1 : 1265.911175
	Epoch 2....
Epoch has taken 0:03:48.807825
Number of used sentences in train = 3226
Total loss for epoch 2: 10351.158868
validation loss after epoch 2 : 1229.712788
	Epoch 3....
Epoch has taken 0:03:48.548730
Number of used sentences in train = 3226
Total loss for epoch 3: 9723.773420
validation loss after epoch 3 : 1351.066000
	Epoch 4....
Epoch has taken 0:03:48.950503
Number of used sentences in train = 3226
Total loss for epoch 4: 9253.684476
validation loss after epoch 4 : 1292.620106
	Epoch 5....
Epoch has taken 0:03:46.747934
Number of used sentences in train = 3226
Total loss for epoch 5: 8892.162214
validation loss after epoch 5 : 1403.500628
	Epoch 6....
Epoch has taken 0:03:48.491763
Number of used sentences in train = 3226
Total loss for epoch 6: 8651.336755
validation loss after epoch 6 : 1360.147222
	Epoch 7....
Epoch has taken 0:03:48.867186
Number of used sentences in train = 3226
Total loss for epoch 7: 8315.133357
validation loss after epoch 7 : 1434.339516
	Epoch 8....
Epoch has taken 0:04:23.191953
Number of used sentences in train = 3226
Total loss for epoch 8: 8136.667635
validation loss after epoch 8 : 1497.519339
	Epoch 9....
Epoch has taken 0:03:48.643010
Number of used sentences in train = 3226
Total loss for epoch 9: 7931.528099
validation loss after epoch 9 : 1563.747555
	Epoch 10....
Epoch has taken 0:03:46.795269
Number of used sentences in train = 3226
Total loss for epoch 10: 7766.722325
validation loss after epoch 10 : 1675.842840
	Epoch 11....
Epoch has taken 0:03:48.394601
Number of used sentences in train = 3226
Total loss for epoch 11: 7598.619699
validation loss after epoch 11 : 1550.200702
	Epoch 12....
Epoch has taken 0:03:48.891067
Number of used sentences in train = 3226
Total loss for epoch 12: 7495.806820
validation loss after epoch 12 : 1580.356220
	Epoch 13....
Epoch has taken 0:03:51.397144
Number of used sentences in train = 3226
Total loss for epoch 13: 7341.973635
validation loss after epoch 13 : 1630.891497
	Epoch 14....
Epoch has taken 0:03:48.672698
Number of used sentences in train = 3226
Total loss for epoch 14: 7237.582783
validation loss after epoch 14 : 1745.258680
	TransitionClassifier(
  (p_embeddings): Embedding(13, 49)
  (w_embeddings): Embedding(1202, 143)
  (lstm): LSTM(192, 110, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1760, out_features=110, bias=True)
  (linear2): Linear(in_features=110, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:46.726884
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2402.918984
	Epoch 1....
Epoch has taken 0:00:22.365204
Number of used sentences in train = 359
Total loss for epoch 1: 1260.918537
	Epoch 2....
Epoch has taken 0:00:22.400708
Number of used sentences in train = 359
Total loss for epoch 2: 1094.218023
	Epoch 3....
Epoch has taken 0:00:22.478391
Number of used sentences in train = 359
Total loss for epoch 3: 934.536515
	Epoch 4....
Epoch has taken 0:00:22.481783
Number of used sentences in train = 359
Total loss for epoch 4: 855.997659
	Epoch 5....
Epoch has taken 0:00:22.483558
Number of used sentences in train = 359
Total loss for epoch 5: 784.686586
	Epoch 6....
Epoch has taken 0:00:22.495608
Number of used sentences in train = 359
Total loss for epoch 6: 756.272357
	Epoch 7....
Epoch has taken 0:00:22.488304
Number of used sentences in train = 359
Total loss for epoch 7: 742.049971
	Epoch 8....
Epoch has taken 0:00:22.498110
Number of used sentences in train = 359
Total loss for epoch 8: 739.800381
	Epoch 9....
Epoch has taken 0:00:22.478336
Number of used sentences in train = 359
Total loss for epoch 9: 732.028266
	Epoch 10....
Epoch has taken 0:00:22.475552
Number of used sentences in train = 359
Total loss for epoch 10: 702.058281
	Epoch 11....
Epoch has taken 0:00:22.409221
Number of used sentences in train = 359
Total loss for epoch 11: 691.139513
	Epoch 12....
Epoch has taken 0:00:22.402563
Number of used sentences in train = 359
Total loss for epoch 12: 682.789072
	Epoch 13....
Epoch has taken 0:00:22.411820
Number of used sentences in train = 359
Total loss for epoch 13: 676.691351
	Epoch 14....
Epoch has taken 0:00:22.407220
Number of used sentences in train = 359
Total loss for epoch 14: 692.816903
Epoch has taken 0:00:22.396242

==================================================================================================
	Training time : 1:03:19.245118
==================================================================================================
	Identification : 0.237

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 35, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 24, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 116, 'lstmDropout': 0.31, 'denseActivation': 'tanh', 'wordDim': 110, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 24)
  (w_embeddings): Embedding(1882, 110)
  (lstm): LSTM(134, 116, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1856, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14587.787067
validation loss after epoch 0 : 920.387031
	Epoch 1....
Epoch has taken 0:02:52.485184
Number of used sentences in train = 2811
Total loss for epoch 1: 8410.653164
validation loss after epoch 1 : 879.337476
	Epoch 2....
Epoch has taken 0:02:52.572043
Number of used sentences in train = 2811
Total loss for epoch 2: 7357.606970
validation loss after epoch 2 : 851.086185
	Epoch 3....
Epoch has taken 0:02:52.596126
Number of used sentences in train = 2811
Total loss for epoch 3: 6780.807314
validation loss after epoch 3 : 822.887011
	Epoch 4....
Epoch has taken 0:02:51.277869
Number of used sentences in train = 2811
Total loss for epoch 4: 6324.588948
validation loss after epoch 4 : 819.237167
	Epoch 5....
Epoch has taken 0:02:51.699072
Number of used sentences in train = 2811
Total loss for epoch 5: 6042.434768
validation loss after epoch 5 : 867.784766
	Epoch 6....
Epoch has taken 0:02:53.309804
Number of used sentences in train = 2811
Total loss for epoch 6: 5761.610513
validation loss after epoch 6 : 850.520818
	Epoch 7....
Epoch has taken 0:02:52.918369
Number of used sentences in train = 2811
Total loss for epoch 7: 5555.562612
validation loss after epoch 7 : 874.001130
	Epoch 8....
Epoch has taken 0:02:53.044794
Number of used sentences in train = 2811
Total loss for epoch 8: 5417.317101
validation loss after epoch 8 : 883.007826
	Epoch 9....
Epoch has taken 0:02:51.254686
Number of used sentences in train = 2811
Total loss for epoch 9: 5238.469127
validation loss after epoch 9 : 942.321239
	Epoch 10....
Epoch has taken 0:02:51.376297
Number of used sentences in train = 2811
Total loss for epoch 10: 5149.927810
validation loss after epoch 10 : 955.135486
	Epoch 11....
Epoch has taken 0:02:53.308289
Number of used sentences in train = 2811
Total loss for epoch 11: 5033.338345
validation loss after epoch 11 : 925.804469
	Epoch 12....
Epoch has taken 0:03:19.225714
Number of used sentences in train = 2811
Total loss for epoch 12: 5011.615543
validation loss after epoch 12 : 945.713882
	Epoch 13....
Epoch has taken 0:02:52.937961
Number of used sentences in train = 2811
Total loss for epoch 13: 4910.564666
validation loss after epoch 13 : 991.964277
	Epoch 14....
Epoch has taken 0:02:51.420138
Number of used sentences in train = 2811
Total loss for epoch 14: 4891.375863
validation loss after epoch 14 : 991.242504
	TransitionClassifier(
  (p_embeddings): Embedding(18, 24)
  (w_embeddings): Embedding(1882, 110)
  (lstm): LSTM(134, 116, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1856, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.412925
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1350.667615
	Epoch 1....
Epoch has taken 0:00:18.309870
Number of used sentences in train = 313
Total loss for epoch 1: 847.253983
	Epoch 2....
Epoch has taken 0:00:18.333397
Number of used sentences in train = 313
Total loss for epoch 2: 692.352749
	Epoch 3....
Epoch has taken 0:00:18.312641
Number of used sentences in train = 313
Total loss for epoch 3: 623.753838
	Epoch 4....
Epoch has taken 0:00:18.310741
Number of used sentences in train = 313
Total loss for epoch 4: 583.712215
	Epoch 5....
Epoch has taken 0:00:18.310727
Number of used sentences in train = 313
Total loss for epoch 5: 589.497058
	Epoch 6....
Epoch has taken 0:00:18.337345
Number of used sentences in train = 313
Total loss for epoch 6: 544.564135
	Epoch 7....
Epoch has taken 0:00:18.285171
Number of used sentences in train = 313
Total loss for epoch 7: 530.301794
	Epoch 8....
Epoch has taken 0:00:18.288900
Number of used sentences in train = 313
Total loss for epoch 8: 525.987559
	Epoch 9....
Epoch has taken 0:00:18.329908
Number of used sentences in train = 313
Total loss for epoch 9: 524.998225
	Epoch 10....
Epoch has taken 0:00:18.314678
Number of used sentences in train = 313
Total loss for epoch 10: 534.851550
	Epoch 11....
Epoch has taken 0:00:18.270930
Number of used sentences in train = 313
Total loss for epoch 11: 522.722216
	Epoch 12....
Epoch has taken 0:00:18.247017
Number of used sentences in train = 313
Total loss for epoch 12: 517.149170
	Epoch 13....
Epoch has taken 0:00:18.280168
Number of used sentences in train = 313
Total loss for epoch 13: 515.837592
	Epoch 14....
Epoch has taken 0:00:18.264478
Number of used sentences in train = 313
Total loss for epoch 14: 511.186833
Epoch has taken 0:00:18.259057

==================================================================================================
	Training time : 0:48:05.801607
==================================================================================================
	Identification : 0.082

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 24)
  (w_embeddings): Embedding(1680, 110)
  (lstm): LSTM(134, 116, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1856, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 14921.221128
validation loss after epoch 0 : 1045.737908
	Epoch 1....
Epoch has taken 0:01:57.132128
Number of used sentences in train = 2074
Total loss for epoch 1: 6693.692839
validation loss after epoch 1 : 726.626361
	Epoch 2....
Epoch has taken 0:01:57.275014
Number of used sentences in train = 2074
Total loss for epoch 2: 5488.868337
validation loss after epoch 2 : 696.531812
	Epoch 3....
Epoch has taken 0:01:58.260552
Number of used sentences in train = 2074
Total loss for epoch 3: 4965.074826
validation loss after epoch 3 : 680.170294
	Epoch 4....
Epoch has taken 0:01:58.242390
Number of used sentences in train = 2074
Total loss for epoch 4: 4554.188851
validation loss after epoch 4 : 682.246746
	Epoch 5....
Epoch has taken 0:01:58.251534
Number of used sentences in train = 2074
Total loss for epoch 5: 4230.472573
validation loss after epoch 5 : 740.830204
	Epoch 6....
Epoch has taken 0:01:58.111381
Number of used sentences in train = 2074
Total loss for epoch 6: 4066.799516
validation loss after epoch 6 : 707.221174
	Epoch 7....
Epoch has taken 0:01:57.059598
Number of used sentences in train = 2074
Total loss for epoch 7: 3845.046489
validation loss after epoch 7 : 739.017797
	Epoch 8....
Epoch has taken 0:01:57.118258
Number of used sentences in train = 2074
Total loss for epoch 8: 3631.209088
validation loss after epoch 8 : 797.863270
	Epoch 9....
Epoch has taken 0:01:58.326107
Number of used sentences in train = 2074
Total loss for epoch 9: 3575.177950
validation loss after epoch 9 : 876.866921
	Epoch 10....
Epoch has taken 0:02:13.767564
Number of used sentences in train = 2074
Total loss for epoch 10: 3547.904729
validation loss after epoch 10 : 800.988186
	Epoch 11....
Epoch has taken 0:02:10.074990
Number of used sentences in train = 2074
Total loss for epoch 11: 3445.687077
validation loss after epoch 11 : 808.458263
	Epoch 12....
Epoch has taken 0:02:09.160454
Number of used sentences in train = 2074
Total loss for epoch 12: 3390.348843
validation loss after epoch 12 : 829.035250
	Epoch 13....
Epoch has taken 0:01:59.749105
Number of used sentences in train = 2074
Total loss for epoch 13: 3357.183079
validation loss after epoch 13 : 847.407560
	Epoch 14....
Epoch has taken 0:01:58.344186
Number of used sentences in train = 2074
Total loss for epoch 14: 3359.938352
validation loss after epoch 14 : 803.749813
	TransitionClassifier(
  (p_embeddings): Embedding(18, 24)
  (w_embeddings): Embedding(1680, 110)
  (lstm): LSTM(134, 116, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1856, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:16.051599
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1413.865794
	Epoch 1....
Epoch has taken 0:00:12.076941
Number of used sentences in train = 231
Total loss for epoch 1: 844.758048
	Epoch 2....
Epoch has taken 0:00:12.127010
Number of used sentences in train = 231
Total loss for epoch 2: 646.407785
	Epoch 3....
Epoch has taken 0:00:12.107904
Number of used sentences in train = 231
Total loss for epoch 3: 553.449342
	Epoch 4....
Epoch has taken 0:00:12.115949
Number of used sentences in train = 231
Total loss for epoch 4: 517.796748
	Epoch 5....
Epoch has taken 0:00:12.076285
Number of used sentences in train = 231
Total loss for epoch 5: 465.999689
	Epoch 6....
Epoch has taken 0:00:12.080384
Number of used sentences in train = 231
Total loss for epoch 6: 469.159153
	Epoch 7....
Epoch has taken 0:00:12.063042
Number of used sentences in train = 231
Total loss for epoch 7: 439.318668
	Epoch 8....
Epoch has taken 0:00:12.085414
Number of used sentences in train = 231
Total loss for epoch 8: 413.331399
	Epoch 9....
Epoch has taken 0:00:12.065822
Number of used sentences in train = 231
Total loss for epoch 9: 402.548250
	Epoch 10....
Epoch has taken 0:00:12.077133
Number of used sentences in train = 231
Total loss for epoch 10: 398.451092
	Epoch 11....
Epoch has taken 0:00:12.094103
Number of used sentences in train = 231
Total loss for epoch 11: 367.577084
	Epoch 12....
Epoch has taken 0:00:12.079533
Number of used sentences in train = 231
Total loss for epoch 12: 361.124500
	Epoch 13....
Epoch has taken 0:00:12.065538
Number of used sentences in train = 231
Total loss for epoch 13: 355.860818
	Epoch 14....
Epoch has taken 0:00:12.098467
Number of used sentences in train = 231
Total loss for epoch 14: 354.569822
Epoch has taken 0:00:12.067079

==================================================================================================
	Training time : 0:33:28.545172
==================================================================================================
	Identification : 0.351

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 24)
  (w_embeddings): Embedding(3369, 110)
  (lstm): LSTM(134, 116, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1856, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 29227.162644
validation loss after epoch 0 : 1184.155882
	Epoch 1....
Epoch has taken 0:03:49.480349
Number of used sentences in train = 3226
Total loss for epoch 1: 10350.398064
validation loss after epoch 1 : 1071.627633
	Epoch 2....
Epoch has taken 0:03:49.491391
Number of used sentences in train = 3226
Total loss for epoch 2: 9106.363356
validation loss after epoch 2 : 1046.636762
	Epoch 3....
Epoch has taken 0:03:49.722675
Number of used sentences in train = 3226
Total loss for epoch 3: 8532.558690
validation loss after epoch 3 : 1055.423289
	Epoch 4....
Epoch has taken 0:03:51.647490
Number of used sentences in train = 3226
Total loss for epoch 4: 7948.469065
validation loss after epoch 4 : 1128.706544
	Epoch 5....
Epoch has taken 0:04:01.503719
Number of used sentences in train = 3226
Total loss for epoch 5: 7663.402576
validation loss after epoch 5 : 1117.701203
	Epoch 6....
Epoch has taken 0:03:51.778985
Number of used sentences in train = 3226
Total loss for epoch 6: 7331.617285
validation loss after epoch 6 : 1179.308339
	Epoch 7....
Epoch has taken 0:03:49.873107
Number of used sentences in train = 3226
Total loss for epoch 7: 7204.075406
validation loss after epoch 7 : 1157.442840
	Epoch 8....
Epoch has taken 0:03:51.729960
Number of used sentences in train = 3226
Total loss for epoch 8: 6982.069350
validation loss after epoch 8 : 1194.133453
	Epoch 9....
Epoch has taken 0:04:26.737183
Number of used sentences in train = 3226
Total loss for epoch 9: 6824.828286
validation loss after epoch 9 : 1282.330584
	Epoch 10....
Epoch has taken 0:03:49.810892
Number of used sentences in train = 3226
Total loss for epoch 10: 6730.810938
validation loss after epoch 10 : 1303.214321
	Epoch 11....
Epoch has taken 0:03:53.035298
Number of used sentences in train = 3226
Total loss for epoch 11: 6605.547700
validation loss after epoch 11 : 1327.460173
	Epoch 12....
Epoch has taken 0:04:14.760281
Number of used sentences in train = 3226
Total loss for epoch 12: 6559.772830
validation loss after epoch 12 : 1364.265703
	Epoch 13....
Epoch has taken 0:04:12.625118
Number of used sentences in train = 3226
Total loss for epoch 13: 6505.970299
validation loss after epoch 13 : 1318.977438
	Epoch 14....
Epoch has taken 0:04:12.643429
Number of used sentences in train = 3226
Total loss for epoch 14: 6456.821573
validation loss after epoch 14 : 1381.185414
	TransitionClassifier(
  (p_embeddings): Embedding(13, 24)
  (w_embeddings): Embedding(3369, 110)
  (lstm): LSTM(134, 116, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1856, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:14.617108
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1889.609240
	Epoch 1....
Epoch has taken 0:00:25.153283
Number of used sentences in train = 359
Total loss for epoch 1: 1100.966928
	Epoch 2....
Epoch has taken 0:00:25.130127
Number of used sentences in train = 359
Total loss for epoch 2: 917.138899
	Epoch 3....
Epoch has taken 0:00:25.138734
Number of used sentences in train = 359
Total loss for epoch 3: 845.050205
	Epoch 4....
Epoch has taken 0:00:25.148157
Number of used sentences in train = 359
Total loss for epoch 4: 776.862675
	Epoch 5....
Epoch has taken 0:00:25.150438
Number of used sentences in train = 359
Total loss for epoch 5: 750.636560
	Epoch 6....
Epoch has taken 0:00:25.162233
Number of used sentences in train = 359
Total loss for epoch 6: 716.549924
	Epoch 7....
Epoch has taken 0:00:25.150528
Number of used sentences in train = 359
Total loss for epoch 7: 721.621111
	Epoch 8....
Epoch has taken 0:00:25.132495
Number of used sentences in train = 359
Total loss for epoch 8: 704.491323
	Epoch 9....
Epoch has taken 0:00:25.141017
Number of used sentences in train = 359
Total loss for epoch 9: 687.019441
	Epoch 10....
Epoch has taken 0:00:25.174222
Number of used sentences in train = 359
Total loss for epoch 10: 688.575578
	Epoch 11....
Epoch has taken 0:00:25.164810
Number of used sentences in train = 359
Total loss for epoch 11: 684.423811
	Epoch 12....
Epoch has taken 0:00:25.134339
Number of used sentences in train = 359
Total loss for epoch 12: 681.752733
	Epoch 13....
Epoch has taken 0:00:25.138087
Number of used sentences in train = 359
Total loss for epoch 13: 672.800020
	Epoch 14....
Epoch has taken 0:00:25.156944
Number of used sentences in train = 359
Total loss for epoch 14: 675.345393
Epoch has taken 0:00:25.130252

==================================================================================================
	Training time : 1:06:17.329164
==================================================================================================
	Identification : 0.037

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 20, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 19, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 109, 'lstmDropout': 0.3, 'denseActivation': 'tanh', 'wordDim': 52, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(9350, 52)
  (lstm): LSTM(71, 109, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 22074.824295
validation loss after epoch 0 : 1448.389510
	Epoch 1....
Epoch has taken 0:03:09.581584
Number of used sentences in train = 2811
Total loss for epoch 1: 12466.928856
validation loss after epoch 1 : 1206.567142
	Epoch 2....
Epoch has taken 0:03:09.521871
Number of used sentences in train = 2811
Total loss for epoch 2: 10220.157861
validation loss after epoch 2 : 1187.726879
	Epoch 3....
Epoch has taken 0:03:08.130337
Number of used sentences in train = 2811
Total loss for epoch 3: 8943.295731
validation loss after epoch 3 : 1124.047283
	Epoch 4....
Epoch has taken 0:03:09.496241
Number of used sentences in train = 2811
Total loss for epoch 4: 7993.983425
validation loss after epoch 4 : 1128.229377
	Epoch 5....
Epoch has taken 0:03:09.677137
Number of used sentences in train = 2811
Total loss for epoch 5: 7113.402807
validation loss after epoch 5 : 1117.526670
	Epoch 6....
Epoch has taken 0:03:09.389535
Number of used sentences in train = 2811
Total loss for epoch 6: 6496.422337
validation loss after epoch 6 : 1204.032533
	Epoch 7....
Epoch has taken 0:03:08.086147
Number of used sentences in train = 2811
Total loss for epoch 7: 6082.205223
validation loss after epoch 7 : 1294.227578
	Epoch 8....
Epoch has taken 0:03:11.033065
Number of used sentences in train = 2811
Total loss for epoch 8: 5780.680479
validation loss after epoch 8 : 1280.584474
	Epoch 9....
Epoch has taken 0:03:09.502664
Number of used sentences in train = 2811
Total loss for epoch 9: 5604.087044
validation loss after epoch 9 : 1422.464769
	Epoch 10....
Epoch has taken 0:03:09.914697
Number of used sentences in train = 2811
Total loss for epoch 10: 5640.092061
validation loss after epoch 10 : 1378.024539
	Epoch 11....
Epoch has taken 0:03:09.393431
Number of used sentences in train = 2811
Total loss for epoch 11: 5432.379192
validation loss after epoch 11 : 1405.230063
	Epoch 12....
Epoch has taken 0:03:08.004278
Number of used sentences in train = 2811
Total loss for epoch 12: 5331.359638
validation loss after epoch 12 : 1461.297350
	Epoch 13....
Epoch has taken 0:03:08.234133
Number of used sentences in train = 2811
Total loss for epoch 13: 5218.852385
validation loss after epoch 13 : 1484.080885
	Epoch 14....
Epoch has taken 0:03:09.752590
Number of used sentences in train = 2811
Total loss for epoch 14: 5089.131970
validation loss after epoch 14 : 1457.270234
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(9350, 52)
  (lstm): LSTM(71, 109, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:09.443581
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1804.796960
	Epoch 1....
Epoch has taken 0:00:20.220119
Number of used sentences in train = 313
Total loss for epoch 1: 1078.596934
	Epoch 2....
Epoch has taken 0:00:20.216888
Number of used sentences in train = 313
Total loss for epoch 2: 894.339142
	Epoch 3....
Epoch has taken 0:00:20.214129
Number of used sentences in train = 313
Total loss for epoch 3: 769.261397
	Epoch 4....
Epoch has taken 0:00:20.192166
Number of used sentences in train = 313
Total loss for epoch 4: 714.212309
	Epoch 5....
Epoch has taken 0:00:20.181698
Number of used sentences in train = 313
Total loss for epoch 5: 621.415781
	Epoch 6....
Epoch has taken 0:00:20.175545
Number of used sentences in train = 313
Total loss for epoch 6: 608.579905
	Epoch 7....
Epoch has taken 0:00:20.181734
Number of used sentences in train = 313
Total loss for epoch 7: 617.110119
	Epoch 8....
Epoch has taken 0:00:20.178586
Number of used sentences in train = 313
Total loss for epoch 8: 586.044214
	Epoch 9....
Epoch has taken 0:00:20.193660
Number of used sentences in train = 313
Total loss for epoch 9: 555.680947
	Epoch 10....
Epoch has taken 0:00:20.177889
Number of used sentences in train = 313
Total loss for epoch 10: 565.910079
	Epoch 11....
Epoch has taken 0:00:20.193779
Number of used sentences in train = 313
Total loss for epoch 11: 536.283237
	Epoch 12....
Epoch has taken 0:00:20.195521
Number of used sentences in train = 313
Total loss for epoch 12: 535.540426
	Epoch 13....
Epoch has taken 0:00:20.182053
Number of used sentences in train = 313
Total loss for epoch 13: 526.600886
	Epoch 14....
Epoch has taken 0:00:20.179541
Number of used sentences in train = 313
Total loss for epoch 14: 529.083049
Epoch has taken 0:00:20.184381

==================================================================================================
	Training time : 0:52:22.535168
==================================================================================================
	Identification : 0.16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(7079, 52)
  (lstm): LSTM(71, 109, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 21394.102899
validation loss after epoch 0 : 2372.057595
	Epoch 1....
Epoch has taken 0:02:08.694111
Number of used sentences in train = 2074
Total loss for epoch 1: 13994.799340
validation loss after epoch 1 : 1149.298585
	Epoch 2....
Epoch has taken 0:02:08.721997
Number of used sentences in train = 2074
Total loss for epoch 2: 8588.289550
validation loss after epoch 2 : 965.443099
	Epoch 3....
Epoch has taken 0:02:09.953628
Number of used sentences in train = 2074
Total loss for epoch 3: 6749.306349
validation loss after epoch 3 : 907.626973
	Epoch 4....
Epoch has taken 0:02:12.334214
Number of used sentences in train = 2074
Total loss for epoch 4: 5673.318102
validation loss after epoch 4 : 978.909890
	Epoch 5....
Epoch has taken 0:02:09.716677
Number of used sentences in train = 2074
Total loss for epoch 5: 5037.436092
validation loss after epoch 5 : 915.839831
	Epoch 6....
Epoch has taken 0:02:08.861377
Number of used sentences in train = 2074
Total loss for epoch 6: 4548.131771
validation loss after epoch 6 : 1120.252642
	Epoch 7....
Epoch has taken 0:02:08.697653
Number of used sentences in train = 2074
Total loss for epoch 7: 4251.976521
validation loss after epoch 7 : 1055.799044
	Epoch 8....
Epoch has taken 0:02:09.909935
Number of used sentences in train = 2074
Total loss for epoch 8: 4012.119062
validation loss after epoch 8 : 1084.727436
	Epoch 9....
Epoch has taken 0:02:12.309609
Number of used sentences in train = 2074
Total loss for epoch 9: 3850.210724
validation loss after epoch 9 : 1127.544417
	Epoch 10....
Epoch has taken 0:02:09.905787
Number of used sentences in train = 2074
Total loss for epoch 10: 3720.722057
validation loss after epoch 10 : 1161.958570
	Epoch 11....
Epoch has taken 0:02:08.710454
Number of used sentences in train = 2074
Total loss for epoch 11: 3599.118020
validation loss after epoch 11 : 1301.062520
	Epoch 12....
Epoch has taken 0:02:08.846974
Number of used sentences in train = 2074
Total loss for epoch 12: 3522.418439
validation loss after epoch 12 : 1477.429135
	Epoch 13....
Epoch has taken 0:02:08.721212
Number of used sentences in train = 2074
Total loss for epoch 13: 3424.021131
validation loss after epoch 13 : 1369.118056
	Epoch 14....
Epoch has taken 0:02:02.422239
Number of used sentences in train = 2074
Total loss for epoch 14: 3402.135433
validation loss after epoch 14 : 1362.614931
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(7079, 52)
  (lstm): LSTM(71, 109, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:15.885134
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1473.051422
	Epoch 1....
Epoch has taken 0:00:12.031669
Number of used sentences in train = 231
Total loss for epoch 1: 884.991890
	Epoch 2....
Epoch has taken 0:00:12.062093
Number of used sentences in train = 231
Total loss for epoch 2: 643.180765
	Epoch 3....
Epoch has taken 0:00:12.026816
Number of used sentences in train = 231
Total loss for epoch 3: 544.079886
	Epoch 4....
Epoch has taken 0:00:12.001477
Number of used sentences in train = 231
Total loss for epoch 4: 467.907321
	Epoch 5....
Epoch has taken 0:00:11.992539
Number of used sentences in train = 231
Total loss for epoch 5: 424.914616
	Epoch 6....
Epoch has taken 0:00:12.026324
Number of used sentences in train = 231
Total loss for epoch 6: 409.387666
	Epoch 7....
Epoch has taken 0:00:11.916373
Number of used sentences in train = 231
Total loss for epoch 7: 378.944947
	Epoch 8....
Epoch has taken 0:00:11.939869
Number of used sentences in train = 231
Total loss for epoch 8: 372.365706
	Epoch 9....
Epoch has taken 0:00:11.914519
Number of used sentences in train = 231
Total loss for epoch 9: 361.453245
	Epoch 10....
Epoch has taken 0:00:11.926345
Number of used sentences in train = 231
Total loss for epoch 10: 359.343570
	Epoch 11....
Epoch has taken 0:00:11.896321
Number of used sentences in train = 231
Total loss for epoch 11: 357.336641
	Epoch 12....
Epoch has taken 0:00:11.937666
Number of used sentences in train = 231
Total loss for epoch 12: 357.388695
	Epoch 13....
Epoch has taken 0:00:11.900820
Number of used sentences in train = 231
Total loss for epoch 13: 352.277668
	Epoch 14....
Epoch has taken 0:00:11.921979
Number of used sentences in train = 231
Total loss for epoch 14: 352.148173
Epoch has taken 0:00:11.913920

==================================================================================================
	Training time : 0:35:23.440655
==================================================================================================
	Identification : 0.091

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(18112, 52)
  (lstm): LSTM(71, 109, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 24315.621237
validation loss after epoch 0 : 1793.784555
	Epoch 1....
Epoch has taken 0:03:48.794795
Number of used sentences in train = 3226
Total loss for epoch 1: 14814.968282
validation loss after epoch 1 : 1640.625279
	Epoch 2....
Epoch has taken 0:03:48.600969
Number of used sentences in train = 3226
Total loss for epoch 2: 12239.514075
validation loss after epoch 2 : 1637.992012
	Epoch 3....
Epoch has taken 0:04:23.052670
Number of used sentences in train = 3226
Total loss for epoch 3: 10472.766952
validation loss after epoch 3 : 1642.885136
	Epoch 4....
Epoch has taken 0:03:47.993285
Number of used sentences in train = 3226
Total loss for epoch 4: 9213.865758
validation loss after epoch 4 : 1764.340464
	Epoch 5....
Epoch has taken 0:03:45.919534
Number of used sentences in train = 3226
Total loss for epoch 5: 8396.561131
validation loss after epoch 5 : 1917.219412
	Epoch 6....
Epoch has taken 0:03:46.051857
Number of used sentences in train = 3226
Total loss for epoch 6: 7770.753802
validation loss after epoch 6 : 2078.460199
	Epoch 7....
Epoch has taken 0:03:50.780897
Number of used sentences in train = 3226
Total loss for epoch 7: 7387.206177
validation loss after epoch 7 : 2060.289121
	Epoch 8....
Epoch has taken 0:03:48.186739
Number of used sentences in train = 3226
Total loss for epoch 8: 6975.844097
validation loss after epoch 8 : 2176.989843
	Epoch 9....
Epoch has taken 0:03:47.724939
Number of used sentences in train = 3226
Total loss for epoch 9: 6810.977584
validation loss after epoch 9 : 2165.626067
	Epoch 10....
Epoch has taken 0:03:45.666414
Number of used sentences in train = 3226
Total loss for epoch 10: 6743.563323
validation loss after epoch 10 : 2412.242690
	Epoch 11....
Epoch has taken 0:03:48.504025
Number of used sentences in train = 3226
Total loss for epoch 11: 6608.815159
validation loss after epoch 11 : 2507.596261
	Epoch 12....
Epoch has taken 0:04:23.387746
Number of used sentences in train = 3226
Total loss for epoch 12: 6531.320875
validation loss after epoch 12 : 2605.333924
	Epoch 13....
Epoch has taken 0:03:46.512401
Number of used sentences in train = 3226
Total loss for epoch 13: 6427.692175
validation loss after epoch 13 : 2605.264097
	Epoch 14....
Epoch has taken 0:03:46.921600
Number of used sentences in train = 3226
Total loss for epoch 14: 6447.928015
validation loss after epoch 14 : 2702.078916
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(18112, 52)
  (lstm): LSTM(71, 109, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.772487
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2336.614240
	Epoch 1....
Epoch has taken 0:00:22.482685
Number of used sentences in train = 359
Total loss for epoch 1: 1466.230545
	Epoch 2....
Epoch has taken 0:00:22.491980
Number of used sentences in train = 359
Total loss for epoch 2: 1182.963839
	Epoch 3....
Epoch has taken 0:00:22.507271
Number of used sentences in train = 359
Total loss for epoch 3: 971.457175
	Epoch 4....
Epoch has taken 0:00:22.461911
Number of used sentences in train = 359
Total loss for epoch 4: 868.637886
	Epoch 5....
Epoch has taken 0:00:22.459621
Number of used sentences in train = 359
Total loss for epoch 5: 792.916788
	Epoch 6....
Epoch has taken 0:00:22.433312
Number of used sentences in train = 359
Total loss for epoch 6: 742.298084
	Epoch 7....
Epoch has taken 0:00:22.441345
Number of used sentences in train = 359
Total loss for epoch 7: 738.440011
	Epoch 8....
Epoch has taken 0:00:22.471683
Number of used sentences in train = 359
Total loss for epoch 8: 701.956420
	Epoch 9....
Epoch has taken 0:00:22.444954
Number of used sentences in train = 359
Total loss for epoch 9: 690.395851
	Epoch 10....
Epoch has taken 0:00:22.458389
Number of used sentences in train = 359
Total loss for epoch 10: 690.060452
	Epoch 11....
Epoch has taken 0:00:22.467744
Number of used sentences in train = 359
Total loss for epoch 11: 702.736221
	Epoch 12....
Epoch has taken 0:00:22.472699
Number of used sentences in train = 359
Total loss for epoch 12: 697.895588
	Epoch 13....
Epoch has taken 0:00:22.479730
Number of used sentences in train = 359
Total loss for epoch 13: 684.344292
	Epoch 14....
Epoch has taken 0:00:22.464925
Number of used sentences in train = 359
Total loss for epoch 14: 676.981112
Epoch has taken 0:00:22.457231

==================================================================================================
	Training time : 1:03:44.538293
==================================================================================================
	Identification : 0.011

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 62, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 32, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 31, 'lstmDropout': 0.29, 'denseActivation': 'tanh', 'wordDim': 155, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1882, 155)
  (lstm): LSTM(187, 31, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 9834.753145
validation loss after epoch 0 : 838.824269
	Epoch 1....
Epoch has taken 0:02:52.143824
Number of used sentences in train = 2811
Total loss for epoch 1: 7153.063742
validation loss after epoch 1 : 793.377948
	Epoch 2....
Epoch has taken 0:02:51.333001
Number of used sentences in train = 2811
Total loss for epoch 2: 6433.707742
validation loss after epoch 2 : 843.265952
	Epoch 3....
Epoch has taken 0:03:07.045450
Number of used sentences in train = 2811
Total loss for epoch 3: 6121.678348
validation loss after epoch 3 : 836.171630
	Epoch 4....
Epoch has taken 0:02:51.209661
Number of used sentences in train = 2811
Total loss for epoch 4: 5770.377003
validation loss after epoch 4 : 815.056737
	Epoch 5....
Epoch has taken 0:02:51.389552
Number of used sentences in train = 2811
Total loss for epoch 5: 5551.740344
validation loss after epoch 5 : 830.241394
	Epoch 6....
Epoch has taken 0:03:14.775937
Number of used sentences in train = 2811
Total loss for epoch 6: 5384.333699
validation loss after epoch 6 : 859.444212
	Epoch 7....
Epoch has taken 0:02:51.214845
Number of used sentences in train = 2811
Total loss for epoch 7: 5227.505679
validation loss after epoch 7 : 864.588498
	Epoch 8....
Epoch has taken 0:02:49.574462
Number of used sentences in train = 2811
Total loss for epoch 8: 5135.793534
validation loss after epoch 8 : 898.593713
	Epoch 9....
Epoch has taken 0:02:51.161487
Number of used sentences in train = 2811
Total loss for epoch 9: 5031.562077
validation loss after epoch 9 : 994.148968
	Epoch 10....
Epoch has taken 0:02:51.403422
Number of used sentences in train = 2811
Total loss for epoch 10: 5027.352713
validation loss after epoch 10 : 898.225871
	Epoch 11....
Epoch has taken 0:03:17.238832
Number of used sentences in train = 2811
Total loss for epoch 11: 4954.767853
validation loss after epoch 11 : 907.802683
	Epoch 12....
Epoch has taken 0:02:51.523012
Number of used sentences in train = 2811
Total loss for epoch 12: 4885.084763
validation loss after epoch 12 : 980.498479
	Epoch 13....
Epoch has taken 0:03:02.491020
Number of used sentences in train = 2811
Total loss for epoch 13: 4855.555831
validation loss after epoch 13 : 958.835775
	Epoch 14....
Epoch has taken 0:02:58.613110
Number of used sentences in train = 2811
Total loss for epoch 14: 4848.599032
validation loss after epoch 14 : 938.248588
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1882, 155)
  (lstm): LSTM(187, 31, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:07.385041
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1229.971151
	Epoch 1....
Epoch has taken 0:00:20.232188
Number of used sentences in train = 313
Total loss for epoch 1: 678.104152
	Epoch 2....
Epoch has taken 0:00:20.119624
Number of used sentences in train = 313
Total loss for epoch 2: 607.271981
	Epoch 3....
Epoch has taken 0:00:19.862861
Number of used sentences in train = 313
Total loss for epoch 3: 568.979709
	Epoch 4....
Epoch has taken 0:00:19.946600
Number of used sentences in train = 313
Total loss for epoch 4: 543.930360
	Epoch 5....
Epoch has taken 0:00:19.901280
Number of used sentences in train = 313
Total loss for epoch 5: 525.838926
	Epoch 6....
Epoch has taken 0:00:19.963266
Number of used sentences in train = 313
Total loss for epoch 6: 520.038004
	Epoch 7....
Epoch has taken 0:00:19.930186
Number of used sentences in train = 313
Total loss for epoch 7: 511.716195
	Epoch 8....
Epoch has taken 0:00:19.853688
Number of used sentences in train = 313
Total loss for epoch 8: 513.943296
	Epoch 9....
Epoch has taken 0:00:19.850263
Number of used sentences in train = 313
Total loss for epoch 9: 506.218526
	Epoch 10....
Epoch has taken 0:00:20.024860
Number of used sentences in train = 313
Total loss for epoch 10: 510.702967
	Epoch 11....
Epoch has taken 0:00:19.836703
Number of used sentences in train = 313
Total loss for epoch 11: 506.262976
	Epoch 12....
Epoch has taken 0:00:19.805313
Number of used sentences in train = 313
Total loss for epoch 12: 505.482732
	Epoch 13....
Epoch has taken 0:00:19.802282
Number of used sentences in train = 313
Total loss for epoch 13: 504.510826
	Epoch 14....
Epoch has taken 0:00:19.798929
Number of used sentences in train = 313
Total loss for epoch 14: 503.877748
Epoch has taken 0:00:19.983973

==================================================================================================
	Training time : 0:49:27.932385
==================================================================================================
	Identification : 0.141

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1680, 155)
  (lstm): LSTM(187, 31, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8625.198781
validation loss after epoch 0 : 670.204304
	Epoch 1....
Epoch has taken 0:02:06.957371
Number of used sentences in train = 2074
Total loss for epoch 1: 5289.951388
validation loss after epoch 1 : 638.407860
	Epoch 2....
Epoch has taken 0:02:06.486795
Number of used sentences in train = 2074
Total loss for epoch 2: 4620.776466
validation loss after epoch 2 : 631.258034
	Epoch 3....
Epoch has taken 0:02:08.153080
Number of used sentences in train = 2074
Total loss for epoch 3: 4188.866503
validation loss after epoch 3 : 685.979733
	Epoch 4....
Epoch has taken 0:02:07.681110
Number of used sentences in train = 2074
Total loss for epoch 4: 3956.613608
validation loss after epoch 4 : 669.881778
	Epoch 5....
Epoch has taken 0:02:08.268355
Number of used sentences in train = 2074
Total loss for epoch 5: 3792.566499
validation loss after epoch 5 : 731.487875
	Epoch 6....
Epoch has taken 0:02:07.745729
Number of used sentences in train = 2074
Total loss for epoch 6: 3644.504308
validation loss after epoch 6 : 738.609629
	Epoch 7....
Epoch has taken 0:02:08.091701
Number of used sentences in train = 2074
Total loss for epoch 7: 3547.806625
validation loss after epoch 7 : 724.847896
	Epoch 8....
Epoch has taken 0:02:08.539474
Number of used sentences in train = 2074
Total loss for epoch 8: 3474.812449
validation loss after epoch 8 : 808.350973
	Epoch 9....
Epoch has taken 0:02:08.318178
Number of used sentences in train = 2074
Total loss for epoch 9: 3413.814763
validation loss after epoch 9 : 797.814093
	Epoch 10....
Epoch has taken 0:02:14.552593
Number of used sentences in train = 2074
Total loss for epoch 10: 3417.906607
validation loss after epoch 10 : 799.159861
	Epoch 11....
Epoch has taken 0:02:21.040770
Number of used sentences in train = 2074
Total loss for epoch 11: 3368.372684
validation loss after epoch 11 : 816.117311
	Epoch 12....
Epoch has taken 0:02:12.119262
Number of used sentences in train = 2074
Total loss for epoch 12: 3311.588088
validation loss after epoch 12 : 824.540899
	Epoch 13....
Epoch has taken 0:01:57.212089
Number of used sentences in train = 2074
Total loss for epoch 13: 3315.632999
validation loss after epoch 13 : 869.117968
	Epoch 14....
Epoch has taken 0:02:00.256372
Number of used sentences in train = 2074
Total loss for epoch 14: 3302.633689
validation loss after epoch 14 : 972.612505
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1680, 155)
  (lstm): LSTM(187, 31, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.448152
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1386.237139
	Epoch 1....
Epoch has taken 0:00:11.864631
Number of used sentences in train = 231
Total loss for epoch 1: 557.485329
	Epoch 2....
Epoch has taken 0:00:11.828859
Number of used sentences in train = 231
Total loss for epoch 2: 453.952768
	Epoch 3....
Epoch has taken 0:00:11.837864
Number of used sentences in train = 231
Total loss for epoch 3: 402.230992
	Epoch 4....
Epoch has taken 0:00:11.830541
Number of used sentences in train = 231
Total loss for epoch 4: 383.635745
	Epoch 5....
Epoch has taken 0:00:11.848733
Number of used sentences in train = 231
Total loss for epoch 5: 378.832646
	Epoch 6....
Epoch has taken 0:00:11.841747
Number of used sentences in train = 231
Total loss for epoch 6: 366.331401
	Epoch 7....
Epoch has taken 0:00:11.863044
Number of used sentences in train = 231
Total loss for epoch 7: 357.763488
	Epoch 8....
Epoch has taken 0:00:11.831560
Number of used sentences in train = 231
Total loss for epoch 8: 361.433676
	Epoch 9....
Epoch has taken 0:00:11.860306
Number of used sentences in train = 231
Total loss for epoch 9: 357.564667
	Epoch 10....
Epoch has taken 0:00:11.826307
Number of used sentences in train = 231
Total loss for epoch 10: 363.299854
	Epoch 11....
Epoch has taken 0:00:11.875129
Number of used sentences in train = 231
Total loss for epoch 11: 353.379951
	Epoch 12....
Epoch has taken 0:00:11.835538
Number of used sentences in train = 231
Total loss for epoch 12: 355.523277
	Epoch 13....
Epoch has taken 0:00:11.959380
Number of used sentences in train = 231
Total loss for epoch 13: 352.079672
	Epoch 14....
Epoch has taken 0:00:11.921768
Number of used sentences in train = 231
Total loss for epoch 14: 353.284848
Epoch has taken 0:00:11.969962

==================================================================================================
	Training time : 0:34:50.208656
==================================================================================================
	Identification : 0.373

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 32)
  (w_embeddings): Embedding(3369, 155)
  (lstm): LSTM(187, 31, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12153.756676
validation loss after epoch 0 : 1101.264788
	Epoch 1....
Epoch has taken 0:03:46.987461
Number of used sentences in train = 3226
Total loss for epoch 1: 9387.879603
validation loss after epoch 1 : 1065.864352
	Epoch 2....
Epoch has taken 0:04:01.177862
Number of used sentences in train = 3226
Total loss for epoch 2: 8590.330640
validation loss after epoch 2 : 1084.346089
	Epoch 3....
Epoch has taken 0:03:46.907111
Number of used sentences in train = 3226
Total loss for epoch 3: 8077.167480
validation loss after epoch 3 : 1099.595506
	Epoch 4....
Epoch has taken 0:03:45.387859
Number of used sentences in train = 3226
Total loss for epoch 4: 7690.659921
validation loss after epoch 4 : 1098.312479
	Epoch 5....
Epoch has taken 0:03:45.564400
Number of used sentences in train = 3226
Total loss for epoch 5: 7406.110355
validation loss after epoch 5 : 1161.931293
	Epoch 6....
Epoch has taken 0:03:47.669082
Number of used sentences in train = 3226
Total loss for epoch 6: 7217.447662
validation loss after epoch 6 : 1157.633905
	Epoch 7....
Epoch has taken 0:03:55.431340
Number of used sentences in train = 3226
Total loss for epoch 7: 7083.747216
validation loss after epoch 7 : 1188.900369
	Epoch 8....
Epoch has taken 0:03:45.287029
Number of used sentences in train = 3226
Total loss for epoch 8: 6945.997101
validation loss after epoch 8 : 1336.409950
	Epoch 9....
Epoch has taken 0:03:47.460071
Number of used sentences in train = 3226
Total loss for epoch 9: 6824.324351
validation loss after epoch 9 : 1293.006067
	Epoch 10....
Epoch has taken 0:03:47.187299
Number of used sentences in train = 3226
Total loss for epoch 10: 6790.588622
validation loss after epoch 10 : 1250.878045
	Epoch 11....
Epoch has taken 0:03:47.409148
Number of used sentences in train = 3226
Total loss for epoch 11: 6678.650447
validation loss after epoch 11 : 1363.129071
	Epoch 12....
Epoch has taken 0:03:46.890179
Number of used sentences in train = 3226
Total loss for epoch 12: 6638.518672
validation loss after epoch 12 : 1351.953263
	Epoch 13....
Epoch has taken 0:03:45.262375
Number of used sentences in train = 3226
Total loss for epoch 13: 6530.745913
validation loss after epoch 13 : 1383.623345
	Epoch 14....
Epoch has taken 0:03:47.308916
Number of used sentences in train = 3226
Total loss for epoch 14: 6535.225035
validation loss after epoch 14 : 1421.793553
	TransitionClassifier(
  (p_embeddings): Embedding(13, 32)
  (w_embeddings): Embedding(3369, 155)
  (lstm): LSTM(187, 31, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:21.621228
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1784.517994
	Epoch 1....
Epoch has taken 0:00:25.511485
Number of used sentences in train = 359
Total loss for epoch 1: 952.195748
	Epoch 2....
Epoch has taken 0:00:25.503815
Number of used sentences in train = 359
Total loss for epoch 2: 813.348454
	Epoch 3....
Epoch has taken 0:00:25.527183
Number of used sentences in train = 359
Total loss for epoch 3: 762.551926
	Epoch 4....
Epoch has taken 0:00:25.490895
Number of used sentences in train = 359
Total loss for epoch 4: 729.572533
	Epoch 5....
Epoch has taken 0:00:25.506325
Number of used sentences in train = 359
Total loss for epoch 5: 717.267764
	Epoch 6....
Epoch has taken 0:00:25.524106
Number of used sentences in train = 359
Total loss for epoch 6: 703.660596
	Epoch 7....
Epoch has taken 0:00:25.513735
Number of used sentences in train = 359
Total loss for epoch 7: 719.234613
	Epoch 8....
Epoch has taken 0:00:25.501448
Number of used sentences in train = 359
Total loss for epoch 8: 686.103871
	Epoch 9....
Epoch has taken 0:00:25.486692
Number of used sentences in train = 359
Total loss for epoch 9: 691.563698
	Epoch 10....
Epoch has taken 0:00:25.482068
Number of used sentences in train = 359
Total loss for epoch 10: 687.425429
	Epoch 11....
Epoch has taken 0:00:25.506541
Number of used sentences in train = 359
Total loss for epoch 11: 689.730948
	Epoch 12....
Epoch has taken 0:00:22.235369
Number of used sentences in train = 359
Total loss for epoch 12: 678.789193
	Epoch 13....
Epoch has taken 0:00:22.258389
Number of used sentences in train = 359
Total loss for epoch 13: 680.631995
	Epoch 14....
Epoch has taken 0:00:22.249946
Number of used sentences in train = 359
Total loss for epoch 14: 687.922588
Epoch has taken 0:00:22.223392

==================================================================================================
	Training time : 1:03:47.736652
==================================================================================================
	Identification : 0.274

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 42, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 39, 'lstmDropout': 0.13, 'denseActivation': 'tanh', 'wordDim': 97, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(5887, 97)
  (lstm): LSTM(139, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12935.055951
validation loss after epoch 0 : 1220.481416
	Epoch 1....
Epoch has taken 0:02:39.241148
Number of used sentences in train = 2811
Total loss for epoch 1: 9129.979055
validation loss after epoch 1 : 1109.762927
	Epoch 2....
Epoch has taken 0:02:39.125664
Number of used sentences in train = 2811
Total loss for epoch 2: 7638.015395
validation loss after epoch 2 : 1091.022098
	Epoch 3....
Epoch has taken 0:02:39.434597
Number of used sentences in train = 2811
Total loss for epoch 3: 6653.579106
validation loss after epoch 3 : 1148.007962
	Epoch 4....
Epoch has taken 0:02:39.166328
Number of used sentences in train = 2811
Total loss for epoch 4: 6073.941433
validation loss after epoch 4 : 1188.936424
	Epoch 5....
Epoch has taken 0:02:38.124252
Number of used sentences in train = 2811
Total loss for epoch 5: 5655.011521
validation loss after epoch 5 : 1243.930106
	Epoch 6....
Epoch has taken 0:02:38.653158
Number of used sentences in train = 2811
Total loss for epoch 6: 5365.300521
validation loss after epoch 6 : 1288.249472
	Epoch 7....
Epoch has taken 0:02:40.356333
Number of used sentences in train = 2811
Total loss for epoch 7: 5179.801707
validation loss after epoch 7 : 1341.280175
	Epoch 8....
Epoch has taken 0:03:02.953535
Number of used sentences in train = 2811
Total loss for epoch 8: 5005.715202
validation loss after epoch 8 : 1396.628696
	Epoch 9....
Epoch has taken 0:02:39.907080
Number of used sentences in train = 2811
Total loss for epoch 9: 4902.902481
validation loss after epoch 9 : 1443.984015
	Epoch 10....
Epoch has taken 0:02:38.283852
Number of used sentences in train = 2811
Total loss for epoch 10: 4827.010454
validation loss after epoch 10 : 1478.485944
	Epoch 11....
Epoch has taken 0:02:38.454754
Number of used sentences in train = 2811
Total loss for epoch 11: 4757.063126
validation loss after epoch 11 : 1496.384216
	Epoch 12....
Epoch has taken 0:02:39.874917
Number of used sentences in train = 2811
Total loss for epoch 12: 4689.732864
validation loss after epoch 12 : 1532.863193
	Epoch 13....
Epoch has taken 0:02:38.348472
Number of used sentences in train = 2811
Total loss for epoch 13: 4645.737035
validation loss after epoch 13 : 1548.299619
	Epoch 14....
Epoch has taken 0:02:38.491241
Number of used sentences in train = 2811
Total loss for epoch 14: 4608.437935
validation loss after epoch 14 : 1566.471297
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(5887, 97)
  (lstm): LSTM(139, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:38.483218
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1504.691519
	Epoch 1....
Epoch has taken 0:00:16.924045
Number of used sentences in train = 313
Total loss for epoch 1: 853.159220
	Epoch 2....
Epoch has taken 0:00:16.917475
Number of used sentences in train = 313
Total loss for epoch 2: 665.357756
	Epoch 3....
Epoch has taken 0:00:16.930447
Number of used sentences in train = 313
Total loss for epoch 3: 583.372686
	Epoch 4....
Epoch has taken 0:00:16.911513
Number of used sentences in train = 313
Total loss for epoch 4: 561.918150
	Epoch 5....
Epoch has taken 0:00:16.913665
Number of used sentences in train = 313
Total loss for epoch 5: 555.090311
	Epoch 6....
Epoch has taken 0:00:16.915760
Number of used sentences in train = 313
Total loss for epoch 6: 554.412815
	Epoch 7....
Epoch has taken 0:00:16.911618
Number of used sentences in train = 313
Total loss for epoch 7: 542.639098
	Epoch 8....
Epoch has taken 0:00:16.905144
Number of used sentences in train = 313
Total loss for epoch 8: 537.553647
	Epoch 9....
Epoch has taken 0:00:16.913409
Number of used sentences in train = 313
Total loss for epoch 9: 533.825971
	Epoch 10....
Epoch has taken 0:00:16.946282
Number of used sentences in train = 313
Total loss for epoch 10: 530.342282
	Epoch 11....
Epoch has taken 0:00:16.917529
Number of used sentences in train = 313
Total loss for epoch 11: 528.114763
	Epoch 12....
Epoch has taken 0:00:16.897234
Number of used sentences in train = 313
Total loss for epoch 12: 526.353737
	Epoch 13....
Epoch has taken 0:00:16.913984
Number of used sentences in train = 313
Total loss for epoch 13: 524.221232
	Epoch 14....
Epoch has taken 0:00:16.924249
Number of used sentences in train = 313
Total loss for epoch 14: 520.097019
Epoch has taken 0:00:16.910182

==================================================================================================
	Training time : 0:44:23.150701
==================================================================================================
	Identification : 0.461

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(5639, 97)
  (lstm): LSTM(139, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10253.539104
validation loss after epoch 0 : 927.519175
	Epoch 1....
Epoch has taken 0:01:49.152960
Number of used sentences in train = 2074
Total loss for epoch 1: 6815.379914
validation loss after epoch 1 : 863.844866
	Epoch 2....
Epoch has taken 0:01:49.153833
Number of used sentences in train = 2074
Total loss for epoch 2: 5584.561467
validation loss after epoch 2 : 897.903283
	Epoch 3....
Epoch has taken 0:01:49.136913
Number of used sentences in train = 2074
Total loss for epoch 3: 4802.462688
validation loss after epoch 3 : 899.995718
	Epoch 4....
Epoch has taken 0:01:48.049527
Number of used sentences in train = 2074
Total loss for epoch 4: 4306.347637
validation loss after epoch 4 : 933.923836
	Epoch 5....
Epoch has taken 0:01:48.042069
Number of used sentences in train = 2074
Total loss for epoch 5: 3965.201853
validation loss after epoch 5 : 970.007758
	Epoch 6....
Epoch has taken 0:01:49.260981
Number of used sentences in train = 2074
Total loss for epoch 6: 3736.374628
validation loss after epoch 6 : 1015.492317
	Epoch 7....
Epoch has taken 0:01:49.171676
Number of used sentences in train = 2074
Total loss for epoch 7: 3569.095102
validation loss after epoch 7 : 1059.764965
	Epoch 8....
Epoch has taken 0:01:49.232980
Number of used sentences in train = 2074
Total loss for epoch 8: 3497.066274
validation loss after epoch 8 : 1096.614367
	Epoch 9....
Epoch has taken 0:01:49.097126
Number of used sentences in train = 2074
Total loss for epoch 9: 3424.608057
validation loss after epoch 9 : 1128.733215
	Epoch 10....
Epoch has taken 0:01:48.031068
Number of used sentences in train = 2074
Total loss for epoch 10: 3386.437783
validation loss after epoch 10 : 1140.068712
	Epoch 11....
Epoch has taken 0:01:48.046706
Number of used sentences in train = 2074
Total loss for epoch 11: 3344.597312
validation loss after epoch 11 : 1165.434166
	Epoch 12....
Epoch has taken 0:01:49.327782
Number of used sentences in train = 2074
Total loss for epoch 12: 3327.534269
validation loss after epoch 12 : 1173.671183
	Epoch 13....
Epoch has taken 0:01:49.085117
Number of used sentences in train = 2074
Total loss for epoch 13: 3311.766461
validation loss after epoch 13 : 1184.105917
	Epoch 14....
Epoch has taken 0:01:48.119418
Number of used sentences in train = 2074
Total loss for epoch 14: 3291.727684
validation loss after epoch 14 : 1194.590397
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(5639, 97)
  (lstm): LSTM(139, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.311573
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1528.069095
	Epoch 1....
Epoch has taken 0:00:11.127476
Number of used sentences in train = 231
Total loss for epoch 1: 712.370270
	Epoch 2....
Epoch has taken 0:00:11.147714
Number of used sentences in train = 231
Total loss for epoch 2: 504.166971
	Epoch 3....
Epoch has taken 0:00:11.130298
Number of used sentences in train = 231
Total loss for epoch 3: 434.543732
	Epoch 4....
Epoch has taken 0:00:11.113628
Number of used sentences in train = 231
Total loss for epoch 4: 422.946498
	Epoch 5....
Epoch has taken 0:00:11.116126
Number of used sentences in train = 231
Total loss for epoch 5: 390.065990
	Epoch 6....
Epoch has taken 0:00:11.122273
Number of used sentences in train = 231
Total loss for epoch 6: 376.960713
	Epoch 7....
Epoch has taken 0:00:11.122534
Number of used sentences in train = 231
Total loss for epoch 7: 368.894688
	Epoch 8....
Epoch has taken 0:00:11.109587
Number of used sentences in train = 231
Total loss for epoch 8: 365.182564
	Epoch 9....
Epoch has taken 0:00:11.126082
Number of used sentences in train = 231
Total loss for epoch 9: 363.501085
	Epoch 10....
Epoch has taken 0:00:11.113394
Number of used sentences in train = 231
Total loss for epoch 10: 362.178015
	Epoch 11....
Epoch has taken 0:00:11.132232
Number of used sentences in train = 231
Total loss for epoch 11: 361.019036
	Epoch 12....
Epoch has taken 0:00:12.515732
Number of used sentences in train = 231
Total loss for epoch 12: 359.770690
	Epoch 13....
Epoch has taken 0:00:12.799799
Number of used sentences in train = 231
Total loss for epoch 13: 358.079408
	Epoch 14....
Epoch has taken 0:00:12.782755
Number of used sentences in train = 231
Total loss for epoch 14: 356.997749
Epoch has taken 0:00:11.127967

==================================================================================================
	Training time : 0:30:03.140991
==================================================================================================
	Identification : 0.216

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 42)
  (w_embeddings): Embedding(6874, 97)
  (lstm): LSTM(139, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18749.609339
validation loss after epoch 0 : 1624.516236
	Epoch 1....
Epoch has taken 0:03:32.731524
Number of used sentences in train = 3226
Total loss for epoch 1: 12680.389539
validation loss after epoch 1 : 1513.714456
	Epoch 2....
Epoch has taken 0:03:32.854887
Number of used sentences in train = 3226
Total loss for epoch 2: 10981.351847
validation loss after epoch 2 : 1589.535329
	Epoch 3....
Epoch has taken 0:03:32.875096
Number of used sentences in train = 3226
Total loss for epoch 3: 9866.903313
validation loss after epoch 3 : 1563.442403
	Epoch 4....
Epoch has taken 0:03:35.239166
Number of used sentences in train = 3226
Total loss for epoch 4: 8931.945653
validation loss after epoch 4 : 1647.121452
	Epoch 5....
Epoch has taken 0:03:35.017370
Number of used sentences in train = 3226
Total loss for epoch 5: 8273.685665
validation loss after epoch 5 : 1664.000888
	Epoch 6....
Epoch has taken 0:03:35.253922
Number of used sentences in train = 3226
Total loss for epoch 6: 7703.746457
validation loss after epoch 6 : 1744.842071
	Epoch 7....
Epoch has taken 0:03:34.583745
Number of used sentences in train = 3226
Total loss for epoch 7: 7276.424915
validation loss after epoch 7 : 1919.212870
	Epoch 8....
Epoch has taken 0:03:33.977879
Number of used sentences in train = 3226
Total loss for epoch 8: 6975.475854
validation loss after epoch 8 : 2019.877528
	Epoch 9....
Epoch has taken 0:04:03.776631
Number of used sentences in train = 3226
Total loss for epoch 9: 6748.186141
validation loss after epoch 9 : 2082.129682
	Epoch 10....
Epoch has taken 0:03:34.561053
Number of used sentences in train = 3226
Total loss for epoch 10: 6563.953837
validation loss after epoch 10 : 2116.155926
	Epoch 11....
Epoch has taken 0:03:32.776023
Number of used sentences in train = 3226
Total loss for epoch 11: 6470.432663
validation loss after epoch 11 : 2165.786620
	Epoch 12....
Epoch has taken 0:03:34.774982
Number of used sentences in train = 3226
Total loss for epoch 12: 6385.817686
validation loss after epoch 12 : 2211.253940
	Epoch 13....
Epoch has taken 0:03:51.775154
Number of used sentences in train = 3226
Total loss for epoch 13: 6335.259268
validation loss after epoch 13 : 2226.780619
	Epoch 14....
Epoch has taken 0:03:34.937175
Number of used sentences in train = 3226
Total loss for epoch 14: 6314.264521
validation loss after epoch 14 : 2323.012076
	TransitionClassifier(
  (p_embeddings): Embedding(13, 42)
  (w_embeddings): Embedding(6874, 97)
  (lstm): LSTM(139, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:32.762724
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2174.629038
	Epoch 1....
Epoch has taken 0:00:20.827056
Number of used sentences in train = 359
Total loss for epoch 1: 1265.177580
	Epoch 2....
Epoch has taken 0:00:20.811309
Number of used sentences in train = 359
Total loss for epoch 2: 993.499656
	Epoch 3....
Epoch has taken 0:00:20.826519
Number of used sentences in train = 359
Total loss for epoch 3: 856.489490
	Epoch 4....
Epoch has taken 0:00:20.839264
Number of used sentences in train = 359
Total loss for epoch 4: 758.799860
	Epoch 5....
Epoch has taken 0:00:20.840187
Number of used sentences in train = 359
Total loss for epoch 5: 705.533923
	Epoch 6....
Epoch has taken 0:00:20.823257
Number of used sentences in train = 359
Total loss for epoch 6: 686.874562
	Epoch 7....
Epoch has taken 0:00:20.827444
Number of used sentences in train = 359
Total loss for epoch 7: 680.242621
	Epoch 8....
Epoch has taken 0:00:20.836916
Number of used sentences in train = 359
Total loss for epoch 8: 676.826076
	Epoch 9....
Epoch has taken 0:00:20.822457
Number of used sentences in train = 359
Total loss for epoch 9: 675.799791
	Epoch 10....
Epoch has taken 0:00:20.844791
Number of used sentences in train = 359
Total loss for epoch 10: 675.131550
	Epoch 11....
Epoch has taken 0:00:20.833189
Number of used sentences in train = 359
Total loss for epoch 11: 674.649113
	Epoch 12....
Epoch has taken 0:00:20.828491
Number of used sentences in train = 359
Total loss for epoch 12: 674.270755
	Epoch 13....
Epoch has taken 0:00:20.839262
Number of used sentences in train = 359
Total loss for epoch 13: 673.968744
	Epoch 14....
Epoch has taken 0:00:20.839430
Number of used sentences in train = 359
Total loss for epoch 14: 673.715884
Epoch has taken 0:00:20.832845

==================================================================================================
	Training time : 0:59:31.016040
==================================================================================================
	Identification : 0.418

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 39, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 66, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 22, 'lstmDropout': 0.32, 'denseActivation': 'tanh', 'wordDim': 111, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1177, 111)
  (lstm): LSTM(177, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=39, bias=True)
  (linear2): Linear(in_features=39, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10512.137051
validation loss after epoch 0 : 957.291650
	Epoch 1....
Epoch has taken 0:02:38.646805
Number of used sentences in train = 2811
Total loss for epoch 1: 8049.612108
validation loss after epoch 1 : 947.266693
	Epoch 2....
Epoch has taken 0:02:38.789107
Number of used sentences in train = 2811
Total loss for epoch 2: 7133.369509
validation loss after epoch 2 : 972.249421
	Epoch 3....
Epoch has taken 0:02:40.101885
Number of used sentences in train = 2811
Total loss for epoch 3: 6591.522630
validation loss after epoch 3 : 985.951310
	Epoch 4....
Epoch has taken 0:02:40.144063
Number of used sentences in train = 2811
Total loss for epoch 4: 6246.853921
validation loss after epoch 4 : 1022.881049
	Epoch 5....
Epoch has taken 0:02:40.422822
Number of used sentences in train = 2811
Total loss for epoch 5: 5998.931509
validation loss after epoch 5 : 1026.154705
	Epoch 6....
Epoch has taken 0:02:38.718939
Number of used sentences in train = 2811
Total loss for epoch 6: 5735.960148
validation loss after epoch 6 : 1068.014224
	Epoch 7....
Epoch has taken 0:02:38.728332
Number of used sentences in train = 2811
Total loss for epoch 7: 5596.726329
validation loss after epoch 7 : 1086.169030
	Epoch 8....
Epoch has taken 0:02:40.352884
Number of used sentences in train = 2811
Total loss for epoch 8: 5441.806882
validation loss after epoch 8 : 1114.859732
	Epoch 9....
Epoch has taken 0:02:51.249321
Number of used sentences in train = 2811
Total loss for epoch 9: 5302.826481
validation loss after epoch 9 : 1162.066677
	Epoch 10....
Epoch has taken 0:02:40.412307
Number of used sentences in train = 2811
Total loss for epoch 10: 5191.467096
validation loss after epoch 10 : 1176.534646
	Epoch 11....
Epoch has taken 0:02:40.014148
Number of used sentences in train = 2811
Total loss for epoch 11: 5116.440852
validation loss after epoch 11 : 1205.528872
	Epoch 12....
Epoch has taken 0:02:38.710636
Number of used sentences in train = 2811
Total loss for epoch 12: 5010.192203
validation loss after epoch 12 : 1221.634993
	Epoch 13....
Epoch has taken 0:02:40.136852
Number of used sentences in train = 2811
Total loss for epoch 13: 4953.948671
validation loss after epoch 13 : 1267.133316
	Epoch 14....
Epoch has taken 0:02:40.588193
Number of used sentences in train = 2811
Total loss for epoch 14: 4900.049309
validation loss after epoch 14 : 1275.256398
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1177, 111)
  (lstm): LSTM(177, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=39, bias=True)
  (linear2): Linear(in_features=39, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:40.341742
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1884.615013
	Epoch 1....
Epoch has taken 0:00:16.970302
Number of used sentences in train = 313
Total loss for epoch 1: 744.430847
	Epoch 2....
Epoch has taken 0:00:16.945969
Number of used sentences in train = 313
Total loss for epoch 2: 626.623735
	Epoch 3....
Epoch has taken 0:00:16.960680
Number of used sentences in train = 313
Total loss for epoch 3: 572.436081
	Epoch 4....
Epoch has taken 0:00:16.941262
Number of used sentences in train = 313
Total loss for epoch 4: 545.360285
	Epoch 5....
Epoch has taken 0:00:16.947500
Number of used sentences in train = 313
Total loss for epoch 5: 532.701641
	Epoch 6....
Epoch has taken 0:00:16.954301
Number of used sentences in train = 313
Total loss for epoch 6: 523.681449
	Epoch 7....
Epoch has taken 0:00:16.966047
Number of used sentences in train = 313
Total loss for epoch 7: 518.987147
	Epoch 8....
Epoch has taken 0:00:16.943470
Number of used sentences in train = 313
Total loss for epoch 8: 513.908290
	Epoch 9....
Epoch has taken 0:00:16.947903
Number of used sentences in train = 313
Total loss for epoch 9: 511.487957
	Epoch 10....
Epoch has taken 0:00:16.951731
Number of used sentences in train = 313
Total loss for epoch 10: 509.416530
	Epoch 11....
Epoch has taken 0:00:16.952031
Number of used sentences in train = 313
Total loss for epoch 11: 508.478981
	Epoch 12....
Epoch has taken 0:00:16.938508
Number of used sentences in train = 313
Total loss for epoch 12: 507.637969
	Epoch 13....
Epoch has taken 0:00:16.955997
Number of used sentences in train = 313
Total loss for epoch 13: 506.874125
	Epoch 14....
Epoch has taken 0:00:16.902569
Number of used sentences in train = 313
Total loss for epoch 14: 506.420040
Epoch has taken 0:00:16.872944

==================================================================================================
	Training time : 0:44:22.002047
==================================================================================================
	Identification : 0.512

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1133, 111)
  (lstm): LSTM(177, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=39, bias=True)
  (linear2): Linear(in_features=39, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8158.905026
validation loss after epoch 0 : 798.710978
	Epoch 1....
Epoch has taken 0:01:48.186862
Number of used sentences in train = 2074
Total loss for epoch 1: 5674.657467
validation loss after epoch 1 : 788.799136
	Epoch 2....
Epoch has taken 0:01:48.170968
Number of used sentences in train = 2074
Total loss for epoch 2: 4892.834754
validation loss after epoch 2 : 798.681883
	Epoch 3....
Epoch has taken 0:01:49.255803
Number of used sentences in train = 2074
Total loss for epoch 3: 4446.909099
validation loss after epoch 3 : 822.292083
	Epoch 4....
Epoch has taken 0:01:48.963190
Number of used sentences in train = 2074
Total loss for epoch 4: 4111.770599
validation loss after epoch 4 : 847.695599
	Epoch 5....
Epoch has taken 0:02:06.131146
Number of used sentences in train = 2074
Total loss for epoch 5: 3879.724374
validation loss after epoch 5 : 835.111593
	Epoch 6....
Epoch has taken 0:01:48.701380
Number of used sentences in train = 2074
Total loss for epoch 6: 3697.024111
validation loss after epoch 6 : 935.215513
	Epoch 7....
Epoch has taken 0:01:48.497181
Number of used sentences in train = 2074
Total loss for epoch 7: 3562.595421
validation loss after epoch 7 : 928.154856
	Epoch 8....
Epoch has taken 0:01:47.686641
Number of used sentences in train = 2074
Total loss for epoch 8: 3479.014906
validation loss after epoch 8 : 967.140781
	Epoch 9....
Epoch has taken 0:01:48.347762
Number of used sentences in train = 2074
Total loss for epoch 9: 3403.206038
validation loss after epoch 9 : 1004.173463
	Epoch 10....
Epoch has taken 0:01:48.703278
Number of used sentences in train = 2074
Total loss for epoch 10: 3343.200237
validation loss after epoch 10 : 1022.647195
	Epoch 11....
Epoch has taken 0:01:48.855809
Number of used sentences in train = 2074
Total loss for epoch 11: 3299.158155
validation loss after epoch 11 : 1076.143084
	Epoch 12....
Epoch has taken 0:01:48.632978
Number of used sentences in train = 2074
Total loss for epoch 12: 3275.033387
validation loss after epoch 12 : 1077.266311
	Epoch 13....
Epoch has taken 0:01:47.683579
Number of used sentences in train = 2074
Total loss for epoch 13: 3248.159648
validation loss after epoch 13 : 1075.447973
	Epoch 14....
Epoch has taken 0:01:47.619991
Number of used sentences in train = 2074
Total loss for epoch 14: 3227.645954
validation loss after epoch 14 : 1112.248229
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1133, 111)
  (lstm): LSTM(177, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=39, bias=True)
  (linear2): Linear(in_features=39, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.016596
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1290.198500
	Epoch 1....
Epoch has taken 0:00:11.047835
Number of used sentences in train = 231
Total loss for epoch 1: 578.481689
	Epoch 2....
Epoch has taken 0:00:11.041097
Number of used sentences in train = 231
Total loss for epoch 2: 449.199136
	Epoch 3....
Epoch has taken 0:00:11.073448
Number of used sentences in train = 231
Total loss for epoch 3: 397.868218
	Epoch 4....
Epoch has taken 0:00:11.055301
Number of used sentences in train = 231
Total loss for epoch 4: 375.143222
	Epoch 5....
Epoch has taken 0:00:11.059436
Number of used sentences in train = 231
Total loss for epoch 5: 366.159354
	Epoch 6....
Epoch has taken 0:00:11.049893
Number of used sentences in train = 231
Total loss for epoch 6: 359.485719
	Epoch 7....
Epoch has taken 0:00:11.064496
Number of used sentences in train = 231
Total loss for epoch 7: 353.924926
	Epoch 8....
Epoch has taken 0:00:11.054900
Number of used sentences in train = 231
Total loss for epoch 8: 351.830617
	Epoch 9....
Epoch has taken 0:00:11.063754
Number of used sentences in train = 231
Total loss for epoch 9: 350.335310
	Epoch 10....
Epoch has taken 0:00:11.066614
Number of used sentences in train = 231
Total loss for epoch 10: 349.498457
	Epoch 11....
Epoch has taken 0:00:11.058636
Number of used sentences in train = 231
Total loss for epoch 11: 348.664897
	Epoch 12....
Epoch has taken 0:00:11.074461
Number of used sentences in train = 231
Total loss for epoch 12: 348.187782
	Epoch 13....
Epoch has taken 0:00:11.050752
Number of used sentences in train = 231
Total loss for epoch 13: 347.708640
	Epoch 14....
Epoch has taken 0:00:11.055010
Number of used sentences in train = 231
Total loss for epoch 14: 347.359130
Epoch has taken 0:00:11.052247

==================================================================================================
	Training time : 0:30:10.649176
==================================================================================================
	Identification : 0.306

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 66)
  (w_embeddings): Embedding(1202, 111)
  (lstm): LSTM(177, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=39, bias=True)
  (linear2): Linear(in_features=39, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14937.253782
validation loss after epoch 0 : 1354.098871
	Epoch 1....
Epoch has taken 0:03:31.956439
Number of used sentences in train = 3226
Total loss for epoch 1: 11842.219337
validation loss after epoch 1 : 1385.930815
	Epoch 2....
Epoch has taken 0:03:32.052712
Number of used sentences in train = 3226
Total loss for epoch 2: 10780.708531
validation loss after epoch 2 : 1334.720794
	Epoch 3....
Epoch has taken 0:03:30.180297
Number of used sentences in train = 3226
Total loss for epoch 3: 9998.501818
validation loss after epoch 3 : 1363.717519
	Epoch 4....
Epoch has taken 0:03:32.179489
Number of used sentences in train = 3226
Total loss for epoch 4: 9443.362461
validation loss after epoch 4 : 1366.437290
	Epoch 5....
Epoch has taken 0:03:33.835855
Number of used sentences in train = 3226
Total loss for epoch 5: 9035.576130
validation loss after epoch 5 : 1438.785533
	Epoch 6....
Epoch has taken 0:03:32.761325
Number of used sentences in train = 3226
Total loss for epoch 6: 8725.224655
validation loss after epoch 6 : 1482.307985
	Epoch 7....
Epoch has taken 0:04:05.677739
Number of used sentences in train = 3226
Total loss for epoch 7: 8472.568856
validation loss after epoch 7 : 1541.925227
	Epoch 8....
Epoch has taken 0:03:32.418615
Number of used sentences in train = 3226
Total loss for epoch 8: 8256.244540
validation loss after epoch 8 : 1553.944093
	Epoch 9....
Epoch has taken 0:03:32.079100
Number of used sentences in train = 3226
Total loss for epoch 9: 8061.290488
validation loss after epoch 9 : 1580.051862
	Epoch 10....
Epoch has taken 0:03:30.271543
Number of used sentences in train = 3226
Total loss for epoch 10: 7922.182817
validation loss after epoch 10 : 1640.403903
	Epoch 11....
Epoch has taken 0:03:32.763995
Number of used sentences in train = 3226
Total loss for epoch 11: 7775.798669
validation loss after epoch 11 : 1642.152016
	Epoch 12....
Epoch has taken 0:03:32.542723
Number of used sentences in train = 3226
Total loss for epoch 12: 7631.022861
validation loss after epoch 12 : 1711.616659
	Epoch 13....
Epoch has taken 0:03:32.425074
Number of used sentences in train = 3226
Total loss for epoch 13: 7521.610073
validation loss after epoch 13 : 1733.287245
	Epoch 14....
Epoch has taken 0:03:30.944430
Number of used sentences in train = 3226
Total loss for epoch 14: 7430.076440
validation loss after epoch 14 : 1779.781284
	TransitionClassifier(
  (p_embeddings): Embedding(13, 66)
  (w_embeddings): Embedding(1202, 111)
  (lstm): LSTM(177, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=39, bias=True)
  (linear2): Linear(in_features=39, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:33.327264
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1902.420456
	Epoch 1....
Epoch has taken 0:00:20.801266
Number of used sentences in train = 359
Total loss for epoch 1: 1271.803339
	Epoch 2....
Epoch has taken 0:00:20.808828
Number of used sentences in train = 359
Total loss for epoch 2: 1049.064399
	Epoch 3....
Epoch has taken 0:00:20.825287
Number of used sentences in train = 359
Total loss for epoch 3: 952.712024
	Epoch 4....
Epoch has taken 0:00:20.806993
Number of used sentences in train = 359
Total loss for epoch 4: 877.321337
	Epoch 5....
Epoch has taken 0:00:20.828169
Number of used sentences in train = 359
Total loss for epoch 5: 852.253201
	Epoch 6....
Epoch has taken 0:00:20.805040
Number of used sentences in train = 359
Total loss for epoch 6: 819.473434
	Epoch 7....
Epoch has taken 0:00:20.796521
Number of used sentences in train = 359
Total loss for epoch 7: 801.329706
	Epoch 8....
Epoch has taken 0:00:20.810105
Number of used sentences in train = 359
Total loss for epoch 8: 781.758948
	Epoch 9....
Epoch has taken 0:00:20.790570
Number of used sentences in train = 359
Total loss for epoch 9: 770.901280
	Epoch 10....
Epoch has taken 0:00:20.803773
Number of used sentences in train = 359
Total loss for epoch 10: 756.192441
	Epoch 11....
Epoch has taken 0:00:20.808000
Number of used sentences in train = 359
Total loss for epoch 11: 748.651056
	Epoch 12....
Epoch has taken 0:00:20.801951
Number of used sentences in train = 359
Total loss for epoch 12: 738.728508
	Epoch 13....
Epoch has taken 0:00:20.786061
Number of used sentences in train = 359
Total loss for epoch 13: 730.302491
	Epoch 14....
Epoch has taken 0:00:20.789990
Number of used sentences in train = 359
Total loss for epoch 14: 745.746984
Epoch has taken 0:00:20.794931

==================================================================================================
	Training time : 0:58:48.112166
==================================================================================================
	Identification : 0.43

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 90, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 31, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 63, 'lstmDropout': 0.13, 'denseActivation': 'tanh', 'wordDim': 99, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 31)
  (w_embeddings): Embedding(5940, 99)
  (lstm): LSTM(130, 63, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=1008, out_features=90, bias=True)
  (linear2): Linear(in_features=90, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13770.091707
validation loss after epoch 0 : 1093.283481
	Epoch 1....
Epoch has taken 0:02:52.576169
Number of used sentences in train = 2811
Total loss for epoch 1: 8674.666303
validation loss after epoch 1 : 1026.124944
	Epoch 2....
Epoch has taken 0:02:52.486185
Number of used sentences in train = 2811
Total loss for epoch 2: 7088.265169
validation loss after epoch 2 : 997.071796
	Epoch 3....
Epoch has taken 0:02:52.077951
Number of used sentences in train = 2811
Total loss for epoch 3: 6073.150828
validation loss after epoch 3 : 1087.701094
	Epoch 4....
Epoch has taken 0:02:52.273171
Number of used sentences in train = 2811
Total loss for epoch 4: 5457.722139
validation loss after epoch 4 : 1120.545450
	Epoch 5....
Epoch has taken 0:02:50.687214
Number of used sentences in train = 2811
Total loss for epoch 5: 5086.353606
validation loss after epoch 5 : 1204.535667
	Epoch 6....
Epoch has taken 0:02:50.553908
Number of used sentences in train = 2811
Total loss for epoch 6: 4894.158790
validation loss after epoch 6 : 1307.719725
	Epoch 7....
Epoch has taken 0:02:51.865341
Number of used sentences in train = 2811
Total loss for epoch 7: 4763.771802
validation loss after epoch 7 : 1301.103793
	Epoch 8....
Epoch has taken 0:02:51.976229
Number of used sentences in train = 2811
Total loss for epoch 8: 4707.299536
validation loss after epoch 8 : 1344.635941
	Epoch 9....
Epoch has taken 0:02:51.486996
Number of used sentences in train = 2811
Total loss for epoch 9: 4613.018143
validation loss after epoch 9 : 1387.903424
	Epoch 10....
Epoch has taken 0:02:51.935328
Number of used sentences in train = 2811
Total loss for epoch 10: 4596.838326
validation loss after epoch 10 : 1465.975564
	Epoch 11....
Epoch has taken 0:02:51.224067
Number of used sentences in train = 2811
Total loss for epoch 11: 4599.364880
validation loss after epoch 11 : 1445.663895
	Epoch 12....
Epoch has taken 0:02:50.231289
Number of used sentences in train = 2811
Total loss for epoch 12: 4529.196163
validation loss after epoch 12 : 1405.238649
	Epoch 13....
Epoch has taken 0:02:51.644099
Number of used sentences in train = 2811
Total loss for epoch 13: 4525.002012
validation loss after epoch 13 : 1451.394123
	Epoch 14....
Epoch has taken 0:02:52.079308
Number of used sentences in train = 2811
Total loss for epoch 14: 4514.541092
validation loss after epoch 14 : 1518.448302
	TransitionClassifier(
  (p_embeddings): Embedding(18, 31)
  (w_embeddings): Embedding(5940, 99)
  (lstm): LSTM(130, 63, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=1008, out_features=90, bias=True)
  (linear2): Linear(in_features=90, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.718147
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1843.271112
	Epoch 1....
Epoch has taken 0:00:17.975107
Number of used sentences in train = 313
Total loss for epoch 1: 738.889989
	Epoch 2....
Epoch has taken 0:00:18.007410
Number of used sentences in train = 313
Total loss for epoch 2: 595.283902
	Epoch 3....
Epoch has taken 0:00:17.954703
Number of used sentences in train = 313
Total loss for epoch 3: 545.264193
	Epoch 4....
Epoch has taken 0:00:17.965694
Number of used sentences in train = 313
Total loss for epoch 4: 528.639817
	Epoch 5....
Epoch has taken 0:00:17.950578
Number of used sentences in train = 313
Total loss for epoch 5: 524.846607
	Epoch 6....
Epoch has taken 0:00:17.998196
Number of used sentences in train = 313
Total loss for epoch 6: 505.663310
	Epoch 7....
Epoch has taken 0:00:17.974718
Number of used sentences in train = 313
Total loss for epoch 7: 503.566850
	Epoch 8....
Epoch has taken 0:00:17.974397
Number of used sentences in train = 313
Total loss for epoch 8: 504.620790
	Epoch 9....
Epoch has taken 0:00:17.971964
Number of used sentences in train = 313
Total loss for epoch 9: 505.332206
	Epoch 10....
Epoch has taken 0:00:18.000233
Number of used sentences in train = 313
Total loss for epoch 10: 502.748687
	Epoch 11....
Epoch has taken 0:00:17.983431
Number of used sentences in train = 313
Total loss for epoch 11: 500.922586
	Epoch 12....
Epoch has taken 0:00:17.982164
Number of used sentences in train = 313
Total loss for epoch 12: 500.397589
	Epoch 13....
Epoch has taken 0:00:17.968601
Number of used sentences in train = 313
Total loss for epoch 13: 500.029688
	Epoch 14....
Epoch has taken 0:00:17.999307
Number of used sentences in train = 313
Total loss for epoch 14: 499.802800
Epoch has taken 0:00:17.977283

==================================================================================================
	Training time : 0:47:26.004249
==================================================================================================
	Identification : 0.427

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 31)
  (w_embeddings): Embedding(5630, 99)
  (lstm): LSTM(130, 63, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=1008, out_features=90, bias=True)
  (linear2): Linear(in_features=90, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 19114.130257
validation loss after epoch 0 : 1116.068845
	Epoch 1....
Epoch has taken 0:01:57.645307
Number of used sentences in train = 2074
Total loss for epoch 1: 7989.055745
validation loss after epoch 1 : 837.396524
	Epoch 2....
Epoch has taken 0:01:57.647446
Number of used sentences in train = 2074
Total loss for epoch 2: 5802.393937
validation loss after epoch 2 : 847.738723
	Epoch 3....
Epoch has taken 0:01:57.565938
Number of used sentences in train = 2074
Total loss for epoch 3: 4754.068212
validation loss after epoch 3 : 956.847583
	Epoch 4....
Epoch has taken 0:01:57.322893
Number of used sentences in train = 2074
Total loss for epoch 4: 4096.587936
validation loss after epoch 4 : 1040.378589
	Epoch 5....
Epoch has taken 0:01:56.512702
Number of used sentences in train = 2074
Total loss for epoch 5: 3752.997360
validation loss after epoch 5 : 1024.902764
	Epoch 6....
Epoch has taken 0:01:57.452329
Number of used sentences in train = 2074
Total loss for epoch 6: 3552.555038
validation loss after epoch 6 : 1203.755256
	Epoch 7....
Epoch has taken 0:01:57.576663
Number of used sentences in train = 2074
Total loss for epoch 7: 3392.887840
validation loss after epoch 7 : 1179.859402
	Epoch 8....
Epoch has taken 0:01:57.339403
Number of used sentences in train = 2074
Total loss for epoch 8: 3369.015791
validation loss after epoch 8 : 1251.584644
	Epoch 9....
Epoch has taken 0:01:57.495936
Number of used sentences in train = 2074
Total loss for epoch 9: 3315.442095
validation loss after epoch 9 : 1192.861940
	Epoch 10....
Epoch has taken 0:01:56.321764
Number of used sentences in train = 2074
Total loss for epoch 10: 3277.618790
validation loss after epoch 10 : 1336.510703
	Epoch 11....
Epoch has taken 0:01:57.354398
Number of used sentences in train = 2074
Total loss for epoch 11: 3244.172097
validation loss after epoch 11 : 1313.334645
	Epoch 12....
Epoch has taken 0:01:57.301753
Number of used sentences in train = 2074
Total loss for epoch 12: 3225.224284
validation loss after epoch 12 : 1391.180505
	Epoch 13....
Epoch has taken 0:01:57.456231
Number of used sentences in train = 2074
Total loss for epoch 13: 3215.688854
validation loss after epoch 13 : 1394.207643
	Epoch 14....
Epoch has taken 0:01:57.340832
Number of used sentences in train = 2074
Total loss for epoch 14: 3198.329146
validation loss after epoch 14 : 1509.430294
	TransitionClassifier(
  (p_embeddings): Embedding(18, 31)
  (w_embeddings): Embedding(5630, 99)
  (lstm): LSTM(130, 63, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=1008, out_features=90, bias=True)
  (linear2): Linear(in_features=90, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.235363
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1709.210813
	Epoch 1....
Epoch has taken 0:00:11.884376
Number of used sentences in train = 231
Total loss for epoch 1: 637.569003
	Epoch 2....
Epoch has taken 0:00:11.857420
Number of used sentences in train = 231
Total loss for epoch 2: 472.202232
	Epoch 3....
Epoch has taken 0:00:11.887249
Number of used sentences in train = 231
Total loss for epoch 3: 399.540410
	Epoch 4....
Epoch has taken 0:00:11.890035
Number of used sentences in train = 231
Total loss for epoch 4: 370.481982
	Epoch 5....
Epoch has taken 0:00:11.957854
Number of used sentences in train = 231
Total loss for epoch 5: 356.441073
	Epoch 6....
Epoch has taken 0:00:11.944264
Number of used sentences in train = 231
Total loss for epoch 6: 354.783987
	Epoch 7....
Epoch has taken 0:00:11.982157
Number of used sentences in train = 231
Total loss for epoch 7: 353.189620
	Epoch 8....
Epoch has taken 0:00:11.950078
Number of used sentences in train = 231
Total loss for epoch 8: 352.665727
	Epoch 9....
Epoch has taken 0:00:11.961521
Number of used sentences in train = 231
Total loss for epoch 9: 353.794617
	Epoch 10....
Epoch has taken 0:00:11.966410
Number of used sentences in train = 231
Total loss for epoch 10: 352.397936
	Epoch 11....
Epoch has taken 0:00:11.989392
Number of used sentences in train = 231
Total loss for epoch 11: 351.867675
	Epoch 12....
Epoch has taken 0:00:11.971920
Number of used sentences in train = 231
Total loss for epoch 12: 352.046231
	Epoch 13....
Epoch has taken 0:00:11.974583
Number of used sentences in train = 231
Total loss for epoch 13: 351.488838
	Epoch 14....
Epoch has taken 0:00:11.969755
Number of used sentences in train = 231
Total loss for epoch 14: 351.388168
Epoch has taken 0:00:11.986381

==================================================================================================
	Training time : 0:32:18.083902
==================================================================================================
	Identification : 0.408

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 31)
  (w_embeddings): Embedding(6953, 99)
  (lstm): LSTM(130, 63, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=1008, out_features=90, bias=True)
  (linear2): Linear(in_features=90, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 21603.812281
validation loss after epoch 0 : 1569.579333
	Epoch 1....
Epoch has taken 0:03:47.744786
Number of used sentences in train = 3226
Total loss for epoch 1: 12821.634546
validation loss after epoch 1 : 1523.223989
	Epoch 2....
Epoch has taken 0:03:47.618637
Number of used sentences in train = 3226
Total loss for epoch 2: 10854.618567
validation loss after epoch 2 : 1513.220921
	Epoch 3....
Epoch has taken 0:03:47.174581
Number of used sentences in train = 3226
Total loss for epoch 3: 9437.334224
validation loss after epoch 3 : 1645.532510
	Epoch 4....
Epoch has taken 0:03:45.848664
Number of used sentences in train = 3226
Total loss for epoch 4: 8536.859559
validation loss after epoch 4 : 1887.940087
	Epoch 5....
Epoch has taken 0:03:50.878478
Number of used sentences in train = 3226
Total loss for epoch 5: 7893.197939
validation loss after epoch 5 : 1880.798640
	Epoch 6....
Epoch has taken 0:03:47.780153
Number of used sentences in train = 3226
Total loss for epoch 6: 7347.102786
validation loss after epoch 6 : 1938.304444
	Epoch 7....
Epoch has taken 0:03:47.421621
Number of used sentences in train = 3226
Total loss for epoch 7: 7005.721010
validation loss after epoch 7 : 2102.527040
	Epoch 8....
Epoch has taken 0:03:47.534897
Number of used sentences in train = 3226
Total loss for epoch 8: 6774.086953
validation loss after epoch 8 : 2245.589141
	Epoch 9....
Epoch has taken 0:03:47.462151
Number of used sentences in train = 3226
Total loss for epoch 9: 6608.438689
validation loss after epoch 9 : 2303.288782
	Epoch 10....
Epoch has taken 0:03:47.408807
Number of used sentences in train = 3226
Total loss for epoch 10: 6497.291779
validation loss after epoch 10 : 2353.939065
	Epoch 11....
Epoch has taken 0:03:45.634167
Number of used sentences in train = 3226
Total loss for epoch 11: 6423.346756
validation loss after epoch 11 : 2431.241453
	Epoch 12....
Epoch has taken 0:03:47.101721
Number of used sentences in train = 3226
Total loss for epoch 12: 6374.838707
validation loss after epoch 12 : 2566.566953
	Epoch 13....
Epoch has taken 0:03:47.387513
Number of used sentences in train = 3226
Total loss for epoch 13: 6385.806164
validation loss after epoch 13 : 2556.903711
	Epoch 14....
Epoch has taken 0:04:19.022491
Number of used sentences in train = 3226
Total loss for epoch 14: 6290.849301
validation loss after epoch 14 : 2617.310290
	TransitionClassifier(
  (p_embeddings): Embedding(13, 31)
  (w_embeddings): Embedding(6953, 99)
  (lstm): LSTM(130, 63, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=1008, out_features=90, bias=True)
  (linear2): Linear(in_features=90, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:45.657987
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2693.677369
	Epoch 1....
Epoch has taken 0:00:22.250226
Number of used sentences in train = 359
Total loss for epoch 1: 1316.721302
	Epoch 2....
Epoch has taken 0:00:22.250807
Number of used sentences in train = 359
Total loss for epoch 2: 996.230956
	Epoch 3....
Epoch has taken 0:00:22.258085
Number of used sentences in train = 359
Total loss for epoch 3: 874.746679
	Epoch 4....
Epoch has taken 0:00:22.269253
Number of used sentences in train = 359
Total loss for epoch 4: 786.397999
	Epoch 5....
Epoch has taken 0:00:22.260742
Number of used sentences in train = 359
Total loss for epoch 5: 732.450985
	Epoch 6....
Epoch has taken 0:00:22.284918
Number of used sentences in train = 359
Total loss for epoch 6: 704.079196
	Epoch 7....
Epoch has taken 0:00:22.261336
Number of used sentences in train = 359
Total loss for epoch 7: 692.503886
	Epoch 8....
Epoch has taken 0:00:22.280952
Number of used sentences in train = 359
Total loss for epoch 8: 687.084400
	Epoch 9....
Epoch has taken 0:00:22.290879
Number of used sentences in train = 359
Total loss for epoch 9: 678.887407
	Epoch 10....
Epoch has taken 0:00:22.246221
Number of used sentences in train = 359
Total loss for epoch 10: 681.463532
	Epoch 11....
Epoch has taken 0:00:22.287616
Number of used sentences in train = 359
Total loss for epoch 11: 673.462430
	Epoch 12....
Epoch has taken 0:00:22.278624
Number of used sentences in train = 359
Total loss for epoch 12: 671.305088
	Epoch 13....
Epoch has taken 0:00:22.253572
Number of used sentences in train = 359
Total loss for epoch 13: 670.958596
	Epoch 14....
Epoch has taken 0:00:22.261023
Number of used sentences in train = 359
Total loss for epoch 14: 671.174667
Epoch has taken 0:00:22.278094

==================================================================================================
	Training time : 1:02:56.335796
==================================================================================================
	Identification : 0.389

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 32, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 72, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 30, 'lstmDropout': 0.21, 'denseActivation': 'tanh', 'wordDim': 141, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 72)
  (w_embeddings): Embedding(1177, 141)
  (lstm): LSTM(213, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10681.503906
validation loss after epoch 0 : 949.188752
	Epoch 1....
Epoch has taken 0:02:39.569107
Number of used sentences in train = 2811
Total loss for epoch 1: 8025.648155
validation loss after epoch 1 : 921.209570
	Epoch 2....
Epoch has taken 0:02:39.504294
Number of used sentences in train = 2811
Total loss for epoch 2: 7206.590464
validation loss after epoch 2 : 953.984988
	Epoch 3....
Epoch has taken 0:02:37.909174
Number of used sentences in train = 2811
Total loss for epoch 3: 6628.192515
validation loss after epoch 3 : 968.869658
	Epoch 4....
Epoch has taken 0:02:38.249288
Number of used sentences in train = 2811
Total loss for epoch 4: 6237.481619
validation loss after epoch 4 : 988.848719
	Epoch 5....
Epoch has taken 0:02:38.315381
Number of used sentences in train = 2811
Total loss for epoch 5: 5931.369474
validation loss after epoch 5 : 1028.760530
	Epoch 6....
Epoch has taken 0:02:57.025931
Number of used sentences in train = 2811
Total loss for epoch 6: 5670.638934
validation loss after epoch 6 : 1060.668692
	Epoch 7....
Epoch has taken 0:02:41.125658
Number of used sentences in train = 2811
Total loss for epoch 7: 5497.259234
validation loss after epoch 7 : 1092.998958
	Epoch 8....
Epoch has taken 0:02:42.093544
Number of used sentences in train = 2811
Total loss for epoch 8: 5335.827867
validation loss after epoch 8 : 1141.502920
	Epoch 9....
Epoch has taken 0:02:39.194486
Number of used sentences in train = 2811
Total loss for epoch 9: 5223.896296
validation loss after epoch 9 : 1144.707100
	Epoch 10....
Epoch has taken 0:02:40.043788
Number of used sentences in train = 2811
Total loss for epoch 10: 5089.519527
validation loss after epoch 10 : 1186.587123
	Epoch 11....
Epoch has taken 0:02:39.619078
Number of used sentences in train = 2811
Total loss for epoch 11: 5017.470436
validation loss after epoch 11 : 1221.759003
	Epoch 12....
Epoch has taken 0:02:39.574460
Number of used sentences in train = 2811
Total loss for epoch 12: 4925.551209
validation loss after epoch 12 : 1248.198277
	Epoch 13....
Epoch has taken 0:02:39.380839
Number of used sentences in train = 2811
Total loss for epoch 13: 4870.859609
validation loss after epoch 13 : 1273.568028
	Epoch 14....
Epoch has taken 0:02:37.967710
Number of used sentences in train = 2811
Total loss for epoch 14: 4811.433030
validation loss after epoch 14 : 1321.437482
	TransitionClassifier(
  (p_embeddings): Embedding(18, 72)
  (w_embeddings): Embedding(1177, 141)
  (lstm): LSTM(213, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:38.167725
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1363.447979
	Epoch 1....
Epoch has taken 0:00:16.855343
Number of used sentences in train = 313
Total loss for epoch 1: 747.162093
	Epoch 2....
Epoch has taken 0:00:16.841097
Number of used sentences in train = 313
Total loss for epoch 2: 609.017249
	Epoch 3....
Epoch has taken 0:00:16.863427
Number of used sentences in train = 313
Total loss for epoch 3: 556.628538
	Epoch 4....
Epoch has taken 0:00:16.868186
Number of used sentences in train = 313
Total loss for epoch 4: 537.982236
	Epoch 5....
Epoch has taken 0:00:16.859061
Number of used sentences in train = 313
Total loss for epoch 5: 527.763965
	Epoch 6....
Epoch has taken 0:00:16.864961
Number of used sentences in train = 313
Total loss for epoch 6: 518.339464
	Epoch 7....
Epoch has taken 0:00:16.876816
Number of used sentences in train = 313
Total loss for epoch 7: 512.813637
	Epoch 8....
Epoch has taken 0:00:16.841079
Number of used sentences in train = 313
Total loss for epoch 8: 510.335917
	Epoch 9....
Epoch has taken 0:00:16.852919
Number of used sentences in train = 313
Total loss for epoch 9: 508.346648
	Epoch 10....
Epoch has taken 0:00:16.879962
Number of used sentences in train = 313
Total loss for epoch 10: 506.776422
	Epoch 11....
Epoch has taken 0:00:16.872351
Number of used sentences in train = 313
Total loss for epoch 11: 506.030624
	Epoch 12....
Epoch has taken 0:00:16.866648
Number of used sentences in train = 313
Total loss for epoch 12: 504.416686
	Epoch 13....
Epoch has taken 0:00:16.864642
Number of used sentences in train = 313
Total loss for epoch 13: 504.406438
	Epoch 14....
Epoch has taken 0:00:16.878432
Number of used sentences in train = 313
Total loss for epoch 14: 504.415244
Epoch has taken 0:00:16.855123

==================================================================================================
	Training time : 0:44:21.171816
==================================================================================================
	Identification : 0.47

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 72)
  (w_embeddings): Embedding(1133, 141)
  (lstm): LSTM(213, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7864.912671
validation loss after epoch 0 : 757.156043
	Epoch 1....
Epoch has taken 0:01:48.964689
Number of used sentences in train = 2074
Total loss for epoch 1: 5586.439374
validation loss after epoch 1 : 728.207229
	Epoch 2....
Epoch has taken 0:01:49.280238
Number of used sentences in train = 2074
Total loss for epoch 2: 4826.674643
validation loss after epoch 2 : 721.284901
	Epoch 3....
Epoch has taken 0:01:48.344532
Number of used sentences in train = 2074
Total loss for epoch 3: 4395.657854
validation loss after epoch 3 : 751.379892
	Epoch 4....
Epoch has taken 0:01:49.520275
Number of used sentences in train = 2074
Total loss for epoch 4: 4080.554003
validation loss after epoch 4 : 778.457774
	Epoch 5....
Epoch has taken 0:02:06.493888
Number of used sentences in train = 2074
Total loss for epoch 5: 3856.823076
validation loss after epoch 5 : 775.548254
	Epoch 6....
Epoch has taken 0:01:49.458418
Number of used sentences in train = 2074
Total loss for epoch 6: 3711.690393
validation loss after epoch 6 : 795.881182
	Epoch 7....
Epoch has taken 0:01:49.340124
Number of used sentences in train = 2074
Total loss for epoch 7: 3575.269774
validation loss after epoch 7 : 853.488056
	Epoch 8....
Epoch has taken 0:01:48.375726
Number of used sentences in train = 2074
Total loss for epoch 8: 3473.258568
validation loss after epoch 8 : 859.328654
	Epoch 9....
Epoch has taken 0:01:48.400060
Number of used sentences in train = 2074
Total loss for epoch 9: 3402.173197
validation loss after epoch 9 : 879.409110
	Epoch 10....
Epoch has taken 0:01:49.452633
Number of used sentences in train = 2074
Total loss for epoch 10: 3349.395912
validation loss after epoch 10 : 933.102751
	Epoch 11....
Epoch has taken 0:01:50.930870
Number of used sentences in train = 2074
Total loss for epoch 11: 3314.966728
validation loss after epoch 11 : 962.070075
	Epoch 12....
Epoch has taken 0:01:49.255286
Number of used sentences in train = 2074
Total loss for epoch 12: 3290.434655
validation loss after epoch 12 : 964.125030
	Epoch 13....
Epoch has taken 0:01:48.432796
Number of used sentences in train = 2074
Total loss for epoch 13: 3266.841330
validation loss after epoch 13 : 955.553294
	Epoch 14....
Epoch has taken 0:01:49.475778
Number of used sentences in train = 2074
Total loss for epoch 14: 3231.707016
validation loss after epoch 14 : 982.086136
	TransitionClassifier(
  (p_embeddings): Embedding(18, 72)
  (w_embeddings): Embedding(1133, 141)
  (lstm): LSTM(213, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.038026
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1440.351128
	Epoch 1....
Epoch has taken 0:00:11.113225
Number of used sentences in train = 231
Total loss for epoch 1: 594.003194
	Epoch 2....
Epoch has taken 0:00:11.147311
Number of used sentences in train = 231
Total loss for epoch 2: 442.909775
	Epoch 3....
Epoch has taken 0:00:11.140722
Number of used sentences in train = 231
Total loss for epoch 3: 394.922640
	Epoch 4....
Epoch has taken 0:00:11.157590
Number of used sentences in train = 231
Total loss for epoch 4: 374.239471
	Epoch 5....
Epoch has taken 0:00:11.135743
Number of used sentences in train = 231
Total loss for epoch 5: 365.933591
	Epoch 6....
Epoch has taken 0:00:11.153124
Number of used sentences in train = 231
Total loss for epoch 6: 357.719308
	Epoch 7....
Epoch has taken 0:00:11.131427
Number of used sentences in train = 231
Total loss for epoch 7: 352.973594
	Epoch 8....
Epoch has taken 0:00:11.141057
Number of used sentences in train = 231
Total loss for epoch 8: 350.633797
	Epoch 9....
Epoch has taken 0:00:11.127663
Number of used sentences in train = 231
Total loss for epoch 9: 348.727548
	Epoch 10....
Epoch has taken 0:00:11.138793
Number of used sentences in train = 231
Total loss for epoch 10: 347.769921
	Epoch 11....
Epoch has taken 0:00:11.148967
Number of used sentences in train = 231
Total loss for epoch 11: 347.216826
	Epoch 12....
Epoch has taken 0:00:11.133239
Number of used sentences in train = 231
Total loss for epoch 12: 346.848852
	Epoch 13....
Epoch has taken 0:00:11.145845
Number of used sentences in train = 231
Total loss for epoch 13: 346.539224
	Epoch 14....
Epoch has taken 0:00:11.149099
Number of used sentences in train = 231
Total loss for epoch 14: 346.306010
Epoch has taken 0:00:11.143180

==================================================================================================
	Training time : 0:30:24.199772
==================================================================================================
	Identification : 0.257

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 72)
  (w_embeddings): Embedding(1202, 141)
  (lstm): LSTM(213, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14603.925019
validation loss after epoch 0 : 1354.416819
	Epoch 1....
Epoch has taken 0:03:33.189503
Number of used sentences in train = 3226
Total loss for epoch 1: 11850.095691
validation loss after epoch 1 : 1363.637967
	Epoch 2....
Epoch has taken 0:03:33.077615
Number of used sentences in train = 3226
Total loss for epoch 2: 10942.450697
validation loss after epoch 2 : 1337.528263
	Epoch 3....
Epoch has taken 0:03:35.689632
Number of used sentences in train = 3226
Total loss for epoch 3: 10427.205871
validation loss after epoch 3 : 1341.683955
	Epoch 4....
Epoch has taken 0:04:00.581557
Number of used sentences in train = 3226
Total loss for epoch 4: 10003.504253
validation loss after epoch 4 : 1363.841450
	Epoch 5....
Epoch has taken 0:03:35.507760
Number of used sentences in train = 3226
Total loss for epoch 5: 9527.926720
validation loss after epoch 5 : 1399.064141
	Epoch 6....
Epoch has taken 0:03:35.216486
Number of used sentences in train = 3226
Total loss for epoch 6: 9202.673833
validation loss after epoch 6 : 1421.482079
	Epoch 7....
Epoch has taken 0:03:33.520995
Number of used sentences in train = 3226
Total loss for epoch 7: 8916.223327
validation loss after epoch 7 : 1527.830081
	Epoch 8....
Epoch has taken 0:03:33.354418
Number of used sentences in train = 3226
Total loss for epoch 8: 8611.185601
validation loss after epoch 8 : 1477.035737
	Epoch 9....
Epoch has taken 0:03:35.511235
Number of used sentences in train = 3226
Total loss for epoch 9: 8386.848613
validation loss after epoch 9 : 1543.995241
	Epoch 10....
Epoch has taken 0:04:09.170215
Number of used sentences in train = 3226
Total loss for epoch 10: 8122.908482
validation loss after epoch 10 : 1567.356599
	Epoch 11....
Epoch has taken 0:03:35.444288
Number of used sentences in train = 3226
Total loss for epoch 11: 7927.561638
validation loss after epoch 11 : 1573.228702
	Epoch 12....
Epoch has taken 0:03:32.963033
Number of used sentences in train = 3226
Total loss for epoch 12: 7769.966278
validation loss after epoch 12 : 1623.728352
	Epoch 13....
Epoch has taken 0:03:33.360723
Number of used sentences in train = 3226
Total loss for epoch 13: 7622.760858
validation loss after epoch 13 : 1645.888866
	Epoch 14....
Epoch has taken 0:03:32.980339
Number of used sentences in train = 3226
Total loss for epoch 14: 7492.354154
validation loss after epoch 14 : 1724.255073
	TransitionClassifier(
  (p_embeddings): Embedding(13, 72)
  (w_embeddings): Embedding(1202, 141)
  (lstm): LSTM(213, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:35.451263
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2182.362289
	Epoch 1....
Epoch has taken 0:00:21.017207
Number of used sentences in train = 359
Total loss for epoch 1: 1127.937453
	Epoch 2....
Epoch has taken 0:00:21.033182
Number of used sentences in train = 359
Total loss for epoch 2: 943.795224
	Epoch 3....
Epoch has taken 0:00:21.029308
Number of used sentences in train = 359
Total loss for epoch 3: 867.501927
	Epoch 4....
Epoch has taken 0:00:21.034938
Number of used sentences in train = 359
Total loss for epoch 4: 803.464248
	Epoch 5....
Epoch has taken 0:00:21.061157
Number of used sentences in train = 359
Total loss for epoch 5: 768.379498
	Epoch 6....
Epoch has taken 0:00:21.042166
Number of used sentences in train = 359
Total loss for epoch 6: 748.192578
	Epoch 7....
Epoch has taken 0:00:21.032446
Number of used sentences in train = 359
Total loss for epoch 7: 732.800584
	Epoch 8....
Epoch has taken 0:00:21.025461
Number of used sentences in train = 359
Total loss for epoch 8: 719.808665
	Epoch 9....
Epoch has taken 0:00:21.039733
Number of used sentences in train = 359
Total loss for epoch 9: 712.338055
	Epoch 10....
Epoch has taken 0:00:21.033259
Number of used sentences in train = 359
Total loss for epoch 10: 708.426128
	Epoch 11....
Epoch has taken 0:00:21.026738
Number of used sentences in train = 359
Total loss for epoch 11: 702.935029
	Epoch 12....
Epoch has taken 0:00:21.020604
Number of used sentences in train = 359
Total loss for epoch 12: 695.730376
	Epoch 13....
Epoch has taken 0:00:21.038628
Number of used sentences in train = 359
Total loss for epoch 13: 696.826330
	Epoch 14....
Epoch has taken 0:00:21.074572
Number of used sentences in train = 359
Total loss for epoch 14: 696.422237
Epoch has taken 0:00:21.035667

==================================================================================================
	Training time : 0:59:51.206825
==================================================================================================
	Identification : 0.485

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 13, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 31, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 24, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 241, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 31)
  (w_embeddings): Embedding(1882, 241)
  (lstm): LSTM(272, 24, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10004.775340
validation loss after epoch 0 : 896.669407
	Epoch 1....
Epoch has taken 0:02:50.009980
Number of used sentences in train = 2811
Total loss for epoch 1: 7274.957093
validation loss after epoch 1 : 833.745836
	Epoch 2....
Epoch has taken 0:02:50.108544
Number of used sentences in train = 2811
Total loss for epoch 2: 6380.443325
validation loss after epoch 2 : 835.607429
	Epoch 3....
Epoch has taken 0:02:58.847033
Number of used sentences in train = 2811
Total loss for epoch 3: 5955.130438
validation loss after epoch 3 : 846.540623
	Epoch 4....
Epoch has taken 0:02:51.546469
Number of used sentences in train = 2811
Total loss for epoch 4: 5621.528146
validation loss after epoch 4 : 860.513046
	Epoch 5....
Epoch has taken 0:03:01.411201
Number of used sentences in train = 2811
Total loss for epoch 5: 5459.467201
validation loss after epoch 5 : 890.084423
	Epoch 6....
Epoch has taken 0:02:50.422108
Number of used sentences in train = 2811
Total loss for epoch 6: 5301.980053
validation loss after epoch 6 : 904.262379
	Epoch 7....
Epoch has taken 0:02:50.209671
Number of used sentences in train = 2811
Total loss for epoch 7: 5160.169587
validation loss after epoch 7 : 935.975561
	Epoch 8....
Epoch has taken 0:02:52.096372
Number of used sentences in train = 2811
Total loss for epoch 8: 5062.698310
validation loss after epoch 8 : 915.373591
	Epoch 9....
Epoch has taken 0:03:17.871716
Number of used sentences in train = 2811
Total loss for epoch 9: 4957.010648
validation loss after epoch 9 : 957.630085
	Epoch 10....
Epoch has taken 0:02:51.005752
Number of used sentences in train = 2811
Total loss for epoch 10: 4918.992837
validation loss after epoch 10 : 982.835400
	Epoch 11....
Epoch has taken 0:02:50.142709
Number of used sentences in train = 2811
Total loss for epoch 11: 4867.001291
validation loss after epoch 11 : 997.478526
	Epoch 12....
Epoch has taken 0:02:50.097622
Number of used sentences in train = 2811
Total loss for epoch 12: 4835.562426
validation loss after epoch 12 : 999.861838
	Epoch 13....
Epoch has taken 0:02:51.966312
Number of used sentences in train = 2811
Total loss for epoch 13: 4797.262698
validation loss after epoch 13 : 1019.654154
	Epoch 14....
Epoch has taken 0:02:51.527125
Number of used sentences in train = 2811
Total loss for epoch 14: 4740.444031
validation loss after epoch 14 : 1042.774329
	TransitionClassifier(
  (p_embeddings): Embedding(18, 31)
  (w_embeddings): Embedding(1882, 241)
  (lstm): LSTM(272, 24, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.759940
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1155.092496
	Epoch 1....
Epoch has taken 0:00:18.038155
Number of used sentences in train = 313
Total loss for epoch 1: 721.034265
	Epoch 2....
Epoch has taken 0:00:18.004194
Number of used sentences in train = 313
Total loss for epoch 2: 618.894745
	Epoch 3....
Epoch has taken 0:00:18.014273
Number of used sentences in train = 313
Total loss for epoch 3: 584.941955
	Epoch 4....
Epoch has taken 0:00:18.030574
Number of used sentences in train = 313
Total loss for epoch 4: 544.138572
	Epoch 5....
Epoch has taken 0:00:18.030950
Number of used sentences in train = 313
Total loss for epoch 5: 535.256908
	Epoch 6....
Epoch has taken 0:00:18.013801
Number of used sentences in train = 313
Total loss for epoch 6: 542.420773
	Epoch 7....
Epoch has taken 0:00:18.035579
Number of used sentences in train = 313
Total loss for epoch 7: 526.110163
	Epoch 8....
Epoch has taken 0:00:18.012176
Number of used sentences in train = 313
Total loss for epoch 8: 520.958297
	Epoch 9....
Epoch has taken 0:00:18.038849
Number of used sentences in train = 313
Total loss for epoch 9: 518.995907
	Epoch 10....
Epoch has taken 0:00:18.014711
Number of used sentences in train = 313
Total loss for epoch 10: 517.656586
	Epoch 11....
Epoch has taken 0:00:18.022833
Number of used sentences in train = 313
Total loss for epoch 11: 516.943175
	Epoch 12....
Epoch has taken 0:00:18.037233
Number of used sentences in train = 313
Total loss for epoch 12: 517.633842
	Epoch 13....
Epoch has taken 0:00:18.030167
Number of used sentences in train = 313
Total loss for epoch 13: 514.219675
	Epoch 14....
Epoch has taken 0:00:18.036712
Number of used sentences in train = 313
Total loss for epoch 14: 514.236921
Epoch has taken 0:00:18.002581

==================================================================================================
	Training time : 0:47:58.884988
==================================================================================================
	Identification : 0.023

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 31)
  (w_embeddings): Embedding(1680, 241)
  (lstm): LSTM(272, 24, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7933.692797
validation loss after epoch 0 : 689.718339
	Epoch 1....
Epoch has taken 0:01:57.959801
Number of used sentences in train = 2074
Total loss for epoch 1: 5038.683265
validation loss after epoch 1 : 713.150693
	Epoch 2....
Epoch has taken 0:01:57.969216
Number of used sentences in train = 2074
Total loss for epoch 2: 4305.076609
validation loss after epoch 2 : 724.848685
	Epoch 3....
Epoch has taken 0:02:15.560218
Number of used sentences in train = 2074
Total loss for epoch 3: 3922.677733
validation loss after epoch 3 : 740.562140
	Epoch 4....
Epoch has taken 0:01:57.287735
Number of used sentences in train = 2074
Total loss for epoch 4: 3683.539018
validation loss after epoch 4 : 800.791669
	Epoch 5....
Epoch has taken 0:01:56.816624
Number of used sentences in train = 2074
Total loss for epoch 5: 3559.438746
validation loss after epoch 5 : 840.359533
	Epoch 6....
Epoch has taken 0:01:58.096846
Number of used sentences in train = 2074
Total loss for epoch 6: 3498.134181
validation loss after epoch 6 : 840.990967
	Epoch 7....
Epoch has taken 0:01:57.803742
Number of used sentences in train = 2074
Total loss for epoch 7: 3394.364769
validation loss after epoch 7 : 831.994421
	Epoch 8....
Epoch has taken 0:01:57.978022
Number of used sentences in train = 2074
Total loss for epoch 8: 3360.751474
validation loss after epoch 8 : 883.083476
	Epoch 9....
Epoch has taken 0:01:57.516529
Number of used sentences in train = 2074
Total loss for epoch 9: 3341.523427
validation loss after epoch 9 : 910.887860
	Epoch 10....
Epoch has taken 0:01:56.336008
Number of used sentences in train = 2074
Total loss for epoch 10: 3314.375389
validation loss after epoch 10 : 903.822354
	Epoch 11....
Epoch has taken 0:01:56.465745
Number of used sentences in train = 2074
Total loss for epoch 11: 3286.057210
validation loss after epoch 11 : 909.103989
	Epoch 12....
Epoch has taken 0:01:57.616699
Number of used sentences in train = 2074
Total loss for epoch 12: 3260.544108
validation loss after epoch 12 : 927.989834
	Epoch 13....
Epoch has taken 0:02:15.194745
Number of used sentences in train = 2074
Total loss for epoch 13: 3264.274480
validation loss after epoch 13 : 990.030267
	Epoch 14....
Epoch has taken 0:01:57.241102
Number of used sentences in train = 2074
Total loss for epoch 14: 3263.953434
validation loss after epoch 14 : 945.935359
	TransitionClassifier(
  (p_embeddings): Embedding(18, 31)
  (w_embeddings): Embedding(1680, 241)
  (lstm): LSTM(272, 24, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.154729
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 997.363299
	Epoch 1....
Epoch has taken 0:00:11.878990
Number of used sentences in train = 231
Total loss for epoch 1: 557.179137
	Epoch 2....
Epoch has taken 0:00:11.896925
Number of used sentences in train = 231
Total loss for epoch 2: 459.734712
	Epoch 3....
Epoch has taken 0:00:11.865629
Number of used sentences in train = 231
Total loss for epoch 3: 409.817446
	Epoch 4....
Epoch has taken 0:00:11.887713
Number of used sentences in train = 231
Total loss for epoch 4: 378.659682
	Epoch 5....
Epoch has taken 0:00:11.892274
Number of used sentences in train = 231
Total loss for epoch 5: 368.090930
	Epoch 6....
Epoch has taken 0:00:11.884737
Number of used sentences in train = 231
Total loss for epoch 6: 356.424839
	Epoch 7....
Epoch has taken 0:00:11.871349
Number of used sentences in train = 231
Total loss for epoch 7: 354.191758
	Epoch 8....
Epoch has taken 0:00:11.883145
Number of used sentences in train = 231
Total loss for epoch 8: 357.239442
	Epoch 9....
Epoch has taken 0:00:11.862604
Number of used sentences in train = 231
Total loss for epoch 9: 351.605872
	Epoch 10....
Epoch has taken 0:00:11.867358
Number of used sentences in train = 231
Total loss for epoch 10: 356.016671
	Epoch 11....
Epoch has taken 0:00:11.857664
Number of used sentences in train = 231
Total loss for epoch 11: 351.293944
	Epoch 12....
Epoch has taken 0:00:11.859179
Number of used sentences in train = 231
Total loss for epoch 12: 350.237578
	Epoch 13....
Epoch has taken 0:00:11.847244
Number of used sentences in train = 231
Total loss for epoch 13: 347.446798
	Epoch 14....
Epoch has taken 0:00:11.879052
Number of used sentences in train = 231
Total loss for epoch 14: 348.471791
Epoch has taken 0:00:11.869786

==================================================================================================
	Training time : 0:32:55.437268
==================================================================================================
	Identification : 0.213

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 31)
  (w_embeddings): Embedding(3369, 241)
  (lstm): LSTM(272, 24, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12288.770319
validation loss after epoch 0 : 1109.801819
	Epoch 1....
Epoch has taken 0:03:47.424333
Number of used sentences in train = 3226
Total loss for epoch 1: 9295.002417
validation loss after epoch 1 : 1107.119760
	Epoch 2....
Epoch has taken 0:03:50.661993
Number of used sentences in train = 3226
Total loss for epoch 2: 8491.957366
validation loss after epoch 2 : 1114.471436
	Epoch 3....
Epoch has taken 0:03:47.692047
Number of used sentences in train = 3226
Total loss for epoch 3: 7989.381720
validation loss after epoch 3 : 1207.223484
	Epoch 4....
Epoch has taken 0:03:45.640402
Number of used sentences in train = 3226
Total loss for epoch 4: 7656.240600
validation loss after epoch 4 : 1191.035766
	Epoch 5....
Epoch has taken 0:03:45.738720
Number of used sentences in train = 3226
Total loss for epoch 5: 7401.243854
validation loss after epoch 5 : 1283.555007
	Epoch 6....
Epoch has taken 0:03:48.125569
Number of used sentences in train = 3226
Total loss for epoch 6: 7200.171252
validation loss after epoch 6 : 1287.955204
	Epoch 7....
Epoch has taken 0:03:47.552431
Number of used sentences in train = 3226
Total loss for epoch 7: 7001.295993
validation loss after epoch 7 : 1338.047254
	Epoch 8....
Epoch has taken 0:03:47.759391
Number of used sentences in train = 3226
Total loss for epoch 8: 6910.647045
validation loss after epoch 8 : 1332.711734
	Epoch 9....
Epoch has taken 0:03:47.230434
Number of used sentences in train = 3226
Total loss for epoch 9: 6823.997373
validation loss after epoch 9 : 1391.387614
	Epoch 10....
Epoch has taken 0:03:45.767122
Number of used sentences in train = 3226
Total loss for epoch 10: 6748.492951
validation loss after epoch 10 : 1464.119320
	Epoch 11....
Epoch has taken 0:03:47.465364
Number of used sentences in train = 3226
Total loss for epoch 11: 6709.120690
validation loss after epoch 11 : 1479.912536
	Epoch 12....
Epoch has taken 0:03:47.498334
Number of used sentences in train = 3226
Total loss for epoch 12: 6640.005577
validation loss after epoch 12 : 1469.480238
	Epoch 13....
Epoch has taken 0:03:47.717992
Number of used sentences in train = 3226
Total loss for epoch 13: 6613.727062
validation loss after epoch 13 : 1493.923468
	Epoch 14....
Epoch has taken 0:03:45.610265
Number of used sentences in train = 3226
Total loss for epoch 14: 6588.497680
validation loss after epoch 14 : 1489.574348
	TransitionClassifier(
  (p_embeddings): Embedding(13, 31)
  (w_embeddings): Embedding(3369, 241)
  (lstm): LSTM(272, 24, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:47.355328
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1618.921019
	Epoch 1....
Epoch has taken 0:00:22.286343
Number of used sentences in train = 359
Total loss for epoch 1: 941.079185
	Epoch 2....
Epoch has taken 0:00:22.267305
Number of used sentences in train = 359
Total loss for epoch 2: 834.228486
	Epoch 3....
Epoch has taken 0:00:22.297483
Number of used sentences in train = 359
Total loss for epoch 3: 770.010682
	Epoch 4....
Epoch has taken 0:00:22.304558
Number of used sentences in train = 359
Total loss for epoch 4: 733.812171
	Epoch 5....
Epoch has taken 0:00:22.291078
Number of used sentences in train = 359
Total loss for epoch 5: 722.832269
	Epoch 6....
Epoch has taken 0:00:22.327645
Number of used sentences in train = 359
Total loss for epoch 6: 705.547575
	Epoch 7....
Epoch has taken 0:00:22.299131
Number of used sentences in train = 359
Total loss for epoch 7: 686.724080
	Epoch 8....
Epoch has taken 0:00:22.288837
Number of used sentences in train = 359
Total loss for epoch 8: 679.042508
	Epoch 9....
Epoch has taken 0:00:22.334642
Number of used sentences in train = 359
Total loss for epoch 9: 684.835372
	Epoch 10....
Epoch has taken 0:00:22.290168
Number of used sentences in train = 359
Total loss for epoch 10: 686.737536
	Epoch 11....
Epoch has taken 0:00:22.289272
Number of used sentences in train = 359
Total loss for epoch 11: 678.002105
	Epoch 12....
Epoch has taken 0:00:22.276856
Number of used sentences in train = 359
Total loss for epoch 12: 677.081630
	Epoch 13....
Epoch has taken 0:00:22.311771
Number of used sentences in train = 359
Total loss for epoch 13: 674.356253
	Epoch 14....
Epoch has taken 0:00:22.328226
Number of used sentences in train = 359
Total loss for epoch 14: 676.140107
Epoch has taken 0:00:22.312457

==================================================================================================
	Training time : 1:02:24.414581
==================================================================================================
	Identification : 0.262

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 22, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 63, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 20, 'lstmDropout': 0.14, 'denseActivation': 'tanh', 'wordDim': 66, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1177, 66)
  (lstm): LSTM(129, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11128.020058
validation loss after epoch 0 : 975.476086
	Epoch 1....
Epoch has taken 0:02:39.355315
Number of used sentences in train = 2811
Total loss for epoch 1: 8222.373769
validation loss after epoch 1 : 938.919542
	Epoch 2....
Epoch has taken 0:02:39.340205
Number of used sentences in train = 2811
Total loss for epoch 2: 7361.051625
validation loss after epoch 2 : 938.309615
	Epoch 3....
Epoch has taken 0:02:37.888490
Number of used sentences in train = 2811
Total loss for epoch 3: 6838.432958
validation loss after epoch 3 : 961.326858
	Epoch 4....
Epoch has taken 0:02:39.483320
Number of used sentences in train = 2811
Total loss for epoch 4: 6492.246436
validation loss after epoch 4 : 966.149647
	Epoch 5....
Epoch has taken 0:03:04.425818
Number of used sentences in train = 2811
Total loss for epoch 5: 6185.385287
validation loss after epoch 5 : 1005.097978
	Epoch 6....
Epoch has taken 0:02:39.354439
Number of used sentences in train = 2811
Total loss for epoch 6: 5941.539411
validation loss after epoch 6 : 1029.655637
	Epoch 7....
Epoch has taken 0:02:39.191797
Number of used sentences in train = 2811
Total loss for epoch 7: 5732.909237
validation loss after epoch 7 : 1049.910292
	Epoch 8....
Epoch has taken 0:02:37.776245
Number of used sentences in train = 2811
Total loss for epoch 8: 5569.095919
validation loss after epoch 8 : 1106.818266
	Epoch 9....
Epoch has taken 0:02:37.851548
Number of used sentences in train = 2811
Total loss for epoch 9: 5447.265186
validation loss after epoch 9 : 1084.717257
	Epoch 10....
Epoch has taken 0:02:39.521686
Number of used sentences in train = 2811
Total loss for epoch 10: 5324.599803
validation loss after epoch 10 : 1092.271491
	Epoch 11....
Epoch has taken 0:02:39.271762
Number of used sentences in train = 2811
Total loss for epoch 11: 5222.549368
validation loss after epoch 11 : 1161.334393
	Epoch 12....
Epoch has taken 0:03:04.420889
Number of used sentences in train = 2811
Total loss for epoch 12: 5153.221491
validation loss after epoch 12 : 1175.861786
	Epoch 13....
Epoch has taken 0:02:39.103209
Number of used sentences in train = 2811
Total loss for epoch 13: 5083.994405
validation loss after epoch 13 : 1178.092117
	Epoch 14....
Epoch has taken 0:02:38.018325
Number of used sentences in train = 2811
Total loss for epoch 14: 5017.028232
validation loss after epoch 14 : 1183.947242
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1177, 66)
  (lstm): LSTM(129, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:37.973853
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1221.724446
	Epoch 1....
Epoch has taken 0:00:16.840100
Number of used sentences in train = 313
Total loss for epoch 1: 778.559984
	Epoch 2....
Epoch has taken 0:00:16.824969
Number of used sentences in train = 313
Total loss for epoch 2: 658.230337
	Epoch 3....
Epoch has taken 0:00:16.854473
Number of used sentences in train = 313
Total loss for epoch 3: 597.694250
	Epoch 4....
Epoch has taken 0:00:16.823595
Number of used sentences in train = 313
Total loss for epoch 4: 561.478976
	Epoch 5....
Epoch has taken 0:00:16.814100
Number of used sentences in train = 313
Total loss for epoch 5: 537.747151
	Epoch 6....
Epoch has taken 0:00:16.830703
Number of used sentences in train = 313
Total loss for epoch 6: 529.676406
	Epoch 7....
Epoch has taken 0:00:16.875401
Number of used sentences in train = 313
Total loss for epoch 7: 523.778836
	Epoch 8....
Epoch has taken 0:00:16.866901
Number of used sentences in train = 313
Total loss for epoch 8: 515.627500
	Epoch 9....
Epoch has taken 0:00:16.875819
Number of used sentences in train = 313
Total loss for epoch 9: 512.778735
	Epoch 10....
Epoch has taken 0:00:16.858015
Number of used sentences in train = 313
Total loss for epoch 10: 510.133386
	Epoch 11....
Epoch has taken 0:00:16.856727
Number of used sentences in train = 313
Total loss for epoch 11: 507.342771
	Epoch 12....
Epoch has taken 0:00:16.851627
Number of used sentences in train = 313
Total loss for epoch 12: 505.858611
	Epoch 13....
Epoch has taken 0:00:16.869364
Number of used sentences in train = 313
Total loss for epoch 13: 506.435697
	Epoch 14....
Epoch has taken 0:00:16.872824
Number of used sentences in train = 313
Total loss for epoch 14: 505.897624
Epoch has taken 0:00:16.862548

==================================================================================================
	Training time : 0:44:46.241931
==================================================================================================
	Identification : 0.474

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1133, 66)
  (lstm): LSTM(129, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8121.538500
validation loss after epoch 0 : 770.004639
	Epoch 1....
Epoch has taken 0:01:48.835117
Number of used sentences in train = 2074
Total loss for epoch 1: 5785.190698
validation loss after epoch 1 : 679.101539
	Epoch 2....
Epoch has taken 0:01:48.804931
Number of used sentences in train = 2074
Total loss for epoch 2: 5093.735880
validation loss after epoch 2 : 659.290186
	Epoch 3....
Epoch has taken 0:01:47.826033
Number of used sentences in train = 2074
Total loss for epoch 3: 4621.079158
validation loss after epoch 3 : 660.029445
	Epoch 4....
Epoch has taken 0:01:47.903705
Number of used sentences in train = 2074
Total loss for epoch 4: 4317.764130
validation loss after epoch 4 : 719.509510
	Epoch 5....
Epoch has taken 0:01:49.082823
Number of used sentences in train = 2074
Total loss for epoch 5: 4077.738615
validation loss after epoch 5 : 696.805773
	Epoch 6....
Epoch has taken 0:01:48.930126
Number of used sentences in train = 2074
Total loss for epoch 6: 3884.678806
validation loss after epoch 6 : 713.910772
	Epoch 7....
Epoch has taken 0:01:48.935611
Number of used sentences in train = 2074
Total loss for epoch 7: 3760.581284
validation loss after epoch 7 : 738.221160
	Epoch 8....
Epoch has taken 0:01:48.709441
Number of used sentences in train = 2074
Total loss for epoch 8: 3624.067492
validation loss after epoch 8 : 752.861511
	Epoch 9....
Epoch has taken 0:01:47.860694
Number of used sentences in train = 2074
Total loss for epoch 9: 3527.017481
validation loss after epoch 9 : 780.813020
	Epoch 10....
Epoch has taken 0:01:47.882815
Number of used sentences in train = 2074
Total loss for epoch 10: 3464.256242
validation loss after epoch 10 : 798.692312
	Epoch 11....
Epoch has taken 0:01:50.378340
Number of used sentences in train = 2074
Total loss for epoch 11: 3400.169579
validation loss after epoch 11 : 811.513047
	Epoch 12....
Epoch has taken 0:01:48.886047
Number of used sentences in train = 2074
Total loss for epoch 12: 3364.568098
validation loss after epoch 12 : 816.347134
	Epoch 13....
Epoch has taken 0:02:06.031564
Number of used sentences in train = 2074
Total loss for epoch 13: 3326.092926
validation loss after epoch 13 : 836.488796
	Epoch 14....
Epoch has taken 0:01:48.925984
Number of used sentences in train = 2074
Total loss for epoch 14: 3285.929125
validation loss after epoch 14 : 856.214019
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1133, 66)
  (lstm): LSTM(129, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:47.919517
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1197.434492
	Epoch 1....
Epoch has taken 0:00:10.964759
Number of used sentences in train = 231
Total loss for epoch 1: 601.659931
	Epoch 2....
Epoch has taken 0:00:10.967169
Number of used sentences in train = 231
Total loss for epoch 2: 478.992179
	Epoch 3....
Epoch has taken 0:00:10.971293
Number of used sentences in train = 231
Total loss for epoch 3: 419.561524
	Epoch 4....
Epoch has taken 0:00:10.979141
Number of used sentences in train = 231
Total loss for epoch 4: 390.138116
	Epoch 5....
Epoch has taken 0:00:10.992525
Number of used sentences in train = 231
Total loss for epoch 5: 374.066491
	Epoch 6....
Epoch has taken 0:00:10.972548
Number of used sentences in train = 231
Total loss for epoch 6: 363.842001
	Epoch 7....
Epoch has taken 0:00:11.003280
Number of used sentences in train = 231
Total loss for epoch 7: 355.630576
	Epoch 8....
Epoch has taken 0:00:10.993236
Number of used sentences in train = 231
Total loss for epoch 8: 351.972124
	Epoch 9....
Epoch has taken 0:00:10.989606
Number of used sentences in train = 231
Total loss for epoch 9: 350.732341
	Epoch 10....
Epoch has taken 0:00:10.998722
Number of used sentences in train = 231
Total loss for epoch 10: 348.824636
	Epoch 11....
Epoch has taken 0:00:10.970816
Number of used sentences in train = 231
Total loss for epoch 11: 348.101673
	Epoch 12....
Epoch has taken 0:00:10.990647
Number of used sentences in train = 231
Total loss for epoch 12: 347.546981
	Epoch 13....
Epoch has taken 0:00:11.062686
Number of used sentences in train = 231
Total loss for epoch 13: 347.167900
	Epoch 14....
Epoch has taken 0:00:11.109590
Number of used sentences in train = 231
Total loss for epoch 14: 346.877197
Epoch has taken 0:00:11.084499

==================================================================================================
	Training time : 0:30:12.289486
==================================================================================================
	Identification : 0.269

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 63)
  (w_embeddings): Embedding(1202, 66)
  (lstm): LSTM(129, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14586.401580
validation loss after epoch 0 : 1441.808685
	Epoch 1....
Epoch has taken 0:04:07.881874
Number of used sentences in train = 3226
Total loss for epoch 1: 11993.723217
validation loss after epoch 1 : 1391.037421
	Epoch 2....
Epoch has taken 0:04:05.031000
Number of used sentences in train = 3226
Total loss for epoch 2: 11152.958043
validation loss after epoch 2 : 1372.205440
	Epoch 3....
Epoch has taken 0:03:34.511653
Number of used sentences in train = 3226
Total loss for epoch 3: 10523.331209
validation loss after epoch 3 : 1402.342498
	Epoch 4....
Epoch has taken 0:03:32.216928
Number of used sentences in train = 3226
Total loss for epoch 4: 10008.250105
validation loss after epoch 4 : 1384.000457
	Epoch 5....
Epoch has taken 0:03:32.111721
Number of used sentences in train = 3226
Total loss for epoch 5: 9613.257609
validation loss after epoch 5 : 1440.968369
	Epoch 6....
Epoch has taken 0:03:34.352187
Number of used sentences in train = 3226
Total loss for epoch 6: 9314.918474
validation loss after epoch 6 : 1466.966641
	Epoch 7....
Epoch has taken 0:03:34.680326
Number of used sentences in train = 3226
Total loss for epoch 7: 9059.427364
validation loss after epoch 7 : 1460.675313
	Epoch 8....
Epoch has taken 0:03:32.244880
Number of used sentences in train = 3226
Total loss for epoch 8: 8843.856628
validation loss after epoch 8 : 1515.677437
	Epoch 9....
Epoch has taken 0:03:32.318786
Number of used sentences in train = 3226
Total loss for epoch 9: 8639.723548
validation loss after epoch 9 : 1524.766625
	Epoch 10....
Epoch has taken 0:03:32.265903
Number of used sentences in train = 3226
Total loss for epoch 10: 8478.302771
validation loss after epoch 10 : 1564.626764
	Epoch 11....
Epoch has taken 0:03:34.838600
Number of used sentences in train = 3226
Total loss for epoch 11: 8333.895514
validation loss after epoch 11 : 1571.967191
	Epoch 12....
Epoch has taken 0:03:34.592235
Number of used sentences in train = 3226
Total loss for epoch 12: 8186.708382
validation loss after epoch 12 : 1649.184073
	Epoch 13....
Epoch has taken 0:03:34.265841
Number of used sentences in train = 3226
Total loss for epoch 13: 8050.071032
validation loss after epoch 13 : 1709.352942
	Epoch 14....
Epoch has taken 0:03:32.474197
Number of used sentences in train = 3226
Total loss for epoch 14: 7925.561809
validation loss after epoch 14 : 1689.381674
	TransitionClassifier(
  (p_embeddings): Embedding(13, 63)
  (w_embeddings): Embedding(1202, 66)
  (lstm): LSTM(129, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:32.363434
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1801.516464
	Epoch 1....
Epoch has taken 0:00:20.958872
Number of used sentences in train = 359
Total loss for epoch 1: 1158.624764
	Epoch 2....
Epoch has taken 0:00:20.952571
Number of used sentences in train = 359
Total loss for epoch 2: 999.892644
	Epoch 3....
Epoch has taken 0:00:20.951396
Number of used sentences in train = 359
Total loss for epoch 3: 898.454916
	Epoch 4....
Epoch has taken 0:00:20.966899
Number of used sentences in train = 359
Total loss for epoch 4: 818.629984
	Epoch 5....
Epoch has taken 0:00:20.957804
Number of used sentences in train = 359
Total loss for epoch 5: 777.725934
	Epoch 6....
Epoch has taken 0:00:20.972045
Number of used sentences in train = 359
Total loss for epoch 6: 736.934775
	Epoch 7....
Epoch has taken 0:00:20.963532
Number of used sentences in train = 359
Total loss for epoch 7: 719.206740
	Epoch 8....
Epoch has taken 0:00:20.985248
Number of used sentences in train = 359
Total loss for epoch 8: 705.875941
	Epoch 9....
Epoch has taken 0:00:20.962136
Number of used sentences in train = 359
Total loss for epoch 9: 696.480335
	Epoch 10....
Epoch has taken 0:00:20.954882
Number of used sentences in train = 359
Total loss for epoch 10: 688.219242
	Epoch 11....
Epoch has taken 0:00:20.935552
Number of used sentences in train = 359
Total loss for epoch 11: 684.091788
	Epoch 12....
Epoch has taken 0:00:20.958696
Number of used sentences in train = 359
Total loss for epoch 12: 681.241618
	Epoch 13....
Epoch has taken 0:00:20.957536
Number of used sentences in train = 359
Total loss for epoch 13: 678.480435
	Epoch 14....
Epoch has taken 0:00:20.941626
Number of used sentences in train = 359
Total loss for epoch 14: 677.038771
Epoch has taken 0:00:21.033070

==================================================================================================
	Training time : 0:59:41.238363
==================================================================================================
	Identification : 0.106

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 72, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 26, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 127, 'lstmDropout': 0.32, 'denseActivation': 'tanh', 'wordDim': 68, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(9350, 68)
  (lstm): LSTM(94, 127, bidirectional=True)
  (linear1): Linear(in_features=2032, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15427.871270
validation loss after epoch 0 : 1209.149007
	Epoch 1....
Epoch has taken 0:02:40.758007
Number of used sentences in train = 2811
Total loss for epoch 1: 9757.304376
validation loss after epoch 1 : 1089.516420
	Epoch 2....
Epoch has taken 0:02:40.928359
Number of used sentences in train = 2811
Total loss for epoch 2: 7908.098904
validation loss after epoch 2 : 1058.114259
	Epoch 3....
Epoch has taken 0:02:52.452362
Number of used sentences in train = 2811
Total loss for epoch 3: 6636.941359
validation loss after epoch 3 : 1095.601542
	Epoch 4....
Epoch has taken 0:02:58.594416
Number of used sentences in train = 2811
Total loss for epoch 4: 5807.399177
validation loss after epoch 4 : 1233.271390
	Epoch 5....
Epoch has taken 0:02:58.571215
Number of used sentences in train = 2811
Total loss for epoch 5: 5337.986408
validation loss after epoch 5 : 1274.050145
	Epoch 6....
Epoch has taken 0:03:04.745368
Number of used sentences in train = 2811
Total loss for epoch 6: 5027.770141
validation loss after epoch 6 : 1372.761424
	Epoch 7....
Epoch has taken 0:03:01.170993
Number of used sentences in train = 2811
Total loss for epoch 7: 4843.897360
validation loss after epoch 7 : 1396.548467
	Epoch 8....
Epoch has taken 0:03:13.015998
Number of used sentences in train = 2811
Total loss for epoch 8: 4723.724575
validation loss after epoch 8 : 1453.321857
	Epoch 9....
Epoch has taken 0:03:04.036686
Number of used sentences in train = 2811
Total loss for epoch 9: 4650.304358
validation loss after epoch 9 : 1511.015642
	Epoch 10....
Epoch has taken 0:02:46.856431
Number of used sentences in train = 2811
Total loss for epoch 10: 4579.524866
validation loss after epoch 10 : 1554.125584
	Epoch 11....
Epoch has taken 0:02:47.915104
Number of used sentences in train = 2811
Total loss for epoch 11: 4546.903737
validation loss after epoch 11 : 1583.368256
	Epoch 12....
Epoch has taken 0:02:52.595097
Number of used sentences in train = 2811
Total loss for epoch 12: 4525.988610
validation loss after epoch 12 : 1614.625178
	Epoch 13....
Epoch has taken 0:03:18.466949
Number of used sentences in train = 2811
Total loss for epoch 13: 4512.785271
validation loss after epoch 13 : 1634.482383
	Epoch 14....
Epoch has taken 0:02:52.228692
Number of used sentences in train = 2811
Total loss for epoch 14: 4502.279402
validation loss after epoch 14 : 1650.690632
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(9350, 68)
  (lstm): LSTM(94, 127, bidirectional=True)
  (linear1): Linear(in_features=2032, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.336556
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2168.400900
	Epoch 1....
Epoch has taken 0:00:18.137920
Number of used sentences in train = 313
Total loss for epoch 1: 914.729993
	Epoch 2....
Epoch has taken 0:00:17.937501
Number of used sentences in train = 313
Total loss for epoch 2: 704.967574
	Epoch 3....
Epoch has taken 0:00:18.117736
Number of used sentences in train = 313
Total loss for epoch 3: 612.409318
	Epoch 4....
Epoch has taken 0:00:17.818987
Number of used sentences in train = 313
Total loss for epoch 4: 554.354404
	Epoch 5....
Epoch has taken 0:00:18.152899
Number of used sentences in train = 313
Total loss for epoch 5: 541.276398
	Epoch 6....
Epoch has taken 0:00:18.038888
Number of used sentences in train = 313
Total loss for epoch 6: 530.561651
	Epoch 7....
Epoch has taken 0:00:18.391349
Number of used sentences in train = 313
Total loss for epoch 7: 526.034346
	Epoch 8....
Epoch has taken 0:00:18.168928
Number of used sentences in train = 313
Total loss for epoch 8: 522.660872
	Epoch 9....
Epoch has taken 0:00:18.149080
Number of used sentences in train = 313
Total loss for epoch 9: 519.135835
	Epoch 10....
Epoch has taken 0:00:18.156653
Number of used sentences in train = 313
Total loss for epoch 10: 517.724460
	Epoch 11....
Epoch has taken 0:00:18.203623
Number of used sentences in train = 313
Total loss for epoch 11: 518.365789
	Epoch 12....
Epoch has taken 0:00:18.442496
Number of used sentences in train = 313
Total loss for epoch 12: 515.567914
	Epoch 13....
Epoch has taken 0:00:18.043953
Number of used sentences in train = 313
Total loss for epoch 13: 514.424570
	Epoch 14....
Epoch has taken 0:00:18.024160
Number of used sentences in train = 313
Total loss for epoch 14: 513.651879
Epoch has taken 0:00:18.135114

==================================================================================================
	Training time : 0:48:37.100909
==================================================================================================
	Identification : 0.385

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(7051, 68)
  (lstm): LSTM(94, 127, bidirectional=True)
  (linear1): Linear(in_features=2032, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11222.262163
validation loss after epoch 0 : 970.866315
	Epoch 1....
Epoch has taken 0:01:57.679171
Number of used sentences in train = 2074
Total loss for epoch 1: 6953.111828
validation loss after epoch 1 : 1005.821835
	Epoch 2....
Epoch has taken 0:01:58.151525
Number of used sentences in train = 2074
Total loss for epoch 2: 5388.500318
validation loss after epoch 2 : 955.019058
	Epoch 3....
Epoch has taken 0:01:56.567048
Number of used sentences in train = 2074
Total loss for epoch 3: 4392.655609
validation loss after epoch 3 : 1051.001209
	Epoch 4....
Epoch has taken 0:01:57.087213
Number of used sentences in train = 2074
Total loss for epoch 4: 3768.264354
validation loss after epoch 4 : 1162.229119
	Epoch 5....
Epoch has taken 0:01:57.632860
Number of used sentences in train = 2074
Total loss for epoch 5: 3497.514240
validation loss after epoch 5 : 1192.256799
	Epoch 6....
Epoch has taken 0:02:15.372098
Number of used sentences in train = 2074
Total loss for epoch 6: 3378.497123
validation loss after epoch 6 : 1225.705733
	Epoch 7....
Epoch has taken 0:01:56.467619
Number of used sentences in train = 2074
Total loss for epoch 7: 3317.592781
validation loss after epoch 7 : 1251.772610
	Epoch 8....
Epoch has taken 0:01:56.210977
Number of used sentences in train = 2074
Total loss for epoch 8: 3262.668675
validation loss after epoch 8 : 1276.677667
	Epoch 9....
Epoch has taken 0:01:57.934793
Number of used sentences in train = 2074
Total loss for epoch 9: 3231.483408
validation loss after epoch 9 : 1314.436797
	Epoch 10....
Epoch has taken 0:01:58.291815
Number of used sentences in train = 2074
Total loss for epoch 10: 3215.179334
validation loss after epoch 10 : 1332.457583
	Epoch 11....
Epoch has taken 0:01:59.520987
Number of used sentences in train = 2074
Total loss for epoch 11: 3202.367890
validation loss after epoch 11 : 1348.256876
	Epoch 12....
Epoch has taken 0:01:57.676454
Number of used sentences in train = 2074
Total loss for epoch 12: 3193.895518
validation loss after epoch 12 : 1366.353646
	Epoch 13....
Epoch has taken 0:01:56.395449
Number of used sentences in train = 2074
Total loss for epoch 13: 3190.757006
validation loss after epoch 13 : 1371.936137
	Epoch 14....
Epoch has taken 0:01:57.427705
Number of used sentences in train = 2074
Total loss for epoch 14: 3188.951545
validation loss after epoch 14 : 1389.816748
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(7051, 68)
  (lstm): LSTM(94, 127, bidirectional=True)
  (linear1): Linear(in_features=2032, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.989386
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1766.045822
	Epoch 1....
Epoch has taken 0:00:12.072891
Number of used sentences in train = 231
Total loss for epoch 1: 670.318669
	Epoch 2....
Epoch has taken 0:00:11.999727
Number of used sentences in train = 231
Total loss for epoch 2: 515.612848
	Epoch 3....
Epoch has taken 0:00:12.007405
Number of used sentences in train = 231
Total loss for epoch 3: 409.937867
	Epoch 4....
Epoch has taken 0:00:12.170890
Number of used sentences in train = 231
Total loss for epoch 4: 367.331378
	Epoch 5....
Epoch has taken 0:00:11.968590
Number of used sentences in train = 231
Total loss for epoch 5: 355.963998
	Epoch 6....
Epoch has taken 0:00:12.065821
Number of used sentences in train = 231
Total loss for epoch 6: 351.988686
	Epoch 7....
Epoch has taken 0:00:12.059775
Number of used sentences in train = 231
Total loss for epoch 7: 349.697320
	Epoch 8....
Epoch has taken 0:00:12.076897
Number of used sentences in train = 231
Total loss for epoch 8: 348.960845
	Epoch 9....
Epoch has taken 0:00:12.055864
Number of used sentences in train = 231
Total loss for epoch 9: 348.139798
	Epoch 10....
Epoch has taken 0:00:12.263907
Number of used sentences in train = 231
Total loss for epoch 10: 347.604067
	Epoch 11....
Epoch has taken 0:00:11.951234
Number of used sentences in train = 231
Total loss for epoch 11: 347.161705
	Epoch 12....
Epoch has taken 0:00:12.278980
Number of used sentences in train = 231
Total loss for epoch 12: 346.753993
	Epoch 13....
Epoch has taken 0:00:11.990223
Number of used sentences in train = 231
Total loss for epoch 13: 346.688124
	Epoch 14....
Epoch has taken 0:00:12.154662
Number of used sentences in train = 231
Total loss for epoch 14: 347.283016
Epoch has taken 0:00:12.066606

==================================================================================================
	Training time : 0:32:41.929216
==================================================================================================
	Identification : 0.235

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(18003, 68)
  (lstm): LSTM(94, 127, bidirectional=True)
  (linear1): Linear(in_features=2032, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
	Epoch 0....
Number of used sentences in train = 3226
Total loss for epoch 0: 19382.765900
validation loss after epoch 0 : 1638.795528
	Epoch 1....
Epoch has taken 0:03:46.140681
Number of used sentences in train = 3226
Total loss for epoch 1: 12959.707217
validation loss after epoch 1 : 1616.745624
	Epoch 2....
Epoch has taken 0:03:46.247690
Number of used sentences in train = 3226
Total loss for epoch 2: 10483.266658
validation loss after epoch 2 : 1673.008790
	Epoch 3....
Epoch has taken 0:03:46.837740
Number of used sentences in train = 3226
Total loss for epoch 3: 8800.646272
validation loss after epoch 3 : 1758.691833
	Epoch 4....
Epoch has taken 0:03:48.899763
Number of used sentences in train = 3226
Total loss for epoch 4: 7666.814705
validation loss after epoch 4 : 1995.252907
	Epoch 5....
Epoch has taken 0:03:48.195766
Number of used sentences in train = 3226
Total loss for epoch 5: 7025.047068
validation loss after epoch 5 : 2126.959231
	Epoch 6....
Epoch has taken 0:03:49.015907
Number of used sentences in train = 3226
Total loss for epoch 6: 6605.201858
validation loss after epoch 6 : 2196.002161
	Epoch 7....
Epoch has taken 0:03:48.756042
Number of used sentences in train = 3226
Total loss for epoch 7: 6422.373218
validation loss after epoch 7 : 2371.765458
	Epoch 8....
Epoch has taken 0:03:46.731892
Number of used sentences in train = 3226
Total loss for epoch 8: 6306.257108
validation loss after epoch 8 : 2431.052456
	Epoch 9....
Epoch has taken 0:03:49.329446
Number of used sentences in train = 3226
Total loss for epoch 9: 6255.871404
validation loss after epoch 9 : 2466.730200
	Epoch 10....
Epoch has taken 0:03:49.233105
Number of used sentences in train = 3226
Total loss for epoch 10: 6216.338467
validation loss after epoch 10 : 2581.217351
	Epoch 11....
Epoch has taken 0:04:23.010610
Number of used sentences in train = 3226
Total loss for epoch 11: 6188.915317
validation loss after epoch 11 : 2585.377464
	Epoch 12....
Epoch has taken 0:03:49.978716
Number of used sentences in train = 3226
Total loss for epoch 12: 6166.316693
validation loss after epoch 12 : 2692.919918
	Epoch 13....
Epoch has taken 0:03:47.888518
Number of used sentences in train = 3226
Total loss for epoch 13: 6160.138943
validation loss after epoch 13 : 2649.314822
	Epoch 14....
Epoch has taken 0:03:47.245838
Number of used sentences in train = 3226
Total loss for epoch 14: 6150.089156
validation loss after epoch 14 : 2777.839038
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(18003, 68)
  (lstm): LSTM(94, 127, bidirectional=True)
  (linear1): Linear(in_features=2032, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:04.846646
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2292.704173
	Epoch 1....
Epoch has taken 0:00:22.591009
Number of used sentences in train = 359
Total loss for epoch 1: 1241.106976
	Epoch 2....
Epoch has taken 0:00:22.458252
Number of used sentences in train = 359
Total loss for epoch 2: 934.022463
	Epoch 3....
Epoch has taken 0:00:22.374515
Number of used sentences in train = 359
Total loss for epoch 3: 776.329658
	Epoch 4....
Epoch has taken 0:00:22.446686
Number of used sentences in train = 359
Total loss for epoch 4: 711.113952
	Epoch 5....
Epoch has taken 0:00:22.498508
Number of used sentences in train = 359
Total loss for epoch 5: 686.832426
	Epoch 6....
Epoch has taken 0:00:22.558342
Number of used sentences in train = 359
Total loss for epoch 6: 677.407966
	Epoch 7....
Epoch has taken 0:00:22.549978
Number of used sentences in train = 359
Total loss for epoch 7: 674.385119
	Epoch 8....
Epoch has taken 0:00:22.560354
Number of used sentences in train = 359
Total loss for epoch 8: 673.640628
	Epoch 9....
Epoch has taken 0:00:22.569792
Number of used sentences in train = 359
Total loss for epoch 9: 672.718066
	Epoch 10....
Epoch has taken 0:00:22.516005
Number of used sentences in train = 359
Total loss for epoch 10: 672.091641
	Epoch 11....
Epoch has taken 0:00:22.677746
Number of used sentences in train = 359
Total loss for epoch 11: 671.829209
	Epoch 12....
Epoch has taken 0:00:22.256646
Number of used sentences in train = 359
Total loss for epoch 12: 671.437290
	Epoch 13....
Epoch has taken 0:00:22.440377
Number of used sentences in train = 359
Total loss for epoch 13: 671.213521
	Epoch 14....
Epoch has taken 0:00:22.373020
Number of used sentences in train = 359
Total loss for epoch 14: 671.073035
Epoch has taken 0:00:22.318893

==================================================================================================
	Training time : 1:03:30.224255
==================================================================================================
	Identification : 0.455

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 11, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 41, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 90, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 239, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12221.679318
validation loss after epoch 0 : 1045.497830
	Epoch 1....
Epoch has taken 0:02:48.713818
Number of used sentences in train = 2811
Total loss for epoch 1: 8937.607796
validation loss after epoch 1 : 1030.080348
	Epoch 2....
Epoch has taken 0:02:48.007851
Number of used sentences in train = 2811
Total loss for epoch 2: 7929.290863
validation loss after epoch 2 : 970.328795
	Epoch 3....
Epoch has taken 0:02:50.975610
Number of used sentences in train = 2811
Total loss for epoch 3: 7269.235439
validation loss after epoch 3 : 998.462867
	Epoch 4....
Epoch has taken 0:02:44.863680
Number of used sentences in train = 2811
Total loss for epoch 4: 6750.903209
validation loss after epoch 4 : 1002.897103
	Epoch 5....
Epoch has taken 0:02:53.986145
Number of used sentences in train = 2811
Total loss for epoch 5: 6323.887426
validation loss after epoch 5 : 1052.728029
	Epoch 6....
Epoch has taken 0:02:43.996159
Number of used sentences in train = 2811
Total loss for epoch 6: 6005.526253
validation loss after epoch 6 : 1061.381232
	Epoch 7....
Epoch has taken 0:02:44.340614
Number of used sentences in train = 2811
Total loss for epoch 7: 5732.974369
validation loss after epoch 7 : 1079.971557
	Epoch 8....
Epoch has taken 0:02:54.124785
Number of used sentences in train = 2811
Total loss for epoch 8: 5571.883637
validation loss after epoch 8 : 1110.009445
	Epoch 9....
Epoch has taken 0:02:54.072380
Number of used sentences in train = 2811
Total loss for epoch 9: 5397.252157
validation loss after epoch 9 : 1125.327020
	Epoch 10....
Epoch has taken 0:02:52.426141
Number of used sentences in train = 2811
Total loss for epoch 10: 5281.272138
validation loss after epoch 10 : 1146.364843
	Epoch 11....
Epoch has taken 0:02:52.432731
Number of used sentences in train = 2811
Total loss for epoch 11: 5165.147427
validation loss after epoch 11 : 1167.175667
	Epoch 12....
Epoch has taken 0:02:57.029305
Number of used sentences in train = 2811
Total loss for epoch 12: 5108.661677
validation loss after epoch 12 : 1172.721731
	Epoch 13....
Epoch has taken 0:02:57.091278
Number of used sentences in train = 2811
Total loss for epoch 13: 5036.436927
validation loss after epoch 13 : 1188.867420
	Epoch 14....
Epoch has taken 0:02:39.828641
Number of used sentences in train = 2811
Total loss for epoch 14: 4976.359031
validation loss after epoch 14 : 1224.361122
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:59.093305
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1383.180961
	Epoch 1....
Epoch has taken 0:00:17.058890
Number of used sentences in train = 313
Total loss for epoch 1: 929.191948
	Epoch 2....
Epoch has taken 0:00:17.062673
Number of used sentences in train = 313
Total loss for epoch 2: 775.186301
	Epoch 3....
Epoch has taken 0:00:17.092618
Number of used sentences in train = 313
Total loss for epoch 3: 701.297688
	Epoch 4....
Epoch has taken 0:00:17.052784
Number of used sentences in train = 313
Total loss for epoch 4: 680.116779
	Epoch 5....
Epoch has taken 0:00:17.057458
Number of used sentences in train = 313
Total loss for epoch 5: 633.607549
	Epoch 6....
Epoch has taken 0:00:17.071484
Number of used sentences in train = 313
Total loss for epoch 6: 616.454454
	Epoch 7....
Epoch has taken 0:00:17.072357
Number of used sentences in train = 313
Total loss for epoch 7: 602.161122
	Epoch 8....
Epoch has taken 0:00:17.059878
Number of used sentences in train = 313
Total loss for epoch 8: 592.852530
	Epoch 9....
Epoch has taken 0:00:17.041045
Number of used sentences in train = 313
Total loss for epoch 9: 587.097329
	Epoch 10....
Epoch has taken 0:00:17.059550
Number of used sentences in train = 313
Total loss for epoch 10: 577.694064
	Epoch 11....
Epoch has taken 0:00:17.053804
Number of used sentences in train = 313
Total loss for epoch 11: 575.028064
	Epoch 12....
Epoch has taken 0:00:17.053562
Number of used sentences in train = 313
Total loss for epoch 12: 572.494226
	Epoch 13....
Epoch has taken 0:00:17.033493
Number of used sentences in train = 313
Total loss for epoch 13: 575.275354
	Epoch 14....
Epoch has taken 0:00:17.048724
Number of used sentences in train = 313
Total loss for epoch 14: 570.379539
Epoch has taken 0:00:17.042522

==================================================================================================
	Training time : 0:46:57.344373
==================================================================================================
	Identification : 0.527

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1133, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9311.448436
validation loss after epoch 0 : 966.912154
	Epoch 1....
Epoch has taken 0:01:48.982750
Number of used sentences in train = 2074
Total loss for epoch 1: 6093.800132
validation loss after epoch 1 : 813.077147
	Epoch 2....
Epoch has taken 0:02:00.055971
Number of used sentences in train = 2074
Total loss for epoch 2: 5174.909210
validation loss after epoch 2 : 803.107259
	Epoch 3....
Epoch has taken 0:02:01.611248
Number of used sentences in train = 2074
Total loss for epoch 3: 4640.383420
validation loss after epoch 3 : 811.709700
	Epoch 4....
Epoch has taken 0:02:01.576214
Number of used sentences in train = 2074
Total loss for epoch 4: 4273.760079
validation loss after epoch 4 : 815.785400
	Epoch 5....
Epoch has taken 0:02:00.562900
Number of used sentences in train = 2074
Total loss for epoch 5: 3968.940967
validation loss after epoch 5 : 838.157705
	Epoch 6....
Epoch has taken 0:02:00.387652
Number of used sentences in train = 2074
Total loss for epoch 6: 3791.156922
validation loss after epoch 6 : 876.314727
	Epoch 7....
Epoch has taken 0:02:01.578297
Number of used sentences in train = 2074
Total loss for epoch 7: 3652.047247
validation loss after epoch 7 : 887.592674
	Epoch 8....
Epoch has taken 0:02:01.732531
Number of used sentences in train = 2074
Total loss for epoch 8: 3568.649450
validation loss after epoch 8 : 890.238621
	Epoch 9....
Epoch has taken 0:02:04.857775
Number of used sentences in train = 2074
Total loss for epoch 9: 3511.213940
validation loss after epoch 9 : 934.739319
	Epoch 10....
Epoch has taken 0:02:00.712309
Number of used sentences in train = 2074
Total loss for epoch 10: 3459.118021
validation loss after epoch 10 : 976.792125
	Epoch 11....
Epoch has taken 0:01:52.275160
Number of used sentences in train = 2074
Total loss for epoch 11: 3414.395286
validation loss after epoch 11 : 966.491433
	Epoch 12....
Epoch has taken 0:01:52.040695
Number of used sentences in train = 2074
Total loss for epoch 12: 3375.639005
validation loss after epoch 12 : 977.478321
	Epoch 13....
Epoch has taken 0:01:50.832776
Number of used sentences in train = 2074
Total loss for epoch 13: 3351.908421
validation loss after epoch 13 : 989.384499
	Epoch 14....
Epoch has taken 0:01:51.223841
Number of used sentences in train = 2074
Total loss for epoch 14: 3326.099017
validation loss after epoch 14 : 992.750070
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1133, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.183603
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1320.459221
	Epoch 1....
Epoch has taken 0:00:11.108838
Number of used sentences in train = 231
Total loss for epoch 1: 737.923049
	Epoch 2....
Epoch has taken 0:00:11.121682
Number of used sentences in train = 231
Total loss for epoch 2: 580.732031
	Epoch 3....
Epoch has taken 0:00:11.113738
Number of used sentences in train = 231
Total loss for epoch 3: 481.312083
	Epoch 4....
Epoch has taken 0:00:11.116649
Number of used sentences in train = 231
Total loss for epoch 4: 417.293190
	Epoch 5....
Epoch has taken 0:00:11.108525
Number of used sentences in train = 231
Total loss for epoch 5: 397.591656
	Epoch 6....
Epoch has taken 0:00:11.112558
Number of used sentences in train = 231
Total loss for epoch 6: 387.389865
	Epoch 7....
Epoch has taken 0:00:11.129284
Number of used sentences in train = 231
Total loss for epoch 7: 383.615834
	Epoch 8....
Epoch has taken 0:00:11.141858
Number of used sentences in train = 231
Total loss for epoch 8: 378.919189
	Epoch 9....
Epoch has taken 0:00:11.106536
Number of used sentences in train = 231
Total loss for epoch 9: 375.063050
	Epoch 10....
Epoch has taken 0:00:11.132271
Number of used sentences in train = 231
Total loss for epoch 10: 373.234534
	Epoch 11....
Epoch has taken 0:00:11.114294
Number of used sentences in train = 231
Total loss for epoch 11: 371.094407
	Epoch 12....
Epoch has taken 0:00:11.118077
Number of used sentences in train = 231
Total loss for epoch 12: 369.967850
	Epoch 13....
Epoch has taken 0:00:11.116275
Number of used sentences in train = 231
Total loss for epoch 13: 367.896441
	Epoch 14....
Epoch has taken 0:00:11.131963
Number of used sentences in train = 231
Total loss for epoch 14: 363.225983
Epoch has taken 0:00:11.114023

==================================================================================================
	Training time : 0:32:04.732611
==================================================================================================
	Identification : 0.457

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 41)
  (w_embeddings): Embedding(1202, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 21715.174322
validation loss after epoch 0 : 1590.320268
	Epoch 1....
Epoch has taken 0:03:33.801935
Number of used sentences in train = 3226
Total loss for epoch 1: 13055.854053
validation loss after epoch 1 : 1435.375122
	Epoch 2....
Epoch has taken 0:03:32.422919
Number of used sentences in train = 3226
Total loss for epoch 2: 11825.898516
validation loss after epoch 2 : 1435.724371
	Epoch 3....
Epoch has taken 0:03:32.252576
Number of used sentences in train = 3226
Total loss for epoch 3: 10897.178576
validation loss after epoch 3 : 1541.821619
	Epoch 4....
Epoch has taken 0:03:33.844498
Number of used sentences in train = 3226
Total loss for epoch 4: 10124.354015
validation loss after epoch 4 : 1507.600319
	Epoch 5....
Epoch has taken 0:03:34.395479
Number of used sentences in train = 3226
Total loss for epoch 5: 9502.473886
validation loss after epoch 5 : 1549.841105
	Epoch 6....
Epoch has taken 0:04:01.584867
Number of used sentences in train = 3226
Total loss for epoch 6: 9058.643070
validation loss after epoch 6 : 1563.372449
	Epoch 7....
Epoch has taken 0:03:54.208724
Number of used sentences in train = 3226
Total loss for epoch 7: 8753.474376
validation loss after epoch 7 : 1630.665374
	Epoch 8....
Epoch has taken 0:03:54.220901
Number of used sentences in train = 3226
Total loss for epoch 8: 8435.400297
validation loss after epoch 8 : 1726.380853
	Epoch 9....
Epoch has taken 0:03:56.178393
Number of used sentences in train = 3226
Total loss for epoch 9: 8124.529711
validation loss after epoch 9 : 1768.497091
	Epoch 10....
Epoch has taken 0:03:56.887356
Number of used sentences in train = 3226
Total loss for epoch 10: 7914.474136
validation loss after epoch 10 : 1736.009900
	Epoch 11....
Epoch has taken 0:03:59.307420
Number of used sentences in train = 3226
Total loss for epoch 11: 7713.623100
validation loss after epoch 11 : 1823.551563
	Epoch 12....
Epoch has taken 0:03:56.620905
Number of used sentences in train = 3226
Total loss for epoch 12: 7546.136025
validation loss after epoch 12 : 1851.626451
	Epoch 13....
Epoch has taken 0:03:54.476436
Number of used sentences in train = 3226
Total loss for epoch 13: 7391.712834
validation loss after epoch 13 : 1951.034977
	Epoch 14....
Epoch has taken 0:03:54.310053
Number of used sentences in train = 3226
Total loss for epoch 14: 7274.525652
validation loss after epoch 14 : 1971.884305
	TransitionClassifier(
  (p_embeddings): Embedding(13, 41)
  (w_embeddings): Embedding(1202, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.830362
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2321.258056
	Epoch 1....
Epoch has taken 0:00:24.105639
Number of used sentences in train = 359
Total loss for epoch 1: 1444.419328
	Epoch 2....
Epoch has taken 0:00:24.097445
Number of used sentences in train = 359
Total loss for epoch 2: 1200.335899
	Epoch 3....
Epoch has taken 0:00:24.053468
Number of used sentences in train = 359
Total loss for epoch 3: 1093.130626
	Epoch 4....
Epoch has taken 0:00:24.056369
Number of used sentences in train = 359
Total loss for epoch 4: 1029.420892
	Epoch 5....
Epoch has taken 0:00:20.942655
Number of used sentences in train = 359
Total loss for epoch 5: 979.857757
	Epoch 6....
Epoch has taken 0:00:20.845674
Number of used sentences in train = 359
Total loss for epoch 6: 943.912107
	Epoch 7....
Epoch has taken 0:00:20.850398
Number of used sentences in train = 359
Total loss for epoch 7: 912.547587
	Epoch 8....
Epoch has taken 0:00:20.858667
Number of used sentences in train = 359
Total loss for epoch 8: 874.144195
	Epoch 9....
Epoch has taken 0:00:20.854250
Number of used sentences in train = 359
Total loss for epoch 9: 845.124384
	Epoch 10....
Epoch has taken 0:00:20.865106
Number of used sentences in train = 359
Total loss for epoch 10: 823.797671
	Epoch 11....
Epoch has taken 0:00:20.866828
Number of used sentences in train = 359
Total loss for epoch 11: 809.737427
	Epoch 12....
Epoch has taken 0:00:20.840158
Number of used sentences in train = 359
Total loss for epoch 12: 788.842925
	Epoch 13....
Epoch has taken 0:00:20.861674
Number of used sentences in train = 359
Total loss for epoch 13: 772.920722
	Epoch 14....
Epoch has taken 0:00:20.845465
Number of used sentences in train = 359
Total loss for epoch 14: 760.771912
Epoch has taken 0:00:20.845395

==================================================================================================
	Training time : 1:02:24.772216
==================================================================================================
	Identification : 0.502

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 33, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 16, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 95, 'lstmDropout': 0.19, 'denseActivation': 'tanh', 'wordDim': 113, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1882, 113)
  (lstm): LSTM(129, 95, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15958.095736
validation loss after epoch 0 : 1008.163401
	Epoch 1....
Epoch has taken 0:02:52.618002
Number of used sentences in train = 2811
Total loss for epoch 1: 9400.410779
validation loss after epoch 1 : 950.605511
	Epoch 2....
Epoch has taken 0:02:52.758743
Number of used sentences in train = 2811
Total loss for epoch 2: 8532.790407
validation loss after epoch 2 : 921.977062
	Epoch 3....
Epoch has taken 0:03:18.667806
Number of used sentences in train = 2811
Total loss for epoch 3: 7742.735805
validation loss after epoch 3 : 880.431775
	Epoch 4....
Epoch has taken 0:02:50.928006
Number of used sentences in train = 2811
Total loss for epoch 4: 7131.826794
validation loss after epoch 4 : 868.815349
	Epoch 5....
Epoch has taken 0:02:52.327533
Number of used sentences in train = 2811
Total loss for epoch 5: 6754.345191
validation loss after epoch 5 : 898.927386
	Epoch 6....
Epoch has taken 0:02:52.737601
Number of used sentences in train = 2811
Total loss for epoch 6: 6411.394365
validation loss after epoch 6 : 892.354960
	Epoch 7....
Epoch has taken 0:03:18.658769
Number of used sentences in train = 2811
Total loss for epoch 7: 6068.689746
validation loss after epoch 7 : 924.304844
	Epoch 8....
Epoch has taken 0:02:52.244344
Number of used sentences in train = 2811
Total loss for epoch 8: 5822.943445
validation loss after epoch 8 : 903.898354
	Epoch 9....
Epoch has taken 0:02:51.011720
Number of used sentences in train = 2811
Total loss for epoch 9: 5729.142597
validation loss after epoch 9 : 958.646027
	Epoch 10....
Epoch has taken 0:02:52.460004
Number of used sentences in train = 2811
Total loss for epoch 10: 5617.627654
validation loss after epoch 10 : 889.847871
	Epoch 11....
Epoch has taken 0:03:18.620401
Number of used sentences in train = 2811
Total loss for epoch 11: 5436.128360
validation loss after epoch 11 : 983.337180
	Epoch 12....
Epoch has taken 0:02:52.610824
Number of used sentences in train = 2811
Total loss for epoch 12: 5302.825120
validation loss after epoch 12 : 963.605353
	Epoch 13....
Epoch has taken 0:02:50.884039
Number of used sentences in train = 2811
Total loss for epoch 13: 5256.000367
validation loss after epoch 13 : 960.415424
	Epoch 14....
Epoch has taken 0:02:51.079547
Number of used sentences in train = 2811
Total loss for epoch 14: 5189.148271
validation loss after epoch 14 : 974.672196
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1882, 113)
  (lstm): LSTM(129, 95, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.911031
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1859.600020
	Epoch 1....
Epoch has taken 0:00:20.936396
Number of used sentences in train = 313
Total loss for epoch 1: 893.629336
	Epoch 2....
Epoch has taken 0:00:20.925059
Number of used sentences in train = 313
Total loss for epoch 2: 791.541518
	Epoch 3....
Epoch has taken 0:00:20.918262
Number of used sentences in train = 313
Total loss for epoch 3: 676.523849
	Epoch 4....
Epoch has taken 0:00:20.924084
Number of used sentences in train = 313
Total loss for epoch 4: 589.356153
	Epoch 5....
Epoch has taken 0:00:20.928320
Number of used sentences in train = 313
Total loss for epoch 5: 585.547030
	Epoch 6....
Epoch has taken 0:00:20.908479
Number of used sentences in train = 313
Total loss for epoch 6: 561.336191
	Epoch 7....
Epoch has taken 0:00:20.908948
Number of used sentences in train = 313
Total loss for epoch 7: 554.558037
	Epoch 8....
Epoch has taken 0:00:20.930978
Number of used sentences in train = 313
Total loss for epoch 8: 542.228082
	Epoch 9....
Epoch has taken 0:00:20.923077
Number of used sentences in train = 313
Total loss for epoch 9: 540.278791
	Epoch 10....
Epoch has taken 0:00:20.935274
Number of used sentences in train = 313
Total loss for epoch 10: 539.073103
	Epoch 11....
Epoch has taken 0:00:20.903633
Number of used sentences in train = 313
Total loss for epoch 11: 537.427063
	Epoch 12....
Epoch has taken 0:00:20.922214
Number of used sentences in train = 313
Total loss for epoch 12: 530.983900
	Epoch 13....
Epoch has taken 0:00:20.921364
Number of used sentences in train = 313
Total loss for epoch 13: 532.598839
	Epoch 14....
Epoch has taken 0:00:20.960200
Number of used sentences in train = 313
Total loss for epoch 14: 531.767930
Epoch has taken 0:00:20.950725

==================================================================================================
	Training time : 0:49:34.921646
==================================================================================================
	Identification : 0.48

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1680, 113)
  (lstm): LSTM(129, 95, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 14281.994911
validation loss after epoch 0 : 778.641004
	Epoch 1....
Epoch has taken 0:01:56.879527
Number of used sentences in train = 2074
Total loss for epoch 1: 6054.768498
validation loss after epoch 1 : 634.203004
	Epoch 2....
Epoch has taken 0:01:56.957029
Number of used sentences in train = 2074
Total loss for epoch 2: 4908.536497
validation loss after epoch 2 : 635.336340
	Epoch 3....
Epoch has taken 0:01:57.117072
Number of used sentences in train = 2074
Total loss for epoch 3: 4241.513495
validation loss after epoch 3 : 662.407905
	Epoch 4....
Epoch has taken 0:01:58.092495
Number of used sentences in train = 2074
Total loss for epoch 4: 3847.608244
validation loss after epoch 4 : 803.102280
	Epoch 5....
Epoch has taken 0:02:15.881387
Number of used sentences in train = 2074
Total loss for epoch 5: 3645.318117
validation loss after epoch 5 : 708.384918
	Epoch 6....
Epoch has taken 0:01:58.231548
Number of used sentences in train = 2074
Total loss for epoch 6: 3481.563428
validation loss after epoch 6 : 763.639399
	Epoch 7....
Epoch has taken 0:01:57.971129
Number of used sentences in train = 2074
Total loss for epoch 7: 3381.241394
validation loss after epoch 7 : 814.042773
	Epoch 8....
Epoch has taken 0:01:57.258403
Number of used sentences in train = 2074
Total loss for epoch 8: 3309.463888
validation loss after epoch 8 : 783.081793
	Epoch 9....
Epoch has taken 0:01:58.117524
Number of used sentences in train = 2074
Total loss for epoch 9: 3265.275801
validation loss after epoch 9 : 843.275571
	Epoch 10....
Epoch has taken 0:01:58.380038
Number of used sentences in train = 2074
Total loss for epoch 10: 3250.809321
validation loss after epoch 10 : 866.863318
	Epoch 11....
Epoch has taken 0:02:16.023107
Number of used sentences in train = 2074
Total loss for epoch 11: 3211.213115
validation loss after epoch 11 : 869.732894
	Epoch 12....
Epoch has taken 0:01:58.094115
Number of used sentences in train = 2074
Total loss for epoch 12: 3223.996736
validation loss after epoch 12 : 889.701328
	Epoch 13....
Epoch has taken 0:01:56.697878
Number of used sentences in train = 2074
Total loss for epoch 13: 3202.685666
validation loss after epoch 13 : 898.625028
	Epoch 14....
Epoch has taken 0:01:57.013622
Number of used sentences in train = 2074
Total loss for epoch 14: 3195.167748
validation loss after epoch 14 : 928.102474
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1680, 113)
  (lstm): LSTM(129, 95, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.093125
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1470.861701
	Epoch 1....
Epoch has taken 0:00:12.066237
Number of used sentences in train = 231
Total loss for epoch 1: 787.474934
	Epoch 2....
Epoch has taken 0:00:12.054782
Number of used sentences in train = 231
Total loss for epoch 2: 508.205524
	Epoch 3....
Epoch has taken 0:00:12.076129
Number of used sentences in train = 231
Total loss for epoch 3: 443.096343
	Epoch 4....
Epoch has taken 0:00:12.074146
Number of used sentences in train = 231
Total loss for epoch 4: 411.333406
	Epoch 5....
Epoch has taken 0:00:12.056854
Number of used sentences in train = 231
Total loss for epoch 5: 375.290961
	Epoch 6....
Epoch has taken 0:00:12.065107
Number of used sentences in train = 231
Total loss for epoch 6: 365.592994
	Epoch 7....
Epoch has taken 0:00:12.085011
Number of used sentences in train = 231
Total loss for epoch 7: 358.874488
	Epoch 8....
Epoch has taken 0:00:12.057187
Number of used sentences in train = 231
Total loss for epoch 8: 355.627829
	Epoch 9....
Epoch has taken 0:00:12.072520
Number of used sentences in train = 231
Total loss for epoch 9: 353.473448
	Epoch 10....
Epoch has taken 0:00:12.059299
Number of used sentences in train = 231
Total loss for epoch 10: 351.329800
	Epoch 11....
Epoch has taken 0:00:12.065193
Number of used sentences in train = 231
Total loss for epoch 11: 348.952357
	Epoch 12....
Epoch has taken 0:00:12.053883
Number of used sentences in train = 231
Total loss for epoch 12: 348.743365
	Epoch 13....
Epoch has taken 0:00:12.087899
Number of used sentences in train = 231
Total loss for epoch 13: 347.649511
	Epoch 14....
Epoch has taken 0:00:12.075151
Number of used sentences in train = 231
Total loss for epoch 14: 346.876620
Epoch has taken 0:00:12.101401

==================================================================================================
	Training time : 0:33:01.195912
==================================================================================================
	Identification : 0.33

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(3369, 113)
  (lstm): LSTM(129, 95, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20311.652601
validation loss after epoch 0 : 1264.239551
	Epoch 1....
Epoch has taken 0:03:48.936164
Number of used sentences in train = 3226
Total loss for epoch 1: 10089.267357
validation loss after epoch 1 : 1093.535103
	Epoch 2....
Epoch has taken 0:03:49.103621
Number of used sentences in train = 3226
Total loss for epoch 2: 9034.477758
validation loss after epoch 2 : 1056.078538
	Epoch 3....
Epoch has taken 0:03:48.863120
Number of used sentences in train = 3226
Total loss for epoch 3: 8383.222378
validation loss after epoch 3 : 1083.594729
	Epoch 4....
Epoch has taken 0:03:46.908360
Number of used sentences in train = 3226
Total loss for epoch 4: 7907.956793
validation loss after epoch 4 : 1096.781644
	Epoch 5....
Epoch has taken 0:03:49.333574
Number of used sentences in train = 3226
Total loss for epoch 5: 7517.059949
validation loss after epoch 5 : 1130.968090
	Epoch 6....
Epoch has taken 0:03:48.891117
Number of used sentences in train = 3226
Total loss for epoch 6: 7267.951179
validation loss after epoch 6 : 1199.354640
	Epoch 7....
Epoch has taken 0:03:49.148728
Number of used sentences in train = 3226
Total loss for epoch 7: 7032.156795
validation loss after epoch 7 : 1226.186052
	Epoch 8....
Epoch has taken 0:03:48.803264
Number of used sentences in train = 3226
Total loss for epoch 8: 6855.514306
validation loss after epoch 8 : 1275.702136
	Epoch 9....
Epoch has taken 0:03:46.914164
Number of used sentences in train = 3226
Total loss for epoch 9: 6698.941078
validation loss after epoch 9 : 1291.408654
	Epoch 10....
Epoch has taken 0:03:48.520210
Number of used sentences in train = 3226
Total loss for epoch 10: 6622.986845
validation loss after epoch 10 : 1381.040003
	Epoch 11....
Epoch has taken 0:03:49.328079
Number of used sentences in train = 3226
Total loss for epoch 11: 6539.341500
validation loss after epoch 11 : 1297.716137
	Epoch 12....
Epoch has taken 0:04:23.553318
Number of used sentences in train = 3226
Total loss for epoch 12: 6458.793393
validation loss after epoch 12 : 1378.803453
	Epoch 13....
Epoch has taken 0:03:49.050587
Number of used sentences in train = 3226
Total loss for epoch 13: 6448.398952
validation loss after epoch 13 : 1356.188312
	Epoch 14....
Epoch has taken 0:03:46.780442
Number of used sentences in train = 3226
Total loss for epoch 14: 6401.035671
validation loss after epoch 14 : 1344.666119
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(3369, 113)
  (lstm): LSTM(129, 95, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.217638
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1910.641744
	Epoch 1....
Epoch has taken 0:00:22.429798
Number of used sentences in train = 359
Total loss for epoch 1: 1071.758708
	Epoch 2....
Epoch has taken 0:00:22.426655
Number of used sentences in train = 359
Total loss for epoch 2: 932.666163
	Epoch 3....
Epoch has taken 0:00:22.411122
Number of used sentences in train = 359
Total loss for epoch 3: 824.980702
	Epoch 4....
Epoch has taken 0:00:22.433062
Number of used sentences in train = 359
Total loss for epoch 4: 775.806637
	Epoch 5....
Epoch has taken 0:00:22.426194
Number of used sentences in train = 359
Total loss for epoch 5: 739.969259
	Epoch 6....
Epoch has taken 0:00:22.422913
Number of used sentences in train = 359
Total loss for epoch 6: 720.763710
	Epoch 7....
Epoch has taken 0:00:22.414643
Number of used sentences in train = 359
Total loss for epoch 7: 713.877689
	Epoch 8....
Epoch has taken 0:00:22.445820
Number of used sentences in train = 359
Total loss for epoch 8: 711.264972
	Epoch 9....
Epoch has taken 0:00:22.435003
Number of used sentences in train = 359
Total loss for epoch 9: 706.397096
	Epoch 10....
Epoch has taken 0:00:22.443975
Number of used sentences in train = 359
Total loss for epoch 10: 690.778609
	Epoch 11....
Epoch has taken 0:00:22.452516
Number of used sentences in train = 359
Total loss for epoch 11: 703.291636
	Epoch 12....
Epoch has taken 0:00:22.442296
Number of used sentences in train = 359
Total loss for epoch 12: 680.843394
	Epoch 13....
Epoch has taken 0:00:22.441355
Number of used sentences in train = 359
Total loss for epoch 13: 686.952410
	Epoch 14....
Epoch has taken 0:00:22.455239
Number of used sentences in train = 359
Total loss for epoch 14: 683.497857
Epoch has taken 0:00:22.434816

==================================================================================================
	Training time : 1:03:19.528262
==================================================================================================
	Identification : 0.321

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 13, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 35, 'lstmDropout': 0.31, 'denseActivation': 'tanh', 'wordDim': 211, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 211)
  (lstm): LSTM(228, 35, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=560, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12541.865897
validation loss after epoch 0 : 957.026572
	Epoch 1....
Epoch has taken 0:02:51.162358
Number of used sentences in train = 2811
Total loss for epoch 1: 8065.500176
validation loss after epoch 1 : 943.145858
	Epoch 2....
Epoch has taken 0:02:51.107497
Number of used sentences in train = 2811
Total loss for epoch 2: 7088.970521
validation loss after epoch 2 : 919.529360
	Epoch 3....
Epoch has taken 0:02:49.917445
Number of used sentences in train = 2811
Total loss for epoch 3: 6523.522895
validation loss after epoch 3 : 928.146273
	Epoch 4....
Epoch has taken 0:02:51.275555
Number of used sentences in train = 2811
Total loss for epoch 4: 6147.897473
validation loss after epoch 4 : 945.421835
	Epoch 5....
Epoch has taken 0:02:49.923229
Number of used sentences in train = 2811
Total loss for epoch 5: 5918.918997
validation loss after epoch 5 : 946.816784
	Epoch 6....
Epoch has taken 0:02:50.819746
Number of used sentences in train = 2811
Total loss for epoch 6: 5759.213739
validation loss after epoch 6 : 938.894228
	Epoch 7....
Epoch has taken 0:02:51.792226
Number of used sentences in train = 2811
Total loss for epoch 7: 5594.320479
validation loss after epoch 7 : 949.052257
	Epoch 8....
Epoch has taken 0:03:17.593784
Number of used sentences in train = 2811
Total loss for epoch 8: 5486.630527
validation loss after epoch 8 : 959.511096
	Epoch 9....
Epoch has taken 0:02:51.236636
Number of used sentences in train = 2811
Total loss for epoch 9: 5358.061284
validation loss after epoch 9 : 991.641903
	Epoch 10....
Epoch has taken 0:02:49.976121
Number of used sentences in train = 2811
Total loss for epoch 10: 5286.938254
validation loss after epoch 10 : 1001.791693
	Epoch 11....
Epoch has taken 0:02:51.205359
Number of used sentences in train = 2811
Total loss for epoch 11: 5207.122245
validation loss after epoch 11 : 997.138624
	Epoch 12....
Epoch has taken 0:02:57.288365
Number of used sentences in train = 2811
Total loss for epoch 12: 5177.295759
validation loss after epoch 12 : 1016.091313
	Epoch 13....
Epoch has taken 0:02:51.584415
Number of used sentences in train = 2811
Total loss for epoch 13: 5051.367574
validation loss after epoch 13 : 1013.546455
	Epoch 14....
Epoch has taken 0:02:49.892629
Number of used sentences in train = 2811
Total loss for epoch 14: 5024.864930
validation loss after epoch 14 : 1000.883619
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 211)
  (lstm): LSTM(228, 35, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=560, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.058310
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1697.022798
	Epoch 1....
Epoch has taken 0:00:18.014523
Number of used sentences in train = 313
Total loss for epoch 1: 774.275633
	Epoch 2....
Epoch has taken 0:00:18.016841
Number of used sentences in train = 313
Total loss for epoch 2: 696.454916
	Epoch 3....
Epoch has taken 0:00:18.032968
Number of used sentences in train = 313
Total loss for epoch 3: 606.190523
	Epoch 4....
Epoch has taken 0:00:18.001980
Number of used sentences in train = 313
Total loss for epoch 4: 585.541118
	Epoch 5....
Epoch has taken 0:00:18.025283
Number of used sentences in train = 313
Total loss for epoch 5: 569.672807
	Epoch 6....
Epoch has taken 0:00:18.040395
Number of used sentences in train = 313
Total loss for epoch 6: 566.481982
	Epoch 7....
Epoch has taken 0:00:18.176897
Number of used sentences in train = 313
Total loss for epoch 7: 563.301593
	Epoch 8....
Epoch has taken 0:00:18.146825
Number of used sentences in train = 313
Total loss for epoch 8: 566.207011
	Epoch 9....
Epoch has taken 0:00:18.166138
Number of used sentences in train = 313
Total loss for epoch 9: 550.303716
	Epoch 10....
Epoch has taken 0:00:18.136736
Number of used sentences in train = 313
Total loss for epoch 10: 549.011479
	Epoch 11....
Epoch has taken 0:00:18.168657
Number of used sentences in train = 313
Total loss for epoch 11: 537.013982
	Epoch 12....
Epoch has taken 0:00:18.153756
Number of used sentences in train = 313
Total loss for epoch 12: 545.931785
	Epoch 13....
Epoch has taken 0:00:18.136790
Number of used sentences in train = 313
Total loss for epoch 13: 536.783124
	Epoch 14....
Epoch has taken 0:00:18.177651
Number of used sentences in train = 313
Total loss for epoch 14: 531.904432
Epoch has taken 0:00:18.181268

==================================================================================================
	Training time : 0:47:46.906869
==================================================================================================
	Identification : 0.453

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 211)
  (lstm): LSTM(228, 35, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=560, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9005.459632
validation loss after epoch 0 : 699.079393
	Epoch 1....
Epoch has taken 0:02:15.294440
Number of used sentences in train = 2074
Total loss for epoch 1: 5712.211565
validation loss after epoch 1 : 676.248996
	Epoch 2....
Epoch has taken 0:02:15.303717
Number of used sentences in train = 2074
Total loss for epoch 2: 4861.009881
validation loss after epoch 2 : 665.240483
	Epoch 3....
Epoch has taken 0:01:57.581092
Number of used sentences in train = 2074
Total loss for epoch 3: 4462.025655
validation loss after epoch 3 : 654.487866
	Epoch 4....
Epoch has taken 0:01:56.466266
Number of used sentences in train = 2074
Total loss for epoch 4: 4143.528832
validation loss after epoch 4 : 707.470472
	Epoch 5....
Epoch has taken 0:01:56.367813
Number of used sentences in train = 2074
Total loss for epoch 5: 3943.953146
validation loss after epoch 5 : 723.232741
	Epoch 6....
Epoch has taken 0:01:57.748723
Number of used sentences in train = 2074
Total loss for epoch 6: 3792.013310
validation loss after epoch 6 : 763.408082
	Epoch 7....
Epoch has taken 0:02:00.577968
Number of used sentences in train = 2074
Total loss for epoch 7: 3727.931254
validation loss after epoch 7 : 737.457659
	Epoch 8....
Epoch has taken 0:01:57.365464
Number of used sentences in train = 2074
Total loss for epoch 8: 3639.564518
validation loss after epoch 8 : 782.148099
	Epoch 9....
Epoch has taken 0:01:56.558029
Number of used sentences in train = 2074
Total loss for epoch 9: 3584.132303
validation loss after epoch 9 : 780.701394
	Epoch 10....
Epoch has taken 0:01:56.926283
Number of used sentences in train = 2074
Total loss for epoch 10: 3536.943087
validation loss after epoch 10 : 790.914683
	Epoch 11....
Epoch has taken 0:01:57.582434
Number of used sentences in train = 2074
Total loss for epoch 11: 3490.345126
validation loss after epoch 11 : 782.362472
	Epoch 12....
Epoch has taken 0:01:57.661664
Number of used sentences in train = 2074
Total loss for epoch 12: 3453.900545
validation loss after epoch 12 : 836.139354
	Epoch 13....
Epoch has taken 0:01:57.644864
Number of used sentences in train = 2074
Total loss for epoch 13: 3460.329412
validation loss after epoch 13 : 821.318902
	Epoch 14....
Epoch has taken 0:01:56.558446
Number of used sentences in train = 2074
Total loss for epoch 14: 3431.903351
validation loss after epoch 14 : 826.713316
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 211)
  (lstm): LSTM(228, 35, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=560, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.511943
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1136.470766
	Epoch 1....
Epoch has taken 0:00:13.752021
Number of used sentences in train = 231
Total loss for epoch 1: 576.391224
	Epoch 2....
Epoch has taken 0:00:13.731191
Number of used sentences in train = 231
Total loss for epoch 2: 472.135318
	Epoch 3....
Epoch has taken 0:00:13.730421
Number of used sentences in train = 231
Total loss for epoch 3: 410.047171
	Epoch 4....
Epoch has taken 0:00:13.734374
Number of used sentences in train = 231
Total loss for epoch 4: 391.611170
	Epoch 5....
Epoch has taken 0:00:13.731834
Number of used sentences in train = 231
Total loss for epoch 5: 387.328236
	Epoch 6....
Epoch has taken 0:00:13.744995
Number of used sentences in train = 231
Total loss for epoch 6: 378.946772
	Epoch 7....
Epoch has taken 0:00:13.728518
Number of used sentences in train = 231
Total loss for epoch 7: 361.095302
	Epoch 8....
Epoch has taken 0:00:13.743928
Number of used sentences in train = 231
Total loss for epoch 8: 361.222537
	Epoch 9....
Epoch has taken 0:00:13.755516
Number of used sentences in train = 231
Total loss for epoch 9: 359.204282
	Epoch 10....
Epoch has taken 0:00:13.727761
Number of used sentences in train = 231
Total loss for epoch 10: 364.967561
	Epoch 11....
Epoch has taken 0:00:13.752785
Number of used sentences in train = 231
Total loss for epoch 11: 358.100412
	Epoch 12....
Epoch has taken 0:00:13.726160
Number of used sentences in train = 231
Total loss for epoch 12: 359.554518
	Epoch 13....
Epoch has taken 0:00:13.750060
Number of used sentences in train = 231
Total loss for epoch 13: 359.149224
	Epoch 14....
Epoch has taken 0:00:13.734684
Number of used sentences in train = 231
Total loss for epoch 14: 354.364893
Epoch has taken 0:00:13.742250

==================================================================================================
	Training time : 0:33:23.572058
==================================================================================================
	Identification : 0.106

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 211)
  (lstm): LSTM(228, 35, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=560, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14986.226015
validation loss after epoch 0 : 1290.670489
	Epoch 1....
Epoch has taken 0:03:45.613816
Number of used sentences in train = 3226
Total loss for epoch 1: 11046.791305
validation loss after epoch 1 : 1264.084528
	Epoch 2....
Epoch has taken 0:03:45.582603
Number of used sentences in train = 3226
Total loss for epoch 2: 10217.591912
validation loss after epoch 2 : 1239.003123
	Epoch 3....
Epoch has taken 0:03:45.908581
Number of used sentences in train = 3226
Total loss for epoch 3: 9639.884896
validation loss after epoch 3 : 1285.246573
	Epoch 4....
Epoch has taken 0:03:48.022381
Number of used sentences in train = 3226
Total loss for epoch 4: 9239.165188
validation loss after epoch 4 : 1331.255073
	Epoch 5....
Epoch has taken 0:03:48.264396
Number of used sentences in train = 3226
Total loss for epoch 5: 8987.996776
validation loss after epoch 5 : 1291.680253
	Epoch 6....
Epoch has taken 0:03:48.104092
Number of used sentences in train = 3226
Total loss for epoch 6: 8667.907104
validation loss after epoch 6 : 1290.649149
	Epoch 7....
Epoch has taken 0:03:45.980802
Number of used sentences in train = 3226
Total loss for epoch 7: 8478.029471
validation loss after epoch 7 : 1333.612727
	Epoch 8....
Epoch has taken 0:03:45.868357
Number of used sentences in train = 3226
Total loss for epoch 8: 8333.235305
validation loss after epoch 8 : 1352.092252
	Epoch 9....
Epoch has taken 0:03:47.992919
Number of used sentences in train = 3226
Total loss for epoch 9: 8212.450187
validation loss after epoch 9 : 1409.192703
	Epoch 10....
Epoch has taken 0:03:47.723945
Number of used sentences in train = 3226
Total loss for epoch 10: 8034.211373
validation loss after epoch 10 : 1443.705084
	Epoch 11....
Epoch has taken 0:03:47.617952
Number of used sentences in train = 3226
Total loss for epoch 11: 7984.538371
validation loss after epoch 11 : 1397.124476
	Epoch 12....
Epoch has taken 0:03:46.269156
Number of used sentences in train = 3226
Total loss for epoch 12: 7869.852664
validation loss after epoch 12 : 1467.939425
	Epoch 13....
Epoch has taken 0:03:48.140206
Number of used sentences in train = 3226
Total loss for epoch 13: 7733.329211
validation loss after epoch 13 : 1485.561850
	Epoch 14....
Epoch has taken 0:04:22.554559
Number of used sentences in train = 3226
Total loss for epoch 14: 7630.149858
validation loss after epoch 14 : 1427.375688
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 211)
  (lstm): LSTM(228, 35, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=560, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.612751
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1700.572846
	Epoch 1....
Epoch has taken 0:00:22.148906
Number of used sentences in train = 359
Total loss for epoch 1: 1192.355537
	Epoch 2....
Epoch has taken 0:00:22.139309
Number of used sentences in train = 359
Total loss for epoch 2: 1040.084617
	Epoch 3....
Epoch has taken 0:00:22.155213
Number of used sentences in train = 359
Total loss for epoch 3: 960.913605
	Epoch 4....
Epoch has taken 0:00:22.183344
Number of used sentences in train = 359
Total loss for epoch 4: 912.469677
	Epoch 5....
Epoch has taken 0:00:22.177356
Number of used sentences in train = 359
Total loss for epoch 5: 861.853211
	Epoch 6....
Epoch has taken 0:00:22.156532
Number of used sentences in train = 359
Total loss for epoch 6: 826.446307
	Epoch 7....
Epoch has taken 0:00:22.177086
Number of used sentences in train = 359
Total loss for epoch 7: 786.666014
	Epoch 8....
Epoch has taken 0:00:22.181855
Number of used sentences in train = 359
Total loss for epoch 8: 788.225204
	Epoch 9....
Epoch has taken 0:00:22.185956
Number of used sentences in train = 359
Total loss for epoch 9: 813.654483
	Epoch 10....
Epoch has taken 0:00:22.180463
Number of used sentences in train = 359
Total loss for epoch 10: 775.912975
	Epoch 11....
Epoch has taken 0:00:22.159868
Number of used sentences in train = 359
Total loss for epoch 11: 769.360317
	Epoch 12....
Epoch has taken 0:00:22.153819
Number of used sentences in train = 359
Total loss for epoch 12: 750.941091
	Epoch 13....
Epoch has taken 0:00:22.150163
Number of used sentences in train = 359
Total loss for epoch 13: 742.220931
	Epoch 14....
Epoch has taken 0:00:22.157342
Number of used sentences in train = 359
Total loss for epoch 14: 732.703974
Epoch has taken 0:00:22.164120

==================================================================================================
	Training time : 1:02:55.375203
==================================================================================================
	Identification : 0.514

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 72, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 72, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(9210, 72)
  (lstm): LSTM(89, 72, bidirectional=True)
  (linear1): Linear(in_features=1152, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14452.657573
validation loss after epoch 0 : 1169.398468
	Epoch 1....
Epoch has taken 0:03:03.773749
Number of used sentences in train = 2811
Total loss for epoch 1: 9202.297529
validation loss after epoch 1 : 1117.869484
	Epoch 2....
Epoch has taken 0:02:41.013380
Number of used sentences in train = 2811
Total loss for epoch 2: 7367.088421
validation loss after epoch 2 : 1077.601473
	Epoch 3....
Epoch has taken 0:02:41.116100
Number of used sentences in train = 2811
Total loss for epoch 3: 6248.529522
validation loss after epoch 3 : 1204.592983
	Epoch 4....
Epoch has taken 0:02:39.382201
Number of used sentences in train = 2811
Total loss for epoch 4: 5625.215993
validation loss after epoch 4 : 1242.638344
	Epoch 5....
Epoch has taken 0:02:39.538814
Number of used sentences in train = 2811
Total loss for epoch 5: 5239.574846
validation loss after epoch 5 : 1309.801459
	Epoch 6....
Epoch has taken 0:02:41.448408
Number of used sentences in train = 2811
Total loss for epoch 6: 4991.023385
validation loss after epoch 6 : 1335.559490
	Epoch 7....
Epoch has taken 0:03:06.277390
Number of used sentences in train = 2811
Total loss for epoch 7: 4878.199985
validation loss after epoch 7 : 1377.641585
	Epoch 8....
Epoch has taken 0:02:41.274225
Number of used sentences in train = 2811
Total loss for epoch 8: 4805.178836
validation loss after epoch 8 : 1402.411280
	Epoch 9....
Epoch has taken 0:02:39.683232
Number of used sentences in train = 2811
Total loss for epoch 9: 4748.290253
validation loss after epoch 9 : 1421.546281
	Epoch 10....
Epoch has taken 0:02:39.710261
Number of used sentences in train = 2811
Total loss for epoch 10: 4721.416862
validation loss after epoch 10 : 1448.703922
	Epoch 11....
Epoch has taken 0:02:46.744049
Number of used sentences in train = 2811
Total loss for epoch 11: 4677.943656
validation loss after epoch 11 : 1499.930985
	Epoch 12....
Epoch has taken 0:02:40.864341
Number of used sentences in train = 2811
Total loss for epoch 12: 4658.517104
validation loss after epoch 12 : 1505.266273
	Epoch 13....
Epoch has taken 0:02:39.686605
Number of used sentences in train = 2811
Total loss for epoch 13: 4631.645986
validation loss after epoch 13 : 1514.599657
	Epoch 14....
Epoch has taken 0:02:41.176170
Number of used sentences in train = 2811
Total loss for epoch 14: 4614.987825
validation loss after epoch 14 : 1528.818709
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(9210, 72)
  (lstm): LSTM(89, 72, bidirectional=True)
  (linear1): Linear(in_features=1152, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:06.267138
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1580.191554
	Epoch 1....
Epoch has taken 0:00:17.031051
Number of used sentences in train = 313
Total loss for epoch 1: 862.305618
	Epoch 2....
Epoch has taken 0:00:17.005657
Number of used sentences in train = 313
Total loss for epoch 2: 667.842870
	Epoch 3....
Epoch has taken 0:00:17.023850
Number of used sentences in train = 313
Total loss for epoch 3: 594.275748
	Epoch 4....
Epoch has taken 0:00:17.000356
Number of used sentences in train = 313
Total loss for epoch 4: 549.000441
	Epoch 5....
Epoch has taken 0:00:17.013526
Number of used sentences in train = 313
Total loss for epoch 5: 537.625691
	Epoch 6....
Epoch has taken 0:00:17.013247
Number of used sentences in train = 313
Total loss for epoch 6: 533.950221
	Epoch 7....
Epoch has taken 0:00:17.023606
Number of used sentences in train = 313
Total loss for epoch 7: 529.578795
	Epoch 8....
Epoch has taken 0:00:17.038269
Number of used sentences in train = 313
Total loss for epoch 8: 527.845163
	Epoch 9....
Epoch has taken 0:00:17.021315
Number of used sentences in train = 313
Total loss for epoch 9: 527.004464
	Epoch 10....
Epoch has taken 0:00:17.000909
Number of used sentences in train = 313
Total loss for epoch 10: 526.333807
	Epoch 11....
Epoch has taken 0:00:16.856234
Number of used sentences in train = 313
Total loss for epoch 11: 525.924706
	Epoch 12....
Epoch has taken 0:00:16.890774
Number of used sentences in train = 313
Total loss for epoch 12: 525.716590
	Epoch 13....
Epoch has taken 0:00:16.863824
Number of used sentences in train = 313
Total loss for epoch 13: 525.360152
	Epoch 14....
Epoch has taken 0:00:16.852876
Number of used sentences in train = 313
Total loss for epoch 14: 525.148398
Epoch has taken 0:00:16.875499

==================================================================================================
	Training time : 0:45:42.976083
==================================================================================================
	Identification : 0.486

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(7120, 72)
  (lstm): LSTM(89, 72, bidirectional=True)
  (linear1): Linear(in_features=1152, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10841.325536
validation loss after epoch 0 : 1004.942111
	Epoch 1....
Epoch has taken 0:01:49.646315
Number of used sentences in train = 2074
Total loss for epoch 1: 6628.676730
validation loss after epoch 1 : 909.257615
	Epoch 2....
Epoch has taken 0:01:49.669044
Number of used sentences in train = 2074
Total loss for epoch 2: 5174.131025
validation loss after epoch 2 : 955.059915
	Epoch 3....
Epoch has taken 0:01:49.732699
Number of used sentences in train = 2074
Total loss for epoch 3: 4260.050779
validation loss after epoch 3 : 1079.513054
	Epoch 4....
Epoch has taken 0:01:49.935065
Number of used sentences in train = 2074
Total loss for epoch 4: 3809.445862
validation loss after epoch 4 : 1189.439919
	Epoch 5....
Epoch has taken 0:01:48.720583
Number of used sentences in train = 2074
Total loss for epoch 5: 3597.478844
validation loss after epoch 5 : 1186.101179
	Epoch 6....
Epoch has taken 0:01:48.924111
Number of used sentences in train = 2074
Total loss for epoch 6: 3485.717632
validation loss after epoch 6 : 1205.400885
	Epoch 7....
Epoch has taken 0:01:48.926874
Number of used sentences in train = 2074
Total loss for epoch 7: 3425.930961
validation loss after epoch 7 : 1215.000512
	Epoch 8....
Epoch has taken 0:01:49.928576
Number of used sentences in train = 2074
Total loss for epoch 8: 3382.259673
validation loss after epoch 8 : 1255.295960
	Epoch 9....
Epoch has taken 0:01:50.587183
Number of used sentences in train = 2074
Total loss for epoch 9: 3348.448886
validation loss after epoch 9 : 1258.285432
	Epoch 10....
Epoch has taken 0:02:07.088942
Number of used sentences in train = 2074
Total loss for epoch 10: 3315.290037
validation loss after epoch 10 : 1276.432267
	Epoch 11....
Epoch has taken 0:01:54.452576
Number of used sentences in train = 2074
Total loss for epoch 11: 3302.333461
validation loss after epoch 11 : 1302.209566
	Epoch 12....
Epoch has taken 0:02:00.270511
Number of used sentences in train = 2074
Total loss for epoch 12: 3284.676643
validation loss after epoch 12 : 1301.570890
	Epoch 13....
Epoch has taken 0:02:00.254248
Number of used sentences in train = 2074
Total loss for epoch 13: 3272.437517
validation loss after epoch 13 : 1316.879125
	Epoch 14....
Epoch has taken 0:02:01.292973
Number of used sentences in train = 2074
Total loss for epoch 14: 3261.483549
validation loss after epoch 14 : 1330.215756
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(7120, 72)
  (lstm): LSTM(89, 72, bidirectional=True)
  (linear1): Linear(in_features=1152, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:02.968483
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1496.207067
	Epoch 1....
Epoch has taken 0:00:12.386082
Number of used sentences in train = 231
Total loss for epoch 1: 706.569768
	Epoch 2....
Epoch has taken 0:00:12.397839
Number of used sentences in train = 231
Total loss for epoch 2: 522.274060
	Epoch 3....
Epoch has taken 0:00:12.399148
Number of used sentences in train = 231
Total loss for epoch 3: 425.407527
	Epoch 4....
Epoch has taken 0:00:11.958033
Number of used sentences in train = 231
Total loss for epoch 4: 391.610240
	Epoch 5....
Epoch has taken 0:00:11.166347
Number of used sentences in train = 231
Total loss for epoch 5: 376.224034
	Epoch 6....
Epoch has taken 0:00:11.173368
Number of used sentences in train = 231
Total loss for epoch 6: 370.498260
	Epoch 7....
Epoch has taken 0:00:11.176065
Number of used sentences in train = 231
Total loss for epoch 7: 368.625177
	Epoch 8....
Epoch has taken 0:00:11.177158
Number of used sentences in train = 231
Total loss for epoch 8: 366.970225
	Epoch 9....
Epoch has taken 0:00:11.191068
Number of used sentences in train = 231
Total loss for epoch 9: 359.548524
	Epoch 10....
Epoch has taken 0:00:11.161087
Number of used sentences in train = 231
Total loss for epoch 10: 356.775312
	Epoch 11....
Epoch has taken 0:00:11.191240
Number of used sentences in train = 231
Total loss for epoch 11: 355.381156
	Epoch 12....
Epoch has taken 0:00:11.157044
Number of used sentences in train = 231
Total loss for epoch 12: 354.879792
	Epoch 13....
Epoch has taken 0:00:11.177244
Number of used sentences in train = 231
Total loss for epoch 13: 354.502944
	Epoch 14....
Epoch has taken 0:00:11.153969
Number of used sentences in train = 231
Total loss for epoch 14: 354.198997
Epoch has taken 0:00:11.170613

==================================================================================================
	Training time : 0:31:24.770760
==================================================================================================
	Identification : 0.333

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(18026, 72)
  (lstm): LSTM(89, 72, bidirectional=True)
  (linear1): Linear(in_features=1152, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18765.171425
validation loss after epoch 0 : 1695.596360
	Epoch 1....
Epoch has taken 0:03:36.375737
Number of used sentences in train = 3226
Total loss for epoch 1: 12829.558962
validation loss after epoch 1 : 1763.916380
	Epoch 2....
Epoch has taken 0:03:36.480352
Number of used sentences in train = 3226
Total loss for epoch 2: 10343.631564
validation loss after epoch 2 : 1683.737773
	Epoch 3....
Epoch has taken 0:04:09.684894
Number of used sentences in train = 3226
Total loss for epoch 3: 8661.286687
validation loss after epoch 3 : 1832.917399
	Epoch 4....
Epoch has taken 0:03:35.784544
Number of used sentences in train = 3226
Total loss for epoch 4: 7581.828625
validation loss after epoch 4 : 1943.848149
	Epoch 5....
Epoch has taken 0:03:34.161683
Number of used sentences in train = 3226
Total loss for epoch 5: 6963.986318
validation loss after epoch 5 : 2144.717102
	Epoch 6....
Epoch has taken 0:03:33.995867
Number of used sentences in train = 3226
Total loss for epoch 6: 6631.572836
validation loss after epoch 6 : 2184.690089
	Epoch 7....
Epoch has taken 0:03:57.940801
Number of used sentences in train = 3226
Total loss for epoch 7: 6451.432637
validation loss after epoch 7 : 2391.768474
	Epoch 8....
Epoch has taken 0:03:59.043810
Number of used sentences in train = 3226
Total loss for epoch 8: 6333.494337
validation loss after epoch 8 : 2437.688832
	Epoch 9....
Epoch has taken 0:03:58.787753
Number of used sentences in train = 3226
Total loss for epoch 9: 6278.517922
validation loss after epoch 9 : 2556.976558
	Epoch 10....
Epoch has taken 0:03:57.011415
Number of used sentences in train = 3226
Total loss for epoch 10: 6229.191264
validation loss after epoch 10 : 2616.307815
	Epoch 11....
Epoch has taken 0:03:56.733322
Number of used sentences in train = 3226
Total loss for epoch 11: 6213.122883
validation loss after epoch 11 : 2579.170326
	Epoch 12....
Epoch has taken 0:03:56.791219
Number of used sentences in train = 3226
Total loss for epoch 12: 6200.515620
validation loss after epoch 12 : 2645.564291
	Epoch 13....
Epoch has taken 0:03:44.479801
Number of used sentences in train = 3226
Total loss for epoch 13: 6187.789548
validation loss after epoch 13 : 2708.470003
	Epoch 14....
Epoch has taken 0:03:36.023534
Number of used sentences in train = 3226
Total loss for epoch 14: 6177.151651
validation loss after epoch 14 : 2726.184795
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(18026, 72)
  (lstm): LSTM(89, 72, bidirectional=True)
  (linear1): Linear(in_features=1152, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:36.063670
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2541.150123
	Epoch 1....
Epoch has taken 0:00:20.906650
Number of used sentences in train = 359
Total loss for epoch 1: 1300.841701
	Epoch 2....
Epoch has taken 0:00:20.879266
Number of used sentences in train = 359
Total loss for epoch 2: 958.813768
	Epoch 3....
Epoch has taken 0:00:20.893894
Number of used sentences in train = 359
Total loss for epoch 3: 792.387184
	Epoch 4....
Epoch has taken 0:00:20.946392
Number of used sentences in train = 359
Total loss for epoch 4: 723.507435
	Epoch 5....
Epoch has taken 0:00:20.897739
Number of used sentences in train = 359
Total loss for epoch 5: 691.164135
	Epoch 6....
Epoch has taken 0:00:20.886105
Number of used sentences in train = 359
Total loss for epoch 6: 684.043608
	Epoch 7....
Epoch has taken 0:00:20.898256
Number of used sentences in train = 359
Total loss for epoch 7: 678.230219
	Epoch 8....
Epoch has taken 0:00:20.901364
Number of used sentences in train = 359
Total loss for epoch 8: 676.820298
	Epoch 9....
Epoch has taken 0:00:20.904885
Number of used sentences in train = 359
Total loss for epoch 9: 675.963167
	Epoch 10....
Epoch has taken 0:00:20.909166
Number of used sentences in train = 359
Total loss for epoch 10: 675.313054
	Epoch 11....
Epoch has taken 0:00:20.911992
Number of used sentences in train = 359
Total loss for epoch 11: 674.801803
	Epoch 12....
Epoch has taken 0:00:20.887358
Number of used sentences in train = 359
Total loss for epoch 12: 674.396851
	Epoch 13....
Epoch has taken 0:00:20.855629
Number of used sentences in train = 359
Total loss for epoch 13: 674.055285
	Epoch 14....
Epoch has taken 0:00:20.867484
Number of used sentences in train = 359
Total loss for epoch 14: 673.761222
Epoch has taken 0:00:20.906860

==================================================================================================
	Training time : 1:02:03.507759
==================================================================================================
	Identification : 0.451

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 132, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 34, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 70, 'lstmDropout': 0.25, 'denseActivation': 'tanh', 'wordDim': 161, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(1882, 161)
  (lstm): LSTM(195, 70, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=132, bias=True)
  (linear2): Linear(in_features=132, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11929.906640
validation loss after epoch 0 : 913.833627
	Epoch 1....
Epoch has taken 0:03:06.781463
Number of used sentences in train = 2811
Total loss for epoch 1: 7625.707364
validation loss after epoch 1 : 878.026747
	Epoch 2....
Epoch has taken 0:03:06.789260
Number of used sentences in train = 2811
Total loss for epoch 2: 6787.725329
validation loss after epoch 2 : 879.279975
	Epoch 3....
Epoch has taken 0:02:40.741727
Number of used sentences in train = 2811
Total loss for epoch 3: 6166.002883
validation loss after epoch 3 : 880.714371
	Epoch 4....
Epoch has taken 0:02:40.277898
Number of used sentences in train = 2811
Total loss for epoch 4: 5749.952051
validation loss after epoch 4 : 923.732588
	Epoch 5....
Epoch has taken 0:02:41.760481
Number of used sentences in train = 2811
Total loss for epoch 5: 5508.099482
validation loss after epoch 5 : 965.015781
	Epoch 6....
Epoch has taken 0:02:42.108431
Number of used sentences in train = 2811
Total loss for epoch 6: 5287.888349
validation loss after epoch 6 : 965.562912
	Epoch 7....
Epoch has taken 0:02:40.012572
Number of used sentences in train = 2811
Total loss for epoch 7: 5135.724792
validation loss after epoch 7 : 996.084027
	Epoch 8....
Epoch has taken 0:02:39.971101
Number of used sentences in train = 2811
Total loss for epoch 8: 5019.618127
validation loss after epoch 8 : 1022.129135
	Epoch 9....
Epoch has taken 0:02:41.298129
Number of used sentences in train = 2811
Total loss for epoch 9: 4947.658467
validation loss after epoch 9 : 1039.959230
	Epoch 10....
Epoch has taken 0:02:42.517434
Number of used sentences in train = 2811
Total loss for epoch 10: 4866.248598
validation loss after epoch 10 : 1071.005291
	Epoch 11....
Epoch has taken 0:03:00.475824
