INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11354.017598
validation loss after epoch 0 : 866.454756
	Epoch 1....
Epoch has taken 0:02:52.620407
Number of used sentences in train = 2811
Total loss for epoch 1: 7612.758761
validation loss after epoch 1 : 843.044730
	Epoch 2....
Epoch has taken 0:02:52.013372
Number of used sentences in train = 2811
Total loss for epoch 2: 6858.608082
validation loss after epoch 2 : 803.027593
	Epoch 3....
Epoch has taken 0:02:52.717983
Number of used sentences in train = 2811
Total loss for epoch 3: 6385.488079
validation loss after epoch 3 : 793.367005
	Epoch 4....
Epoch has taken 0:02:53.180184
Number of used sentences in train = 2811
Total loss for epoch 4: 6085.406482
validation loss after epoch 4 : 774.751086
	Epoch 5....
Epoch has taken 0:02:51.381115
Number of used sentences in train = 2811
Total loss for epoch 5: 5739.886095
validation loss after epoch 5 : 838.206632
	Epoch 6....
Epoch has taken 0:02:51.658320
Number of used sentences in train = 2811
Total loss for epoch 6: 5491.828758
validation loss after epoch 6 : 842.585631
	Epoch 7....
Epoch has taken 0:02:53.008979
Number of used sentences in train = 2811
Total loss for epoch 7: 5348.627263
validation loss after epoch 7 : 819.915338
	Epoch 8....
Epoch has taken 0:02:52.609174
Number of used sentences in train = 2811
Total loss for epoch 8: 5185.333820
validation loss after epoch 8 : 845.496372
	Epoch 9....
Epoch has taken 0:02:52.271104
Number of used sentences in train = 2811
Total loss for epoch 9: 5106.997238
validation loss after epoch 9 : 860.032624
	Epoch 10....
Epoch has taken 0:03:10.955764
Number of used sentences in train = 2811
Total loss for epoch 10: 5015.988879
validation loss after epoch 10 : 868.138370
	Epoch 11....
Epoch has taken 0:02:53.283328
Number of used sentences in train = 2811
Total loss for epoch 11: 4903.079279
validation loss after epoch 11 : 933.219282
	Epoch 12....
Epoch has taken 0:03:02.893979
Number of used sentences in train = 2811
Total loss for epoch 12: 4866.431682
validation loss after epoch 12 : 937.839239
	Epoch 13....
Epoch has taken 0:02:53.607405
Number of used sentences in train = 2811
Total loss for epoch 13: 4836.422326
validation loss after epoch 13 : 963.357719
	Epoch 14....
Epoch has taken 0:02:53.957338
Number of used sentences in train = 2811
Total loss for epoch 14: 4807.044559
validation loss after epoch 14 : 982.480989
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:53.115211
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1269.924790
	Epoch 1....
Epoch has taken 0:00:18.285739
Number of used sentences in train = 313
Total loss for epoch 1: 793.844377
	Epoch 2....
Epoch has taken 0:00:18.271983
Number of used sentences in train = 313
Total loss for epoch 2: 716.926477
	Epoch 3....
Epoch has taken 0:00:18.282101
Number of used sentences in train = 313
Total loss for epoch 3: 621.397684
	Epoch 4....
Epoch has taken 0:00:18.287082
Number of used sentences in train = 313
Total loss for epoch 4: 597.966245
	Epoch 5....
Epoch has taken 0:00:18.285082
Number of used sentences in train = 313
Total loss for epoch 5: 571.770360
	Epoch 6....
Epoch has taken 0:00:18.285871
Number of used sentences in train = 313
Total loss for epoch 6: 558.664540
	Epoch 7....
Epoch has taken 0:00:18.287216
Number of used sentences in train = 313
Total loss for epoch 7: 553.820126
	Epoch 8....
Epoch has taken 0:00:18.280895
Number of used sentences in train = 313
Total loss for epoch 8: 544.278940
	Epoch 9....
Epoch has taken 0:00:18.288611
Number of used sentences in train = 313
Total loss for epoch 9: 539.025276
	Epoch 10....
Epoch has taken 0:00:18.285045
Number of used sentences in train = 313
Total loss for epoch 10: 536.994028
	Epoch 11....
Epoch has taken 0:00:18.281715
Number of used sentences in train = 313
Total loss for epoch 11: 537.272140
	Epoch 12....
Epoch has taken 0:00:18.292671
Number of used sentences in train = 313
Total loss for epoch 12: 536.162639
	Epoch 13....
Epoch has taken 0:00:18.307463
Number of used sentences in train = 313
Total loss for epoch 13: 535.663985
	Epoch 14....
Epoch has taken 0:00:18.269385
Number of used sentences in train = 313
Total loss for epoch 14: 535.530124
Epoch has taken 0:00:18.274629

==================================================================================================
	Training time : 0:48:20.025280
==================================================================================================
	Identification : 0.467

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9088.608989
validation loss after epoch 0 : 654.254293
	Epoch 1....
Epoch has taken 0:01:59.326795
Number of used sentences in train = 2074
Total loss for epoch 1: 5451.230651
validation loss after epoch 1 : 649.104341
	Epoch 2....
Epoch has taken 0:01:59.337390
Number of used sentences in train = 2074
Total loss for epoch 2: 4799.612832
validation loss after epoch 2 : 650.005339
	Epoch 3....
Epoch has taken 0:01:59.497226
Number of used sentences in train = 2074
Total loss for epoch 3: 4382.826671
validation loss after epoch 3 : 655.479226
	Epoch 4....
Epoch has taken 0:01:59.634918
Number of used sentences in train = 2074
Total loss for epoch 4: 4005.620282
validation loss after epoch 4 : 688.440311
	Epoch 5....
Epoch has taken 0:01:58.493876
Number of used sentences in train = 2074
Total loss for epoch 5: 3761.346738
validation loss after epoch 5 : 675.778915
	Epoch 6....
Epoch has taken 0:01:58.615950
Number of used sentences in train = 2074
Total loss for epoch 6: 3553.358722
validation loss after epoch 6 : 748.266921
	Epoch 7....
Epoch has taken 0:01:59.655008
Number of used sentences in train = 2074
Total loss for epoch 7: 3448.443132
validation loss after epoch 7 : 767.692205
	Epoch 8....
Epoch has taken 0:01:59.654308
Number of used sentences in train = 2074
Total loss for epoch 8: 3391.126084
validation loss after epoch 8 : 849.193012
	Epoch 9....
Epoch has taken 0:01:59.695284
Number of used sentences in train = 2074
Total loss for epoch 9: 3345.618516
validation loss after epoch 9 : 779.055673
	Epoch 10....
Epoch has taken 0:01:58.649955
Number of used sentences in train = 2074
Total loss for epoch 10: 3307.953354
validation loss after epoch 10 : 811.269098
	Epoch 11....
Epoch has taken 0:01:58.573577
Number of used sentences in train = 2074
Total loss for epoch 11: 3277.707860
validation loss after epoch 11 : 793.321617
	Epoch 12....
Epoch has taken 0:01:59.726938
Number of used sentences in train = 2074
Total loss for epoch 12: 3273.514832
validation loss after epoch 12 : 820.319923
	Epoch 13....
Epoch has taken 0:01:59.524394
Number of used sentences in train = 2074
Total loss for epoch 13: 3228.031591
validation loss after epoch 13 : 864.134277
	Epoch 14....
Epoch has taken 0:01:58.681589
Number of used sentences in train = 2074
Total loss for epoch 14: 3220.951910
validation loss after epoch 14 : 977.097102
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:58.613452
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1405.465862
	Epoch 1....
Epoch has taken 0:00:12.182451
Number of used sentences in train = 231
Total loss for epoch 1: 601.413521
	Epoch 2....
Epoch has taken 0:00:12.197794
Number of used sentences in train = 231
Total loss for epoch 2: 437.031852
	Epoch 3....
Epoch has taken 0:00:12.188535
Number of used sentences in train = 231
Total loss for epoch 3: 407.062737
	Epoch 4....
Epoch has taken 0:00:12.178401
Number of used sentences in train = 231
Total loss for epoch 4: 382.370567
	Epoch 5....
Epoch has taken 0:00:12.184676
Number of used sentences in train = 231
Total loss for epoch 5: 366.484039
	Epoch 6....
Epoch has taken 0:00:12.178845
Number of used sentences in train = 231
Total loss for epoch 6: 356.915113
	Epoch 7....
Epoch has taken 0:00:12.179665
Number of used sentences in train = 231
Total loss for epoch 7: 355.793580
	Epoch 8....
Epoch has taken 0:00:12.175702
Number of used sentences in train = 231
Total loss for epoch 8: 351.996756
	Epoch 9....
Epoch has taken 0:00:12.181425
Number of used sentences in train = 231
Total loss for epoch 9: 352.513711
	Epoch 10....
Epoch has taken 0:00:12.183640
Number of used sentences in train = 231
Total loss for epoch 10: 349.887915
	Epoch 11....
Epoch has taken 0:00:12.190436
Number of used sentences in train = 231
Total loss for epoch 11: 350.468814
	Epoch 12....
Epoch has taken 0:00:12.178083
Number of used sentences in train = 231
Total loss for epoch 12: 348.969832
	Epoch 13....
Epoch has taken 0:00:12.167289
Number of used sentences in train = 231
Total loss for epoch 13: 348.274477
	Epoch 14....
Epoch has taken 0:00:12.190271
Number of used sentences in train = 231
Total loss for epoch 14: 348.907451
Epoch has taken 0:00:12.171643

==================================================================================================
	Training time : 0:32:50.745414
==================================================================================================
	Identification : 0.318

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 26022.889513
validation loss after epoch 0 : 1200.788921
	Epoch 1....
Epoch has taken 0:03:52.050744
Number of used sentences in train = 3226
Total loss for epoch 1: 10242.427515
validation loss after epoch 1 : 1095.412873
	Epoch 2....
Epoch has taken 0:03:51.576328
Number of used sentences in train = 3226
Total loss for epoch 2: 9129.637850
validation loss after epoch 2 : 1078.736054
	Epoch 3....
Epoch has taken 0:03:52.714916
Number of used sentences in train = 3226
Total loss for epoch 3: 8541.357185
validation loss after epoch 3 : 1094.847199
	Epoch 4....
Epoch has taken 0:03:49.597931
Number of used sentences in train = 3226
Total loss for epoch 4: 8120.349236
validation loss after epoch 4 : 1108.893417
	Epoch 5....
Epoch has taken 0:03:49.897577
Number of used sentences in train = 3226
Total loss for epoch 5: 7791.531427
validation loss after epoch 5 : 1114.918948
	Epoch 6....
Epoch has taken 0:03:52.416678
Number of used sentences in train = 3226
Total loss for epoch 6: 7506.829431
validation loss after epoch 6 : 1186.920447
	Epoch 7....
Epoch has taken 0:03:55.536270
Number of used sentences in train = 3226
Total loss for epoch 7: 7278.070315
validation loss after epoch 7 : 1286.173839
	Epoch 8....
Epoch has taken 0:03:53.672446
Number of used sentences in train = 3226
Total loss for epoch 8: 7126.362793
validation loss after epoch 8 : 1192.159668
	Epoch 9....
Epoch has taken 0:03:51.274173
Number of used sentences in train = 3226
Total loss for epoch 9: 6967.074001
validation loss after epoch 9 : 1284.107947
	Epoch 10....
Epoch has taken 0:04:34.448304
Number of used sentences in train = 3226
Total loss for epoch 10: 6898.397374
validation loss after epoch 10 : 1313.128130
	Epoch 11....
Epoch has taken 0:03:57.651422
Number of used sentences in train = 3226
Total loss for epoch 11: 6796.812208
validation loss after epoch 11 : 1301.751980
	Epoch 12....
Epoch has taken 0:03:55.638808
Number of used sentences in train = 3226
Total loss for epoch 12: 6689.995410
validation loss after epoch 12 : 1379.871936
	Epoch 13....
Epoch has taken 0:03:56.141880
Number of used sentences in train = 3226
Total loss for epoch 13: 6632.245554
validation loss after epoch 13 : 1442.085934
	Epoch 14....
Epoch has taken 0:03:50.144838
Number of used sentences in train = 3226
Total loss for epoch 14: 6560.946379
validation loss after epoch 14 : 1464.611978
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:50.030951
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1549.515919
	Epoch 1....
Epoch has taken 0:00:22.697178
Number of used sentences in train = 359
Total loss for epoch 1: 983.021814
	Epoch 2....
Epoch has taken 0:00:22.987949
Number of used sentences in train = 359
Total loss for epoch 2: 880.812747
	Epoch 3....
Epoch has taken 0:00:22.607986
Number of used sentences in train = 359
Total loss for epoch 3: 814.890700
	Epoch 4....
Epoch has taken 0:00:22.624376
Number of used sentences in train = 359
Total loss for epoch 4: 772.573687
	Epoch 5....
Epoch has taken 0:00:22.672303
Number of used sentences in train = 359
Total loss for epoch 5: 739.772774
	Epoch 6....
Epoch has taken 0:00:22.674450
Number of used sentences in train = 359
Total loss for epoch 6: 716.198056
	Epoch 7....
Epoch has taken 0:00:22.650683
Number of used sentences in train = 359
Total loss for epoch 7: 700.252350
	Epoch 8....
Epoch has taken 0:00:22.671108
Number of used sentences in train = 359
Total loss for epoch 8: 705.989960
	Epoch 9....
Epoch has taken 0:00:22.673301
Number of used sentences in train = 359
Total loss for epoch 9: 684.114018
	Epoch 10....
Epoch has taken 0:00:22.676088
Number of used sentences in train = 359
Total loss for epoch 10: 684.423565
	Epoch 11....
Epoch has taken 0:00:22.666278
Number of used sentences in train = 359
Total loss for epoch 11: 678.957939
	Epoch 12....
Epoch has taken 0:00:22.653977
Number of used sentences in train = 359
Total loss for epoch 12: 683.734787
	Epoch 13....
Epoch has taken 0:00:22.633203
Number of used sentences in train = 359
Total loss for epoch 13: 674.456145
	Epoch 14....
Epoch has taken 0:00:22.536346
Number of used sentences in train = 359
Total loss for epoch 14: 674.284725
Epoch has taken 0:00:22.594962

==================================================================================================
	Training time : 1:04:33.496106
==================================================================================================
	Identification : 0.023

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 65, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 22, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 81, 'lstmDropout': 0.19, 'denseActivation': 'tanh', 'wordDim': 104, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(5879, 104)
  (lstm): LSTM(126, 81, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1296, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15649.497013
validation loss after epoch 0 : 1164.228184
	Epoch 1....
Epoch has taken 0:02:55.103191
Number of used sentences in train = 2811
Total loss for epoch 1: 9637.034466
validation loss after epoch 1 : 1044.006818
	Epoch 2....
Epoch has taken 0:02:55.125080
Number of used sentences in train = 2811
Total loss for epoch 2: 7904.631310
validation loss after epoch 2 : 1061.542298
	Epoch 3....
Epoch has taken 0:02:54.379214
Number of used sentences in train = 2811
Total loss for epoch 3: 6947.753495
validation loss after epoch 3 : 1057.129828
	Epoch 4....
Epoch has taken 0:02:54.688709
Number of used sentences in train = 2811
Total loss for epoch 4: 6215.605374
validation loss after epoch 4 : 1122.845594
	Epoch 5....
Epoch has taken 0:02:55.088234
Number of used sentences in train = 2811
Total loss for epoch 5: 5805.544737
validation loss after epoch 5 : 1119.416172
	Epoch 6....
Epoch has taken 0:02:53.566333
Number of used sentences in train = 2811
Total loss for epoch 6: 5492.313760
validation loss after epoch 6 : 1203.046225
	Epoch 7....
Epoch has taken 0:02:54.977952
Number of used sentences in train = 2811
Total loss for epoch 7: 5264.757624
validation loss after epoch 7 : 1215.168261
	Epoch 8....
Epoch has taken 0:02:55.280846
Number of used sentences in train = 2811
Total loss for epoch 8: 5043.536345
validation loss after epoch 8 : 1281.082952
	Epoch 9....
Epoch has taken 0:02:54.590953
Number of used sentences in train = 2811
Total loss for epoch 9: 4959.390468
validation loss after epoch 9 : 1303.547075
	Epoch 10....
Epoch has taken 0:02:54.770454
Number of used sentences in train = 2811
Total loss for epoch 10: 4804.490566
validation loss after epoch 10 : 1324.650637
	Epoch 11....
Epoch has taken 0:02:55.517611
Number of used sentences in train = 2811
Total loss for epoch 11: 4763.878731
validation loss after epoch 11 : 1430.215339
	Epoch 12....
Epoch has taken 0:02:53.435596
Number of used sentences in train = 2811
Total loss for epoch 12: 4728.927035
validation loss after epoch 12 : 1357.197963
	Epoch 13....
Epoch has taken 0:02:53.353734
Number of used sentences in train = 2811
Total loss for epoch 13: 4751.893089
validation loss after epoch 13 : 1329.478172
	Epoch 14....
Epoch has taken 0:02:54.881061
Number of used sentences in train = 2811
Total loss for epoch 14: 4666.752902
validation loss after epoch 14 : 1449.551442
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(5879, 104)
  (lstm): LSTM(126, 81, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1296, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.873811
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1863.195992
	Epoch 1....
Epoch has taken 0:00:18.433112
Number of used sentences in train = 313
Total loss for epoch 1: 928.689806
	Epoch 2....
Epoch has taken 0:00:18.425207
Number of used sentences in train = 313
Total loss for epoch 2: 742.931492
	Epoch 3....
Epoch has taken 0:00:18.408086
Number of used sentences in train = 313
Total loss for epoch 3: 634.816187
	Epoch 4....
Epoch has taken 0:00:18.333717
Number of used sentences in train = 313
Total loss for epoch 4: 581.351478
	Epoch 5....
Epoch has taken 0:00:18.383145
Number of used sentences in train = 313
Total loss for epoch 5: 544.378150
	Epoch 6....
Epoch has taken 0:00:18.432086
Number of used sentences in train = 313
Total loss for epoch 6: 539.598958
	Epoch 7....
Epoch has taken 0:00:18.427511
Number of used sentences in train = 313
Total loss for epoch 7: 534.070573
	Epoch 8....
Epoch has taken 0:00:18.427447
Number of used sentences in train = 313
Total loss for epoch 8: 523.535178
	Epoch 9....
Epoch has taken 0:00:18.413188
Number of used sentences in train = 313
Total loss for epoch 9: 517.357823
	Epoch 10....
Epoch has taken 0:00:18.396712
Number of used sentences in train = 313
Total loss for epoch 10: 511.960144
	Epoch 11....
Epoch has taken 0:00:18.395613
Number of used sentences in train = 313
Total loss for epoch 11: 509.072664
	Epoch 12....
Epoch has taken 0:00:18.399270
Number of used sentences in train = 313
Total loss for epoch 12: 505.876141
	Epoch 13....
Epoch has taken 0:00:18.393479
Number of used sentences in train = 313
Total loss for epoch 13: 511.149805
	Epoch 14....
Epoch has taken 0:00:18.408099
Number of used sentences in train = 313
Total loss for epoch 14: 506.911238
Epoch has taken 0:00:18.393526

==================================================================================================
	Training time : 0:48:18.218056
==================================================================================================
	Identification : 0.111

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(5646, 104)
  (lstm): LSTM(126, 81, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1296, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11899.616111
validation loss after epoch 0 : 946.775636
	Epoch 1....
Epoch has taken 0:01:59.583631
Number of used sentences in train = 2074
Total loss for epoch 1: 6563.054743
validation loss after epoch 1 : 886.084219
	Epoch 2....
Epoch has taken 0:01:59.641166
Number of used sentences in train = 2074
Total loss for epoch 2: 5059.531610
validation loss after epoch 2 : 893.650291
	Epoch 3....
Epoch has taken 0:01:59.673448
Number of used sentences in train = 2074
Total loss for epoch 3: 4261.045848
validation loss after epoch 3 : 1011.972926
	Epoch 4....
Epoch has taken 0:01:59.638255
Number of used sentences in train = 2074
Total loss for epoch 4: 3801.858073
validation loss after epoch 4 : 1092.968259
	Epoch 5....
Epoch has taken 0:01:59.407085
Number of used sentences in train = 2074
Total loss for epoch 5: 3588.128292
validation loss after epoch 5 : 1268.796910
	Epoch 6....
Epoch has taken 0:01:59.570919
Number of used sentences in train = 2074
Total loss for epoch 6: 3481.258543
validation loss after epoch 6 : 1306.115251
	Epoch 7....
Epoch has taken 0:01:59.737047
Number of used sentences in train = 2074
Total loss for epoch 7: 3343.381653
validation loss after epoch 7 : 1260.490234
	Epoch 8....
Epoch has taken 0:01:58.640040
Number of used sentences in train = 2074
Total loss for epoch 8: 3277.378934
validation loss after epoch 8 : 1179.620288
	Epoch 9....
Epoch has taken 0:01:58.607288
Number of used sentences in train = 2074
Total loss for epoch 9: 3242.195115
validation loss after epoch 9 : 1360.948740
	Epoch 10....
Epoch has taken 0:01:59.760437
Number of used sentences in train = 2074
Total loss for epoch 10: 3236.961239
validation loss after epoch 10 : 1438.954815
	Epoch 11....
Epoch has taken 0:02:00.179091
Number of used sentences in train = 2074
Total loss for epoch 11: 3207.939622
validation loss after epoch 11 : 1265.627243
	Epoch 12....
Epoch has taken 0:02:00.966926
Number of used sentences in train = 2074
Total loss for epoch 12: 3191.659599
validation loss after epoch 12 : 1402.237137
	Epoch 13....
Epoch has taken 0:01:59.576777
Number of used sentences in train = 2074
Total loss for epoch 13: 3201.009918
validation loss after epoch 13 : 1500.061494
	Epoch 14....
Epoch has taken 0:01:59.375231
Number of used sentences in train = 2074
Total loss for epoch 14: 3189.561015
validation loss after epoch 14 : 1461.927739
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(5646, 104)
  (lstm): LSTM(126, 81, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1296, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:59.372568
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1520.315402
	Epoch 1....
Epoch has taken 0:00:12.209589
Number of used sentences in train = 231
Total loss for epoch 1: 594.290044
	Epoch 2....
Epoch has taken 0:00:12.227807
Number of used sentences in train = 231
Total loss for epoch 2: 437.063984
	Epoch 3....
Epoch has taken 0:00:12.226291
Number of used sentences in train = 231
Total loss for epoch 3: 387.499553
	Epoch 4....
Epoch has taken 0:00:12.220432
Number of used sentences in train = 231
Total loss for epoch 4: 370.972704
	Epoch 5....
Epoch has taken 0:00:12.174742
Number of used sentences in train = 231
Total loss for epoch 5: 359.586921
	Epoch 6....
Epoch has taken 0:00:12.181407
Number of used sentences in train = 231
Total loss for epoch 6: 350.267834
	Epoch 7....
Epoch has taken 0:00:12.226412
Number of used sentences in train = 231
Total loss for epoch 7: 347.838787
	Epoch 8....
Epoch has taken 0:00:12.187644
Number of used sentences in train = 231
Total loss for epoch 8: 348.938124
	Epoch 9....
Epoch has taken 0:00:12.201348
Number of used sentences in train = 231
Total loss for epoch 9: 347.060933
	Epoch 10....
Epoch has taken 0:00:12.171116
Number of used sentences in train = 231
Total loss for epoch 10: 346.785028
	Epoch 11....
Epoch has taken 0:00:12.165754
Number of used sentences in train = 231
Total loss for epoch 11: 345.873042
	Epoch 12....
Epoch has taken 0:00:12.202986
Number of used sentences in train = 231
Total loss for epoch 12: 345.766297
	Epoch 13....
Epoch has taken 0:00:12.197360
Number of used sentences in train = 231
Total loss for epoch 13: 345.648044
	Epoch 14....
Epoch has taken 0:00:12.198273
Number of used sentences in train = 231
Total loss for epoch 14: 345.271012
Epoch has taken 0:00:12.154263

==================================================================================================
	Training time : 0:32:57.023994
==================================================================================================
	Identification : 0.354

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 22)
  (w_embeddings): Embedding(6898, 104)
  (lstm): LSTM(126, 81, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1296, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20961.606275
validation loss after epoch 0 : 1548.134246
	Epoch 1....
Epoch has taken 0:03:55.132179
Number of used sentences in train = 3226
Total loss for epoch 1: 12366.872206
validation loss after epoch 1 : 1489.515844
	Epoch 2....
Epoch has taken 0:03:55.855215
Number of used sentences in train = 3226
Total loss for epoch 2: 10438.421470
validation loss after epoch 2 : 1657.954112
	Epoch 3....
Epoch has taken 0:03:55.706875
Number of used sentences in train = 3226
Total loss for epoch 3: 9082.801365
validation loss after epoch 3 : 1619.608978
	Epoch 4....
Epoch has taken 0:03:55.703239
Number of used sentences in train = 3226
Total loss for epoch 4: 8076.593259
validation loss after epoch 4 : 1831.728997
	Epoch 5....
Epoch has taken 0:03:53.374141
Number of used sentences in train = 3226
Total loss for epoch 5: 7516.500899
validation loss after epoch 5 : 1944.686726
	Epoch 6....
Epoch has taken 0:03:52.945654
Number of used sentences in train = 3226
Total loss for epoch 6: 7068.481439
validation loss after epoch 6 : 2033.697735
	Epoch 7....
Epoch has taken 0:03:55.430160
Number of used sentences in train = 3226
Total loss for epoch 7: 6790.980277
validation loss after epoch 7 : 2141.747234
	Epoch 8....
Epoch has taken 0:03:54.484992
Number of used sentences in train = 3226
Total loss for epoch 8: 6614.758484
validation loss after epoch 8 : 2326.389228
	Epoch 9....
Epoch has taken 0:03:53.716572
Number of used sentences in train = 3226
Total loss for epoch 9: 6456.590829
validation loss after epoch 9 : 2348.817631
	Epoch 10....
Epoch has taken 0:03:58.035905
Number of used sentences in train = 3226
Total loss for epoch 10: 6425.612037
validation loss after epoch 10 : 2394.557342
	Epoch 11....
Epoch has taken 0:04:09.950129
Number of used sentences in train = 3226
Total loss for epoch 11: 6292.523888
validation loss after epoch 11 : 2523.511717
	Epoch 12....
Epoch has taken 0:04:11.328113
Number of used sentences in train = 3226
Total loss for epoch 12: 6308.051966
validation loss after epoch 12 : 2462.716779
	Epoch 13....
Epoch has taken 0:04:09.431241
Number of used sentences in train = 3226
Total loss for epoch 13: 6231.117999
validation loss after epoch 13 : 2479.412918
	Epoch 14....
Epoch has taken 0:04:09.027348
Number of used sentences in train = 3226
Total loss for epoch 14: 6250.527137
validation loss after epoch 14 : 2552.764758
	TransitionClassifier(
  (p_embeddings): Embedding(13, 22)
  (w_embeddings): Embedding(6898, 104)
  (lstm): LSTM(126, 81, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1296, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:09.810425
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2467.311902
	Epoch 1....
Epoch has taken 0:00:23.715323
Number of used sentences in train = 359
Total loss for epoch 1: 1131.190265
	Epoch 2....
Epoch has taken 0:00:22.554165
Number of used sentences in train = 359
Total loss for epoch 2: 869.155981
	Epoch 3....
Epoch has taken 0:00:25.853826
Number of used sentences in train = 359
Total loss for epoch 3: 772.635803
	Epoch 4....
Epoch has taken 0:00:25.825359
Number of used sentences in train = 359
Total loss for epoch 4: 738.080952
	Epoch 5....
Epoch has taken 0:00:25.809994
Number of used sentences in train = 359
Total loss for epoch 5: 716.457568
	Epoch 6....
Epoch has taken 0:00:25.854891
Number of used sentences in train = 359
Total loss for epoch 6: 697.250187
	Epoch 7....
Epoch has taken 0:00:25.837959
Number of used sentences in train = 359
Total loss for epoch 7: 679.536439
	Epoch 8....
Epoch has taken 0:00:25.843689
Number of used sentences in train = 359
Total loss for epoch 8: 678.476668
	Epoch 9....
Epoch has taken 0:00:25.827115
Number of used sentences in train = 359
Total loss for epoch 9: 673.306673
	Epoch 10....
Epoch has taken 0:00:25.847430
Number of used sentences in train = 359
Total loss for epoch 10: 672.247121
	Epoch 11....
Epoch has taken 0:00:25.820152
Number of used sentences in train = 359
Total loss for epoch 11: 671.602422
	Epoch 12....
Epoch has taken 0:00:25.827438
Number of used sentences in train = 359
Total loss for epoch 12: 671.457795
	Epoch 13....
Epoch has taken 0:00:25.845028
Number of used sentences in train = 359
Total loss for epoch 13: 671.079401
	Epoch 14....
Epoch has taken 0:00:25.862765
Number of used sentences in train = 359
Total loss for epoch 14: 671.064502
Epoch has taken 0:00:25.842190

==================================================================================================
	Training time : 1:06:22.767770
==================================================================================================
	Identification : 0.334

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 45, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 16, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 86, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 62, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1177, 62)
  (lstm): LSTM(78, 86, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12159.302711
validation loss after epoch 0 : 902.948969
	Epoch 1....
Epoch has taken 0:02:58.106369
Number of used sentences in train = 2811
Total loss for epoch 1: 8027.657786
validation loss after epoch 1 : 868.450577
	Epoch 2....
Epoch has taken 0:02:57.257399
Number of used sentences in train = 2811
Total loss for epoch 2: 7156.450107
validation loss after epoch 2 : 882.059776
	Epoch 3....
Epoch has taken 0:02:55.581315
Number of used sentences in train = 2811
Total loss for epoch 3: 6530.377202
validation loss after epoch 3 : 894.914006
	Epoch 4....
Epoch has taken 0:02:55.342890
Number of used sentences in train = 2811
Total loss for epoch 4: 6040.746928
validation loss after epoch 4 : 879.613711
	Epoch 5....
Epoch has taken 0:02:55.320775
Number of used sentences in train = 2811
Total loss for epoch 5: 5733.986168
validation loss after epoch 5 : 887.554932
	Epoch 6....
Epoch has taken 0:02:54.106161
Number of used sentences in train = 2811
Total loss for epoch 6: 5415.886632
validation loss after epoch 6 : 956.489567
	Epoch 7....
Epoch has taken 0:02:55.703965
Number of used sentences in train = 2811
Total loss for epoch 7: 5228.865127
validation loss after epoch 7 : 961.869308
	Epoch 8....
Epoch has taken 0:02:55.124809
Number of used sentences in train = 2811
Total loss for epoch 8: 5097.228762
validation loss after epoch 8 : 994.730474
	Epoch 9....
Epoch has taken 0:02:55.096440
Number of used sentences in train = 2811
Total loss for epoch 9: 4941.628379
validation loss after epoch 9 : 1053.476574
	Epoch 10....
Epoch has taken 0:02:55.044250
Number of used sentences in train = 2811
Total loss for epoch 10: 4821.706969
validation loss after epoch 10 : 1045.289804
	Epoch 11....
Epoch has taken 0:03:30.899725
Number of used sentences in train = 2811
Total loss for epoch 11: 4794.446946
validation loss after epoch 11 : 1054.276412
	Epoch 12....
Epoch has taken 0:02:56.586805
Number of used sentences in train = 2811
Total loss for epoch 12: 4810.005129
validation loss after epoch 12 : 1049.678041
	Epoch 13....
Epoch has taken 0:02:55.436055
Number of used sentences in train = 2811
Total loss for epoch 13: 4746.549999
validation loss after epoch 13 : 1066.311940
	Epoch 14....
Epoch has taken 0:02:59.502631
Number of used sentences in train = 2811
Total loss for epoch 14: 4672.857831
validation loss after epoch 14 : 1124.676433
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1177, 62)
  (lstm): LSTM(78, 86, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.172994
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1446.607582
	Epoch 1....
Epoch has taken 0:00:22.311196
Number of used sentences in train = 313
Total loss for epoch 1: 907.330398
	Epoch 2....
Epoch has taken 0:00:22.078432
Number of used sentences in train = 313
Total loss for epoch 2: 720.396827
	Epoch 3....
Epoch has taken 0:00:22.362583
Number of used sentences in train = 313
Total loss for epoch 3: 651.024491
	Epoch 4....
Epoch has taken 0:00:22.413234
Number of used sentences in train = 313
Total loss for epoch 4: 594.663495
	Epoch 5....
Epoch has taken 0:00:22.311011
Number of used sentences in train = 313
Total loss for epoch 5: 545.191678
	Epoch 6....
Epoch has taken 0:00:22.362688
Number of used sentences in train = 313
Total loss for epoch 6: 534.784163
	Epoch 7....
Epoch has taken 0:00:19.734458
Number of used sentences in train = 313
Total loss for epoch 7: 529.174805
	Epoch 8....
Epoch has taken 0:00:19.546811
Number of used sentences in train = 313
Total loss for epoch 8: 526.085283
	Epoch 9....
Epoch has taken 0:00:19.532340
Number of used sentences in train = 313
Total loss for epoch 9: 525.360917
	Epoch 10....
Epoch has taken 0:00:19.438048
Number of used sentences in train = 313
Total loss for epoch 10: 528.810577
	Epoch 11....
Epoch has taken 0:00:19.490457
Number of used sentences in train = 313
Total loss for epoch 11: 527.904165
	Epoch 12....
Epoch has taken 0:00:19.650178
Number of used sentences in train = 313
Total loss for epoch 12: 521.961569
	Epoch 13....
Epoch has taken 0:00:18.616144
Number of used sentences in train = 313
Total loss for epoch 13: 518.590966
	Epoch 14....
Epoch has taken 0:00:19.736211
Number of used sentences in train = 313
Total loss for epoch 14: 518.807303
Epoch has taken 0:00:18.917079

==================================================================================================
	Training time : 0:49:45.319356
==================================================================================================
	Identification : 0.498

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1133, 62)
  (lstm): LSTM(78, 86, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10361.297203
validation loss after epoch 0 : 719.053429
	Epoch 1....
Epoch has taken 0:02:05.593383
Number of used sentences in train = 2074
Total loss for epoch 1: 5876.925697
validation loss after epoch 1 : 656.045158
	Epoch 2....
Epoch has taken 0:02:05.107707
Number of used sentences in train = 2074
Total loss for epoch 2: 5169.803793
validation loss after epoch 2 : 670.007776
	Epoch 3....
Epoch has taken 0:02:01.474859
Number of used sentences in train = 2074
Total loss for epoch 3: 4726.401818
validation loss after epoch 3 : 662.331846
	Epoch 4....
Epoch has taken 0:02:04.207528
Number of used sentences in train = 2074
Total loss for epoch 4: 4345.863961
validation loss after epoch 4 : 688.879920
	Epoch 5....
Epoch has taken 0:02:03.856588
Number of used sentences in train = 2074
Total loss for epoch 5: 4161.227034
validation loss after epoch 5 : 704.052438
	Epoch 6....
Epoch has taken 0:02:05.231654
Number of used sentences in train = 2074
Total loss for epoch 6: 3873.236372
validation loss after epoch 6 : 674.100996
	Epoch 7....
Epoch has taken 0:02:05.561401
Number of used sentences in train = 2074
Total loss for epoch 7: 3690.079678
validation loss after epoch 7 : 745.999310
	Epoch 8....
Epoch has taken 0:02:05.895262
Number of used sentences in train = 2074
Total loss for epoch 8: 3578.486968
validation loss after epoch 8 : 760.567624
	Epoch 9....
Epoch has taken 0:02:01.047027
Number of used sentences in train = 2074
Total loss for epoch 9: 3530.300369
validation loss after epoch 9 : 706.800934
	Epoch 10....
Epoch has taken 0:01:59.743168
Number of used sentences in train = 2074
Total loss for epoch 10: 3455.070419
validation loss after epoch 10 : 852.657102
	Epoch 11....
Epoch has taken 0:02:03.056937
Number of used sentences in train = 2074
Total loss for epoch 11: 3408.990058
validation loss after epoch 11 : 783.053059
	Epoch 12....
Epoch has taken 0:02:00.325686
Number of used sentences in train = 2074
Total loss for epoch 12: 3332.220184
validation loss after epoch 12 : 799.541596
	Epoch 13....
Epoch has taken 0:02:03.239192
Number of used sentences in train = 2074
Total loss for epoch 13: 3347.656224
validation loss after epoch 13 : 956.948194
	Epoch 14....
Epoch has taken 0:01:59.909771
Number of used sentences in train = 2074
Total loss for epoch 14: 3303.655791
validation loss after epoch 14 : 822.459800
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1133, 62)
  (lstm): LSTM(78, 86, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:59.934684
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1487.186397
	Epoch 1....
Epoch has taken 0:00:12.225638
Number of used sentences in train = 231
Total loss for epoch 1: 715.396977
	Epoch 2....
Epoch has taken 0:00:12.182710
Number of used sentences in train = 231
Total loss for epoch 2: 549.999144
	Epoch 3....
Epoch has taken 0:00:12.189482
Number of used sentences in train = 231
Total loss for epoch 3: 466.769518
	Epoch 4....
Epoch has taken 0:00:12.184088
Number of used sentences in train = 231
Total loss for epoch 4: 464.351486
	Epoch 5....
Epoch has taken 0:00:12.227728
Number of used sentences in train = 231
Total loss for epoch 5: 389.346946
	Epoch 6....
Epoch has taken 0:00:12.219234
Number of used sentences in train = 231
Total loss for epoch 6: 375.138782
	Epoch 7....
Epoch has taken 0:00:12.179739
Number of used sentences in train = 231
Total loss for epoch 7: 368.919120
	Epoch 8....
Epoch has taken 0:00:12.200231
Number of used sentences in train = 231
Total loss for epoch 8: 363.676446
	Epoch 9....
Epoch has taken 0:00:12.240757
Number of used sentences in train = 231
Total loss for epoch 9: 365.224408
	Epoch 10....
Epoch has taken 0:00:12.207028
Number of used sentences in train = 231
Total loss for epoch 10: 360.810383
	Epoch 11....
Epoch has taken 0:00:12.225707
Number of used sentences in train = 231
Total loss for epoch 11: 356.459708
	Epoch 12....
Epoch has taken 0:00:12.205203
Number of used sentences in train = 231
Total loss for epoch 12: 353.309989
	Epoch 13....
Epoch has taken 0:00:12.200996
Number of used sentences in train = 231
Total loss for epoch 13: 351.424288
	Epoch 14....
Epoch has taken 0:00:12.222526
Number of used sentences in train = 231
Total loss for epoch 14: 350.590052
Epoch has taken 0:00:12.201422

==================================================================================================
	Training time : 0:33:47.667987
==================================================================================================
	Identification : 0.422

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(1202, 62)
  (lstm): LSTM(78, 86, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20653.430647
validation loss after epoch 0 : 1368.867032
	Epoch 1....
Epoch has taken 0:03:53.476288
Number of used sentences in train = 3226
Total loss for epoch 1: 11721.434370
validation loss after epoch 1 : 1244.032256
	Epoch 2....
Epoch has taken 0:03:59.898184
Number of used sentences in train = 3226
Total loss for epoch 2: 10468.936380
validation loss after epoch 2 : 1236.835665
	Epoch 3....
Epoch has taken 0:03:58.884368
Number of used sentences in train = 3226
Total loss for epoch 3: 9748.558888
validation loss after epoch 3 : 1254.463014
	Epoch 4....
Epoch has taken 0:04:00.115030
Number of used sentences in train = 3226
Total loss for epoch 4: 9201.078877
validation loss after epoch 4 : 1305.625881
	Epoch 5....
Epoch has taken 0:04:03.817094
Number of used sentences in train = 3226
Total loss for epoch 5: 8770.760273
validation loss after epoch 5 : 1342.467145
	Epoch 6....
Epoch has taken 0:04:04.464445
Number of used sentences in train = 3226
Total loss for epoch 6: 8392.100360
validation loss after epoch 6 : 1423.075133
	Epoch 7....
Epoch has taken 0:04:04.226410
Number of used sentences in train = 3226
Total loss for epoch 7: 8085.115907
validation loss after epoch 7 : 1414.224599
	Epoch 8....
Epoch has taken 0:04:02.538961
Number of used sentences in train = 3226
Total loss for epoch 8: 7860.756418
validation loss after epoch 8 : 1460.843745
	Epoch 9....
Epoch has taken 0:04:01.761285
Number of used sentences in train = 3226
Total loss for epoch 9: 7681.869003
validation loss after epoch 9 : 1546.167134
	Epoch 10....
Epoch has taken 0:04:04.163371
Number of used sentences in train = 3226
Total loss for epoch 10: 7448.702966
validation loss after epoch 10 : 1593.135459
	Epoch 11....
Epoch has taken 0:04:02.280259
Number of used sentences in train = 3226
Total loss for epoch 11: 7224.151439
validation loss after epoch 11 : 1546.201441
	Epoch 12....
Epoch has taken 0:04:01.650854
Number of used sentences in train = 3226
Total loss for epoch 12: 7121.214314
validation loss after epoch 12 : 1687.158126
	Epoch 13....
Epoch has taken 0:04:04.252754
Number of used sentences in train = 3226
Total loss for epoch 13: 6994.883772
validation loss after epoch 13 : 1783.334482
	Epoch 14....
Epoch has taken 0:04:04.455064
Number of used sentences in train = 3226
Total loss for epoch 14: 6871.463749
validation loss after epoch 14 : 1769.085817
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(1202, 62)
  (lstm): LSTM(78, 86, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:04.475914
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1927.340516
	Epoch 1....
Epoch has taken 0:00:24.008310
Number of used sentences in train = 359
Total loss for epoch 1: 1240.757320
	Epoch 2....
Epoch has taken 0:00:23.892697
Number of used sentences in train = 359
Total loss for epoch 2: 1092.969778
	Epoch 3....
Epoch has taken 0:00:24.022918
Number of used sentences in train = 359
Total loss for epoch 3: 960.184790
	Epoch 4....
Epoch has taken 0:00:23.935728
Number of used sentences in train = 359
Total loss for epoch 4: 882.610765
	Epoch 5....
Epoch has taken 0:00:23.792566
Number of used sentences in train = 359
Total loss for epoch 5: 818.866198
	Epoch 6....
Epoch has taken 0:00:23.860929
Number of used sentences in train = 359
Total loss for epoch 6: 775.414084
	Epoch 7....
Epoch has taken 0:00:23.895873
Number of used sentences in train = 359
Total loss for epoch 7: 752.885130
	Epoch 8....
Epoch has taken 0:00:23.993020
Number of used sentences in train = 359
Total loss for epoch 8: 728.916151
	Epoch 9....
Epoch has taken 0:00:23.923893
Number of used sentences in train = 359
Total loss for epoch 9: 736.728253
	Epoch 10....
Epoch has taken 0:00:23.967848
Number of used sentences in train = 359
Total loss for epoch 10: 723.204696
	Epoch 11....
Epoch has taken 0:00:23.961414
Number of used sentences in train = 359
Total loss for epoch 11: 716.473562
	Epoch 12....
Epoch has taken 0:00:23.914072
Number of used sentences in train = 359
Total loss for epoch 12: 702.497990
	Epoch 13....
Epoch has taken 0:00:23.693734
Number of used sentences in train = 359
Total loss for epoch 13: 709.993785
	Epoch 14....
Epoch has taken 0:00:24.166756
Number of used sentences in train = 359
Total loss for epoch 14: 689.692162
Epoch has taken 0:00:23.837159

==================================================================================================
	Training time : 1:06:29.992381
==================================================================================================
	Identification : 0.305

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 46, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 29, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 37, 'lstmDropout': 0.2, 'denseActivation': 'tanh', 'wordDim': 106, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 29)
  (w_embeddings): Embedding(5882, 106)
  (lstm): LSTM(135, 37, bidirectional=True)
  (linear1): Linear(in_features=592, out_features=46, bias=True)
  (linear2): Linear(in_features=46, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12574.263712
validation loss after epoch 0 : 1456.279083
	Epoch 1....
Epoch has taken 0:02:50.976914
Number of used sentences in train = 2811
Total loss for epoch 1: 8443.762040
validation loss after epoch 1 : 1135.333807
	Epoch 2....
Epoch has taken 0:02:51.513181
Number of used sentences in train = 2811
Total loss for epoch 2: 6945.379398
validation loss after epoch 2 : 1175.757463
	Epoch 3....
Epoch has taken 0:02:46.830788
Number of used sentences in train = 2811
Total loss for epoch 3: 6049.870279
validation loss after epoch 3 : 1234.280676
	Epoch 4....
Epoch has taken 0:02:51.250578
Number of used sentences in train = 2811
Total loss for epoch 4: 5510.393930
validation loss after epoch 4 : 1287.009359
	Epoch 5....
Epoch has taken 0:02:51.508949
Number of used sentences in train = 2811
Total loss for epoch 5: 5153.122406
validation loss after epoch 5 : 1393.784491
	Epoch 6....
Epoch has taken 0:02:50.550173
Number of used sentences in train = 2811
Total loss for epoch 6: 4902.260648
validation loss after epoch 6 : 1504.818150
	Epoch 7....
Epoch has taken 0:02:49.458081
Number of used sentences in train = 2811
Total loss for epoch 7: 4741.901684
validation loss after epoch 7 : 1551.973522
	Epoch 8....
Epoch has taken 0:02:46.075441
Number of used sentences in train = 2811
Total loss for epoch 8: 4652.842862
validation loss after epoch 8 : 1624.688759
	Epoch 9....
Epoch has taken 0:02:42.673243
Number of used sentences in train = 2811
Total loss for epoch 9: 4608.683377
validation loss after epoch 9 : 1673.541826
	Epoch 10....
Epoch has taken 0:02:43.914298
Number of used sentences in train = 2811
Total loss for epoch 10: 4573.582718
validation loss after epoch 10 : 1712.607092
	Epoch 11....
Epoch has taken 0:03:17.005434
Number of used sentences in train = 2811
Total loss for epoch 11: 4552.666246
validation loss after epoch 11 : 1748.063053
	Epoch 12....
Epoch has taken 0:02:50.419379
Number of used sentences in train = 2811
Total loss for epoch 12: 4536.795684
validation loss after epoch 12 : 1785.903027
	Epoch 13....
Epoch has taken 0:02:49.298232
Number of used sentences in train = 2811
Total loss for epoch 13: 4527.604083
validation loss after epoch 13 : 1812.124573
	Epoch 14....
Epoch has taken 0:02:49.643372
Number of used sentences in train = 2811
Total loss for epoch 14: 4520.396090
validation loss after epoch 14 : 1839.167891
	TransitionClassifier(
  (p_embeddings): Embedding(18, 29)
  (w_embeddings): Embedding(5882, 106)
  (lstm): LSTM(135, 37, bidirectional=True)
  (linear1): Linear(in_features=592, out_features=46, bias=True)
  (linear2): Linear(in_features=46, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.248140
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1940.509521
	Epoch 1....
Epoch has taken 0:00:18.088782
Number of used sentences in train = 313
Total loss for epoch 1: 755.012401
	Epoch 2....
Epoch has taken 0:00:18.157108
Number of used sentences in train = 313
Total loss for epoch 2: 574.916982
	Epoch 3....
Epoch has taken 0:00:17.915378
Number of used sentences in train = 313
Total loss for epoch 3: 531.621526
	Epoch 4....
Epoch has taken 0:00:18.028547
Number of used sentences in train = 313
Total loss for epoch 4: 516.780354
	Epoch 5....
Epoch has taken 0:00:18.061580
Number of used sentences in train = 313
Total loss for epoch 5: 510.638851
	Epoch 6....
Epoch has taken 0:00:18.237738
Number of used sentences in train = 313
Total loss for epoch 6: 509.792081
	Epoch 7....
Epoch has taken 0:00:18.044262
Number of used sentences in train = 313
Total loss for epoch 7: 505.106314
	Epoch 8....
Epoch has taken 0:00:17.994228
Number of used sentences in train = 313
Total loss for epoch 8: 505.430411
	Epoch 9....
Epoch has taken 0:00:18.143212
Number of used sentences in train = 313
Total loss for epoch 9: 505.073509
	Epoch 10....
Epoch has taken 0:00:19.452624
Number of used sentences in train = 313
Total loss for epoch 10: 504.236855
	Epoch 11....
Epoch has taken 0:00:20.805992
Number of used sentences in train = 313
Total loss for epoch 11: 503.649942
	Epoch 12....
Epoch has taken 0:00:20.771971
Number of used sentences in train = 313
Total loss for epoch 12: 502.585141
	Epoch 13....
Epoch has taken 0:00:20.786274
Number of used sentences in train = 313
Total loss for epoch 13: 502.541244
	Epoch 14....
Epoch has taken 0:00:20.795107
Number of used sentences in train = 313
Total loss for epoch 14: 502.849410
Epoch has taken 0:00:18.200668

==================================================================================================
	Training time : 0:47:26.373698
==================================================================================================
	Identification : 0.477

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 29)
  (w_embeddings): Embedding(5576, 106)
  (lstm): LSTM(135, 37, bidirectional=True)
  (linear1): Linear(in_features=592, out_features=46, bias=True)
  (linear2): Linear(in_features=46, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9750.751027
validation loss after epoch 0 : 938.828962
	Epoch 1....
Epoch has taken 0:01:56.317030
Number of used sentences in train = 2074
Total loss for epoch 1: 5974.492728
validation loss after epoch 1 : 916.740493
	Epoch 2....
Epoch has taken 0:01:56.330015
Number of used sentences in train = 2074
Total loss for epoch 2: 4645.607936
validation loss after epoch 2 : 1006.719390
	Epoch 3....
Epoch has taken 0:01:56.359526
Number of used sentences in train = 2074
Total loss for epoch 3: 3908.844381
validation loss after epoch 3 : 1126.308614
	Epoch 4....
Epoch has taken 0:01:57.204894
Number of used sentences in train = 2074
Total loss for epoch 4: 3528.472422
validation loss after epoch 4 : 1232.625571
	Epoch 5....
Epoch has taken 0:02:14.800872
Number of used sentences in train = 2074
Total loss for epoch 5: 3334.049335
validation loss after epoch 5 : 1325.232280
	Epoch 6....
Epoch has taken 0:01:57.381509
Number of used sentences in train = 2074
Total loss for epoch 6: 3251.103395
validation loss after epoch 6 : 1416.224874
	Epoch 7....
Epoch has taken 0:01:56.030289
Number of used sentences in train = 2074
Total loss for epoch 7: 3208.051440
validation loss after epoch 7 : 1459.275661
	Epoch 8....
Epoch has taken 0:01:56.260939
Number of used sentences in train = 2074
Total loss for epoch 8: 3189.502524
validation loss after epoch 8 : 1495.172904
	Epoch 9....
Epoch has taken 0:01:52.819170
Number of used sentences in train = 2074
Total loss for epoch 9: 3184.828880
validation loss after epoch 9 : 1516.547282
	Epoch 10....
Epoch has taken 0:01:48.980501
Number of used sentences in train = 2074
Total loss for epoch 10: 3179.127352
validation loss after epoch 10 : 1571.155958
	Epoch 11....
Epoch has taken 0:01:48.060798
Number of used sentences in train = 2074
Total loss for epoch 11: 3175.602718
validation loss after epoch 11 : 1581.126387
	Epoch 12....
Epoch has taken 0:01:47.925710
Number of used sentences in train = 2074
Total loss for epoch 12: 3172.768187
validation loss after epoch 12 : 1607.743407
	Epoch 13....
Epoch has taken 0:01:49.166885
Number of used sentences in train = 2074
Total loss for epoch 13: 3171.118728
validation loss after epoch 13 : 1613.905308
	Epoch 14....
Epoch has taken 0:02:06.035636
Number of used sentences in train = 2074
Total loss for epoch 14: 3168.728371
validation loss after epoch 14 : 1629.057451
	TransitionClassifier(
  (p_embeddings): Embedding(18, 29)
  (w_embeddings): Embedding(5576, 106)
  (lstm): LSTM(135, 37, bidirectional=True)
  (linear1): Linear(in_features=592, out_features=46, bias=True)
  (linear2): Linear(in_features=46, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.587903
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1691.719741
	Epoch 1....
Epoch has taken 0:00:11.000977
Number of used sentences in train = 231
Total loss for epoch 1: 636.101089
	Epoch 2....
Epoch has taken 0:00:11.004047
Number of used sentences in train = 231
Total loss for epoch 2: 436.962824
	Epoch 3....
Epoch has taken 0:00:11.000884
Number of used sentences in train = 231
Total loss for epoch 3: 378.050840
	Epoch 4....
Epoch has taken 0:00:10.993487
Number of used sentences in train = 231
Total loss for epoch 4: 362.459664
	Epoch 5....
Epoch has taken 0:00:11.011208
Number of used sentences in train = 231
Total loss for epoch 5: 355.745468
	Epoch 6....
Epoch has taken 0:00:10.991990
Number of used sentences in train = 231
Total loss for epoch 6: 352.941732
	Epoch 7....
Epoch has taken 0:00:10.998368
Number of used sentences in train = 231
Total loss for epoch 7: 350.612049
	Epoch 8....
Epoch has taken 0:00:10.983681
Number of used sentences in train = 231
Total loss for epoch 8: 349.201952
	Epoch 9....
Epoch has taken 0:00:10.999676
Number of used sentences in train = 231
Total loss for epoch 9: 348.286496
	Epoch 10....
Epoch has taken 0:00:10.985614
Number of used sentences in train = 231
Total loss for epoch 10: 347.681826
	Epoch 11....
Epoch has taken 0:00:11.003411
Number of used sentences in train = 231
Total loss for epoch 11: 347.240778
	Epoch 12....
Epoch has taken 0:00:10.982710
Number of used sentences in train = 231
Total loss for epoch 12: 346.854564
	Epoch 13....
Epoch has taken 0:00:10.993210
Number of used sentences in train = 231
Total loss for epoch 13: 346.868465
	Epoch 14....
Epoch has taken 0:00:10.979638
Number of used sentences in train = 231
Total loss for epoch 14: 346.487306
Epoch has taken 0:00:10.983842

==================================================================================================
	Training time : 0:31:37.539321
==================================================================================================
	Identification : 0.249

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 29)
  (w_embeddings): Embedding(6880, 106)
  (lstm): LSTM(135, 37, bidirectional=True)
  (linear1): Linear(in_features=592, out_features=46, bias=True)
  (linear2): Linear(in_features=46, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16275.117872
validation loss after epoch 0 : 1507.526547
	Epoch 1....
Epoch has taken 0:03:34.757247
Number of used sentences in train = 3226
Total loss for epoch 1: 11756.090790
validation loss after epoch 1 : 1463.089317
	Epoch 2....
Epoch has taken 0:04:03.467539
Number of used sentences in train = 3226
Total loss for epoch 2: 10086.933873
validation loss after epoch 2 : 1542.171122
	Epoch 3....
Epoch has taken 0:03:34.205963
Number of used sentences in train = 3226
Total loss for epoch 3: 8914.954292
validation loss after epoch 3 : 1611.915019
	Epoch 4....
Epoch has taken 0:03:32.800128
Number of used sentences in train = 3226
Total loss for epoch 4: 8031.750018
validation loss after epoch 4 : 1803.807159
	Epoch 5....
Epoch has taken 0:03:32.741804
Number of used sentences in train = 3226
Total loss for epoch 5: 7458.330304
validation loss after epoch 5 : 1906.068880
	Epoch 6....
Epoch has taken 0:03:34.583933
Number of used sentences in train = 3226
Total loss for epoch 6: 7025.972595
validation loss after epoch 6 : 2110.780711
	Epoch 7....
Epoch has taken 0:03:32.658953
Number of used sentences in train = 3226
Total loss for epoch 7: 6735.349141
validation loss after epoch 7 : 2205.242705
	Epoch 8....
Epoch has taken 0:03:34.772094
Number of used sentences in train = 3226
Total loss for epoch 8: 6557.135056
validation loss after epoch 8 : 2315.696389
	Epoch 9....
Epoch has taken 0:04:07.971106
Number of used sentences in train = 3226
Total loss for epoch 9: 6439.531044
validation loss after epoch 9 : 2457.881526
	Epoch 10....
Epoch has taken 0:03:34.328013
Number of used sentences in train = 3226
Total loss for epoch 10: 6361.868118
validation loss after epoch 10 : 2498.013693
	Epoch 11....
Epoch has taken 0:03:32.793761
Number of used sentences in train = 3226
Total loss for epoch 11: 6304.939586
validation loss after epoch 11 : 2515.767608
	Epoch 12....
Epoch has taken 0:03:34.976654
Number of used sentences in train = 3226
Total loss for epoch 12: 6257.561866
validation loss after epoch 12 : 2569.169576
	Epoch 13....
Epoch has taken 0:03:34.571334
Number of used sentences in train = 3226
Total loss for epoch 13: 6238.710674
validation loss after epoch 13 : 2598.994516
	Epoch 14....
Epoch has taken 0:03:32.459681
Number of used sentences in train = 3226
Total loss for epoch 14: 6225.272150
validation loss after epoch 14 : 2584.203016
	TransitionClassifier(
  (p_embeddings): Embedding(13, 29)
  (w_embeddings): Embedding(6880, 106)
  (lstm): LSTM(135, 37, bidirectional=True)
  (linear1): Linear(in_features=592, out_features=46, bias=True)
  (linear2): Linear(in_features=46, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:32.781861
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2084.329400
	Epoch 1....
Epoch has taken 0:00:20.762076
Number of used sentences in train = 359
Total loss for epoch 1: 1115.824081
	Epoch 2....
Epoch has taken 0:00:20.737227
Number of used sentences in train = 359
Total loss for epoch 2: 828.848699
	Epoch 3....
Epoch has taken 0:00:20.739673
Number of used sentences in train = 359
Total loss for epoch 3: 732.237828
	Epoch 4....
Epoch has taken 0:00:20.701928
Number of used sentences in train = 359
Total loss for epoch 4: 692.009694
	Epoch 5....
Epoch has taken 0:00:20.731660
Number of used sentences in train = 359
Total loss for epoch 5: 678.407298
	Epoch 6....
Epoch has taken 0:00:20.730496
Number of used sentences in train = 359
Total loss for epoch 6: 675.804054
	Epoch 7....
Epoch has taken 0:00:20.721517
Number of used sentences in train = 359
Total loss for epoch 7: 673.759475
	Epoch 8....
Epoch has taken 0:00:20.713203
Number of used sentences in train = 359
Total loss for epoch 8: 672.834476
	Epoch 9....
Epoch has taken 0:00:20.732834
Number of used sentences in train = 359
Total loss for epoch 9: 672.325569
	Epoch 10....
Epoch has taken 0:00:20.728673
Number of used sentences in train = 359
Total loss for epoch 10: 671.957665
	Epoch 11....
Epoch has taken 0:00:20.721056
Number of used sentences in train = 359
Total loss for epoch 11: 671.690918
	Epoch 12....
Epoch has taken 0:00:20.716283
Number of used sentences in train = 359
Total loss for epoch 12: 671.481608
	Epoch 13....
Epoch has taken 0:00:20.934255
Number of used sentences in train = 359
Total loss for epoch 13: 671.315832
	Epoch 14....
Epoch has taken 0:00:20.954150
Number of used sentences in train = 359
Total loss for epoch 14: 671.176773
Epoch has taken 0:00:20.936464

==================================================================================================
	Training time : 0:59:42.095687
==================================================================================================
	Identification : 0.335

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 31, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 15, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 105, 'lstmDropout': 0.25, 'denseActivation': 'tanh', 'wordDim': 62, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1177, 62)
  (lstm): LSTM(77, 105, bidirectional=True)
  (linear1): Linear(in_features=1680, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
	Epoch 0....
Number of used sentences in train = 2811
Total loss for epoch 0: 12773.115070
validation loss after epoch 0 : 1028.171548
	Epoch 1....
Epoch has taken 0:02:38.472913
Number of used sentences in train = 2811
Total loss for epoch 1: 8465.461068
validation loss after epoch 1 : 903.182374
	Epoch 2....
Epoch has taken 0:02:37.892355
Number of used sentences in train = 2811
Total loss for epoch 2: 7609.361452
validation loss after epoch 2 : 876.106613
	Epoch 3....
Epoch has taken 0:02:38.286344
Number of used sentences in train = 2811
Total loss for epoch 3: 6881.415428
validation loss after epoch 3 : 868.417104
	Epoch 4....
Epoch has taken 0:02:39.780296
Number of used sentences in train = 2811
Total loss for epoch 4: 6374.773533
validation loss after epoch 4 : 887.630959
	Epoch 5....
Epoch has taken 0:02:39.472527
Number of used sentences in train = 2811
Total loss for epoch 5: 6025.520940
validation loss after epoch 5 : 887.331707
	Epoch 6....
Epoch has taken 0:02:38.071867
Number of used sentences in train = 2811
Total loss for epoch 6: 5658.030668
validation loss after epoch 6 : 897.476421
	Epoch 7....
Epoch has taken 0:02:37.639327
Number of used sentences in train = 2811
Total loss for epoch 7: 5405.506764
validation loss after epoch 7 : 918.825887
	Epoch 8....
Epoch has taken 0:02:39.456628
Number of used sentences in train = 2811
Total loss for epoch 8: 5199.617472
validation loss after epoch 8 : 954.307589
	Epoch 9....
Epoch has taken 0:02:39.092955
Number of used sentences in train = 2811
Total loss for epoch 9: 5094.392240
validation loss after epoch 9 : 968.467236
	Epoch 10....
Epoch has taken 0:02:39.166697
Number of used sentences in train = 2811
Total loss for epoch 10: 4972.293836
validation loss after epoch 10 : 986.001725
	Epoch 11....
Epoch has taken 0:02:39.063952
Number of used sentences in train = 2811
Total loss for epoch 11: 4864.100492
validation loss after epoch 11 : 998.876533
	Epoch 12....
Epoch has taken 0:02:37.660583
Number of used sentences in train = 2811
Total loss for epoch 12: 4806.763860
validation loss after epoch 12 : 1003.997805
	Epoch 13....
Epoch has taken 0:02:37.531614
Number of used sentences in train = 2811
Total loss for epoch 13: 4766.707342
validation loss after epoch 13 : 1034.882574
	Epoch 14....
Epoch has taken 0:02:39.315417
Number of used sentences in train = 2811
Total loss for epoch 14: 4730.700569
validation loss after epoch 14 : 1031.387134
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1177, 62)
  (lstm): LSTM(77, 105, bidirectional=True)
  (linear1): Linear(in_features=1680, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:39.132394
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1424.535818
	Epoch 1....
Epoch has taken 0:00:19.350819
Number of used sentences in train = 313
Total loss for epoch 1: 867.865845
	Epoch 2....
Epoch has taken 0:00:19.346191
Number of used sentences in train = 313
Total loss for epoch 2: 749.031543
	Epoch 3....
Epoch has taken 0:00:19.343617
Number of used sentences in train = 313
Total loss for epoch 3: 677.459019
	Epoch 4....
Epoch has taken 0:00:19.349850
Number of used sentences in train = 313
Total loss for epoch 4: 621.787100
	Epoch 5....
Epoch has taken 0:00:19.350127
Number of used sentences in train = 313
Total loss for epoch 5: 584.564344
	Epoch 6....
Epoch has taken 0:00:16.805440
Number of used sentences in train = 313
Total loss for epoch 6: 574.876641
	Epoch 7....
Epoch has taken 0:00:16.803030
Number of used sentences in train = 313
Total loss for epoch 7: 557.651420
	Epoch 8....
Epoch has taken 0:00:16.797175
Number of used sentences in train = 313
Total loss for epoch 8: 547.852641
	Epoch 9....
Epoch has taken 0:00:16.818456
Number of used sentences in train = 313
Total loss for epoch 9: 546.226757
	Epoch 10....
Epoch has taken 0:00:16.804370
Number of used sentences in train = 313
Total loss for epoch 10: 537.210639
	Epoch 11....
Epoch has taken 0:00:16.800272
Number of used sentences in train = 313
Total loss for epoch 11: 533.368075
	Epoch 12....
Epoch has taken 0:00:16.817822
Number of used sentences in train = 313
Total loss for epoch 12: 533.425130
	Epoch 13....
Epoch has taken 0:00:16.789681
Number of used sentences in train = 313
Total loss for epoch 13: 530.224812
	Epoch 14....
Epoch has taken 0:00:16.783628
Number of used sentences in train = 313
Total loss for epoch 14: 529.315013
Epoch has taken 0:00:16.815854

==================================================================================================
	Training time : 0:44:05.319914
==================================================================================================
	Identification : 0.076

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1133, 62)
  (lstm): LSTM(77, 105, bidirectional=True)
  (linear1): Linear(in_features=1680, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9350.258826
validation loss after epoch 0 : 690.579830
	Epoch 1....
Epoch has taken 0:01:47.978999
Number of used sentences in train = 2074
Total loss for epoch 1: 5623.104259
validation loss after epoch 1 : 656.705172
	Epoch 2....
Epoch has taken 0:01:47.877264
Number of used sentences in train = 2074
Total loss for epoch 2: 4846.884894
validation loss after epoch 2 : 641.613382
	Epoch 3....
Epoch has taken 0:01:49.010844
Number of used sentences in train = 2074
Total loss for epoch 3: 4335.138577
validation loss after epoch 3 : 605.568558
	Epoch 4....
Epoch has taken 0:01:48.904708
Number of used sentences in train = 2074
Total loss for epoch 4: 3911.909082
validation loss after epoch 4 : 638.845558
	Epoch 5....
Epoch has taken 0:01:48.780048
Number of used sentences in train = 2074
Total loss for epoch 5: 3652.023706
validation loss after epoch 5 : 666.869502
	Epoch 6....
Epoch has taken 0:01:47.999105
Number of used sentences in train = 2074
Total loss for epoch 6: 3477.203186
validation loss after epoch 6 : 670.549059
	Epoch 7....
Epoch has taken 0:01:49.239545
Number of used sentences in train = 2074
Total loss for epoch 7: 3354.793341
validation loss after epoch 7 : 681.573090
	Epoch 8....
Epoch has taken 0:01:48.973233
Number of used sentences in train = 2074
Total loss for epoch 8: 3296.935169
validation loss after epoch 8 : 705.901984
	Epoch 9....
Epoch has taken 0:02:05.894209
Number of used sentences in train = 2074
Total loss for epoch 9: 3256.705048
validation loss after epoch 9 : 715.685002
	Epoch 10....
Epoch has taken 0:01:49.097654
Number of used sentences in train = 2074
Total loss for epoch 10: 3224.905565
validation loss after epoch 10 : 728.432866
	Epoch 11....
Epoch has taken 0:01:48.088253
Number of used sentences in train = 2074
Total loss for epoch 11: 3210.145754
validation loss after epoch 11 : 748.346528
	Epoch 12....
Epoch has taken 0:01:49.100139
Number of used sentences in train = 2074
Total loss for epoch 12: 3200.231346
validation loss after epoch 12 : 752.799505
	Epoch 13....
Epoch has taken 0:01:49.146721
Number of used sentences in train = 2074
Total loss for epoch 13: 3194.099773
validation loss after epoch 13 : 761.693704
	Epoch 14....
Epoch has taken 0:01:48.922145
Number of used sentences in train = 2074
Total loss for epoch 14: 3188.776457
validation loss after epoch 14 : 765.220070
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1133, 62)
  (lstm): LSTM(77, 105, bidirectional=True)
  (linear1): Linear(in_features=1680, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.877125
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1211.613845
	Epoch 1....
Epoch has taken 0:00:10.966892
Number of used sentences in train = 231
Total loss for epoch 1: 572.961233
	Epoch 2....
Epoch has taken 0:00:10.981930
Number of used sentences in train = 231
Total loss for epoch 2: 456.567077
	Epoch 3....
Epoch has taken 0:00:10.964656
Number of used sentences in train = 231
Total loss for epoch 3: 384.445879
	Epoch 4....
Epoch has taken 0:00:10.993146
Number of used sentences in train = 231
Total loss for epoch 4: 366.365193
	Epoch 5....
Epoch has taken 0:00:10.961484
Number of used sentences in train = 231
Total loss for epoch 5: 355.726274
	Epoch 6....
Epoch has taken 0:00:10.980707
Number of used sentences in train = 231
Total loss for epoch 6: 351.872885
	Epoch 7....
Epoch has taken 0:00:10.981195
Number of used sentences in train = 231
Total loss for epoch 7: 349.201952
	Epoch 8....
Epoch has taken 0:00:10.977360
Number of used sentences in train = 231
Total loss for epoch 8: 348.336801
	Epoch 9....
Epoch has taken 0:00:10.988901
Number of used sentences in train = 231
Total loss for epoch 9: 347.749259
	Epoch 10....
Epoch has taken 0:00:10.980146
Number of used sentences in train = 231
Total loss for epoch 10: 347.424629
	Epoch 11....
Epoch has taken 0:00:10.993471
Number of used sentences in train = 231
Total loss for epoch 11: 347.163749
	Epoch 12....
Epoch has taken 0:00:10.985944
Number of used sentences in train = 231
Total loss for epoch 12: 346.917641
	Epoch 13....
Epoch has taken 0:00:10.989875
Number of used sentences in train = 231
Total loss for epoch 13: 346.714006
	Epoch 14....
Epoch has taken 0:00:10.974501
Number of used sentences in train = 231
Total loss for epoch 14: 346.513194
Epoch has taken 0:00:10.985251

==================================================================================================
	Training time : 0:30:12.934392
==================================================================================================
	Identification : 0.422

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(1202, 62)
  (lstm): LSTM(77, 105, bidirectional=True)
  (linear1): Linear(in_features=1680, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14683.650099
validation loss after epoch 0 : 1355.247517
	Epoch 1....
Epoch has taken 0:03:54.991686
Number of used sentences in train = 3226
Total loss for epoch 1: 11471.628122
validation loss after epoch 1 : 1293.366659
	Epoch 2....
Epoch has taken 0:04:05.850035
Number of used sentences in train = 3226
Total loss for epoch 2: 10267.527455
validation loss after epoch 2 : 1270.002006
	Epoch 3....
Epoch has taken 0:03:32.809261
Number of used sentences in train = 3226
Total loss for epoch 3: 9527.014252
validation loss after epoch 3 : 1324.559756
	Epoch 4....
Epoch has taken 0:03:32.607281
Number of used sentences in train = 3226
Total loss for epoch 4: 8966.404941
validation loss after epoch 4 : 1375.770671
	Epoch 5....
Epoch has taken 0:03:30.967562
Number of used sentences in train = 3226
Total loss for epoch 5: 8441.762501
validation loss after epoch 5 : 1441.987945
	Epoch 6....
Epoch has taken 0:03:32.863199
Number of used sentences in train = 3226
Total loss for epoch 6: 8127.597849
validation loss after epoch 6 : 1463.252663
	Epoch 7....
Epoch has taken 0:03:32.721321
Number of used sentences in train = 3226
Total loss for epoch 7: 7765.869643
validation loss after epoch 7 : 1482.426055
	Epoch 8....
Epoch has taken 0:04:05.727452
Number of used sentences in train = 3226
Total loss for epoch 8: 7464.942139
validation loss after epoch 8 : 1564.678808
	Epoch 9....
Epoch has taken 0:03:32.129404
Number of used sentences in train = 3226
Total loss for epoch 9: 7221.825231
validation loss after epoch 9 : 1639.069349
	Epoch 10....
Epoch has taken 0:03:32.706431
Number of used sentences in train = 3226
Total loss for epoch 10: 7005.726090
validation loss after epoch 10 : 1700.431069
	Epoch 11....
Epoch has taken 0:03:32.961171
Number of used sentences in train = 3226
Total loss for epoch 11: 6875.642956
validation loss after epoch 11 : 1721.502145
	Epoch 12....
Epoch has taken 0:04:05.687596
Number of used sentences in train = 3226
Total loss for epoch 12: 6731.758711
validation loss after epoch 12 : 1796.867924
	Epoch 13....
Epoch has taken 0:03:32.418618
Number of used sentences in train = 3226
Total loss for epoch 13: 6618.650679
validation loss after epoch 13 : 1854.466724
	Epoch 14....
Epoch has taken 0:03:30.728247
Number of used sentences in train = 3226
Total loss for epoch 14: 6510.872170
validation loss after epoch 14 : 1913.158648
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(1202, 62)
  (lstm): LSTM(77, 105, bidirectional=True)
  (linear1): Linear(in_features=1680, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:33.059009
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2186.154567
	Epoch 1....
Epoch has taken 0:00:23.820787
Number of used sentences in train = 359
Total loss for epoch 1: 1247.124617
	Epoch 2....
Epoch has taken 0:00:23.815592
Number of used sentences in train = 359
Total loss for epoch 2: 1020.956780
	Epoch 3....
Epoch has taken 0:00:23.813492
Number of used sentences in train = 359
Total loss for epoch 3: 925.588641
	Epoch 4....
Epoch has taken 0:00:23.812796
Number of used sentences in train = 359
Total loss for epoch 4: 833.874540
	Epoch 5....
Epoch has taken 0:00:23.827601
Number of used sentences in train = 359
Total loss for epoch 5: 776.608319
	Epoch 6....
Epoch has taken 0:00:23.831335
Number of used sentences in train = 359
Total loss for epoch 6: 741.912400
	Epoch 7....
Epoch has taken 0:00:23.820345
Number of used sentences in train = 359
Total loss for epoch 7: 715.254070
	Epoch 8....
Epoch has taken 0:00:23.855685
Number of used sentences in train = 359
Total loss for epoch 8: 693.543996
	Epoch 9....
Epoch has taken 0:00:23.796607
Number of used sentences in train = 359
Total loss for epoch 9: 688.199503
	Epoch 10....
Epoch has taken 0:00:23.802669
Number of used sentences in train = 359
Total loss for epoch 10: 692.371297
	Epoch 11....
Epoch has taken 0:00:23.798754
Number of used sentences in train = 359
Total loss for epoch 11: 680.350236
	Epoch 12....
Epoch has taken 0:00:23.821955
Number of used sentences in train = 359
Total loss for epoch 12: 686.916720
	Epoch 13....
Epoch has taken 0:00:23.848881
Number of used sentences in train = 359
Total loss for epoch 13: 675.905479
	Epoch 14....
Epoch has taken 0:00:23.819177
Number of used sentences in train = 359
Total loss for epoch 14: 675.014213
Epoch has taken 0:00:23.826273

==================================================================================================
	Training time : 1:01:06.201142
==================================================================================================
	Identification : 0.428

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 33, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 38, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 23, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 58, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(1177, 58)
  (lstm): LSTM(96, 23, bidirectional=True)
  (linear1): Linear(in_features=368, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10868.313734
validation loss after epoch 0 : 953.357097
	Epoch 1....
Epoch has taken 0:02:36.776387
Number of used sentences in train = 2811
Total loss for epoch 1: 7886.844867
validation loss after epoch 1 : 925.391868
	Epoch 2....
Epoch has taken 0:02:36.882388
Number of used sentences in train = 2811
Total loss for epoch 2: 7011.599583
validation loss after epoch 2 : 930.617690
	Epoch 3....
Epoch has taken 0:02:37.584559
Number of used sentences in train = 2811
Total loss for epoch 3: 6436.510871
validation loss after epoch 3 : 927.391576
	Epoch 4....
Epoch has taken 0:02:37.765692
Number of used sentences in train = 2811
Total loss for epoch 4: 6032.887196
validation loss after epoch 4 : 951.904514
	Epoch 5....
Epoch has taken 0:02:39.299152
Number of used sentences in train = 2811
Total loss for epoch 5: 5710.731484
validation loss after epoch 5 : 1000.356048
	Epoch 6....
Epoch has taken 0:02:39.021732
Number of used sentences in train = 2811
Total loss for epoch 6: 5485.156829
validation loss after epoch 6 : 1040.112243
	Epoch 7....
Epoch has taken 0:02:38.706655
Number of used sentences in train = 2811
Total loss for epoch 7: 5262.616767
validation loss after epoch 7 : 1091.892697
	Epoch 8....
Epoch has taken 0:02:37.610903
Number of used sentences in train = 2811
Total loss for epoch 8: 5116.318341
validation loss after epoch 8 : 1088.928686
	Epoch 9....
Epoch has taken 0:02:37.406705
Number of used sentences in train = 2811
Total loss for epoch 9: 4992.271442
validation loss after epoch 9 : 1165.708823
	Epoch 10....
Epoch has taken 0:02:39.015250
Number of used sentences in train = 2811
Total loss for epoch 10: 4891.519546
validation loss after epoch 10 : 1163.740551
	Epoch 11....
Epoch has taken 0:02:39.068050
Number of used sentences in train = 2811
Total loss for epoch 11: 4826.370624
validation loss after epoch 11 : 1248.896646
	Epoch 12....
Epoch has taken 0:03:04.028643
Number of used sentences in train = 2811
Total loss for epoch 12: 4768.979614
validation loss after epoch 12 : 1230.887756
	Epoch 13....
Epoch has taken 0:02:39.047025
Number of used sentences in train = 2811
Total loss for epoch 13: 4714.675660
validation loss after epoch 13 : 1260.615699
	Epoch 14....
Epoch has taken 0:02:38.778501
Number of used sentences in train = 2811
Total loss for epoch 14: 4674.057302
validation loss after epoch 14 : 1294.794701
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(1177, 58)
  (lstm): LSTM(96, 23, bidirectional=True)
  (linear1): Linear(in_features=368, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:37.677065
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1428.913755
	Epoch 1....
Epoch has taken 0:00:16.620904
Number of used sentences in train = 313
Total loss for epoch 1: 727.124841
	Epoch 2....
Epoch has taken 0:00:16.640951
Number of used sentences in train = 313
Total loss for epoch 2: 610.744328
	Epoch 3....
Epoch has taken 0:00:16.628253
Number of used sentences in train = 313
Total loss for epoch 3: 558.701801
	Epoch 4....
Epoch has taken 0:00:16.617408
Number of used sentences in train = 313
Total loss for epoch 4: 535.096515
	Epoch 5....
Epoch has taken 0:00:16.648001
Number of used sentences in train = 313
Total loss for epoch 5: 522.710642
	Epoch 6....
Epoch has taken 0:00:16.627485
Number of used sentences in train = 313
Total loss for epoch 6: 512.557794
	Epoch 7....
Epoch has taken 0:00:16.625571
Number of used sentences in train = 313
Total loss for epoch 7: 507.953246
	Epoch 8....
Epoch has taken 0:00:16.817263
Number of used sentences in train = 313
Total loss for epoch 8: 507.593120
	Epoch 9....
Epoch has taken 0:00:16.818256
Number of used sentences in train = 313
Total loss for epoch 9: 508.455139
	Epoch 10....
Epoch has taken 0:00:16.782044
Number of used sentences in train = 313
Total loss for epoch 10: 504.180871
	Epoch 11....
Epoch has taken 0:00:16.794170
Number of used sentences in train = 313
Total loss for epoch 11: 503.333889
	Epoch 12....
Epoch has taken 0:00:16.815161
Number of used sentences in train = 313
Total loss for epoch 12: 502.722836
	Epoch 13....
Epoch has taken 0:00:16.788387
Number of used sentences in train = 313
Total loss for epoch 13: 502.322003
	Epoch 14....
Epoch has taken 0:00:16.801401
Number of used sentences in train = 313
Total loss for epoch 14: 502.220313
Epoch has taken 0:00:16.821195

==================================================================================================
	Training time : 0:44:10.016715
==================================================================================================
	Identification : 0.397

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(1133, 58)
  (lstm): LSTM(96, 23, bidirectional=True)
  (linear1): Linear(in_features=368, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8020.046088
validation loss after epoch 0 : 702.475354
	Epoch 1....
Epoch has taken 0:02:05.223348
Number of used sentences in train = 2074
Total loss for epoch 1: 5536.273712
validation loss after epoch 1 : 689.693155
	Epoch 2....
Epoch has taken 0:02:05.251375
Number of used sentences in train = 2074
Total loss for epoch 2: 4812.666829
validation loss after epoch 2 : 679.690573
	Epoch 3....
Epoch has taken 0:01:47.390363
Number of used sentences in train = 2074
Total loss for epoch 3: 4334.533329
validation loss after epoch 3 : 702.074826
	Epoch 4....
Epoch has taken 0:01:47.459259
Number of used sentences in train = 2074
Total loss for epoch 4: 4011.021631
validation loss after epoch 4 : 723.479992
	Epoch 5....
Epoch has taken 0:01:48.412372
Number of used sentences in train = 2074
Total loss for epoch 5: 3792.451956
validation loss after epoch 5 : 760.758601
	Epoch 6....
Epoch has taken 0:02:02.700586
Number of used sentences in train = 2074
Total loss for epoch 6: 3620.080015
validation loss after epoch 6 : 761.627299
	Epoch 7....
Epoch has taken 0:01:48.316473
Number of used sentences in train = 2074
Total loss for epoch 7: 3478.844044
validation loss after epoch 7 : 819.617644
	Epoch 8....
Epoch has taken 0:01:47.360005
Number of used sentences in train = 2074
Total loss for epoch 8: 3389.502220
validation loss after epoch 8 : 843.410635
	Epoch 9....
Epoch has taken 0:01:48.339093
Number of used sentences in train = 2074
Total loss for epoch 9: 3331.699498
validation loss after epoch 9 : 867.852072
	Epoch 10....
Epoch has taken 0:01:48.583634
Number of used sentences in train = 2074
Total loss for epoch 10: 3290.282449
validation loss after epoch 10 : 900.713683
	Epoch 11....
Epoch has taken 0:02:05.317758
Number of used sentences in train = 2074
Total loss for epoch 11: 3257.918070
validation loss after epoch 11 : 905.579555
	Epoch 12....
Epoch has taken 0:01:48.214153
Number of used sentences in train = 2074
Total loss for epoch 12: 3232.412697
validation loss after epoch 12 : 943.465709
	Epoch 13....
Epoch has taken 0:01:48.230828
Number of used sentences in train = 2074
Total loss for epoch 13: 3218.675221
validation loss after epoch 13 : 975.578191
	Epoch 14....
Epoch has taken 0:01:47.241771
Number of used sentences in train = 2074
Total loss for epoch 14: 3204.324723
validation loss after epoch 14 : 977.266250
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(1133, 58)
  (lstm): LSTM(96, 23, bidirectional=True)
  (linear1): Linear(in_features=368, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:47.030221
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1396.932692
	Epoch 1....
Epoch has taken 0:00:10.989585
Number of used sentences in train = 231
Total loss for epoch 1: 535.387862
	Epoch 2....
Epoch has taken 0:00:10.983123
Number of used sentences in train = 231
Total loss for epoch 2: 426.103077
	Epoch 3....
Epoch has taken 0:00:11.003754
Number of used sentences in train = 231
Total loss for epoch 3: 384.173411
	Epoch 4....
Epoch has taken 0:00:10.985460
Number of used sentences in train = 231
Total loss for epoch 4: 366.241179
	Epoch 5....
Epoch has taken 0:00:11.006981
Number of used sentences in train = 231
Total loss for epoch 5: 358.628982
	Epoch 6....
Epoch has taken 0:00:10.990036
Number of used sentences in train = 231
Total loss for epoch 6: 353.929667
	Epoch 7....
Epoch has taken 0:00:10.988798
Number of used sentences in train = 231
Total loss for epoch 7: 350.545504
	Epoch 8....
Epoch has taken 0:00:10.988878
Number of used sentences in train = 231
Total loss for epoch 8: 349.283338
	Epoch 9....
Epoch has taken 0:00:11.014873
Number of used sentences in train = 231
Total loss for epoch 9: 348.362993
	Epoch 10....
Epoch has taken 0:00:10.986943
Number of used sentences in train = 231
Total loss for epoch 10: 347.538799
	Epoch 11....
Epoch has taken 0:00:10.996528
Number of used sentences in train = 231
Total loss for epoch 11: 347.063371
	Epoch 12....
Epoch has taken 0:00:10.994008
Number of used sentences in train = 231
Total loss for epoch 12: 346.642518
	Epoch 13....
Epoch has taken 0:00:10.989642
Number of used sentences in train = 231
Total loss for epoch 13: 346.337075
	Epoch 14....
Epoch has taken 0:00:11.000968
Number of used sentences in train = 231
Total loss for epoch 14: 346.081570
Epoch has taken 0:00:10.968001

==================================================================================================
	Training time : 0:30:50.292617
==================================================================================================
	Identification : 0.262

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(1202, 58)
  (lstm): LSTM(96, 23, bidirectional=True)
  (linear1): Linear(in_features=368, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14516.817257
validation loss after epoch 0 : 1399.450857
	Epoch 1....
Epoch has taken 0:03:30.743683
Number of used sentences in train = 3226
Total loss for epoch 1: 11813.741404
validation loss after epoch 1 : 1344.829106
	Epoch 2....
Epoch has taken 0:03:30.881407
Number of used sentences in train = 3226
Total loss for epoch 2: 10857.689824
validation loss after epoch 2 : 1323.504953
	Epoch 3....
Epoch has taken 0:03:29.533633
Number of used sentences in train = 3226
Total loss for epoch 3: 10160.170116
validation loss after epoch 3 : 1356.443345
	Epoch 4....
Epoch has taken 0:03:45.190372
Number of used sentences in train = 3226
Total loss for epoch 4: 9593.525040
validation loss after epoch 4 : 1366.259072
	Epoch 5....
Epoch has taken 0:03:54.467930
Number of used sentences in train = 3226
Total loss for epoch 5: 9172.927634
validation loss after epoch 5 : 1416.612879
	Epoch 6....
Epoch has taken 0:03:52.806189
Number of used sentences in train = 3226
Total loss for epoch 6: 8758.102347
validation loss after epoch 6 : 1440.029297
	Epoch 7....
Epoch has taken 0:03:52.750761
Number of used sentences in train = 3226
Total loss for epoch 7: 8444.407900
validation loss after epoch 7 : 1482.286597
	Epoch 8....
Epoch has taken 0:03:57.877940
Number of used sentences in train = 3226
Total loss for epoch 8: 8208.271700
validation loss after epoch 8 : 1549.067443
	Epoch 9....
Epoch has taken 0:03:31.570101
Number of used sentences in train = 3226
Total loss for epoch 9: 7971.323314
validation loss after epoch 9 : 1611.485427
	Epoch 10....
Epoch has taken 0:03:29.736268
Number of used sentences in train = 3226
Total loss for epoch 10: 7778.209570
validation loss after epoch 10 : 1602.905513
	Epoch 11....
Epoch has taken 0:03:29.880087
Number of used sentences in train = 3226
Total loss for epoch 11: 7589.595203
validation loss after epoch 11 : 1643.293069
	Epoch 12....
Epoch has taken 0:03:29.612102
Number of used sentences in train = 3226
Total loss for epoch 12: 7423.662914
validation loss after epoch 12 : 1755.463098
	Epoch 13....
Epoch has taken 0:03:31.716215
Number of used sentences in train = 3226
Total loss for epoch 13: 7304.528220
validation loss after epoch 13 : 1830.313631
	Epoch 14....
Epoch has taken 0:04:04.599041
Number of used sentences in train = 3226
Total loss for epoch 14: 7189.682070
validation loss after epoch 14 : 1825.612341
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(1202, 58)
  (lstm): LSTM(96, 23, bidirectional=True)
  (linear1): Linear(in_features=368, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:31.182721
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1711.290702
	Epoch 1....
Epoch has taken 0:00:20.411197
Number of used sentences in train = 359
Total loss for epoch 1: 1178.940969
	Epoch 2....
Epoch has taken 0:00:20.400777
Number of used sentences in train = 359
Total loss for epoch 2: 983.881479
	Epoch 3....
Epoch has taken 0:00:20.400416
Number of used sentences in train = 359
Total loss for epoch 3: 864.069925
	Epoch 4....
Epoch has taken 0:00:20.406060
Number of used sentences in train = 359
Total loss for epoch 4: 803.745764
	Epoch 5....
Epoch has taken 0:00:20.409171
Number of used sentences in train = 359
Total loss for epoch 5: 754.221384
	Epoch 6....
Epoch has taken 0:00:20.414114
Number of used sentences in train = 359
Total loss for epoch 6: 728.127454
	Epoch 7....
Epoch has taken 0:00:20.433373
Number of used sentences in train = 359
Total loss for epoch 7: 715.283938
	Epoch 8....
Epoch has taken 0:00:20.424953
Number of used sentences in train = 359
Total loss for epoch 8: 704.350849
	Epoch 9....
Epoch has taken 0:00:20.402719
Number of used sentences in train = 359
Total loss for epoch 9: 694.581295
	Epoch 10....
Epoch has taken 0:00:20.428894
Number of used sentences in train = 359
Total loss for epoch 10: 691.423936
	Epoch 11....
Epoch has taken 0:00:20.424907
Number of used sentences in train = 359
Total loss for epoch 11: 689.987053
	Epoch 12....
Epoch has taken 0:00:20.424572
Number of used sentences in train = 359
Total loss for epoch 12: 686.023055
	Epoch 13....
Epoch has taken 0:00:20.402727
Number of used sentences in train = 359
Total loss for epoch 13: 680.564065
	Epoch 14....
Epoch has taken 0:00:20.429060
Number of used sentences in train = 359
Total loss for epoch 14: 679.311359
Epoch has taken 0:00:20.420238

==================================================================================================
	Training time : 1:00:09.433259
==================================================================================================
	Identification : 0.485

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 11, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 35, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 109, 'lstmDropout': 0.36, 'denseActivation': 'tanh', 'wordDim': 59, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1882, 59)
  (lstm): LSTM(94, 109, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15016.911151
validation loss after epoch 0 : 934.332039
	Epoch 1....
Epoch has taken 0:02:54.366338
Number of used sentences in train = 2811
Total loss for epoch 1: 8448.748732
validation loss after epoch 1 : 867.782920
	Epoch 2....
Epoch has taken 0:03:09.595616
Number of used sentences in train = 2811
Total loss for epoch 2: 7580.108778
validation loss after epoch 2 : 838.974235
	Epoch 3....
Epoch has taken 0:03:09.967667
Number of used sentences in train = 2811
Total loss for epoch 3: 7117.306618
validation loss after epoch 3 : 826.525712
	Epoch 4....
Epoch has taken 0:03:09.652326
Number of used sentences in train = 2811
Total loss for epoch 4: 6721.723080
validation loss after epoch 4 : 804.169634
	Epoch 5....
Epoch has taken 0:03:09.334114
Number of used sentences in train = 2811
Total loss for epoch 5: 6419.921103
validation loss after epoch 5 : 791.909957
	Epoch 6....
Epoch has taken 0:03:08.215992
Number of used sentences in train = 2811
Total loss for epoch 6: 6150.755475
validation loss after epoch 6 : 831.522038
	Epoch 7....
Epoch has taken 0:03:08.362411
Number of used sentences in train = 2811
Total loss for epoch 7: 5964.618274
validation loss after epoch 7 : 827.201966
	Epoch 8....
Epoch has taken 0:03:09.913635
Number of used sentences in train = 2811
Total loss for epoch 8: 5768.278203
validation loss after epoch 8 : 868.362550
	Epoch 9....
Epoch has taken 0:03:09.853026
Number of used sentences in train = 2811
Total loss for epoch 9: 5586.300593
validation loss after epoch 9 : 909.780745
	Epoch 10....
Epoch has taken 0:03:09.113210
Number of used sentences in train = 2811
Total loss for epoch 10: 5415.310954
validation loss after epoch 10 : 860.911259
	Epoch 11....
Epoch has taken 0:02:50.325532
Number of used sentences in train = 2811
Total loss for epoch 11: 5331.440158
validation loss after epoch 11 : 925.772466
	Epoch 12....
Epoch has taken 0:02:50.556791
Number of used sentences in train = 2811
Total loss for epoch 12: 5204.898429
validation loss after epoch 12 : 951.749432
	Epoch 13....
Epoch has taken 0:02:51.875152
Number of used sentences in train = 2811
Total loss for epoch 13: 5128.001742
validation loss after epoch 13 : 932.064151
	Epoch 14....
Epoch has taken 0:02:57.993569
Number of used sentences in train = 2811
Total loss for epoch 14: 5094.694305
validation loss after epoch 14 : 925.660402
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1882, 59)
  (lstm): LSTM(94, 109, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.288599
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1367.716704
	Epoch 1....
Epoch has taken 0:00:18.026238
Number of used sentences in train = 313
Total loss for epoch 1: 913.091566
	Epoch 2....
Epoch has taken 0:00:18.056381
Number of used sentences in train = 313
Total loss for epoch 2: 823.457610
	Epoch 3....
Epoch has taken 0:00:18.067156
Number of used sentences in train = 313
Total loss for epoch 3: 681.041443
	Epoch 4....
Epoch has taken 0:00:18.053407
Number of used sentences in train = 313
Total loss for epoch 4: 643.141967
	Epoch 5....
Epoch has taken 0:00:18.057897
Number of used sentences in train = 313
Total loss for epoch 5: 649.048341
	Epoch 6....
Epoch has taken 0:00:18.040130
Number of used sentences in train = 313
Total loss for epoch 6: 640.990133
	Epoch 7....
Epoch has taken 0:00:18.055191
Number of used sentences in train = 313
Total loss for epoch 7: 598.726734
	Epoch 8....
Epoch has taken 0:00:18.048046
Number of used sentences in train = 313
Total loss for epoch 8: 598.706736
	Epoch 9....
Epoch has taken 0:00:18.017481
Number of used sentences in train = 313
Total loss for epoch 9: 573.932591
	Epoch 10....
Epoch has taken 0:00:18.015545
Number of used sentences in train = 313
Total loss for epoch 10: 559.029418
	Epoch 11....
Epoch has taken 0:00:18.046435
Number of used sentences in train = 313
Total loss for epoch 11: 552.056088
	Epoch 12....
Epoch has taken 0:00:18.079185
Number of used sentences in train = 313
Total loss for epoch 12: 546.937021
	Epoch 13....
Epoch has taken 0:00:18.043088
Number of used sentences in train = 313
Total loss for epoch 13: 541.966669
	Epoch 14....
Epoch has taken 0:00:18.059644
Number of used sentences in train = 313
Total loss for epoch 14: 532.493080
Epoch has taken 0:00:18.041815

==================================================================================================
	Training time : 0:50:10.640647
==================================================================================================
	Identification : 0.475

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1680, 59)
  (lstm): LSTM(94, 109, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 21834.829019
validation loss after epoch 0 : 2384.316743
	Epoch 1....
Epoch has taken 0:01:56.689357
Number of used sentences in train = 2074
Total loss for epoch 1: 20872.783688
validation loss after epoch 1 : 2359.316335
	Epoch 2....
Epoch has taken 0:01:56.764479
Number of used sentences in train = 2074
Total loss for epoch 2: 20636.113988
validation loss after epoch 2 : 2348.170110
	Epoch 3....
Epoch has taken 0:01:57.956266
Number of used sentences in train = 2074
Total loss for epoch 3: 20599.140203
validation loss after epoch 3 : 2355.750636
	Epoch 4....
Epoch has taken 0:01:57.905246
Number of used sentences in train = 2074
Total loss for epoch 4: 18484.087090
validation loss after epoch 4 : 1144.507866
	Epoch 5....
Epoch has taken 0:02:13.734264
Number of used sentences in train = 2074
Total loss for epoch 5: 8623.646084
validation loss after epoch 5 : 827.664709
	Epoch 6....
Epoch has taken 0:01:56.910655
Number of used sentences in train = 2074
Total loss for epoch 6: 6861.142720
validation loss after epoch 6 : 732.330788
	Epoch 7....
Epoch has taken 0:01:57.849994
Number of used sentences in train = 2074
Total loss for epoch 7: 5767.536541
validation loss after epoch 7 : 673.363247
	Epoch 8....
Epoch has taken 0:01:57.856024
Number of used sentences in train = 2074
Total loss for epoch 8: 5271.597478
validation loss after epoch 8 : 663.830834
	Epoch 9....
Epoch has taken 0:01:57.899320
Number of used sentences in train = 2074
Total loss for epoch 9: 4834.436671
validation loss after epoch 9 : 730.883224
	Epoch 10....
Epoch has taken 0:01:57.451763
Number of used sentences in train = 2074
Total loss for epoch 10: 4634.350493
validation loss after epoch 10 : 722.995910
	Epoch 11....
Epoch has taken 0:01:56.901943
Number of used sentences in train = 2074
Total loss for epoch 11: 4468.090279
validation loss after epoch 11 : 671.415473
	Epoch 12....
Epoch has taken 0:01:56.613118
Number of used sentences in train = 2074
Total loss for epoch 12: 4173.202955
validation loss after epoch 12 : 696.365318
	Epoch 13....
Epoch has taken 0:01:59.988812
Number of used sentences in train = 2074
Total loss for epoch 13: 4017.126747
validation loss after epoch 13 : 689.646589
	Epoch 14....
Epoch has taken 0:02:11.994824
Number of used sentences in train = 2074
Total loss for epoch 14: 3943.984602
validation loss after epoch 14 : 749.282869
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1680, 59)
  (lstm): LSTM(94, 109, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:09.813980
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1501.561132
	Epoch 1....
Epoch has taken 0:00:13.230558
Number of used sentences in train = 231
Total loss for epoch 1: 877.509192
	Epoch 2....
Epoch has taken 0:00:13.262386
Number of used sentences in train = 231
Total loss for epoch 2: 686.951326
	Epoch 3....
Epoch has taken 0:00:13.225977
Number of used sentences in train = 231
Total loss for epoch 3: 625.964935
	Epoch 4....
Epoch has taken 0:00:13.224359
Number of used sentences in train = 231
Total loss for epoch 4: 523.315545
	Epoch 5....
Epoch has taken 0:00:12.601856
Number of used sentences in train = 231
Total loss for epoch 5: 476.149205
	Epoch 6....
Epoch has taken 0:00:11.882211
Number of used sentences in train = 231
Total loss for epoch 6: 454.768550
	Epoch 7....
Epoch has taken 0:00:11.868591
Number of used sentences in train = 231
Total loss for epoch 7: 418.042695
	Epoch 8....
Epoch has taken 0:00:11.869052
Number of used sentences in train = 231
Total loss for epoch 8: 411.834343
	Epoch 9....
Epoch has taken 0:00:11.880086
Number of used sentences in train = 231
Total loss for epoch 9: 391.490222
	Epoch 10....
Epoch has taken 0:00:11.900630
Number of used sentences in train = 231
Total loss for epoch 10: 379.861514
	Epoch 11....
Epoch has taken 0:00:11.835098
Number of used sentences in train = 231
Total loss for epoch 11: 378.650585
	Epoch 12....
Epoch has taken 0:00:11.868359
Number of used sentences in train = 231
Total loss for epoch 12: 384.581848
	Epoch 13....
Epoch has taken 0:00:11.847067
Number of used sentences in train = 231
Total loss for epoch 13: 381.781472
	Epoch 14....
Epoch has taken 0:00:11.873251
Number of used sentences in train = 231
Total loss for epoch 14: 399.378411
Epoch has taken 0:00:11.827701

==================================================================================================
	Training time : 0:33:10.869009
==================================================================================================
	Identification : 0.43

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(3369, 59)
  (lstm): LSTM(94, 109, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 38461.172396
validation loss after epoch 0 : 3990.025959
	Epoch 1....
Epoch has taken 0:03:54.434806
Number of used sentences in train = 3226
Total loss for epoch 1: 25554.044774
validation loss after epoch 1 : 1457.286309
	Epoch 2....
Epoch has taken 0:03:54.035328
Number of used sentences in train = 3226
Total loss for epoch 2: 11567.074618
validation loss after epoch 2 : 1194.136642
	Epoch 3....
Epoch has taken 0:03:50.585783
Number of used sentences in train = 3226
Total loss for epoch 3: 10073.450684
validation loss after epoch 3 : 1071.217625
	Epoch 4....
Epoch has taken 0:03:50.888792
Number of used sentences in train = 3226
Total loss for epoch 4: 9378.931637
validation loss after epoch 4 : 1090.444116
	Epoch 5....
Epoch has taken 0:03:48.897575
Number of used sentences in train = 3226
Total loss for epoch 5: 8903.263319
validation loss after epoch 5 : 1069.960047
	Epoch 6....
Epoch has taken 0:03:50.912437
Number of used sentences in train = 3226
Total loss for epoch 6: 8525.398420
validation loss after epoch 6 : 1048.434155
	Epoch 7....
Epoch has taken 0:03:49.785802
Number of used sentences in train = 3226
Total loss for epoch 7: 8258.432129
validation loss after epoch 7 : 1086.784230
	Epoch 8....
Epoch has taken 0:03:48.188785
Number of used sentences in train = 3226
Total loss for epoch 8: 7975.773705
validation loss after epoch 8 : 1086.625506
	Epoch 9....
Epoch has taken 0:03:49.924686
Number of used sentences in train = 3226
Total loss for epoch 9: 7785.727363
validation loss after epoch 9 : 1105.288716
	Epoch 10....
Epoch has taken 0:03:50.060713
Number of used sentences in train = 3226
Total loss for epoch 10: 7604.043387
validation loss after epoch 10 : 1181.435019
	Epoch 11....
Epoch has taken 0:04:24.335720
Number of used sentences in train = 3226
Total loss for epoch 11: 7424.963718
validation loss after epoch 11 : 1150.462244
	Epoch 12....
Epoch has taken 0:03:50.438742
Number of used sentences in train = 3226
Total loss for epoch 12: 7316.169707
validation loss after epoch 12 : 1148.126242
	Epoch 13....
Epoch has taken 0:03:49.716448
Number of used sentences in train = 3226
Total loss for epoch 13: 7149.417347
validation loss after epoch 13 : 1268.195170
	Epoch 14....
Epoch has taken 0:03:48.472277
Number of used sentences in train = 3226
Total loss for epoch 14: 7048.716264
validation loss after epoch 14 : 1236.369595
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(3369, 59)
  (lstm): LSTM(94, 109, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.973139
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2064.703297
	Epoch 1....
Epoch has taken 0:00:22.580343
Number of used sentences in train = 359
Total loss for epoch 1: 1282.089576
	Epoch 2....
Epoch has taken 0:00:22.595807
Number of used sentences in train = 359
Total loss for epoch 2: 1096.782859
	Epoch 3....
Epoch has taken 0:00:22.595890
Number of used sentences in train = 359
Total loss for epoch 3: 1043.292898
	Epoch 4....
Epoch has taken 0:00:22.606531
Number of used sentences in train = 359
Total loss for epoch 4: 986.297643
	Epoch 5....
Epoch has taken 0:00:22.568357
Number of used sentences in train = 359
Total loss for epoch 5: 932.597290
	Epoch 6....
Epoch has taken 0:00:22.585436
Number of used sentences in train = 359
Total loss for epoch 6: 940.423016
	Epoch 7....
Epoch has taken 0:00:22.617022
Number of used sentences in train = 359
Total loss for epoch 7: 906.821772
	Epoch 8....
Epoch has taken 0:00:22.615660
Number of used sentences in train = 359
Total loss for epoch 8: 883.320686
	Epoch 9....
Epoch has taken 0:00:22.550410
Number of used sentences in train = 359
Total loss for epoch 9: 843.141283
	Epoch 10....
Epoch has taken 0:00:22.548782
Number of used sentences in train = 359
Total loss for epoch 10: 840.495700
	Epoch 11....
Epoch has taken 0:00:22.571667
Number of used sentences in train = 359
Total loss for epoch 11: 868.098514
	Epoch 12....
Epoch has taken 0:00:22.566039
Number of used sentences in train = 359
Total loss for epoch 12: 804.030417
	Epoch 13....
Epoch has taken 0:00:24.516605
Number of used sentences in train = 359
Total loss for epoch 13: 809.206149
	Epoch 14....
Epoch has taken 0:00:24.522305
Number of used sentences in train = 359
Total loss for epoch 14: 770.031965
Epoch has taken 0:00:24.364575

==================================================================================================
	Training time : 1:03:54.744687
==================================================================================================
	Identification : 0.461

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 76, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 26, 'lstmDropout': 0.12, 'denseActivation': 'tanh', 'wordDim': 101, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(9217, 101)
  (lstm): LSTM(118, 26, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13637.937159
validation loss after epoch 0 : 1178.308874
	Epoch 1....
Epoch has taken 0:02:54.490379
Number of used sentences in train = 2811
Total loss for epoch 1: 8469.640933
validation loss after epoch 1 : 1178.982245
	Epoch 2....
Epoch has taken 0:02:54.423735
Number of used sentences in train = 2811
Total loss for epoch 2: 6569.410402
validation loss after epoch 2 : 1214.961026
	Epoch 3....
Epoch has taken 0:02:55.337349
Number of used sentences in train = 2811
Total loss for epoch 3: 5637.016534
validation loss after epoch 3 : 1321.406315
	Epoch 4....
Epoch has taken 0:02:53.612121
Number of used sentences in train = 2811
Total loss for epoch 4: 5121.016646
validation loss after epoch 4 : 1545.230441
	Epoch 5....
Epoch has taken 0:02:39.548790
Number of used sentences in train = 2811
Total loss for epoch 5: 4837.530002
validation loss after epoch 5 : 1605.976680
	Epoch 6....
Epoch has taken 0:02:39.401686
Number of used sentences in train = 2811
Total loss for epoch 6: 4664.254582
validation loss after epoch 6 : 1686.979314
	Epoch 7....
Epoch has taken 0:02:38.086096
Number of used sentences in train = 2811
Total loss for epoch 7: 4584.180974
validation loss after epoch 7 : 1724.163096
	Epoch 8....
Epoch has taken 0:02:39.738489
Number of used sentences in train = 2811
Total loss for epoch 8: 4544.051140
validation loss after epoch 8 : 1813.017395
	Epoch 9....
Epoch has taken 0:02:39.473195
Number of used sentences in train = 2811
Total loss for epoch 9: 4524.013567
validation loss after epoch 9 : 1826.173123
	Epoch 10....
Epoch has taken 0:02:39.471942
Number of used sentences in train = 2811
Total loss for epoch 10: 4516.319854
validation loss after epoch 10 : 1852.252168
	Epoch 11....
Epoch has taken 0:02:38.265120
Number of used sentences in train = 2811
Total loss for epoch 11: 4503.010311
validation loss after epoch 11 : 1892.142050
	Epoch 12....
Epoch has taken 0:02:38.206519
Number of used sentences in train = 2811
Total loss for epoch 12: 4497.962553
validation loss after epoch 12 : 1892.799426
	Epoch 13....
Epoch has taken 0:02:39.783655
Number of used sentences in train = 2811
Total loss for epoch 13: 4494.742764
validation loss after epoch 13 : 1936.277253
	Epoch 14....
Epoch has taken 0:03:04.425178
Number of used sentences in train = 2811
Total loss for epoch 14: 4486.248131
validation loss after epoch 14 : 1963.386741
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(9217, 101)
  (lstm): LSTM(118, 26, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:39.431958
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1795.699648
	Epoch 1....
Epoch has taken 0:00:16.759480
Number of used sentences in train = 313
Total loss for epoch 1: 855.272869
	Epoch 2....
Epoch has taken 0:00:16.770445
Number of used sentences in train = 313
Total loss for epoch 2: 613.067904
	Epoch 3....
Epoch has taken 0:00:16.797538
Number of used sentences in train = 313
Total loss for epoch 3: 565.235349
	Epoch 4....
Epoch has taken 0:00:16.781528
Number of used sentences in train = 313
Total loss for epoch 4: 544.907926
	Epoch 5....
Epoch has taken 0:00:16.770829
Number of used sentences in train = 313
Total loss for epoch 5: 530.725608
	Epoch 6....
Epoch has taken 0:00:16.804253
Number of used sentences in train = 313
Total loss for epoch 6: 522.610310
	Epoch 7....
Epoch has taken 0:00:16.807353
Number of used sentences in train = 313
Total loss for epoch 7: 517.424893
	Epoch 8....
Epoch has taken 0:00:16.805514
Number of used sentences in train = 313
Total loss for epoch 8: 514.173796
	Epoch 9....
Epoch has taken 0:00:16.760862
Number of used sentences in train = 313
Total loss for epoch 9: 511.608815
	Epoch 10....
Epoch has taken 0:00:16.783500
Number of used sentences in train = 313
Total loss for epoch 10: 509.030077
	Epoch 11....
Epoch has taken 0:00:16.789657
Number of used sentences in train = 313
Total loss for epoch 11: 506.314663
	Epoch 12....
Epoch has taken 0:00:16.795913
Number of used sentences in train = 313
Total loss for epoch 12: 504.905609
	Epoch 13....
Epoch has taken 0:00:16.803807
Number of used sentences in train = 313
Total loss for epoch 13: 503.065354
	Epoch 14....
Epoch has taken 0:00:16.819401
Number of used sentences in train = 313
Total loss for epoch 14: 502.106691
Epoch has taken 0:00:16.664892

==================================================================================================
	Training time : 0:45:25.954117
==================================================================================================
	Identification : 0.3

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(7065, 101)
  (lstm): LSTM(118, 26, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11148.994076
validation loss after epoch 0 : 1082.895948
	Epoch 1....
Epoch has taken 0:01:48.951420
Number of used sentences in train = 2074
Total loss for epoch 1: 6293.059082
validation loss after epoch 1 : 1071.449148
	Epoch 2....
Epoch has taken 0:01:48.892439
Number of used sentences in train = 2074
Total loss for epoch 2: 4720.908305
validation loss after epoch 2 : 1250.384268
	Epoch 3....
Epoch has taken 0:01:48.576067
Number of used sentences in train = 2074
Total loss for epoch 3: 3915.823187
validation loss after epoch 3 : 1494.113048
	Epoch 4....
Epoch has taken 0:01:47.953257
Number of used sentences in train = 2074
Total loss for epoch 4: 3521.280706
validation loss after epoch 4 : 1594.686985
	Epoch 5....
Epoch has taken 0:01:48.860452
Number of used sentences in train = 2074
Total loss for epoch 5: 3338.544703
validation loss after epoch 5 : 1699.899292
	Epoch 6....
Epoch has taken 0:01:48.956327
Number of used sentences in train = 2074
Total loss for epoch 6: 3244.666625
validation loss after epoch 6 : 1769.697567
	Epoch 7....
Epoch has taken 0:01:48.903128
Number of used sentences in train = 2074
Total loss for epoch 7: 3214.286960
validation loss after epoch 7 : 1815.014011
	Epoch 8....
Epoch has taken 0:01:48.862337
Number of used sentences in train = 2074
Total loss for epoch 8: 3195.567349
validation loss after epoch 8 : 1857.788036
	Epoch 9....
Epoch has taken 0:01:47.672530
Number of used sentences in train = 2074
Total loss for epoch 9: 3179.395377
validation loss after epoch 9 : 1887.941726
	Epoch 10....
Epoch has taken 0:01:47.845522
Number of used sentences in train = 2074
Total loss for epoch 10: 3171.719767
validation loss after epoch 10 : 1914.642279
	Epoch 11....
Epoch has taken 0:01:47.777127
Number of used sentences in train = 2074
Total loss for epoch 11: 3166.584579
validation loss after epoch 11 : 1938.601168
	Epoch 12....
Epoch has taken 0:01:50.818461
Number of used sentences in train = 2074
Total loss for epoch 12: 3163.812169
validation loss after epoch 12 : 1966.810906
	Epoch 13....
Epoch has taken 0:01:48.829035
Number of used sentences in train = 2074
Total loss for epoch 13: 3162.188612
validation loss after epoch 13 : 1983.216518
	Epoch 14....
Epoch has taken 0:01:48.823717
Number of used sentences in train = 2074
Total loss for epoch 14: 3160.988985
validation loss after epoch 14 : 2003.612806
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(7065, 101)
  (lstm): LSTM(118, 26, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:47.802211
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1755.868133
	Epoch 1....
Epoch has taken 0:00:10.999836
Number of used sentences in train = 231
Total loss for epoch 1: 611.382204
	Epoch 2....
Epoch has taken 0:00:10.964240
Number of used sentences in train = 231
Total loss for epoch 2: 409.785840
	Epoch 3....
Epoch has taken 0:00:10.971237
Number of used sentences in train = 231
Total loss for epoch 3: 361.641675
	Epoch 4....
Epoch has taken 0:00:10.952651
Number of used sentences in train = 231
Total loss for epoch 4: 353.581499
	Epoch 5....
Epoch has taken 0:00:10.983149
Number of used sentences in train = 231
Total loss for epoch 5: 350.594296
	Epoch 6....
Epoch has taken 0:00:10.945919
Number of used sentences in train = 231
Total loss for epoch 6: 349.016604
	Epoch 7....
Epoch has taken 0:00:10.981097
Number of used sentences in train = 231
Total loss for epoch 7: 348.737308
	Epoch 8....
Epoch has taken 0:00:10.980639
Number of used sentences in train = 231
Total loss for epoch 8: 348.284714
	Epoch 9....
Epoch has taken 0:00:10.977913
Number of used sentences in train = 231
Total loss for epoch 9: 346.959188
	Epoch 10....
Epoch has taken 0:00:10.972285
Number of used sentences in train = 231
Total loss for epoch 10: 346.569610
	Epoch 11....
Epoch has taken 0:00:10.972931
Number of used sentences in train = 231
Total loss for epoch 11: 346.221476
	Epoch 12....
Epoch has taken 0:00:10.990325
Number of used sentences in train = 231
Total loss for epoch 12: 346.002847
	Epoch 13....
Epoch has taken 0:00:10.960625
Number of used sentences in train = 231
Total loss for epoch 13: 345.795605
	Epoch 14....
Epoch has taken 0:00:10.977004
Number of used sentences in train = 231
Total loss for epoch 14: 345.637841
Epoch has taken 0:00:10.959608

==================================================================================================
	Training time : 0:29:54.466972
==================================================================================================
	Identification : 0.275

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(18073, 101)
  (lstm): LSTM(118, 26, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18141.343044
validation loss after epoch 0 : 1635.795632
	Epoch 1....
Epoch has taken 0:03:34.308743
Number of used sentences in train = 3226
Total loss for epoch 1: 11741.801661
validation loss after epoch 1 : 1583.346060
	Epoch 2....
Epoch has taken 0:03:34.318486
Number of used sentences in train = 3226
Total loss for epoch 2: 9343.035045
validation loss after epoch 2 : 1717.214678
	Epoch 3....
Epoch has taken 0:03:35.190334
Number of used sentences in train = 3226
Total loss for epoch 3: 7935.352511
validation loss after epoch 3 : 1936.293056
	Epoch 4....
Epoch has taken 0:03:34.946490
Number of used sentences in train = 3226
Total loss for epoch 4: 7111.285648
validation loss after epoch 4 : 2135.130549
	Epoch 5....
Epoch has taken 0:04:07.813588
Number of used sentences in train = 3226
Total loss for epoch 5: 6640.556278
validation loss after epoch 5 : 2239.789808
	Epoch 6....
Epoch has taken 0:03:58.559883
Number of used sentences in train = 3226
Total loss for epoch 6: 6454.842597
validation loss after epoch 6 : 2368.827080
	Epoch 7....
Epoch has taken 0:03:57.449453
Number of used sentences in train = 3226
Total loss for epoch 7: 6362.362826
validation loss after epoch 7 : 2489.941524
	Epoch 8....
Epoch has taken 0:03:33.413136
Number of used sentences in train = 3226
Total loss for epoch 8: 6284.862404
validation loss after epoch 8 : 2640.655005
	Epoch 9....
Epoch has taken 0:03:34.878708
Number of used sentences in train = 3226
Total loss for epoch 9: 6255.212175
validation loss after epoch 9 : 2546.100015
	Epoch 10....
Epoch has taken 0:03:34.564717
Number of used sentences in train = 3226
Total loss for epoch 10: 6221.618413
validation loss after epoch 10 : 2666.680434
	Epoch 11....
Epoch has taken 0:03:34.296499
Number of used sentences in train = 3226
Total loss for epoch 11: 6212.279018
validation loss after epoch 11 : 2706.093173
	Epoch 12....
Epoch has taken 0:03:32.458205
Number of used sentences in train = 3226
Total loss for epoch 12: 6202.744244
validation loss after epoch 12 : 2665.077126
	Epoch 13....
Epoch has taken 0:03:32.418218
Number of used sentences in train = 3226
Total loss for epoch 13: 6188.996035
validation loss after epoch 13 : 2755.191100
	Epoch 14....
Epoch has taken 0:03:34.471996
Number of used sentences in train = 3226
Total loss for epoch 14: 6181.751906
validation loss after epoch 14 : 2751.710651
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(18073, 101)
  (lstm): LSTM(118, 26, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:35.355145
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 5290.936547
	Epoch 1....
Epoch has taken 0:00:20.883037
Number of used sentences in train = 359
Total loss for epoch 1: 1023.480104
	Epoch 2....
Epoch has taken 0:00:20.876745
Number of used sentences in train = 359
Total loss for epoch 2: 812.049281
	Epoch 3....
Epoch has taken 0:00:20.912089
Number of used sentences in train = 359
Total loss for epoch 3: 743.684865
	Epoch 4....
Epoch has taken 0:00:20.901888
Number of used sentences in train = 359
Total loss for epoch 4: 709.705653
	Epoch 5....
Epoch has taken 0:00:20.918940
Number of used sentences in train = 359
Total loss for epoch 5: 691.253756
	Epoch 6....
Epoch has taken 0:00:20.889379
Number of used sentences in train = 359
Total loss for epoch 6: 683.405121
	Epoch 7....
Epoch has taken 0:00:20.913688
Number of used sentences in train = 359
Total loss for epoch 7: 679.801032
	Epoch 8....
Epoch has taken 0:00:20.913560
Number of used sentences in train = 359
Total loss for epoch 8: 677.046017
	Epoch 9....
Epoch has taken 0:00:20.901579
Number of used sentences in train = 359
Total loss for epoch 9: 675.655704
	Epoch 10....
Epoch has taken 0:00:20.850079
Number of used sentences in train = 359
Total loss for epoch 10: 674.843988
	Epoch 11....
Epoch has taken 0:00:20.857330
Number of used sentences in train = 359
Total loss for epoch 11: 674.233962
	Epoch 12....
Epoch has taken 0:00:20.712963
Number of used sentences in train = 359
Total loss for epoch 12: 673.745786
	Epoch 13....
Epoch has taken 0:00:20.735214
Number of used sentences in train = 359
Total loss for epoch 13: 673.350304
	Epoch 14....
Epoch has taken 0:00:20.686083
Number of used sentences in train = 359
Total loss for epoch 14: 673.027312
Epoch has taken 0:00:20.692595

==================================================================================================
	Training time : 1:00:07.790930
==================================================================================================
	Identification : 0.117

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 41, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 71, 'lstmDropout': 0.19, 'denseActivation': 'tanh', 'wordDim': 138, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1882, 138)
  (lstm): LSTM(179, 71, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10976.170673
validation loss after epoch 0 : 884.233808
	Epoch 1....
Epoch has taken 0:02:39.378784
Number of used sentences in train = 2811
Total loss for epoch 1: 7870.107434
validation loss after epoch 1 : 888.781485
	Epoch 2....
Epoch has taken 0:02:39.456420
Number of used sentences in train = 2811
Total loss for epoch 2: 6903.892908
validation loss after epoch 2 : 863.103829
	Epoch 3....
Epoch has taken 0:02:40.892497
Number of used sentences in train = 2811
Total loss for epoch 3: 6325.395992
validation loss after epoch 3 : 894.019045
	Epoch 4....
Epoch has taken 0:02:39.142891
Number of used sentences in train = 2811
Total loss for epoch 4: 5834.913425
validation loss after epoch 4 : 919.855802
	Epoch 5....
Epoch has taken 0:02:37.965108
Number of used sentences in train = 2811
Total loss for epoch 5: 5543.906597
validation loss after epoch 5 : 945.139515
	Epoch 6....
Epoch has taken 0:02:39.498097
Number of used sentences in train = 2811
Total loss for epoch 6: 5361.682860
validation loss after epoch 6 : 957.011615
	Epoch 7....
Epoch has taken 0:02:50.159101
Number of used sentences in train = 2811
Total loss for epoch 7: 5197.808916
validation loss after epoch 7 : 996.718984
	Epoch 8....
Epoch has taken 0:02:39.164133
Number of used sentences in train = 2811
Total loss for epoch 8: 5121.069707
validation loss after epoch 8 : 1020.808968
	Epoch 9....
Epoch has taken 0:02:38.097456
Number of used sentences in train = 2811
Total loss for epoch 9: 5032.538321
validation loss after epoch 9 : 1046.371598
	Epoch 10....
Epoch has taken 0:02:55.888962
Number of used sentences in train = 2811
Total loss for epoch 10: 4978.801447
validation loss after epoch 10 : 1042.688687
	Epoch 11....
Epoch has taken 0:02:55.111437
Number of used sentences in train = 2811
Total loss for epoch 11: 4917.923372
validation loss after epoch 11 : 1043.607313
	Epoch 12....
Epoch has taken 0:02:56.799342
Number of used sentences in train = 2811
Total loss for epoch 12: 4892.023193
validation loss after epoch 12 : 1112.983312
	Epoch 13....
Epoch has taken 0:02:56.515164
Number of used sentences in train = 2811
Total loss for epoch 13: 4851.060914
validation loss after epoch 13 : 1092.582845
	Epoch 14....
Epoch has taken 0:02:54.935763
Number of used sentences in train = 2811
Total loss for epoch 14: 4821.952341
validation loss after epoch 14 : 1107.123080
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1882, 138)
  (lstm): LSTM(179, 71, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:55.230952
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1234.137466
	Epoch 1....
Epoch has taken 0:00:18.782738
Number of used sentences in train = 313
Total loss for epoch 1: 727.624802
	Epoch 2....
Epoch has taken 0:00:18.777237
Number of used sentences in train = 313
Total loss for epoch 2: 615.984971
	Epoch 3....
Epoch has taken 0:00:18.801316
Number of used sentences in train = 313
Total loss for epoch 3: 579.308377
	Epoch 4....
Epoch has taken 0:00:18.786797
Number of used sentences in train = 313
Total loss for epoch 4: 565.747897
	Epoch 5....
Epoch has taken 0:00:18.792652
Number of used sentences in train = 313
Total loss for epoch 5: 547.454674
	Epoch 6....
Epoch has taken 0:00:18.798458
Number of used sentences in train = 313
Total loss for epoch 6: 542.277133
	Epoch 7....
Epoch has taken 0:00:18.825105
Number of used sentences in train = 313
Total loss for epoch 7: 540.291448
	Epoch 8....
Epoch has taken 0:00:18.834473
Number of used sentences in train = 313
Total loss for epoch 8: 539.358471
	Epoch 9....
Epoch has taken 0:00:18.824747
Number of used sentences in train = 313
Total loss for epoch 9: 538.258013
	Epoch 10....
Epoch has taken 0:00:18.788425
Number of used sentences in train = 313
Total loss for epoch 10: 537.595497
	Epoch 11....
Epoch has taken 0:00:18.811161
Number of used sentences in train = 313
Total loss for epoch 11: 536.835637
	Epoch 12....
Epoch has taken 0:00:18.829170
Number of used sentences in train = 313
Total loss for epoch 12: 536.332477
	Epoch 13....
Epoch has taken 0:00:18.809854
Number of used sentences in train = 313
Total loss for epoch 13: 535.878065
	Epoch 14....
Epoch has taken 0:00:18.755591
Number of used sentences in train = 313
Total loss for epoch 14: 535.354255
Epoch has taken 0:00:18.776852

==================================================================================================
	Training time : 0:46:20.746349
==================================================================================================
	Identification : 0.04

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1680, 138)
  (lstm): LSTM(179, 71, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9161.265982
validation loss after epoch 0 : 800.056475
	Epoch 1....
Epoch has taken 0:02:00.905549
Number of used sentences in train = 2074
Total loss for epoch 1: 5762.544293
validation loss after epoch 1 : 778.426357
	Epoch 2....
Epoch has taken 0:02:00.901525
Number of used sentences in train = 2074
Total loss for epoch 2: 4823.712069
validation loss after epoch 2 : 725.123854
	Epoch 3....
Epoch has taken 0:02:00.703962
Number of used sentences in train = 2074
Total loss for epoch 3: 4290.688822
validation loss after epoch 3 : 744.801822
	Epoch 4....
Epoch has taken 0:02:00.056060
Number of used sentences in train = 2074
Total loss for epoch 4: 3931.479965
validation loss after epoch 4 : 769.477891
	Epoch 5....
Epoch has taken 0:02:01.146350
Number of used sentences in train = 2074
Total loss for epoch 5: 3697.481108
validation loss after epoch 5 : 835.419926
	Epoch 6....
Epoch has taken 0:02:02.302523
Number of used sentences in train = 2074
Total loss for epoch 6: 3542.303504
validation loss after epoch 6 : 834.180815
	Epoch 7....
Epoch has taken 0:01:59.932734
Number of used sentences in train = 2074
Total loss for epoch 7: 3451.821532
validation loss after epoch 7 : 831.553748
	Epoch 8....
Epoch has taken 0:02:00.108279
Number of used sentences in train = 2074
Total loss for epoch 8: 3393.033370
validation loss after epoch 8 : 859.612429
	Epoch 9....
Epoch has taken 0:02:01.118885
Number of used sentences in train = 2074
Total loss for epoch 9: 3352.605284
validation loss after epoch 9 : 885.340488
	Epoch 10....
Epoch has taken 0:01:59.869266
Number of used sentences in train = 2074
Total loss for epoch 10: 3320.788960
validation loss after epoch 10 : 891.579166
	Epoch 11....
Epoch has taken 0:01:49.076674
Number of used sentences in train = 2074
Total loss for epoch 11: 3292.779778
validation loss after epoch 11 : 912.612202
	Epoch 12....
Epoch has taken 0:01:53.625656
Number of used sentences in train = 2074
Total loss for epoch 12: 3274.608017
validation loss after epoch 12 : 921.557904
	Epoch 13....
Epoch has taken 0:01:59.023320
Number of used sentences in train = 2074
Total loss for epoch 13: 3261.723814
validation loss after epoch 13 : 938.733246
	Epoch 14....
Epoch has taken 0:01:58.984908
Number of used sentences in train = 2074
Total loss for epoch 14: 3251.090871
validation loss after epoch 14 : 936.123535
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1680, 138)
  (lstm): LSTM(179, 71, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:04.189253
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1173.299638
	Epoch 1....
Epoch has taken 0:00:13.384342
Number of used sentences in train = 231
Total loss for epoch 1: 655.443843
	Epoch 2....
Epoch has taken 0:00:12.528686
Number of used sentences in train = 231
Total loss for epoch 2: 501.615863
	Epoch 3....
Epoch has taken 0:00:13.468598
Number of used sentences in train = 231
Total loss for epoch 3: 444.178596
	Epoch 4....
Epoch has taken 0:00:13.335033
Number of used sentences in train = 231
Total loss for epoch 4: 403.338271
	Epoch 5....
Epoch has taken 0:00:13.509280
Number of used sentences in train = 231
Total loss for epoch 5: 385.017288
	Epoch 6....
Epoch has taken 0:00:13.428826
Number of used sentences in train = 231
Total loss for epoch 6: 375.452026
	Epoch 7....
Epoch has taken 0:00:13.604791
Number of used sentences in train = 231
Total loss for epoch 7: 371.645914
	Epoch 8....
Epoch has taken 0:00:13.606844
Number of used sentences in train = 231
Total loss for epoch 8: 366.036474
	Epoch 9....
Epoch has taken 0:00:13.341095
Number of used sentences in train = 231
Total loss for epoch 9: 365.118485
	Epoch 10....
Epoch has taken 0:00:13.479014
Number of used sentences in train = 231
Total loss for epoch 10: 364.405537
	Epoch 11....
Epoch has taken 0:00:13.544746
Number of used sentences in train = 231
Total loss for epoch 11: 363.561743
	Epoch 12....
Epoch has taken 0:00:13.321435
Number of used sentences in train = 231
Total loss for epoch 12: 362.932570
	Epoch 13....
Epoch has taken 0:00:13.442497
Number of used sentences in train = 231
Total loss for epoch 13: 362.530514
	Epoch 14....
Epoch has taken 0:00:13.494905
Number of used sentences in train = 231
Total loss for epoch 14: 362.173690
Epoch has taken 0:00:13.555610

==================================================================================================
	Training time : 0:33:13.332243
==================================================================================================
	Identification : 0.02

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 41)
  (w_embeddings): Embedding(3369, 138)
  (lstm): LSTM(179, 71, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14601.980521
validation loss after epoch 0 : 1209.582733
	Epoch 1....
Epoch has taken 0:03:50.652714
Number of used sentences in train = 3226
Total loss for epoch 1: 9701.017011
validation loss after epoch 1 : 1132.686350
	Epoch 2....
Epoch has taken 0:03:48.402192
Number of used sentences in train = 3226
Total loss for epoch 2: 8565.304931
validation loss after epoch 2 : 1125.243883
	Epoch 3....
Epoch has taken 0:03:48.348547
Number of used sentences in train = 3226
Total loss for epoch 3: 7905.364257
validation loss after epoch 3 : 1169.493849
	Epoch 4....
Epoch has taken 0:03:55.629054
Number of used sentences in train = 3226
Total loss for epoch 4: 7509.673893
validation loss after epoch 4 : 1185.586271
	Epoch 5....
Epoch has taken 0:03:37.990899
Number of used sentences in train = 3226
Total loss for epoch 5: 7212.568878
validation loss after epoch 5 : 1275.807745
	Epoch 6....
Epoch has taken 0:03:44.095646
Number of used sentences in train = 3226
Total loss for epoch 6: 6998.966775
validation loss after epoch 6 : 1320.266061
	Epoch 7....
Epoch has taken 0:03:48.014990
Number of used sentences in train = 3226
Total loss for epoch 7: 6870.115284
validation loss after epoch 7 : 1341.114352
	Epoch 8....
Epoch has taken 0:03:31.936353
Number of used sentences in train = 3226
Total loss for epoch 8: 6746.754599
validation loss after epoch 8 : 1416.043309
	Epoch 9....
Epoch has taken 0:03:40.135892
Number of used sentences in train = 3226
Total loss for epoch 9: 6626.427940
validation loss after epoch 9 : 1398.332632
	Epoch 10....
Epoch has taken 0:03:32.345690
Number of used sentences in train = 3226
Total loss for epoch 10: 6548.155740
validation loss after epoch 10 : 1446.052574
	Epoch 11....
Epoch has taken 0:03:30.371782
Number of used sentences in train = 3226
Total loss for epoch 11: 6489.061942
validation loss after epoch 11 : 1459.422920
	Epoch 12....
Epoch has taken 0:03:47.393329
Number of used sentences in train = 3226
Total loss for epoch 12: 6432.824948
validation loss after epoch 12 : 1547.180950
	Epoch 13....
Epoch has taken 0:03:39.839824
Number of used sentences in train = 3226
Total loss for epoch 13: 6394.997665
validation loss after epoch 13 : 1508.601387
	Epoch 14....
Epoch has taken 0:03:34.412036
Number of used sentences in train = 3226
Total loss for epoch 14: 6360.121650
validation loss after epoch 14 : 1573.136506
	TransitionClassifier(
  (p_embeddings): Embedding(13, 41)
  (w_embeddings): Embedding(3369, 138)
  (lstm): LSTM(179, 71, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:46.397061
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1745.602035
	Epoch 1....
Epoch has taken 0:00:20.696190
Number of used sentences in train = 359
Total loss for epoch 1: 1008.334362
	Epoch 2....
Epoch has taken 0:00:20.684662
Number of used sentences in train = 359
Total loss for epoch 2: 847.668405
	Epoch 3....
Epoch has taken 0:00:20.642341
Number of used sentences in train = 359
Total loss for epoch 3: 775.612302
	Epoch 4....
Epoch has taken 0:00:20.639986
Number of used sentences in train = 359
Total loss for epoch 4: 721.138673
	Epoch 5....
Epoch has taken 0:00:20.629983
Number of used sentences in train = 359
Total loss for epoch 5: 702.114717
	Epoch 6....
Epoch has taken 0:00:20.616927
Number of used sentences in train = 359
Total loss for epoch 6: 692.057688
	Epoch 7....
Epoch has taken 0:00:20.633615
Number of used sentences in train = 359
Total loss for epoch 7: 690.002036
	Epoch 8....
Epoch has taken 0:00:20.645999
Number of used sentences in train = 359
Total loss for epoch 8: 680.916055
	Epoch 9....
Epoch has taken 0:00:20.606580
Number of used sentences in train = 359
Total loss for epoch 9: 678.680112
	Epoch 10....
Epoch has taken 0:00:20.443014
Number of used sentences in train = 359
Total loss for epoch 10: 676.841473
	Epoch 11....
Epoch has taken 0:00:20.435957
Number of used sentences in train = 359
Total loss for epoch 11: 675.757505
	Epoch 12....
Epoch has taken 0:00:20.445018
Number of used sentences in train = 359
Total loss for epoch 12: 675.119880
	Epoch 13....
Epoch has taken 0:00:22.402328
Number of used sentences in train = 359
Total loss for epoch 13: 674.841948
	Epoch 14....
Epoch has taken 0:00:22.461366
Number of used sentences in train = 359
Total loss for epoch 14: 674.687899
Epoch has taken 0:00:20.450943

==================================================================================================
	Training time : 1:00:49.086690
==================================================================================================
	Identification : 0.071

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 15, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 92, 'lstmDropout': 0.14, 'denseActivation': 'tanh', 'wordDim': 57, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(9209, 57)
  (lstm): LSTM(72, 92, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14670.085118
validation loss after epoch 0 : 1257.168417
	Epoch 1....
Epoch has taken 0:02:39.380052
Number of used sentences in train = 2811
Total loss for epoch 1: 10182.658355
validation loss after epoch 1 : 1183.994511
	Epoch 2....
Epoch has taken 0:02:39.471138
Number of used sentences in train = 2811
Total loss for epoch 2: 8312.278266
validation loss after epoch 2 : 1192.303250
	Epoch 3....
Epoch has taken 0:03:03.807348
Number of used sentences in train = 2811
Total loss for epoch 3: 7197.885208
validation loss after epoch 3 : 1167.643528
	Epoch 4....
Epoch has taken 0:02:39.263638
Number of used sentences in train = 2811
Total loss for epoch 4: 6420.393590
validation loss after epoch 4 : 1252.474329
	Epoch 5....
Epoch has taken 0:02:38.098668
Number of used sentences in train = 2811
Total loss for epoch 5: 5877.330390
validation loss after epoch 5 : 1293.844294
	Epoch 6....
Epoch has taken 0:02:39.476321
Number of used sentences in train = 2811
Total loss for epoch 6: 5524.512027
validation loss after epoch 6 : 1346.078095
	Epoch 7....
Epoch has taken 0:02:39.526587
Number of used sentences in train = 2811
Total loss for epoch 7: 5339.961185
validation loss after epoch 7 : 1354.003410
	Epoch 8....
Epoch has taken 0:02:39.402205
Number of used sentences in train = 2811
Total loss for epoch 8: 5188.237116
validation loss after epoch 8 : 1409.282043
	Epoch 9....
Epoch has taken 0:02:39.145039
Number of used sentences in train = 2811
Total loss for epoch 9: 5076.791333
validation loss after epoch 9 : 1451.319031
	Epoch 10....
Epoch has taken 0:02:37.933810
Number of used sentences in train = 2811
Total loss for epoch 10: 4993.627314
validation loss after epoch 10 : 1423.074054
	Epoch 11....
Epoch has taken 0:02:39.527601
Number of used sentences in train = 2811
Total loss for epoch 11: 4931.655974
validation loss after epoch 11 : 1462.105681
	Epoch 12....
Epoch has taken 0:02:39.486747
Number of used sentences in train = 2811
Total loss for epoch 12: 4889.654235
validation loss after epoch 12 : 1458.101371
	Epoch 13....
Epoch has taken 0:02:39.529935
Number of used sentences in train = 2811
Total loss for epoch 13: 4855.748916
validation loss after epoch 13 : 1471.490047
	Epoch 14....
Epoch has taken 0:02:39.233245
Number of used sentences in train = 2811
Total loss for epoch 14: 4835.486638
validation loss after epoch 14 : 1486.167276
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(9209, 57)
  (lstm): LSTM(72, 92, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:37.892743
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1610.525627
	Epoch 1....
Epoch has taken 0:00:16.854231
Number of used sentences in train = 313
Total loss for epoch 1: 1011.036240
	Epoch 2....
Epoch has taken 0:00:16.877793
Number of used sentences in train = 313
Total loss for epoch 2: 778.202941
	Epoch 3....
Epoch has taken 0:00:16.865514
Number of used sentences in train = 313
Total loss for epoch 3: 673.572535
	Epoch 4....
Epoch has taken 0:00:16.867117
Number of used sentences in train = 313
Total loss for epoch 4: 606.442692
	Epoch 5....
Epoch has taken 0:00:16.868586
Number of used sentences in train = 313
Total loss for epoch 5: 576.652981
	Epoch 6....
Epoch has taken 0:00:16.875322
Number of used sentences in train = 313
Total loss for epoch 6: 569.853979
	Epoch 7....
Epoch has taken 0:00:16.854187
Number of used sentences in train = 313
Total loss for epoch 7: 565.561488
	Epoch 8....
Epoch has taken 0:00:16.886562
Number of used sentences in train = 313
Total loss for epoch 8: 565.860774
	Epoch 9....
Epoch has taken 0:00:16.881539
Number of used sentences in train = 313
Total loss for epoch 9: 560.377734
	Epoch 10....
Epoch has taken 0:00:16.870481
Number of used sentences in train = 313
Total loss for epoch 10: 554.057395
	Epoch 11....
Epoch has taken 0:00:16.849311
Number of used sentences in train = 313
Total loss for epoch 11: 551.844593
	Epoch 12....
Epoch has taken 0:00:16.845843
Number of used sentences in train = 313
Total loss for epoch 12: 550.562102
	Epoch 13....
Epoch has taken 0:00:16.849389
Number of used sentences in train = 313
Total loss for epoch 13: 550.082336
	Epoch 14....
Epoch has taken 0:00:16.835818
Number of used sentences in train = 313
Total loss for epoch 14: 549.617216
Epoch has taken 0:00:16.836372

==================================================================================================
	Training time : 0:44:24.627205
==================================================================================================
	Identification : 0.479

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(7063, 57)
  (lstm): LSTM(72, 92, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11589.628355
validation loss after epoch 0 : 952.767772
	Epoch 1....
Epoch has taken 0:01:47.761637
Number of used sentences in train = 2074
Total loss for epoch 1: 7338.127543
validation loss after epoch 1 : 925.437046
	Epoch 2....
Epoch has taken 0:01:52.760831
Number of used sentences in train = 2074
Total loss for epoch 2: 5915.076248
validation loss after epoch 2 : 907.595341
	Epoch 3....
Epoch has taken 0:02:00.942130
Number of used sentences in train = 2074
Total loss for epoch 3: 5023.473930
validation loss after epoch 3 : 930.713783
	Epoch 4....
Epoch has taken 0:02:00.948083
Number of used sentences in train = 2074
Total loss for epoch 4: 4323.922685
validation loss after epoch 4 : 1028.684757
	Epoch 5....
Epoch has taken 0:02:00.931286
Number of used sentences in train = 2074
Total loss for epoch 5: 3987.818730
validation loss after epoch 5 : 1049.416993
	Epoch 6....
Epoch has taken 0:01:59.781895
Number of used sentences in train = 2074
Total loss for epoch 6: 3758.360221
validation loss after epoch 6 : 1143.268856
	Epoch 7....
Epoch has taken 0:02:00.847035
Number of used sentences in train = 2074
Total loss for epoch 7: 3613.928121
validation loss after epoch 7 : 1195.796640
	Epoch 8....
Epoch has taken 0:02:00.844735
Number of used sentences in train = 2074
Total loss for epoch 8: 3514.032318
validation loss after epoch 8 : 1254.649053
	Epoch 9....
Epoch has taken 0:02:00.866655
Number of used sentences in train = 2074
Total loss for epoch 9: 3455.456039
validation loss after epoch 9 : 1292.350947
	Epoch 10....
Epoch has taken 0:02:00.640928
Number of used sentences in train = 2074
Total loss for epoch 10: 3412.523513
validation loss after epoch 10 : 1299.224632
	Epoch 11....
Epoch has taken 0:02:00.080487
Number of used sentences in train = 2074
Total loss for epoch 11: 3393.074899
validation loss after epoch 11 : 1325.043467
	Epoch 12....
Epoch has taken 0:01:59.964677
Number of used sentences in train = 2074
Total loss for epoch 12: 3366.062492
validation loss after epoch 12 : 1344.080462
	Epoch 13....
Epoch has taken 0:02:00.990439
Number of used sentences in train = 2074
Total loss for epoch 13: 3348.998315
validation loss after epoch 13 : 1373.065795
	Epoch 14....
Epoch has taken 0:02:01.072192
Number of used sentences in train = 2074
Total loss for epoch 14: 3333.070610
validation loss after epoch 14 : 1353.731361
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(7063, 57)
  (lstm): LSTM(72, 92, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.386873
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1483.370376
	Epoch 1....
Epoch has taken 0:00:11.063229
Number of used sentences in train = 231
Total loss for epoch 1: 824.864072
	Epoch 2....
Epoch has taken 0:00:11.085124
Number of used sentences in train = 231
Total loss for epoch 2: 606.199714
	Epoch 3....
Epoch has taken 0:00:11.045694
Number of used sentences in train = 231
Total loss for epoch 3: 504.998028
	Epoch 4....
Epoch has taken 0:00:11.077105
Number of used sentences in train = 231
Total loss for epoch 4: 419.076940
	Epoch 5....
Epoch has taken 0:00:11.086010
Number of used sentences in train = 231
Total loss for epoch 5: 393.460930
	Epoch 6....
Epoch has taken 0:00:11.083393
Number of used sentences in train = 231
Total loss for epoch 6: 384.618764
	Epoch 7....
Epoch has taken 0:00:11.069075
Number of used sentences in train = 231
Total loss for epoch 7: 380.754732
	Epoch 8....
Epoch has taken 0:00:11.081079
Number of used sentences in train = 231
Total loss for epoch 8: 376.010376
	Epoch 9....
Epoch has taken 0:00:11.074375
Number of used sentences in train = 231
Total loss for epoch 9: 373.775002
	Epoch 10....
Epoch has taken 0:00:11.085219
Number of used sentences in train = 231
Total loss for epoch 10: 372.503279
	Epoch 11....
Epoch has taken 0:00:11.061553
Number of used sentences in train = 231
Total loss for epoch 11: 370.589069
	Epoch 12....
Epoch has taken 0:00:11.078705
Number of used sentences in train = 231
Total loss for epoch 12: 369.838525
	Epoch 13....
Epoch has taken 0:00:11.057982
Number of used sentences in train = 231
Total loss for epoch 13: 366.497285
	Epoch 14....
Epoch has taken 0:00:11.066204
Number of used sentences in train = 231
Total loss for epoch 14: 366.016163
Epoch has taken 0:00:11.064372

==================================================================================================
	Training time : 0:32:32.243687
==================================================================================================
	Identification : 0.319

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(18064, 57)
  (lstm): LSTM(72, 92, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19158.538306
validation loss after epoch 0 : 1707.312298
	Epoch 1....
Epoch has taken 0:03:30.286678
Number of used sentences in train = 3226
Total loss for epoch 1: 13085.920182
validation loss after epoch 1 : 1605.911628
	Epoch 2....
Epoch has taken 0:03:30.320135
Number of used sentences in train = 3226
Total loss for epoch 2: 10578.271384
validation loss after epoch 2 : 1576.667317
	Epoch 3....
Epoch has taken 0:03:32.626286
Number of used sentences in train = 3226
Total loss for epoch 3: 8804.358120
validation loss after epoch 3 : 1733.879289
	Epoch 4....
Epoch has taken 0:04:05.348525
Number of used sentences in train = 3226
Total loss for epoch 4: 7786.664149
validation loss after epoch 4 : 1829.874009
	Epoch 5....
Epoch has taken 0:03:32.611786
Number of used sentences in train = 3226
Total loss for epoch 5: 7085.274076
validation loss after epoch 5 : 1949.984339
	Epoch 6....
Epoch has taken 0:03:30.533616
Number of used sentences in train = 3226
Total loss for epoch 6: 6788.967597
validation loss after epoch 6 : 2021.460128
	Epoch 7....
Epoch has taken 0:03:32.506857
Number of used sentences in train = 3226
Total loss for epoch 7: 6598.283603
validation loss after epoch 7 : 2033.875789
	Epoch 8....
Epoch has taken 0:03:32.532150
Number of used sentences in train = 3226
Total loss for epoch 8: 6492.683557
validation loss after epoch 8 : 2198.542707
	Epoch 9....
Epoch has taken 0:03:45.369393
Number of used sentences in train = 3226
Total loss for epoch 9: 6415.130169
validation loss after epoch 9 : 2196.101725
	Epoch 10....
Epoch has taken 0:03:47.378126
Number of used sentences in train = 3226
Total loss for epoch 10: 6371.000221
validation loss after epoch 10 : 2283.495329
	Epoch 11....
Epoch has taken 0:04:05.536140
Number of used sentences in train = 3226
Total loss for epoch 11: 6327.164014
validation loss after epoch 11 : 2360.270603
	Epoch 12....
Epoch has taken 0:03:30.607025
Number of used sentences in train = 3226
Total loss for epoch 12: 6287.673864
validation loss after epoch 12 : 2366.997572
	Epoch 13....
Epoch has taken 0:03:32.446715
Number of used sentences in train = 3226
Total loss for epoch 13: 6252.566887
validation loss after epoch 13 : 2388.606831
	Epoch 14....
Epoch has taken 0:03:32.755944
Number of used sentences in train = 3226
Total loss for epoch 14: 6241.403007
validation loss after epoch 14 : 2413.062758
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(18064, 57)
  (lstm): LSTM(72, 92, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:32.321604
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2061.087392
	Epoch 1....
Epoch has taken 0:00:20.700280
Number of used sentences in train = 359
Total loss for epoch 1: 1366.294206
	Epoch 2....
Epoch has taken 0:00:20.678235
Number of used sentences in train = 359
Total loss for epoch 2: 1062.617281
	Epoch 3....
Epoch has taken 0:00:20.702438
Number of used sentences in train = 359
Total loss for epoch 3: 867.573856
	Epoch 4....
Epoch has taken 0:00:20.676949
Number of used sentences in train = 359
Total loss for epoch 4: 766.867123
	Epoch 5....
Epoch has taken 0:00:20.696008
Number of used sentences in train = 359
Total loss for epoch 5: 719.439610
	Epoch 6....
Epoch has taken 0:00:20.685272
Number of used sentences in train = 359
Total loss for epoch 6: 699.842955
	Epoch 7....
Epoch has taken 0:00:20.702978
Number of used sentences in train = 359
Total loss for epoch 7: 695.631094
	Epoch 8....
Epoch has taken 0:00:20.683588
Number of used sentences in train = 359
Total loss for epoch 8: 692.667631
	Epoch 9....
Epoch has taken 0:00:20.660185
Number of used sentences in train = 359
Total loss for epoch 9: 691.287679
	Epoch 10....
Epoch has taken 0:00:20.687085
Number of used sentences in train = 359
Total loss for epoch 10: 685.877147
	Epoch 11....
Epoch has taken 0:00:20.647366
Number of used sentences in train = 359
Total loss for epoch 11: 684.877660
	Epoch 12....
Epoch has taken 0:00:20.667624
Number of used sentences in train = 359
Total loss for epoch 12: 684.299987
	Epoch 13....
Epoch has taken 0:00:20.670250
Number of used sentences in train = 359
Total loss for epoch 13: 683.892872
	Epoch 14....
Epoch has taken 0:00:20.678273
Number of used sentences in train = 359
Total loss for epoch 14: 683.535076
Epoch has taken 0:00:20.670581

==================================================================================================
	Training time : 0:59:44.072136
==================================================================================================
	Identification : 0.46

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 54, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 26, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 90, 'lstmDropout': 0.26, 'denseActivation': 'tanh', 'wordDim': 76, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1882, 76)
  (lstm): LSTM(102, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=54, bias=True)
  (linear2): Linear(in_features=54, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11341.951842
validation loss after epoch 0 : 877.786189
	Epoch 1....
Epoch has taken 0:02:39.365791
Number of used sentences in train = 2811
Total loss for epoch 1: 7448.768637
validation loss after epoch 1 : 818.349492
	Epoch 2....
Epoch has taken 0:02:39.383338
Number of used sentences in train = 2811
Total loss for epoch 2: 6603.469697
validation loss after epoch 2 : 829.090266
	Epoch 3....
Epoch has taken 0:03:04.123149
Number of used sentences in train = 2811
Total loss for epoch 3: 5978.049362
validation loss after epoch 3 : 842.357728
	Epoch 4....
Epoch has taken 0:02:39.127988
Number of used sentences in train = 2811
Total loss for epoch 4: 5575.944785
validation loss after epoch 4 : 854.008161
	Epoch 5....
Epoch has taken 0:02:37.623365
Number of used sentences in train = 2811
Total loss for epoch 5: 5298.643484
validation loss after epoch 5 : 885.183296
	Epoch 6....
Epoch has taken 0:02:37.836186
Number of used sentences in train = 2811
Total loss for epoch 6: 5074.327439
validation loss after epoch 6 : 919.548252
	Epoch 7....
Epoch has taken 0:02:39.290246
Number of used sentences in train = 2811
Total loss for epoch 7: 4950.489454
validation loss after epoch 7 : 930.888379
	Epoch 8....
Epoch has taken 0:02:39.371164
Number of used sentences in train = 2811
Total loss for epoch 8: 4816.963371
validation loss after epoch 8 : 992.128404
	Epoch 9....
Epoch has taken 0:03:04.174074
Number of used sentences in train = 2811
Total loss for epoch 9: 4737.816684
validation loss after epoch 9 : 986.845826
	Epoch 10....
Epoch has taken 0:02:38.958031
Number of used sentences in train = 2811
Total loss for epoch 10: 4697.291869
validation loss after epoch 10 : 1023.333531
	Epoch 11....
Epoch has taken 0:02:37.878259
Number of used sentences in train = 2811
Total loss for epoch 11: 4648.887270
validation loss after epoch 11 : 1007.225827
	Epoch 12....
Epoch has taken 0:02:38.513700
Number of used sentences in train = 2811
Total loss for epoch 12: 4612.185238
validation loss after epoch 12 : 1043.294532
	Epoch 13....
Epoch has taken 0:02:40.002942
Number of used sentences in train = 2811
Total loss for epoch 13: 4589.221039
validation loss after epoch 13 : 1049.306379
	Epoch 14....
Epoch has taken 0:02:42.337433
Number of used sentences in train = 2811
Total loss for epoch 14: 4563.255716
validation loss after epoch 14 : 1060.973202
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1882, 76)
  (lstm): LSTM(102, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=54, bias=True)
  (linear2): Linear(in_features=54, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:45.165380
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1117.469269
	Epoch 1....
Epoch has taken 0:00:18.651363
Number of used sentences in train = 313
Total loss for epoch 1: 760.067931
	Epoch 2....
Epoch has taken 0:00:18.831531
Number of used sentences in train = 313
Total loss for epoch 2: 640.664113
	Epoch 3....
Epoch has taken 0:00:18.800380
Number of used sentences in train = 313
Total loss for epoch 3: 584.935588
	Epoch 4....
Epoch has taken 0:00:18.690285
Number of used sentences in train = 313
Total loss for epoch 4: 555.978724
	Epoch 5....
Epoch has taken 0:00:18.445117
Number of used sentences in train = 313
Total loss for epoch 5: 533.852830
	Epoch 6....
Epoch has taken 0:00:18.491375
Number of used sentences in train = 313
Total loss for epoch 6: 533.514247
	Epoch 7....
Epoch has taken 0:00:18.707679
Number of used sentences in train = 313
Total loss for epoch 7: 527.132538
	Epoch 8....
Epoch has taken 0:00:18.550576
Number of used sentences in train = 313
Total loss for epoch 8: 521.738097
	Epoch 9....
Epoch has taken 0:00:18.323322
Number of used sentences in train = 313
Total loss for epoch 9: 518.802086
	Epoch 10....
Epoch has taken 0:00:18.480970
Number of used sentences in train = 313
Total loss for epoch 10: 517.391276
	Epoch 11....
Epoch has taken 0:00:18.408699
Number of used sentences in train = 313
Total loss for epoch 11: 516.495661
	Epoch 12....
Epoch has taken 0:00:18.766954
Number of used sentences in train = 313
Total loss for epoch 12: 515.626878
	Epoch 13....
Epoch has taken 0:00:18.548909
Number of used sentences in train = 313
Total loss for epoch 13: 515.868003
	Epoch 14....
Epoch has taken 0:00:18.735936
Number of used sentences in train = 313
Total loss for epoch 14: 514.439447
Epoch has taken 0:00:18.556984

==================================================================================================
	Training time : 0:45:22.652997
==================================================================================================
	Identification : 0.476

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1680, 76)
  (lstm): LSTM(102, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=54, bias=True)
  (linear2): Linear(in_features=54, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8334.137913
validation loss after epoch 0 : 679.773144
	Epoch 1....
Epoch has taken 0:01:59.089620
Number of used sentences in train = 2074
Total loss for epoch 1: 5296.745066
validation loss after epoch 1 : 638.390216
	Epoch 2....
Epoch has taken 0:01:58.911376
Number of used sentences in train = 2074
Total loss for epoch 2: 4475.561432
validation loss after epoch 2 : 749.406136
	Epoch 3....
Epoch has taken 0:02:00.496576
Number of used sentences in train = 2074
Total loss for epoch 3: 3973.805774
validation loss after epoch 3 : 717.312349
	Epoch 4....
Epoch has taken 0:01:59.927177
Number of used sentences in train = 2074
Total loss for epoch 4: 3663.015737
validation loss after epoch 4 : 778.779532
	Epoch 5....
Epoch has taken 0:02:00.497115
Number of used sentences in train = 2074
Total loss for epoch 5: 3465.017820
validation loss after epoch 5 : 833.622305
	Epoch 6....
Epoch has taken 0:02:00.095456
Number of used sentences in train = 2074
Total loss for epoch 6: 3348.365047
validation loss after epoch 6 : 872.008720
	Epoch 7....
Epoch has taken 0:02:00.732580
Number of used sentences in train = 2074
Total loss for epoch 7: 3311.474359
validation loss after epoch 7 : 881.633139
	Epoch 8....
Epoch has taken 0:02:00.402009
Number of used sentences in train = 2074
Total loss for epoch 8: 3284.278778
validation loss after epoch 8 : 897.473555
	Epoch 9....
Epoch has taken 0:02:00.059967
Number of used sentences in train = 2074
Total loss for epoch 9: 3259.336042
validation loss after epoch 9 : 917.279691
	Epoch 10....
Epoch has taken 0:02:00.267574
Number of used sentences in train = 2074
Total loss for epoch 10: 3244.935999
validation loss after epoch 10 : 934.541128
	Epoch 11....
Epoch has taken 0:02:00.665707
Number of used sentences in train = 2074
Total loss for epoch 11: 3233.924020
validation loss after epoch 11 : 949.215937
	Epoch 12....
Epoch has taken 0:02:00.600881
Number of used sentences in train = 2074
Total loss for epoch 12: 3222.322515
validation loss after epoch 12 : 948.140342
	Epoch 13....
Epoch has taken 0:02:00.533884
Number of used sentences in train = 2074
Total loss for epoch 13: 3208.877274
validation loss after epoch 13 : 970.004647
	Epoch 14....
Epoch has taken 0:02:00.367301
Number of used sentences in train = 2074
Total loss for epoch 14: 3201.244826
validation loss after epoch 14 : 982.220032
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1680, 76)
  (lstm): LSTM(102, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=54, bias=True)
  (linear2): Linear(in_features=54, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:00.027294
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1151.183456
	Epoch 1....
Epoch has taken 0:00:12.324488
Number of used sentences in train = 231
Total loss for epoch 1: 574.568909
	Epoch 2....
Epoch has taken 0:00:11.935796
Number of used sentences in train = 231
Total loss for epoch 2: 428.078631
	Epoch 3....
Epoch has taken 0:00:11.110453
Number of used sentences in train = 231
Total loss for epoch 3: 375.704995
	Epoch 4....
Epoch has taken 0:00:12.389440
Number of used sentences in train = 231
Total loss for epoch 4: 358.908215
	Epoch 5....
Epoch has taken 0:00:12.375055
Number of used sentences in train = 231
Total loss for epoch 5: 351.891003
	Epoch 6....
Epoch has taken 0:00:12.313075
Number of used sentences in train = 231
Total loss for epoch 6: 349.410809
	Epoch 7....
Epoch has taken 0:00:12.186125
Number of used sentences in train = 231
Total loss for epoch 7: 347.955906
	Epoch 8....
Epoch has taken 0:00:12.354073
Number of used sentences in train = 231
Total loss for epoch 8: 347.107858
	Epoch 9....
Epoch has taken 0:00:12.389120
Number of used sentences in train = 231
Total loss for epoch 9: 346.437648
	Epoch 10....
Epoch has taken 0:00:12.355211
Number of used sentences in train = 231
Total loss for epoch 10: 346.046238
	Epoch 11....
Epoch has taken 0:00:12.168285
Number of used sentences in train = 231
Total loss for epoch 11: 345.760079
	Epoch 12....
Epoch has taken 0:00:12.224637
Number of used sentences in train = 231
Total loss for epoch 12: 345.490077
	Epoch 13....
Epoch has taken 0:00:12.317073
Number of used sentences in train = 231
Total loss for epoch 13: 345.361728
	Epoch 14....
Epoch has taken 0:00:12.346302
Number of used sentences in train = 231
Total loss for epoch 14: 345.268676
Epoch has taken 0:00:12.310930

==================================================================================================
	Training time : 0:33:06.115415
==================================================================================================
	Identification : 0.277

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(3369, 76)
  (lstm): LSTM(102, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=54, bias=True)
  (linear2): Linear(in_features=54, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12258.125771
validation loss after epoch 0 : 1125.610760
	Epoch 1....
Epoch has taken 0:03:49.120791
Number of used sentences in train = 3226
Total loss for epoch 1: 9425.161248
validation loss after epoch 1 : 1086.354183
	Epoch 2....
Epoch has taken 0:03:48.058223
Number of used sentences in train = 3226
Total loss for epoch 2: 8446.369813
validation loss after epoch 2 : 1094.141856
	Epoch 3....
Epoch has taken 0:03:48.885572
Number of used sentences in train = 3226
Total loss for epoch 3: 7865.483423
validation loss after epoch 3 : 1133.584758
	Epoch 4....
Epoch has taken 0:03:48.078505
Number of used sentences in train = 3226
Total loss for epoch 4: 7409.711941
validation loss after epoch 4 : 1192.264277
	Epoch 5....
Epoch has taken 0:03:47.872057
Number of used sentences in train = 3226
Total loss for epoch 5: 7091.663022
validation loss after epoch 5 : 1283.778495
	Epoch 6....
Epoch has taken 0:03:39.846431
Number of used sentences in train = 3226
Total loss for epoch 6: 6849.856735
validation loss after epoch 6 : 1246.300718
	Epoch 7....
Epoch has taken 0:03:33.696390
Number of used sentences in train = 3226
Total loss for epoch 7: 6681.822325
validation loss after epoch 7 : 1364.918627
	Epoch 8....
Epoch has taken 0:03:37.541056
Number of used sentences in train = 3226
Total loss for epoch 8: 6538.151081
validation loss after epoch 8 : 1379.901655
	Epoch 9....
Epoch has taken 0:03:33.523566
Number of used sentences in train = 3226
Total loss for epoch 9: 6442.131043
validation loss after epoch 9 : 1442.795635
	Epoch 10....
Epoch has taken 0:03:55.189821
Number of used sentences in train = 3226
Total loss for epoch 10: 6369.868494
validation loss after epoch 10 : 1451.340155
	Epoch 11....
Epoch has taken 0:03:49.024494
Number of used sentences in train = 3226
Total loss for epoch 11: 6322.405071
validation loss after epoch 11 : 1545.226969
	Epoch 12....
Epoch has taken 0:03:48.655739
Number of used sentences in train = 3226
Total loss for epoch 12: 6290.781992
validation loss after epoch 12 : 1520.278562
	Epoch 13....
Epoch has taken 0:03:46.596055
Number of used sentences in train = 3226
Total loss for epoch 13: 6243.879442
validation loss after epoch 13 : 1567.774310
	Epoch 14....
Epoch has taken 0:03:49.357586
Number of used sentences in train = 3226
Total loss for epoch 14: 6224.673368
validation loss after epoch 14 : 1632.184307
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(3369, 76)
  (lstm): LSTM(102, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=54, bias=True)
  (linear2): Linear(in_features=54, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:49.350758
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2408.899008
	Epoch 1....
Epoch has taken 0:00:22.788149
Number of used sentences in train = 359
Total loss for epoch 1: 981.610544
	Epoch 2....
Epoch has taken 0:00:22.601473
Number of used sentences in train = 359
Total loss for epoch 2: 834.811036
	Epoch 3....
Epoch has taken 0:00:22.607159
Number of used sentences in train = 359
Total loss for epoch 3: 765.042750
	Epoch 4....
Epoch has taken 0:00:22.628430
Number of used sentences in train = 359
Total loss for epoch 4: 722.609469
	Epoch 5....
Epoch has taken 0:00:22.600683
Number of used sentences in train = 359
Total loss for epoch 5: 699.457994
	Epoch 6....
Epoch has taken 0:00:22.818800
Number of used sentences in train = 359
Total loss for epoch 6: 689.957684
	Epoch 7....
Epoch has taken 0:00:22.328066
Number of used sentences in train = 359
Total loss for epoch 7: 679.003940
	Epoch 8....
Epoch has taken 0:00:22.657105
Number of used sentences in train = 359
Total loss for epoch 8: 675.622285
	Epoch 9....
Epoch has taken 0:00:22.576496
Number of used sentences in train = 359
Total loss for epoch 9: 682.718256
	Epoch 10....
Epoch has taken 0:00:22.668479
Number of used sentences in train = 359
Total loss for epoch 10: 675.472515
	Epoch 11....
Epoch has taken 0:00:22.736540
Number of used sentences in train = 359
Total loss for epoch 11: 673.693977
	Epoch 12....
Epoch has taken 0:00:22.701239
Number of used sentences in train = 359
Total loss for epoch 12: 672.944166
	Epoch 13....
Epoch has taken 0:00:22.588873
Number of used sentences in train = 359
Total loss for epoch 13: 672.422947
	Epoch 14....
Epoch has taken 0:00:22.340310
Number of used sentences in train = 359
Total loss for epoch 14: 671.778798
Epoch has taken 0:00:22.627521

==================================================================================================
	Training time : 1:02:04.747325
==================================================================================================
	Identification : 0.058

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 21, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 26, 'lstmDropout': 0.4, 'denseActivation': 'tanh', 'wordDim': 96, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(1177, 96)
  (lstm): LSTM(117, 26, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
	Epoch 0....
Number of used sentences in train = 2811
Total loss for epoch 0: 12241.783102
validation loss after epoch 0 : 1009.104486
	Epoch 1....
Epoch has taken 0:03:06.075663
Number of used sentences in train = 2811
Total loss for epoch 1: 8698.044735
validation loss after epoch 1 : 1024.504536
	Epoch 2....
Epoch has taken 0:03:05.442148
Number of used sentences in train = 2811
Total loss for epoch 2: 7901.983504
validation loss after epoch 2 : 921.716832
	Epoch 3....
Epoch has taken 0:03:04.422393
Number of used sentences in train = 2811
Total loss for epoch 3: 7454.376488
validation loss after epoch 3 : 925.793181
	Epoch 4....
Epoch has taken 0:03:01.590109
Number of used sentences in train = 2811
Total loss for epoch 4: 7127.244192
validation loss after epoch 4 : 896.479458
	Epoch 5....
Epoch has taken 0:03:02.152469
Number of used sentences in train = 2811
Total loss for epoch 5: 6775.622450
validation loss after epoch 5 : 901.249065
	Epoch 6....
Epoch has taken 0:02:56.602236
Number of used sentences in train = 2811
Total loss for epoch 6: 6551.493672
validation loss after epoch 6 : 872.557807
	Epoch 7....
Epoch has taken 0:03:04.778605
Number of used sentences in train = 2811
Total loss for epoch 7: 6395.733573
validation loss after epoch 7 : 924.544063
	Epoch 8....
Epoch has taken 0:03:01.718997
Number of used sentences in train = 2811
Total loss for epoch 8: 6184.158968
validation loss after epoch 8 : 957.735084
	Epoch 9....
Epoch has taken 0:03:03.426533
Number of used sentences in train = 2811
Total loss for epoch 9: 6062.832296
validation loss after epoch 9 : 902.563205
	Epoch 10....
Epoch has taken 0:02:58.976177
Number of used sentences in train = 2811
Total loss for epoch 10: 6022.884886
validation loss after epoch 10 : 941.235289
	Epoch 11....
Epoch has taken 0:03:04.070764
Number of used sentences in train = 2811
Total loss for epoch 11: 5843.506485
validation loss after epoch 11 : 986.427937
	Epoch 12....
Epoch has taken 0:02:55.709888
Number of used sentences in train = 2811
Total loss for epoch 12: 5832.410071
validation loss after epoch 12 : 1000.062955
	Epoch 13....
Epoch has taken 0:03:05.654839
Number of used sentences in train = 2811
Total loss for epoch 13: 5773.951623
validation loss after epoch 13 : 932.006785
	Epoch 14....
Epoch has taken 0:03:00.790196
Number of used sentences in train = 2811
Total loss for epoch 14: 5734.660029
validation loss after epoch 14 : 962.160983
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(1177, 96)
  (lstm): LSTM(117, 26, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:58.925018
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1208.790615
	Epoch 1....
Epoch has taken 0:00:19.925500
Number of used sentences in train = 313
Total loss for epoch 1: 808.594366
	Epoch 2....
Epoch has taken 0:00:19.927930
Number of used sentences in train = 313
Total loss for epoch 2: 725.479946
	Epoch 3....
Epoch has taken 0:00:19.936688
Number of used sentences in train = 313
Total loss for epoch 3: 673.763951
	Epoch 4....
Epoch has taken 0:00:19.911016
Number of used sentences in train = 313
Total loss for epoch 4: 651.499882
	Epoch 5....
Epoch has taken 0:00:19.914996
Number of used sentences in train = 313
Total loss for epoch 5: 635.472339
	Epoch 6....
Epoch has taken 0:00:20.785347
Number of used sentences in train = 313
Total loss for epoch 6: 584.525021
	Epoch 7....
Epoch has taken 0:00:18.945822
Number of used sentences in train = 313
Total loss for epoch 7: 596.655072
	Epoch 8....
Epoch has taken 0:00:19.314216
Number of used sentences in train = 313
Total loss for epoch 8: 581.107326
	Epoch 9....
Epoch has taken 0:00:17.962685
Number of used sentences in train = 313
Total loss for epoch 9: 590.198983
	Epoch 10....
Epoch has taken 0:00:17.941279
Number of used sentences in train = 313
Total loss for epoch 10: 565.001098
	Epoch 11....
Epoch has taken 0:00:19.333847
Number of used sentences in train = 313
Total loss for epoch 11: 574.804092
	Epoch 12....
Epoch has taken 0:00:19.889945
Number of used sentences in train = 313
Total loss for epoch 12: 554.194255
	Epoch 13....
Epoch has taken 0:00:19.923138
Number of used sentences in train = 313
Total loss for epoch 13: 560.564271
	Epoch 14....
Epoch has taken 0:00:20.229287
Number of used sentences in train = 313
Total loss for epoch 14: 542.691994
Epoch has taken 0:00:19.910207

==================================================================================================
	Training time : 0:50:24.701661
==================================================================================================
	Identification : 0.469

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(1133, 96)
  (lstm): LSTM(117, 26, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9144.177807
validation loss after epoch 0 : 792.226067
	Epoch 1....
Epoch has taken 0:02:09.393151
Number of used sentences in train = 2074
Total loss for epoch 1: 6256.636954
validation loss after epoch 1 : 700.033634
	Epoch 2....
Epoch has taken 0:02:09.363005
Number of used sentences in train = 2074
Total loss for epoch 2: 5517.077005
validation loss after epoch 2 : 704.941460
	Epoch 3....
Epoch has taken 0:02:09.481690
Number of used sentences in train = 2074
Total loss for epoch 3: 5077.568561
validation loss after epoch 3 : 685.563149
	Epoch 4....
Epoch has taken 0:02:09.189750
Number of used sentences in train = 2074
Total loss for epoch 4: 4793.569368
validation loss after epoch 4 : 670.018083
	Epoch 5....
Epoch has taken 0:02:08.372662
Number of used sentences in train = 2074
Total loss for epoch 5: 4559.671600
validation loss after epoch 5 : 685.664770
	Epoch 6....
Epoch has taken 0:02:09.515298
Number of used sentences in train = 2074
Total loss for epoch 6: 4389.422201
validation loss after epoch 6 : 695.497579
	Epoch 7....
Epoch has taken 0:01:57.685148
Number of used sentences in train = 2074
Total loss for epoch 7: 4201.797337
validation loss after epoch 7 : 655.936101
	Epoch 8....
Epoch has taken 0:02:02.217230
Number of used sentences in train = 2074
Total loss for epoch 8: 4089.571055
validation loss after epoch 8 : 709.038278
	Epoch 9....
Epoch has taken 0:02:01.750544
Number of used sentences in train = 2074
Total loss for epoch 9: 3952.987402
validation loss after epoch 9 : 699.339953
	Epoch 10....
Epoch has taken 0:01:58.431748
Number of used sentences in train = 2074
Total loss for epoch 10: 3871.817453
validation loss after epoch 10 : 665.656350
	Epoch 11....
Epoch has taken 0:02:00.237143
Number of used sentences in train = 2074
Total loss for epoch 11: 3856.067537
validation loss after epoch 11 : 693.693627
	Epoch 12....
Epoch has taken 0:01:57.124676
Number of used sentences in train = 2074
Total loss for epoch 12: 3820.328498
validation loss after epoch 12 : 688.267019
	Epoch 13....
Epoch has taken 0:01:56.252062
Number of used sentences in train = 2074
Total loss for epoch 13: 3728.453871
validation loss after epoch 13 : 749.665970
	Epoch 14....
Epoch has taken 0:01:56.170931
Number of used sentences in train = 2074
Total loss for epoch 14: 3627.846686
validation loss after epoch 14 : 760.299307
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(1133, 96)
  (lstm): LSTM(117, 26, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.221947
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1518.973023
	Epoch 1....
Epoch has taken 0:00:11.930741
Number of used sentences in train = 231
Total loss for epoch 1: 581.347365
	Epoch 2....
Epoch has taken 0:00:11.956059
Number of used sentences in train = 231
Total loss for epoch 2: 479.141546
	Epoch 3....
Epoch has taken 0:00:11.932515
Number of used sentences in train = 231
Total loss for epoch 3: 443.244376
	Epoch 4....
Epoch has taken 0:00:11.953308
Number of used sentences in train = 231
Total loss for epoch 4: 406.156257
	Epoch 5....
Epoch has taken 0:00:11.921967
Number of used sentences in train = 231
Total loss for epoch 5: 428.222878
	Epoch 6....
Epoch has taken 0:00:11.938874
Number of used sentences in train = 231
Total loss for epoch 6: 385.518940
	Epoch 7....
Epoch has taken 0:00:11.931375
Number of used sentences in train = 231
Total loss for epoch 7: 395.411014
	Epoch 8....
Epoch has taken 0:00:11.958046
Number of used sentences in train = 231
Total loss for epoch 8: 375.767274
	Epoch 9....
Epoch has taken 0:00:11.934662
Number of used sentences in train = 231
Total loss for epoch 9: 381.600029
	Epoch 10....
Epoch has taken 0:00:11.939224
Number of used sentences in train = 231
Total loss for epoch 10: 368.977952
	Epoch 11....
Epoch has taken 0:00:11.929533
Number of used sentences in train = 231
Total loss for epoch 11: 377.028377
	Epoch 12....
Epoch has taken 0:00:11.955261
Number of used sentences in train = 231
Total loss for epoch 12: 368.533295
	Epoch 13....
Epoch has taken 0:00:11.921765
Number of used sentences in train = 231
Total loss for epoch 13: 361.914315
	Epoch 14....
Epoch has taken 0:00:11.938511
Number of used sentences in train = 231
Total loss for epoch 14: 365.840887
Epoch has taken 0:00:11.931809

==================================================================================================
	Training time : 0:33:41.818488
==================================================================================================
	Identification : 0.423

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 21)
  (w_embeddings): Embedding(1202, 96)
  (lstm): LSTM(117, 26, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15090.117238
validation loss after epoch 0 : 1369.336733
	Epoch 1....
Epoch has taken 0:03:46.590130
Number of used sentences in train = 3226
Total loss for epoch 1: 11525.719156
validation loss after epoch 1 : 1307.254577
	Epoch 2....
Epoch has taken 0:03:46.726129
Number of used sentences in train = 3226
Total loss for epoch 2: 10660.469687
validation loss after epoch 2 : 1261.926996
	Epoch 3....
Epoch has taken 0:03:46.805175
Number of used sentences in train = 3226
Total loss for epoch 3: 10076.056172
validation loss after epoch 3 : 1290.985855
	Epoch 4....
Epoch has taken 0:03:48.950101
Number of used sentences in train = 3226
Total loss for epoch 4: 9756.220403
validation loss after epoch 4 : 1301.893901
	Epoch 5....
Epoch has taken 0:03:49.083600
Number of used sentences in train = 3226
Total loss for epoch 5: 9433.236775
validation loss after epoch 5 : 1325.068942
	Epoch 6....
Epoch has taken 0:04:23.263033
Number of used sentences in train = 3226
Total loss for epoch 6: 9181.093633
validation loss after epoch 6 : 1306.074950
	Epoch 7....
Epoch has taken 0:03:46.462975
Number of used sentences in train = 3226
Total loss for epoch 7: 9001.669394
validation loss after epoch 7 : 1346.443758
	Epoch 8....
Epoch has taken 0:03:47.103648
Number of used sentences in train = 3226
Total loss for epoch 8: 8937.396356
validation loss after epoch 8 : 1325.813535
	Epoch 9....
Epoch has taken 0:03:49.002536
Number of used sentences in train = 3226
Total loss for epoch 9: 8738.706484
validation loss after epoch 9 : 1338.134919
	Epoch 10....
Epoch has taken 0:04:23.337678
Number of used sentences in train = 3226
Total loss for epoch 10: 8640.776235
validation loss after epoch 10 : 1381.988787
	Epoch 11....
Epoch has taken 0:03:48.919300
Number of used sentences in train = 3226
Total loss for epoch 11: 8512.793595
validation loss after epoch 11 : 1385.713042
	Epoch 12....
Epoch has taken 0:03:46.693189
Number of used sentences in train = 3226
Total loss for epoch 12: 8412.833858
validation loss after epoch 12 : 1425.270478
	Epoch 13....
Epoch has taken 0:03:46.993110
Number of used sentences in train = 3226
Total loss for epoch 13: 8351.403005
validation loss after epoch 13 : 1397.524200
	Epoch 14....
Epoch has taken 0:03:46.610650
Number of used sentences in train = 3226
Total loss for epoch 14: 8263.850777
validation loss after epoch 14 : 1430.388014
	TransitionClassifier(
  (p_embeddings): Embedding(13, 21)
  (w_embeddings): Embedding(1202, 96)
  (lstm): LSTM(117, 26, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=416, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.810510
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1698.284760
	Epoch 1....
Epoch has taken 0:00:22.400828
Number of used sentences in train = 359
Total loss for epoch 1: 1139.728434
	Epoch 2....
Epoch has taken 0:00:22.416485
Number of used sentences in train = 359
Total loss for epoch 2: 1042.994048
	Epoch 3....
Epoch has taken 0:00:22.395851
Number of used sentences in train = 359
Total loss for epoch 3: 1026.933206
	Epoch 4....
Epoch has taken 0:00:22.406080
Number of used sentences in train = 359
Total loss for epoch 4: 927.787253
	Epoch 5....
Epoch has taken 0:00:22.412714
Number of used sentences in train = 359
Total loss for epoch 5: 898.602407
	Epoch 6....
Epoch has taken 0:00:22.395322
Number of used sentences in train = 359
Total loss for epoch 6: 846.787523
	Epoch 7....
Epoch has taken 0:00:22.376770
Number of used sentences in train = 359
Total loss for epoch 7: 840.493438
	Epoch 8....
Epoch has taken 0:00:22.357213
Number of used sentences in train = 359
Total loss for epoch 8: 829.170527
	Epoch 9....
Epoch has taken 0:00:22.385480
Number of used sentences in train = 359
Total loss for epoch 9: 790.428693
	Epoch 10....
Epoch has taken 0:00:22.381317
Number of used sentences in train = 359
Total loss for epoch 10: 776.567719
	Epoch 11....
Epoch has taken 0:00:22.384102
Number of used sentences in train = 359
Total loss for epoch 11: 786.716750
	Epoch 12....
Epoch has taken 0:00:22.360227
Number of used sentences in train = 359
Total loss for epoch 12: 786.163977
	Epoch 13....
Epoch has taken 0:00:22.351838
Number of used sentences in train = 359
Total loss for epoch 13: 768.228960
	Epoch 14....
Epoch has taken 0:00:22.363710
Number of used sentences in train = 359
Total loss for epoch 14: 757.359412
Epoch has taken 0:00:22.324347

==================================================================================================
	Training time : 1:03:41.723659
==================================================================================================
	Identification : 0.488

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 32, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 65, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 44, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 91, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 65)
  (w_embeddings): Embedding(9223, 91)
  (lstm): LSTM(156, 44, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13873.839024
validation loss after epoch 0 : 1124.858793
	Epoch 1....
Epoch has taken 0:02:49.420554
Number of used sentences in train = 2811
Total loss for epoch 1: 9148.736894
validation loss after epoch 1 : 1085.441465
	Epoch 2....
Epoch has taken 0:02:49.453790
Number of used sentences in train = 2811
Total loss for epoch 2: 7517.934792
validation loss after epoch 2 : 1098.560795
	Epoch 3....
Epoch has taken 0:02:54.015692
Number of used sentences in train = 2811
Total loss for epoch 3: 6472.771877
validation loss after epoch 3 : 1148.750143
	Epoch 4....
Epoch has taken 0:02:49.368047
Number of used sentences in train = 2811
Total loss for epoch 4: 5864.384787
validation loss after epoch 4 : 1307.005807
	Epoch 5....
Epoch has taken 0:02:51.031330
Number of used sentences in train = 2811
Total loss for epoch 5: 5408.051592
validation loss after epoch 5 : 1290.264842
	Epoch 6....
Epoch has taken 0:02:50.875360
Number of used sentences in train = 2811
Total loss for epoch 6: 5215.113924
validation loss after epoch 6 : 1382.919795
	Epoch 7....
Epoch has taken 0:02:50.772234
Number of used sentences in train = 2811
Total loss for epoch 7: 5072.200655
validation loss after epoch 7 : 1380.341411
	Epoch 8....
Epoch has taken 0:02:49.269558
Number of used sentences in train = 2811
Total loss for epoch 8: 4923.554139
validation loss after epoch 8 : 1438.481895
	Epoch 9....
Epoch has taken 0:02:49.875691
Number of used sentences in train = 2811
Total loss for epoch 9: 4842.256204
validation loss after epoch 9 : 1474.692528
	Epoch 10....
Epoch has taken 0:02:51.211559
Number of used sentences in train = 2811
Total loss for epoch 10: 4786.083469
validation loss after epoch 10 : 1507.100645
	Epoch 11....
Epoch has taken 0:02:50.908414
Number of used sentences in train = 2811
Total loss for epoch 11: 4725.739201
validation loss after epoch 11 : 1513.274500
	Epoch 12....
Epoch has taken 0:02:50.813530
Number of used sentences in train = 2811
Total loss for epoch 12: 4671.840027
validation loss after epoch 12 : 1637.860682
	Epoch 13....
Epoch has taken 0:03:08.386959
Number of used sentences in train = 2811
Total loss for epoch 13: 4621.007408
validation loss after epoch 13 : 1608.524317
	Epoch 14....
Epoch has taken 0:03:07.636319
Number of used sentences in train = 2811
Total loss for epoch 14: 4620.942157
validation loss after epoch 14 : 1606.070326
	TransitionClassifier(
  (p_embeddings): Embedding(18, 65)
  (w_embeddings): Embedding(9223, 91)
  (lstm): LSTM(156, 44, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:07.979190
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1711.999808
	Epoch 1....
Epoch has taken 0:00:20.036615
Number of used sentences in train = 313
Total loss for epoch 1: 828.922314
	Epoch 2....
Epoch has taken 0:00:19.948861
Number of used sentences in train = 313
Total loss for epoch 2: 644.635653
	Epoch 3....
Epoch has taken 0:00:19.956378
Number of used sentences in train = 313
Total loss for epoch 3: 565.691044
	Epoch 4....
Epoch has taken 0:00:19.989455
Number of used sentences in train = 313
Total loss for epoch 4: 538.589746
	Epoch 5....
Epoch has taken 0:00:20.001581
Number of used sentences in train = 313
Total loss for epoch 5: 526.109308
	Epoch 6....
Epoch has taken 0:00:19.970515
Number of used sentences in train = 313
Total loss for epoch 6: 522.179487
	Epoch 7....
Epoch has taken 0:00:20.150450
Number of used sentences in train = 313
Total loss for epoch 7: 519.855605
	Epoch 8....
Epoch has taken 0:00:20.032464
Number of used sentences in train = 313
Total loss for epoch 8: 510.021015
	Epoch 9....
Epoch has taken 0:00:20.014789
Number of used sentences in train = 313
Total loss for epoch 9: 505.452150
	Epoch 10....
Epoch has taken 0:00:19.998421
Number of used sentences in train = 313
Total loss for epoch 10: 504.180298
	Epoch 11....
Epoch has taken 0:00:19.992618
Number of used sentences in train = 313
Total loss for epoch 11: 506.112012
	Epoch 12....
Epoch has taken 0:00:19.926677
Number of used sentences in train = 313
Total loss for epoch 12: 512.878582
	Epoch 13....
Epoch has taken 0:00:19.986384
Number of used sentences in train = 313
Total loss for epoch 13: 508.209009
	Epoch 14....
Epoch has taken 0:00:20.172338
Number of used sentences in train = 313
Total loss for epoch 14: 506.792357
Epoch has taken 0:00:19.992084

==================================================================================================
	Training time : 0:48:31.712339
==================================================================================================
	Identification : 0.386

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 65)
  (w_embeddings): Embedding(7079, 91)
  (lstm): LSTM(156, 44, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11552.693911
validation loss after epoch 0 : 990.426087
	Epoch 1....
Epoch has taken 0:02:06.984600
Number of used sentences in train = 2074
Total loss for epoch 1: 7463.537122
validation loss after epoch 1 : 925.764128
	Epoch 2....
Epoch has taken 0:02:07.480009
Number of used sentences in train = 2074
Total loss for epoch 2: 5987.307747
validation loss after epoch 2 : 830.939876
	Epoch 3....
Epoch has taken 0:02:05.874623
Number of used sentences in train = 2074
Total loss for epoch 3: 5018.259127
validation loss after epoch 3 : 986.921111
	Epoch 4....
Epoch has taken 0:02:06.801685
Number of used sentences in train = 2074
Total loss for epoch 4: 4433.842108
validation loss after epoch 4 : 945.712470
	Epoch 5....
Epoch has taken 0:02:07.229842
Number of used sentences in train = 2074
Total loss for epoch 5: 3973.893779
validation loss after epoch 5 : 1016.991398
	Epoch 6....
Epoch has taken 0:02:06.704969
Number of used sentences in train = 2074
Total loss for epoch 6: 3775.219439
validation loss after epoch 6 : 1074.104704
	Epoch 7....
Epoch has taken 0:02:07.189409
Number of used sentences in train = 2074
Total loss for epoch 7: 3642.427043
validation loss after epoch 7 : 1140.893216
	Epoch 8....
Epoch has taken 0:02:08.440335
Number of used sentences in train = 2074
Total loss for epoch 8: 3524.523302
validation loss after epoch 8 : 1128.378520
	Epoch 9....
Epoch has taken 0:02:06.625125
Number of used sentences in train = 2074
Total loss for epoch 9: 3467.979855
validation loss after epoch 9 : 1182.406044
	Epoch 10....
Epoch has taken 0:02:06.942603
Number of used sentences in train = 2074
Total loss for epoch 10: 3417.380931
validation loss after epoch 10 : 1179.670502
	Epoch 11....
Epoch has taken 0:02:07.956569
Number of used sentences in train = 2074
Total loss for epoch 11: 3369.522727
validation loss after epoch 11 : 1200.662035
	Epoch 12....
Epoch has taken 0:02:08.639687
Number of used sentences in train = 2074
Total loss for epoch 12: 3350.889134
validation loss after epoch 12 : 1268.920507
	Epoch 13....
Epoch has taken 0:02:28.424224
Number of used sentences in train = 2074
Total loss for epoch 13: 3282.003799
validation loss after epoch 13 : 1265.817931
	Epoch 14....
Epoch has taken 0:02:07.545406
Number of used sentences in train = 2074
Total loss for epoch 14: 3278.648119
validation loss after epoch 14 : 1342.365940
	TransitionClassifier(
  (p_embeddings): Embedding(18, 65)
  (w_embeddings): Embedding(7079, 91)
  (lstm): LSTM(156, 44, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:11.071730
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1378.400478
	Epoch 1....
Epoch has taken 0:00:12.882814
Number of used sentences in train = 231
Total loss for epoch 1: 859.762206
	Epoch 2....
Epoch has taken 0:00:12.873468
Number of used sentences in train = 231
Total loss for epoch 2: 511.395006
	Epoch 3....
Epoch has taken 0:00:12.951530
Number of used sentences in train = 231
Total loss for epoch 3: 424.298000
	Epoch 4....
Epoch has taken 0:00:12.927344
Number of used sentences in train = 231
Total loss for epoch 4: 370.046905
	Epoch 5....
Epoch has taken 0:00:12.754634
Number of used sentences in train = 231
Total loss for epoch 5: 376.349163
	Epoch 6....
Epoch has taken 0:00:12.805753
Number of used sentences in train = 231
Total loss for epoch 6: 360.905108
	Epoch 7....
Epoch has taken 0:00:12.955553
Number of used sentences in train = 231
Total loss for epoch 7: 360.345268
	Epoch 8....
Epoch has taken 0:00:12.912323
Number of used sentences in train = 231
Total loss for epoch 8: 351.408220
	Epoch 9....
Epoch has taken 0:00:12.653604
Number of used sentences in train = 231
Total loss for epoch 9: 351.709672
	Epoch 10....
Epoch has taken 0:00:12.847534
Number of used sentences in train = 231
Total loss for epoch 10: 353.022224
	Epoch 11....
Epoch has taken 0:00:13.031112
Number of used sentences in train = 231
Total loss for epoch 11: 349.015407
	Epoch 12....
Epoch has taken 0:00:12.694504
Number of used sentences in train = 231
Total loss for epoch 12: 347.999414
	Epoch 13....
Epoch has taken 0:00:12.756315
Number of used sentences in train = 231
Total loss for epoch 13: 350.016739
	Epoch 14....
Epoch has taken 0:00:13.019961
Number of used sentences in train = 231
Total loss for epoch 14: 347.491721
Epoch has taken 0:00:13.151100

==================================================================================================
	Training time : 0:35:27.479318
==================================================================================================
	Identification : 0.148

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 65)
  (w_embeddings): Embedding(17984, 91)
  (lstm): LSTM(156, 44, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 22378.539672
validation loss after epoch 0 : 1630.550595
	Epoch 1....
Epoch has taken 0:04:11.763653
Number of used sentences in train = 3226
Total loss for epoch 1: 12753.435810
validation loss after epoch 1 : 1513.698028
	Epoch 2....
Epoch has taken 0:04:10.009107
Number of used sentences in train = 3226
Total loss for epoch 2: 10500.091944
validation loss after epoch 2 : 1586.105755
	Epoch 3....
Epoch has taken 0:04:11.657741
Number of used sentences in train = 3226
Total loss for epoch 3: 9100.037704
validation loss after epoch 3 : 1691.841944
	Epoch 4....
Epoch has taken 0:04:07.138690
Number of used sentences in train = 3226
Total loss for epoch 4: 8184.109504
validation loss after epoch 4 : 1734.465449
	Epoch 5....
Epoch has taken 0:04:12.840986
Number of used sentences in train = 3226
Total loss for epoch 5: 7516.574304
validation loss after epoch 5 : 1959.716743
	Epoch 6....
Epoch has taken 0:04:11.529665
Number of used sentences in train = 3226
Total loss for epoch 6: 7126.073735
validation loss after epoch 6 : 2027.257788
	Epoch 7....
Epoch has taken 0:04:26.679626
Number of used sentences in train = 3226
Total loss for epoch 7: 6882.380054
validation loss after epoch 7 : 2131.993513
	Epoch 8....
Epoch has taken 0:04:05.969610
Number of used sentences in train = 3226
Total loss for epoch 8: 6700.344698
validation loss after epoch 8 : 2285.269196
	Epoch 9....
Epoch has taken 0:03:52.345028
Number of used sentences in train = 3226
Total loss for epoch 9: 6595.077694
validation loss after epoch 9 : 2365.960480
	Epoch 10....
Epoch has taken 0:04:05.334862
Number of used sentences in train = 3226
Total loss for epoch 10: 6465.240551
validation loss after epoch 10 : 2419.351799
	Epoch 11....
Epoch has taken 0:04:04.210078
Number of used sentences in train = 3226
Total loss for epoch 11: 6398.141764
validation loss after epoch 11 : 2423.334194
	Epoch 12....
Epoch has taken 0:04:05.713550
Number of used sentences in train = 3226
Total loss for epoch 12: 6331.228565
validation loss after epoch 12 : 2610.506767
	Epoch 13....
Epoch has taken 0:04:09.099054
Number of used sentences in train = 3226
Total loss for epoch 13: 6350.115166
validation loss after epoch 13 : 2495.164323
	Epoch 14....
Epoch has taken 0:04:08.049227
Number of used sentences in train = 3226
Total loss for epoch 14: 6298.221895
validation loss after epoch 14 : 2617.288372
	TransitionClassifier(
  (p_embeddings): Embedding(13, 65)
  (w_embeddings): Embedding(17984, 91)
  (lstm): LSTM(156, 44, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:12.454999
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2122.375345
	Epoch 1....
Epoch has taken 0:00:24.773620
Number of used sentences in train = 359
Total loss for epoch 1: 1306.995535
	Epoch 2....
Epoch has taken 0:00:24.862745
Number of used sentences in train = 359
Total loss for epoch 2: 965.551302
	Epoch 3....
Epoch has taken 0:00:24.410817
Number of used sentences in train = 359
Total loss for epoch 3: 836.956329
	Epoch 4....
Epoch has taken 0:00:24.870527
Number of used sentences in train = 359
Total loss for epoch 4: 765.856023
	Epoch 5....
Epoch has taken 0:00:24.887191
Number of used sentences in train = 359
Total loss for epoch 5: 749.391750
	Epoch 6....
Epoch has taken 0:00:24.844967
Number of used sentences in train = 359
Total loss for epoch 6: 703.333523
	Epoch 7....
Epoch has taken 0:00:24.790628
Number of used sentences in train = 359
Total loss for epoch 7: 684.505419
	Epoch 8....
Epoch has taken 0:00:24.856471
Number of used sentences in train = 359
Total loss for epoch 8: 687.470199
	Epoch 9....
Epoch has taken 0:00:24.830910
Number of used sentences in train = 359
Total loss for epoch 9: 688.981957
	Epoch 10....
Epoch has taken 0:00:24.869238
Number of used sentences in train = 359
Total loss for epoch 10: 681.209051
	Epoch 11....
Epoch has taken 0:00:24.867932
Number of used sentences in train = 359
Total loss for epoch 11: 678.124543
	Epoch 12....
Epoch has taken 0:00:24.744117
Number of used sentences in train = 359
Total loss for epoch 12: 675.602777
	Epoch 13....
Epoch has taken 0:00:24.933385
Number of used sentences in train = 359
Total loss for epoch 13: 673.084265
	Epoch 14....
Epoch has taken 0:00:24.667546
Number of used sentences in train = 359
Total loss for epoch 14: 674.606044
Epoch has taken 0:00:24.812741

==================================================================================================
	Training time : 1:08:27.520445
==================================================================================================
	Identification : 0.439

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 17, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 25, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.15, 'denseActivation': 'tanh', 'wordDim': 161, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(5858, 161)
  (lstm): LSTM(186, 51, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15649.435183
validation loss after epoch 0 : 1190.163062
	Epoch 1....
Epoch has taken 0:03:06.553138
Number of used sentences in train = 2811
Total loss for epoch 1: 9596.965332
validation loss after epoch 1 : 1061.914229
	Epoch 2....
Epoch has taken 0:03:06.202106
Number of used sentences in train = 2811
Total loss for epoch 2: 7651.622229
validation loss after epoch 2 : 1120.770247
	Epoch 3....
Epoch has taken 0:03:08.461875
Number of used sentences in train = 2811
Total loss for epoch 3: 6655.396328
validation loss after epoch 3 : 1079.974307
	Epoch 4....
Epoch has taken 0:03:08.398013
Number of used sentences in train = 2811
Total loss for epoch 4: 5976.838563
validation loss after epoch 4 : 1171.297966
	Epoch 5....
Epoch has taken 0:03:07.164230
Number of used sentences in train = 2811
Total loss for epoch 5: 5662.943512
validation loss after epoch 5 : 1219.004412
	Epoch 6....
Epoch has taken 0:03:07.946885
Number of used sentences in train = 2811
Total loss for epoch 6: 5394.689179
validation loss after epoch 6 : 1318.070186
	Epoch 7....
Epoch has taken 0:03:07.615941
Number of used sentences in train = 2811
Total loss for epoch 7: 5207.208607
validation loss after epoch 7 : 1363.639942
	Epoch 8....
Epoch has taken 0:03:08.202980
Number of used sentences in train = 2811
Total loss for epoch 8: 5023.858175
validation loss after epoch 8 : 1438.095892
	Epoch 9....
Epoch has taken 0:03:01.759323
Number of used sentences in train = 2811
Total loss for epoch 9: 4891.392810
validation loss after epoch 9 : 1480.084253
	Epoch 10....
Epoch has taken 0:02:58.318188
Number of used sentences in train = 2811
Total loss for epoch 10: 4853.759977
validation loss after epoch 10 : 1497.853668
	Epoch 11....
Epoch has taken 0:03:06.099129
Number of used sentences in train = 2811
Total loss for epoch 11: 4813.808991
validation loss after epoch 11 : 1471.146715
	Epoch 12....
Epoch has taken 0:03:07.346074
Number of used sentences in train = 2811
Total loss for epoch 12: 4771.202903
validation loss after epoch 12 : 1440.975162
	Epoch 13....
Epoch has taken 0:03:06.974562
Number of used sentences in train = 2811
Total loss for epoch 13: 4709.994393
validation loss after epoch 13 : 1527.464173
	Epoch 14....
Epoch has taken 0:03:07.060801
Number of used sentences in train = 2811
Total loss for epoch 14: 4667.418118
validation loss after epoch 14 : 1593.473089
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(5858, 161)
  (lstm): LSTM(186, 51, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:05.563714
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1524.019693
	Epoch 1....
Epoch has taken 0:00:20.822437
Number of used sentences in train = 313
Total loss for epoch 1: 866.682099
	Epoch 2....
Epoch has taken 0:00:20.829299
Number of used sentences in train = 313
Total loss for epoch 2: 694.315581
	Epoch 3....
Epoch has taken 0:00:20.833585
Number of used sentences in train = 313
Total loss for epoch 3: 645.429758
	Epoch 4....
Epoch has taken 0:00:20.842201
Number of used sentences in train = 313
Total loss for epoch 4: 606.393729
	Epoch 5....
Epoch has taken 0:00:20.831803
Number of used sentences in train = 313
Total loss for epoch 5: 583.403669
	Epoch 6....
Epoch has taken 0:00:20.328183
Number of used sentences in train = 313
Total loss for epoch 6: 572.304582
	Epoch 7....
Epoch has taken 0:00:20.819019
Number of used sentences in train = 313
Total loss for epoch 7: 571.529860
	Epoch 8....
Epoch has taken 0:00:20.817479
Number of used sentences in train = 313
Total loss for epoch 8: 564.167761
	Epoch 9....
Epoch has taken 0:00:20.472136
Number of used sentences in train = 313
Total loss for epoch 9: 563.009549
	Epoch 10....
Epoch has taken 0:00:20.828145
Number of used sentences in train = 313
Total loss for epoch 10: 560.044513
	Epoch 11....
Epoch has taken 0:00:20.809064
Number of used sentences in train = 313
Total loss for epoch 11: 558.034292
	Epoch 12....
Epoch has taken 0:00:20.809636
Number of used sentences in train = 313
Total loss for epoch 12: 557.264694
	Epoch 13....
Epoch has taken 0:00:20.817477
Number of used sentences in train = 313
Total loss for epoch 13: 556.031688
	Epoch 14....
Epoch has taken 0:00:20.840359
Number of used sentences in train = 313
Total loss for epoch 14: 561.482161
Epoch has taken 0:00:20.819760

==================================================================================================
	Training time : 0:51:45.713013
==================================================================================================
	Identification : 0.103

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(5666, 161)
  (lstm): LSTM(186, 51, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Number of used sentences in train = 2074
Total loss for epoch 0: 15073.244188
validation loss after epoch 0 : 1024.093475
	Epoch 1....
Epoch has taken 0:01:56.497366
Number of used sentences in train = 2074
Total loss for epoch 1: 7122.190380
validation loss after epoch 1 : 841.339036
	Epoch 2....
Epoch has taken 0:01:56.521352
Number of used sentences in train = 2074
Total loss for epoch 2: 5249.822718
validation loss after epoch 2 : 868.599448
	Epoch 3....
Epoch has taken 0:01:57.747085
Number of used sentences in train = 2074
Total loss for epoch 3: 4401.427050
validation loss after epoch 3 : 904.750931
	Epoch 4....
Epoch has taken 0:02:15.268855
Number of used sentences in train = 2074
Total loss for epoch 4: 3999.061561
validation loss after epoch 4 : 1015.925695
	Epoch 5....
Epoch has taken 0:01:57.564702
Number of used sentences in train = 2074
Total loss for epoch 5: 3752.922948
validation loss after epoch 5 : 1004.752787
	Epoch 6....
Epoch has taken 0:01:56.473226
Number of used sentences in train = 2074
Total loss for epoch 6: 3545.042763
validation loss after epoch 6 : 1075.734798
	Epoch 7....
Epoch has taken 0:01:56.618127
Number of used sentences in train = 2074
Total loss for epoch 7: 3498.270441
validation loss after epoch 7 : 1138.327138
	Epoch 8....
Epoch has taken 0:01:57.683033
Number of used sentences in train = 2074
Total loss for epoch 8: 3419.546783
validation loss after epoch 8 : 1182.793725
	Epoch 9....
Epoch has taken 0:01:57.496249
Number of used sentences in train = 2074
Total loss for epoch 9: 3389.279602
validation loss after epoch 9 : 1222.823172
	Epoch 10....
Epoch has taken 0:01:56.522028
Number of used sentences in train = 2074
Total loss for epoch 10: 3321.831449
validation loss after epoch 10 : 1229.508602
	Epoch 11....
Epoch has taken 0:01:56.627984
Number of used sentences in train = 2074
Total loss for epoch 11: 3288.800724
validation loss after epoch 11 : 1199.916084
	Epoch 12....
Epoch has taken 0:01:57.452663
Number of used sentences in train = 2074
Total loss for epoch 12: 3288.777258
validation loss after epoch 12 : 1168.401619
	Epoch 13....
Epoch has taken 0:01:57.591347
Number of used sentences in train = 2074
Total loss for epoch 13: 3254.251007
validation loss after epoch 13 : 1225.721797
	Epoch 14....
Epoch has taken 0:01:57.587057
Number of used sentences in train = 2074
Total loss for epoch 14: 3239.023216
validation loss after epoch 14 : 1254.320652
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(5666, 161)
  (lstm): LSTM(186, 51, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.528904
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1563.357629
	Epoch 1....
Epoch has taken 0:00:11.902182
Number of used sentences in train = 231
Total loss for epoch 1: 735.183957
	Epoch 2....
Epoch has taken 0:00:11.875883
Number of used sentences in train = 231
Total loss for epoch 2: 535.622474
	Epoch 3....
Epoch has taken 0:00:11.883274
Number of used sentences in train = 231
Total loss for epoch 3: 450.555521
	Epoch 4....
Epoch has taken 0:00:11.883824
Number of used sentences in train = 231
Total loss for epoch 4: 418.117728
	Epoch 5....
Epoch has taken 0:00:11.885579
Number of used sentences in train = 231
Total loss for epoch 5: 387.929984
	Epoch 6....
Epoch has taken 0:00:11.889740
Number of used sentences in train = 231
Total loss for epoch 6: 382.425674
	Epoch 7....
Epoch has taken 0:00:11.897569
Number of used sentences in train = 231
Total loss for epoch 7: 373.429281
	Epoch 8....
Epoch has taken 0:00:11.885224
Number of used sentences in train = 231
Total loss for epoch 8: 368.563563
	Epoch 9....
Epoch has taken 0:00:11.870543
Number of used sentences in train = 231
Total loss for epoch 9: 364.977190
	Epoch 10....
Epoch has taken 0:00:11.882553
Number of used sentences in train = 231
Total loss for epoch 10: 361.379145
	Epoch 11....
Epoch has taken 0:00:11.879175
Number of used sentences in train = 231
Total loss for epoch 11: 359.981386
	Epoch 12....
Epoch has taken 0:00:11.872181
Number of used sentences in train = 231
Total loss for epoch 12: 359.198759
	Epoch 13....
Epoch has taken 0:00:11.897792
Number of used sentences in train = 231
Total loss for epoch 13: 360.371922
	Epoch 14....
Epoch has taken 0:00:11.892855
Number of used sentences in train = 231
Total loss for epoch 14: 357.996098
Epoch has taken 0:00:11.873854

==================================================================================================
	Training time : 0:32:32.811262
==================================================================================================
	Identification : 0.368

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 25)
  (w_embeddings): Embedding(6879, 161)
  (lstm): LSTM(186, 51, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18722.933011
validation loss after epoch 0 : 1549.354230
	Epoch 1....
Epoch has taken 0:03:48.231316
Number of used sentences in train = 3226
Total loss for epoch 1: 12264.042471
validation loss after epoch 1 : 1504.883304
	Epoch 2....
Epoch has taken 0:03:48.121904
Number of used sentences in train = 3226
Total loss for epoch 2: 10151.753674
validation loss after epoch 2 : 1507.293266
	Epoch 3....
Epoch has taken 0:03:47.489396
Number of used sentences in train = 3226
Total loss for epoch 3: 8844.204462
validation loss after epoch 3 : 1588.521994
	Epoch 4....
Epoch has taken 0:03:45.966792
Number of used sentences in train = 3226
Total loss for epoch 4: 7970.935806
validation loss after epoch 4 : 1697.632198
	Epoch 5....
Epoch has taken 0:03:48.047330
Number of used sentences in train = 3226
Total loss for epoch 5: 7403.595527
validation loss after epoch 5 : 1829.828578
	Epoch 6....
Epoch has taken 0:03:47.816216
Number of used sentences in train = 3226
Total loss for epoch 6: 7008.836247
validation loss after epoch 6 : 2002.095677
	Epoch 7....
Epoch has taken 0:03:47.697166
Number of used sentences in train = 3226
Total loss for epoch 7: 6779.039677
validation loss after epoch 7 : 2010.548786
	Epoch 8....
Epoch has taken 0:03:46.029027
Number of used sentences in train = 3226
Total loss for epoch 8: 6617.165878
validation loss after epoch 8 : 2159.363285
	Epoch 9....
Epoch has taken 0:03:48.191931
Number of used sentences in train = 3226
Total loss for epoch 9: 6543.622477
validation loss after epoch 9 : 2147.838350
	Epoch 10....
Epoch has taken 0:03:47.979383
Number of used sentences in train = 3226
Total loss for epoch 10: 6416.900000
validation loss after epoch 10 : 2234.173199
	Epoch 11....
Epoch has taken 0:03:47.530934
Number of used sentences in train = 3226
Total loss for epoch 11: 6379.259719
validation loss after epoch 11 : 2225.143643
	Epoch 12....
Epoch has taken 0:03:46.426941
Number of used sentences in train = 3226
Total loss for epoch 12: 6333.722474
validation loss after epoch 12 : 2368.538503
	Epoch 13....
Epoch has taken 0:03:46.073518
Number of used sentences in train = 3226
Total loss for epoch 13: 6299.139223
validation loss after epoch 13 : 2294.469737
	Epoch 14....
Epoch has taken 0:03:48.430356
Number of used sentences in train = 3226
Total loss for epoch 14: 6267.002928
validation loss after epoch 14 : 2348.062030
	TransitionClassifier(
  (p_embeddings): Embedding(13, 25)
  (w_embeddings): Embedding(6879, 161)
  (lstm): LSTM(186, 51, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:47.923936
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2155.710979
	Epoch 1....
Epoch has taken 0:00:22.303388
Number of used sentences in train = 359
Total loss for epoch 1: 1181.247292
	Epoch 2....
Epoch has taken 0:00:22.241258
Number of used sentences in train = 359
Total loss for epoch 2: 916.660805
	Epoch 3....
Epoch has taken 0:00:22.277900
Number of used sentences in train = 359
Total loss for epoch 3: 767.822975
	Epoch 4....
Epoch has taken 0:00:22.286005
Number of used sentences in train = 359
Total loss for epoch 4: 713.677227
	Epoch 5....
Epoch has taken 0:00:22.280607
Number of used sentences in train = 359
Total loss for epoch 5: 691.704670
	Epoch 6....
Epoch has taken 0:00:22.282363
Number of used sentences in train = 359
Total loss for epoch 6: 680.217782
	Epoch 7....
Epoch has taken 0:00:22.297311
Number of used sentences in train = 359
Total loss for epoch 7: 687.401139
	Epoch 8....
Epoch has taken 0:00:22.284805
Number of used sentences in train = 359
Total loss for epoch 8: 680.267757
	Epoch 9....
Epoch has taken 0:00:22.280740
Number of used sentences in train = 359
Total loss for epoch 9: 674.208328
	Epoch 10....
Epoch has taken 0:00:22.307287
Number of used sentences in train = 359
Total loss for epoch 10: 673.144983
	Epoch 11....
Epoch has taken 0:00:22.292109
Number of used sentences in train = 359
Total loss for epoch 11: 672.608617
	Epoch 12....
Epoch has taken 0:00:22.306520
Number of used sentences in train = 359
Total loss for epoch 12: 672.509043
	Epoch 13....
Epoch has taken 0:00:22.280878
Number of used sentences in train = 359
Total loss for epoch 13: 671.841175
	Epoch 14....
Epoch has taken 0:00:22.287494
Number of used sentences in train = 359
Total loss for epoch 14: 671.627672
Epoch has taken 0:00:22.297885

==================================================================================================
	Training time : 1:02:26.935818
==================================================================================================
	Identification : 0.101

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 117, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 108, 'lstmDropout': 0.19, 'denseActivation': 'tanh', 'wordDim': 97, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 97)
  (lstm): LSTM(114, 108, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1728, out_features=117, bias=True)
  (linear2): Linear(in_features=117, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13684.039302
validation loss after epoch 0 : 993.308135
	Epoch 1....
Epoch has taken 0:02:50.997733
Number of used sentences in train = 2811
Total loss for epoch 1: 8915.443455
validation loss after epoch 1 : 1110.598322
	Epoch 2....
Epoch has taken 0:02:50.923160
Number of used sentences in train = 2811
Total loss for epoch 2: 7834.107251
validation loss after epoch 2 : 864.353370
	Epoch 3....
Epoch has taken 0:02:52.830470
Number of used sentences in train = 2811
Total loss for epoch 3: 7063.635771
validation loss after epoch 3 : 896.878662
	Epoch 4....
Epoch has taken 0:03:09.544270
Number of used sentences in train = 2811
Total loss for epoch 4: 6525.889444
validation loss after epoch 4 : 900.472661
	Epoch 5....
Epoch has taken 0:02:52.304176
Number of used sentences in train = 2811
Total loss for epoch 5: 6069.028683
validation loss after epoch 5 : 916.041116
	Epoch 6....
Epoch has taken 0:02:51.016951
Number of used sentences in train = 2811
Total loss for epoch 6: 5663.515526
validation loss after epoch 6 : 925.314054
	Epoch 7....
Epoch has taken 0:02:51.285774
Number of used sentences in train = 2811
Total loss for epoch 7: 5422.623550
validation loss after epoch 7 : 987.350770
	Epoch 8....
Epoch has taken 0:02:52.815709
Number of used sentences in train = 2811
Total loss for epoch 8: 5214.435790
validation loss after epoch 8 : 998.897413
	Epoch 9....
Epoch has taken 0:02:52.450398
Number of used sentences in train = 2811
Total loss for epoch 9: 5064.081733
validation loss after epoch 9 : 1075.613891
	Epoch 10....
Epoch has taken 0:02:52.428432
Number of used sentences in train = 2811
Total loss for epoch 10: 4957.751940
validation loss after epoch 10 : 1031.691261
	Epoch 11....
Epoch has taken 0:02:51.201328
Number of used sentences in train = 2811
Total loss for epoch 11: 4848.473577
validation loss after epoch 11 : 1130.566088
	Epoch 12....
Epoch has taken 0:02:51.058100
Number of used sentences in train = 2811
Total loss for epoch 12: 4806.435043
validation loss after epoch 12 : 1136.384983
	Epoch 13....
Epoch has taken 0:02:52.634166
Number of used sentences in train = 2811
Total loss for epoch 13: 4755.736857
validation loss after epoch 13 : 1136.329612
	Epoch 14....
Epoch has taken 0:02:52.554173
Number of used sentences in train = 2811
Total loss for epoch 14: 4767.385883
validation loss after epoch 14 : 1110.738797
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 97)
  (lstm): LSTM(114, 108, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1728, out_features=117, bias=True)
  (linear2): Linear(in_features=117, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.406757
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2208.247092
	Epoch 1....
Epoch has taken 0:00:18.112476
Number of used sentences in train = 313
Total loss for epoch 1: 996.206663
	Epoch 2....
Epoch has taken 0:00:18.125084
Number of used sentences in train = 313
Total loss for epoch 2: 798.327690
	Epoch 3....
Epoch has taken 0:00:19.161125
Number of used sentences in train = 313
Total loss for epoch 3: 709.064396
	Epoch 4....
Epoch has taken 0:00:20.042401
Number of used sentences in train = 313
Total loss for epoch 4: 617.405536
	Epoch 5....
Epoch has taken 0:00:20.167006
Number of used sentences in train = 313
Total loss for epoch 5: 595.052703
	Epoch 6....
Epoch has taken 0:00:20.167712
Number of used sentences in train = 313
Total loss for epoch 6: 587.134216
	Epoch 7....
Epoch has taken 0:00:20.182979
Number of used sentences in train = 313
Total loss for epoch 7: 556.018270
	Epoch 8....
Epoch has taken 0:00:20.277408
Number of used sentences in train = 313
Total loss for epoch 8: 542.914270
	Epoch 9....
Epoch has taken 0:00:19.092276
Number of used sentences in train = 313
Total loss for epoch 9: 543.354664
	Epoch 10....
Epoch has taken 0:00:20.135558
Number of used sentences in train = 313
Total loss for epoch 10: 539.140412
	Epoch 11....
Epoch has taken 0:00:20.157988
Number of used sentences in train = 313
Total loss for epoch 11: 544.156433
	Epoch 12....
Epoch has taken 0:00:20.166997
Number of used sentences in train = 313
Total loss for epoch 12: 539.588832
	Epoch 13....
Epoch has taken 0:00:20.150073
Number of used sentences in train = 313
Total loss for epoch 13: 540.546754
	Epoch 14....
Epoch has taken 0:00:20.213334
Number of used sentences in train = 313
Total loss for epoch 14: 538.428560
Epoch has taken 0:00:20.151777

==================================================================================================
	Training time : 0:48:13.268478
==================================================================================================
	Identification : 0.081

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 97)
  (lstm): LSTM(114, 108, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1728, out_features=117, bias=True)
  (linear2): Linear(in_features=117, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 12060.364313
validation loss after epoch 0 : 869.284983
	Epoch 1....
Epoch has taken 0:02:08.978730
Number of used sentences in train = 2074
Total loss for epoch 1: 6347.469442
validation loss after epoch 1 : 683.927136
	Epoch 2....
Epoch has taken 0:02:08.936707
Number of used sentences in train = 2074
Total loss for epoch 2: 5316.468068
validation loss after epoch 2 : 657.043133
	Epoch 3....
Epoch has taken 0:02:10.314518
Number of used sentences in train = 2074
Total loss for epoch 3: 4721.266371
validation loss after epoch 3 : 709.381540
	Epoch 4....
Epoch has taken 0:01:57.815353
Number of used sentences in train = 2074
Total loss for epoch 4: 4363.890815
validation loss after epoch 4 : 768.228765
	Epoch 5....
Epoch has taken 0:02:05.708306
Number of used sentences in train = 2074
Total loss for epoch 5: 4106.194042
validation loss after epoch 5 : 824.770632
	Epoch 6....
Epoch has taken 0:01:56.840492
Number of used sentences in train = 2074
Total loss for epoch 6: 3804.693759
validation loss after epoch 6 : 788.180950
	Epoch 7....
Epoch has taken 0:01:58.037328
Number of used sentences in train = 2074
Total loss for epoch 7: 3683.644679
validation loss after epoch 7 : 897.595578
	Epoch 8....
Epoch has taken 0:02:15.507404
Number of used sentences in train = 2074
Total loss for epoch 8: 3554.577118
validation loss after epoch 8 : 975.136003
	Epoch 9....
Epoch has taken 0:02:01.570157
Number of used sentences in train = 2074
Total loss for epoch 9: 3449.349591
validation loss after epoch 9 : 941.178825
	Epoch 10....
Epoch has taken 0:02:01.884858
Number of used sentences in train = 2074
Total loss for epoch 10: 3351.451193
validation loss after epoch 10 : 1026.292147
	Epoch 11....
Epoch has taken 0:02:02.422432
Number of used sentences in train = 2074
Total loss for epoch 11: 3324.993469
validation loss after epoch 11 : 1025.511996
	Epoch 12....
Epoch has taken 0:02:05.194016
Number of used sentences in train = 2074
Total loss for epoch 12: 3278.385252
validation loss after epoch 12 : 1049.472329
	Epoch 13....
Epoch has taken 0:02:00.439572
Number of used sentences in train = 2074
Total loss for epoch 13: 3256.842607
validation loss after epoch 13 : 1022.918719
	Epoch 14....
Epoch has taken 0:02:08.899252
Number of used sentences in train = 2074
Total loss for epoch 14: 3281.703622
validation loss after epoch 14 : 1011.633957
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 97)
  (lstm): LSTM(114, 108, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1728, out_features=117, bias=True)
  (linear2): Linear(in_features=117, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:02.027543
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1721.805167
	Epoch 1....
Epoch has taken 0:00:11.899259
Number of used sentences in train = 231
Total loss for epoch 1: 706.648444
	Epoch 2....
Epoch has taken 0:00:12.989745
Number of used sentences in train = 231
Total loss for epoch 2: 529.613879
	Epoch 3....
Epoch has taken 0:00:13.123801
Number of used sentences in train = 231
Total loss for epoch 3: 481.860059
	Epoch 4....
Epoch has taken 0:00:11.916525
Number of used sentences in train = 231
Total loss for epoch 4: 414.512776
	Epoch 5....
Epoch has taken 0:00:11.917126
Number of used sentences in train = 231
Total loss for epoch 5: 372.823973
	Epoch 6....
Epoch has taken 0:00:11.908790
Number of used sentences in train = 231
Total loss for epoch 6: 363.719857
	Epoch 7....
Epoch has taken 0:00:11.934854
Number of used sentences in train = 231
Total loss for epoch 7: 356.262668
	Epoch 8....
Epoch has taken 0:00:11.894170
Number of used sentences in train = 231
Total loss for epoch 8: 349.260957
	Epoch 9....
Epoch has taken 0:00:11.912748
Number of used sentences in train = 231
Total loss for epoch 9: 346.195508
	Epoch 10....
Epoch has taken 0:00:11.896750
Number of used sentences in train = 231
Total loss for epoch 10: 345.762530
	Epoch 11....
Epoch has taken 0:00:11.924039
Number of used sentences in train = 231
Total loss for epoch 11: 345.341129
	Epoch 12....
Epoch has taken 0:00:11.906878
Number of used sentences in train = 231
Total loss for epoch 12: 345.014080
	Epoch 13....
Epoch has taken 0:00:11.919348
Number of used sentences in train = 231
Total loss for epoch 13: 344.593894
	Epoch 14....
Epoch has taken 0:00:11.899585
Number of used sentences in train = 231
Total loss for epoch 14: 344.632908
Epoch has taken 0:00:11.932330

==================================================================================================
	Training time : 0:34:05.898518
==================================================================================================
	Identification : 0.346

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 97)
  (lstm): LSTM(114, 108, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1728, out_features=117, bias=True)
  (linear2): Linear(in_features=117, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 36137.389974
validation loss after epoch 0 : 1914.012887
	Epoch 1....
Epoch has taken 0:03:49.221356
Number of used sentences in train = 3226
Total loss for epoch 1: 13735.890265
validation loss after epoch 1 : 1330.875811
	Epoch 2....
Epoch has taken 0:03:49.425790
Number of used sentences in train = 3226
Total loss for epoch 2: 11215.056036
validation loss after epoch 2 : 1221.665793
	Epoch 3....
Epoch has taken 0:03:49.261648
Number of used sentences in train = 3226
Total loss for epoch 3: 10105.255507
validation loss after epoch 3 : 1277.192087
	Epoch 4....
Epoch has taken 0:03:48.724497
Number of used sentences in train = 3226
Total loss for epoch 4: 9388.385927
validation loss after epoch 4 : 1259.493783
	Epoch 5....
Epoch has taken 0:03:47.483062
Number of used sentences in train = 3226
Total loss for epoch 5: 8864.961526
validation loss after epoch 5 : 1304.557606
	Epoch 6....
Epoch has taken 0:03:55.650616
Number of used sentences in train = 3226
Total loss for epoch 6: 8468.795116
validation loss after epoch 6 : 1371.236432
	Epoch 7....
Epoch has taken 0:03:49.201035
Number of used sentences in train = 3226
Total loss for epoch 7: 8116.864264
validation loss after epoch 7 : 1437.295348
	Epoch 8....
Epoch has taken 0:03:49.036777
Number of used sentences in train = 3226
Total loss for epoch 8: 7893.119944
validation loss after epoch 8 : 1565.503686
	Epoch 9....
Epoch has taken 0:03:47.296558
Number of used sentences in train = 3226
Total loss for epoch 9: 7633.701638
validation loss after epoch 9 : 1562.801573
	Epoch 10....
Epoch has taken 0:03:49.416041
Number of used sentences in train = 3226
Total loss for epoch 10: 7429.765599
validation loss after epoch 10 : 1574.332230
	Epoch 11....
Epoch has taken 0:03:49.376219
Number of used sentences in train = 3226
Total loss for epoch 11: 7227.659289
validation loss after epoch 11 : 1643.688323
	Epoch 12....
Epoch has taken 0:03:47.284826
Number of used sentences in train = 3226
Total loss for epoch 12: 7117.951022
validation loss after epoch 12 : 1717.569254
	Epoch 13....
Epoch has taken 0:03:49.357601
Number of used sentences in train = 3226
Total loss for epoch 13: 6962.426459
validation loss after epoch 13 : 1741.669878
	Epoch 14....
Epoch has taken 0:03:55.180686
Number of used sentences in train = 3226
Total loss for epoch 14: 6858.455351
validation loss after epoch 14 : 1739.849300
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 97)
  (lstm): LSTM(114, 108, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=1728, out_features=117, bias=True)
  (linear2): Linear(in_features=117, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:01.285823
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2660.274683
	Epoch 1....
Epoch has taken 0:00:24.398738
Number of used sentences in train = 359
Total loss for epoch 1: 1257.432705
	Epoch 2....
Epoch has taken 0:00:24.648861
Number of used sentences in train = 359
Total loss for epoch 2: 1056.651364
	Epoch 3....
Epoch has taken 0:00:25.806019
Number of used sentences in train = 359
Total loss for epoch 3: 929.173048
	Epoch 4....
Epoch has taken 0:00:26.920559
Number of used sentences in train = 359
Total loss for epoch 4: 804.583643
	Epoch 5....
Epoch has taken 0:00:26.675695
Number of used sentences in train = 359
Total loss for epoch 5: 773.909921
	Epoch 6....
Epoch has taken 0:00:26.566309
Number of used sentences in train = 359
Total loss for epoch 6: 740.313561
	Epoch 7....
Epoch has taken 0:00:26.692440
Number of used sentences in train = 359
Total loss for epoch 7: 732.272736
	Epoch 8....
Epoch has taken 0:00:26.807377
Number of used sentences in train = 359
Total loss for epoch 8: 706.956810
	Epoch 9....
Epoch has taken 0:00:27.020230
Number of used sentences in train = 359
Total loss for epoch 9: 690.503707
	Epoch 10....
Epoch has taken 0:00:25.494812
Number of used sentences in train = 359
Total loss for epoch 10: 682.383996
	Epoch 11....
Epoch has taken 0:00:25.658567
Number of used sentences in train = 359
Total loss for epoch 11: 689.524878
	Epoch 12....
Epoch has taken 0:00:26.701941
Number of used sentences in train = 359
Total loss for epoch 12: 686.123695
	Epoch 13....
Epoch has taken 0:00:26.708579
Number of used sentences in train = 359
Total loss for epoch 13: 686.166072
	Epoch 14....
Epoch has taken 0:00:26.586448
Number of used sentences in train = 359
Total loss for epoch 14: 682.001755
Epoch has taken 0:00:26.775275

==================================================================================================
	Training time : 1:04:11.321245
==================================================================================================
	Identification : 0.057

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 73, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 32, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 29, 'lstmDropout': 0.32, 'denseActivation': 'tanh', 'wordDim': 61, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1177, 61)
  (lstm): LSTM(93, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10657.934803
validation loss after epoch 0 : 943.754457
	Epoch 1....
Epoch has taken 0:02:56.824068
Number of used sentences in train = 2811
Total loss for epoch 1: 7638.591486
validation loss after epoch 1 : 890.603656
	Epoch 2....
Epoch has taken 0:03:01.621414
Number of used sentences in train = 2811
Total loss for epoch 2: 6775.304716
validation loss after epoch 2 : 911.642122
	Epoch 3....
Epoch has taken 0:02:58.072067
Number of used sentences in train = 2811
Total loss for epoch 3: 6214.528671
validation loss after epoch 3 : 938.037498
	Epoch 4....
Epoch has taken 0:02:40.198102
Number of used sentences in train = 2811
Total loss for epoch 4: 5835.849804
validation loss after epoch 4 : 979.542013
	Epoch 5....
Epoch has taken 0:02:58.584985
Number of used sentences in train = 2811
Total loss for epoch 5: 5516.833818
validation loss after epoch 5 : 1022.346289
	Epoch 6....
Epoch has taken 0:02:55.207957
Number of used sentences in train = 2811
Total loss for epoch 6: 5273.778213
validation loss after epoch 6 : 1064.298953
	Epoch 7....
Epoch has taken 0:02:43.420420
Number of used sentences in train = 2811
Total loss for epoch 7: 5101.278171
validation loss after epoch 7 : 1117.278682
	Epoch 8....
Epoch has taken 0:02:39.789542
Number of used sentences in train = 2811
Total loss for epoch 8: 4971.802498
validation loss after epoch 8 : 1165.700732
	Epoch 9....
Epoch has taken 0:02:39.806532
Number of used sentences in train = 2811
Total loss for epoch 9: 4831.235974
validation loss after epoch 9 : 1212.275052
	Epoch 10....
Epoch has taken 0:02:49.602533
Number of used sentences in train = 2811
Total loss for epoch 10: 4774.043544
validation loss after epoch 10 : 1230.219556
	Epoch 11....
Epoch has taken 0:02:39.249373
Number of used sentences in train = 2811
Total loss for epoch 11: 4708.453776
validation loss after epoch 11 : 1276.014825
	Epoch 12....
Epoch has taken 0:02:37.852872
Number of used sentences in train = 2811
Total loss for epoch 12: 4643.680817
validation loss after epoch 12 : 1339.699252
	Epoch 13....
Epoch has taken 0:02:38.062014
Number of used sentences in train = 2811
Total loss for epoch 13: 4608.102014
validation loss after epoch 13 : 1343.991205
	Epoch 14....
Epoch has taken 0:02:37.964066
Number of used sentences in train = 2811
Total loss for epoch 14: 4589.689253
validation loss after epoch 14 : 1376.515794
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1177, 61)
  (lstm): LSTM(93, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.447139
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1474.888795
	Epoch 1....
Epoch has taken 0:00:18.834680
Number of used sentences in train = 313
Total loss for epoch 1: 744.092721
	Epoch 2....
Epoch has taken 0:00:18.846051
Number of used sentences in train = 313
Total loss for epoch 2: 607.781979
	Epoch 3....
Epoch has taken 0:00:18.840430
Number of used sentences in train = 313
Total loss for epoch 3: 557.739825
	Epoch 4....
Epoch has taken 0:00:18.833044
Number of used sentences in train = 313
Total loss for epoch 4: 534.002454
	Epoch 5....
Epoch has taken 0:00:18.828682
Number of used sentences in train = 313
Total loss for epoch 5: 522.583676
	Epoch 6....
Epoch has taken 0:00:18.834255
Number of used sentences in train = 313
Total loss for epoch 6: 513.846856
	Epoch 7....
Epoch has taken 0:00:18.835767
Number of used sentences in train = 313
Total loss for epoch 7: 507.886262
	Epoch 8....
Epoch has taken 0:00:18.835069
Number of used sentences in train = 313
Total loss for epoch 8: 504.933192
	Epoch 9....
Epoch has taken 0:00:18.846776
Number of used sentences in train = 313
Total loss for epoch 9: 503.087733
	Epoch 10....
Epoch has taken 0:00:18.841983
Number of used sentences in train = 313
Total loss for epoch 10: 502.369141
	Epoch 11....
Epoch has taken 0:00:18.813874
Number of used sentences in train = 313
Total loss for epoch 11: 503.478967
	Epoch 12....
Epoch has taken 0:00:18.835482
Number of used sentences in train = 313
Total loss for epoch 12: 507.587129
	Epoch 13....
Epoch has taken 0:00:18.841202
Number of used sentences in train = 313
Total loss for epoch 13: 503.281944
	Epoch 14....
Epoch has taken 0:00:18.819427
Number of used sentences in train = 313
Total loss for epoch 14: 502.238507
Epoch has taken 0:00:18.831512

==================================================================================================
	Training time : 0:46:25.727497
==================================================================================================
	Identification : 0.495

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1133, 61)
  (lstm): LSTM(93, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8201.126805
validation loss after epoch 0 : 696.166560
	Epoch 1....
Epoch has taken 0:01:59.942404
Number of used sentences in train = 2074
Total loss for epoch 1: 5326.863210
validation loss after epoch 1 : 654.843269
	Epoch 2....
Epoch has taken 0:02:00.004575
Number of used sentences in train = 2074
Total loss for epoch 2: 4589.475164
validation loss after epoch 2 : 671.591179
	Epoch 3....
Epoch has taken 0:01:59.811339
Number of used sentences in train = 2074
Total loss for epoch 3: 4097.182962
validation loss after epoch 3 : 708.413195
	Epoch 4....
Epoch has taken 0:02:00.872945
Number of used sentences in train = 2074
Total loss for epoch 4: 3790.515304
validation loss after epoch 4 : 731.593338
	Epoch 5....
Epoch has taken 0:02:00.765752
Number of used sentences in train = 2074
Total loss for epoch 5: 3578.945009
validation loss after epoch 5 : 786.082763
	Epoch 6....
Epoch has taken 0:01:59.794169
Number of used sentences in train = 2074
Total loss for epoch 6: 3430.467376
validation loss after epoch 6 : 834.601553
	Epoch 7....
Epoch has taken 0:01:59.897627
Number of used sentences in train = 2074
Total loss for epoch 7: 3342.796720
validation loss after epoch 7 : 897.117072
	Epoch 8....
Epoch has taken 0:01:59.792450
Number of used sentences in train = 2074
Total loss for epoch 8: 3271.003333
validation loss after epoch 8 : 910.525883
	Epoch 9....
Epoch has taken 0:02:00.977073
Number of used sentences in train = 2074
Total loss for epoch 9: 3229.446191
validation loss after epoch 9 : 962.222444
	Epoch 10....
Epoch has taken 0:02:03.471414
Number of used sentences in train = 2074
Total loss for epoch 10: 3207.914978
validation loss after epoch 10 : 964.723925
	Epoch 11....
Epoch has taken 0:01:49.109581
Number of used sentences in train = 2074
Total loss for epoch 11: 3195.313579
validation loss after epoch 11 : 983.534111
	Epoch 12....
Epoch has taken 0:01:48.249538
Number of used sentences in train = 2074
Total loss for epoch 12: 3180.822743
validation loss after epoch 12 : 1008.520447
	Epoch 13....
Epoch has taken 0:01:48.106607
Number of used sentences in train = 2074
Total loss for epoch 13: 3173.932139
validation loss after epoch 13 : 1030.900579
	Epoch 14....
Epoch has taken 0:01:47.993551
Number of used sentences in train = 2074
Total loss for epoch 14: 3170.486501
validation loss after epoch 14 : 1034.044398
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1133, 61)
  (lstm): LSTM(93, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.702927
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1313.744586
	Epoch 1....
Epoch has taken 0:00:11.098355
Number of used sentences in train = 231
Total loss for epoch 1: 533.702434
	Epoch 2....
Epoch has taken 0:00:11.105889
Number of used sentences in train = 231
Total loss for epoch 2: 415.296701
	Epoch 3....
Epoch has taken 0:00:11.095134
Number of used sentences in train = 231
Total loss for epoch 3: 374.376659
	Epoch 4....
Epoch has taken 0:00:11.122979
Number of used sentences in train = 231
Total loss for epoch 4: 356.911977
	Epoch 5....
Epoch has taken 0:00:11.093139
Number of used sentences in train = 231
Total loss for epoch 5: 351.862653
	Epoch 6....
Epoch has taken 0:00:11.115647
Number of used sentences in train = 231
Total loss for epoch 6: 349.047217
	Epoch 7....
Epoch has taken 0:00:11.106792
Number of used sentences in train = 231
Total loss for epoch 7: 347.681822
	Epoch 8....
Epoch has taken 0:00:11.103242
Number of used sentences in train = 231
Total loss for epoch 8: 346.818412
	Epoch 9....
Epoch has taken 0:00:11.102003
Number of used sentences in train = 231
Total loss for epoch 9: 346.255306
	Epoch 10....
Epoch has taken 0:00:11.094618
Number of used sentences in train = 231
Total loss for epoch 10: 345.914496
	Epoch 11....
Epoch has taken 0:00:11.107653
Number of used sentences in train = 231
Total loss for epoch 11: 345.667194
	Epoch 12....
Epoch has taken 0:00:11.087855
Number of used sentences in train = 231
Total loss for epoch 12: 345.473617
	Epoch 13....
Epoch has taken 0:00:11.110680
Number of used sentences in train = 231
Total loss for epoch 13: 345.330950
	Epoch 14....
Epoch has taken 0:00:11.092769
Number of used sentences in train = 231
Total loss for epoch 14: 345.174104
Epoch has taken 0:00:11.121872

==================================================================================================
	Training time : 0:31:57.387314
==================================================================================================
	Identification : 0.261

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 32)
  (w_embeddings): Embedding(1202, 61)
  (lstm): LSTM(93, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14427.755456
validation loss after epoch 0 : 1313.946760
	Epoch 1....
Epoch has taken 0:03:30.652019
Number of used sentences in train = 3226
Total loss for epoch 1: 11268.407671
validation loss after epoch 1 : 1292.756963
	Epoch 2....
Epoch has taken 0:03:30.551200
Number of used sentences in train = 3226
Total loss for epoch 2: 10324.602014
validation loss after epoch 2 : 1297.433534
	Epoch 3....
Epoch has taken 0:03:30.449282
Number of used sentences in train = 3226
Total loss for epoch 3: 9627.896367
validation loss after epoch 3 : 1329.621344
	Epoch 4....
Epoch has taken 0:03:30.528782
Number of used sentences in train = 3226
Total loss for epoch 4: 9137.955784
validation loss after epoch 4 : 1357.930335
	Epoch 5....
Epoch has taken 0:03:46.056460
Number of used sentences in train = 3226
Total loss for epoch 5: 8747.498065
validation loss after epoch 5 : 1402.323823
	Epoch 6....
Epoch has taken 0:03:32.460095
Number of used sentences in train = 3226
Total loss for epoch 6: 8387.602304
validation loss after epoch 6 : 1457.617803
	Epoch 7....
Epoch has taken 0:03:30.196879
Number of used sentences in train = 3226
Total loss for epoch 7: 8127.002133
validation loss after epoch 7 : 1477.076833
	Epoch 8....
Epoch has taken 0:03:30.362475
Number of used sentences in train = 3226
Total loss for epoch 8: 7862.242183
validation loss after epoch 8 : 1494.734891
	Epoch 9....
Epoch has taken 0:03:32.215578
Number of used sentences in train = 3226
Total loss for epoch 9: 7655.554579
validation loss after epoch 9 : 1569.395578
	Epoch 10....
Epoch has taken 0:04:05.334688
Number of used sentences in train = 3226
Total loss for epoch 10: 7477.799471
validation loss after epoch 10 : 1629.467435
	Epoch 11....
Epoch has taken 0:03:32.523192
Number of used sentences in train = 3226
Total loss for epoch 11: 7309.679696
validation loss after epoch 11 : 1703.873931
	Epoch 12....
Epoch has taken 0:03:30.298760
Number of used sentences in train = 3226
Total loss for epoch 12: 7160.773486
validation loss after epoch 12 : 1739.008433
	Epoch 13....
Epoch has taken 0:03:30.373109
Number of used sentences in train = 3226
Total loss for epoch 13: 7013.676945
validation loss after epoch 13 : 1795.399752
	Epoch 14....
Epoch has taken 0:03:32.351057
Number of used sentences in train = 3226
Total loss for epoch 14: 6909.806152
validation loss after epoch 14 : 1853.155159
	TransitionClassifier(
  (p_embeddings): Embedding(13, 32)
  (w_embeddings): Embedding(1202, 61)
  (lstm): LSTM(93, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.249991
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1974.026602
	Epoch 1....
Epoch has taken 0:00:20.743875
Number of used sentences in train = 359
Total loss for epoch 1: 1171.206810
	Epoch 2....
Epoch has taken 0:00:20.720716
Number of used sentences in train = 359
Total loss for epoch 2: 997.961442
	Epoch 3....
Epoch has taken 0:00:20.723035
Number of used sentences in train = 359
Total loss for epoch 3: 899.762972
	Epoch 4....
Epoch has taken 0:00:20.775547
Number of used sentences in train = 359
Total loss for epoch 4: 836.212121
	Epoch 5....
Epoch has taken 0:00:20.767781
Number of used sentences in train = 359
Total loss for epoch 5: 791.987390
	Epoch 6....
Epoch has taken 0:00:20.783660
Number of used sentences in train = 359
Total loss for epoch 6: 743.728483
	Epoch 7....
Epoch has taken 0:00:20.777229
Number of used sentences in train = 359
Total loss for epoch 7: 718.832613
	Epoch 8....
Epoch has taken 0:00:20.741093
Number of used sentences in train = 359
Total loss for epoch 8: 701.589605
	Epoch 9....
Epoch has taken 0:00:20.755074
Number of used sentences in train = 359
Total loss for epoch 9: 690.931313
	Epoch 10....
Epoch has taken 0:00:20.756969
Number of used sentences in train = 359
Total loss for epoch 10: 687.048605
	Epoch 11....
Epoch has taken 0:00:20.759515
Number of used sentences in train = 359
Total loss for epoch 11: 680.429452
	Epoch 12....
Epoch has taken 0:00:20.733798
Number of used sentences in train = 359
Total loss for epoch 12: 675.825029
	Epoch 13....
Epoch has taken 0:00:20.762435
Number of used sentences in train = 359
Total loss for epoch 13: 674.153797
	Epoch 14....
Epoch has taken 0:00:20.738212
Number of used sentences in train = 359
Total loss for epoch 14: 673.301848
Epoch has taken 0:00:20.742290

==================================================================================================
	Training time : 0:59:04.541648
==================================================================================================
	Identification : 0.401

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 43, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 40, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 25, 'lstmDropout': 0.21, 'denseActivation': 'tanh', 'wordDim': 100, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1882, 100)
  (lstm): LSTM(140, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=43, bias=True)
  (linear2): Linear(in_features=43, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 9732.210423
validation loss after epoch 0 : 859.276698
	Epoch 1....
Epoch has taken 0:02:39.534414
Number of used sentences in train = 2811
Total loss for epoch 1: 7382.466430
validation loss after epoch 1 : 848.944473
	Epoch 2....
Epoch has taken 0:02:39.504299
Number of used sentences in train = 2811
Total loss for epoch 2: 6644.825736
validation loss after epoch 2 : 854.926076
	Epoch 3....
Epoch has taken 0:03:01.807198
Number of used sentences in train = 2811
Total loss for epoch 3: 6061.243374
validation loss after epoch 3 : 908.095765
	Epoch 4....
Epoch has taken 0:02:39.141965
Number of used sentences in train = 2811
Total loss for epoch 4: 5696.798674
validation loss after epoch 4 : 907.853318
	Epoch 5....
Epoch has taken 0:02:37.653046
Number of used sentences in train = 2811
Total loss for epoch 5: 5434.916427
validation loss after epoch 5 : 916.374286
	Epoch 6....
Epoch has taken 0:02:37.920457
Number of used sentences in train = 2811
Total loss for epoch 6: 5225.290070
validation loss after epoch 6 : 957.211869
	Epoch 7....
Epoch has taken 0:02:39.229733
Number of used sentences in train = 2811
Total loss for epoch 7: 5049.704918
validation loss after epoch 7 : 996.891176
	Epoch 8....
Epoch has taken 0:02:39.466127
Number of used sentences in train = 2811
Total loss for epoch 8: 4939.645249
validation loss after epoch 8 : 1015.479406
	Epoch 9....
Epoch has taken 0:02:37.255094
Number of used sentences in train = 2811
Total loss for epoch 9: 4841.521701
validation loss after epoch 9 : 1031.247934
	Epoch 10....
Epoch has taken 0:02:36.948541
Number of used sentences in train = 2811
Total loss for epoch 10: 4787.973071
validation loss after epoch 10 : 1080.771763
	Epoch 11....
Epoch has taken 0:02:38.908015
Number of used sentences in train = 2811
Total loss for epoch 11: 4721.097077
validation loss after epoch 11 : 1082.245898
	Epoch 12....
Epoch has taken 0:03:03.713581
Number of used sentences in train = 2811
Total loss for epoch 12: 4670.552812
validation loss after epoch 12 : 1109.747612
	Epoch 13....
Epoch has taken 0:02:38.763686
Number of used sentences in train = 2811
Total loss for epoch 13: 4636.473975
validation loss after epoch 13 : 1128.608510
	Epoch 14....
Epoch has taken 0:02:38.382134
Number of used sentences in train = 2811
Total loss for epoch 14: 4601.585836
validation loss after epoch 14 : 1135.658664
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1882, 100)
  (lstm): LSTM(140, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=43, bias=True)
  (linear2): Linear(in_features=43, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:37.344722
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1485.465227
	Epoch 1....
Epoch has taken 0:00:16.596416
Number of used sentences in train = 313
Total loss for epoch 1: 676.421067
	Epoch 2....
Epoch has taken 0:00:16.608670
Number of used sentences in train = 313
Total loss for epoch 2: 581.330479
	Epoch 3....
Epoch has taken 0:00:16.612487
Number of used sentences in train = 313
Total loss for epoch 3: 543.497127
	Epoch 4....
Epoch has taken 0:00:16.618259
Number of used sentences in train = 313
Total loss for epoch 4: 525.725641
	Epoch 5....
Epoch has taken 0:00:16.601885
Number of used sentences in train = 313
Total loss for epoch 5: 516.934338
	Epoch 6....
Epoch has taken 0:00:16.588761
Number of used sentences in train = 313
Total loss for epoch 6: 512.763600
	Epoch 7....
Epoch has taken 0:00:16.623517
Number of used sentences in train = 313
Total loss for epoch 7: 509.113537
	Epoch 8....
Epoch has taken 0:00:16.593864
Number of used sentences in train = 313
Total loss for epoch 8: 507.961228
	Epoch 9....
Epoch has taken 0:00:16.612600
Number of used sentences in train = 313
Total loss for epoch 9: 506.975605
	Epoch 10....
Epoch has taken 0:00:16.605150
Number of used sentences in train = 313
Total loss for epoch 10: 506.324396
	Epoch 11....
Epoch has taken 0:00:16.630981
Number of used sentences in train = 313
Total loss for epoch 11: 505.864784
	Epoch 12....
Epoch has taken 0:00:16.612230
Number of used sentences in train = 313
Total loss for epoch 12: 505.475218
	Epoch 13....
Epoch has taken 0:00:16.600022
Number of used sentences in train = 313
Total loss for epoch 13: 505.022550
	Epoch 14....
Epoch has taken 0:00:16.612831
Number of used sentences in train = 313
Total loss for epoch 14: 505.034530
Epoch has taken 0:00:16.613437

==================================================================================================
	Training time : 0:44:35.213473
==================================================================================================
	Identification : 0.317

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1680, 100)
  (lstm): LSTM(140, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=43, bias=True)
  (linear2): Linear(in_features=43, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7677.476111
validation loss after epoch 0 : 740.525786
	Epoch 1....
Epoch has taken 0:01:48.460682
Number of used sentences in train = 2074
Total loss for epoch 1: 5066.217576
validation loss after epoch 1 : 753.480991
	Epoch 2....
Epoch has taken 0:01:48.459652
Number of used sentences in train = 2074
Total loss for epoch 2: 4297.155532
validation loss after epoch 2 : 740.295882
	Epoch 3....
Epoch has taken 0:01:48.519646
Number of used sentences in train = 2074
Total loss for epoch 3: 3831.192412
validation loss after epoch 3 : 806.008475
	Epoch 4....
Epoch has taken 0:01:47.850971
Number of used sentences in train = 2074
Total loss for epoch 4: 3542.209565
validation loss after epoch 4 : 857.045478
	Epoch 5....
Epoch has taken 0:01:47.452450
Number of used sentences in train = 2074
Total loss for epoch 5: 3378.022622
validation loss after epoch 5 : 881.563611
	Epoch 6....
Epoch has taken 0:01:48.514681
Number of used sentences in train = 2074
Total loss for epoch 6: 3283.575518
validation loss after epoch 6 : 947.187238
	Epoch 7....
Epoch has taken 0:01:48.609778
Number of used sentences in train = 2074
Total loss for epoch 7: 3240.203743
validation loss after epoch 7 : 964.059977
	Epoch 8....
Epoch has taken 0:01:48.429717
Number of used sentences in train = 2074
Total loss for epoch 8: 3218.774509
validation loss after epoch 8 : 979.330910
	Epoch 9....
Epoch has taken 0:01:47.536565
Number of used sentences in train = 2074
Total loss for epoch 9: 3201.599746
validation loss after epoch 9 : 1028.136262
	Epoch 10....
Epoch has taken 0:01:48.511253
Number of used sentences in train = 2074
Total loss for epoch 10: 3184.131383
validation loss after epoch 10 : 1028.446600
	Epoch 11....
Epoch has taken 0:01:48.598999
Number of used sentences in train = 2074
Total loss for epoch 11: 3175.416591
validation loss after epoch 11 : 1050.483988
	Epoch 12....
Epoch has taken 0:02:05.621159
Number of used sentences in train = 2074
Total loss for epoch 12: 3170.389190
validation loss after epoch 12 : 1063.915254
	Epoch 13....
Epoch has taken 0:01:48.387581
Number of used sentences in train = 2074
Total loss for epoch 13: 3166.174923
validation loss after epoch 13 : 1075.841486
	Epoch 14....
Epoch has taken 0:01:47.484106
Number of used sentences in train = 2074
Total loss for epoch 14: 3164.188795
validation loss after epoch 14 : 1084.700590
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1680, 100)
  (lstm): LSTM(140, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=43, bias=True)
  (linear2): Linear(in_features=43, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.579071
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1177.031398
	Epoch 1....
Epoch has taken 0:00:11.055331
Number of used sentences in train = 231
Total loss for epoch 1: 524.999337
	Epoch 2....
Epoch has taken 0:00:11.039520
Number of used sentences in train = 231
Total loss for epoch 2: 412.382575
	Epoch 3....
Epoch has taken 0:00:11.057460
Number of used sentences in train = 231
Total loss for epoch 3: 368.730120
	Epoch 4....
Epoch has taken 0:00:11.038320
Number of used sentences in train = 231
Total loss for epoch 4: 356.813009
	Epoch 5....
Epoch has taken 0:00:11.021676
Number of used sentences in train = 231
Total loss for epoch 5: 351.202034
	Epoch 6....
Epoch has taken 0:00:11.060791
Number of used sentences in train = 231
Total loss for epoch 6: 348.994347
	Epoch 7....
Epoch has taken 0:00:11.034460
Number of used sentences in train = 231
Total loss for epoch 7: 347.920525
	Epoch 8....
Epoch has taken 0:00:11.047261
Number of used sentences in train = 231
Total loss for epoch 8: 347.202376
	Epoch 9....
Epoch has taken 0:00:11.024103
Number of used sentences in train = 231
Total loss for epoch 9: 346.727126
	Epoch 10....
Epoch has taken 0:00:11.046662
Number of used sentences in train = 231
Total loss for epoch 10: 346.373603
	Epoch 11....
Epoch has taken 0:00:11.056332
Number of used sentences in train = 231
Total loss for epoch 11: 346.101748
	Epoch 12....
Epoch has taken 0:00:11.046421
Number of used sentences in train = 231
Total loss for epoch 12: 345.886047
	Epoch 13....
Epoch has taken 0:00:11.025639
Number of used sentences in train = 231
Total loss for epoch 13: 345.713326
	Epoch 14....
Epoch has taken 0:00:11.060738
Number of used sentences in train = 231
Total loss for epoch 14: 345.562255
Epoch has taken 0:00:11.042860

==================================================================================================
	Training time : 0:30:07.014491
==================================================================================================
	Identification : 0.108

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 40)
  (w_embeddings): Embedding(3369, 100)
  (lstm): LSTM(140, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=43, bias=True)
  (linear2): Linear(in_features=43, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12738.266867
validation loss after epoch 0 : 1148.582618
	Epoch 1....
Epoch has taken 0:03:32.761429
Number of used sentences in train = 3226
Total loss for epoch 1: 9421.100738
validation loss after epoch 1 : 1075.231276
	Epoch 2....
Epoch has taken 0:03:32.829698
Number of used sentences in train = 3226
Total loss for epoch 2: 8445.200574
validation loss after epoch 2 : 1104.829112
	Epoch 3....
Epoch has taken 0:03:31.041419
Number of used sentences in train = 3226
Total loss for epoch 3: 7864.352905
validation loss after epoch 3 : 1141.429947
	Epoch 4....
Epoch has taken 0:03:31.169963
Number of used sentences in train = 3226
Total loss for epoch 4: 7483.618851
validation loss after epoch 4 : 1202.406695
	Epoch 5....
Epoch has taken 0:03:33.469239
Number of used sentences in train = 3226
Total loss for epoch 5: 7220.401486
validation loss after epoch 5 : 1289.787580
	Epoch 6....
Epoch has taken 0:04:06.538094
Number of used sentences in train = 3226
Total loss for epoch 6: 7017.992739
validation loss after epoch 6 : 1291.473904
	Epoch 7....
Epoch has taken 0:03:33.413011
Number of used sentences in train = 3226
Total loss for epoch 7: 6875.552370
validation loss after epoch 7 : 1349.226305
	Epoch 8....
Epoch has taken 0:03:52.457475
Number of used sentences in train = 3226
Total loss for epoch 8: 6739.457064
validation loss after epoch 8 : 1373.439535
	Epoch 9....
Epoch has taken 0:04:03.398986
Number of used sentences in train = 3226
Total loss for epoch 9: 6649.287794
validation loss after epoch 9 : 1466.262981
	Epoch 10....
Epoch has taken 0:04:00.003893
Number of used sentences in train = 3226
Total loss for epoch 10: 6588.835438
validation loss after epoch 10 : 1427.588219
	Epoch 11....
Epoch has taken 0:03:32.828523
Number of used sentences in train = 3226
Total loss for epoch 11: 6508.314254
validation loss after epoch 11 : 1501.740413
	Epoch 12....
Epoch has taken 0:03:31.258561
Number of used sentences in train = 3226
Total loss for epoch 12: 6456.826649
validation loss after epoch 12 : 1518.283939
	Epoch 13....
Epoch has taken 0:03:33.366721
Number of used sentences in train = 3226
Total loss for epoch 13: 6436.097334
validation loss after epoch 13 : 1556.323339
	Epoch 14....
Epoch has taken 0:04:06.519796
Number of used sentences in train = 3226
Total loss for epoch 14: 6393.776284
validation loss after epoch 14 : 1568.052311
	TransitionClassifier(
  (p_embeddings): Embedding(13, 40)
  (w_embeddings): Embedding(3369, 100)
  (lstm): LSTM(140, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=43, bias=True)
  (linear2): Linear(in_features=43, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:59.639374
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1547.854935
	Epoch 1....
Epoch has taken 0:00:20.789704
Number of used sentences in train = 359
Total loss for epoch 1: 939.420123
	Epoch 2....
Epoch has taken 0:00:20.780776
Number of used sentences in train = 359
Total loss for epoch 2: 786.293121
	Epoch 3....
Epoch has taken 0:00:20.785277
Number of used sentences in train = 359
Total loss for epoch 3: 728.788583
	Epoch 4....
Epoch has taken 0:00:20.797768
Number of used sentences in train = 359
Total loss for epoch 4: 698.348123
	Epoch 5....
Epoch has taken 0:00:20.783321
Number of used sentences in train = 359
Total loss for epoch 5: 687.250886
	Epoch 6....
Epoch has taken 0:00:20.759405
Number of used sentences in train = 359
Total loss for epoch 6: 678.430556
	Epoch 7....
Epoch has taken 0:00:20.557531
Number of used sentences in train = 359
Total loss for epoch 7: 675.249201
	Epoch 8....
Epoch has taken 0:00:20.593740
Number of used sentences in train = 359
Total loss for epoch 8: 673.236655
	Epoch 9....
Epoch has taken 0:00:20.577881
Number of used sentences in train = 359
Total loss for epoch 9: 672.647787
	Epoch 10....
Epoch has taken 0:00:20.599104
Number of used sentences in train = 359
Total loss for epoch 10: 672.128959
	Epoch 11....
Epoch has taken 0:00:20.575225
Number of used sentences in train = 359
Total loss for epoch 11: 671.960325
	Epoch 12....
Epoch has taken 0:00:20.556077
Number of used sentences in train = 359
Total loss for epoch 12: 671.427336
	Epoch 13....
Epoch has taken 0:00:20.591784
Number of used sentences in train = 359
Total loss for epoch 13: 671.155661
	Epoch 14....
Epoch has taken 0:00:20.592905
Number of used sentences in train = 359
Total loss for epoch 14: 671.051001
Epoch has taken 0:00:20.578870

==================================================================================================
	Training time : 1:01:11.296046
==================================================================================================
	Identification : 0.203

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 49, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 55, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 223, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 223)
  (lstm): LSTM(240, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=49, bias=True)
  (linear2): Linear(in_features=49, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10775.041567
validation loss after epoch 0 : 1100.465318
	Epoch 1....
Epoch has taken 0:02:39.495047
Number of used sentences in train = 2811
Total loss for epoch 1: 7549.646970
validation loss after epoch 1 : 932.874566
	Epoch 2....
Epoch has taken 0:02:39.548741
Number of used sentences in train = 2811
Total loss for epoch 2: 6505.460722
validation loss after epoch 2 : 947.521546
	Epoch 3....
Epoch has taken 0:02:39.408515
Number of used sentences in train = 2811
Total loss for epoch 3: 5865.957765
validation loss after epoch 3 : 949.036659
	Epoch 4....
Epoch has taken 0:02:39.769827
Number of used sentences in train = 2811
Total loss for epoch 4: 5460.517154
validation loss after epoch 4 : 1001.052429
	Epoch 5....
Epoch has taken 0:02:38.743032
Number of used sentences in train = 2811
Total loss for epoch 5: 5218.743457
validation loss after epoch 5 : 1052.835028
	Epoch 6....
Epoch has taken 0:02:40.244658
Number of used sentences in train = 2811
Total loss for epoch 6: 5027.909959
validation loss after epoch 6 : 1076.768099
	Epoch 7....
Epoch has taken 0:03:04.994893
Number of used sentences in train = 2811
Total loss for epoch 7: 4905.740751
validation loss after epoch 7 : 1122.169900
	Epoch 8....
Epoch has taken 0:02:39.373858
Number of used sentences in train = 2811
Total loss for epoch 8: 4829.866240
validation loss after epoch 8 : 1145.044510
	Epoch 9....
Epoch has taken 0:02:37.879015
Number of used sentences in train = 2811
Total loss for epoch 9: 4744.655680
validation loss after epoch 9 : 1176.890536
	Epoch 10....
Epoch has taken 0:02:38.040759
Number of used sentences in train = 2811
Total loss for epoch 10: 4691.577684
validation loss after epoch 10 : 1200.068747
	Epoch 11....
Epoch has taken 0:02:39.932498
Number of used sentences in train = 2811
Total loss for epoch 11: 4659.769128
validation loss after epoch 11 : 1211.304932
	Epoch 12....
Epoch has taken 0:02:42.425089
Number of used sentences in train = 2811
Total loss for epoch 12: 4619.887550
validation loss after epoch 12 : 1244.712395
	Epoch 13....
Epoch has taken 0:02:39.971324
Number of used sentences in train = 2811
Total loss for epoch 13: 4592.724241
validation loss after epoch 13 : 1251.309457
	Epoch 14....
Epoch has taken 0:02:38.459682
Number of used sentences in train = 2811
Total loss for epoch 14: 4576.755900
validation loss after epoch 14 : 1259.868794
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 223)
  (lstm): LSTM(240, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=49, bias=True)
  (linear2): Linear(in_features=49, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:38.592252
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1813.833716
	Epoch 1....
Epoch has taken 0:00:16.902577
Number of used sentences in train = 313
Total loss for epoch 1: 760.872316
	Epoch 2....
Epoch has taken 0:00:16.889761
Number of used sentences in train = 313
Total loss for epoch 2: 618.900588
	Epoch 3....
Epoch has taken 0:00:16.897846
Number of used sentences in train = 313
Total loss for epoch 3: 562.873949
	Epoch 4....
Epoch has taken 0:00:16.900056
Number of used sentences in train = 313
Total loss for epoch 4: 541.176522
	Epoch 5....
Epoch has taken 0:00:16.917216
Number of used sentences in train = 313
Total loss for epoch 5: 531.647342
	Epoch 6....
Epoch has taken 0:00:16.888282
Number of used sentences in train = 313
Total loss for epoch 6: 528.019312
	Epoch 7....
Epoch has taken 0:00:16.887884
Number of used sentences in train = 313
Total loss for epoch 7: 518.956858
	Epoch 8....
Epoch has taken 0:00:16.880698
Number of used sentences in train = 313
Total loss for epoch 8: 516.824878
	Epoch 9....
Epoch has taken 0:00:16.888321
Number of used sentences in train = 313
Total loss for epoch 9: 513.902412
	Epoch 10....
Epoch has taken 0:00:16.876015
Number of used sentences in train = 313
Total loss for epoch 10: 511.416882
	Epoch 11....
Epoch has taken 0:00:16.850257
Number of used sentences in train = 313
Total loss for epoch 11: 510.108147
	Epoch 12....
Epoch has taken 0:00:16.902118
Number of used sentences in train = 313
Total loss for epoch 12: 508.361014
	Epoch 13....
Epoch has taken 0:00:16.882099
Number of used sentences in train = 313
Total loss for epoch 13: 509.715204
	Epoch 14....
Epoch has taken 0:00:16.885757
Number of used sentences in train = 313
Total loss for epoch 14: 508.330268
Epoch has taken 0:00:16.897164

==================================================================================================
	Training time : 0:44:30.736233
==================================================================================================
	Identification : 0.244

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 223)
  (lstm): LSTM(240, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=49, bias=True)
  (linear2): Linear(in_features=49, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8383.906969
validation loss after epoch 0 : 822.404826
	Epoch 1....
Epoch has taken 0:01:48.388967
Number of used sentences in train = 2074
Total loss for epoch 1: 5262.545287
validation loss after epoch 1 : 774.233780
	Epoch 2....
Epoch has taken 0:01:48.485913
Number of used sentences in train = 2074
Total loss for epoch 2: 4348.787468
validation loss after epoch 2 : 812.709521
	Epoch 3....
Epoch has taken 0:01:48.368450
Number of used sentences in train = 2074
Total loss for epoch 3: 3888.266830
validation loss after epoch 3 : 908.831320
	Epoch 4....
Epoch has taken 0:01:49.362172
Number of used sentences in train = 2074
Total loss for epoch 4: 3574.657035
validation loss after epoch 4 : 933.268545
	Epoch 5....
Epoch has taken 0:01:49.182939
Number of used sentences in train = 2074
Total loss for epoch 5: 3424.116893
validation loss after epoch 5 : 1045.519483
	Epoch 6....
Epoch has taken 0:01:48.357351
Number of used sentences in train = 2074
Total loss for epoch 6: 3350.705580
validation loss after epoch 6 : 1071.920431
	Epoch 7....
Epoch has taken 0:01:49.525587
Number of used sentences in train = 2074
Total loss for epoch 7: 3287.222718
validation loss after epoch 7 : 1118.593634
	Epoch 8....
Epoch has taken 0:01:49.507003
Number of used sentences in train = 2074
Total loss for epoch 8: 3256.388893
validation loss after epoch 8 : 1186.419885
	Epoch 9....
Epoch has taken 0:01:53.619026
Number of used sentences in train = 2074
Total loss for epoch 9: 3233.183055
validation loss after epoch 9 : 1177.892009
	Epoch 10....
Epoch has taken 0:01:48.419682
Number of used sentences in train = 2074
Total loss for epoch 10: 3226.424974
validation loss after epoch 10 : 1214.385692
	Epoch 11....
Epoch has taken 0:01:48.481953
Number of used sentences in train = 2074
Total loss for epoch 11: 3210.949477
validation loss after epoch 11 : 1257.514684
	Epoch 12....
Epoch has taken 0:01:49.519616
Number of used sentences in train = 2074
Total loss for epoch 12: 3201.915466
validation loss after epoch 12 : 1275.978997
	Epoch 13....
Epoch has taken 0:01:49.350383
Number of used sentences in train = 2074
Total loss for epoch 13: 3193.510557
validation loss after epoch 13 : 1323.506124
	Epoch 14....
Epoch has taken 0:02:06.478701
Number of used sentences in train = 2074
Total loss for epoch 14: 3184.952854
validation loss after epoch 14 : 1283.435338
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 223)
  (lstm): LSTM(240, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=49, bias=True)
  (linear2): Linear(in_features=49, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.228719
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1603.900815
	Epoch 1....
Epoch has taken 0:00:11.045632
Number of used sentences in train = 231
Total loss for epoch 1: 557.019637
	Epoch 2....
Epoch has taken 0:00:11.035763
Number of used sentences in train = 231
Total loss for epoch 2: 433.898339
	Epoch 3....
Epoch has taken 0:00:11.011671
Number of used sentences in train = 231
Total loss for epoch 3: 382.101165
	Epoch 4....
Epoch has taken 0:00:11.020676
Number of used sentences in train = 231
Total loss for epoch 4: 368.581823
	Epoch 5....
Epoch has taken 0:00:11.114934
Number of used sentences in train = 231
Total loss for epoch 5: 359.993221
	Epoch 6....
Epoch has taken 0:00:11.107500
Number of used sentences in train = 231
Total loss for epoch 6: 355.511270
	Epoch 7....
Epoch has taken 0:00:11.095930
Number of used sentences in train = 231
Total loss for epoch 7: 354.261431
	Epoch 8....
Epoch has taken 0:00:11.101087
Number of used sentences in train = 231
Total loss for epoch 8: 352.057087
	Epoch 9....
Epoch has taken 0:00:11.083684
Number of used sentences in train = 231
Total loss for epoch 9: 350.656687
	Epoch 10....
Epoch has taken 0:00:11.104107
Number of used sentences in train = 231
Total loss for epoch 10: 349.766613
	Epoch 11....
Epoch has taken 0:00:11.068529
Number of used sentences in train = 231
Total loss for epoch 11: 349.871132
	Epoch 12....
Epoch has taken 0:00:11.098124
Number of used sentences in train = 231
Total loss for epoch 12: 348.621013
	Epoch 13....
Epoch has taken 0:00:11.096082
Number of used sentences in train = 231
Total loss for epoch 13: 348.128830
	Epoch 14....
Epoch has taken 0:00:11.099532
Number of used sentences in train = 231
Total loss for epoch 14: 347.587548
Epoch has taken 0:00:11.083639

==================================================================================================
	Training time : 0:30:22.783266
==================================================================================================
	Identification : 0.421

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 223)
  (lstm): LSTM(240, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=49, bias=True)
  (linear2): Linear(in_features=49, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14978.871367
validation loss after epoch 0 : 1410.895175
	Epoch 1....
Epoch has taken 0:04:11.020934
Number of used sentences in train = 3226
Total loss for epoch 1: 11360.518137
validation loss after epoch 1 : 1357.603351
	Epoch 2....
Epoch has taken 0:04:11.121387
Number of used sentences in train = 3226
Total loss for epoch 2: 10232.869419
validation loss after epoch 2 : 1395.974725
	Epoch 3....
Epoch has taken 0:03:37.337613
Number of used sentences in train = 3226
Total loss for epoch 3: 9322.912890
validation loss after epoch 3 : 1436.361219
	Epoch 4....
Epoch has taken 0:03:36.854856
Number of used sentences in train = 3226
Total loss for epoch 4: 8696.597487
validation loss after epoch 4 : 1504.734383
	Epoch 5....
Epoch has taken 0:03:34.976066
Number of used sentences in train = 3226
Total loss for epoch 5: 8257.882904
validation loss after epoch 5 : 1479.990767
	Epoch 6....
Epoch has taken 0:03:37.374382
Number of used sentences in train = 3226
Total loss for epoch 6: 7926.802139
validation loss after epoch 6 : 1602.928749
	Epoch 7....
Epoch has taken 0:04:11.253317
Number of used sentences in train = 3226
Total loss for epoch 7: 7596.466159
validation loss after epoch 7 : 1656.567549
	Epoch 8....
Epoch has taken 0:03:37.817638
Number of used sentences in train = 3226
Total loss for epoch 8: 7386.968212
validation loss after epoch 8 : 1714.621760
	Epoch 9....
Epoch has taken 0:03:35.786737
Number of used sentences in train = 3226
Total loss for epoch 9: 7198.968693
validation loss after epoch 9 : 1796.821268
	Epoch 10....
Epoch has taken 0:03:35.942785
Number of used sentences in train = 3226
Total loss for epoch 10: 7046.031188
validation loss after epoch 10 : 1871.765166
	Epoch 11....
Epoch has taken 0:03:37.030760
Number of used sentences in train = 3226
Total loss for epoch 11: 6916.433852
validation loss after epoch 11 : 1863.988071
	Epoch 12....
Epoch has taken 0:04:11.951553
Number of used sentences in train = 3226
Total loss for epoch 12: 6809.967081
validation loss after epoch 12 : 1916.307296
	Epoch 13....
Epoch has taken 0:03:38.105360
Number of used sentences in train = 3226
Total loss for epoch 13: 6718.697669
validation loss after epoch 13 : 2024.438220
	Epoch 14....
Epoch has taken 0:03:35.157757
Number of used sentences in train = 3226
Total loss for epoch 14: 6656.834321
validation loss after epoch 14 : 2048.518029
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 223)
  (lstm): LSTM(240, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=49, bias=True)
  (linear2): Linear(in_features=49, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:37.325077
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1834.394842
	Epoch 1....
Epoch has taken 0:00:21.123196
Number of used sentences in train = 359
Total loss for epoch 1: 1137.057434
	Epoch 2....
Epoch has taken 0:00:21.089430
Number of used sentences in train = 359
Total loss for epoch 2: 946.614053
	Epoch 3....
Epoch has taken 0:00:21.098420
Number of used sentences in train = 359
Total loss for epoch 3: 848.707115
	Epoch 4....
Epoch has taken 0:00:21.105371
Number of used sentences in train = 359
Total loss for epoch 4: 776.521292
	Epoch 5....
Epoch has taken 0:00:21.091923
Number of used sentences in train = 359
Total loss for epoch 5: 761.108512
	Epoch 6....
Epoch has taken 0:00:21.124489
Number of used sentences in train = 359
Total loss for epoch 6: 740.268259
	Epoch 7....
Epoch has taken 0:00:21.105984
Number of used sentences in train = 359
Total loss for epoch 7: 728.981204
	Epoch 8....
Epoch has taken 0:00:21.071972
Number of used sentences in train = 359
Total loss for epoch 8: 711.498303
	Epoch 9....
Epoch has taken 0:00:21.092020
Number of used sentences in train = 359
Total loss for epoch 9: 702.274503
	Epoch 10....
Epoch has taken 0:00:21.112702
Number of used sentences in train = 359
Total loss for epoch 10: 699.876832
	Epoch 11....
Epoch has taken 0:00:21.105956
Number of used sentences in train = 359
Total loss for epoch 11: 694.195061
	Epoch 12....
Epoch has taken 0:00:21.092419
Number of used sentences in train = 359
Total loss for epoch 12: 683.756519
	Epoch 13....
Epoch has taken 0:00:21.116656
Number of used sentences in train = 359
Total loss for epoch 13: 679.824045
	Epoch 14....
Epoch has taken 0:00:21.110686
Number of used sentences in train = 359
Total loss for epoch 14: 677.410328
Epoch has taken 0:00:21.107121

==================================================================================================
	Training time : 1:01:50.811218
==================================================================================================
	Identification : 0.438

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 105, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 20, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 59, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 239, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(1882, 239)
  (lstm): LSTM(259, 59, bidirectional=True)
  (linear1): Linear(in_features=944, out_features=105, bias=True)
  (linear2): Linear(in_features=105, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10565.259400
validation loss after epoch 0 : 861.085295
	Epoch 1....
Epoch has taken 0:02:38.884941
Number of used sentences in train = 2811
Total loss for epoch 1: 6872.250311
validation loss after epoch 1 : 867.398127
	Epoch 2....
Epoch has taken 0:02:39.099874
Number of used sentences in train = 2811
Total loss for epoch 2: 5837.956630
validation loss after epoch 2 : 917.168175
	Epoch 3....
Epoch has taken 0:03:03.744818
Number of used sentences in train = 2811
Total loss for epoch 3: 5311.590970
validation loss after epoch 3 : 947.530375
	Epoch 4....
Epoch has taken 0:02:38.814473
Number of used sentences in train = 2811
Total loss for epoch 4: 5047.675281
validation loss after epoch 4 : 987.167126
	Epoch 5....
Epoch has taken 0:02:38.976930
Number of used sentences in train = 2811
Total loss for epoch 5: 4842.597467
validation loss after epoch 5 : 1029.303206
	Epoch 6....
Epoch has taken 0:02:39.127193
Number of used sentences in train = 2811
Total loss for epoch 6: 4762.279785
validation loss after epoch 6 : 1040.857798
	Epoch 7....
Epoch has taken 0:03:03.695453
Number of used sentences in train = 2811
Total loss for epoch 7: 4681.234999
validation loss after epoch 7 : 1073.811775
	Epoch 8....
Epoch has taken 0:02:39.533059
Number of used sentences in train = 2811
Total loss for epoch 8: 4652.839453
validation loss after epoch 8 : 1093.196571
	Epoch 9....
Epoch has taken 0:03:04.315958
Number of used sentences in train = 2811
Total loss for epoch 9: 4624.067048
validation loss after epoch 9 : 1144.116380
	Epoch 10....
Epoch has taken 0:02:39.500483
Number of used sentences in train = 2811
Total loss for epoch 10: 4606.932443
validation loss after epoch 10 : 1142.738406
	Epoch 11....
Epoch has taken 0:02:39.158109
Number of used sentences in train = 2811
Total loss for epoch 11: 4601.292959
validation loss after epoch 11 : 1149.320538
	Epoch 12....
Epoch has taken 0:02:38.249010
Number of used sentences in train = 2811
Total loss for epoch 12: 4581.273380
validation loss after epoch 12 : 1158.369860
	Epoch 13....
Epoch has taken 0:02:46.541890
