INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++

# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 41, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 32, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 44, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 163, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13670.338000
validation loss after epoch 0 : 1172.994375
	Epoch 1....
Epoch has taken 0:02:45.788974
Number of used sentences in train = 2811
Total loss for epoch 1: 8608.328661
validation loss after epoch 1 : 1021.760367
	Epoch 2....
Epoch has taken 0:02:47.935122
Number of used sentences in train = 2811
Total loss for epoch 2: 7149.597722
validation loss after epoch 2 : 1044.362137
	Epoch 3....
Epoch has taken 0:02:47.674933
Number of used sentences in train = 2811
Total loss for epoch 3: 6254.647279
validation loss after epoch 3 : 1067.243463
	Epoch 4....
Epoch has taken 0:02:47.711104
Number of used sentences in train = 2811
Total loss for epoch 4: 5656.497772
validation loss after epoch 4 : 1106.320539
	Epoch 5....
Epoch has taken 0:02:47.807548
Number of used sentences in train = 2811
Total loss for epoch 5: 5452.613194
validation loss after epoch 5 : 1243.067621
	Epoch 6....
Epoch has taken 0:02:47.846836
Number of used sentences in train = 2811
Total loss for epoch 6: 5204.214631
validation loss after epoch 6 : 1273.343906
	Epoch 7....
Epoch has taken 0:02:48.062883
Number of used sentences in train = 2811
Total loss for epoch 7: 4998.590758
validation loss after epoch 7 : 1237.445475
	Epoch 8....
Epoch has taken 0:02:48.017960
Number of used sentences in train = 2811
Total loss for epoch 8: 4906.964489
validation loss after epoch 8 : 1305.846205
	Epoch 9....
Epoch has taken 0:02:48.053955
Number of used sentences in train = 2811
Total loss for epoch 9: 4864.505532
validation loss after epoch 9 : 1345.627117
	Epoch 10....
Epoch has taken 0:02:57.992974
Number of used sentences in train = 2811
Total loss for epoch 10: 4849.531243
validation loss after epoch 10 : 1374.465257
	Epoch 11....
Epoch has taken 0:03:05.194819
Number of used sentences in train = 2811
Total loss for epoch 11: 4721.369786
validation loss after epoch 11 : 1391.093597
	Epoch 12....
Epoch has taken 0:03:05.166607
Number of used sentences in train = 2811
Total loss for epoch 12: 4705.551828
validation loss after epoch 12 : 1474.702653
	Epoch 13....
Epoch has taken 0:03:05.170578
Number of used sentences in train = 2811
Total loss for epoch 13: 4700.812855
validation loss after epoch 13 : 1439.474382
	Epoch 14....
Epoch has taken 0:02:49.756744
Number of used sentences in train = 2811
Total loss for epoch 14: 4662.971985
validation loss after epoch 14 : 1509.938194
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(5925, 163)
  (lstm): LSTM(195, 44, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:48.103038
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1589.302487
	Epoch 1....
Epoch has taken 0:00:17.739730
Number of used sentences in train = 313
Total loss for epoch 1: 820.770596
	Epoch 2....
Epoch has taken 0:00:17.745372
Number of used sentences in train = 313
Total loss for epoch 2: 635.221310
	Epoch 3....
Epoch has taken 0:00:17.737456
Number of used sentences in train = 313
Total loss for epoch 3: 574.817451
	Epoch 4....
Epoch has taken 0:00:17.744013
Number of used sentences in train = 313
Total loss for epoch 4: 557.561851
	Epoch 5....
Epoch has taken 0:00:17.742911
Number of used sentences in train = 313
Total loss for epoch 5: 536.821977
	Epoch 6....
Epoch has taken 0:00:17.745315
Number of used sentences in train = 313
Total loss for epoch 6: 535.458826
	Epoch 7....
Epoch has taken 0:00:17.735777
Number of used sentences in train = 313
Total loss for epoch 7: 526.963614
	Epoch 8....
Epoch has taken 0:00:17.759754
Number of used sentences in train = 313
Total loss for epoch 8: 530.909137
	Epoch 9....
Epoch has taken 0:00:17.728986
Number of used sentences in train = 313
Total loss for epoch 9: 532.268087
	Epoch 10....
Epoch has taken 0:00:17.747285
Number of used sentences in train = 313
Total loss for epoch 10: 519.949119
	Epoch 11....
Epoch has taken 0:00:17.733892
Number of used sentences in train = 313
Total loss for epoch 11: 519.462374
	Epoch 12....
Epoch has taken 0:00:17.742232
Number of used sentences in train = 313
Total loss for epoch 12: 522.813880
	Epoch 13....
Epoch has taken 0:00:17.732733
Number of used sentences in train = 313
Total loss for epoch 13: 522.933106
	Epoch 14....
Epoch has taken 0:00:17.733842
Number of used sentences in train = 313
Total loss for epoch 14: 516.655604
Epoch has taken 0:00:17.732945

==================================================================================================
	Training time : 0:47:39.635847
==================================================================================================
	Identification : 0.033

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(5618, 163)
  (lstm): LSTM(195, 44, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 14062.184397
validation loss after epoch 0 : 960.848326
	Epoch 1....
Epoch has taken 0:01:56.509108
Number of used sentences in train = 2074
Total loss for epoch 1: 6700.001087
validation loss after epoch 1 : 847.449540
	Epoch 2....
Epoch has taken 0:02:01.101610
Number of used sentences in train = 2074
Total loss for epoch 2: 5156.967076
validation loss after epoch 2 : 815.228062
	Epoch 3....
Epoch has taken 0:02:08.539686
Number of used sentences in train = 2074
Total loss for epoch 3: 4506.106287
validation loss after epoch 3 : 1001.741571
	Epoch 4....
Epoch has taken 0:02:08.753258
Number of used sentences in train = 2074
Total loss for epoch 4: 4102.180303
validation loss after epoch 4 : 1103.306500
	Epoch 5....
Epoch has taken 0:02:08.555899
Number of used sentences in train = 2074
Total loss for epoch 5: 3842.590246
validation loss after epoch 5 : 1090.832568
	Epoch 6....
Epoch has taken 0:02:08.515953
Number of used sentences in train = 2074
Total loss for epoch 6: 3674.351438
validation loss after epoch 6 : 1118.205732
	Epoch 7....
Epoch has taken 0:02:01.938027
Number of used sentences in train = 2074
Total loss for epoch 7: 3552.285069
validation loss after epoch 7 : 1161.814101
	Epoch 8....
Epoch has taken 0:01:56.647259
Number of used sentences in train = 2074
Total loss for epoch 8: 3511.259098
validation loss after epoch 8 : 1198.484916
	Epoch 9....
Epoch has taken 0:01:57.480315
Number of used sentences in train = 2074
Total loss for epoch 9: 3412.429986
validation loss after epoch 9 : 1196.593849
	Epoch 10....
Epoch has taken 0:02:00.678422
Number of used sentences in train = 2074
Total loss for epoch 10: 3418.586484
validation loss after epoch 10 : 1221.993106
	Epoch 11....
Epoch has taken 0:02:08.502986
Number of used sentences in train = 2074
Total loss for epoch 11: 3358.649835
validation loss after epoch 11 : 1366.093940
	Epoch 12....
Epoch has taken 0:02:08.466080
Number of used sentences in train = 2074
Total loss for epoch 12: 3330.080957
validation loss after epoch 12 : 1159.318975
	Epoch 13....
Epoch has taken 0:02:08.485508
Number of used sentences in train = 2074
Total loss for epoch 13: 3318.168213
validation loss after epoch 13 : 1279.939701
	Epoch 14....
Epoch has taken 0:02:08.429884
Number of used sentences in train = 2074
Total loss for epoch 14: 3309.178016
validation loss after epoch 14 : 1347.646443
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(5618, 163)
  (lstm): LSTM(195, 44, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:08.252601
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1790.046265
	Epoch 1....
Epoch has taken 0:00:13.273169
Number of used sentences in train = 231
Total loss for epoch 1: 581.896107
	Epoch 2....
Epoch has taken 0:00:13.172158
Number of used sentences in train = 231
Total loss for epoch 2: 455.063103
	Epoch 3....
Epoch has taken 0:00:13.173036
Number of used sentences in train = 231
Total loss for epoch 3: 411.367502
	Epoch 4....
Epoch has taken 0:00:13.175993
Number of used sentences in train = 231
Total loss for epoch 4: 384.118708
	Epoch 5....
Epoch has taken 0:00:13.189385
Number of used sentences in train = 231
Total loss for epoch 5: 362.971814
	Epoch 6....
Epoch has taken 0:00:13.190912
Number of used sentences in train = 231
Total loss for epoch 6: 358.161608
	Epoch 7....
Epoch has taken 0:00:13.188337
Number of used sentences in train = 231
Total loss for epoch 7: 356.973826
	Epoch 8....
Epoch has taken 0:00:13.174611
Number of used sentences in train = 231
Total loss for epoch 8: 354.021893
	Epoch 9....
Epoch has taken 0:00:13.185450
Number of used sentences in train = 231
Total loss for epoch 9: 353.793989
	Epoch 10....
Epoch has taken 0:00:13.170409
Number of used sentences in train = 231
Total loss for epoch 10: 354.087766
	Epoch 11....
Epoch has taken 0:00:13.185421
Number of used sentences in train = 231
Total loss for epoch 11: 352.556361
	Epoch 12....
Epoch has taken 0:00:13.182833
Number of used sentences in train = 231
Total loss for epoch 12: 353.484364
	Epoch 13....
Epoch has taken 0:00:13.174512
Number of used sentences in train = 231
Total loss for epoch 13: 354.793895
	Epoch 14....
Epoch has taken 0:00:13.172062
Number of used sentences in train = 231
Total loss for epoch 14: 352.000249
Epoch has taken 0:00:13.178920

==================================================================================================
	Training time : 0:34:28.985643
==================================================================================================
	Identification : 0.367

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 32)
  (w_embeddings): Embedding(6848, 163)
  (lstm): LSTM(195, 44, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16529.037439
validation loss after epoch 0 : 1597.466698
	Epoch 1....
Epoch has taken 0:03:53.284944
Number of used sentences in train = 3226
Total loss for epoch 1: 12173.170551
validation loss after epoch 1 : 1532.469329
	Epoch 2....
Epoch has taken 0:03:45.165515
Number of used sentences in train = 3226
Total loss for epoch 2: 10451.537292
validation loss after epoch 2 : 1625.925912
	Epoch 3....
Epoch has taken 0:03:41.930932
Number of used sentences in train = 3226
Total loss for epoch 3: 9357.559435
validation loss after epoch 3 : 1643.610670
	Epoch 4....
Epoch has taken 0:03:40.205128
Number of used sentences in train = 3226
Total loss for epoch 4: 8532.669213
validation loss after epoch 4 : 1726.630394
	Epoch 5....
Epoch has taken 0:03:40.202523
Number of used sentences in train = 3226
Total loss for epoch 5: 8062.598845
validation loss after epoch 5 : 1858.453554
	Epoch 6....
Epoch has taken 0:03:40.241365
Number of used sentences in train = 3226
Total loss for epoch 6: 7569.865041
validation loss after epoch 6 : 1907.417091
	Epoch 7....
Epoch has taken 0:03:40.160252
Number of used sentences in train = 3226
Total loss for epoch 7: 7341.710441
validation loss after epoch 7 : 2008.716187
	Epoch 8....
Epoch has taken 0:03:40.278387
Number of used sentences in train = 3226
Total loss for epoch 8: 7152.430425
validation loss after epoch 8 : 1999.592067
	Epoch 9....
Epoch has taken 0:03:40.504263
Number of used sentences in train = 3226
Total loss for epoch 9: 6964.806649
validation loss after epoch 9 : 2046.944296
	Epoch 10....
Epoch has taken 0:03:40.534049
Number of used sentences in train = 3226
Total loss for epoch 10: 6811.354232
validation loss after epoch 10 : 2164.351487
	Epoch 11....
Epoch has taken 0:03:40.390542
Number of used sentences in train = 3226
Total loss for epoch 11: 6646.786546
validation loss after epoch 11 : 2099.082590
	Epoch 12....
Epoch has taken 0:03:40.160771
Number of used sentences in train = 3226
Total loss for epoch 12: 6570.127440
validation loss after epoch 12 : 2263.336359
	Epoch 13....
Epoch has taken 0:03:40.498264
Number of used sentences in train = 3226
Total loss for epoch 13: 6562.444131
validation loss after epoch 13 : 2291.052115
	Epoch 14....
Epoch has taken 0:03:40.314731
Number of used sentences in train = 3226
Total loss for epoch 14: 6533.870548
validation loss after epoch 14 : 2265.965175
	TransitionClassifier(
  (p_embeddings): Embedding(13, 32)
  (w_embeddings): Embedding(6848, 163)
  (lstm): LSTM(195, 44, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:40.147717
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2166.854105
	Epoch 1....
Epoch has taken 0:00:21.558458
Number of used sentences in train = 359
Total loss for epoch 1: 1373.258952
	Epoch 2....
Epoch has taken 0:00:21.563119
Number of used sentences in train = 359
Total loss for epoch 2: 1037.660111
	Epoch 3....
Epoch has taken 0:00:21.564906
Number of used sentences in train = 359
Total loss for epoch 3: 915.060826
	Epoch 4....
Epoch has taken 0:00:21.556179
Number of used sentences in train = 359
Total loss for epoch 4: 802.274105
	Epoch 5....
Epoch has taken 0:00:21.561208
Number of used sentences in train = 359
Total loss for epoch 5: 763.817821
	Epoch 6....
Epoch has taken 0:00:21.557125
Number of used sentences in train = 359
Total loss for epoch 6: 746.245614
	Epoch 7....
Epoch has taken 0:00:21.546667
Number of used sentences in train = 359
Total loss for epoch 7: 726.517644
	Epoch 8....
Epoch has taken 0:00:21.553178
Number of used sentences in train = 359
Total loss for epoch 8: 709.459171
	Epoch 9....
Epoch has taken 0:00:21.556744
Number of used sentences in train = 359
Total loss for epoch 9: 690.933591
	Epoch 10....
Epoch has taken 0:00:21.562586
Number of used sentences in train = 359
Total loss for epoch 10: 681.345226
	Epoch 11....
Epoch has taken 0:00:21.564520
Number of used sentences in train = 359
Total loss for epoch 11: 682.809811
	Epoch 12....
Epoch has taken 0:00:21.564955
Number of used sentences in train = 359
Total loss for epoch 12: 682.188861
	Epoch 13....
Epoch has taken 0:00:21.566677
Number of used sentences in train = 359
Total loss for epoch 13: 675.691296
	Epoch 14....
Epoch has taken 0:00:21.557137
Number of used sentences in train = 359
Total loss for epoch 14: 684.002394
Epoch has taken 0:00:21.560260

==================================================================================================
	Training time : 1:00:48.065923
==================================================================================================
	Identification : 0.103

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 17, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 71, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 61, 'lstmDropout': 0.28, 'denseActivation': 'tanh', 'wordDim': 53, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 71)
  (w_embeddings): Embedding(9249, 53)
  (lstm): LSTM(124, 61, bidirectional=True)
  (linear1): Linear(in_features=976, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13575.353429
validation loss after epoch 0 : 1163.792211
	Epoch 1....
Epoch has taken 0:02:34.673166
Number of used sentences in train = 2811
Total loss for epoch 1: 9920.365560
validation loss after epoch 1 : 1092.422041
	Epoch 2....
Epoch has taken 0:02:34.604913
Number of used sentences in train = 2811
Total loss for epoch 2: 8345.277902
validation loss after epoch 2 : 1059.522370
	Epoch 3....
Epoch has taken 0:02:34.777507
Number of used sentences in train = 2811
Total loss for epoch 3: 7269.728267
validation loss after epoch 3 : 1094.813876
	Epoch 4....
Epoch has taken 0:02:34.530036
Number of used sentences in train = 2811
Total loss for epoch 4: 6529.115735
validation loss after epoch 4 : 1168.546996
	Epoch 5....
Epoch has taken 0:02:34.545112
Number of used sentences in train = 2811
Total loss for epoch 5: 6009.586880
validation loss after epoch 5 : 1179.483256
	Epoch 6....
Epoch has taken 0:02:34.419676
Number of used sentences in train = 2811
Total loss for epoch 6: 5640.774884
validation loss after epoch 6 : 1243.848285
	Epoch 7....
Epoch has taken 0:02:34.529239
Number of used sentences in train = 2811
Total loss for epoch 7: 5353.071814
validation loss after epoch 7 : 1270.740160
	Epoch 8....
Epoch has taken 0:02:34.505220
Number of used sentences in train = 2811
Total loss for epoch 8: 5178.093131
validation loss after epoch 8 : 1332.977061
	Epoch 9....
Epoch has taken 0:02:34.686395
Number of used sentences in train = 2811
Total loss for epoch 9: 5022.067140
validation loss after epoch 9 : 1378.218890
	Epoch 10....
Epoch has taken 0:02:34.604733
Number of used sentences in train = 2811
Total loss for epoch 10: 4912.132964
validation loss after epoch 10 : 1397.989254
	Epoch 11....
Epoch has taken 0:02:34.459120
Number of used sentences in train = 2811
Total loss for epoch 11: 4849.857922
validation loss after epoch 11 : 1467.997117
	Epoch 12....
Epoch has taken 0:02:34.478009
Number of used sentences in train = 2811
Total loss for epoch 12: 4765.420684
validation loss after epoch 12 : 1482.642740
	Epoch 13....
Epoch has taken 0:02:34.443807
Number of used sentences in train = 2811
Total loss for epoch 13: 4701.064892
validation loss after epoch 13 : 1519.288300
	Epoch 14....
Epoch has taken 0:02:34.366917
Number of used sentences in train = 2811
Total loss for epoch 14: 4657.428137
validation loss after epoch 14 : 1546.581765
	TransitionClassifier(
  (p_embeddings): Embedding(18, 71)
  (w_embeddings): Embedding(9249, 53)
  (lstm): LSTM(124, 61, bidirectional=True)
  (linear1): Linear(in_features=976, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:34.448067
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1680.339337
	Epoch 1....
Epoch has taken 0:00:16.304028
Number of used sentences in train = 313
Total loss for epoch 1: 941.831371
	Epoch 2....
Epoch has taken 0:00:16.291046
Number of used sentences in train = 313
Total loss for epoch 2: 681.567528
	Epoch 3....
Epoch has taken 0:00:16.288774
Number of used sentences in train = 313
Total loss for epoch 3: 594.347787
	Epoch 4....
Epoch has taken 0:00:16.303352
Number of used sentences in train = 313
Total loss for epoch 4: 567.926852
	Epoch 5....
Epoch has taken 0:00:16.295646
Number of used sentences in train = 313
Total loss for epoch 5: 545.825127
	Epoch 6....
Epoch has taken 0:00:16.298840
Number of used sentences in train = 313
Total loss for epoch 6: 540.419206
	Epoch 7....
Epoch has taken 0:00:16.302619
Number of used sentences in train = 313
Total loss for epoch 7: 533.374859
	Epoch 8....
Epoch has taken 0:00:16.312812
Number of used sentences in train = 313
Total loss for epoch 8: 531.826938
	Epoch 9....
Epoch has taken 0:00:16.294179
Number of used sentences in train = 313
Total loss for epoch 9: 530.456097
	Epoch 10....
Epoch has taken 0:00:16.305865
Number of used sentences in train = 313
Total loss for epoch 10: 527.371827
	Epoch 11....
Epoch has taken 0:00:16.310781
Number of used sentences in train = 313
Total loss for epoch 11: 522.935692
	Epoch 12....
Epoch has taken 0:00:16.306022
Number of used sentences in train = 313
Total loss for epoch 12: 523.717745
	Epoch 13....
Epoch has taken 0:00:16.308139
Number of used sentences in train = 313
Total loss for epoch 13: 523.073074
	Epoch 14....
Epoch has taken 0:00:16.305461
Number of used sentences in train = 313
Total loss for epoch 14: 520.373153
Epoch has taken 0:00:16.306749

==================================================================================================
	Training time : 0:42:43.101808
==================================================================================================
	Identification : 0.483

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 71)
  (w_embeddings): Embedding(7068, 53)
  (lstm): LSTM(124, 61, bidirectional=True)
  (linear1): Linear(in_features=976, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11208.376234
validation loss after epoch 0 : 956.839204
	Epoch 1....
Epoch has taken 0:01:45.732756
Number of used sentences in train = 2074
Total loss for epoch 1: 7423.924281
validation loss after epoch 1 : 886.140816
	Epoch 2....
Epoch has taken 0:01:45.757442
Number of used sentences in train = 2074
Total loss for epoch 2: 6145.217212
validation loss after epoch 2 : 888.384629
	Epoch 3....
Epoch has taken 0:01:45.807962
Number of used sentences in train = 2074
Total loss for epoch 3: 5234.882285
validation loss after epoch 3 : 895.369557
	Epoch 4....
Epoch has taken 0:01:45.766875
Number of used sentences in train = 2074
Total loss for epoch 4: 4606.834475
validation loss after epoch 4 : 924.946568
	Epoch 5....
Epoch has taken 0:01:45.821043
Number of used sentences in train = 2074
Total loss for epoch 5: 4172.370610
validation loss after epoch 5 : 982.657089
	Epoch 6....
Epoch has taken 0:01:45.786147
Number of used sentences in train = 2074
Total loss for epoch 6: 3878.124356
validation loss after epoch 6 : 1066.336173
	Epoch 7....
Epoch has taken 0:01:45.769126
Number of used sentences in train = 2074
Total loss for epoch 7: 3678.267633
validation loss after epoch 7 : 1098.617567
	Epoch 8....
Epoch has taken 0:01:45.905482
Number of used sentences in train = 2074
Total loss for epoch 8: 3543.718934
validation loss after epoch 8 : 1106.001114
	Epoch 9....
Epoch has taken 0:01:45.792789
Number of used sentences in train = 2074
Total loss for epoch 9: 3458.585275
validation loss after epoch 9 : 1172.309325
	Epoch 10....
Epoch has taken 0:01:45.873714
Number of used sentences in train = 2074
Total loss for epoch 10: 3404.533144
validation loss after epoch 10 : 1169.823516
	Epoch 11....
Epoch has taken 0:01:45.862114
Number of used sentences in train = 2074
Total loss for epoch 11: 3371.289426
validation loss after epoch 11 : 1232.856439
	Epoch 12....
Epoch has taken 0:01:45.856644
Number of used sentences in train = 2074
Total loss for epoch 12: 3341.035056
validation loss after epoch 12 : 1242.704132
	Epoch 13....
Epoch has taken 0:01:45.750075
Number of used sentences in train = 2074
Total loss for epoch 13: 3319.106057
validation loss after epoch 13 : 1286.617621
	Epoch 14....
Epoch has taken 0:01:45.791547
Number of used sentences in train = 2074
Total loss for epoch 14: 3285.208508
validation loss after epoch 14 : 1265.270584
	TransitionClassifier(
  (p_embeddings): Embedding(18, 71)
  (w_embeddings): Embedding(7068, 53)
  (lstm): LSTM(124, 61, bidirectional=True)
  (linear1): Linear(in_features=976, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:45.770599
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1640.125741
	Epoch 1....
Epoch has taken 0:00:10.740097
Number of used sentences in train = 231
Total loss for epoch 1: 728.995683
	Epoch 2....
Epoch has taken 0:00:10.741167
Number of used sentences in train = 231
Total loss for epoch 2: 561.115649
	Epoch 3....
Epoch has taken 0:00:10.743918
Number of used sentences in train = 231
Total loss for epoch 3: 457.983829
	Epoch 4....
Epoch has taken 0:00:10.743447
Number of used sentences in train = 231
Total loss for epoch 4: 416.462922
	Epoch 5....
Epoch has taken 0:00:10.740407
Number of used sentences in train = 231
Total loss for epoch 5: 401.345270
	Epoch 6....
Epoch has taken 0:00:10.749141
Number of used sentences in train = 231
Total loss for epoch 6: 396.185340
	Epoch 7....
Epoch has taken 0:00:10.744824
Number of used sentences in train = 231
Total loss for epoch 7: 389.630273
	Epoch 8....
Epoch has taken 0:00:10.746343
Number of used sentences in train = 231
Total loss for epoch 8: 385.872142
	Epoch 9....
Epoch has taken 0:00:10.750326
Number of used sentences in train = 231
Total loss for epoch 9: 387.428704
	Epoch 10....
Epoch has taken 0:00:10.739935
Number of used sentences in train = 231
Total loss for epoch 10: 381.433581
	Epoch 11....
Epoch has taken 0:00:10.747810
Number of used sentences in train = 231
Total loss for epoch 11: 375.995607
	Epoch 12....
Epoch has taken 0:00:10.736087
Number of used sentences in train = 231
Total loss for epoch 12: 380.672425
	Epoch 13....
Epoch has taken 0:00:10.744103
Number of used sentences in train = 231
Total loss for epoch 13: 374.766085
	Epoch 14....
Epoch has taken 0:00:10.743560
Number of used sentences in train = 231
Total loss for epoch 14: 369.959719
Epoch has taken 0:00:10.745342

==================================================================================================
	Training time : 0:29:08.568974
==================================================================================================
	Identification : 0.38

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 71)
  (w_embeddings): Embedding(18063, 53)
  (lstm): LSTM(124, 61, bidirectional=True)
  (linear1): Linear(in_features=976, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 21180.917951
validation loss after epoch 0 : 1741.251073
	Epoch 1....
Epoch has taken 0:03:28.061805
Number of used sentences in train = 3226
Total loss for epoch 1: 14090.855191
validation loss after epoch 1 : 1631.343875
	Epoch 2....
Epoch has taken 0:03:28.261302
Number of used sentences in train = 3226
Total loss for epoch 2: 11991.881819
validation loss after epoch 2 : 1671.753463
	Epoch 3....
Epoch has taken 0:03:28.077702
Number of used sentences in train = 3226
Total loss for epoch 3: 10533.100792
validation loss after epoch 3 : 1710.502833
	Epoch 4....
Epoch has taken 0:03:28.167403
Number of used sentences in train = 3226
Total loss for epoch 4: 9472.335546
validation loss after epoch 4 : 1844.443378
	Epoch 5....
Epoch has taken 0:03:28.095950
Number of used sentences in train = 3226
Total loss for epoch 5: 8683.530313
validation loss after epoch 5 : 1933.584668
	Epoch 6....
Epoch has taken 0:03:27.940264
Number of used sentences in train = 3226
Total loss for epoch 6: 8050.035197
validation loss after epoch 6 : 2081.851050
	Epoch 7....
Epoch has taken 0:03:28.051516
Number of used sentences in train = 3226
Total loss for epoch 7: 7579.615546
validation loss after epoch 7 : 2132.174071
	Epoch 8....
Epoch has taken 0:03:28.186831
Number of used sentences in train = 3226
Total loss for epoch 8: 7188.627158
validation loss after epoch 8 : 2152.614860
	Epoch 9....
Epoch has taken 0:03:28.280870
Number of used sentences in train = 3226
Total loss for epoch 9: 6939.626744
validation loss after epoch 9 : 2253.624126
	Epoch 10....
Epoch has taken 0:03:28.234558
Number of used sentences in train = 3226
Total loss for epoch 10: 6762.675425
validation loss after epoch 10 : 2362.325258
	Epoch 11....
Epoch has taken 0:03:28.373502
Number of used sentences in train = 3226
Total loss for epoch 11: 6563.276049
validation loss after epoch 11 : 2381.282687
	Epoch 12....
Epoch has taken 0:03:28.133868
Number of used sentences in train = 3226
Total loss for epoch 12: 6451.679913
validation loss after epoch 12 : 2473.179080
	Epoch 13....
Epoch has taken 0:03:28.462621
Number of used sentences in train = 3226
Total loss for epoch 13: 6376.678195
validation loss after epoch 13 : 2496.498096
	Epoch 14....
Epoch has taken 0:03:28.172928
Number of used sentences in train = 3226
Total loss for epoch 14: 6340.496120
validation loss after epoch 14 : 2522.075485
	TransitionClassifier(
  (p_embeddings): Embedding(13, 71)
  (w_embeddings): Embedding(18063, 53)
  (lstm): LSTM(124, 61, bidirectional=True)
  (linear1): Linear(in_features=976, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:28.211554
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2955.908030
	Epoch 1....
Epoch has taken 0:00:20.319539
Number of used sentences in train = 359
Total loss for epoch 1: 1375.552570
	Epoch 2....
Epoch has taken 0:00:20.312895
Number of used sentences in train = 359
Total loss for epoch 2: 1107.382299
	Epoch 3....
Epoch has taken 0:00:20.322112
Number of used sentences in train = 359
Total loss for epoch 3: 916.587847
	Epoch 4....
Epoch has taken 0:00:20.301258
Number of used sentences in train = 359
Total loss for epoch 4: 795.702162
	Epoch 5....
Epoch has taken 0:00:20.321584
Number of used sentences in train = 359
Total loss for epoch 5: 745.228406
	Epoch 6....
Epoch has taken 0:00:20.313159
Number of used sentences in train = 359
Total loss for epoch 6: 703.284508
	Epoch 7....
Epoch has taken 0:00:20.318127
Number of used sentences in train = 359
Total loss for epoch 7: 689.398003
	Epoch 8....
Epoch has taken 0:00:20.308404
Number of used sentences in train = 359
Total loss for epoch 8: 682.951172
	Epoch 9....
Epoch has taken 0:00:20.302975
Number of used sentences in train = 359
Total loss for epoch 9: 679.909085
	Epoch 10....
Epoch has taken 0:00:20.303541
Number of used sentences in train = 359
Total loss for epoch 10: 677.673774
	Epoch 11....
Epoch has taken 0:00:20.303356
Number of used sentences in train = 359
Total loss for epoch 11: 676.156844
	Epoch 12....
Epoch has taken 0:00:20.313112
Number of used sentences in train = 359
Total loss for epoch 12: 674.512936
	Epoch 13....
Epoch has taken 0:00:20.296824
Number of used sentences in train = 359
Total loss for epoch 13: 673.893284
	Epoch 14....
Epoch has taken 0:00:20.304317
Number of used sentences in train = 359
Total loss for epoch 14: 673.446391
Epoch has taken 0:00:20.307756

==================================================================================================
	Training time : 0:57:08.035588
==================================================================================================
	Identification : 0.452

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 51, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 29, 'lstmDropout': 0.33, 'denseActivation': 'tanh', 'wordDim': 220, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 51)
  (w_embeddings): Embedding(1177, 220)
  (lstm): LSTM(271, 29, num_layers=2, dropout=0.33, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11250.725657
validation loss after epoch 0 : 962.284959
	Epoch 1....
Epoch has taken 0:02:45.835575
Number of used sentences in train = 2811
Total loss for epoch 1: 8238.094140
validation loss after epoch 1 : 942.140393
	Epoch 2....
Epoch has taken 0:02:45.792347
Number of used sentences in train = 2811
Total loss for epoch 2: 7514.633076
validation loss after epoch 2 : 908.745193
	Epoch 3....
Epoch has taken 0:02:45.764329
Number of used sentences in train = 2811
Total loss for epoch 3: 7065.303582
validation loss after epoch 3 : 911.595313
	Epoch 4....
Epoch has taken 0:02:45.778875
Number of used sentences in train = 2811
Total loss for epoch 4: 6716.643894
validation loss after epoch 4 : 895.296858
	Epoch 5....
Epoch has taken 0:02:46.335039
Number of used sentences in train = 2811
Total loss for epoch 5: 6467.161877
validation loss after epoch 5 : 907.239443
	Epoch 6....
Epoch has taken 0:02:45.769437
Number of used sentences in train = 2811
Total loss for epoch 6: 6284.825028
validation loss after epoch 6 : 926.090965
	Epoch 7....
Epoch has taken 0:02:45.913684
Number of used sentences in train = 2811
Total loss for epoch 7: 6159.815767
validation loss after epoch 7 : 952.017239
	Epoch 8....
Epoch has taken 0:02:45.895045
Number of used sentences in train = 2811
Total loss for epoch 8: 6059.498344
validation loss after epoch 8 : 939.629265
	Epoch 9....
Epoch has taken 0:02:45.879764
Number of used sentences in train = 2811
Total loss for epoch 9: 5919.432135
validation loss after epoch 9 : 941.660549
	Epoch 10....
Epoch has taken 0:02:45.876815
Number of used sentences in train = 2811
Total loss for epoch 10: 5840.796448
validation loss after epoch 10 : 943.681496
	Epoch 11....
Epoch has taken 0:02:45.938739
Number of used sentences in train = 2811
Total loss for epoch 11: 5693.837936
validation loss after epoch 11 : 931.087190
	Epoch 12....
Epoch has taken 0:02:45.872221
Number of used sentences in train = 2811
Total loss for epoch 12: 5644.848935
validation loss after epoch 12 : 990.642671
	Epoch 13....
Epoch has taken 0:02:45.840284
Number of used sentences in train = 2811
Total loss for epoch 13: 5595.533751
validation loss after epoch 13 : 986.193683
	Epoch 14....
Epoch has taken 0:02:45.818046
Number of used sentences in train = 2811
Total loss for epoch 14: 5483.070570
validation loss after epoch 14 : 971.241627
	TransitionClassifier(
  (p_embeddings): Embedding(18, 51)
  (w_embeddings): Embedding(1177, 220)
  (lstm): LSTM(271, 29, num_layers=2, dropout=0.33, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:45.818463
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1481.933022
	Epoch 1....
Epoch has taken 0:00:17.511471
Number of used sentences in train = 313
Total loss for epoch 1: 863.902453
	Epoch 2....
Epoch has taken 0:00:17.497289
Number of used sentences in train = 313
Total loss for epoch 2: 738.870745
	Epoch 3....
Epoch has taken 0:00:17.499512
Number of used sentences in train = 313
Total loss for epoch 3: 699.425339
	Epoch 4....
Epoch has taken 0:00:17.507135
Number of used sentences in train = 313
Total loss for epoch 4: 630.474138
	Epoch 5....
Epoch has taken 0:00:17.512665
Number of used sentences in train = 313
Total loss for epoch 5: 605.244284
	Epoch 6....
Epoch has taken 0:00:17.775626
Number of used sentences in train = 313
Total loss for epoch 6: 589.924772
	Epoch 7....
Epoch has taken 0:00:17.498163
Number of used sentences in train = 313
Total loss for epoch 7: 582.344728
	Epoch 8....
Epoch has taken 0:00:17.498641
Number of used sentences in train = 313
Total loss for epoch 8: 559.250247
	Epoch 9....
Epoch has taken 0:00:17.502117
Number of used sentences in train = 313
Total loss for epoch 9: 564.643640
	Epoch 10....
Epoch has taken 0:00:17.502475
Number of used sentences in train = 313
Total loss for epoch 10: 560.436195
	Epoch 11....
Epoch has taken 0:00:17.499218
Number of used sentences in train = 313
Total loss for epoch 11: 551.010259
	Epoch 12....
Epoch has taken 0:00:17.509186
Number of used sentences in train = 313
Total loss for epoch 12: 549.013954
	Epoch 13....
Epoch has taken 0:00:17.495227
Number of used sentences in train = 313
Total loss for epoch 13: 539.366679
	Epoch 14....
Epoch has taken 0:00:17.495426
Number of used sentences in train = 313
Total loss for epoch 14: 535.639539
Epoch has taken 0:00:17.495142

==================================================================================================
	Training time : 0:45:51.422199
==================================================================================================
	Identification : 0.425

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 51)
  (w_embeddings): Embedding(1133, 220)
  (lstm): LSTM(271, 29, num_layers=2, dropout=0.33, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9623.799411
validation loss after epoch 0 : 756.236095
	Epoch 1....
Epoch has taken 0:01:54.003984
Number of used sentences in train = 2074
Total loss for epoch 1: 6006.477427
validation loss after epoch 1 : 709.426764
	Epoch 2....
Epoch has taken 0:01:54.094692
Number of used sentences in train = 2074
Total loss for epoch 2: 5291.394011
validation loss after epoch 2 : 676.083280
	Epoch 3....
Epoch has taken 0:01:54.127885
Number of used sentences in train = 2074
Total loss for epoch 3: 4882.212616
validation loss after epoch 3 : 718.775858
	Epoch 4....
Epoch has taken 0:01:54.068696
Number of used sentences in train = 2074
Total loss for epoch 4: 4633.898745
validation loss after epoch 4 : 736.069508
	Epoch 5....
Epoch has taken 0:01:54.052001
Number of used sentences in train = 2074
Total loss for epoch 5: 4393.296588
validation loss after epoch 5 : 742.357796
	Epoch 6....
Epoch has taken 0:01:54.239972
Number of used sentences in train = 2074
Total loss for epoch 6: 4187.182079
validation loss after epoch 6 : 710.264336
	Epoch 7....
Epoch has taken 0:01:54.183562
Number of used sentences in train = 2074
Total loss for epoch 7: 4105.186139
validation loss after epoch 7 : 778.969442
	Epoch 8....
Epoch has taken 0:01:54.210195
Number of used sentences in train = 2074
Total loss for epoch 8: 3953.063149
validation loss after epoch 8 : 754.910367
	Epoch 9....
Epoch has taken 0:01:54.225240
Number of used sentences in train = 2074
Total loss for epoch 9: 3880.951649
validation loss after epoch 9 : 777.802626
	Epoch 10....
Epoch has taken 0:01:54.289023
Number of used sentences in train = 2074
Total loss for epoch 10: 3814.569481
validation loss after epoch 10 : 783.367183
	Epoch 11....
Epoch has taken 0:01:54.200312
Number of used sentences in train = 2074
Total loss for epoch 11: 3781.826537
validation loss after epoch 11 : 793.956322
	Epoch 12....
Epoch has taken 0:01:54.289525
Number of used sentences in train = 2074
Total loss for epoch 12: 3736.824582
validation loss after epoch 12 : 787.785551
	Epoch 13....
Epoch has taken 0:01:54.257295
Number of used sentences in train = 2074
Total loss for epoch 13: 3667.935338
validation loss after epoch 13 : 850.128774
	Epoch 14....
Epoch has taken 0:01:54.273947
Number of used sentences in train = 2074
Total loss for epoch 14: 3663.696084
validation loss after epoch 14 : 808.101977
	TransitionClassifier(
  (p_embeddings): Embedding(18, 51)
  (w_embeddings): Embedding(1133, 220)
  (lstm): LSTM(271, 29, num_layers=2, dropout=0.33, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.126943
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1151.242203
	Epoch 1....
Epoch has taken 0:00:11.660674
Number of used sentences in train = 231
Total loss for epoch 1: 651.946670
	Epoch 2....
Epoch has taken 0:00:11.661856
Number of used sentences in train = 231
Total loss for epoch 2: 500.689119
	Epoch 3....
Epoch has taken 0:00:11.657869
Number of used sentences in train = 231
Total loss for epoch 3: 479.491287
	Epoch 4....
Epoch has taken 0:00:11.659828
Number of used sentences in train = 231
Total loss for epoch 4: 447.614955
	Epoch 5....
Epoch has taken 0:00:11.656072
Number of used sentences in train = 231
Total loss for epoch 5: 440.631654
	Epoch 6....
Epoch has taken 0:00:11.658096
Number of used sentences in train = 231
Total loss for epoch 6: 415.697360
	Epoch 7....
Epoch has taken 0:00:11.656478
Number of used sentences in train = 231
Total loss for epoch 7: 409.640972
	Epoch 8....
Epoch has taken 0:00:11.666652
Number of used sentences in train = 231
Total loss for epoch 8: 397.412568
	Epoch 9....
Epoch has taken 0:00:11.666014
Number of used sentences in train = 231
Total loss for epoch 9: 441.309171
	Epoch 10....
Epoch has taken 0:00:11.658923
Number of used sentences in train = 231
Total loss for epoch 10: 404.756359
	Epoch 11....
Epoch has taken 0:00:11.665108
Number of used sentences in train = 231
Total loss for epoch 11: 378.644081
	Epoch 12....
Epoch has taken 0:00:11.661734
Number of used sentences in train = 231
Total loss for epoch 12: 378.977160
	Epoch 13....
Epoch has taken 0:00:11.665479
Number of used sentences in train = 231
Total loss for epoch 13: 372.205333
	Epoch 14....
Epoch has taken 0:00:11.665106
Number of used sentences in train = 231
Total loss for epoch 14: 379.541881
Epoch has taken 0:00:11.655800

==================================================================================================
	Training time : 0:31:27.893684
==================================================================================================
	Identification : 0.404

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 51)
  (w_embeddings): Embedding(1202, 220)
  (lstm): LSTM(271, 29, num_layers=2, dropout=0.33, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16238.297136
validation loss after epoch 0 : 1404.068539
	Epoch 1....
Epoch has taken 0:03:41.115978
Number of used sentences in train = 3226
Total loss for epoch 1: 12002.993898
validation loss after epoch 1 : 1355.211266
	Epoch 2....
Epoch has taken 0:03:41.248085
Number of used sentences in train = 3226
Total loss for epoch 2: 11278.338964
validation loss after epoch 2 : 1327.158048
	Epoch 3....
Epoch has taken 0:03:40.906751
Number of used sentences in train = 3226
Total loss for epoch 3: 10618.899716
validation loss after epoch 3 : 1324.321855
	Epoch 4....
Epoch has taken 0:03:40.992871
Number of used sentences in train = 3226
Total loss for epoch 4: 10311.281114
validation loss after epoch 4 : 1321.350161
	Epoch 5....
Epoch has taken 0:03:40.987179
Number of used sentences in train = 3226
Total loss for epoch 5: 9928.917901
validation loss after epoch 5 : 1378.112603
	Epoch 6....
Epoch has taken 0:03:41.124307
Number of used sentences in train = 3226
Total loss for epoch 6: 9684.959028
validation loss after epoch 6 : 1381.584858
	Epoch 7....
Epoch has taken 0:03:40.837935
Number of used sentences in train = 3226
Total loss for epoch 7: 9478.191463
validation loss after epoch 7 : 1360.809169
	Epoch 8....
Epoch has taken 0:03:40.894884
Number of used sentences in train = 3226
Total loss for epoch 8: 9267.648827
validation loss after epoch 8 : 1326.786090
	Epoch 9....
Epoch has taken 0:03:40.976761
Number of used sentences in train = 3226
Total loss for epoch 9: 9089.461344
validation loss after epoch 9 : 1344.297072
	Epoch 10....
Epoch has taken 0:03:41.135762
Number of used sentences in train = 3226
Total loss for epoch 10: 8935.553083
validation loss after epoch 10 : 1355.495994
	Epoch 11....
Epoch has taken 0:03:41.118035
Number of used sentences in train = 3226
Total loss for epoch 11: 8836.000128
validation loss after epoch 11 : 1402.129786
	Epoch 12....
Epoch has taken 0:03:40.925571
Number of used sentences in train = 3226
Total loss for epoch 12: 8766.871647
validation loss after epoch 12 : 1344.658217
	Epoch 13....
Epoch has taken 0:03:40.921883
Number of used sentences in train = 3226
Total loss for epoch 13: 8668.555274
validation loss after epoch 13 : 1366.414155
	Epoch 14....
Epoch has taken 0:03:40.969363
Number of used sentences in train = 3226
Total loss for epoch 14: 8518.996748
validation loss after epoch 14 : 1402.793548
	TransitionClassifier(
  (p_embeddings): Embedding(13, 51)
  (w_embeddings): Embedding(1202, 220)
  (lstm): LSTM(271, 29, num_layers=2, dropout=0.33, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:40.891613
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1724.314521
	Epoch 1....
Epoch has taken 0:00:21.571774
Number of used sentences in train = 359
Total loss for epoch 1: 1240.056648
	Epoch 2....
Epoch has taken 0:00:21.580790
Number of used sentences in train = 359
Total loss for epoch 2: 1094.924312
	Epoch 3....
Epoch has taken 0:00:21.591103
Number of used sentences in train = 359
Total loss for epoch 3: 1020.187106
	Epoch 4....
Epoch has taken 0:00:21.589851
Number of used sentences in train = 359
Total loss for epoch 4: 927.780879
	Epoch 5....
Epoch has taken 0:00:21.586176
Number of used sentences in train = 359
Total loss for epoch 5: 907.877752
	Epoch 6....
Epoch has taken 0:00:21.587740
Number of used sentences in train = 359
Total loss for epoch 6: 884.786057
	Epoch 7....
Epoch has taken 0:00:21.566632
Number of used sentences in train = 359
Total loss for epoch 7: 846.709936
	Epoch 8....
Epoch has taken 0:00:21.565243
Number of used sentences in train = 359
Total loss for epoch 8: 830.765559
	Epoch 9....
Epoch has taken 0:00:21.584446
Number of used sentences in train = 359
Total loss for epoch 9: 837.724747
	Epoch 10....
Epoch has taken 0:00:21.583753
Number of used sentences in train = 359
Total loss for epoch 10: 812.885512
	Epoch 11....
Epoch has taken 0:00:21.567594
Number of used sentences in train = 359
Total loss for epoch 11: 800.367519
	Epoch 12....
Epoch has taken 0:00:21.575657
Number of used sentences in train = 359
Total loss for epoch 12: 789.482694
	Epoch 13....
Epoch has taken 0:00:21.575308
Number of used sentences in train = 359
Total loss for epoch 13: 805.112394
	Epoch 14....
Epoch has taken 0:00:21.572371
Number of used sentences in train = 359
Total loss for epoch 14: 757.686758
Epoch has taken 0:00:21.579044

==================================================================================================
	Training time : 1:00:39.369481
==================================================================================================
	Identification : 0.492

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 36, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 43, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 92, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 134, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 43)
  (w_embeddings): Embedding(9375, 134)
  (lstm): LSTM(177, 92, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 19869.510352
validation loss after epoch 0 : 1342.889030
	Epoch 1....
Epoch has taken 0:02:47.016309
Number of used sentences in train = 2811
Total loss for epoch 1: 11217.326618
validation loss after epoch 1 : 1129.485035
	Epoch 2....
Epoch has taken 0:02:46.826065
Number of used sentences in train = 2811
Total loss for epoch 2: 8544.701953
validation loss after epoch 2 : 1241.684674
	Epoch 3....
Epoch has taken 0:02:46.997607
Number of used sentences in train = 2811
Total loss for epoch 3: 7093.518775
validation loss after epoch 3 : 1133.099453
	Epoch 4....
Epoch has taken 0:02:47.056946
Number of used sentences in train = 2811
Total loss for epoch 4: 6238.039074
validation loss after epoch 4 : 1181.548622
	Epoch 5....
Epoch has taken 0:02:46.789633
Number of used sentences in train = 2811
Total loss for epoch 5: 5728.916373
validation loss after epoch 5 : 1228.556737
	Epoch 6....
Epoch has taken 0:02:46.812675
Number of used sentences in train = 2811
Total loss for epoch 6: 5390.571643
validation loss after epoch 6 : 1292.952107
	Epoch 7....
Epoch has taken 0:02:46.718716
Number of used sentences in train = 2811
Total loss for epoch 7: 5174.305448
validation loss after epoch 7 : 1311.572233
	Epoch 8....
Epoch has taken 0:02:46.696399
Number of used sentences in train = 2811
Total loss for epoch 8: 5010.174684
validation loss after epoch 8 : 1333.971596
	Epoch 9....
Epoch has taken 0:02:46.740339
Number of used sentences in train = 2811
Total loss for epoch 9: 4853.317966
validation loss after epoch 9 : 1492.927935
	Epoch 10....
Epoch has taken 0:02:46.751550
Number of used sentences in train = 2811
Total loss for epoch 10: 4763.063962
validation loss after epoch 10 : 1562.601074
	Epoch 11....
Epoch has taken 0:02:46.729268
Number of used sentences in train = 2811
Total loss for epoch 11: 4749.922307
validation loss after epoch 11 : 1539.259769
	Epoch 12....
Epoch has taken 0:02:46.823924
Number of used sentences in train = 2811
Total loss for epoch 12: 4690.023934
validation loss after epoch 12 : 1549.643106
	Epoch 13....
Epoch has taken 0:02:46.709142
Number of used sentences in train = 2811
Total loss for epoch 13: 4662.483786
validation loss after epoch 13 : 1594.874288
	Epoch 14....
Epoch has taken 0:02:46.763698
Number of used sentences in train = 2811
Total loss for epoch 14: 4622.730592
validation loss after epoch 14 : 1619.348771
	TransitionClassifier(
  (p_embeddings): Embedding(18, 43)
  (w_embeddings): Embedding(9375, 134)
  (lstm): LSTM(177, 92, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.701011
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1731.176770
	Epoch 1....
Epoch has taken 0:00:17.635787
Number of used sentences in train = 313
Total loss for epoch 1: 966.632999
	Epoch 2....
Epoch has taken 0:00:17.631329
Number of used sentences in train = 313
Total loss for epoch 2: 774.165729
	Epoch 3....
Epoch has taken 0:00:17.639418
Number of used sentences in train = 313
Total loss for epoch 3: 647.398022
	Epoch 4....
Epoch has taken 0:00:17.623479
Number of used sentences in train = 313
Total loss for epoch 4: 614.477641
	Epoch 5....
Epoch has taken 0:00:17.628501
Number of used sentences in train = 313
Total loss for epoch 5: 579.717202
	Epoch 6....
Epoch has taken 0:00:17.624623
Number of used sentences in train = 313
Total loss for epoch 6: 554.748772
	Epoch 7....
Epoch has taken 0:00:17.625800
Number of used sentences in train = 313
Total loss for epoch 7: 539.970302
	Epoch 8....
Epoch has taken 0:00:17.616648
Number of used sentences in train = 313
Total loss for epoch 8: 536.198178
	Epoch 9....
Epoch has taken 0:00:17.626236
Number of used sentences in train = 313
Total loss for epoch 9: 527.847362
	Epoch 10....
Epoch has taken 0:00:17.624039
Number of used sentences in train = 313
Total loss for epoch 10: 524.495033
	Epoch 11....
Epoch has taken 0:00:17.630488
Number of used sentences in train = 313
Total loss for epoch 11: 524.437186
	Epoch 12....
Epoch has taken 0:00:17.623750
Number of used sentences in train = 313
Total loss for epoch 12: 530.030052
	Epoch 13....
Epoch has taken 0:00:17.624803
Number of used sentences in train = 313
Total loss for epoch 13: 519.308345
	Epoch 14....
Epoch has taken 0:00:17.628140
Number of used sentences in train = 313
Total loss for epoch 14: 517.001427
Epoch has taken 0:00:17.621802

==================================================================================================
	Training time : 0:46:07.051054
==================================================================================================
	Identification : 0.112

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 43)
  (w_embeddings): Embedding(7075, 134)
  (lstm): LSTM(177, 92, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 21210.974932
validation loss after epoch 0 : 2296.845620
	Epoch 1....
Epoch has taken 0:01:53.957214
Number of used sentences in train = 2074
Total loss for epoch 1: 16834.855842
validation loss after epoch 1 : 1296.445552
	Epoch 2....
Epoch has taken 0:01:53.835273
Number of used sentences in train = 2074
Total loss for epoch 2: 8960.287906
validation loss after epoch 2 : 957.210753
	Epoch 3....
Epoch has taken 0:01:53.951198
Number of used sentences in train = 2074
Total loss for epoch 3: 6370.752203
validation loss after epoch 3 : 899.546541
	Epoch 4....
Epoch has taken 0:01:53.913046
Number of used sentences in train = 2074
Total loss for epoch 4: 5178.322858
validation loss after epoch 4 : 1048.550832
	Epoch 5....
Epoch has taken 0:01:53.940141
Number of used sentences in train = 2074
Total loss for epoch 5: 4410.285500
validation loss after epoch 5 : 1036.483777
	Epoch 6....
Epoch has taken 0:01:53.913746
Number of used sentences in train = 2074
Total loss for epoch 6: 3978.729010
validation loss after epoch 6 : 1046.364445
	Epoch 7....
Epoch has taken 0:01:53.930528
Number of used sentences in train = 2074
Total loss for epoch 7: 3767.759253
validation loss after epoch 7 : 1138.798516
	Epoch 8....
Epoch has taken 0:01:53.943696
Number of used sentences in train = 2074
Total loss for epoch 8: 3543.191941
validation loss after epoch 8 : 1114.786977
	Epoch 9....
Epoch has taken 0:01:53.949642
Number of used sentences in train = 2074
Total loss for epoch 9: 3421.265623
validation loss after epoch 9 : 1216.919938
	Epoch 10....
Epoch has taken 0:01:53.939587
Number of used sentences in train = 2074
Total loss for epoch 10: 3366.783399
validation loss after epoch 10 : 1262.745126
	Epoch 11....
Epoch has taken 0:01:53.964577
Number of used sentences in train = 2074
Total loss for epoch 11: 3347.035677
validation loss after epoch 11 : 1284.715857
	Epoch 12....
Epoch has taken 0:01:53.950943
Number of used sentences in train = 2074
Total loss for epoch 12: 3306.219124
validation loss after epoch 12 : 1365.729243
	Epoch 13....
Epoch has taken 0:01:53.952736
Number of used sentences in train = 2074
Total loss for epoch 13: 3312.174966
validation loss after epoch 13 : 1389.944144
	Epoch 14....
Epoch has taken 0:01:53.902670
Number of used sentences in train = 2074
Total loss for epoch 14: 3269.546049
validation loss after epoch 14 : 1340.495787
	TransitionClassifier(
  (p_embeddings): Embedding(18, 43)
  (w_embeddings): Embedding(7075, 134)
  (lstm): LSTM(177, 92, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.908801
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1639.358159
	Epoch 1....
Epoch has taken 0:00:11.590313
Number of used sentences in train = 231
Total loss for epoch 1: 798.918437
	Epoch 2....
Epoch has taken 0:00:11.593722
Number of used sentences in train = 231
Total loss for epoch 2: 588.479058
	Epoch 3....
Epoch has taken 0:00:11.597195
Number of used sentences in train = 231
Total loss for epoch 3: 446.482596
	Epoch 4....
Epoch has taken 0:00:11.591966
Number of used sentences in train = 231
Total loss for epoch 4: 400.758180
	Epoch 5....
Epoch has taken 0:00:11.592060
Number of used sentences in train = 231
Total loss for epoch 5: 366.250254
	Epoch 6....
Epoch has taken 0:00:11.604227
Number of used sentences in train = 231
Total loss for epoch 6: 356.876580
	Epoch 7....
Epoch has taken 0:00:11.594577
Number of used sentences in train = 231
Total loss for epoch 7: 354.017496
	Epoch 8....
Epoch has taken 0:00:11.591383
Number of used sentences in train = 231
Total loss for epoch 8: 351.849833
	Epoch 9....
Epoch has taken 0:00:11.590218
Number of used sentences in train = 231
Total loss for epoch 9: 349.811057
	Epoch 10....
Epoch has taken 0:00:11.591981
Number of used sentences in train = 231
Total loss for epoch 10: 348.864964
	Epoch 11....
Epoch has taken 0:00:11.705694
Number of used sentences in train = 231
Total loss for epoch 11: 347.574435
	Epoch 12....
Epoch has taken 0:00:11.589573
Number of used sentences in train = 231
Total loss for epoch 12: 348.701231
	Epoch 13....
Epoch has taken 0:00:11.595863
Number of used sentences in train = 231
Total loss for epoch 13: 347.506643
	Epoch 14....
Epoch has taken 0:00:11.587320
Number of used sentences in train = 231
Total loss for epoch 14: 346.639578
Epoch has taken 0:00:11.586061

==================================================================================================
	Training time : 0:31:23.302785
==================================================================================================
	Identification : 0.342

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 43)
  (w_embeddings): Embedding(18044, 134)
  (lstm): LSTM(177, 92, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 22654.503955
validation loss after epoch 0 : 1725.038696
	Epoch 1....
Epoch has taken 0:03:41.684252
Number of used sentences in train = 3226
Total loss for epoch 1: 13735.305064
validation loss after epoch 1 : 1608.044735
	Epoch 2....
Epoch has taken 0:03:41.681345
Number of used sentences in train = 3226
Total loss for epoch 2: 10833.793969
validation loss after epoch 2 : 1556.569414
	Epoch 3....
Epoch has taken 0:03:42.273838
Number of used sentences in train = 3226
Total loss for epoch 3: 9068.664141
validation loss after epoch 3 : 1786.760617
	Epoch 4....
Epoch has taken 0:03:42.150083
Number of used sentences in train = 3226
Total loss for epoch 4: 7958.964452
validation loss after epoch 4 : 1881.147240
	Epoch 5....
Epoch has taken 0:03:42.006913
Number of used sentences in train = 3226
Total loss for epoch 5: 7263.726785
validation loss after epoch 5 : 2084.120166
	Epoch 6....
Epoch has taken 0:03:41.926830
Number of used sentences in train = 3226
Total loss for epoch 6: 6843.178980
validation loss after epoch 6 : 2265.663403
	Epoch 7....
Epoch has taken 0:03:42.015171
Number of used sentences in train = 3226
Total loss for epoch 7: 6593.926059
validation loss after epoch 7 : 2362.543460
	Epoch 8....
Epoch has taken 0:03:41.982994
Number of used sentences in train = 3226
Total loss for epoch 8: 6459.607241
validation loss after epoch 8 : 2521.322203
	Epoch 9....
Epoch has taken 0:03:42.329599
Number of used sentences in train = 3226
Total loss for epoch 9: 6342.978992
validation loss after epoch 9 : 2554.530698
	Epoch 10....
Epoch has taken 0:03:42.336895
Number of used sentences in train = 3226
Total loss for epoch 10: 6282.509784
validation loss after epoch 10 : 2611.480297
	Epoch 11....
Epoch has taken 0:03:42.090579
Number of used sentences in train = 3226
Total loss for epoch 11: 6276.889713
validation loss after epoch 11 : 2534.292558
	Epoch 12....
Epoch has taken 0:03:41.967018
Number of used sentences in train = 3226
Total loss for epoch 12: 6225.405580
validation loss after epoch 12 : 2781.576059
	Epoch 13....
Epoch has taken 0:03:42.074512
Number of used sentences in train = 3226
Total loss for epoch 13: 6195.693732
validation loss after epoch 13 : 2832.987290
	Epoch 14....
Epoch has taken 0:03:42.059077
Number of used sentences in train = 3226
Total loss for epoch 14: 6210.346824
validation loss after epoch 14 : 2743.069424
	TransitionClassifier(
  (p_embeddings): Embedding(13, 43)
  (w_embeddings): Embedding(18044, 134)
  (lstm): LSTM(177, 92, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1472, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:42.170339
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2705.260887
	Epoch 1....
Epoch has taken 0:00:21.745355
Number of used sentences in train = 359
Total loss for epoch 1: 1338.223853
	Epoch 2....
Epoch has taken 0:00:21.743498
Number of used sentences in train = 359
Total loss for epoch 2: 985.774180
	Epoch 3....
Epoch has taken 0:00:21.742450
Number of used sentences in train = 359
Total loss for epoch 3: 826.248356
	Epoch 4....
Epoch has taken 0:00:21.741303
Number of used sentences in train = 359
Total loss for epoch 4: 730.204920
	Epoch 5....
Epoch has taken 0:00:21.746249
Number of used sentences in train = 359
Total loss for epoch 5: 700.740689
	Epoch 6....
Epoch has taken 0:00:21.749394
Number of used sentences in train = 359
Total loss for epoch 6: 688.432894
	Epoch 7....
Epoch has taken 0:00:21.725070
Number of used sentences in train = 359
Total loss for epoch 7: 684.081295
	Epoch 8....
Epoch has taken 0:00:21.712020
Number of used sentences in train = 359
Total loss for epoch 8: 675.130702
	Epoch 9....
Epoch has taken 0:00:21.715061
Number of used sentences in train = 359
Total loss for epoch 9: 674.276812
	Epoch 10....
Epoch has taken 0:00:21.709475
Number of used sentences in train = 359
Total loss for epoch 10: 672.733559
	Epoch 11....
Epoch has taken 0:00:21.708920
Number of used sentences in train = 359
Total loss for epoch 11: 674.874430
	Epoch 12....
Epoch has taken 0:00:21.724672
Number of used sentences in train = 359
Total loss for epoch 12: 671.335674
	Epoch 13....
Epoch has taken 0:00:21.716144
Number of used sentences in train = 359
Total loss for epoch 13: 671.353076
	Epoch 14....
Epoch has taken 0:00:21.715765
Number of used sentences in train = 359
Total loss for epoch 14: 671.102919
Epoch has taken 0:00:21.724960

==================================================================================================
	Training time : 1:00:57.362963
==================================================================================================
	Identification : 0.448

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 27, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 20, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 27, 'lstmDropout': 0.21, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(5882, 63)
  (lstm): LSTM(83, 27, num_layers=2, dropout=0.21, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13568.599440
validation loss after epoch 0 : 1167.728909
	Epoch 1....
Epoch has taken 0:02:46.071141
Number of used sentences in train = 2811
Total loss for epoch 1: 9045.504609
validation loss after epoch 1 : 1104.277610
	Epoch 2....
Epoch has taken 0:02:46.186685
Number of used sentences in train = 2811
Total loss for epoch 2: 7782.619345
validation loss after epoch 2 : 1100.109000
	Epoch 3....
Epoch has taken 0:02:46.016360
Number of used sentences in train = 2811
Total loss for epoch 3: 6864.866527
validation loss after epoch 3 : 1135.806458
	Epoch 4....
Epoch has taken 0:02:46.064457
Number of used sentences in train = 2811
Total loss for epoch 4: 6231.129347
validation loss after epoch 4 : 1119.440103
	Epoch 5....
Epoch has taken 0:02:45.979976
Number of used sentences in train = 2811
Total loss for epoch 5: 5784.401764
validation loss after epoch 5 : 1225.220392
	Epoch 6....
Epoch has taken 0:02:46.021064
Number of used sentences in train = 2811
Total loss for epoch 6: 5526.942701
validation loss after epoch 6 : 1232.740154
	Epoch 7....
Epoch has taken 0:02:46.058061
Number of used sentences in train = 2811
Total loss for epoch 7: 5256.589391
validation loss after epoch 7 : 1305.131706
	Epoch 8....
Epoch has taken 0:02:46.176960
Number of used sentences in train = 2811
Total loss for epoch 8: 5124.261606
validation loss after epoch 8 : 1335.724540
	Epoch 9....
Epoch has taken 0:02:46.160286
Number of used sentences in train = 2811
Total loss for epoch 9: 5039.963439
validation loss after epoch 9 : 1324.573349
	Epoch 10....
Epoch has taken 0:02:46.204225
Number of used sentences in train = 2811
Total loss for epoch 10: 4955.459517
validation loss after epoch 10 : 1370.640997
	Epoch 11....
Epoch has taken 0:02:46.161939
Number of used sentences in train = 2811
Total loss for epoch 11: 4912.916959
validation loss after epoch 11 : 1430.280446
	Epoch 12....
Epoch has taken 0:02:46.138060
Number of used sentences in train = 2811
Total loss for epoch 12: 4844.809988
validation loss after epoch 12 : 1441.706559
	Epoch 13....
Epoch has taken 0:02:45.952666
Number of used sentences in train = 2811
Total loss for epoch 13: 4798.665643
validation loss after epoch 13 : 1430.738260
	Epoch 14....
Epoch has taken 0:02:45.975221
Number of used sentences in train = 2811
Total loss for epoch 14: 4736.480286
validation loss after epoch 14 : 1473.682880
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(5882, 63)
  (lstm): LSTM(83, 27, num_layers=2, dropout=0.21, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.008491
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1577.137491
	Epoch 1....
Epoch has taken 0:00:17.570479
Number of used sentences in train = 313
Total loss for epoch 1: 809.910114
	Epoch 2....
Epoch has taken 0:00:17.571814
Number of used sentences in train = 313
Total loss for epoch 2: 673.037035
	Epoch 3....
Epoch has taken 0:00:17.557819
Number of used sentences in train = 313
Total loss for epoch 3: 598.212225
	Epoch 4....
Epoch has taken 0:00:17.570978
Number of used sentences in train = 313
Total loss for epoch 4: 567.108314
	Epoch 5....
Epoch has taken 0:00:17.576902
Number of used sentences in train = 313
Total loss for epoch 5: 552.507093
	Epoch 6....
Epoch has taken 0:00:17.571871
Number of used sentences in train = 313
Total loss for epoch 6: 542.448049
	Epoch 7....
Epoch has taken 0:00:17.579130
Number of used sentences in train = 313
Total loss for epoch 7: 542.448174
	Epoch 8....
Epoch has taken 0:00:17.573693
Number of used sentences in train = 313
Total loss for epoch 8: 536.900347
	Epoch 9....
Epoch has taken 0:00:17.562789
Number of used sentences in train = 313
Total loss for epoch 9: 534.264633
	Epoch 10....
Epoch has taken 0:00:17.573307
Number of used sentences in train = 313
Total loss for epoch 10: 527.619462
	Epoch 11....
Epoch has taken 0:00:17.565245
Number of used sentences in train = 313
Total loss for epoch 11: 527.296124
	Epoch 12....
Epoch has taken 0:00:17.579251
Number of used sentences in train = 313
Total loss for epoch 12: 524.516426
	Epoch 13....
Epoch has taken 0:00:17.569813
Number of used sentences in train = 313
Total loss for epoch 13: 529.600957
	Epoch 14....
Epoch has taken 0:00:17.576991
Number of used sentences in train = 313
Total loss for epoch 14: 530.776325
Epoch has taken 0:00:17.573492

==================================================================================================
	Training time : 0:45:55.245790
==================================================================================================
	Identification : 0.479

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(5578, 63)
  (lstm): LSTM(83, 27, num_layers=2, dropout=0.21, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11227.268297
validation loss after epoch 0 : 931.821854
	Epoch 1....
Epoch has taken 0:01:53.640340
Number of used sentences in train = 2074
Total loss for epoch 1: 7051.089995
validation loss after epoch 1 : 897.500278
	Epoch 2....
Epoch has taken 0:01:53.612909
Number of used sentences in train = 2074
Total loss for epoch 2: 6014.462770
validation loss after epoch 2 : 901.343437
	Epoch 3....
Epoch has taken 0:01:53.719515
Number of used sentences in train = 2074
Total loss for epoch 3: 5262.425654
validation loss after epoch 3 : 945.305978
	Epoch 4....
Epoch has taken 0:01:53.674401
Number of used sentences in train = 2074
Total loss for epoch 4: 4786.545661
validation loss after epoch 4 : 985.476922
	Epoch 5....
Epoch has taken 0:01:53.753937
Number of used sentences in train = 2074
Total loss for epoch 5: 4371.045767
validation loss after epoch 5 : 971.298260
	Epoch 6....
Epoch has taken 0:01:53.787742
Number of used sentences in train = 2074
Total loss for epoch 6: 4099.089773
validation loss after epoch 6 : 965.834307
	Epoch 7....
Epoch has taken 0:01:53.847415
Number of used sentences in train = 2074
Total loss for epoch 7: 3941.187192
validation loss after epoch 7 : 1102.080919
	Epoch 8....
Epoch has taken 0:01:53.924555
Number of used sentences in train = 2074
Total loss for epoch 8: 3766.625783
validation loss after epoch 8 : 1199.633293
	Epoch 9....
Epoch has taken 0:01:53.819698
Number of used sentences in train = 2074
Total loss for epoch 9: 3671.821114
validation loss after epoch 9 : 1182.663182
	Epoch 10....
Epoch has taken 0:01:53.822974
Number of used sentences in train = 2074
Total loss for epoch 10: 3582.699397
validation loss after epoch 10 : 1151.483565
	Epoch 11....
Epoch has taken 0:01:53.716583
Number of used sentences in train = 2074
Total loss for epoch 11: 3528.957978
validation loss after epoch 11 : 1248.316353
	Epoch 12....
Epoch has taken 0:01:53.695636
Number of used sentences in train = 2074
Total loss for epoch 12: 3487.515414
validation loss after epoch 12 : 1314.853395
	Epoch 13....
Epoch has taken 0:01:53.724611
Number of used sentences in train = 2074
Total loss for epoch 13: 3457.681724
validation loss after epoch 13 : 1203.336281
	Epoch 14....
Epoch has taken 0:01:53.713609
Number of used sentences in train = 2074
Total loss for epoch 14: 3399.093325
validation loss after epoch 14 : 1296.707714
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(5578, 63)
  (lstm): LSTM(83, 27, num_layers=2, dropout=0.21, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.773719
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1643.820478
	Epoch 1....
Epoch has taken 0:00:11.574567
Number of used sentences in train = 231
Total loss for epoch 1: 798.886570
	Epoch 2....
Epoch has taken 0:00:11.572674
Number of used sentences in train = 231
Total loss for epoch 2: 599.987769
	Epoch 3....
Epoch has taken 0:00:11.579589
Number of used sentences in train = 231
Total loss for epoch 3: 471.950438
	Epoch 4....
Epoch has taken 0:00:11.571384
Number of used sentences in train = 231
Total loss for epoch 4: 424.055120
	Epoch 5....
Epoch has taken 0:00:11.573933
Number of used sentences in train = 231
Total loss for epoch 5: 395.323789
	Epoch 6....
Epoch has taken 0:00:11.577295
Number of used sentences in train = 231
Total loss for epoch 6: 379.182012
	Epoch 7....
Epoch has taken 0:00:11.579428
Number of used sentences in train = 231
Total loss for epoch 7: 373.129995
	Epoch 8....
Epoch has taken 0:00:11.582600
Number of used sentences in train = 231
Total loss for epoch 8: 369.737353
	Epoch 9....
Epoch has taken 0:00:11.587437
Number of used sentences in train = 231
Total loss for epoch 9: 367.207594
	Epoch 10....
Epoch has taken 0:00:11.581976
Number of used sentences in train = 231
Total loss for epoch 10: 363.130933
	Epoch 11....
Epoch has taken 0:00:11.584693
Number of used sentences in train = 231
Total loss for epoch 11: 362.015940
	Epoch 12....
Epoch has taken 0:00:11.581061
Number of used sentences in train = 231
Total loss for epoch 12: 356.877710
	Epoch 13....
Epoch has taken 0:00:11.583854
Number of used sentences in train = 231
Total loss for epoch 13: 355.803056
	Epoch 14....
Epoch has taken 0:00:11.587207
Number of used sentences in train = 231
Total loss for epoch 14: 352.117218
Epoch has taken 0:00:11.587170

==================================================================================================
	Training time : 0:31:20.265590
==================================================================================================
	Identification : 0.106

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 20)
  (w_embeddings): Embedding(6885, 63)
  (lstm): LSTM(83, 27, num_layers=2, dropout=0.21, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16896.144174
validation loss after epoch 0 : 1520.112897
	Epoch 1....
Epoch has taken 0:03:40.017483
Number of used sentences in train = 3226
Total loss for epoch 1: 12345.231950
validation loss after epoch 1 : 1474.119506
	Epoch 2....
Epoch has taken 0:03:40.030864
Number of used sentences in train = 3226
Total loss for epoch 2: 10721.335892
validation loss after epoch 2 : 1467.913280
	Epoch 3....
Epoch has taken 0:03:40.070795
Number of used sentences in train = 3226
Total loss for epoch 3: 9824.735065
validation loss after epoch 3 : 1499.180218
	Epoch 4....
Epoch has taken 0:03:39.960067
Number of used sentences in train = 3226
Total loss for epoch 4: 9091.781264
validation loss after epoch 4 : 1550.517030
	Epoch 5....
Epoch has taken 0:03:39.979606
Number of used sentences in train = 3226
Total loss for epoch 5: 8606.387678
validation loss after epoch 5 : 1595.863241
	Epoch 6....
Epoch has taken 0:03:39.825416
Number of used sentences in train = 3226
Total loss for epoch 6: 8116.253190
validation loss after epoch 6 : 1719.344417
	Epoch 7....
Epoch has taken 0:03:39.777081
Number of used sentences in train = 3226
Total loss for epoch 7: 7754.660475
validation loss after epoch 7 : 1803.303856
	Epoch 8....
Epoch has taken 0:03:39.714458
Number of used sentences in train = 3226
Total loss for epoch 8: 7487.599439
validation loss after epoch 8 : 1901.846199
	Epoch 9....
Epoch has taken 0:03:39.864665
Number of used sentences in train = 3226
Total loss for epoch 9: 7229.254450
validation loss after epoch 9 : 1902.175762
	Epoch 10....
Epoch has taken 0:03:39.787089
Number of used sentences in train = 3226
Total loss for epoch 10: 7156.471534
validation loss after epoch 10 : 2062.129908
	Epoch 11....
Epoch has taken 0:03:39.872939
Number of used sentences in train = 3226
Total loss for epoch 11: 6916.325338
validation loss after epoch 11 : 2052.740791
	Epoch 12....
Epoch has taken 0:03:39.817586
Number of used sentences in train = 3226
Total loss for epoch 12: 6793.494499
validation loss after epoch 12 : 2166.753016
	Epoch 13....
Epoch has taken 0:03:39.865228
Number of used sentences in train = 3226
Total loss for epoch 13: 6794.313246
validation loss after epoch 13 : 2115.809965
	Epoch 14....
Epoch has taken 0:03:39.701046
Number of used sentences in train = 3226
Total loss for epoch 14: 6680.544104
validation loss after epoch 14 : 2284.864662
	TransitionClassifier(
  (p_embeddings): Embedding(13, 20)
  (w_embeddings): Embedding(6885, 63)
  (lstm): LSTM(83, 27, num_layers=2, dropout=0.21, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:39.908704
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2331.597247
	Epoch 1....
Epoch has taken 0:00:21.503300
Number of used sentences in train = 359
Total loss for epoch 1: 1218.760041
	Epoch 2....
Epoch has taken 0:00:21.499292
Number of used sentences in train = 359
Total loss for epoch 2: 994.687407
	Epoch 3....
Epoch has taken 0:00:21.508110
Number of used sentences in train = 359
Total loss for epoch 3: 858.588604
	Epoch 4....
Epoch has taken 0:00:21.504450
Number of used sentences in train = 359
Total loss for epoch 4: 783.904405
	Epoch 5....
Epoch has taken 0:00:21.503785
Number of used sentences in train = 359
Total loss for epoch 5: 730.232402
	Epoch 6....
Epoch has taken 0:00:21.504979
Number of used sentences in train = 359
Total loss for epoch 6: 714.551435
	Epoch 7....
Epoch has taken 0:00:21.509527
Number of used sentences in train = 359
Total loss for epoch 7: 720.322056
	Epoch 8....
Epoch has taken 0:00:21.505082
Number of used sentences in train = 359
Total loss for epoch 8: 692.631507
	Epoch 9....
Epoch has taken 0:00:21.500582
Number of used sentences in train = 359
Total loss for epoch 9: 696.371055
	Epoch 10....
Epoch has taken 0:00:21.497873
Number of used sentences in train = 359
Total loss for epoch 10: 693.587353
	Epoch 11....
Epoch has taken 0:00:21.499160
Number of used sentences in train = 359
Total loss for epoch 11: 688.393635
	Epoch 12....
Epoch has taken 0:00:21.505345
Number of used sentences in train = 359
Total loss for epoch 12: 681.482851
	Epoch 13....
Epoch has taken 0:00:21.510398
Number of used sentences in train = 359
Total loss for epoch 13: 681.373399
	Epoch 14....
Epoch has taken 0:00:21.517201
Number of used sentences in train = 359
Total loss for epoch 14: 678.375098
Epoch has taken 0:00:21.510962

==================================================================================================
	Training time : 1:00:21.418536
==================================================================================================
	Identification : 0.15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 34, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 28, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 79, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5879, 79)
  (lstm): LSTM(96, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12747.457744
validation loss after epoch 0 : 1587.127330
	Epoch 1....
Epoch has taken 0:02:35.115574
Number of used sentences in train = 2811
Total loss for epoch 1: 8578.112137
validation loss after epoch 1 : 1103.844234
	Epoch 2....
Epoch has taken 0:02:35.098623
Number of used sentences in train = 2811
Total loss for epoch 2: 6929.191839
validation loss after epoch 2 : 1137.885314
	Epoch 3....
Epoch has taken 0:02:35.285694
Number of used sentences in train = 2811
Total loss for epoch 3: 6060.884945
validation loss after epoch 3 : 1243.400889
	Epoch 4....
Epoch has taken 0:02:35.173448
Number of used sentences in train = 2811
Total loss for epoch 4: 5520.398977
validation loss after epoch 4 : 1301.326737
	Epoch 5....
Epoch has taken 0:02:35.445822
Number of used sentences in train = 2811
Total loss for epoch 5: 5135.165997
validation loss after epoch 5 : 1421.194071
	Epoch 6....
Epoch has taken 0:02:35.309087
Number of used sentences in train = 2811
Total loss for epoch 6: 4927.362880
validation loss after epoch 6 : 1486.217387
	Epoch 7....
Epoch has taken 0:02:35.294264
Number of used sentences in train = 2811
Total loss for epoch 7: 4782.627057
validation loss after epoch 7 : 1556.262316
	Epoch 8....
Epoch has taken 0:02:35.301940
Number of used sentences in train = 2811
Total loss for epoch 8: 4700.447711
validation loss after epoch 8 : 1608.688927
	Epoch 9....
Epoch has taken 0:02:35.125673
Number of used sentences in train = 2811
Total loss for epoch 9: 4640.702163
validation loss after epoch 9 : 1660.973992
	Epoch 10....
Epoch has taken 0:02:35.087841
Number of used sentences in train = 2811
Total loss for epoch 10: 4620.540028
validation loss after epoch 10 : 1680.238518
	Epoch 11....
Epoch has taken 0:02:35.152805
Number of used sentences in train = 2811
Total loss for epoch 11: 4588.311338
validation loss after epoch 11 : 1721.226129
	Epoch 12....
Epoch has taken 0:02:35.177344
Number of used sentences in train = 2811
Total loss for epoch 12: 4563.737413
validation loss after epoch 12 : 1745.993298
	Epoch 13....
Epoch has taken 0:02:35.131999
Number of used sentences in train = 2811
Total loss for epoch 13: 4545.743946
validation loss after epoch 13 : 1763.062938
	Epoch 14....
Epoch has taken 0:02:35.215969
Number of used sentences in train = 2811
Total loss for epoch 14: 4533.528786
validation loss after epoch 14 : 1786.783639
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5879, 79)
  (lstm): LSTM(96, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:35.272027
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1493.532957
	Epoch 1....
Epoch has taken 0:00:16.377719
Number of used sentences in train = 313
Total loss for epoch 1: 853.646135
	Epoch 2....
Epoch has taken 0:00:16.364009
Number of used sentences in train = 313
Total loss for epoch 2: 618.977522
	Epoch 3....
Epoch has taken 0:00:16.368731
Number of used sentences in train = 313
Total loss for epoch 3: 555.825088
	Epoch 4....
Epoch has taken 0:00:16.361790
Number of used sentences in train = 313
Total loss for epoch 4: 529.892987
	Epoch 5....
Epoch has taken 0:00:16.363376
Number of used sentences in train = 313
Total loss for epoch 5: 524.139796
	Epoch 6....
Epoch has taken 0:00:16.359419
Number of used sentences in train = 313
Total loss for epoch 6: 520.844854
	Epoch 7....
Epoch has taken 0:00:16.351664
Number of used sentences in train = 313
Total loss for epoch 7: 517.684713
	Epoch 8....
Epoch has taken 0:00:16.371484
Number of used sentences in train = 313
Total loss for epoch 8: 517.409169
	Epoch 9....
Epoch has taken 0:00:16.363037
Number of used sentences in train = 313
Total loss for epoch 9: 516.829471
	Epoch 10....
Epoch has taken 0:00:16.365748
Number of used sentences in train = 313
Total loss for epoch 10: 514.913247
	Epoch 11....
Epoch has taken 0:00:16.363810
Number of used sentences in train = 313
Total loss for epoch 11: 511.703048
	Epoch 12....
Epoch has taken 0:00:16.365608
Number of used sentences in train = 313
Total loss for epoch 12: 510.921570
	Epoch 13....
Epoch has taken 0:00:16.373176
Number of used sentences in train = 313
Total loss for epoch 13: 510.622475
	Epoch 14....
Epoch has taken 0:00:16.373191
Number of used sentences in train = 313
Total loss for epoch 14: 510.595585
Epoch has taken 0:00:16.362615

==================================================================================================
	Training time : 0:42:54.169739
==================================================================================================
	Identification : 0.402

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5683, 79)
  (lstm): LSTM(96, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10203.724737
validation loss after epoch 0 : 993.371024
	Epoch 1....
Epoch has taken 0:01:46.207461
Number of used sentences in train = 2074
Total loss for epoch 1: 6272.325450
validation loss after epoch 1 : 988.666721
	Epoch 2....
Epoch has taken 0:01:46.294939
Number of used sentences in train = 2074
Total loss for epoch 2: 4966.870012
validation loss after epoch 2 : 1000.324485
	Epoch 3....
Epoch has taken 0:01:46.521313
Number of used sentences in train = 2074
Total loss for epoch 3: 4211.497836
validation loss after epoch 3 : 1141.697256
	Epoch 4....
Epoch has taken 0:01:46.365134
Number of used sentences in train = 2074
Total loss for epoch 4: 3777.548030
validation loss after epoch 4 : 1281.202330
	Epoch 5....
Epoch has taken 0:01:46.347752
Number of used sentences in train = 2074
Total loss for epoch 5: 3538.237140
validation loss after epoch 5 : 1324.054231
	Epoch 6....
Epoch has taken 0:01:46.360051
Number of used sentences in train = 2074
Total loss for epoch 6: 3396.435444
validation loss after epoch 6 : 1401.397106
	Epoch 7....
Epoch has taken 0:01:46.286030
Number of used sentences in train = 2074
Total loss for epoch 7: 3303.576556
validation loss after epoch 7 : 1429.984418
	Epoch 8....
Epoch has taken 0:01:46.216968
Number of used sentences in train = 2074
Total loss for epoch 8: 3259.217724
validation loss after epoch 8 : 1573.335206
	Epoch 9....
Epoch has taken 0:01:46.318960
Number of used sentences in train = 2074
Total loss for epoch 9: 3237.668022
validation loss after epoch 9 : 1497.527170
	Epoch 10....
Epoch has taken 0:01:46.336495
Number of used sentences in train = 2074
Total loss for epoch 10: 3229.859377
validation loss after epoch 10 : 1501.809467
	Epoch 11....
Epoch has taken 0:01:46.244736
Number of used sentences in train = 2074
Total loss for epoch 11: 3209.604352
validation loss after epoch 11 : 1534.368245
	Epoch 12....
Epoch has taken 0:01:46.221119
Number of used sentences in train = 2074
Total loss for epoch 12: 3199.404563
validation loss after epoch 12 : 1549.837568
	Epoch 13....
Epoch has taken 0:01:46.280783
Number of used sentences in train = 2074
Total loss for epoch 13: 3191.954670
validation loss after epoch 13 : 1577.144570
	Epoch 14....
Epoch has taken 0:01:46.302449
Number of used sentences in train = 2074
Total loss for epoch 14: 3183.099771
validation loss after epoch 14 : 1596.531058
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5683, 79)
  (lstm): LSTM(96, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:46.286042
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2029.636151
	Epoch 1....
Epoch has taken 0:00:10.802658
Number of used sentences in train = 231
Total loss for epoch 1: 641.746303
	Epoch 2....
Epoch has taken 0:00:10.801235
Number of used sentences in train = 231
Total loss for epoch 2: 458.130874
	Epoch 3....
Epoch has taken 0:00:10.794861
Number of used sentences in train = 231
Total loss for epoch 3: 387.862718
	Epoch 4....
Epoch has taken 0:00:10.805243
Number of used sentences in train = 231
Total loss for epoch 4: 364.135081
	Epoch 5....
Epoch has taken 0:00:10.796946
Number of used sentences in train = 231
Total loss for epoch 5: 356.616101
	Epoch 6....
Epoch has taken 0:00:10.799101
Number of used sentences in train = 231
Total loss for epoch 6: 354.299854
	Epoch 7....
Epoch has taken 0:00:10.789655
Number of used sentences in train = 231
Total loss for epoch 7: 352.096351
	Epoch 8....
Epoch has taken 0:00:10.796778
Number of used sentences in train = 231
Total loss for epoch 8: 350.351228
	Epoch 9....
Epoch has taken 0:00:10.797958
Number of used sentences in train = 231
Total loss for epoch 9: 349.363269
	Epoch 10....
Epoch has taken 0:00:10.790592
Number of used sentences in train = 231
Total loss for epoch 10: 348.593976
	Epoch 11....
Epoch has taken 0:00:10.795374
Number of used sentences in train = 231
Total loss for epoch 11: 348.455276
	Epoch 12....
Epoch has taken 0:00:10.785337
Number of used sentences in train = 231
Total loss for epoch 12: 347.861180
	Epoch 13....
Epoch has taken 0:00:10.798154
Number of used sentences in train = 231
Total loss for epoch 13: 347.554396
	Epoch 14....
Epoch has taken 0:00:10.812066
Number of used sentences in train = 231
Total loss for epoch 14: 346.960147
Epoch has taken 0:00:10.816970

==================================================================================================
	Training time : 0:29:16.907791
==================================================================================================
	Identification : 0.23

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(6829, 79)
  (lstm): LSTM(96, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17085.937136
validation loss after epoch 0 : 1535.175512
	Epoch 1....
Epoch has taken 0:03:26.824957
Number of used sentences in train = 3226
Total loss for epoch 1: 12220.415024
validation loss after epoch 1 : 1534.830461
	Epoch 2....
Epoch has taken 0:03:26.882966
Number of used sentences in train = 3226
Total loss for epoch 2: 10571.956917
validation loss after epoch 2 : 1552.747741
	Epoch 3....
Epoch has taken 0:03:27.109188
Number of used sentences in train = 3226
Total loss for epoch 3: 9468.588931
validation loss after epoch 3 : 1650.734808
	Epoch 4....
Epoch has taken 0:03:26.916912
Number of used sentences in train = 3226
Total loss for epoch 4: 8572.780312
validation loss after epoch 4 : 1734.309607
	Epoch 5....
Epoch has taken 0:03:27.017902
Number of used sentences in train = 3226
Total loss for epoch 5: 7901.975603
validation loss after epoch 5 : 1917.145181
	Epoch 6....
Epoch has taken 0:03:26.983369
Number of used sentences in train = 3226
Total loss for epoch 6: 7376.462935
validation loss after epoch 6 : 2022.517374
	Epoch 7....
Epoch has taken 0:03:27.028676
Number of used sentences in train = 3226
Total loss for epoch 7: 7003.091379
validation loss after epoch 7 : 2161.332903
	Epoch 8....
Epoch has taken 0:03:27.052552
Number of used sentences in train = 3226
Total loss for epoch 8: 6726.564136
validation loss after epoch 8 : 2288.926932
	Epoch 9....
Epoch has taken 0:03:27.091758
Number of used sentences in train = 3226
Total loss for epoch 9: 6564.689350
validation loss after epoch 9 : 2398.633665
	Epoch 10....
Epoch has taken 0:03:27.180375
Number of used sentences in train = 3226
Total loss for epoch 10: 6475.662848
validation loss after epoch 10 : 2385.446894
	Epoch 11....
Epoch has taken 0:03:27.209457
Number of used sentences in train = 3226
Total loss for epoch 11: 6386.882634
validation loss after epoch 11 : 2503.349219
	Epoch 12....
Epoch has taken 0:03:26.873209
Number of used sentences in train = 3226
Total loss for epoch 12: 6320.651224
validation loss after epoch 12 : 2592.372217
	Epoch 13....
Epoch has taken 0:03:26.904636
Number of used sentences in train = 3226
Total loss for epoch 13: 6272.948672
validation loss after epoch 13 : 2639.372770
	Epoch 14....
Epoch has taken 0:03:26.910080
Number of used sentences in train = 3226
Total loss for epoch 14: 6254.292801
validation loss after epoch 14 : 2660.033231
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(6829, 79)
  (lstm): LSTM(96, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:26.909983
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2204.137259
	Epoch 1....
Epoch has taken 0:00:20.160932
Number of used sentences in train = 359
Total loss for epoch 1: 1136.708089
	Epoch 2....
Epoch has taken 0:00:20.170934
Number of used sentences in train = 359
Total loss for epoch 2: 910.087201
	Epoch 3....
Epoch has taken 0:00:20.168843
Number of used sentences in train = 359
Total loss for epoch 3: 777.309184
	Epoch 4....
Epoch has taken 0:00:20.176213
Number of used sentences in train = 359
Total loss for epoch 4: 728.191231
	Epoch 5....
Epoch has taken 0:00:20.154864
Number of used sentences in train = 359
Total loss for epoch 5: 693.871006
	Epoch 6....
Epoch has taken 0:00:20.165875
Number of used sentences in train = 359
Total loss for epoch 6: 682.353964
	Epoch 7....
Epoch has taken 0:00:20.156156
Number of used sentences in train = 359
Total loss for epoch 7: 677.952192
	Epoch 8....
Epoch has taken 0:00:20.161846
Number of used sentences in train = 359
Total loss for epoch 8: 676.223751
	Epoch 9....
Epoch has taken 0:00:20.162047
Number of used sentences in train = 359
Total loss for epoch 9: 674.740775
	Epoch 10....
Epoch has taken 0:00:20.150876
Number of used sentences in train = 359
Total loss for epoch 10: 673.786243
	Epoch 11....
Epoch has taken 0:00:20.168581
Number of used sentences in train = 359
Total loss for epoch 11: 673.261123
	Epoch 12....
Epoch has taken 0:00:20.175045
Number of used sentences in train = 359
Total loss for epoch 12: 672.844922
	Epoch 13....
Epoch has taken 0:00:20.174608
Number of used sentences in train = 359
Total loss for epoch 13: 672.513440
	Epoch 14....
Epoch has taken 0:00:20.169098
Number of used sentences in train = 359
Total loss for epoch 14: 672.243165
Epoch has taken 0:00:20.162547

==================================================================================================
	Training time : 0:56:48.018965
==================================================================================================
	Identification : 0.466

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 83, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 34, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 96, 'lstmDropout': 0.13, 'denseActivation': 'tanh', 'wordDim': 184, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(5963, 184)
  (lstm): LSTM(218, 96, bidirectional=True)
  (linear1): Linear(in_features=1536, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13474.789206
validation loss after epoch 0 : 1100.438685
	Epoch 1....
Epoch has taken 0:02:35.845431
Number of used sentences in train = 2811
Total loss for epoch 1: 8376.234191
validation loss after epoch 1 : 1062.083350
	Epoch 2....
Epoch has taken 0:02:35.903136
Number of used sentences in train = 2811
Total loss for epoch 2: 6814.840012
validation loss after epoch 2 : 1112.556969
	Epoch 3....
Epoch has taken 0:02:36.020770
Number of used sentences in train = 2811
Total loss for epoch 3: 5858.290715
validation loss after epoch 3 : 1198.446653
	Epoch 4....
Epoch has taken 0:02:35.951066
Number of used sentences in train = 2811
Total loss for epoch 4: 5248.590596
validation loss after epoch 4 : 1270.998333
	Epoch 5....
Epoch has taken 0:02:36.024820
Number of used sentences in train = 2811
Total loss for epoch 5: 4898.895491
validation loss after epoch 5 : 1343.800803
	Epoch 6....
Epoch has taken 0:02:36.113088
Number of used sentences in train = 2811
Total loss for epoch 6: 4731.702474
validation loss after epoch 6 : 1472.174690
	Epoch 7....
Epoch has taken 0:02:36.131257
Number of used sentences in train = 2811
Total loss for epoch 7: 4643.035347
validation loss after epoch 7 : 1432.930897
	Epoch 8....
Epoch has taken 0:02:36.180575
Number of used sentences in train = 2811
Total loss for epoch 8: 4604.737589
validation loss after epoch 8 : 1472.606368
	Epoch 9....
Epoch has taken 0:02:36.171623
Number of used sentences in train = 2811
Total loss for epoch 9: 4565.366236
validation loss after epoch 9 : 1489.037860
	Epoch 10....
Epoch has taken 0:02:36.193585
Number of used sentences in train = 2811
Total loss for epoch 10: 4544.729049
validation loss after epoch 10 : 1506.534416
	Epoch 11....
Epoch has taken 0:02:36.114448
Number of used sentences in train = 2811
Total loss for epoch 11: 4533.133577
validation loss after epoch 11 : 1521.499830
	Epoch 12....
Epoch has taken 0:02:36.191599
Number of used sentences in train = 2811
Total loss for epoch 12: 4516.329899
validation loss after epoch 12 : 1549.984860
	Epoch 13....
Epoch has taken 0:02:36.158022
Number of used sentences in train = 2811
Total loss for epoch 13: 4518.062913
validation loss after epoch 13 : 1549.602951
	Epoch 14....
Epoch has taken 0:02:36.196581
Number of used sentences in train = 2811
Total loss for epoch 14: 4509.419192
validation loss after epoch 14 : 1561.412887
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(5963, 184)
  (lstm): LSTM(218, 96, bidirectional=True)
  (linear1): Linear(in_features=1536, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:36.185406
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1928.089773
	Epoch 1....
Epoch has taken 0:00:16.467590
Number of used sentences in train = 313
Total loss for epoch 1: 801.699273
	Epoch 2....
Epoch has taken 0:00:16.465291
Number of used sentences in train = 313
Total loss for epoch 2: 609.147302
	Epoch 3....
Epoch has taken 0:00:16.458314
Number of used sentences in train = 313
Total loss for epoch 3: 553.562049
	Epoch 4....
Epoch has taken 0:00:16.456547
Number of used sentences in train = 313
Total loss for epoch 4: 530.016632
	Epoch 5....
Epoch has taken 0:00:16.471558
Number of used sentences in train = 313
Total loss for epoch 5: 521.975730
	Epoch 6....
Epoch has taken 0:00:16.454316
Number of used sentences in train = 313
Total loss for epoch 6: 518.661858
	Epoch 7....
Epoch has taken 0:00:16.458050
Number of used sentences in train = 313
Total loss for epoch 7: 517.145733
	Epoch 8....
Epoch has taken 0:00:16.460221
Number of used sentences in train = 313
Total loss for epoch 8: 514.516161
	Epoch 9....
Epoch has taken 0:00:16.473868
Number of used sentences in train = 313
Total loss for epoch 9: 511.808295
	Epoch 10....
Epoch has taken 0:00:18.436176
Number of used sentences in train = 313
Total loss for epoch 10: 510.511871
	Epoch 11....
Epoch has taken 0:00:16.458695
Number of used sentences in train = 313
Total loss for epoch 11: 511.635604
	Epoch 12....
Epoch has taken 0:00:16.462386
Number of used sentences in train = 313
Total loss for epoch 12: 509.957323
	Epoch 13....
Epoch has taken 0:00:16.506977
Number of used sentences in train = 313
Total loss for epoch 13: 510.243752
	Epoch 14....
Epoch has taken 0:00:16.451860
Number of used sentences in train = 313
Total loss for epoch 14: 509.946724
Epoch has taken 0:00:16.448499

==================================================================================================
	Training time : 0:43:10.817689
==================================================================================================
	Identification : 0.304

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(5613, 184)
  (lstm): LSTM(218, 96, bidirectional=True)
  (linear1): Linear(in_features=1536, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10754.276287
validation loss after epoch 0 : 1037.869059
	Epoch 1....
Epoch has taken 0:01:46.855187
Number of used sentences in train = 2074
Total loss for epoch 1: 6084.874852
validation loss after epoch 1 : 937.113172
	Epoch 2....
Epoch has taken 0:01:46.864734
Number of used sentences in train = 2074
Total loss for epoch 2: 4666.311079
validation loss after epoch 2 : 1075.236162
	Epoch 3....
Epoch has taken 0:01:47.079504
Number of used sentences in train = 2074
Total loss for epoch 3: 3931.254494
validation loss after epoch 3 : 1042.106843
	Epoch 4....
Epoch has taken 0:01:47.077696
Number of used sentences in train = 2074
Total loss for epoch 4: 3537.720454
validation loss after epoch 4 : 1175.293821
	Epoch 5....
Epoch has taken 0:01:47.041311
Number of used sentences in train = 2074
Total loss for epoch 5: 3375.502066
validation loss after epoch 5 : 1146.259571
	Epoch 6....
Epoch has taken 0:01:47.064370
Number of used sentences in train = 2074
Total loss for epoch 6: 3298.632280
validation loss after epoch 6 : 1193.085865
	Epoch 7....
Epoch has taken 0:01:47.041985
Number of used sentences in train = 2074
Total loss for epoch 7: 3253.055069
validation loss after epoch 7 : 1222.434378
	Epoch 8....
Epoch has taken 0:01:47.019423
Number of used sentences in train = 2074
Total loss for epoch 8: 3220.750915
validation loss after epoch 8 : 1240.369669
	Epoch 9....
Epoch has taken 0:01:46.987570
Number of used sentences in train = 2074
Total loss for epoch 9: 3207.183596
validation loss after epoch 9 : 1265.383808
	Epoch 10....
Epoch has taken 0:01:46.995006
Number of used sentences in train = 2074
Total loss for epoch 10: 3197.682099
validation loss after epoch 10 : 1269.129854
	Epoch 11....
Epoch has taken 0:01:47.039038
Number of used sentences in train = 2074
Total loss for epoch 11: 3190.390049
validation loss after epoch 11 : 1285.167670
	Epoch 12....
Epoch has taken 0:01:47.030014
Number of used sentences in train = 2074
Total loss for epoch 12: 3186.412456
validation loss after epoch 12 : 1294.050064
	Epoch 13....
Epoch has taken 0:01:46.968411
Number of used sentences in train = 2074
Total loss for epoch 13: 3183.073050
validation loss after epoch 13 : 1290.486566
	Epoch 14....
Epoch has taken 0:01:47.049452
Number of used sentences in train = 2074
Total loss for epoch 14: 3179.970829
validation loss after epoch 14 : 1304.606012
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(5613, 184)
  (lstm): LSTM(218, 96, bidirectional=True)
  (linear1): Linear(in_features=1536, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:47.000471
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1726.898309
	Epoch 1....
Epoch has taken 0:00:10.876822
Number of used sentences in train = 231
Total loss for epoch 1: 655.993780
	Epoch 2....
Epoch has taken 0:00:10.883363
Number of used sentences in train = 231
Total loss for epoch 2: 463.422661
	Epoch 3....
Epoch has taken 0:00:10.885026
Number of used sentences in train = 231
Total loss for epoch 3: 395.695962
	Epoch 4....
Epoch has taken 0:00:10.882935
Number of used sentences in train = 231
Total loss for epoch 4: 372.322788
	Epoch 5....
Epoch has taken 0:00:10.876005
Number of used sentences in train = 231
Total loss for epoch 5: 366.386866
	Epoch 6....
Epoch has taken 0:00:10.877696
Number of used sentences in train = 231
Total loss for epoch 6: 362.310074
	Epoch 7....
Epoch has taken 0:00:10.886696
Number of used sentences in train = 231
Total loss for epoch 7: 357.278347
	Epoch 8....
Epoch has taken 0:00:10.882967
Number of used sentences in train = 231
Total loss for epoch 8: 354.109185
	Epoch 9....
Epoch has taken 0:00:10.884709
Number of used sentences in train = 231
Total loss for epoch 9: 352.202564
	Epoch 10....
Epoch has taken 0:00:10.881688
Number of used sentences in train = 231
Total loss for epoch 10: 350.842252
	Epoch 11....
Epoch has taken 0:00:10.889698
Number of used sentences in train = 231
Total loss for epoch 11: 349.859148
	Epoch 12....
Epoch has taken 0:00:10.884345
Number of used sentences in train = 231
Total loss for epoch 12: 349.363190
	Epoch 13....
Epoch has taken 0:00:10.881047
Number of used sentences in train = 231
Total loss for epoch 13: 348.972101
	Epoch 14....
Epoch has taken 0:00:10.896346
Number of used sentences in train = 231
Total loss for epoch 14: 348.317604
Epoch has taken 0:00:10.882217

==================================================================================================
	Training time : 0:29:28.708113
==================================================================================================
	Identification : 0.235

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 34)
  (w_embeddings): Embedding(6862, 184)
  (lstm): LSTM(218, 96, bidirectional=True)
  (linear1): Linear(in_features=1536, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18032.475518
validation loss after epoch 0 : 1628.945698
	Epoch 1....
Epoch has taken 0:03:28.356143
Number of used sentences in train = 3226
Total loss for epoch 1: 12016.829231
validation loss after epoch 1 : 1506.579015
	Epoch 2....
Epoch has taken 0:03:28.522967
Number of used sentences in train = 3226
Total loss for epoch 2: 10053.016405
validation loss after epoch 2 : 1539.304106
	Epoch 3....
Epoch has taken 0:03:32.065315
Number of used sentences in train = 3226
Total loss for epoch 3: 8686.943966
validation loss after epoch 3 : 1640.085419
	Epoch 4....
Epoch has taken 0:04:18.911254
Number of used sentences in train = 3226
Total loss for epoch 4: 7660.642137
validation loss after epoch 4 : 1802.678602
	Epoch 5....
Epoch has taken 0:03:28.544441
Number of used sentences in train = 3226
Total loss for epoch 5: 7078.613665
validation loss after epoch 5 : 1956.685250
	Epoch 6....
Epoch has taken 0:03:28.502189
Number of used sentences in train = 3226
Total loss for epoch 6: 6699.254539
validation loss after epoch 6 : 2012.650141
	Epoch 7....
Epoch has taken 0:03:29.609842
Number of used sentences in train = 3226
Total loss for epoch 7: 6485.823708
validation loss after epoch 7 : 2150.836775
	Epoch 8....
Epoch has taken 0:03:41.414623
Number of used sentences in train = 3226
Total loss for epoch 8: 6333.317958
validation loss after epoch 8 : 2181.369463
	Epoch 9....
Epoch has taken 0:03:28.270339
Number of used sentences in train = 3226
Total loss for epoch 9: 6261.956743
validation loss after epoch 9 : 2230.913601
	Epoch 10....
Epoch has taken 0:03:28.375384
Number of used sentences in train = 3226
Total loss for epoch 10: 6224.863746
validation loss after epoch 10 : 2353.941641
	Epoch 11....
Epoch has taken 0:03:28.438138
Number of used sentences in train = 3226
Total loss for epoch 11: 6206.690135
validation loss after epoch 11 : 2346.631231
	Epoch 12....
Epoch has taken 0:03:28.369327
Number of used sentences in train = 3226
Total loss for epoch 12: 6186.573042
validation loss after epoch 12 : 2397.591835
	Epoch 13....
Epoch has taken 0:03:28.301046
Number of used sentences in train = 3226
Total loss for epoch 13: 6172.362217
validation loss after epoch 13 : 2445.160792
	Epoch 14....
Epoch has taken 0:03:28.422507
Number of used sentences in train = 3226
Total loss for epoch 14: 6159.273271
validation loss after epoch 14 : 2536.703421
	TransitionClassifier(
  (p_embeddings): Embedding(13, 34)
  (w_embeddings): Embedding(6862, 184)
  (lstm): LSTM(218, 96, bidirectional=True)
  (linear1): Linear(in_features=1536, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:28.388356
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2248.252450
	Epoch 1....
Epoch has taken 0:00:20.304912
Number of used sentences in train = 359
Total loss for epoch 1: 1224.903813
	Epoch 2....
Epoch has taken 0:00:20.316703
Number of used sentences in train = 359
Total loss for epoch 2: 906.925178
	Epoch 3....
Epoch has taken 0:00:20.315544
Number of used sentences in train = 359
Total loss for epoch 3: 773.182116
	Epoch 4....
Epoch has taken 0:00:20.309018
Number of used sentences in train = 359
Total loss for epoch 4: 728.380093
	Epoch 5....
Epoch has taken 0:00:20.316689
Number of used sentences in train = 359
Total loss for epoch 5: 706.357380
	Epoch 6....
Epoch has taken 0:00:20.305644
Number of used sentences in train = 359
Total loss for epoch 6: 689.792477
	Epoch 7....
Epoch has taken 0:00:20.309036
Number of used sentences in train = 359
Total loss for epoch 7: 685.342412
	Epoch 8....
Epoch has taken 0:00:20.304464
Number of used sentences in train = 359
Total loss for epoch 8: 679.471144
	Epoch 9....
Epoch has taken 0:00:20.299708
Number of used sentences in train = 359
Total loss for epoch 9: 676.921765
	Epoch 10....
Epoch has taken 0:00:20.318830
Number of used sentences in train = 359
Total loss for epoch 10: 674.799116
	Epoch 11....
Epoch has taken 0:00:20.299790
Number of used sentences in train = 359
Total loss for epoch 11: 672.742116
	Epoch 12....
Epoch has taken 0:00:20.295042
Number of used sentences in train = 359
Total loss for epoch 12: 672.295463
	Epoch 13....
Epoch has taken 0:00:20.407101
Number of used sentences in train = 359
Total loss for epoch 13: 671.940966
	Epoch 14....
Epoch has taken 0:00:20.289911
Number of used sentences in train = 359
Total loss for epoch 14: 671.610346
Epoch has taken 0:00:20.307466

==================================================================================================
	Training time : 0:58:19.847998
==================================================================================================
	Identification : 0.303

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 73, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 62, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 95, 'lstmDropout': 0.18, 'denseActivation': 'tanh', 'wordDim': 124, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 62)
  (w_embeddings): Embedding(9360, 124)
  (lstm): LSTM(186, 95, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13765.829727
validation loss after epoch 0 : 1136.709770
	Epoch 1....
Epoch has taken 0:02:36.129196
Number of used sentences in train = 2811
Total loss for epoch 1: 9177.958567
validation loss after epoch 1 : 1077.361656
	Epoch 2....
Epoch has taken 0:02:36.299255
Number of used sentences in train = 2811
Total loss for epoch 2: 7155.314323
validation loss after epoch 2 : 1102.628839
	Epoch 3....
Epoch has taken 0:02:36.340518
Number of used sentences in train = 2811
Total loss for epoch 3: 5974.600700
validation loss after epoch 3 : 1243.454233
	Epoch 4....
Epoch has taken 0:02:36.149231
Number of used sentences in train = 2811
Total loss for epoch 4: 5332.866610
validation loss after epoch 4 : 1276.519663
	Epoch 5....
Epoch has taken 0:02:36.162551
Number of used sentences in train = 2811
Total loss for epoch 5: 4973.708871
validation loss after epoch 5 : 1348.445542
	Epoch 6....
Epoch has taken 0:02:36.085101
Number of used sentences in train = 2811
Total loss for epoch 6: 4759.996883
validation loss after epoch 6 : 1405.767912
	Epoch 7....
Epoch has taken 0:02:36.297157
Number of used sentences in train = 2811
Total loss for epoch 7: 4656.536183
validation loss after epoch 7 : 1439.298533
	Epoch 8....
Epoch has taken 0:02:36.374579
Number of used sentences in train = 2811
Total loss for epoch 8: 4580.891017
validation loss after epoch 8 : 1490.189171
	Epoch 9....
Epoch has taken 0:02:36.391960
Number of used sentences in train = 2811
Total loss for epoch 9: 4550.807575
validation loss after epoch 9 : 1534.490702
	Epoch 10....
Epoch has taken 0:02:36.480061
Number of used sentences in train = 2811
Total loss for epoch 10: 4529.619860
validation loss after epoch 10 : 1548.264820
	Epoch 11....
Epoch has taken 0:02:36.389933
Number of used sentences in train = 2811
Total loss for epoch 11: 4516.207340
validation loss after epoch 11 : 1600.215077
	Epoch 12....
Epoch has taken 0:02:36.277189
Number of used sentences in train = 2811
Total loss for epoch 12: 4509.171436
validation loss after epoch 12 : 1612.359843
	Epoch 13....
Epoch has taken 0:02:36.241197
Number of used sentences in train = 2811
Total loss for epoch 13: 4503.156507
validation loss after epoch 13 : 1641.456480
	Epoch 14....
Epoch has taken 0:02:36.285338
Number of used sentences in train = 2811
Total loss for epoch 14: 4501.343558
validation loss after epoch 14 : 1655.683871
	TransitionClassifier(
  (p_embeddings): Embedding(18, 62)
  (w_embeddings): Embedding(9360, 124)
  (lstm): LSTM(186, 95, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:36.390244
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1631.737765
	Epoch 1....
Epoch has taken 0:00:16.518419
Number of used sentences in train = 313
Total loss for epoch 1: 866.014021
	Epoch 2....
Epoch has taken 0:00:16.513627
Number of used sentences in train = 313
Total loss for epoch 2: 626.421691
	Epoch 3....
Epoch has taken 0:00:16.523261
Number of used sentences in train = 313
Total loss for epoch 3: 573.719523
	Epoch 4....
Epoch has taken 0:00:16.514012
Number of used sentences in train = 313
Total loss for epoch 4: 549.882939
	Epoch 5....
Epoch has taken 0:00:16.516034
Number of used sentences in train = 313
Total loss for epoch 5: 542.453066
	Epoch 6....
Epoch has taken 0:00:16.508374
Number of used sentences in train = 313
Total loss for epoch 6: 530.375234
	Epoch 7....
Epoch has taken 0:00:16.509531
Number of used sentences in train = 313
Total loss for epoch 7: 524.128810
	Epoch 8....
Epoch has taken 0:00:16.505246
Number of used sentences in train = 313
Total loss for epoch 8: 521.498036
	Epoch 9....
Epoch has taken 0:00:16.507867
Number of used sentences in train = 313
Total loss for epoch 9: 510.997023
	Epoch 10....
Epoch has taken 0:00:16.505727
Number of used sentences in train = 313
Total loss for epoch 10: 506.227875
	Epoch 11....
Epoch has taken 0:00:16.495891
Number of used sentences in train = 313
Total loss for epoch 11: 504.432784
	Epoch 12....
Epoch has taken 0:00:16.499711
Number of used sentences in train = 313
Total loss for epoch 12: 503.669029
	Epoch 13....
Epoch has taken 0:00:16.496074
Number of used sentences in train = 313
Total loss for epoch 13: 502.964671
	Epoch 14....
Epoch has taken 0:00:16.498827
Number of used sentences in train = 313
Total loss for epoch 14: 502.538993
Epoch has taken 0:00:16.500919

==================================================================================================
	Training time : 0:43:12.420150
==================================================================================================
	Identification : 0.352

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 62)
  (w_embeddings): Embedding(7074, 124)
  (lstm): LSTM(186, 95, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10335.079415
validation loss after epoch 0 : 996.792748
	Epoch 1....
Epoch has taken 0:01:46.915317
Number of used sentences in train = 2074
Total loss for epoch 1: 6277.229356
validation loss after epoch 1 : 933.967274
	Epoch 2....
Epoch has taken 0:01:46.962783
Number of used sentences in train = 2074
Total loss for epoch 2: 4705.897441
validation loss after epoch 2 : 1061.346381
	Epoch 3....
Epoch has taken 0:01:47.000694
Number of used sentences in train = 2074
Total loss for epoch 3: 3855.817014
validation loss after epoch 3 : 1105.949778
	Epoch 4....
Epoch has taken 0:01:47.033341
Number of used sentences in train = 2074
Total loss for epoch 4: 3488.344344
validation loss after epoch 4 : 1197.518021
	Epoch 5....
Epoch has taken 0:01:46.940807
Number of used sentences in train = 2074
Total loss for epoch 5: 3321.286617
validation loss after epoch 5 : 1263.994536
	Epoch 6....
Epoch has taken 0:01:46.917645
Number of used sentences in train = 2074
Total loss for epoch 6: 3257.181336
validation loss after epoch 6 : 1287.783617
	Epoch 7....
Epoch has taken 0:01:46.879177
Number of used sentences in train = 2074
Total loss for epoch 7: 3214.994719
validation loss after epoch 7 : 1331.144301
	Epoch 8....
Epoch has taken 0:01:46.894986
Number of used sentences in train = 2074
Total loss for epoch 8: 3202.663278
validation loss after epoch 8 : 1345.691226
	Epoch 9....
Epoch has taken 0:01:46.877285
Number of used sentences in train = 2074
Total loss for epoch 9: 3193.500415
validation loss after epoch 9 : 1377.029437
	Epoch 10....
Epoch has taken 0:01:46.887250
Number of used sentences in train = 2074
Total loss for epoch 10: 3188.763658
validation loss after epoch 10 : 1395.154575
	Epoch 11....
Epoch has taken 0:01:46.939744
Number of used sentences in train = 2074
Total loss for epoch 11: 3185.758551
validation loss after epoch 11 : 1416.276412
	Epoch 12....
Epoch has taken 0:01:46.969409
Number of used sentences in train = 2074
Total loss for epoch 12: 3181.877838
validation loss after epoch 12 : 1423.917937
	Epoch 13....
Epoch has taken 0:01:46.906936
Number of used sentences in train = 2074
Total loss for epoch 13: 3179.623119
validation loss after epoch 13 : 1442.270309
	Epoch 14....
Epoch has taken 0:01:47.017486
Number of used sentences in train = 2074
Total loss for epoch 14: 3176.425395
validation loss after epoch 14 : 1462.516266
	TransitionClassifier(
  (p_embeddings): Embedding(18, 62)
  (w_embeddings): Embedding(7074, 124)
  (lstm): LSTM(186, 95, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:47.031860
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1863.521321
	Epoch 1....
Epoch has taken 0:00:10.898381
Number of used sentences in train = 231
Total loss for epoch 1: 672.967216
	Epoch 2....
Epoch has taken 0:00:10.882931
Number of used sentences in train = 231
Total loss for epoch 2: 484.692446
	Epoch 3....
Epoch has taken 0:00:10.896338
Number of used sentences in train = 231
Total loss for epoch 3: 399.820808
	Epoch 4....
Epoch has taken 0:00:10.888981
Number of used sentences in train = 231
Total loss for epoch 4: 365.753117
	Epoch 5....
Epoch has taken 0:00:10.893087
Number of used sentences in train = 231
Total loss for epoch 5: 356.549730
	Epoch 6....
Epoch has taken 0:00:10.890498
Number of used sentences in train = 231
Total loss for epoch 6: 353.553710
	Epoch 7....
Epoch has taken 0:00:10.897551
Number of used sentences in train = 231
Total loss for epoch 7: 351.414321
	Epoch 8....
Epoch has taken 0:00:10.888523
Number of used sentences in train = 231
Total loss for epoch 8: 349.753712
	Epoch 9....
Epoch has taken 0:00:10.893983
Number of used sentences in train = 231
Total loss for epoch 9: 348.678821
	Epoch 10....
Epoch has taken 0:00:10.892963
Number of used sentences in train = 231
Total loss for epoch 10: 348.357822
	Epoch 11....
Epoch has taken 0:00:10.888119
Number of used sentences in train = 231
Total loss for epoch 11: 347.804170
	Epoch 12....
Epoch has taken 0:00:10.900778
Number of used sentences in train = 231
Total loss for epoch 12: 347.411801
	Epoch 13....
Epoch has taken 0:00:10.892854
Number of used sentences in train = 231
Total loss for epoch 13: 347.111174
	Epoch 14....
Epoch has taken 0:00:10.889152
Number of used sentences in train = 231
Total loss for epoch 14: 346.885044
Epoch has taken 0:00:10.900799

==================================================================================================
	Training time : 0:29:27.927647
==================================================================================================
	Identification : 0.127

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 62)
  (w_embeddings): Embedding(18011, 124)
  (lstm): LSTM(186, 95, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18151.784266
validation loss after epoch 0 : 1615.230015
	Epoch 1....
Epoch has taken 0:03:28.388291
Number of used sentences in train = 3226
Total loss for epoch 1: 12239.688611
validation loss after epoch 1 : 1564.091671
	Epoch 2....
Epoch has taken 0:03:28.625160
Number of used sentences in train = 3226
Total loss for epoch 2: 9706.426224
validation loss after epoch 2 : 1622.279596
	Epoch 3....
Epoch has taken 0:03:29.103620
Number of used sentences in train = 3226
Total loss for epoch 3: 8030.583930
validation loss after epoch 3 : 1826.046047
	Epoch 4....
Epoch has taken 0:03:29.184697
Number of used sentences in train = 3226
Total loss for epoch 4: 7090.473993
validation loss after epoch 4 : 1971.277354
	Epoch 5....
Epoch has taken 0:03:29.302220
Number of used sentences in train = 3226
Total loss for epoch 5: 6634.440162
validation loss after epoch 5 : 2140.712289
	Epoch 6....
Epoch has taken 0:03:29.298590
Number of used sentences in train = 3226
Total loss for epoch 6: 6405.064441
validation loss after epoch 6 : 2216.067175
	Epoch 7....
Epoch has taken 0:03:29.154313
Number of used sentences in train = 3226
Total loss for epoch 7: 6296.222393
validation loss after epoch 7 : 2294.300074
	Epoch 8....
Epoch has taken 0:03:29.141633
Number of used sentences in train = 3226
Total loss for epoch 8: 6240.396065
validation loss after epoch 8 : 2373.403659
	Epoch 9....
Epoch has taken 0:03:29.223215
Number of used sentences in train = 3226
Total loss for epoch 9: 6204.870862
validation loss after epoch 9 : 2414.678523
	Epoch 10....
Epoch has taken 0:03:29.351023
Number of used sentences in train = 3226
Total loss for epoch 10: 6184.819170
validation loss after epoch 10 : 2502.977611
	Epoch 11....
Epoch has taken 0:03:28.973073
Number of used sentences in train = 3226
Total loss for epoch 11: 6169.934856
validation loss after epoch 11 : 2527.895203
	Epoch 12....
Epoch has taken 0:03:29.330757
Number of used sentences in train = 3226
Total loss for epoch 12: 6161.465519
validation loss after epoch 12 : 2575.346962
	Epoch 13....
Epoch has taken 0:03:29.163268
Number of used sentences in train = 3226
Total loss for epoch 13: 6152.927663
validation loss after epoch 13 : 2621.643487
	Epoch 14....
Epoch has taken 0:03:29.280723
Number of used sentences in train = 3226
Total loss for epoch 14: 6150.725844
validation loss after epoch 14 : 2616.985110
	TransitionClassifier(
  (p_embeddings): Embedding(13, 62)
  (w_embeddings): Embedding(18011, 124)
  (lstm): LSTM(186, 95, bidirectional=True)
  (linear1): Linear(in_features=1520, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:29.207144
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2214.103070
	Epoch 1....
Epoch has taken 0:00:20.372118
Number of used sentences in train = 359
Total loss for epoch 1: 1162.630454
	Epoch 2....
Epoch has taken 0:00:20.370685
Number of used sentences in train = 359
Total loss for epoch 2: 839.002624
	Epoch 3....
Epoch has taken 0:00:20.378775
Number of used sentences in train = 359
Total loss for epoch 3: 723.695833
	Epoch 4....
Epoch has taken 0:00:20.392211
Number of used sentences in train = 359
Total loss for epoch 4: 694.839894
	Epoch 5....
Epoch has taken 0:00:20.373018
Number of used sentences in train = 359
Total loss for epoch 5: 680.393736
	Epoch 6....
Epoch has taken 0:00:20.378368
Number of used sentences in train = 359
Total loss for epoch 6: 675.656994
	Epoch 7....
Epoch has taken 0:00:20.376791
Number of used sentences in train = 359
Total loss for epoch 7: 673.916682
	Epoch 8....
Epoch has taken 0:00:20.386065
Number of used sentences in train = 359
Total loss for epoch 8: 673.118286
	Epoch 9....
Epoch has taken 0:00:20.377752
Number of used sentences in train = 359
Total loss for epoch 9: 672.585131
	Epoch 10....
Epoch has taken 0:00:20.373033
Number of used sentences in train = 359
Total loss for epoch 10: 672.216765
	Epoch 11....
Epoch has taken 0:00:20.378896
Number of used sentences in train = 359
Total loss for epoch 11: 671.955451
	Epoch 12....
Epoch has taken 0:00:20.385106
Number of used sentences in train = 359
Total loss for epoch 12: 671.721863
	Epoch 13....
Epoch has taken 0:00:20.374211
Number of used sentences in train = 359
Total loss for epoch 13: 671.553481
	Epoch 14....
Epoch has taken 0:00:20.384402
Number of used sentences in train = 359
Total loss for epoch 14: 671.378199
Epoch has taken 0:00:20.388263

==================================================================================================
	Training time : 0:57:23.105989
==================================================================================================
	Identification : 0.345

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 105, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 30, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 45, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 151, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5919, 151)
  (lstm): LSTM(181, 45, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=105, bias=True)
  (linear2): Linear(in_features=105, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14879.148408
validation loss after epoch 0 : 1126.798272
	Epoch 1....
Epoch has taken 0:02:46.619062
Number of used sentences in train = 2811
Total loss for epoch 1: 8905.960728
validation loss after epoch 1 : 1078.659489
	Epoch 2....
Epoch has taken 0:02:46.637601
Number of used sentences in train = 2811
Total loss for epoch 2: 7297.415019
validation loss after epoch 2 : 1093.406570
	Epoch 3....
Epoch has taken 0:02:46.787786
Number of used sentences in train = 2811
Total loss for epoch 3: 6345.781921
validation loss after epoch 3 : 1142.187863
	Epoch 4....
Epoch has taken 0:02:46.790363
Number of used sentences in train = 2811
Total loss for epoch 4: 5856.974731
validation loss after epoch 4 : 1195.381486
	Epoch 5....
Epoch has taken 0:02:46.810338
Number of used sentences in train = 2811
Total loss for epoch 5: 5499.520387
validation loss after epoch 5 : 1330.706337
	Epoch 6....
Epoch has taken 0:02:46.677085
Number of used sentences in train = 2811
Total loss for epoch 6: 5185.679108
validation loss after epoch 6 : 1319.287710
	Epoch 7....
Epoch has taken 0:02:46.620442
Number of used sentences in train = 2811
Total loss for epoch 7: 5064.223072
validation loss after epoch 7 : 1301.510454
	Epoch 8....
Epoch has taken 0:02:46.730734
Number of used sentences in train = 2811
Total loss for epoch 8: 4875.021993
validation loss after epoch 8 : 1478.481771
	Epoch 9....
Epoch has taken 0:02:46.791359
Number of used sentences in train = 2811
Total loss for epoch 9: 4820.775864
validation loss after epoch 9 : 1384.256084
	Epoch 10....
Epoch has taken 0:02:46.700587
Number of used sentences in train = 2811
Total loss for epoch 10: 4771.081754
validation loss after epoch 10 : 1406.465771
	Epoch 11....
Epoch has taken 0:02:46.717398
Number of used sentences in train = 2811
Total loss for epoch 11: 4659.201046
validation loss after epoch 11 : 1545.299425
	Epoch 12....
Epoch has taken 0:02:46.788135
Number of used sentences in train = 2811
Total loss for epoch 12: 4620.793070
validation loss after epoch 12 : 1512.689409
	Epoch 13....
Epoch has taken 0:02:46.783246
Number of used sentences in train = 2811
Total loss for epoch 13: 4645.182846
validation loss after epoch 13 : 1574.325554
	Epoch 14....
Epoch has taken 0:02:46.814529
Number of used sentences in train = 2811
Total loss for epoch 14: 4601.651089
validation loss after epoch 14 : 1552.138893
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5919, 151)
  (lstm): LSTM(181, 45, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=105, bias=True)
  (linear2): Linear(in_features=105, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.770861
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2165.591350
	Epoch 1....
Epoch has taken 0:00:17.608162
Number of used sentences in train = 313
Total loss for epoch 1: 765.367130
	Epoch 2....
Epoch has taken 0:00:17.608385
Number of used sentences in train = 313
Total loss for epoch 2: 607.277455
	Epoch 3....
Epoch has taken 0:00:17.596272
Number of used sentences in train = 313
Total loss for epoch 3: 568.298033
	Epoch 4....
Epoch has taken 0:00:17.599659
Number of used sentences in train = 313
Total loss for epoch 4: 544.304062
	Epoch 5....
Epoch has taken 0:00:17.597010
Number of used sentences in train = 313
Total loss for epoch 5: 523.606467
	Epoch 6....
Epoch has taken 0:00:17.593139
Number of used sentences in train = 313
Total loss for epoch 6: 517.526130
	Epoch 7....
Epoch has taken 0:00:17.606777
Number of used sentences in train = 313
Total loss for epoch 7: 517.838572
	Epoch 8....
Epoch has taken 0:00:17.599924
Number of used sentences in train = 313
Total loss for epoch 8: 516.637830
	Epoch 9....
Epoch has taken 0:00:17.597748
Number of used sentences in train = 313
Total loss for epoch 9: 510.675816
	Epoch 10....
Epoch has taken 0:00:17.599268
Number of used sentences in train = 313
Total loss for epoch 10: 509.672130
	Epoch 11....
Epoch has taken 0:00:17.610980
Number of used sentences in train = 313
Total loss for epoch 11: 516.505292
	Epoch 12....
Epoch has taken 0:00:17.604777
Number of used sentences in train = 313
Total loss for epoch 12: 507.579816
	Epoch 13....
Epoch has taken 0:00:17.607554
Number of used sentences in train = 313
Total loss for epoch 13: 504.729967
	Epoch 14....
Epoch has taken 0:00:17.611890
Number of used sentences in train = 313
Total loss for epoch 14: 512.353190
Epoch has taken 0:00:17.604922

==================================================================================================
	Training time : 0:46:05.586880
==================================================================================================
	Identification : 0.117

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5571, 151)
  (lstm): LSTM(181, 45, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=105, bias=True)
  (linear2): Linear(in_features=105, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11470.376577
validation loss after epoch 0 : 965.938966
	Epoch 1....
Epoch has taken 0:01:54.112189
Number of used sentences in train = 2074
Total loss for epoch 1: 6538.991461
validation loss after epoch 1 : 958.355512
	Epoch 2....
Epoch has taken 0:01:53.909731
Number of used sentences in train = 2074
Total loss for epoch 2: 5170.313020
validation loss after epoch 2 : 1026.325045
	Epoch 3....
Epoch has taken 0:01:53.951122
Number of used sentences in train = 2074
Total loss for epoch 3: 4429.836722
validation loss after epoch 3 : 1065.982987
	Epoch 4....
Epoch has taken 0:01:54.106493
Number of used sentences in train = 2074
Total loss for epoch 4: 4035.019832
validation loss after epoch 4 : 1162.320180
	Epoch 5....
Epoch has taken 0:01:53.944033
Number of used sentences in train = 2074
Total loss for epoch 5: 3706.370660
validation loss after epoch 5 : 1279.314270
	Epoch 6....
Epoch has taken 0:01:54.101323
Number of used sentences in train = 2074
Total loss for epoch 6: 3593.599350
validation loss after epoch 6 : 1200.589588
	Epoch 7....
Epoch has taken 0:01:54.075761
Number of used sentences in train = 2074
Total loss for epoch 7: 3523.435317
validation loss after epoch 7 : 1272.992110
	Epoch 8....
Epoch has taken 0:01:54.102232
Number of used sentences in train = 2074
Total loss for epoch 8: 3443.909189
validation loss after epoch 8 : 1346.034034
	Epoch 9....
Epoch has taken 0:01:54.070658
Number of used sentences in train = 2074
Total loss for epoch 9: 3324.090630
validation loss after epoch 9 : 1454.426842
	Epoch 10....
Epoch has taken 0:01:54.110701
Number of used sentences in train = 2074
Total loss for epoch 10: 3288.388749
validation loss after epoch 10 : 1420.255555
	Epoch 11....
Epoch has taken 0:01:55.595123
Number of used sentences in train = 2074
Total loss for epoch 11: 3262.276341
validation loss after epoch 11 : 1448.404843
	Epoch 12....
Epoch has taken 0:01:54.064640
Number of used sentences in train = 2074
Total loss for epoch 12: 3258.989550
validation loss after epoch 12 : 1488.220502
	Epoch 13....
Epoch has taken 0:01:54.014137
Number of used sentences in train = 2074
Total loss for epoch 13: 3257.461279
validation loss after epoch 13 : 1523.510531
	Epoch 14....
Epoch has taken 0:01:54.022035
Number of used sentences in train = 2074
Total loss for epoch 14: 3249.649193
validation loss after epoch 14 : 1496.360047
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5571, 151)
  (lstm): LSTM(181, 45, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=105, bias=True)
  (linear2): Linear(in_features=105, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.047815
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2065.498211
	Epoch 1....
Epoch has taken 0:00:11.661212
Number of used sentences in train = 231
Total loss for epoch 1: 622.119255
	Epoch 2....
Epoch has taken 0:00:11.654432
Number of used sentences in train = 231
Total loss for epoch 2: 451.664565
	Epoch 3....
Epoch has taken 0:00:11.643596
Number of used sentences in train = 231
Total loss for epoch 3: 410.390176
	Epoch 4....
Epoch has taken 0:00:11.651314
Number of used sentences in train = 231
Total loss for epoch 4: 384.848829
	Epoch 5....
Epoch has taken 0:00:11.648877
Number of used sentences in train = 231
Total loss for epoch 5: 362.424736
	Epoch 6....
Epoch has taken 0:00:11.652917
Number of used sentences in train = 231
Total loss for epoch 6: 353.548267
	Epoch 7....
Epoch has taken 0:00:11.653936
Number of used sentences in train = 231
Total loss for epoch 7: 349.804827
	Epoch 8....
Epoch has taken 0:00:11.643082
Number of used sentences in train = 231
Total loss for epoch 8: 350.584851
	Epoch 9....
Epoch has taken 0:00:11.651000
Number of used sentences in train = 231
Total loss for epoch 9: 348.781004
	Epoch 10....
Epoch has taken 0:00:11.646830
Number of used sentences in train = 231
Total loss for epoch 10: 350.354643
	Epoch 11....
Epoch has taken 0:00:11.644930
Number of used sentences in train = 231
Total loss for epoch 11: 347.541458
	Epoch 12....
Epoch has taken 0:00:11.641412
Number of used sentences in train = 231
Total loss for epoch 12: 347.005724
	Epoch 13....
Epoch has taken 0:00:11.663832
Number of used sentences in train = 231
Total loss for epoch 13: 346.773708
	Epoch 14....
Epoch has taken 0:00:11.644855
Number of used sentences in train = 231
Total loss for epoch 14: 346.695947
Epoch has taken 0:00:11.653045

==================================================================================================
	Training time : 0:31:27.324977
==================================================================================================
	Identification : 0.267

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(6849, 151)
  (lstm): LSTM(181, 45, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=105, bias=True)
  (linear2): Linear(in_features=105, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17475.547364
validation loss after epoch 0 : 1502.137672
	Epoch 1....
Epoch has taken 0:03:48.427124
Number of used sentences in train = 3226
Total loss for epoch 1: 11781.149054
validation loss after epoch 1 : 1383.689478
	Epoch 2....
Epoch has taken 0:03:46.359453
Number of used sentences in train = 3226
Total loss for epoch 2: 9898.439099
validation loss after epoch 2 : 1481.745172
	Epoch 3....
Epoch has taken 0:03:46.104185
Number of used sentences in train = 3226
Total loss for epoch 3: 8858.048821
validation loss after epoch 3 : 1492.954493
	Epoch 4....
Epoch has taken 0:03:46.199298
Number of used sentences in train = 3226
Total loss for epoch 4: 8058.467501
validation loss after epoch 4 : 1610.940740
	Epoch 5....
Epoch has taken 0:03:46.167936
Number of used sentences in train = 3226
Total loss for epoch 5: 7637.572352
validation loss after epoch 5 : 1740.149007
	Epoch 6....
Epoch has taken 0:03:46.187040
Number of used sentences in train = 3226
Total loss for epoch 6: 7230.730917
validation loss after epoch 6 : 1786.625612
	Epoch 7....
Epoch has taken 0:03:46.134396
Number of used sentences in train = 3226
Total loss for epoch 7: 7066.930256
validation loss after epoch 7 : 1972.360509
	Epoch 8....
Epoch has taken 0:03:46.469574
Number of used sentences in train = 3226
Total loss for epoch 8: 6874.685699
validation loss after epoch 8 : 2010.290151
	Epoch 9....
Epoch has taken 0:03:46.346593
Number of used sentences in train = 3226
Total loss for epoch 9: 6702.457616
validation loss after epoch 9 : 2146.036170
	Epoch 10....
Epoch has taken 0:03:46.316015
Number of used sentences in train = 3226
Total loss for epoch 10: 6614.564737
validation loss after epoch 10 : 2197.065932
	Epoch 11....
Epoch has taken 0:03:46.652719
Number of used sentences in train = 3226
Total loss for epoch 11: 6601.724921
validation loss after epoch 11 : 2056.507071
	Epoch 12....
Epoch has taken 0:03:46.252779
Number of used sentences in train = 3226
Total loss for epoch 12: 6526.123905
validation loss after epoch 12 : 2177.943926
	Epoch 13....
Epoch has taken 0:03:46.246302
Number of used sentences in train = 3226
Total loss for epoch 13: 6431.379257
validation loss after epoch 13 : 2200.349333
	Epoch 14....
Epoch has taken 0:03:46.224894
Number of used sentences in train = 3226
Total loss for epoch 14: 6400.548095
validation loss after epoch 14 : 2189.666308
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(6849, 151)
  (lstm): LSTM(181, 45, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=105, bias=True)
  (linear2): Linear(in_features=105, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:46.126124
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2349.320927
	Epoch 1....
Epoch has taken 0:00:22.096795
Number of used sentences in train = 359
Total loss for epoch 1: 1143.410579
	Epoch 2....
Epoch has taken 0:00:22.081629
Number of used sentences in train = 359
Total loss for epoch 2: 893.731010
	Epoch 3....
Epoch has taken 0:00:22.097331
Number of used sentences in train = 359
Total loss for epoch 3: 782.422123
	Epoch 4....
Epoch has taken 0:00:22.083437
Number of used sentences in train = 359
Total loss for epoch 4: 741.142898
	Epoch 5....
Epoch has taken 0:00:22.087846
Number of used sentences in train = 359
Total loss for epoch 5: 710.866724
	Epoch 6....
Epoch has taken 0:00:22.097821
Number of used sentences in train = 359
Total loss for epoch 6: 717.710276
	Epoch 7....
Epoch has taken 0:00:22.087030
Number of used sentences in train = 359
Total loss for epoch 7: 689.791975
	Epoch 8....
Epoch has taken 0:00:22.095310
Number of used sentences in train = 359
Total loss for epoch 8: 679.179037
	Epoch 9....
Epoch has taken 0:00:22.092167
Number of used sentences in train = 359
Total loss for epoch 9: 683.555077
	Epoch 10....
Epoch has taken 0:00:22.107070
Number of used sentences in train = 359
Total loss for epoch 10: 677.523745
	Epoch 11....
Epoch has taken 0:00:22.089893
Number of used sentences in train = 359
Total loss for epoch 11: 678.690158
	Epoch 12....
Epoch has taken 0:00:22.093450
Number of used sentences in train = 359
Total loss for epoch 12: 673.432547
	Epoch 13....
Epoch has taken 0:00:22.085791
Number of used sentences in train = 359
Total loss for epoch 13: 675.667660
	Epoch 14....
Epoch has taken 0:00:22.096545
Number of used sentences in train = 359
Total loss for epoch 14: 673.198568
Epoch has taken 0:00:22.094897

==================================================================================================
	Training time : 1:02:08.257649
==================================================================================================
	Identification : 0.237

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 42, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 63, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 28, 'lstmDropout': 0.29, 'denseActivation': 'tanh', 'wordDim': 98, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1177, 98)
  (lstm): LSTM(161, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10639.802056
validation loss after epoch 0 : 938.740551
	Epoch 1....
Epoch has taken 0:02:35.318863
Number of used sentences in train = 2811
Total loss for epoch 1: 7931.578218
validation loss after epoch 1 : 906.611439
	Epoch 2....
Epoch has taken 0:02:35.412856
Number of used sentences in train = 2811
Total loss for epoch 2: 7057.454905
validation loss after epoch 2 : 905.773867
	Epoch 3....
Epoch has taken 0:02:35.075426
Number of used sentences in train = 2811
Total loss for epoch 3: 6464.321712
validation loss after epoch 3 : 952.106721
	Epoch 4....
Epoch has taken 0:02:35.220158
Number of used sentences in train = 2811
Total loss for epoch 4: 6071.100105
validation loss after epoch 4 : 985.937075
	Epoch 5....
Epoch has taken 0:02:35.051619
Number of used sentences in train = 2811
Total loss for epoch 5: 5773.946032
validation loss after epoch 5 : 1034.263125
	Epoch 6....
Epoch has taken 0:02:35.065511
Number of used sentences in train = 2811
Total loss for epoch 6: 5556.938054
validation loss after epoch 6 : 1066.003917
	Epoch 7....
Epoch has taken 0:02:35.009283
Number of used sentences in train = 2811
Total loss for epoch 7: 5355.719466
validation loss after epoch 7 : 1108.354638
	Epoch 8....
Epoch has taken 0:02:35.105284
Number of used sentences in train = 2811
Total loss for epoch 8: 5210.047019
validation loss after epoch 8 : 1115.019272
	Epoch 9....
Epoch has taken 0:02:35.056711
Number of used sentences in train = 2811
Total loss for epoch 9: 5086.360956
validation loss after epoch 9 : 1165.201941
	Epoch 10....
Epoch has taken 0:02:35.072029
Number of used sentences in train = 2811
Total loss for epoch 10: 4962.548555
validation loss after epoch 10 : 1188.712263
	Epoch 11....
Epoch has taken 0:02:35.002898
Number of used sentences in train = 2811
Total loss for epoch 11: 4888.893776
validation loss after epoch 11 : 1217.823711
	Epoch 12....
Epoch has taken 0:02:35.116341
Number of used sentences in train = 2811
Total loss for epoch 12: 4822.458881
validation loss after epoch 12 : 1236.292159
	Epoch 13....
Epoch has taken 0:02:35.203048
Number of used sentences in train = 2811
Total loss for epoch 13: 4778.048666
validation loss after epoch 13 : 1276.879542
	Epoch 14....
Epoch has taken 0:02:35.154323
Number of used sentences in train = 2811
Total loss for epoch 14: 4715.984796
validation loss after epoch 14 : 1317.292501
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1177, 98)
  (lstm): LSTM(161, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:35.019959
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1320.890197
	Epoch 1....
Epoch has taken 0:00:16.317236
Number of used sentences in train = 313
Total loss for epoch 1: 712.013032
	Epoch 2....
Epoch has taken 0:00:16.310571
Number of used sentences in train = 313
Total loss for epoch 2: 601.761163
	Epoch 3....
Epoch has taken 0:00:16.326927
Number of used sentences in train = 313
Total loss for epoch 3: 555.438081
	Epoch 4....
Epoch has taken 0:00:16.319780
Number of used sentences in train = 313
Total loss for epoch 4: 531.019031
	Epoch 5....
Epoch has taken 0:00:16.326787
Number of used sentences in train = 313
Total loss for epoch 5: 518.051136
	Epoch 6....
Epoch has taken 0:00:16.315302
Number of used sentences in train = 313
Total loss for epoch 6: 513.703971
	Epoch 7....
Epoch has taken 0:00:16.321277
Number of used sentences in train = 313
Total loss for epoch 7: 511.242913
	Epoch 8....
Epoch has taken 0:00:16.321659
Number of used sentences in train = 313
Total loss for epoch 8: 509.521947
	Epoch 9....
Epoch has taken 0:00:16.308713
Number of used sentences in train = 313
Total loss for epoch 9: 508.963012
	Epoch 10....
Epoch has taken 0:00:16.305257
Number of used sentences in train = 313
Total loss for epoch 10: 506.721593
	Epoch 11....
Epoch has taken 0:00:16.311219
Number of used sentences in train = 313
Total loss for epoch 11: 506.054343
	Epoch 12....
Epoch has taken 0:00:16.313278
Number of used sentences in train = 313
Total loss for epoch 12: 505.517821
	Epoch 13....
Epoch has taken 0:00:16.353531
Number of used sentences in train = 313
Total loss for epoch 13: 505.020507
	Epoch 14....
Epoch has taken 0:00:16.344550
Number of used sentences in train = 313
Total loss for epoch 14: 503.690059
Epoch has taken 0:00:16.341906

==================================================================================================
	Training time : 0:42:52.210447
==================================================================================================
	Identification : 0.327

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1133, 98)
  (lstm): LSTM(161, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8146.152057
validation loss after epoch 0 : 813.832010
	Epoch 1....
Epoch has taken 0:01:46.504386
Number of used sentences in train = 2074
Total loss for epoch 1: 5556.794861
validation loss after epoch 1 : 729.342821
	Epoch 2....
Epoch has taken 0:01:46.443739
Number of used sentences in train = 2074
Total loss for epoch 2: 4802.982045
validation loss after epoch 2 : 705.016855
	Epoch 3....
Epoch has taken 0:01:46.460322
Number of used sentences in train = 2074
Total loss for epoch 3: 4277.370288
validation loss after epoch 3 : 722.252995
	Epoch 4....
Epoch has taken 0:01:46.589820
Number of used sentences in train = 2074
Total loss for epoch 4: 3919.367967
validation loss after epoch 4 : 772.873403
	Epoch 5....
Epoch has taken 0:01:46.601883
Number of used sentences in train = 2074
Total loss for epoch 5: 3697.504310
validation loss after epoch 5 : 820.021456
	Epoch 6....
Epoch has taken 0:01:46.531682
Number of used sentences in train = 2074
Total loss for epoch 6: 3521.218857
validation loss after epoch 6 : 896.074614
	Epoch 7....
Epoch has taken 0:01:46.453397
Number of used sentences in train = 2074
Total loss for epoch 7: 3409.070339
validation loss after epoch 7 : 883.869977
	Epoch 8....
Epoch has taken 0:01:46.484641
Number of used sentences in train = 2074
Total loss for epoch 8: 3359.900543
validation loss after epoch 8 : 926.752378
	Epoch 9....
Epoch has taken 0:01:46.689800
Number of used sentences in train = 2074
Total loss for epoch 9: 3271.845236
validation loss after epoch 9 : 977.865830
	Epoch 10....
Epoch has taken 0:01:46.547038
Number of used sentences in train = 2074
Total loss for epoch 10: 3245.916561
validation loss after epoch 10 : 983.068683
	Epoch 11....
Epoch has taken 0:01:46.525970
Number of used sentences in train = 2074
Total loss for epoch 11: 3214.276490
validation loss after epoch 11 : 1012.251370
	Epoch 12....
Epoch has taken 0:01:46.521692
Number of used sentences in train = 2074
Total loss for epoch 12: 3203.104311
validation loss after epoch 12 : 1011.476512
	Epoch 13....
Epoch has taken 0:01:46.589873
Number of used sentences in train = 2074
Total loss for epoch 13: 3193.254804
validation loss after epoch 13 : 1035.239738
	Epoch 14....
Epoch has taken 0:01:46.643382
Number of used sentences in train = 2074
Total loss for epoch 14: 3184.140696
validation loss after epoch 14 : 1042.890217
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1133, 98)
  (lstm): LSTM(161, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:46.708258
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1315.620649
	Epoch 1....
Epoch has taken 0:00:10.861969
Number of used sentences in train = 231
Total loss for epoch 1: 612.934629
	Epoch 2....
Epoch has taken 0:00:10.856903
Number of used sentences in train = 231
Total loss for epoch 2: 458.876081
	Epoch 3....
Epoch has taken 0:00:10.857177
Number of used sentences in train = 231
Total loss for epoch 3: 401.319081
	Epoch 4....
Epoch has taken 0:00:10.856590
Number of used sentences in train = 231
Total loss for epoch 4: 375.946846
	Epoch 5....
Epoch has taken 0:00:10.847407
Number of used sentences in train = 231
Total loss for epoch 5: 365.009188
	Epoch 6....
Epoch has taken 0:00:10.849436
Number of used sentences in train = 231
Total loss for epoch 6: 356.762475
	Epoch 7....
Epoch has taken 0:00:10.841966
Number of used sentences in train = 231
Total loss for epoch 7: 352.771196
	Epoch 8....
Epoch has taken 0:00:10.857337
Number of used sentences in train = 231
Total loss for epoch 8: 350.345502
	Epoch 9....
Epoch has taken 0:00:10.846396
Number of used sentences in train = 231
Total loss for epoch 9: 350.003242
	Epoch 10....
Epoch has taken 0:00:10.848717
Number of used sentences in train = 231
Total loss for epoch 10: 348.870685
	Epoch 11....
Epoch has taken 0:00:10.842329
Number of used sentences in train = 231
Total loss for epoch 11: 347.659073
	Epoch 12....
Epoch has taken 0:00:10.848636
Number of used sentences in train = 231
Total loss for epoch 12: 347.127580
	Epoch 13....
Epoch has taken 0:00:10.852075
Number of used sentences in train = 231
Total loss for epoch 13: 346.804816
	Epoch 14....
Epoch has taken 0:00:10.861640
Number of used sentences in train = 231
Total loss for epoch 14: 346.497129
Epoch has taken 0:00:10.858430

==================================================================================================
	Training time : 0:29:21.413632
==================================================================================================
	Identification : 0.307

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 63)
  (w_embeddings): Embedding(1202, 98)
  (lstm): LSTM(161, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14743.115258
validation loss after epoch 0 : 1371.584481
	Epoch 1....
Epoch has taken 0:03:26.955646
Number of used sentences in train = 3226
Total loss for epoch 1: 11786.844173
validation loss after epoch 1 : 1302.655023
	Epoch 2....
Epoch has taken 0:03:26.818190
Number of used sentences in train = 3226
Total loss for epoch 2: 10818.030583
validation loss after epoch 2 : 1339.516870
	Epoch 3....
Epoch has taken 0:03:26.867949
Number of used sentences in train = 3226
Total loss for epoch 3: 10185.386195
validation loss after epoch 3 : 1335.698261
	Epoch 4....
Epoch has taken 0:03:27.394896
Number of used sentences in train = 3226
Total loss for epoch 4: 9641.935676
validation loss after epoch 4 : 1350.180087
	Epoch 5....
Epoch has taken 0:03:27.079410
Number of used sentences in train = 3226
Total loss for epoch 5: 9244.682333
validation loss after epoch 5 : 1373.160304
	Epoch 6....
Epoch has taken 0:03:27.017371
Number of used sentences in train = 3226
Total loss for epoch 6: 8933.261856
validation loss after epoch 6 : 1361.448328
	Epoch 7....
Epoch has taken 0:03:27.135496
Number of used sentences in train = 3226
Total loss for epoch 7: 8635.247857
validation loss after epoch 7 : 1416.689327
	Epoch 8....
Epoch has taken 0:03:27.305821
Number of used sentences in train = 3226
Total loss for epoch 8: 8366.854928
validation loss after epoch 8 : 1468.749230
	Epoch 9....
Epoch has taken 0:03:27.347070
Number of used sentences in train = 3226
Total loss for epoch 9: 8164.732592
validation loss after epoch 9 : 1501.346338
	Epoch 10....
Epoch has taken 0:03:40.823097
Number of used sentences in train = 3226
Total loss for epoch 10: 8001.568209
validation loss after epoch 10 : 1494.597980
	Epoch 11....
Epoch has taken 0:03:27.080891
Number of used sentences in train = 3226
Total loss for epoch 11: 7818.511449
validation loss after epoch 11 : 1589.642219
	Epoch 12....
Epoch has taken 0:03:27.344251
Number of used sentences in train = 3226
Total loss for epoch 12: 7679.562283
validation loss after epoch 12 : 1610.834582
	Epoch 13....
Epoch has taken 0:03:27.399015
Number of used sentences in train = 3226
Total loss for epoch 13: 7539.141823
validation loss after epoch 13 : 1627.495455
	Epoch 14....
Epoch has taken 0:03:27.372822
Number of used sentences in train = 3226
Total loss for epoch 14: 7434.710603
validation loss after epoch 14 : 1656.232669
	TransitionClassifier(
  (p_embeddings): Embedding(13, 63)
  (w_embeddings): Embedding(1202, 98)
  (lstm): LSTM(161, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:27.317426
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2061.574842
	Epoch 1....
Epoch has taken 0:00:20.196748
Number of used sentences in train = 359
Total loss for epoch 1: 1149.555087
	Epoch 2....
Epoch has taken 0:00:20.180872
Number of used sentences in train = 359
Total loss for epoch 2: 1008.848971
	Epoch 3....
Epoch has taken 0:00:20.175906
Number of used sentences in train = 359
Total loss for epoch 3: 916.507654
	Epoch 4....
Epoch has taken 0:00:20.165445
Number of used sentences in train = 359
Total loss for epoch 4: 852.470111
	Epoch 5....
Epoch has taken 0:00:20.177406
Number of used sentences in train = 359
Total loss for epoch 5: 806.941585
	Epoch 6....
Epoch has taken 0:00:20.189144
Number of used sentences in train = 359
Total loss for epoch 6: 775.110958
	Epoch 7....
Epoch has taken 0:00:20.180737
Number of used sentences in train = 359
Total loss for epoch 7: 754.104950
	Epoch 8....
Epoch has taken 0:00:20.173686
Number of used sentences in train = 359
Total loss for epoch 8: 737.549942
	Epoch 9....
Epoch has taken 0:00:20.164967
Number of used sentences in train = 359
Total loss for epoch 9: 724.527059
	Epoch 10....
Epoch has taken 0:00:20.176034
Number of used sentences in train = 359
Total loss for epoch 10: 712.054787
	Epoch 11....
Epoch has taken 0:00:20.173043
Number of used sentences in train = 359
Total loss for epoch 11: 706.153679
	Epoch 12....
Epoch has taken 0:00:20.178451
Number of used sentences in train = 359
Total loss for epoch 12: 701.191120
	Epoch 13....
Epoch has taken 0:00:20.170225
Number of used sentences in train = 359
Total loss for epoch 13: 695.000319
	Epoch 14....
Epoch has taken 0:00:20.179015
Number of used sentences in train = 359
Total loss for epoch 14: 688.494036
Epoch has taken 0:00:20.173240

==================================================================================================
	Training time : 0:57:04.564618
==================================================================================================
	Identification : 0.303

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 25, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 70, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 86, 'lstmDropout': 0.18, 'denseActivation': 'tanh', 'wordDim': 160, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(1882, 160)
  (lstm): LSTM(230, 86, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10893.061884
validation loss after epoch 0 : 884.175723
	Epoch 1....
Epoch has taken 0:02:47.193523
Number of used sentences in train = 2811
Total loss for epoch 1: 7627.315714
validation loss after epoch 1 : 791.956480
	Epoch 2....
Epoch has taken 0:02:47.232608
Number of used sentences in train = 2811
Total loss for epoch 2: 6808.151202
validation loss after epoch 2 : 797.693004
	Epoch 3....
Epoch has taken 0:02:47.409438
Number of used sentences in train = 2811
Total loss for epoch 3: 6227.732193
validation loss after epoch 3 : 789.331437
	Epoch 4....
Epoch has taken 0:02:47.364412
Number of used sentences in train = 2811
Total loss for epoch 4: 5766.922371
validation loss after epoch 4 : 816.655873
	Epoch 5....
Epoch has taken 0:02:47.321174
Number of used sentences in train = 2811
Total loss for epoch 5: 5527.819823
validation loss after epoch 5 : 843.119888
	Epoch 6....
Epoch has taken 0:02:47.107914
Number of used sentences in train = 2811
Total loss for epoch 6: 5288.483630
validation loss after epoch 6 : 862.713955
	Epoch 7....
Epoch has taken 0:02:47.463961
Number of used sentences in train = 2811
Total loss for epoch 7: 5119.352168
validation loss after epoch 7 : 872.557534
	Epoch 8....
Epoch has taken 0:02:47.467316
Number of used sentences in train = 2811
Total loss for epoch 8: 5021.791137
validation loss after epoch 8 : 927.280811
	Epoch 9....
Epoch has taken 0:02:47.504470
Number of used sentences in train = 2811
Total loss for epoch 9: 4942.923017
validation loss after epoch 9 : 945.824087
	Epoch 10....
Epoch has taken 0:02:47.345157
Number of used sentences in train = 2811
Total loss for epoch 10: 4825.977475
validation loss after epoch 10 : 951.937125
	Epoch 11....
Epoch has taken 0:02:47.445106
Number of used sentences in train = 2811
Total loss for epoch 11: 4751.248747
validation loss after epoch 11 : 1026.816665
	Epoch 12....
Epoch has taken 0:02:48.186356
Number of used sentences in train = 2811
Total loss for epoch 12: 4724.383748
validation loss after epoch 12 : 985.317356
	Epoch 13....
Epoch has taken 0:02:49.072845
Number of used sentences in train = 2811
Total loss for epoch 13: 4707.051848
validation loss after epoch 13 : 960.074209
	Epoch 14....
Epoch has taken 0:02:47.832879
Number of used sentences in train = 2811
Total loss for epoch 14: 4682.525045
validation loss after epoch 14 : 997.739736
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(1882, 160)
  (lstm): LSTM(230, 86, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.510852
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1293.644752
	Epoch 1....
Epoch has taken 0:00:17.699370
Number of used sentences in train = 313
Total loss for epoch 1: 831.021692
	Epoch 2....
Epoch has taken 0:00:17.689919
Number of used sentences in train = 313
Total loss for epoch 2: 682.320744
	Epoch 3....
Epoch has taken 0:00:17.683397
Number of used sentences in train = 313
Total loss for epoch 3: 638.104411
	Epoch 4....
Epoch has taken 0:00:17.685990
Number of used sentences in train = 313
Total loss for epoch 4: 581.948111
	Epoch 5....
Epoch has taken 0:00:17.683106
Number of used sentences in train = 313
Total loss for epoch 5: 568.143309
	Epoch 6....
Epoch has taken 0:00:17.676758
Number of used sentences in train = 313
Total loss for epoch 6: 550.650502
	Epoch 7....
Epoch has taken 0:00:17.689267
Number of used sentences in train = 313
Total loss for epoch 7: 542.918850
	Epoch 8....
Epoch has taken 0:00:17.691372
Number of used sentences in train = 313
Total loss for epoch 8: 539.785679
	Epoch 9....
Epoch has taken 0:00:17.690291
Number of used sentences in train = 313
Total loss for epoch 9: 532.438253
	Epoch 10....
Epoch has taken 0:00:17.686332
Number of used sentences in train = 313
Total loss for epoch 10: 533.552370
	Epoch 11....
Epoch has taken 0:00:17.696022
Number of used sentences in train = 313
Total loss for epoch 11: 532.411865
	Epoch 12....
Epoch has taken 0:00:17.688825
Number of used sentences in train = 313
Total loss for epoch 12: 529.610031
	Epoch 13....
Epoch has taken 0:00:17.688579
Number of used sentences in train = 313
Total loss for epoch 13: 529.878937
	Epoch 14....
Epoch has taken 0:00:17.698070
Number of used sentences in train = 313
Total loss for epoch 14: 528.553759
Epoch has taken 0:00:17.691078

==================================================================================================
	Training time : 0:46:19.295763
==================================================================================================
	Identification : 0.058

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(1680, 160)
  (lstm): LSTM(230, 86, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10160.191554
validation loss after epoch 0 : 731.687382
	Epoch 1....
Epoch has taken 0:01:54.630843
Number of used sentences in train = 2074
Total loss for epoch 1: 5723.801670
validation loss after epoch 1 : 626.871810
	Epoch 2....
Epoch has taken 0:01:54.615407
Number of used sentences in train = 2074
Total loss for epoch 2: 4844.634763
validation loss after epoch 2 : 614.742950
	Epoch 3....
Epoch has taken 0:01:54.673812
Number of used sentences in train = 2074
Total loss for epoch 3: 4319.149890
validation loss after epoch 3 : 616.390044
	Epoch 4....
Epoch has taken 0:01:54.677334
Number of used sentences in train = 2074
Total loss for epoch 4: 3999.140751
validation loss after epoch 4 : 685.173155
	Epoch 5....
Epoch has taken 0:01:54.736487
Number of used sentences in train = 2074
Total loss for epoch 5: 3794.746943
validation loss after epoch 5 : 744.856518
	Epoch 6....
Epoch has taken 0:01:54.842524
Number of used sentences in train = 2074
Total loss for epoch 6: 3670.881593
validation loss after epoch 6 : 675.158176
	Epoch 7....
Epoch has taken 0:01:54.747427
Number of used sentences in train = 2074
Total loss for epoch 7: 3534.296122
validation loss after epoch 7 : 732.579723
	Epoch 8....
Epoch has taken 0:01:54.885210
Number of used sentences in train = 2074
Total loss for epoch 8: 3441.372238
validation loss after epoch 8 : 754.803366
	Epoch 9....
Epoch has taken 0:01:54.913272
Number of used sentences in train = 2074
Total loss for epoch 9: 3370.845096
validation loss after epoch 9 : 748.894405
	Epoch 10....
Epoch has taken 0:01:54.723240
Number of used sentences in train = 2074
Total loss for epoch 10: 3329.545302
validation loss after epoch 10 : 772.548309
	Epoch 11....
Epoch has taken 0:01:54.930366
Number of used sentences in train = 2074
Total loss for epoch 11: 3291.348500
validation loss after epoch 11 : 783.304525
	Epoch 12....
Epoch has taken 0:01:54.843977
Number of used sentences in train = 2074
Total loss for epoch 12: 3260.038433
validation loss after epoch 12 : 782.470843
	Epoch 13....
Epoch has taken 0:01:54.827854
Number of used sentences in train = 2074
Total loss for epoch 13: 3241.652849
validation loss after epoch 13 : 834.936882
	Epoch 14....
Epoch has taken 0:01:54.868001
Number of used sentences in train = 2074
Total loss for epoch 14: 3238.964125
validation loss after epoch 14 : 807.568777
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(1680, 160)
  (lstm): LSTM(230, 86, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.837916
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1248.854364
	Epoch 1....
Epoch has taken 0:00:11.711523
Number of used sentences in train = 231
Total loss for epoch 1: 682.339718
	Epoch 2....
Epoch has taken 0:00:11.710875
Number of used sentences in train = 231
Total loss for epoch 2: 518.810990
	Epoch 3....
Epoch has taken 0:00:11.718998
Number of used sentences in train = 231
Total loss for epoch 3: 457.908340
	Epoch 4....
Epoch has taken 0:00:11.706898
Number of used sentences in train = 231
Total loss for epoch 4: 395.030699
	Epoch 5....
Epoch has taken 0:00:11.709814
Number of used sentences in train = 231
Total loss for epoch 5: 382.511874
	Epoch 6....
Epoch has taken 0:00:11.706473
Number of used sentences in train = 231
Total loss for epoch 6: 360.499262
	Epoch 7....
Epoch has taken 0:00:11.712265
Number of used sentences in train = 231
Total loss for epoch 7: 360.844744
	Epoch 8....
Epoch has taken 0:00:11.712793
Number of used sentences in train = 231
Total loss for epoch 8: 358.228222
	Epoch 9....
Epoch has taken 0:00:11.709667
Number of used sentences in train = 231
Total loss for epoch 9: 353.919500
	Epoch 10....
Epoch has taken 0:00:11.716738
Number of used sentences in train = 231
Total loss for epoch 10: 353.924628
	Epoch 11....
Epoch has taken 0:00:11.711914
Number of used sentences in train = 231
Total loss for epoch 11: 351.297641
	Epoch 12....
Epoch has taken 0:00:11.714292
Number of used sentences in train = 231
Total loss for epoch 12: 350.588631
	Epoch 13....
Epoch has taken 0:00:11.707492
Number of used sentences in train = 231
Total loss for epoch 13: 353.391293
	Epoch 14....
Epoch has taken 0:00:11.711735
Number of used sentences in train = 231
Total loss for epoch 14: 349.432440
Epoch has taken 0:00:11.708188

==================================================================================================
	Training time : 0:31:37.759756
==================================================================================================
	Identification : 0.159

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 70)
  (w_embeddings): Embedding(3369, 160)
  (lstm): LSTM(230, 86, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20560.461404
validation loss after epoch 0 : 1127.052132
	Epoch 1....
Epoch has taken 0:03:42.100816
Number of used sentences in train = 3226
Total loss for epoch 1: 10040.429906
validation loss after epoch 1 : 1071.070489
	Epoch 2....
Epoch has taken 0:03:42.169092
Number of used sentences in train = 3226
Total loss for epoch 2: 9098.661940
validation loss after epoch 2 : 1073.946396
	Epoch 3....
Epoch has taken 0:03:42.191901
Number of used sentences in train = 3226
Total loss for epoch 3: 8417.481544
validation loss after epoch 3 : 1073.818041
	Epoch 4....
Epoch has taken 0:03:42.206472
Number of used sentences in train = 3226
Total loss for epoch 4: 8049.409844
validation loss after epoch 4 : 1106.904699
	Epoch 5....
Epoch has taken 0:03:42.221188
Number of used sentences in train = 3226
Total loss for epoch 5: 7680.156271
validation loss after epoch 5 : 1099.324402
	Epoch 6....
Epoch has taken 0:03:42.100042
Number of used sentences in train = 3226
Total loss for epoch 6: 7380.612820
validation loss after epoch 6 : 1195.983830
	Epoch 7....
Epoch has taken 0:03:42.175903
Number of used sentences in train = 3226
Total loss for epoch 7: 7154.537439
validation loss after epoch 7 : 1190.744103
	Epoch 8....
Epoch has taken 0:04:06.886409
Number of used sentences in train = 3226
Total loss for epoch 8: 6976.706294
validation loss after epoch 8 : 1223.743602
	Epoch 9....
Epoch has taken 0:03:42.352571
Number of used sentences in train = 3226
Total loss for epoch 9: 6894.450301
validation loss after epoch 9 : 1223.542727
	Epoch 10....
Epoch has taken 0:03:42.196769
Number of used sentences in train = 3226
Total loss for epoch 10: 6738.758954
validation loss after epoch 10 : 1310.686615
	Epoch 11....
Epoch has taken 0:03:42.294505
Number of used sentences in train = 3226
Total loss for epoch 11: 6691.506863
validation loss after epoch 11 : 1356.795546
	Epoch 12....
Epoch has taken 0:03:42.189285
Number of used sentences in train = 3226
Total loss for epoch 12: 6588.456945
validation loss after epoch 12 : 1313.183903
	Epoch 13....
Epoch has taken 0:03:42.825408
Number of used sentences in train = 3226
Total loss for epoch 13: 6524.709833
validation loss after epoch 13 : 1403.037247
	Epoch 14....
Epoch has taken 0:03:42.066217
Number of used sentences in train = 3226
Total loss for epoch 14: 6497.338562
validation loss after epoch 14 : 1414.994838
	TransitionClassifier(
  (p_embeddings): Embedding(13, 70)
  (w_embeddings): Embedding(3369, 160)
  (lstm): LSTM(230, 86, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=1376, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:42.259406
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1554.779024
	Epoch 1....
Epoch has taken 0:00:21.753827
Number of used sentences in train = 359
Total loss for epoch 1: 1036.374987
	Epoch 2....
Epoch has taken 0:00:21.761949
Number of used sentences in train = 359
Total loss for epoch 2: 888.884477
	Epoch 3....
Epoch has taken 0:00:21.753483
Number of used sentences in train = 359
Total loss for epoch 3: 802.218472
	Epoch 4....
Epoch has taken 0:00:21.755638
Number of used sentences in train = 359
Total loss for epoch 4: 758.612117
	Epoch 5....
Epoch has taken 0:00:21.761338
Number of used sentences in train = 359
Total loss for epoch 5: 727.359462
	Epoch 6....
Epoch has taken 0:00:21.757571
Number of used sentences in train = 359
Total loss for epoch 6: 720.475171
	Epoch 7....
Epoch has taken 0:00:21.755460
Number of used sentences in train = 359
Total loss for epoch 7: 714.940452
	Epoch 8....
Epoch has taken 0:00:21.765907
Number of used sentences in train = 359
Total loss for epoch 8: 697.239714
	Epoch 9....
Epoch has taken 0:00:21.771392
Number of used sentences in train = 359
Total loss for epoch 9: 704.653341
	Epoch 10....
Epoch has taken 0:00:21.756810
Number of used sentences in train = 359
Total loss for epoch 10: 692.980957
	Epoch 11....
Epoch has taken 0:00:21.757251
Number of used sentences in train = 359
Total loss for epoch 11: 683.377139
	Epoch 12....
Epoch has taken 0:00:21.761709
Number of used sentences in train = 359
Total loss for epoch 12: 677.944573
	Epoch 13....
Epoch has taken 0:00:21.759637
Number of used sentences in train = 359
Total loss for epoch 13: 674.717057
	Epoch 14....
Epoch has taken 0:00:21.748789
Number of used sentences in train = 359
Total loss for epoch 14: 681.812754
Epoch has taken 0:00:21.750546

==================================================================================================
	Training time : 1:01:25.274378
==================================================================================================
	Identification : 0.198

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 35, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 26, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 52, 'lstmDropout': 0.18, 'denseActivation': 'tanh', 'wordDim': 146, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1882, 146)
  (lstm): LSTM(172, 52, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10792.172614
validation loss after epoch 0 : 981.686462
	Epoch 1....
Epoch has taken 0:02:47.005405
Number of used sentences in train = 2811
Total loss for epoch 1: 7302.328740
validation loss after epoch 1 : 783.947442
	Epoch 2....
Epoch has taken 0:02:46.952624
Number of used sentences in train = 2811
Total loss for epoch 2: 6443.100568
validation loss after epoch 2 : 796.135557
	Epoch 3....
Epoch has taken 0:02:47.062394
Number of used sentences in train = 2811
Total loss for epoch 3: 5881.372680
validation loss after epoch 3 : 815.355138
	Epoch 4....
Epoch has taken 0:02:47.151194
Number of used sentences in train = 2811
Total loss for epoch 4: 5455.801963
validation loss after epoch 4 : 851.479210
	Epoch 5....
Epoch has taken 0:02:47.099259
Number of used sentences in train = 2811
Total loss for epoch 5: 5196.500657
validation loss after epoch 5 : 929.885083
	Epoch 6....
Epoch has taken 0:02:46.988480
Number of used sentences in train = 2811
Total loss for epoch 6: 5103.679665
validation loss after epoch 6 : 940.742351
	Epoch 7....
Epoch has taken 0:02:47.084614
Number of used sentences in train = 2811
Total loss for epoch 7: 4938.317326
validation loss after epoch 7 : 965.587551
	Epoch 8....
Epoch has taken 0:02:47.177383
Number of used sentences in train = 2811
Total loss for epoch 8: 4852.644183
validation loss after epoch 8 : 947.992492
	Epoch 9....
Epoch has taken 0:02:47.001880
Number of used sentences in train = 2811
Total loss for epoch 9: 4763.100343
validation loss after epoch 9 : 966.802628
	Epoch 10....
Epoch has taken 0:02:47.019877
Number of used sentences in train = 2811
Total loss for epoch 10: 4715.580568
validation loss after epoch 10 : 1002.305685
	Epoch 11....
Epoch has taken 0:02:47.078036
Number of used sentences in train = 2811
Total loss for epoch 11: 4699.154779
validation loss after epoch 11 : 1012.766258
	Epoch 12....
Epoch has taken 0:02:47.086897
Number of used sentences in train = 2811
Total loss for epoch 12: 4644.837180
validation loss after epoch 12 : 1058.900344
	Epoch 13....
Epoch has taken 0:02:47.159745
Number of used sentences in train = 2811
Total loss for epoch 13: 4614.698732
validation loss after epoch 13 : 1022.163138
	Epoch 14....
Epoch has taken 0:02:47.050951
Number of used sentences in train = 2811
Total loss for epoch 14: 4588.526905
validation loss after epoch 14 : 1070.566182
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1882, 146)
  (lstm): LSTM(172, 52, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.280115
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1180.780759
	Epoch 1....
Epoch has taken 0:00:17.704213
Number of used sentences in train = 313
Total loss for epoch 1: 690.638800
	Epoch 2....
Epoch has taken 0:00:17.692750
Number of used sentences in train = 313
Total loss for epoch 2: 576.156841
	Epoch 3....
Epoch has taken 0:00:17.691413
Number of used sentences in train = 313
Total loss for epoch 3: 544.557172
	Epoch 4....
Epoch has taken 0:00:17.692368
Number of used sentences in train = 313
Total loss for epoch 4: 536.421939
	Epoch 5....
Epoch has taken 0:00:17.719603
Number of used sentences in train = 313
Total loss for epoch 5: 522.485141
	Epoch 6....
Epoch has taken 0:00:17.705190
Number of used sentences in train = 313
Total loss for epoch 6: 516.252988
	Epoch 7....
Epoch has taken 0:00:17.712244
Number of used sentences in train = 313
Total loss for epoch 7: 515.008803
	Epoch 8....
Epoch has taken 0:00:17.700895
Number of used sentences in train = 313
Total loss for epoch 8: 518.147342
	Epoch 9....
Epoch has taken 0:00:17.712143
Number of used sentences in train = 313
Total loss for epoch 9: 515.763626
	Epoch 10....
Epoch has taken 0:00:17.713092
Number of used sentences in train = 313
Total loss for epoch 10: 513.207082
	Epoch 11....
Epoch has taken 0:00:17.717348
Number of used sentences in train = 313
Total loss for epoch 11: 511.559942
	Epoch 12....
Epoch has taken 0:00:17.703249
Number of used sentences in train = 313
Total loss for epoch 12: 510.325245
	Epoch 13....
Epoch has taken 0:00:17.679279
Number of used sentences in train = 313
Total loss for epoch 13: 514.374454
	Epoch 14....
Epoch has taken 0:00:17.669576
Number of used sentences in train = 313
Total loss for epoch 14: 512.280957
Epoch has taken 0:00:17.672342

==================================================================================================
	Training time : 0:46:12.182158
==================================================================================================
	Identification : 0.174

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1680, 146)
  (lstm): LSTM(172, 52, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8434.191326
validation loss after epoch 0 : 668.665213
	Epoch 1....
Epoch has taken 0:01:54.231250
Number of used sentences in train = 2074
Total loss for epoch 1: 5308.051740
validation loss after epoch 1 : 665.634903
	Epoch 2....
Epoch has taken 0:01:54.344523
Number of used sentences in train = 2074
Total loss for epoch 2: 4537.684183
validation loss after epoch 2 : 689.463269
	Epoch 3....
Epoch has taken 0:01:54.298043
Number of used sentences in train = 2074
Total loss for epoch 3: 4003.567381
validation loss after epoch 3 : 688.385876
	Epoch 4....
Epoch has taken 0:01:54.244925
Number of used sentences in train = 2074
Total loss for epoch 4: 3702.806416
validation loss after epoch 4 : 762.272828
	Epoch 5....
Epoch has taken 0:01:54.367452
Number of used sentences in train = 2074
Total loss for epoch 5: 3523.225874
validation loss after epoch 5 : 790.592474
	Epoch 6....
Epoch has taken 0:01:54.332995
Number of used sentences in train = 2074
Total loss for epoch 6: 3389.971591
validation loss after epoch 6 : 843.843669
	Epoch 7....
Epoch has taken 0:01:54.224846
Number of used sentences in train = 2074
Total loss for epoch 7: 3333.315972
validation loss after epoch 7 : 866.113181
	Epoch 8....
Epoch has taken 0:01:54.305379
Number of used sentences in train = 2074
Total loss for epoch 8: 3298.705680
validation loss after epoch 8 : 838.247063
	Epoch 9....
Epoch has taken 0:01:54.252097
Number of used sentences in train = 2074
Total loss for epoch 9: 3269.380543
validation loss after epoch 9 : 897.367214
	Epoch 10....
Epoch has taken 0:01:54.303946
Number of used sentences in train = 2074
Total loss for epoch 10: 3246.953834
validation loss after epoch 10 : 897.482132
	Epoch 11....
Epoch has taken 0:01:54.237640
Number of used sentences in train = 2074
Total loss for epoch 11: 3235.826258
validation loss after epoch 11 : 909.811969
	Epoch 12....
Epoch has taken 0:01:54.228836
Number of used sentences in train = 2074
Total loss for epoch 12: 3222.139241
validation loss after epoch 12 : 929.241267
	Epoch 13....
Epoch has taken 0:01:54.186147
Number of used sentences in train = 2074
Total loss for epoch 13: 3212.901649
validation loss after epoch 13 : 946.637850
	Epoch 14....
Epoch has taken 0:01:54.197779
Number of used sentences in train = 2074
Total loss for epoch 14: 3188.458006
validation loss after epoch 14 : 978.819211
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1680, 146)
  (lstm): LSTM(172, 52, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.349305
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1191.660662
	Epoch 1....
Epoch has taken 0:00:11.673188
Number of used sentences in train = 231
Total loss for epoch 1: 506.955293
	Epoch 2....
Epoch has taken 0:00:11.673387
Number of used sentences in train = 231
Total loss for epoch 2: 409.315673
	Epoch 3....
Epoch has taken 0:00:11.675657
Number of used sentences in train = 231
Total loss for epoch 3: 367.946819
	Epoch 4....
Epoch has taken 0:00:11.675429
Number of used sentences in train = 231
Total loss for epoch 4: 354.871733
	Epoch 5....
Epoch has taken 0:00:11.685501
Number of used sentences in train = 231
Total loss for epoch 5: 350.197923
	Epoch 6....
Epoch has taken 0:00:11.666737
Number of used sentences in train = 231
Total loss for epoch 6: 348.956211
	Epoch 7....
Epoch has taken 0:00:11.677397
Number of used sentences in train = 231
Total loss for epoch 7: 348.268364
	Epoch 8....
Epoch has taken 0:00:11.669692
Number of used sentences in train = 231
Total loss for epoch 8: 347.419797
	Epoch 9....
Epoch has taken 0:00:11.676232
Number of used sentences in train = 231
Total loss for epoch 9: 347.730408
	Epoch 10....
Epoch has taken 0:00:11.674695
Number of used sentences in train = 231
Total loss for epoch 10: 346.719892
	Epoch 11....
Epoch has taken 0:00:11.677777
Number of used sentences in train = 231
Total loss for epoch 11: 346.895196
	Epoch 12....
Epoch has taken 0:00:11.665690
Number of used sentences in train = 231
Total loss for epoch 12: 345.913397
	Epoch 13....
Epoch has taken 0:00:11.682950
Number of used sentences in train = 231
Total loss for epoch 13: 346.285975
	Epoch 14....
Epoch has taken 0:00:11.675241
Number of used sentences in train = 231
Total loss for epoch 14: 345.554939
Epoch has taken 0:00:11.682783

==================================================================================================
	Training time : 0:31:29.573287
==================================================================================================
	Identification : 0.364

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(3369, 146)
  (lstm): LSTM(172, 52, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14143.184262
validation loss after epoch 0 : 1107.592033
	Epoch 1....
Epoch has taken 0:03:43.773377
Number of used sentences in train = 3226
Total loss for epoch 1: 9384.272139
validation loss after epoch 1 : 1074.218521
	Epoch 2....
Epoch has taken 0:03:43.897072
Number of used sentences in train = 3226
Total loss for epoch 2: 8431.591797
validation loss after epoch 2 : 1104.268884
	Epoch 3....
Epoch has taken 0:03:43.796652
Number of used sentences in train = 3226
Total loss for epoch 3: 7893.926246
validation loss after epoch 3 : 1133.130585
	Epoch 4....
Epoch has taken 0:03:43.955802
Number of used sentences in train = 3226
Total loss for epoch 4: 7506.199956
validation loss after epoch 4 : 1245.523987
	Epoch 5....
Epoch has taken 0:03:43.963952
Number of used sentences in train = 3226
Total loss for epoch 5: 7214.365747
validation loss after epoch 5 : 1316.755438
	Epoch 6....
Epoch has taken 0:03:44.054452
Number of used sentences in train = 3226
Total loss for epoch 6: 7022.234755
validation loss after epoch 6 : 1307.122563
	Epoch 7....
Epoch has taken 0:03:44.857146
Number of used sentences in train = 3226
Total loss for epoch 7: 6864.945503
validation loss after epoch 7 : 1419.234648
	Epoch 8....
Epoch has taken 0:03:44.435466
Number of used sentences in train = 3226
Total loss for epoch 8: 6730.467443
validation loss after epoch 8 : 1340.804086
	Epoch 9....
Epoch has taken 0:03:44.267925
Number of used sentences in train = 3226
Total loss for epoch 9: 6651.564724
validation loss after epoch 9 : 1374.336912
	Epoch 10....
Epoch has taken 0:03:44.376905
Number of used sentences in train = 3226
Total loss for epoch 10: 6607.614560
validation loss after epoch 10 : 1438.520666
	Epoch 11....
Epoch has taken 0:03:43.955151
Number of used sentences in train = 3226
Total loss for epoch 11: 6534.001787
validation loss after epoch 11 : 1501.023787
	Epoch 12....
Epoch has taken 0:03:44.171791
Number of used sentences in train = 3226
Total loss for epoch 12: 6472.100942
validation loss after epoch 12 : 1482.330475
	Epoch 13....
Epoch has taken 0:03:44.407554
Number of used sentences in train = 3226
Total loss for epoch 13: 6416.184661
validation loss after epoch 13 : 1519.376955
	Epoch 14....
Epoch has taken 0:03:44.410427
Number of used sentences in train = 3226
Total loss for epoch 14: 6399.039932
validation loss after epoch 14 : 1586.106560
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(3369, 146)
  (lstm): LSTM(172, 52, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=35, bias=True)
  (linear2): Linear(in_features=35, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:44.327788
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1983.642484
	Epoch 1....
Epoch has taken 0:00:21.923439
Number of used sentences in train = 359
Total loss for epoch 1: 1018.739854
	Epoch 2....
Epoch has taken 0:00:21.905498
Number of used sentences in train = 359
Total loss for epoch 2: 855.415890
	Epoch 3....
Epoch has taken 0:00:21.920761
Number of used sentences in train = 359
Total loss for epoch 3: 792.118150
	Epoch 4....
Epoch has taken 0:00:21.915958
Number of used sentences in train = 359
Total loss for epoch 4: 741.806926
	Epoch 5....
Epoch has taken 0:00:21.916118
Number of used sentences in train = 359
Total loss for epoch 5: 719.346609
	Epoch 6....
Epoch has taken 0:00:21.905248
Number of used sentences in train = 359
Total loss for epoch 6: 706.614943
	Epoch 7....
Epoch has taken 0:00:21.883624
Number of used sentences in train = 359
Total loss for epoch 7: 691.652693
	Epoch 8....
Epoch has taken 0:00:21.892693
Number of used sentences in train = 359
Total loss for epoch 8: 695.721246
	Epoch 9....
Epoch has taken 0:00:21.880051
Number of used sentences in train = 359
Total loss for epoch 9: 678.092303
	Epoch 10....
Epoch has taken 0:00:21.871066
Number of used sentences in train = 359
Total loss for epoch 10: 677.328397
	Epoch 11....
Epoch has taken 0:00:21.891288
Number of used sentences in train = 359
Total loss for epoch 11: 679.556117
	Epoch 12....
Epoch has taken 0:00:21.881548
Number of used sentences in train = 359
Total loss for epoch 12: 677.989313
	Epoch 13....
Epoch has taken 0:00:21.879851
Number of used sentences in train = 359
Total loss for epoch 13: 679.021524
	Epoch 14....
Epoch has taken 0:00:21.870547
Number of used sentences in train = 359
Total loss for epoch 14: 673.781085
Epoch has taken 0:00:21.879639

==================================================================================================
	Training time : 1:01:31.734824
==================================================================================================
	Identification : 0.053

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 37, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 33, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 79, 'lstmDropout': 0.15, 'denseActivation': 'tanh', 'wordDim': 148, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1882, 148)
  (lstm): LSTM(181, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=37, bias=True)
  (linear2): Linear(in_features=37, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10528.553589
validation loss after epoch 0 : 881.084668
	Epoch 1....
Epoch has taken 0:02:36.355677
Number of used sentences in train = 2811
Total loss for epoch 1: 7762.392049
validation loss after epoch 1 : 862.027609
	Epoch 2....
Epoch has taken 0:02:36.127471
Number of used sentences in train = 2811
Total loss for epoch 2: 6842.320004
validation loss after epoch 2 : 842.905462
	Epoch 3....
Epoch has taken 0:02:36.305426
Number of used sentences in train = 2811
Total loss for epoch 3: 6273.577566
validation loss after epoch 3 : 894.193374
	Epoch 4....
Epoch has taken 0:02:36.242188
Number of used sentences in train = 2811
Total loss for epoch 4: 5830.215885
validation loss after epoch 4 : 891.663126
	Epoch 5....
Epoch has taken 0:02:36.264239
Number of used sentences in train = 2811
Total loss for epoch 5: 5478.236272
validation loss after epoch 5 : 901.979791
	Epoch 6....
Epoch has taken 0:02:36.241512
Number of used sentences in train = 2811
Total loss for epoch 6: 5274.389415
validation loss after epoch 6 : 917.438391
	Epoch 7....
Epoch has taken 0:02:36.142087
Number of used sentences in train = 2811
Total loss for epoch 7: 5115.116524
validation loss after epoch 7 : 919.596811
	Epoch 8....
Epoch has taken 0:02:36.297779
Number of used sentences in train = 2811
Total loss for epoch 8: 4998.458394
validation loss after epoch 8 : 948.466858
	Epoch 9....
Epoch has taken 0:02:36.446948
Number of used sentences in train = 2811
Total loss for epoch 9: 4914.255862
validation loss after epoch 9 : 982.969643
	Epoch 10....
Epoch has taken 0:02:36.490227
Number of used sentences in train = 2811
Total loss for epoch 10: 4876.327965
validation loss after epoch 10 : 988.699831
	Epoch 11....
Epoch has taken 0:02:36.379518
Number of used sentences in train = 2811
Total loss for epoch 11: 4804.088732
validation loss after epoch 11 : 1005.012050
	Epoch 12....
Epoch has taken 0:02:36.459581
Number of used sentences in train = 2811
Total loss for epoch 12: 4744.973168
validation loss after epoch 12 : 1054.795715
	Epoch 13....
Epoch has taken 0:02:36.464383
Number of used sentences in train = 2811
Total loss for epoch 13: 4714.311301
validation loss after epoch 13 : 1032.625130
	Epoch 14....
Epoch has taken 0:02:36.269026
Number of used sentences in train = 2811
Total loss for epoch 14: 4674.904497
validation loss after epoch 14 : 1027.866631
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1882, 148)
  (lstm): LSTM(181, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=37, bias=True)
  (linear2): Linear(in_features=37, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:36.407046
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1419.672132
	Epoch 1....
Epoch has taken 0:00:16.510134
Number of used sentences in train = 313
Total loss for epoch 1: 798.109383
	Epoch 2....
Epoch has taken 0:00:16.520372
Number of used sentences in train = 313
Total loss for epoch 2: 672.871999
	Epoch 3....
Epoch has taken 0:00:16.507883
Number of used sentences in train = 313
Total loss for epoch 3: 612.843076
	Epoch 4....
Epoch has taken 0:00:16.509533
Number of used sentences in train = 313
Total loss for epoch 4: 586.733094
	Epoch 5....
Epoch has taken 0:00:16.513610
Number of used sentences in train = 313
Total loss for epoch 5: 565.999391
	Epoch 6....
Epoch has taken 0:00:16.506669
Number of used sentences in train = 313
Total loss for epoch 6: 554.864171
	Epoch 7....
Epoch has taken 0:00:16.511771
Number of used sentences in train = 313
Total loss for epoch 7: 545.748994
	Epoch 8....
Epoch has taken 0:00:16.511355
Number of used sentences in train = 313
Total loss for epoch 8: 539.117684
	Epoch 9....
Epoch has taken 0:00:16.506944
Number of used sentences in train = 313
Total loss for epoch 9: 534.852887
	Epoch 10....
Epoch has taken 0:00:16.516299
Number of used sentences in train = 313
Total loss for epoch 10: 531.866669
	Epoch 11....
Epoch has taken 0:00:16.527459
Number of used sentences in train = 313
Total loss for epoch 11: 529.885485
	Epoch 12....
Epoch has taken 0:00:16.505535
Number of used sentences in train = 313
Total loss for epoch 12: 528.794703
	Epoch 13....
Epoch has taken 0:00:16.520820
Number of used sentences in train = 313
Total loss for epoch 13: 527.633885
	Epoch 14....
Epoch has taken 0:00:16.512895
Number of used sentences in train = 313
Total loss for epoch 14: 526.404680
Epoch has taken 0:00:16.499551

==================================================================================================
	Training time : 0:43:13.071944
==================================================================================================
	Identification : 0.479

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1680, 148)
  (lstm): LSTM(181, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=37, bias=True)
  (linear2): Linear(in_features=37, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8430.819694
validation loss after epoch 0 : 801.969978
	Epoch 1....
Epoch has taken 0:01:46.882196
Number of used sentences in train = 2074
Total loss for epoch 1: 5400.133095
validation loss after epoch 1 : 718.061855
	Epoch 2....
Epoch has taken 0:01:47.074913
Number of used sentences in train = 2074
Total loss for epoch 2: 4498.732603
validation loss after epoch 2 : 755.184422
	Epoch 3....
Epoch has taken 0:01:47.068990
Number of used sentences in train = 2074
Total loss for epoch 3: 3975.212126
validation loss after epoch 3 : 804.302749
	Epoch 4....
Epoch has taken 0:01:46.981760
Number of used sentences in train = 2074
Total loss for epoch 4: 3660.249298
validation loss after epoch 4 : 834.858398
	Epoch 5....
Epoch has taken 0:01:47.004850
Number of used sentences in train = 2074
Total loss for epoch 5: 3498.418416
validation loss after epoch 5 : 880.265763
	Epoch 6....
Epoch has taken 0:01:46.891330
Number of used sentences in train = 2074
Total loss for epoch 6: 3374.189880
validation loss after epoch 6 : 894.995078
	Epoch 7....
Epoch has taken 0:01:47.118839
Number of used sentences in train = 2074
Total loss for epoch 7: 3314.986269
validation loss after epoch 7 : 910.783768
	Epoch 8....
Epoch has taken 0:01:47.070500
Number of used sentences in train = 2074
Total loss for epoch 8: 3273.676973
validation loss after epoch 8 : 937.402975
	Epoch 9....
Epoch has taken 0:01:47.079212
Number of used sentences in train = 2074
Total loss for epoch 9: 3254.843375
validation loss after epoch 9 : 942.606892
	Epoch 10....
Epoch has taken 0:01:47.137936
Number of used sentences in train = 2074
Total loss for epoch 10: 3233.491814
validation loss after epoch 10 : 945.693191
	Epoch 11....
Epoch has taken 0:01:47.083709
Number of used sentences in train = 2074
Total loss for epoch 11: 3218.367235
validation loss after epoch 11 : 953.605032
	Epoch 12....
Epoch has taken 0:01:47.104251
Number of used sentences in train = 2074
Total loss for epoch 12: 3206.644620
validation loss after epoch 12 : 976.663437
	Epoch 13....
Epoch has taken 0:01:47.108337
Number of used sentences in train = 2074
Total loss for epoch 13: 3196.219817
validation loss after epoch 13 : 969.205888
	Epoch 14....
Epoch has taken 0:01:47.016213
Number of used sentences in train = 2074
Total loss for epoch 14: 3190.804451
validation loss after epoch 14 : 979.590365
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1680, 148)
  (lstm): LSTM(181, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=37, bias=True)
  (linear2): Linear(in_features=37, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:47.021896
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1589.121008
	Epoch 1....
Epoch has taken 0:00:10.895826
Number of used sentences in train = 231
Total loss for epoch 1: 550.754324
	Epoch 2....
Epoch has taken 0:00:10.889332
Number of used sentences in train = 231
Total loss for epoch 2: 439.436282
	Epoch 3....
Epoch has taken 0:00:10.874420
Number of used sentences in train = 231
Total loss for epoch 3: 399.778744
	Epoch 4....
Epoch has taken 0:00:10.895219
Number of used sentences in train = 231
Total loss for epoch 4: 373.540351
	Epoch 5....
Epoch has taken 0:00:10.897727
Number of used sentences in train = 231
Total loss for epoch 5: 360.651970
	Epoch 6....
Epoch has taken 0:00:10.905712
Number of used sentences in train = 231
Total loss for epoch 6: 355.903036
	Epoch 7....
Epoch has taken 0:00:10.898457
Number of used sentences in train = 231
Total loss for epoch 7: 354.857062
	Epoch 8....
Epoch has taken 0:00:10.905284
Number of used sentences in train = 231
Total loss for epoch 8: 352.280404
	Epoch 9....
Epoch has taken 0:00:10.900731
Number of used sentences in train = 231
Total loss for epoch 9: 349.782066
	Epoch 10....
Epoch has taken 0:00:10.900792
Number of used sentences in train = 231
Total loss for epoch 10: 349.122701
	Epoch 11....
Epoch has taken 0:00:10.903298
Number of used sentences in train = 231
Total loss for epoch 11: 348.203731
	Epoch 12....
Epoch has taken 0:00:10.896050
Number of used sentences in train = 231
Total loss for epoch 12: 347.864952
	Epoch 13....
Epoch has taken 0:00:10.891978
Number of used sentences in train = 231
Total loss for epoch 13: 347.480886
	Epoch 14....
Epoch has taken 0:00:10.897745
Number of used sentences in train = 231
Total loss for epoch 14: 347.192502
Epoch has taken 0:00:10.904087

==================================================================================================
	Training time : 0:29:29.436219
==================================================================================================
	Identification : 0.148

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 33)
  (w_embeddings): Embedding(3369, 148)
  (lstm): LSTM(181, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=37, bias=True)
  (linear2): Linear(in_features=37, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14258.545544
validation loss after epoch 0 : 1165.939276
	Epoch 1....
Epoch has taken 0:03:27.960767
Number of used sentences in train = 3226
Total loss for epoch 1: 9811.743838
validation loss after epoch 1 : 1136.186685
	Epoch 2....
Epoch has taken 0:03:28.083820
Number of used sentences in train = 3226
Total loss for epoch 2: 8812.851190
validation loss after epoch 2 : 1147.201674
	Epoch 3....
Epoch has taken 0:03:28.024448
Number of used sentences in train = 3226
Total loss for epoch 3: 8079.361604
validation loss after epoch 3 : 1174.463221
	Epoch 4....
Epoch has taken 0:03:27.907621
Number of used sentences in train = 3226
Total loss for epoch 4: 7523.831902
validation loss after epoch 4 : 1260.241040
	Epoch 5....
Epoch has taken 0:03:27.972987
Number of used sentences in train = 3226
Total loss for epoch 5: 7210.071306
validation loss after epoch 5 : 1317.826444
	Epoch 6....
Epoch has taken 0:03:27.996079
Number of used sentences in train = 3226
Total loss for epoch 6: 6967.064345
validation loss after epoch 6 : 1379.616219
	Epoch 7....
Epoch has taken 0:03:27.688716
Number of used sentences in train = 3226
Total loss for epoch 7: 6792.034093
validation loss after epoch 7 : 1399.184395
	Epoch 8....
Epoch has taken 0:03:27.817771
Number of used sentences in train = 3226
Total loss for epoch 8: 6652.946756
validation loss after epoch 8 : 1446.146843
	Epoch 9....
Epoch has taken 0:03:27.866523
Number of used sentences in train = 3226
Total loss for epoch 9: 6541.359196
validation loss after epoch 9 : 1443.623605
	Epoch 10....
Epoch has taken 0:03:28.078124
Number of used sentences in train = 3226
Total loss for epoch 10: 6463.798670
validation loss after epoch 10 : 1520.261766
	Epoch 11....
Epoch has taken 0:03:28.089777
Number of used sentences in train = 3226
Total loss for epoch 11: 6397.964937
validation loss after epoch 11 : 1565.613642
	Epoch 12....
Epoch has taken 0:03:28.096936
Number of used sentences in train = 3226
Total loss for epoch 12: 6364.448148
validation loss after epoch 12 : 1597.032700
	Epoch 13....
Epoch has taken 0:03:28.192303
Number of used sentences in train = 3226
Total loss for epoch 13: 6323.212355
validation loss after epoch 13 : 1711.565056
	Epoch 14....
Epoch has taken 0:03:28.138642
Number of used sentences in train = 3226
Total loss for epoch 14: 6297.107487
validation loss after epoch 14 : 1665.800144
	TransitionClassifier(
  (p_embeddings): Embedding(13, 33)
  (w_embeddings): Embedding(3369, 148)
  (lstm): LSTM(181, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=37, bias=True)
  (linear2): Linear(in_features=37, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:28.048159
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1863.349060
	Epoch 1....
Epoch has taken 0:00:20.269143
Number of used sentences in train = 359
Total loss for epoch 1: 1057.807473
	Epoch 2....
Epoch has taken 0:00:20.273023
Number of used sentences in train = 359
Total loss for epoch 2: 874.631540
	Epoch 3....
Epoch has taken 0:00:20.277745
Number of used sentences in train = 359
Total loss for epoch 3: 780.037641
	Epoch 4....
Epoch has taken 0:00:20.271060
Number of used sentences in train = 359
Total loss for epoch 4: 721.976559
	Epoch 5....
Epoch has taken 0:00:20.266576
Number of used sentences in train = 359
Total loss for epoch 5: 708.614676
	Epoch 6....
Epoch has taken 0:00:20.225814
Number of used sentences in train = 359
Total loss for epoch 6: 693.861053
	Epoch 7....
Epoch has taken 0:00:20.236828
Number of used sentences in train = 359
Total loss for epoch 7: 689.723691
	Epoch 8....
Epoch has taken 0:00:20.227192
Number of used sentences in train = 359
Total loss for epoch 8: 682.567586
	Epoch 9....
Epoch has taken 0:00:20.223376
Number of used sentences in train = 359
Total loss for epoch 9: 682.130700
	Epoch 10....
Epoch has taken 0:00:20.252051
Number of used sentences in train = 359
Total loss for epoch 10: 677.161095
	Epoch 11....
Epoch has taken 0:00:20.222234
Number of used sentences in train = 359
Total loss for epoch 11: 674.932549
	Epoch 12....
Epoch has taken 0:00:20.234413
Number of used sentences in train = 359
Total loss for epoch 12: 675.134745
	Epoch 13....
Epoch has taken 0:00:20.234494
Number of used sentences in train = 359
Total loss for epoch 13: 673.829759
	Epoch 14....
Epoch has taken 0:00:20.233338
Number of used sentences in train = 359
Total loss for epoch 14: 672.959116
Epoch has taken 0:00:20.240918

==================================================================================================
	Training time : 0:57:04.329693
==================================================================================================
	Identification : 0.059

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 73, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 26, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 75, 'lstmDropout': 0.33, 'denseActivation': 'tanh', 'wordDim': 213, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(9272, 213)
  (lstm): LSTM(239, 75, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13271.482764
validation loss after epoch 0 : 1110.754579
	Epoch 1....
Epoch has taken 0:02:36.547217
Number of used sentences in train = 2811
Total loss for epoch 1: 7996.049827
validation loss after epoch 1 : 1094.026437
	Epoch 2....
Epoch has taken 0:02:36.539599
Number of used sentences in train = 2811
Total loss for epoch 2: 6047.704613
validation loss after epoch 2 : 1222.153704
	Epoch 3....
Epoch has taken 0:02:36.799924
Number of used sentences in train = 2811
Total loss for epoch 3: 5217.763229
validation loss after epoch 3 : 1291.338086
	Epoch 4....
Epoch has taken 0:02:36.618589
Number of used sentences in train = 2811
Total loss for epoch 4: 4811.424088
validation loss after epoch 4 : 1407.201558
	Epoch 5....
Epoch has taken 0:02:36.641734
Number of used sentences in train = 2811
Total loss for epoch 5: 4634.755207
validation loss after epoch 5 : 1489.259220
	Epoch 6....
Epoch has taken 0:02:36.606323
Number of used sentences in train = 2811
Total loss for epoch 6: 4571.516914
validation loss after epoch 6 : 1498.963491
	Epoch 7....
Epoch has taken 0:02:36.546542
Number of used sentences in train = 2811
Total loss for epoch 7: 4532.418506
validation loss after epoch 7 : 1532.452364
	Epoch 8....
Epoch has taken 0:02:36.561874
Number of used sentences in train = 2811
Total loss for epoch 8: 4508.372551
validation loss after epoch 8 : 1583.428764
	Epoch 9....
Epoch has taken 0:02:36.505551
Number of used sentences in train = 2811
Total loss for epoch 9: 4495.453438
validation loss after epoch 9 : 1607.803242
	Epoch 10....
Epoch has taken 0:02:36.534593
Number of used sentences in train = 2811
Total loss for epoch 10: 4487.253474
validation loss after epoch 10 : 1653.136077
	Epoch 11....
Epoch has taken 0:02:36.598427
Number of used sentences in train = 2811
Total loss for epoch 11: 4483.842068
validation loss after epoch 11 : 1649.086833
	Epoch 12....
Epoch has taken 0:02:36.504219
Number of used sentences in train = 2811
Total loss for epoch 12: 4481.470700
validation loss after epoch 12 : 1683.982149
	Epoch 13....
Epoch has taken 0:02:36.518849
Number of used sentences in train = 2811
Total loss for epoch 13: 4479.478220
validation loss after epoch 13 : 1692.112698
	Epoch 14....
Epoch has taken 0:02:36.452214
Number of used sentences in train = 2811
Total loss for epoch 14: 4477.526659
validation loss after epoch 14 : 1706.864981
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(9272, 213)
  (lstm): LSTM(239, 75, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:36.571133
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1612.516965
	Epoch 1....
Epoch has taken 0:00:16.531846
Number of used sentences in train = 313
Total loss for epoch 1: 869.220201
	Epoch 2....
Epoch has taken 0:00:16.534461
Number of used sentences in train = 313
Total loss for epoch 2: 611.375148
	Epoch 3....
Epoch has taken 0:00:16.531584
Number of used sentences in train = 313
Total loss for epoch 3: 537.581689
	Epoch 4....
Epoch has taken 0:00:16.543499
Number of used sentences in train = 313
Total loss for epoch 4: 519.375420
	Epoch 5....
Epoch has taken 0:00:16.519726
Number of used sentences in train = 313
Total loss for epoch 5: 513.732926
	Epoch 6....
Epoch has taken 0:00:16.535343
Number of used sentences in train = 313
Total loss for epoch 6: 511.832448
	Epoch 7....
Epoch has taken 0:00:16.533036
Number of used sentences in train = 313
Total loss for epoch 7: 509.931799
	Epoch 8....
Epoch has taken 0:00:16.530521
Number of used sentences in train = 313
Total loss for epoch 8: 508.623958
	Epoch 9....
Epoch has taken 0:00:16.532122
Number of used sentences in train = 313
Total loss for epoch 9: 507.438950
	Epoch 10....
Epoch has taken 0:00:16.537947
Number of used sentences in train = 313
Total loss for epoch 10: 506.817041
	Epoch 11....
Epoch has taken 0:00:16.530834
Number of used sentences in train = 313
Total loss for epoch 11: 506.039178
	Epoch 12....
Epoch has taken 0:00:16.533792
Number of used sentences in train = 313
Total loss for epoch 12: 506.725227
	Epoch 13....
Epoch has taken 0:00:16.531081
Number of used sentences in train = 313
Total loss for epoch 13: 505.851065
	Epoch 14....
Epoch has taken 0:00:16.535891
Number of used sentences in train = 313
Total loss for epoch 14: 505.215985
Epoch has taken 0:00:16.524108

==================================================================================================
	Training time : 0:43:17.050685
==================================================================================================
	Identification : 0.251

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(7114, 213)
  (lstm): LSTM(239, 75, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10744.214707
validation loss after epoch 0 : 1028.776888
	Epoch 1....
Epoch has taken 0:01:46.768355
Number of used sentences in train = 2074
Total loss for epoch 1: 5736.763372
validation loss after epoch 1 : 1208.046834
	Epoch 2....
Epoch has taken 0:01:46.860630
Number of used sentences in train = 2074
Total loss for epoch 2: 4220.436432
validation loss after epoch 2 : 1349.613356
	Epoch 3....
Epoch has taken 0:01:46.927240
Number of used sentences in train = 2074
Total loss for epoch 3: 3632.068295
validation loss after epoch 3 : 1469.349897
	Epoch 4....
Epoch has taken 0:01:46.930972
Number of used sentences in train = 2074
Total loss for epoch 4: 3373.838386
validation loss after epoch 4 : 1597.421567
	Epoch 5....
Epoch has taken 0:01:46.911695
Number of used sentences in train = 2074
Total loss for epoch 5: 3276.737469
validation loss after epoch 5 : 1656.508492
	Epoch 6....
Epoch has taken 0:01:46.938984
Number of used sentences in train = 2074
Total loss for epoch 6: 3223.456594
validation loss after epoch 6 : 1702.756009
	Epoch 7....
Epoch has taken 0:01:46.864528
Number of used sentences in train = 2074
Total loss for epoch 7: 3197.306467
validation loss after epoch 7 : 1753.443182
	Epoch 8....
Epoch has taken 0:01:46.772679
Number of used sentences in train = 2074
Total loss for epoch 8: 3180.562707
validation loss after epoch 8 : 1810.103933
	Epoch 9....
Epoch has taken 0:01:46.754815
Number of used sentences in train = 2074
Total loss for epoch 9: 3173.737628
validation loss after epoch 9 : 1800.594706
	Epoch 10....
Epoch has taken 0:01:46.760460
Number of used sentences in train = 2074
Total loss for epoch 10: 3169.589999
validation loss after epoch 10 : 1840.868354
	Epoch 11....
Epoch has taken 0:01:46.818360
Number of used sentences in train = 2074
Total loss for epoch 11: 3166.385323
validation loss after epoch 11 : 1861.310590
	Epoch 12....
Epoch has taken 0:01:46.789163
Number of used sentences in train = 2074
Total loss for epoch 12: 3163.404444
validation loss after epoch 12 : 1885.228245
	Epoch 13....
Epoch has taken 0:01:46.775169
Number of used sentences in train = 2074
Total loss for epoch 13: 3161.729212
validation loss after epoch 13 : 1892.061371
	Epoch 14....
Epoch has taken 0:01:46.805329
Number of used sentences in train = 2074
Total loss for epoch 14: 3160.305856
validation loss after epoch 14 : 1915.597294
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(7114, 213)
  (lstm): LSTM(239, 75, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:46.821063
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1843.828622
	Epoch 1....
Epoch has taken 0:00:10.890625
Number of used sentences in train = 231
Total loss for epoch 1: 612.391540
	Epoch 2....
Epoch has taken 0:00:10.893314
Number of used sentences in train = 231
Total loss for epoch 2: 427.163951
	Epoch 3....
Epoch has taken 0:00:10.900052
Number of used sentences in train = 231
Total loss for epoch 3: 379.443254
	Epoch 4....
Epoch has taken 0:00:10.895115
Number of used sentences in train = 231
Total loss for epoch 4: 356.944489
	Epoch 5....
Epoch has taken 0:00:10.892764
Number of used sentences in train = 231
Total loss for epoch 5: 351.532438
	Epoch 6....
Epoch has taken 0:00:10.894804
Number of used sentences in train = 231
Total loss for epoch 6: 350.130291
	Epoch 7....
Epoch has taken 0:00:10.893991
Number of used sentences in train = 231
Total loss for epoch 7: 348.982389
	Epoch 8....
Epoch has taken 0:00:10.891140
Number of used sentences in train = 231
Total loss for epoch 8: 347.896778
	Epoch 9....
Epoch has taken 0:00:10.893729
Number of used sentences in train = 231
Total loss for epoch 9: 347.377724
	Epoch 10....
Epoch has taken 0:00:10.894041
Number of used sentences in train = 231
Total loss for epoch 10: 347.010730
	Epoch 11....
Epoch has taken 0:00:10.896052
Number of used sentences in train = 231
Total loss for epoch 11: 346.680230
	Epoch 12....
Epoch has taken 0:00:10.895018
Number of used sentences in train = 231
Total loss for epoch 12: 346.480441
	Epoch 13....
Epoch has taken 0:00:10.892145
Number of used sentences in train = 231
Total loss for epoch 13: 346.286272
	Epoch 14....
Epoch has taken 0:00:10.883888
Number of used sentences in train = 231
Total loss for epoch 14: 346.048207
Epoch has taken 0:00:10.874013

==================================================================================================
	Training time : 0:29:26.232686
==================================================================================================
	Identification : 0.161

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(17896, 213)
  (lstm): LSTM(239, 75, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17193.650563
validation loss after epoch 0 : 1594.155168
	Epoch 1....
Epoch has taken 0:03:33.769566
Number of used sentences in train = 3226
Total loss for epoch 1: 10636.956966
validation loss after epoch 1 : 1954.099738
	Epoch 2....
Epoch has taken 0:03:34.386470
Number of used sentences in train = 3226
Total loss for epoch 2: 8100.947017
validation loss after epoch 2 : 1740.582053
	Epoch 3....
Epoch has taken 0:03:34.521690
Number of used sentences in train = 3226
Total loss for epoch 3: 7027.638783
validation loss after epoch 3 : 1930.436549
	Epoch 4....
Epoch has taken 0:03:34.573824
Number of used sentences in train = 3226
Total loss for epoch 4: 6536.667558
validation loss after epoch 4 : 2070.304685
	Epoch 5....
Epoch has taken 0:03:34.589124
Number of used sentences in train = 3226
Total loss for epoch 5: 6339.298132
validation loss after epoch 5 : 2169.460062
	Epoch 6....
Epoch has taken 0:03:34.499857
Number of used sentences in train = 3226
Total loss for epoch 6: 6249.731199
validation loss after epoch 6 : 2235.354435
	Epoch 7....
Epoch has taken 0:03:34.478806
Number of used sentences in train = 3226
Total loss for epoch 7: 6198.995283
validation loss after epoch 7 : 2325.152386
	Epoch 8....
Epoch has taken 0:03:34.520435
Number of used sentences in train = 3226
Total loss for epoch 8: 6181.101525
validation loss after epoch 8 : 2349.953093
	Epoch 9....
Epoch has taken 0:03:34.618951
Number of used sentences in train = 3226
Total loss for epoch 9: 6163.607353
validation loss after epoch 9 : 2419.491646
	Epoch 10....
Epoch has taken 0:03:34.470896
Number of used sentences in train = 3226
Total loss for epoch 10: 6155.618702
validation loss after epoch 10 : 2437.049669
	Epoch 11....
Epoch has taken 0:03:34.579674
Number of used sentences in train = 3226
Total loss for epoch 11: 6148.312176
validation loss after epoch 11 : 2511.850564
	Epoch 12....
Epoch has taken 0:03:34.723898
Number of used sentences in train = 3226
Total loss for epoch 12: 6144.670010
validation loss after epoch 12 : 2541.342812
	Epoch 13....
Epoch has taken 0:03:34.691012
Number of used sentences in train = 3226
Total loss for epoch 13: 6141.541389
validation loss after epoch 13 : 2554.909788
	Epoch 14....
Epoch has taken 0:03:35.067505
Number of used sentences in train = 3226
Total loss for epoch 14: 6138.165763
validation loss after epoch 14 : 2566.442136
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(17896, 213)
  (lstm): LSTM(239, 75, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:35.051272
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2169.446609
	Epoch 1....
Epoch has taken 0:00:21.003414
Number of used sentences in train = 359
Total loss for epoch 1: 1014.597226
	Epoch 2....
Epoch has taken 0:00:21.010058
Number of used sentences in train = 359
Total loss for epoch 2: 762.096993
	Epoch 3....
Epoch has taken 0:00:21.010403
Number of used sentences in train = 359
Total loss for epoch 3: 693.562846
	Epoch 4....
Epoch has taken 0:00:21.005013
Number of used sentences in train = 359
Total loss for epoch 4: 683.796585
	Epoch 5....
Epoch has taken 0:00:21.010858
Number of used sentences in train = 359
Total loss for epoch 5: 676.158947
	Epoch 6....
Epoch has taken 0:00:20.998492
Number of used sentences in train = 359
Total loss for epoch 6: 674.002764
	Epoch 7....
Epoch has taken 0:00:21.009595
Number of used sentences in train = 359
Total loss for epoch 7: 673.122553
	Epoch 8....
Epoch has taken 0:00:21.000634
Number of used sentences in train = 359
Total loss for epoch 8: 672.552902
	Epoch 9....
Epoch has taken 0:00:21.006402
Number of used sentences in train = 359
Total loss for epoch 9: 672.097988
	Epoch 10....
Epoch has taken 0:00:20.997964
Number of used sentences in train = 359
Total loss for epoch 10: 671.714387
	Epoch 11....
Epoch has taken 0:00:20.984578
Number of used sentences in train = 359
Total loss for epoch 11: 671.460837
	Epoch 12....
Epoch has taken 0:00:20.996567
Number of used sentences in train = 359
Total loss for epoch 12: 671.254695
	Epoch 13....
Epoch has taken 0:00:21.002092
Number of used sentences in train = 359
Total loss for epoch 13: 671.071932
	Epoch 14....
Epoch has taken 0:00:21.005707
Number of used sentences in train = 359
Total loss for epoch 14: 670.933905
Epoch has taken 0:00:21.010310

==================================================================================================
	Training time : 0:58:54.308756
==================================================================================================
	Identification : 0.276

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 31, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 18, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 128, 'lstmDropout': 0.22, 'denseActivation': 'tanh', 'wordDim': 81, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1177, 81)
  (lstm): LSTM(99, 128, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=2048, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 21637.829730
validation loss after epoch 0 : 1053.710273
	Epoch 1....
Epoch has taken 0:02:47.658558
Number of used sentences in train = 2811
Total loss for epoch 1: 9573.539046
validation loss after epoch 1 : 952.495184
	Epoch 2....
Epoch has taken 0:02:47.787830
Number of used sentences in train = 2811
Total loss for epoch 2: 8045.971458
validation loss after epoch 2 : 1082.879898
	Epoch 3....
Epoch has taken 0:02:47.700272
Number of used sentences in train = 2811
Total loss for epoch 3: 7318.113563
validation loss after epoch 3 : 860.206474
	Epoch 4....
Epoch has taken 0:02:47.586396
Number of used sentences in train = 2811
Total loss for epoch 4: 6662.716256
validation loss after epoch 4 : 828.352459
	Epoch 5....
Epoch has taken 0:02:47.593386
Number of used sentences in train = 2811
Total loss for epoch 5: 6357.873167
validation loss after epoch 5 : 893.266525
	Epoch 6....
Epoch has taken 0:02:47.314875
Number of used sentences in train = 2811
Total loss for epoch 6: 5961.481971
validation loss after epoch 6 : 899.020031
	Epoch 7....
Epoch has taken 0:02:47.294592
Number of used sentences in train = 2811
Total loss for epoch 7: 5690.628495
validation loss after epoch 7 : 889.007689
	Epoch 8....
Epoch has taken 0:02:47.491959
Number of used sentences in train = 2811
Total loss for epoch 8: 5525.363497
validation loss after epoch 8 : 913.149294
	Epoch 9....
Epoch has taken 0:02:47.513268
Number of used sentences in train = 2811
Total loss for epoch 9: 5359.671322
validation loss after epoch 9 : 996.635863
	Epoch 10....
Epoch has taken 0:02:47.547990
Number of used sentences in train = 2811
Total loss for epoch 10: 5206.575027
validation loss after epoch 10 : 991.894032
	Epoch 11....
Epoch has taken 0:02:47.558130
Number of used sentences in train = 2811
Total loss for epoch 11: 5108.764968
validation loss after epoch 11 : 974.874307
	Epoch 12....
Epoch has taken 0:02:47.555285
Number of used sentences in train = 2811
Total loss for epoch 12: 4927.980580
validation loss after epoch 12 : 984.640805
	Epoch 13....
Epoch has taken 0:02:47.387012
Number of used sentences in train = 2811
Total loss for epoch 13: 4882.178843
validation loss after epoch 13 : 1050.880873
	Epoch 14....
Epoch has taken 0:02:47.459850
Number of used sentences in train = 2811
Total loss for epoch 14: 4804.730205
validation loss after epoch 14 : 1018.927343
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1177, 81)
  (lstm): LSTM(99, 128, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=2048, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.492741
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1478.374433
	Epoch 1....
Epoch has taken 0:00:17.663939
Number of used sentences in train = 313
Total loss for epoch 1: 854.816782
	Epoch 2....
Epoch has taken 0:00:17.658828
Number of used sentences in train = 313
Total loss for epoch 2: 725.557467
	Epoch 3....
Epoch has taken 0:00:17.657627
Number of used sentences in train = 313
Total loss for epoch 3: 613.004427
	Epoch 4....
Epoch has taken 0:00:17.657340
Number of used sentences in train = 313
Total loss for epoch 4: 594.694135
	Epoch 5....
Epoch has taken 0:00:17.651864
Number of used sentences in train = 313
Total loss for epoch 5: 559.617406
	Epoch 6....
Epoch has taken 0:00:17.646694
Number of used sentences in train = 313
Total loss for epoch 6: 536.489795
	Epoch 7....
Epoch has taken 0:00:17.649125
Number of used sentences in train = 313
Total loss for epoch 7: 529.929355
	Epoch 8....
Epoch has taken 0:00:17.648217
Number of used sentences in train = 313
Total loss for epoch 8: 520.047140
	Epoch 9....
Epoch has taken 0:00:17.657378
Number of used sentences in train = 313
Total loss for epoch 9: 517.972333
	Epoch 10....
Epoch has taken 0:00:17.652252
Number of used sentences in train = 313
Total loss for epoch 10: 513.852596
	Epoch 11....
Epoch has taken 0:00:17.646237
Number of used sentences in train = 313
Total loss for epoch 11: 514.057710
	Epoch 12....
Epoch has taken 0:00:17.655828
Number of used sentences in train = 313
Total loss for epoch 12: 512.164486
	Epoch 13....
Epoch has taken 0:00:17.647928
Number of used sentences in train = 313
Total loss for epoch 13: 510.946809
	Epoch 14....
Epoch has taken 0:00:17.646426
Number of used sentences in train = 313
Total loss for epoch 14: 511.525675
Epoch has taken 0:00:17.650415

==================================================================================================
	Training time : 0:46:18.227920
==================================================================================================
	Identification : 0.027

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1133, 81)
  (lstm): LSTM(99, 128, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=2048, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 12026.083262
validation loss after epoch 0 : 774.896978
	Epoch 1....
Epoch has taken 0:01:55.060530
Number of used sentences in train = 2074
Total loss for epoch 1: 6082.468041
validation loss after epoch 1 : 648.663683
	Epoch 2....
Epoch has taken 0:01:55.115101
Number of used sentences in train = 2074
Total loss for epoch 2: 5264.529588
validation loss after epoch 2 : 647.161857
	Epoch 3....
Epoch has taken 0:01:55.208944
Number of used sentences in train = 2074
Total loss for epoch 3: 4737.295295
validation loss after epoch 3 : 632.574411
	Epoch 4....
Epoch has taken 0:01:55.128793
Number of used sentences in train = 2074
Total loss for epoch 4: 4395.215948
validation loss after epoch 4 : 662.933070
	Epoch 5....
Epoch has taken 0:01:55.128798
Number of used sentences in train = 2074
Total loss for epoch 5: 4073.798626
validation loss after epoch 5 : 704.533011
	Epoch 6....
Epoch has taken 0:01:55.099306
Number of used sentences in train = 2074
Total loss for epoch 6: 3865.379844
validation loss after epoch 6 : 683.955681
	Epoch 7....
Epoch has taken 0:01:55.088014
Number of used sentences in train = 2074
Total loss for epoch 7: 3720.424471
validation loss after epoch 7 : 693.904624
	Epoch 8....
Epoch has taken 0:01:55.203596
Number of used sentences in train = 2074
Total loss for epoch 8: 3606.090576
validation loss after epoch 8 : 706.581680
	Epoch 9....
Epoch has taken 0:01:55.216088
Number of used sentences in train = 2074
Total loss for epoch 9: 3521.726821
validation loss after epoch 9 : 699.951774
	Epoch 10....
Epoch has taken 0:01:55.265807
Number of used sentences in train = 2074
Total loss for epoch 10: 3435.240774
validation loss after epoch 10 : 777.339707
	Epoch 11....
Epoch has taken 0:01:59.786071
Number of used sentences in train = 2074
Total loss for epoch 11: 3417.356678
validation loss after epoch 11 : 774.651933
	Epoch 12....
Epoch has taken 0:01:55.130102
Number of used sentences in train = 2074
Total loss for epoch 12: 3344.800911
validation loss after epoch 12 : 782.884739
	Epoch 13....
Epoch has taken 0:01:55.100520
Number of used sentences in train = 2074
Total loss for epoch 13: 3322.160382
validation loss after epoch 13 : 818.677153
	Epoch 14....
Epoch has taken 0:01:55.028092
Number of used sentences in train = 2074
Total loss for epoch 14: 3285.683094
validation loss after epoch 14 : 777.228460
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1133, 81)
  (lstm): LSTM(99, 128, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=2048, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:55.186297
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1508.970923
	Epoch 1....
Epoch has taken 0:00:11.754449
Number of used sentences in train = 231
Total loss for epoch 1: 744.028805
	Epoch 2....
Epoch has taken 0:00:11.754527
Number of used sentences in train = 231
Total loss for epoch 2: 515.988027
	Epoch 3....
Epoch has taken 0:00:11.746873
Number of used sentences in train = 231
Total loss for epoch 3: 470.059019
	Epoch 4....
Epoch has taken 0:00:11.746060
Number of used sentences in train = 231
Total loss for epoch 4: 409.411228
	Epoch 5....
Epoch has taken 0:00:11.749248
Number of used sentences in train = 231
Total loss for epoch 5: 383.649206
	Epoch 6....
Epoch has taken 0:00:11.750790
Number of used sentences in train = 231
Total loss for epoch 6: 373.571396
	Epoch 7....
Epoch has taken 0:00:11.756008
Number of used sentences in train = 231
Total loss for epoch 7: 357.880994
	Epoch 8....
Epoch has taken 0:00:11.748709
Number of used sentences in train = 231
Total loss for epoch 8: 355.554913
	Epoch 9....
Epoch has taken 0:00:11.750281
Number of used sentences in train = 231
Total loss for epoch 9: 350.458352
	Epoch 10....
Epoch has taken 0:00:11.752581
Number of used sentences in train = 231
Total loss for epoch 10: 350.476428
	Epoch 11....
Epoch has taken 0:00:11.753021
Number of used sentences in train = 231
Total loss for epoch 11: 348.049073
	Epoch 12....
Epoch has taken 0:00:11.754634
Number of used sentences in train = 231
Total loss for epoch 12: 347.673200
	Epoch 13....
Epoch has taken 0:00:11.751490
Number of used sentences in train = 231
Total loss for epoch 13: 347.175968
	Epoch 14....
Epoch has taken 0:00:11.747848
Number of used sentences in train = 231
Total loss for epoch 14: 346.829338
Epoch has taken 0:00:11.749931

==================================================================================================
	Training time : 0:31:48.348623
==================================================================================================
	Identification : 0.435

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(1202, 81)
  (lstm): LSTM(99, 128, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=2048, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 40316.604599
validation loss after epoch 0 : 4018.200016
	Epoch 1....
Epoch has taken 0:03:43.388143
Number of used sentences in train = 3226
Total loss for epoch 1: 37447.555091
validation loss after epoch 1 : 4006.846784
	Epoch 2....
Epoch has taken 0:03:43.071089
Number of used sentences in train = 3226
Total loss for epoch 2: 37998.751495
validation loss after epoch 2 : 3985.884583
	Epoch 3....
Epoch has taken 0:03:43.185170
Number of used sentences in train = 3226
Total loss for epoch 3: 36951.105576
validation loss after epoch 3 : 3926.917219
	Epoch 4....
Epoch has taken 0:03:43.078297
Number of used sentences in train = 3226
Total loss for epoch 4: 36655.171372
validation loss after epoch 4 : 3914.452082
	Epoch 5....
Epoch has taken 0:03:43.045727
Number of used sentences in train = 3226
Total loss for epoch 5: 36595.179754
validation loss after epoch 5 : 3906.663717
	Epoch 6....
Epoch has taken 0:03:43.006720
Number of used sentences in train = 3226
Total loss for epoch 6: 36552.286351
validation loss after epoch 6 : 3907.057543
	Epoch 7....
Epoch has taken 0:03:43.276684
Number of used sentences in train = 3226
Total loss for epoch 7: 36518.644110
validation loss after epoch 7 : 3907.240158
	Epoch 8....
Epoch has taken 0:03:43.383627
Number of used sentences in train = 3226
Total loss for epoch 8: 36481.380007
validation loss after epoch 8 : 3902.984285
	Epoch 9....
Epoch has taken 0:03:43.142299
Number of used sentences in train = 3226
Total loss for epoch 9: 36478.773852
validation loss after epoch 9 : 3899.644716
	Epoch 10....
Epoch has taken 0:03:43.190825
Number of used sentences in train = 3226
Total loss for epoch 10: 36656.863858
validation loss after epoch 10 : 3892.924756
	Epoch 11....
Epoch has taken 0:03:43.172563
Number of used sentences in train = 3226
Total loss for epoch 11: 36442.628180
validation loss after epoch 11 : 3894.605101
	Epoch 12....
Epoch has taken 0:03:43.053299
Number of used sentences in train = 3226
Total loss for epoch 12: 36450.569942
validation loss after epoch 12 : 3896.243988
	Epoch 13....
Epoch has taken 0:03:43.090398
Number of used sentences in train = 3226
Total loss for epoch 13: 36420.267563
validation loss after epoch 13 : 3894.866846
	Epoch 14....
Epoch has taken 0:03:43.562361
Number of used sentences in train = 3226
Total loss for epoch 14: 36421.155558
validation loss after epoch 14 : 3893.506750
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(1202, 81)
  (lstm): LSTM(99, 128, num_layers=2, dropout=0.22, bidirectional=True)
  (linear1): Linear(in_features=2048, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.433617
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 3315.518740
	Epoch 1....
Epoch has taken 0:00:21.853115
Number of used sentences in train = 359
Total loss for epoch 1: 2310.372662
	Epoch 2....
Epoch has taken 0:00:21.848370
Number of used sentences in train = 359
Total loss for epoch 2: 1962.921758
	Epoch 3....
Epoch has taken 0:00:21.840256
Number of used sentences in train = 359
Total loss for epoch 3: 1763.841711
	Epoch 4....
Epoch has taken 0:00:21.842690
Number of used sentences in train = 359
Total loss for epoch 4: 1537.458320
	Epoch 5....
Epoch has taken 0:00:21.839714
Number of used sentences in train = 359
Total loss for epoch 5: 1415.763487
	Epoch 6....
Epoch has taken 0:00:21.834672
Number of used sentences in train = 359
Total loss for epoch 6: 1279.211793
	Epoch 7....
Epoch has taken 0:00:21.848467
Number of used sentences in train = 359
Total loss for epoch 7: 1168.129125
	Epoch 8....
Epoch has taken 0:00:21.837716
Number of used sentences in train = 359
Total loss for epoch 8: 1073.571479
	Epoch 9....
Epoch has taken 0:00:21.844823
Number of used sentences in train = 359
Total loss for epoch 9: 1034.576878
	Epoch 10....
Epoch has taken 0:00:21.852740
Number of used sentences in train = 359
Total loss for epoch 10: 1004.550584
	Epoch 11....
Epoch has taken 0:00:21.859469
Number of used sentences in train = 359
Total loss for epoch 11: 928.659808
	Epoch 12....
Epoch has taken 0:00:21.847910
Number of used sentences in train = 359
Total loss for epoch 12: 899.906573
	Epoch 13....
Epoch has taken 0:00:21.845689
Number of used sentences in train = 359
Total loss for epoch 13: 873.281482
	Epoch 14....
Epoch has taken 0:00:21.852858
Number of used sentences in train = 359
Total loss for epoch 14: 855.807512
Epoch has taken 0:00:21.847685

==================================================================================================
	Training time : 1:01:16.423186
==================================================================================================
	Identification : 0.493

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 52, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 20, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 39, 'lstmDropout': 0.14, 'denseActivation': 'tanh', 'wordDim': 147, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(9276, 147)
  (lstm): LSTM(167, 39, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=52, bias=True)
  (linear2): Linear(in_features=52, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14255.701088
validation loss after epoch 0 : 1150.457127
	Epoch 1....
Epoch has taken 0:02:47.487829
Number of used sentences in train = 2811
Total loss for epoch 1: 8808.843134
validation loss after epoch 1 : 1097.084875
	Epoch 2....
Epoch has taken 0:02:47.973328
Number of used sentences in train = 2811
Total loss for epoch 2: 6975.337781
validation loss after epoch 2 : 1127.810027
	Epoch 3....
Epoch has taken 0:02:47.506154
Number of used sentences in train = 2811
Total loss for epoch 3: 5941.573177
validation loss after epoch 3 : 1194.616703
	Epoch 4....
Epoch has taken 0:02:47.438425
Number of used sentences in train = 2811
Total loss for epoch 4: 5389.663522
validation loss after epoch 4 : 1329.138234
	Epoch 5....
Epoch has taken 0:02:47.162775
Number of used sentences in train = 2811
Total loss for epoch 5: 5105.344238
validation loss after epoch 5 : 1319.589171
	Epoch 6....
Epoch has taken 0:02:47.066578
Number of used sentences in train = 2811
Total loss for epoch 6: 4857.749632
validation loss after epoch 6 : 1504.237501
	Epoch 7....
Epoch has taken 0:02:47.213784
Number of used sentences in train = 2811
Total loss for epoch 7: 4785.941267
validation loss after epoch 7 : 1527.652131
	Epoch 8....
Epoch has taken 0:02:47.164532
Number of used sentences in train = 2811
Total loss for epoch 8: 4707.800784
validation loss after epoch 8 : 1541.147369
	Epoch 9....
Epoch has taken 0:02:47.357587
Number of used sentences in train = 2811
Total loss for epoch 9: 4638.973866
validation loss after epoch 9 : 1515.314451
	Epoch 10....
Epoch has taken 0:02:47.312763
Number of used sentences in train = 2811
Total loss for epoch 10: 4628.736026
validation loss after epoch 10 : 1576.418763
	Epoch 11....
Epoch has taken 0:02:47.349483
Number of used sentences in train = 2811
Total loss for epoch 11: 4563.400805
validation loss after epoch 11 : 1587.950083
	Epoch 12....
Epoch has taken 0:02:47.470555
Number of used sentences in train = 2811
Total loss for epoch 12: 4540.179938
validation loss after epoch 12 : 1649.462745
	Epoch 13....
Epoch has taken 0:02:47.432966
Number of used sentences in train = 2811
Total loss for epoch 13: 4530.312595
validation loss after epoch 13 : 1651.877218
	Epoch 14....
Epoch has taken 0:02:47.149283
Number of used sentences in train = 2811
Total loss for epoch 14: 4519.996652
validation loss after epoch 14 : 1758.350195
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(9276, 147)
  (lstm): LSTM(167, 39, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=52, bias=True)
  (linear2): Linear(in_features=52, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.116207
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2008.282290
	Epoch 1....
Epoch has taken 0:00:17.708814
Number of used sentences in train = 313
Total loss for epoch 1: 741.221926
	Epoch 2....
Epoch has taken 0:00:17.697181
Number of used sentences in train = 313
Total loss for epoch 2: 575.160498
	Epoch 3....
Epoch has taken 0:00:17.699034
Number of used sentences in train = 313
Total loss for epoch 3: 536.680263
	Epoch 4....
Epoch has taken 0:00:17.704327
Number of used sentences in train = 313
Total loss for epoch 4: 518.579152
	Epoch 5....
Epoch has taken 0:00:17.697716
Number of used sentences in train = 313
Total loss for epoch 5: 515.300895
	Epoch 6....
Epoch has taken 0:00:17.697630
Number of used sentences in train = 313
Total loss for epoch 6: 509.751211
	Epoch 7....
Epoch has taken 0:00:17.696868
Number of used sentences in train = 313
Total loss for epoch 7: 505.946596
	Epoch 8....
Epoch has taken 0:00:17.692036
Number of used sentences in train = 313
Total loss for epoch 8: 504.043721
	Epoch 9....
Epoch has taken 0:00:17.689777
Number of used sentences in train = 313
Total loss for epoch 9: 505.504322
	Epoch 10....
Epoch has taken 0:00:17.690062
Number of used sentences in train = 313
Total loss for epoch 10: 509.682075
	Epoch 11....
Epoch has taken 0:00:17.700488
Number of used sentences in train = 313
Total loss for epoch 11: 503.412159
	Epoch 12....
Epoch has taken 0:00:17.682394
Number of used sentences in train = 313
Total loss for epoch 12: 501.599584
	Epoch 13....
Epoch has taken 0:00:17.688086
Number of used sentences in train = 313
Total loss for epoch 13: 501.369133
	Epoch 14....
Epoch has taken 0:00:17.674821
Number of used sentences in train = 313
Total loss for epoch 14: 501.280982
Epoch has taken 0:00:17.682216

==================================================================================================
	Training time : 0:46:16.115297
==================================================================================================
	Identification : 0.409

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(7068, 147)
  (lstm): LSTM(167, 39, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=52, bias=True)
  (linear2): Linear(in_features=52, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11807.613058
validation loss after epoch 0 : 924.350334
	Epoch 1....
Epoch has taken 0:01:54.275324
Number of used sentences in train = 2074
Total loss for epoch 1: 6300.638871
validation loss after epoch 1 : 833.987547
	Epoch 2....
Epoch has taken 0:01:54.301477
Number of used sentences in train = 2074
Total loss for epoch 2: 4914.642813
validation loss after epoch 2 : 822.999502
	Epoch 3....
Epoch has taken 0:01:54.475487
Number of used sentences in train = 2074
Total loss for epoch 3: 4158.458695
validation loss after epoch 3 : 1019.828757
	Epoch 4....
Epoch has taken 0:01:54.419291
Number of used sentences in train = 2074
Total loss for epoch 4: 3863.395178
validation loss after epoch 4 : 1125.666073
	Epoch 5....
Epoch has taken 0:01:54.631350
Number of used sentences in train = 2074
Total loss for epoch 5: 3624.320898
validation loss after epoch 5 : 1136.079838
	Epoch 6....
Epoch has taken 0:01:54.388318
Number of used sentences in train = 2074
Total loss for epoch 6: 3519.898145
validation loss after epoch 6 : 1230.445585
	Epoch 7....
Epoch has taken 0:01:54.310632
Number of used sentences in train = 2074
Total loss for epoch 7: 3428.993751
validation loss after epoch 7 : 1282.238486
	Epoch 8....
Epoch has taken 0:01:54.402907
Number of used sentences in train = 2074
Total loss for epoch 8: 3328.242775
validation loss after epoch 8 : 1287.906311
	Epoch 9....
Epoch has taken 0:01:54.367331
Number of used sentences in train = 2074
Total loss for epoch 9: 3279.938653
validation loss after epoch 9 : 1226.074180
	Epoch 10....
Epoch has taken 0:01:54.326700
Number of used sentences in train = 2074
Total loss for epoch 10: 3248.520171
validation loss after epoch 10 : 1380.120728
	Epoch 11....
Epoch has taken 0:01:54.324988
Number of used sentences in train = 2074
Total loss for epoch 11: 3216.681138
validation loss after epoch 11 : 1269.095423
	Epoch 12....
Epoch has taken 0:01:54.329099
Number of used sentences in train = 2074
Total loss for epoch 12: 3197.875837
validation loss after epoch 12 : 1268.682985
	Epoch 13....
Epoch has taken 0:01:54.540196
Number of used sentences in train = 2074
Total loss for epoch 13: 3194.364957
validation loss after epoch 13 : 1382.070022
	Epoch 14....
Epoch has taken 0:01:54.434346
Number of used sentences in train = 2074
Total loss for epoch 14: 3199.012820
validation loss after epoch 14 : 1381.409810
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(7068, 147)
  (lstm): LSTM(167, 39, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=52, bias=True)
  (linear2): Linear(in_features=52, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.442978
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1816.108620
	Epoch 1....
Epoch has taken 0:00:11.664695
Number of used sentences in train = 231
Total loss for epoch 1: 609.743632
	Epoch 2....
Epoch has taken 0:00:11.656550
Number of used sentences in train = 231
Total loss for epoch 2: 465.877159
	Epoch 3....
Epoch has taken 0:00:11.656989
Number of used sentences in train = 231
Total loss for epoch 3: 393.251272
	Epoch 4....
Epoch has taken 0:00:11.656383
Number of used sentences in train = 231
Total loss for epoch 4: 383.050465
	Epoch 5....
Epoch has taken 0:00:11.654459
Number of used sentences in train = 231
Total loss for epoch 5: 369.512478
	Epoch 6....
Epoch has taken 0:00:11.657245
Number of used sentences in train = 231
Total loss for epoch 6: 365.003634
	Epoch 7....
Epoch has taken 0:00:11.661312
Number of used sentences in train = 231
Total loss for epoch 7: 359.218116
	Epoch 8....
Epoch has taken 0:00:11.656928
Number of used sentences in train = 231
Total loss for epoch 8: 354.932073
	Epoch 9....
Epoch has taken 0:00:11.660528
Number of used sentences in train = 231
Total loss for epoch 9: 351.887581
	Epoch 10....
Epoch has taken 0:00:11.659796
Number of used sentences in train = 231
Total loss for epoch 10: 349.798313
	Epoch 11....
Epoch has taken 0:00:11.663740
Number of used sentences in train = 231
Total loss for epoch 11: 350.356018
	Epoch 12....
Epoch has taken 0:00:11.659552
Number of used sentences in train = 231
Total loss for epoch 12: 349.372980
	Epoch 13....
Epoch has taken 0:00:11.658176
Number of used sentences in train = 231
Total loss for epoch 13: 350.376313
	Epoch 14....
Epoch has taken 0:00:11.655320
Number of used sentences in train = 231
Total loss for epoch 14: 348.123499
Epoch has taken 0:00:11.659546

==================================================================================================
	Training time : 0:31:31.197013
==================================================================================================
	Identification : 0.258

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 20)
  (w_embeddings): Embedding(17901, 147)
  (lstm): LSTM(167, 39, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=52, bias=True)
  (linear2): Linear(in_features=52, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19201.331151
validation loss after epoch 0 : 1597.020195
	Epoch 1....
Epoch has taken 0:03:42.460149
Number of used sentences in train = 3226
Total loss for epoch 1: 11781.530009
validation loss after epoch 1 : 1605.659019
	Epoch 2....
Epoch has taken 0:03:42.631943
Number of used sentences in train = 3226
Total loss for epoch 2: 9351.698692
validation loss after epoch 2 : 1774.101982
	Epoch 3....
Epoch has taken 0:03:42.933551
Number of used sentences in train = 3226
Total loss for epoch 3: 8026.114338
validation loss after epoch 3 : 1963.986002
	Epoch 4....
Epoch has taken 0:03:42.932822
Number of used sentences in train = 3226
Total loss for epoch 4: 7314.846529
validation loss after epoch 4 : 2160.203893
	Epoch 5....
Epoch has taken 0:03:42.802801
Number of used sentences in train = 3226
Total loss for epoch 5: 6962.397634
validation loss after epoch 5 : 2353.078345
	Epoch 6....
Epoch has taken 0:03:42.858640
Number of used sentences in train = 3226
Total loss for epoch 6: 6711.701571
validation loss after epoch 6 : 2361.791571
	Epoch 7....
Epoch has taken 0:03:55.841440
Number of used sentences in train = 3226
Total loss for epoch 7: 6560.069633
validation loss after epoch 7 : 2435.377718
	Epoch 8....
Epoch has taken 0:03:43.007033
Number of used sentences in train = 3226
Total loss for epoch 8: 6457.862707
validation loss after epoch 8 : 2412.371659
	Epoch 9....
Epoch has taken 0:03:42.965038
Number of used sentences in train = 3226
Total loss for epoch 9: 6387.326030
validation loss after epoch 9 : 2569.085447
	Epoch 10....
Epoch has taken 0:03:42.924723
Number of used sentences in train = 3226
Total loss for epoch 10: 6379.602076
validation loss after epoch 10 : 2557.645649
	Epoch 11....
Epoch has taken 0:03:42.982959
Number of used sentences in train = 3226
Total loss for epoch 11: 6300.548221
validation loss after epoch 11 : 2709.885752
	Epoch 12....
Epoch has taken 0:03:43.059892
Number of used sentences in train = 3226
Total loss for epoch 12: 6261.555260
validation loss after epoch 12 : 2692.666991
	Epoch 13....
Epoch has taken 0:03:42.924794
Number of used sentences in train = 3226
Total loss for epoch 13: 6237.173290
validation loss after epoch 13 : 2860.101974
	Epoch 14....
Epoch has taken 0:03:42.854028
Number of used sentences in train = 3226
Total loss for epoch 14: 6243.340754
validation loss after epoch 14 : 2738.783031
	TransitionClassifier(
  (p_embeddings): Embedding(13, 20)
  (w_embeddings): Embedding(17901, 147)
  (lstm): LSTM(167, 39, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=52, bias=True)
  (linear2): Linear(in_features=52, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.062510
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2097.735460
	Epoch 1....
Epoch has taken 0:00:21.764998
Number of used sentences in train = 359
Total loss for epoch 1: 1056.656393
	Epoch 2....
Epoch has taken 0:00:21.782937
Number of used sentences in train = 359
Total loss for epoch 2: 812.266485
	Epoch 3....
Epoch has taken 0:00:21.768873
Number of used sentences in train = 359
Total loss for epoch 3: 738.512297
	Epoch 4....
Epoch has taken 0:00:21.766368
Number of used sentences in train = 359
Total loss for epoch 4: 706.164742
	Epoch 5....
Epoch has taken 0:00:21.778140
Number of used sentences in train = 359
Total loss for epoch 5: 697.058755
	Epoch 6....
Epoch has taken 0:00:21.783104
Number of used sentences in train = 359
Total loss for epoch 6: 681.201315
	Epoch 7....
Epoch has taken 0:00:21.778898
Number of used sentences in train = 359
Total loss for epoch 7: 675.191753
	Epoch 8....
Epoch has taken 0:00:21.776487
Number of used sentences in train = 359
Total loss for epoch 8: 673.273120
	Epoch 9....
Epoch has taken 0:00:21.781483
Number of used sentences in train = 359
Total loss for epoch 9: 676.140159
	Epoch 10....
Epoch has taken 0:00:21.776657
Number of used sentences in train = 359
Total loss for epoch 10: 671.802296
	Epoch 11....
Epoch has taken 0:00:21.761921
Number of used sentences in train = 359
Total loss for epoch 11: 671.410280
	Epoch 12....
Epoch has taken 0:00:21.774164
Number of used sentences in train = 359
Total loss for epoch 12: 671.344352
	Epoch 13....
Epoch has taken 0:00:21.771992
Number of used sentences in train = 359
Total loss for epoch 13: 670.857875
	Epoch 14....
Epoch has taken 0:00:21.782013
Number of used sentences in train = 359
Total loss for epoch 14: 670.765257
Epoch has taken 0:00:21.764748

==================================================================================================
	Training time : 1:01:23.564077
==================================================================================================
	Identification : 0.302

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 32, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 125, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 106, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1882, 106)
  (lstm): LSTM(123, 125, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15688.651955
validation loss after epoch 0 : 941.004008
	Epoch 1....
Epoch has taken 0:02:48.259078
Number of used sentences in train = 2811
Total loss for epoch 1: 8364.140265
validation loss after epoch 1 : 887.486973
	Epoch 2....
Epoch has taken 0:02:48.080016
Number of used sentences in train = 2811
Total loss for epoch 2: 7226.481328
validation loss after epoch 2 : 830.934691
	Epoch 3....
Epoch has taken 0:02:48.281255
Number of used sentences in train = 2811
Total loss for epoch 3: 6576.749606
validation loss after epoch 3 : 830.025374
	Epoch 4....
Epoch has taken 0:02:48.253154
Number of used sentences in train = 2811
Total loss for epoch 4: 6006.458218
validation loss after epoch 4 : 864.672294
	Epoch 5....
Epoch has taken 0:02:48.297306
Number of used sentences in train = 2811
Total loss for epoch 5: 5701.494845
validation loss after epoch 5 : 887.245577
	Epoch 6....
Epoch has taken 0:02:48.254314
Number of used sentences in train = 2811
Total loss for epoch 6: 5458.666791
validation loss after epoch 6 : 948.988037
	Epoch 7....
Epoch has taken 0:02:48.359972
Number of used sentences in train = 2811
Total loss for epoch 7: 5226.679071
validation loss after epoch 7 : 934.030355
	Epoch 8....
Epoch has taken 0:02:48.259524
Number of used sentences in train = 2811
Total loss for epoch 8: 5045.484428
validation loss after epoch 8 : 975.064288
	Epoch 9....
Epoch has taken 0:02:48.243518
Number of used sentences in train = 2811
Total loss for epoch 9: 4943.947493
validation loss after epoch 9 : 960.906971
	Epoch 10....
Epoch has taken 0:02:48.273798
Number of used sentences in train = 2811
Total loss for epoch 10: 4852.527533
validation loss after epoch 10 : 1006.884254
	Epoch 11....
Epoch has taken 0:02:48.196591
Number of used sentences in train = 2811
Total loss for epoch 11: 4768.275055
validation loss after epoch 11 : 1008.175428
	Epoch 12....
Epoch has taken 0:02:47.962483
Number of used sentences in train = 2811
Total loss for epoch 12: 4721.943255
validation loss after epoch 12 : 1005.916756
	Epoch 13....
Epoch has taken 0:02:48.091352
Number of used sentences in train = 2811
Total loss for epoch 13: 4680.666460
validation loss after epoch 13 : 1008.852461
	Epoch 14....
Epoch has taken 0:02:48.240517
Number of used sentences in train = 2811
Total loss for epoch 14: 4650.724751
validation loss after epoch 14 : 1031.587676
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1882, 106)
  (lstm): LSTM(123, 125, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:48.091569
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1410.378052
	Epoch 1....
Epoch has taken 0:00:17.817884
Number of used sentences in train = 313
Total loss for epoch 1: 862.552129
	Epoch 2....
Epoch has taken 0:00:17.821321
Number of used sentences in train = 313
Total loss for epoch 2: 730.511019
	Epoch 3....
Epoch has taken 0:00:17.812855
Number of used sentences in train = 313
Total loss for epoch 3: 658.637974
	Epoch 4....
Epoch has taken 0:00:17.805215
Number of used sentences in train = 313
Total loss for epoch 4: 619.163953
	Epoch 5....
Epoch has taken 0:00:17.802007
Number of used sentences in train = 313
Total loss for epoch 5: 581.789993
	Epoch 6....
Epoch has taken 0:00:17.805079
Number of used sentences in train = 313
Total loss for epoch 6: 570.214093
	Epoch 7....
Epoch has taken 0:00:17.805527
Number of used sentences in train = 313
Total loss for epoch 7: 546.860999
	Epoch 8....
Epoch has taken 0:00:17.798302
Number of used sentences in train = 313
Total loss for epoch 8: 539.559346
	Epoch 9....
Epoch has taken 0:00:17.801466
Number of used sentences in train = 313
Total loss for epoch 9: 534.254717
	Epoch 10....
Epoch has taken 0:00:17.807789
Number of used sentences in train = 313
Total loss for epoch 10: 528.092586
	Epoch 11....
Epoch has taken 0:00:17.799308
Number of used sentences in train = 313
Total loss for epoch 11: 522.332099
	Epoch 12....
Epoch has taken 0:00:17.775724
Number of used sentences in train = 313
Total loss for epoch 12: 521.783040
	Epoch 13....
Epoch has taken 0:00:17.786918
Number of used sentences in train = 313
Total loss for epoch 13: 520.336762
	Epoch 14....
Epoch has taken 0:00:17.786220
Number of used sentences in train = 313
Total loss for epoch 14: 516.877197
Epoch has taken 0:00:17.777656

==================================================================================================
	Training time : 0:46:30.648294
==================================================================================================
	Identification : 0.41

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1680, 106)
  (lstm): LSTM(123, 125, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 12469.637342
validation loss after epoch 0 : 788.966226
	Epoch 1....
Epoch has taken 0:01:55.174315
Number of used sentences in train = 2074
Total loss for epoch 1: 6337.725876
validation loss after epoch 1 : 705.882797
	Epoch 2....
Epoch has taken 0:01:55.304583
Number of used sentences in train = 2074
Total loss for epoch 2: 5421.052747
validation loss after epoch 2 : 750.338812
	Epoch 3....
Epoch has taken 0:01:55.422911
Number of used sentences in train = 2074
Total loss for epoch 3: 4883.542502
validation loss after epoch 3 : 761.351170
	Epoch 4....
Epoch has taken 0:01:55.225044
Number of used sentences in train = 2074
Total loss for epoch 4: 4459.564291
validation loss after epoch 4 : 791.950287
	Epoch 5....
Epoch has taken 0:01:55.308190
Number of used sentences in train = 2074
Total loss for epoch 5: 4203.967287
validation loss after epoch 5 : 804.503467
	Epoch 6....
Epoch has taken 0:01:55.253201
Number of used sentences in train = 2074
Total loss for epoch 6: 3993.667857
validation loss after epoch 6 : 854.653183
	Epoch 7....
Epoch has taken 0:01:55.229781
Number of used sentences in train = 2074
Total loss for epoch 7: 3763.967086
validation loss after epoch 7 : 842.845424
	Epoch 8....
Epoch has taken 0:01:55.402225
Number of used sentences in train = 2074
Total loss for epoch 8: 3693.415696
validation loss after epoch 8 : 874.209452
	Epoch 9....
Epoch has taken 0:01:55.291566
Number of used sentences in train = 2074
Total loss for epoch 9: 3594.539655
validation loss after epoch 9 : 908.993389
	Epoch 10....
Epoch has taken 0:01:55.258238
Number of used sentences in train = 2074
Total loss for epoch 10: 3486.997214
validation loss after epoch 10 : 942.782597
	Epoch 11....
Epoch has taken 0:01:55.390408
Number of used sentences in train = 2074
Total loss for epoch 11: 3458.707083
validation loss after epoch 11 : 995.897178
	Epoch 12....
Epoch has taken 0:01:55.373368
Number of used sentences in train = 2074
Total loss for epoch 12: 3421.206244
validation loss after epoch 12 : 986.500243
	Epoch 13....
Epoch has taken 0:01:55.275338
Number of used sentences in train = 2074
Total loss for epoch 13: 3406.988638
validation loss after epoch 13 : 1029.167266
	Epoch 14....
Epoch has taken 0:01:55.322743
Number of used sentences in train = 2074
Total loss for epoch 14: 3375.656034
validation loss after epoch 14 : 1063.646498
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1680, 106)
  (lstm): LSTM(123, 125, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:55.321491
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1429.368843
	Epoch 1....
Epoch has taken 0:00:11.757608
Number of used sentences in train = 231
Total loss for epoch 1: 775.058216
	Epoch 2....
Epoch has taken 0:00:11.753097
Number of used sentences in train = 231
Total loss for epoch 2: 620.934727
	Epoch 3....
Epoch has taken 0:00:11.752668
Number of used sentences in train = 231
Total loss for epoch 3: 526.276856
	Epoch 4....
Epoch has taken 0:00:11.745209
Number of used sentences in train = 231
Total loss for epoch 4: 468.695481
	Epoch 5....
Epoch has taken 0:00:11.742528
Number of used sentences in train = 231
Total loss for epoch 5: 426.210055
	Epoch 6....
Epoch has taken 0:00:11.762406
Number of used sentences in train = 231
Total loss for epoch 6: 387.121982
	Epoch 7....
Epoch has taken 0:00:11.747441
Number of used sentences in train = 231
Total loss for epoch 7: 370.362710
	Epoch 8....
Epoch has taken 0:00:11.763639
Number of used sentences in train = 231
Total loss for epoch 8: 371.452872
	Epoch 9....
Epoch has taken 0:00:11.758096
Number of used sentences in train = 231
Total loss for epoch 9: 359.983554
	Epoch 10....
Epoch has taken 0:00:11.756862
Number of used sentences in train = 231
Total loss for epoch 10: 351.552178
	Epoch 11....
Epoch has taken 0:00:11.751309
Number of used sentences in train = 231
Total loss for epoch 11: 350.777514
	Epoch 12....
Epoch has taken 0:00:11.755137
Number of used sentences in train = 231
Total loss for epoch 12: 350.296390
	Epoch 13....
Epoch has taken 0:00:11.754285
Number of used sentences in train = 231
Total loss for epoch 13: 350.036799
	Epoch 14....
Epoch has taken 0:00:11.754396
Number of used sentences in train = 231
Total loss for epoch 14: 348.547958
Epoch has taken 0:00:11.753761

==================================================================================================
	Training time : 0:31:46.203735
==================================================================================================
	Identification : 0.443

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(3369, 106)
  (lstm): LSTM(123, 125, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 37951.032165
validation loss after epoch 0 : 3943.909765
	Epoch 1....
Epoch has taken 0:03:46.041610
Number of used sentences in train = 3226
Total loss for epoch 1: 36837.608253
validation loss after epoch 1 : 3920.757970
	Epoch 2....
Epoch has taken 0:03:46.089177
Number of used sentences in train = 3226
Total loss for epoch 2: 36736.329087
validation loss after epoch 2 : 3978.245610
	Epoch 3....
Epoch has taken 0:03:46.309741
Number of used sentences in train = 3226
Total loss for epoch 3: 36965.248738
validation loss after epoch 3 : 4438.969805
	Epoch 4....
Epoch has taken 0:03:46.520457
Number of used sentences in train = 3226
Total loss for epoch 4: 25279.876146
validation loss after epoch 4 : 1329.993710
	Epoch 5....
Epoch has taken 0:03:46.112437
Number of used sentences in train = 3226
Total loss for epoch 5: 10703.235446
validation loss after epoch 5 : 1092.419596
	Epoch 6....
Epoch has taken 0:03:46.120299
Number of used sentences in train = 3226
Total loss for epoch 6: 9396.117037
validation loss after epoch 6 : 1089.590751
	Epoch 7....
Epoch has taken 0:03:45.914361
Number of used sentences in train = 3226
Total loss for epoch 7: 8600.929844
validation loss after epoch 7 : 1061.238555
	Epoch 8....
Epoch has taken 0:03:45.997632
Number of used sentences in train = 3226
Total loss for epoch 8: 8195.137473
validation loss after epoch 8 : 1064.868284
	Epoch 9....
Epoch has taken 0:03:46.217543
Number of used sentences in train = 3226
Total loss for epoch 9: 7742.865838
validation loss after epoch 9 : 1208.787727
	Epoch 10....
Epoch has taken 0:03:46.132725
Number of used sentences in train = 3226
Total loss for epoch 10: 7530.471034
validation loss after epoch 10 : 1173.526143
	Epoch 11....
Epoch has taken 0:03:46.335562
Number of used sentences in train = 3226
Total loss for epoch 11: 7253.001134
validation loss after epoch 11 : 1278.964954
	Epoch 12....
Epoch has taken 0:03:46.061756
Number of used sentences in train = 3226
Total loss for epoch 12: 7023.463775
validation loss after epoch 12 : 1258.985950
	Epoch 13....
Epoch has taken 0:03:46.185833
Number of used sentences in train = 3226
Total loss for epoch 13: 6844.604715
validation loss after epoch 13 : 1312.369704
	Epoch 14....
Epoch has taken 0:03:46.194069
Number of used sentences in train = 3226
Total loss for epoch 14: 6743.810885
validation loss after epoch 14 : 1406.484687
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(3369, 106)
  (lstm): LSTM(123, 125, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:46.212501
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1591.826860
	Epoch 1....
Epoch has taken 0:00:22.137066
Number of used sentences in train = 359
Total loss for epoch 1: 1159.332211
	Epoch 2....
Epoch has taken 0:00:22.135351
Number of used sentences in train = 359
Total loss for epoch 2: 988.366912
	Epoch 3....
Epoch has taken 0:00:22.121758
Number of used sentences in train = 359
Total loss for epoch 3: 944.395780
	Epoch 4....
Epoch has taken 0:00:22.163876
Number of used sentences in train = 359
Total loss for epoch 4: 873.307770
	Epoch 5....
Epoch has taken 0:00:22.143035
Number of used sentences in train = 359
Total loss for epoch 5: 811.093223
	Epoch 6....
Epoch has taken 0:00:22.140175
Number of used sentences in train = 359
Total loss for epoch 6: 777.726192
	Epoch 7....
Epoch has taken 0:00:22.103928
Number of used sentences in train = 359
Total loss for epoch 7: 743.610581
	Epoch 8....
Epoch has taken 0:00:22.117933
Number of used sentences in train = 359
Total loss for epoch 8: 723.300603
	Epoch 9....
Epoch has taken 0:00:22.138627
Number of used sentences in train = 359
Total loss for epoch 9: 715.464762
	Epoch 10....
Epoch has taken 0:00:22.131213
Number of used sentences in train = 359
Total loss for epoch 10: 722.244718
	Epoch 11....
Epoch has taken 0:00:22.127742
Number of used sentences in train = 359
Total loss for epoch 11: 707.359221
	Epoch 12....
Epoch has taken 0:00:22.114782
Number of used sentences in train = 359
Total loss for epoch 12: 697.998322
	Epoch 13....
Epoch has taken 0:00:22.121995
Number of used sentences in train = 359
Total loss for epoch 13: 697.840090
	Epoch 14....
Epoch has taken 0:00:22.124027
Number of used sentences in train = 359
Total loss for epoch 14: 700.945689
Epoch has taken 0:00:22.141491

==================================================================================================
	Training time : 1:02:05.079017
==================================================================================================
	Identification : 0.015

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 83, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 38, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 94, 'lstmDropout': 0.26, 'denseActivation': 'tanh', 'wordDim': 88, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(5893, 88)
  (lstm): LSTM(126, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13195.109567
validation loss after epoch 0 : 1092.214289
	Epoch 1....
Epoch has taken 0:02:36.836368
Number of used sentences in train = 2811
Total loss for epoch 1: 8569.208470
validation loss after epoch 1 : 1038.190071
	Epoch 2....
Epoch has taken 0:02:37.231939
Number of used sentences in train = 2811
Total loss for epoch 2: 6832.647055
validation loss after epoch 2 : 1076.071682
	Epoch 3....
Epoch has taken 0:02:36.916946
Number of used sentences in train = 2811
Total loss for epoch 3: 5789.513107
validation loss after epoch 3 : 1180.830120
	Epoch 4....
Epoch has taken 0:02:36.785262
Number of used sentences in train = 2811
Total loss for epoch 4: 5256.170794
validation loss after epoch 4 : 1273.278137
	Epoch 5....
Epoch has taken 0:02:36.754895
Number of used sentences in train = 2811
Total loss for epoch 5: 4942.321264
validation loss after epoch 5 : 1334.823962
	Epoch 6....
Epoch has taken 0:02:37.086689
Number of used sentences in train = 2811
Total loss for epoch 6: 4742.332140
validation loss after epoch 6 : 1383.078321
	Epoch 7....
Epoch has taken 0:02:37.069644
Number of used sentences in train = 2811
Total loss for epoch 7: 4659.102077
validation loss after epoch 7 : 1437.926686
	Epoch 8....
Epoch has taken 0:02:36.789879
Number of used sentences in train = 2811
Total loss for epoch 8: 4599.498396
validation loss after epoch 8 : 1472.324512
	Epoch 9....
Epoch has taken 0:02:36.894719
Number of used sentences in train = 2811
Total loss for epoch 9: 4569.491669
validation loss after epoch 9 : 1496.707621
	Epoch 10....
Epoch has taken 0:02:36.798924
Number of used sentences in train = 2811
Total loss for epoch 10: 4556.549941
validation loss after epoch 10 : 1511.685494
	Epoch 11....
Epoch has taken 0:02:37.000785
Number of used sentences in train = 2811
Total loss for epoch 11: 4534.337436
validation loss after epoch 11 : 1537.592637
	Epoch 12....
Epoch has taken 0:02:37.012172
Number of used sentences in train = 2811
Total loss for epoch 12: 4524.119963
validation loss after epoch 12 : 1556.661237
	Epoch 13....
Epoch has taken 0:02:37.081642
Number of used sentences in train = 2811
Total loss for epoch 13: 4514.274180
validation loss after epoch 13 : 1565.450895
	Epoch 14....
Epoch has taken 0:02:37.064415
Number of used sentences in train = 2811
Total loss for epoch 14: 4508.101110
validation loss after epoch 14 : 1591.698601
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(5893, 88)
  (lstm): LSTM(126, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:36.978288
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2103.771697
	Epoch 1....
Epoch has taken 0:00:16.611125
Number of used sentences in train = 313
Total loss for epoch 1: 875.796491
	Epoch 2....
Epoch has taken 0:00:16.622424
Number of used sentences in train = 313
Total loss for epoch 2: 649.532298
	Epoch 3....
Epoch has taken 0:00:16.619918
Number of used sentences in train = 313
Total loss for epoch 3: 573.565281
	Epoch 4....
Epoch has taken 0:00:16.613456
Number of used sentences in train = 313
Total loss for epoch 4: 542.552308
	Epoch 5....
Epoch has taken 0:00:16.600256
Number of used sentences in train = 313
Total loss for epoch 5: 529.921618
	Epoch 6....
Epoch has taken 0:00:16.592965
Number of used sentences in train = 313
Total loss for epoch 6: 521.386424
	Epoch 7....
Epoch has taken 0:00:16.610829
Number of used sentences in train = 313
Total loss for epoch 7: 516.782516
	Epoch 8....
Epoch has taken 0:00:16.594278
Number of used sentences in train = 313
Total loss for epoch 8: 514.328510
	Epoch 9....
Epoch has taken 0:00:16.606577
Number of used sentences in train = 313
Total loss for epoch 9: 511.817209
	Epoch 10....
Epoch has taken 0:00:16.607835
Number of used sentences in train = 313
Total loss for epoch 10: 509.944467
	Epoch 11....
Epoch has taken 0:00:16.613237
Number of used sentences in train = 313
Total loss for epoch 11: 508.042721
	Epoch 12....
Epoch has taken 0:00:16.599104
Number of used sentences in train = 313
Total loss for epoch 12: 507.473709
	Epoch 13....
Epoch has taken 0:00:16.593922
Number of used sentences in train = 313
Total loss for epoch 13: 507.058406
	Epoch 14....
Epoch has taken 0:00:16.600217
Number of used sentences in train = 313
Total loss for epoch 14: 506.661803
Epoch has taken 0:00:16.597648

==================================================================================================
	Training time : 0:43:23.883654
==================================================================================================
	Identification : 0.133

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(5619, 88)
  (lstm): LSTM(126, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9748.673517
validation loss after epoch 0 : 949.844851
	Epoch 1....
Epoch has taken 0:01:47.366104
Number of used sentences in train = 2074
Total loss for epoch 1: 6154.310698
validation loss after epoch 1 : 842.323568
	Epoch 2....
Epoch has taken 0:01:47.450769
Number of used sentences in train = 2074
Total loss for epoch 2: 4788.176877
validation loss after epoch 2 : 968.442822
	Epoch 3....
Epoch has taken 0:01:47.446355
Number of used sentences in train = 2074
Total loss for epoch 3: 3980.618608
validation loss after epoch 3 : 1074.933977
	Epoch 4....
Epoch has taken 0:01:47.493709
Number of used sentences in train = 2074
Total loss for epoch 4: 3549.010144
validation loss after epoch 4 : 1170.625096
	Epoch 5....
Epoch has taken 0:01:47.477142
Number of used sentences in train = 2074
Total loss for epoch 5: 3358.384869
validation loss after epoch 5 : 1228.566607
	Epoch 6....
Epoch has taken 0:01:47.471699
Number of used sentences in train = 2074
Total loss for epoch 6: 3278.869147
validation loss after epoch 6 : 1284.251757
	Epoch 7....
Epoch has taken 0:01:47.527555
Number of used sentences in train = 2074
Total loss for epoch 7: 3233.528540
validation loss after epoch 7 : 1316.990003
	Epoch 8....
Epoch has taken 0:01:47.482257
Number of used sentences in train = 2074
Total loss for epoch 8: 3209.739955
validation loss after epoch 8 : 1367.027649
	Epoch 9....
Epoch has taken 0:01:47.496295
Number of used sentences in train = 2074
Total loss for epoch 9: 3193.638778
validation loss after epoch 9 : 1387.972389
	Epoch 10....
Epoch has taken 0:01:47.511315
Number of used sentences in train = 2074
Total loss for epoch 10: 3185.668894
validation loss after epoch 10 : 1419.350425
	Epoch 11....
Epoch has taken 0:01:47.463438
Number of used sentences in train = 2074
Total loss for epoch 11: 3177.204391
validation loss after epoch 11 : 1435.562434
	Epoch 12....
Epoch has taken 0:01:47.482211
Number of used sentences in train = 2074
Total loss for epoch 12: 3170.758264
validation loss after epoch 12 : 1460.856128
	Epoch 13....
Epoch has taken 0:01:47.433025
Number of used sentences in train = 2074
Total loss for epoch 13: 3168.002856
validation loss after epoch 13 : 1470.720636
	Epoch 14....
Epoch has taken 0:01:47.415456
Number of used sentences in train = 2074
Total loss for epoch 14: 3165.527285
validation loss after epoch 14 : 1490.595780
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(5619, 88)
  (lstm): LSTM(126, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:47.440376
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1759.041025
	Epoch 1....
Epoch has taken 0:00:10.934614
Number of used sentences in train = 231
Total loss for epoch 1: 604.089318
	Epoch 2....
Epoch has taken 0:00:10.932659
Number of used sentences in train = 231
Total loss for epoch 2: 447.846235
	Epoch 3....
Epoch has taken 0:00:10.921119
Number of used sentences in train = 231
Total loss for epoch 3: 383.457944
	Epoch 4....
Epoch has taken 0:00:10.937957
Number of used sentences in train = 231
Total loss for epoch 4: 367.557779
	Epoch 5....
Epoch has taken 0:00:10.931036
Number of used sentences in train = 231
Total loss for epoch 5: 358.386499
	Epoch 6....
Epoch has taken 0:00:10.929590
Number of used sentences in train = 231
Total loss for epoch 6: 354.286733
	Epoch 7....
Epoch has taken 0:00:10.920426
Number of used sentences in train = 231
Total loss for epoch 7: 353.454848
	Epoch 8....
Epoch has taken 0:00:10.920600
Number of used sentences in train = 231
Total loss for epoch 8: 350.772038
	Epoch 9....
Epoch has taken 0:00:10.910995
Number of used sentences in train = 231
Total loss for epoch 9: 349.540349
	Epoch 10....
Epoch has taken 0:00:10.912722
Number of used sentences in train = 231
Total loss for epoch 10: 348.148064
	Epoch 11....
Epoch has taken 0:00:10.924911
Number of used sentences in train = 231
Total loss for epoch 11: 347.612210
	Epoch 12....
Epoch has taken 0:00:10.920496
Number of used sentences in train = 231
Total loss for epoch 12: 347.091378
	Epoch 13....
Epoch has taken 0:00:10.922546
Number of used sentences in train = 231
Total loss for epoch 13: 346.818662
	Epoch 14....
Epoch has taken 0:00:10.923382
Number of used sentences in train = 231
Total loss for epoch 14: 346.415205
Epoch has taken 0:00:10.914417

==================================================================================================
	Training time : 0:29:36.156409
==================================================================================================
	Identification : 0.345

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(6880, 88)
  (lstm): LSTM(126, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17910.702904
validation loss after epoch 0 : 1534.448007
	Epoch 1....
Epoch has taken 0:03:30.647076
Number of used sentences in train = 3226
Total loss for epoch 1: 11595.533277
validation loss after epoch 1 : 1469.192822
	Epoch 2....
Epoch has taken 0:03:30.842312
Number of used sentences in train = 3226
Total loss for epoch 2: 9726.062895
validation loss after epoch 2 : 1517.744104
	Epoch 3....
Epoch has taken 0:03:30.971714
Number of used sentences in train = 3226
Total loss for epoch 3: 8451.919159
validation loss after epoch 3 : 1612.951844
	Epoch 4....
Epoch has taken 0:03:31.197845
Number of used sentences in train = 3226
Total loss for epoch 4: 7452.227827
validation loss after epoch 4 : 1749.960556
	Epoch 5....
Epoch has taken 0:03:31.011209
Number of used sentences in train = 3226
Total loss for epoch 5: 6880.552708
validation loss after epoch 5 : 1912.666960
	Epoch 6....
Epoch has taken 0:03:30.939471
Number of used sentences in train = 3226
Total loss for epoch 6: 6564.226446
validation loss after epoch 6 : 2047.879406
	Epoch 7....
Epoch has taken 0:03:30.990146
Number of used sentences in train = 3226
Total loss for epoch 7: 6380.450368
validation loss after epoch 7 : 2149.718991
	Epoch 8....
Epoch has taken 0:03:31.004309
Number of used sentences in train = 3226
Total loss for epoch 8: 6285.799171
validation loss after epoch 8 : 2190.328251
	Epoch 9....
Epoch has taken 0:03:30.769123
Number of used sentences in train = 3226
Total loss for epoch 9: 6233.595730
validation loss after epoch 9 : 2242.279711
	Epoch 10....
Epoch has taken 0:03:30.792986
Number of used sentences in train = 3226
Total loss for epoch 10: 6199.083265
validation loss after epoch 10 : 2325.548981
	Epoch 11....
Epoch has taken 0:03:30.826485
Number of used sentences in train = 3226
Total loss for epoch 11: 6173.236488
validation loss after epoch 11 : 2330.501410
	Epoch 12....
Epoch has taken 0:03:30.826068
Number of used sentences in train = 3226
Total loss for epoch 12: 6162.957214
validation loss after epoch 12 : 2382.858526
	Epoch 13....
Epoch has taken 0:03:30.879147
Number of used sentences in train = 3226
Total loss for epoch 13: 6152.700526
validation loss after epoch 13 : 2397.303403
	Epoch 14....
Epoch has taken 0:03:31.118817
Number of used sentences in train = 3226
Total loss for epoch 14: 6143.321501
validation loss after epoch 14 : 2448.698100
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(6880, 88)
  (lstm): LSTM(126, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=83, bias=True)
  (linear2): Linear(in_features=83, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:31.112658
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2031.633843
	Epoch 1....
Epoch has taken 0:00:20.579449
Number of used sentences in train = 359
Total loss for epoch 1: 1136.979091
	Epoch 2....
Epoch has taken 0:00:20.570834
Number of used sentences in train = 359
Total loss for epoch 2: 832.091407
	Epoch 3....
Epoch has taken 0:00:20.550053
Number of used sentences in train = 359
Total loss for epoch 3: 711.821360
	Epoch 4....
Epoch has taken 0:00:20.556906
Number of used sentences in train = 359
Total loss for epoch 4: 677.592377
	Epoch 5....
Epoch has taken 0:00:20.561349
Number of used sentences in train = 359
Total loss for epoch 5: 673.190130
	Epoch 6....
Epoch has taken 0:00:20.563954
Number of used sentences in train = 359
Total loss for epoch 6: 672.190312
	Epoch 7....
Epoch has taken 0:00:20.547544
Number of used sentences in train = 359
Total loss for epoch 7: 671.696431
	Epoch 8....
Epoch has taken 0:00:20.561606
Number of used sentences in train = 359
Total loss for epoch 8: 671.370074
	Epoch 9....
Epoch has taken 0:00:20.558586
Number of used sentences in train = 359
Total loss for epoch 9: 671.145185
	Epoch 10....
Epoch has taken 0:00:20.552694
Number of used sentences in train = 359
Total loss for epoch 10: 670.951414
	Epoch 11....
Epoch has taken 0:00:20.554816
Number of used sentences in train = 359
Total loss for epoch 11: 671.204695
	Epoch 12....
Epoch has taken 0:00:20.551790
Number of used sentences in train = 359
Total loss for epoch 12: 670.720401
	Epoch 13....
Epoch has taken 0:00:20.571452
Number of used sentences in train = 359
Total loss for epoch 13: 670.937147
	Epoch 14....
Epoch has taken 0:00:20.561187
Number of used sentences in train = 359
Total loss for epoch 14: 670.781216
Epoch has taken 0:00:20.556786

==================================================================================================
	Training time : 0:57:52.979469
==================================================================================================
	Identification : 0.046

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 92, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 57, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 20, 'lstmDropout': 0.14, 'denseActivation': 'tanh', 'wordDim': 85, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 57)
  (w_embeddings): Embedding(1882, 85)
  (lstm): LSTM(142, 20, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=92, bias=True)
  (linear2): Linear(in_features=92, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10534.952474
validation loss after epoch 0 : 824.636433
	Epoch 1....
Epoch has taken 0:02:47.150936
Number of used sentences in train = 2811
Total loss for epoch 1: 7458.889511
validation loss after epoch 1 : 811.617573
	Epoch 2....
Epoch has taken 0:02:46.884488
Number of used sentences in train = 2811
Total loss for epoch 2: 6811.254463
validation loss after epoch 2 : 816.392311
	Epoch 3....
Epoch has taken 0:02:46.978212
Number of used sentences in train = 2811
Total loss for epoch 3: 6408.976378
validation loss after epoch 3 : 848.582063
	Epoch 4....
Epoch has taken 0:02:47.074650
Number of used sentences in train = 2811
Total loss for epoch 4: 6112.250365
validation loss after epoch 4 : 826.612558
	Epoch 5....
Epoch has taken 0:02:47.080228
Number of used sentences in train = 2811
Total loss for epoch 5: 5876.914616
validation loss after epoch 5 : 880.342381
	Epoch 6....
Epoch has taken 0:02:46.923270
Number of used sentences in train = 2811
Total loss for epoch 6: 5669.803582
validation loss after epoch 6 : 914.182081
	Epoch 7....
Epoch has taken 0:02:47.004837
Number of used sentences in train = 2811
Total loss for epoch 7: 5559.825928
validation loss after epoch 7 : 882.829182
	Epoch 8....
Epoch has taken 0:02:46.956849
Number of used sentences in train = 2811
Total loss for epoch 8: 5418.120131
validation loss after epoch 8 : 911.825779
	Epoch 9....
Epoch has taken 0:02:47.116195
Number of used sentences in train = 2811
Total loss for epoch 9: 5287.907120
validation loss after epoch 9 : 949.639099
	Epoch 10....
Epoch has taken 0:02:46.975648
Number of used sentences in train = 2811
Total loss for epoch 10: 5209.248639
validation loss after epoch 10 : 945.720377
	Epoch 11....
Epoch has taken 0:02:47.035782
Number of used sentences in train = 2811
Total loss for epoch 11: 5097.171101
validation loss after epoch 11 : 976.759269
	Epoch 12....
Epoch has taken 0:02:46.960227
Number of used sentences in train = 2811
Total loss for epoch 12: 5070.405110
validation loss after epoch 12 : 949.742928
	Epoch 13....
Epoch has taken 0:02:47.114607
Number of used sentences in train = 2811
Total loss for epoch 13: 5021.491061
validation loss after epoch 13 : 1010.196418
	Epoch 14....
Epoch has taken 0:02:47.282183
Number of used sentences in train = 2811
Total loss for epoch 14: 4940.793676
validation loss after epoch 14 : 1016.865864
	TransitionClassifier(
  (p_embeddings): Embedding(18, 57)
  (w_embeddings): Embedding(1882, 85)
  (lstm): LSTM(142, 20, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=92, bias=True)
  (linear2): Linear(in_features=92, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.025959
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1418.991813
	Epoch 1....
Epoch has taken 0:00:17.651047
Number of used sentences in train = 313
Total loss for epoch 1: 734.048949
	Epoch 2....
Epoch has taken 0:00:17.657725
Number of used sentences in train = 313
Total loss for epoch 2: 630.853985
	Epoch 3....
Epoch has taken 0:00:17.647670
Number of used sentences in train = 313
Total loss for epoch 3: 594.107200
	Epoch 4....
Epoch has taken 0:00:17.655344
Number of used sentences in train = 313
Total loss for epoch 4: 548.250719
	Epoch 5....
Epoch has taken 0:00:17.654621
Number of used sentences in train = 313
Total loss for epoch 5: 544.801048
	Epoch 6....
Epoch has taken 0:00:17.655511
Number of used sentences in train = 313
Total loss for epoch 6: 537.021757
	Epoch 7....
Epoch has taken 0:00:17.653066
Number of used sentences in train = 313
Total loss for epoch 7: 520.750452
	Epoch 8....
Epoch has taken 0:00:17.652560
Number of used sentences in train = 313
Total loss for epoch 8: 515.674864
	Epoch 9....
Epoch has taken 0:00:17.662051
Number of used sentences in train = 313
Total loss for epoch 9: 518.786460
	Epoch 10....
Epoch has taken 0:00:17.664754
Number of used sentences in train = 313
Total loss for epoch 10: 512.714449
	Epoch 11....
Epoch has taken 0:00:17.655186
Number of used sentences in train = 313
Total loss for epoch 11: 510.167350
	Epoch 12....
Epoch has taken 0:00:17.643175
Number of used sentences in train = 313
Total loss for epoch 12: 513.811664
	Epoch 13....
Epoch has taken 0:00:17.643986
Number of used sentences in train = 313
Total loss for epoch 13: 510.081759
	Epoch 14....
Epoch has taken 0:00:17.646911
Number of used sentences in train = 313
Total loss for epoch 14: 511.269717
Epoch has taken 0:00:17.647248

==================================================================================================
	Training time : 0:46:10.853450
==================================================================================================
	Identification : 0.396

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 57)
  (w_embeddings): Embedding(1680, 85)
  (lstm): LSTM(142, 20, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=92, bias=True)
  (linear2): Linear(in_features=92, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8639.492527
validation loss after epoch 0 : 628.030333
	Epoch 1....
Epoch has taken 0:01:54.185199
Number of used sentences in train = 2074
Total loss for epoch 1: 5415.713239
validation loss after epoch 1 : 677.614725
	Epoch 2....
Epoch has taken 0:01:54.214484
Number of used sentences in train = 2074
Total loss for epoch 2: 4822.792876
validation loss after epoch 2 : 687.438154
	Epoch 3....
Epoch has taken 0:01:54.349300
Number of used sentences in train = 2074
Total loss for epoch 3: 4382.542029
validation loss after epoch 3 : 742.320164
	Epoch 4....
Epoch has taken 0:01:54.377589
Number of used sentences in train = 2074
Total loss for epoch 4: 4138.732445
validation loss after epoch 4 : 772.962522
	Epoch 5....
Epoch has taken 0:01:54.465405
Number of used sentences in train = 2074
Total loss for epoch 5: 3860.393360
validation loss after epoch 5 : 821.428695
	Epoch 6....
Epoch has taken 0:01:54.378575
Number of used sentences in train = 2074
Total loss for epoch 6: 3723.870757
validation loss after epoch 6 : 823.970081
	Epoch 7....
Epoch has taken 0:01:54.457183
Number of used sentences in train = 2074
Total loss for epoch 7: 3603.547787
validation loss after epoch 7 : 881.620548
	Epoch 8....
Epoch has taken 0:01:54.395411
Number of used sentences in train = 2074
Total loss for epoch 8: 3505.210214
validation loss after epoch 8 : 930.695544
	Epoch 9....
Epoch has taken 0:01:54.490757
Number of used sentences in train = 2074
Total loss for epoch 9: 3423.017182
validation loss after epoch 9 : 819.873944
	Epoch 10....
Epoch has taken 0:01:54.338274
Number of used sentences in train = 2074
Total loss for epoch 10: 3386.051027
validation loss after epoch 10 : 864.109632
	Epoch 11....
Epoch has taken 0:01:54.470336
Number of used sentences in train = 2074
Total loss for epoch 11: 3404.609770
validation loss after epoch 11 : 960.174425
	Epoch 12....
Epoch has taken 0:01:54.325222
Number of used sentences in train = 2074
Total loss for epoch 12: 3343.876816
validation loss after epoch 12 : 952.905573
	Epoch 13....
Epoch has taken 0:01:54.400083
Number of used sentences in train = 2074
Total loss for epoch 13: 3304.967717
validation loss after epoch 13 : 985.939841
	Epoch 14....
Epoch has taken 0:01:54.309510
Number of used sentences in train = 2074
Total loss for epoch 14: 3274.077278
validation loss after epoch 14 : 943.403101
	TransitionClassifier(
  (p_embeddings): Embedding(18, 57)
  (w_embeddings): Embedding(1680, 85)
  (lstm): LSTM(142, 20, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=92, bias=True)
  (linear2): Linear(in_features=92, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.470076
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1896.406686
	Epoch 1....
Epoch has taken 0:00:11.656846
Number of used sentences in train = 231
Total loss for epoch 1: 587.753199
	Epoch 2....
Epoch has taken 0:00:11.653858
Number of used sentences in train = 231
Total loss for epoch 2: 475.103364
	Epoch 3....
Epoch has taken 0:00:11.650665
Number of used sentences in train = 231
Total loss for epoch 3: 422.074122
	Epoch 4....
Epoch has taken 0:00:11.652664
Number of used sentences in train = 231
Total loss for epoch 4: 386.193417
	Epoch 5....
Epoch has taken 0:00:11.650957
Number of used sentences in train = 231
Total loss for epoch 5: 380.586536
	Epoch 6....
Epoch has taken 0:00:11.653543
Number of used sentences in train = 231
Total loss for epoch 6: 370.667345
	Epoch 7....
Epoch has taken 0:00:11.653808
Number of used sentences in train = 231
Total loss for epoch 7: 356.081861
	Epoch 8....
Epoch has taken 0:00:11.654057
Number of used sentences in train = 231
Total loss for epoch 8: 360.069408
	Epoch 9....
Epoch has taken 0:00:11.643586
Number of used sentences in train = 231
Total loss for epoch 9: 354.276199
	Epoch 10....
Epoch has taken 0:00:11.651870
Number of used sentences in train = 231
Total loss for epoch 10: 354.296649
	Epoch 11....
Epoch has taken 0:00:11.647264
Number of used sentences in train = 231
Total loss for epoch 11: 357.785155
	Epoch 12....
Epoch has taken 0:00:11.653352
Number of used sentences in train = 231
Total loss for epoch 12: 350.830214
	Epoch 13....
Epoch has taken 0:00:11.648311
Number of used sentences in train = 231
Total loss for epoch 13: 350.975531
	Epoch 14....
Epoch has taken 0:00:11.645874
Number of used sentences in train = 231
Total loss for epoch 14: 352.711449
Epoch has taken 0:00:11.649088

==================================================================================================
	Training time : 0:31:30.726229
==================================================================================================
	Identification : 0.284

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 57)
  (w_embeddings): Embedding(3369, 85)
  (lstm): LSTM(142, 20, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=92, bias=True)
  (linear2): Linear(in_features=92, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12317.867562
validation loss after epoch 0 : 1080.866673
	Epoch 1....
Epoch has taken 0:03:41.553765
Number of used sentences in train = 3226
Total loss for epoch 1: 9422.762282
validation loss after epoch 1 : 1066.347493
	Epoch 2....
Epoch has taken 0:03:41.622520
Number of used sentences in train = 3226
Total loss for epoch 2: 8804.537362
validation loss after epoch 2 : 1049.072313
	Epoch 3....
Epoch has taken 0:03:41.653899
Number of used sentences in train = 3226
Total loss for epoch 3: 8329.600887
validation loss after epoch 3 : 1065.199899
	Epoch 4....
Epoch has taken 0:03:41.641437
Number of used sentences in train = 3226
Total loss for epoch 4: 7946.045390
validation loss after epoch 4 : 1097.803003
	Epoch 5....
Epoch has taken 0:03:41.816617
Number of used sentences in train = 3226
Total loss for epoch 5: 7663.562750
validation loss after epoch 5 : 1149.604939
	Epoch 6....
Epoch has taken 0:03:41.906170
Number of used sentences in train = 3226
Total loss for epoch 6: 7459.342957
validation loss after epoch 6 : 1151.660276
	Epoch 7....
Epoch has taken 0:03:41.662532
Number of used sentences in train = 3226
Total loss for epoch 7: 7287.378674
validation loss after epoch 7 : 1210.693205
	Epoch 8....
Epoch has taken 0:03:41.879910
Number of used sentences in train = 3226
Total loss for epoch 8: 7150.031806
validation loss after epoch 8 : 1264.066690
	Epoch 9....
Epoch has taken 0:03:41.314461
Number of used sentences in train = 3226
Total loss for epoch 9: 6986.316083
validation loss after epoch 9 : 1244.513442
	Epoch 10....
Epoch has taken 0:03:41.335814
Number of used sentences in train = 3226
Total loss for epoch 10: 6876.343573
validation loss after epoch 10 : 1302.465864
	Epoch 11....
Epoch has taken 0:03:41.200524
Number of used sentences in train = 3226
Total loss for epoch 11: 6809.429771
validation loss after epoch 11 : 1315.486105
	Epoch 12....
Epoch has taken 0:03:41.364655
Number of used sentences in train = 3226
Total loss for epoch 12: 6731.768234
validation loss after epoch 12 : 1382.566267
	Epoch 13....
Epoch has taken 0:03:41.340901
Number of used sentences in train = 3226
Total loss for epoch 13: 6693.917794
validation loss after epoch 13 : 1386.853883
	Epoch 14....
Epoch has taken 0:03:41.614738
Number of used sentences in train = 3226
Total loss for epoch 14: 6624.677654
validation loss after epoch 14 : 1400.788898
	TransitionClassifier(
  (p_embeddings): Embedding(13, 57)
  (w_embeddings): Embedding(3369, 85)
  (lstm): LSTM(142, 20, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=92, bias=True)
  (linear2): Linear(in_features=92, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:41.206512
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1610.798909
	Epoch 1....
Epoch has taken 0:00:21.626863
Number of used sentences in train = 359
Total loss for epoch 1: 922.467398
	Epoch 2....
Epoch has taken 0:00:21.610276
Number of used sentences in train = 359
Total loss for epoch 2: 818.532688
	Epoch 3....
Epoch has taken 0:00:21.636838
Number of used sentences in train = 359
Total loss for epoch 3: 768.015022
	Epoch 4....
Epoch has taken 0:00:21.635407
Number of used sentences in train = 359
Total loss for epoch 4: 726.689248
	Epoch 5....
Epoch has taken 0:00:21.632486
Number of used sentences in train = 359
Total loss for epoch 5: 710.427858
	Epoch 6....
Epoch has taken 0:00:21.627606
Number of used sentences in train = 359
Total loss for epoch 6: 701.788879
	Epoch 7....
Epoch has taken 0:00:21.639278
Number of used sentences in train = 359
Total loss for epoch 7: 687.508028
	Epoch 8....
Epoch has taken 0:00:21.648444
Number of used sentences in train = 359
Total loss for epoch 8: 689.941847
	Epoch 9....
Epoch has taken 0:00:21.624044
Number of used sentences in train = 359
Total loss for epoch 9: 680.873593
	Epoch 10....
Epoch has taken 0:00:21.622295
Number of used sentences in train = 359
Total loss for epoch 10: 679.685208
	Epoch 11....
Epoch has taken 0:00:21.618464
Number of used sentences in train = 359
Total loss for epoch 11: 676.407134
	Epoch 12....
Epoch has taken 0:00:21.629347
Number of used sentences in train = 359
Total loss for epoch 12: 673.255488
	Epoch 13....
Epoch has taken 0:00:21.621857
Number of used sentences in train = 359
Total loss for epoch 13: 682.835647
	Epoch 14....
Epoch has taken 0:00:21.614383
Number of used sentences in train = 359
Total loss for epoch 14: 674.626372
Epoch has taken 0:00:21.629713

==================================================================================================
	Training time : 1:00:48.191318
==================================================================================================
	Identification : 0.441

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 19, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 82, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 150, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(9253, 150)
  (lstm): LSTM(169, 82, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1312, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 17029.873536
validation loss after epoch 0 : 1363.094795
	Epoch 1....
Epoch has taken 0:02:47.903148
Number of used sentences in train = 2811
Total loss for epoch 1: 11592.511771
validation loss after epoch 1 : 1199.353627
	Epoch 2....
Epoch has taken 0:02:47.877313
Number of used sentences in train = 2811
Total loss for epoch 2: 9151.090753
validation loss after epoch 2 : 1142.757381
	Epoch 3....
Epoch has taken 0:02:47.964424
Number of used sentences in train = 2811
Total loss for epoch 3: 7708.621848
validation loss after epoch 3 : 1123.272086
	Epoch 4....
Epoch has taken 0:02:48.039026
Number of used sentences in train = 2811
Total loss for epoch 4: 6808.570572
validation loss after epoch 4 : 1167.533878
	Epoch 5....
Epoch has taken 0:02:47.951897
Number of used sentences in train = 2811
Total loss for epoch 5: 6279.556282
validation loss after epoch 5 : 1242.909323
	Epoch 6....
Epoch has taken 0:02:47.758056
Number of used sentences in train = 2811
Total loss for epoch 6: 5869.948078
validation loss after epoch 6 : 1238.841048
	Epoch 7....
Epoch has taken 0:02:47.794271
Number of used sentences in train = 2811
Total loss for epoch 7: 5672.671408
validation loss after epoch 7 : 1311.176792
	Epoch 8....
Epoch has taken 0:02:47.981692
Number of used sentences in train = 2811
Total loss for epoch 8: 5503.853011
validation loss after epoch 8 : 1245.808748
	Epoch 9....
Epoch has taken 0:02:48.200316
Number of used sentences in train = 2811
Total loss for epoch 9: 5384.080739
validation loss after epoch 9 : 1358.983659
	Epoch 10....
Epoch has taken 0:02:48.055315
Number of used sentences in train = 2811
Total loss for epoch 10: 5257.794671
validation loss after epoch 10 : 1416.478828
	Epoch 11....
Epoch has taken 0:02:48.319186
Number of used sentences in train = 2811
Total loss for epoch 11: 5197.512611
validation loss after epoch 11 : 1422.819903
	Epoch 12....
Epoch has taken 0:02:48.124828
Number of used sentences in train = 2811
Total loss for epoch 12: 5105.207514
validation loss after epoch 12 : 1414.876814
	Epoch 13....
Epoch has taken 0:02:48.200032
