INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_iSh7nR.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 10619/11178 Mb (0.950000) on cuda
Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:03:00.0)
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpnco6rP and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqhikgK). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
1000 67
1000 349
1000 273
1000 68
1000 346
1000 457
1000 160
1000 396
1000 791
1000 834
1000 191
1000 484
1000 356
1000 735
1000 474
1000 977
1000 46
1000 674
1000 55
1000 411
1000 667
1000 944
1000 399
1000 809
1000 665
1000 680
1000 637
1000 864
1000 939
1000 783
1000 407
1000 408
1000 217
1000 720
1000 487
1000 387
1000 460
1000 993
1000 989
1000 332
1000 103
1000 682
1000 188
1000 811
1000 175
1000 711
1000 338
1000 239
1000 152
1000 378
1000 714
1000 955
1000 264
1000 686
1000 637
1000 84
5_False_True_357_85_frequent_True_36_relu_0.572_False_512_relu_0.2_adagrad_0.032_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12174

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12174 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12174
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 357)       4346118     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 85)        12920       input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 1428)         0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 340)          0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 1768)         0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 36)           63684       concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 36)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            296         dropout_1[0][0]                  
==================================================================================================
Total params: 4,423,018
Trainable params: 4,423,018
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27106, 1.0: 26494, 2.0: 407, 4.0: 181, 6.0: 103, 5.0: 67, 3.0: 3})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 7s - loss: 0.0474 - acc: 0.9843 - val_loss: 0.0279 - val_acc: 0.9885
Epoch 2/40
 - 7s - loss: 0.0300 - acc: 0.9878 - val_loss: 0.0280 - val_acc: 0.9888
Epoch 3/40
 - 7s - loss: 0.0258 - acc: 0.9889 - val_loss: 0.0295 - val_acc: 0.9889

==================================================================================================
	Training time : 0:01:22.041223
==================================================================================================
	Identification : 0.061
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	25 : 3
	100 : 1
	5 : 4
	0 : 2

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 277
	200 : 1
	100 : 1
	5 : 86
	25 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12139

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12139 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12139
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 357)       4333623     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 85)        12920       input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 1428)         0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 340)          0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 1768)         0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 36)           63684       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 36)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            296         dropout_2[0][0]                  
==================================================================================================
Total params: 4,410,523
Trainable params: 4,410,523
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27001, 1.0: 26618, 2.0: 384, 4.0: 160, 6.0: 112, 5.0: 82, 3.0: 4})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 7s - loss: 0.0472 - acc: 0.9850 - val_loss: 0.0278 - val_acc: 0.9882
Epoch 2/40
 - 7s - loss: 0.0290 - acc: 0.9884 - val_loss: 0.0274 - val_acc: 0.9884
Epoch 3/40
 - 7s - loss: 0.0251 - acc: 0.9891 - val_loss: 0.0287 - val_acc: 0.9883

==================================================================================================
	Training time : 0:01:10.335646
==================================================================================================
	Identification : 0.306
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 8
	100 : 1
	5 : 10
	25 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 273
	25 : 16
	200 : 1
	100 : 1
	5 : 83

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12111

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12111 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12111
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 357)       4323627     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 85)        12920       input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 1428)         0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 340)          0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 1768)         0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 36)           63684       concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 36)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            296         dropout_3[0][0]                  
==================================================================================================
Total params: 4,400,527
Trainable params: 4,400,527
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26846, 1.0: 26765, 2.0: 387, 4.0: 173, 6.0: 115, 5.0: 68, 3.0: 7})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 7s - loss: 0.0461 - acc: 0.9851 - val_loss: 0.0304 - val_acc: 0.9878
Epoch 2/40
 - 7s - loss: 0.0304 - acc: 0.9881 - val_loss: 0.0311 - val_acc: 0.9880
Epoch 3/40
 - 7s - loss: 0.0271 - acc: 0.9888 - val_loss: 0.0320 - val_acc: 0.9880

==================================================================================================
	Training time : 0:01:08.646258
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 279
	100 : 1
	5 : 86
	25 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15313

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15313 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15313
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 357)       5466741     input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 85)        14365       input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 1428)         0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 340)          0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 1768)         0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 36)           63684       concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 36)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            296         dropout_4[0][0]                  
==================================================================================================
Total params: 5,545,086
Trainable params: 5,545,086
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({1.0: 26937, 0.0: 26782, 2.0: 284, 6.0: 169, 5.0: 51, 4.0: 38})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0418 - acc: 0.9871 - val_loss: 0.0255 - val_acc: 0.9903
Epoch 2/40
 - 8s - loss: 0.0255 - acc: 0.9907 - val_loss: 0.0252 - val_acc: 0.9908
Epoch 3/40
 - 8s - loss: 0.0220 - acc: 0.9914 - val_loss: 0.0248 - val_acc: 0.9907

==================================================================================================
	Training time : 0:01:11.647091
==================================================================================================
	Identification : 0.011
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 243
	25 : 1
	5 : 45

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15281

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15281 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15281
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 357)       5455317     input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 85)        14365       input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 1428)         0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 340)          0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 1768)         0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 36)           63684       concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 36)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            296         dropout_5[0][0]                  
==================================================================================================
Total params: 5,533,662
Trainable params: 5,533,662
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 26975, 1.0: 26725, 2.0: 301, 6.0: 167, 5.0: 49, 4.0: 44})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0409 - acc: 0.9874 - val_loss: 0.0240 - val_acc: 0.9904
Epoch 2/40
 - 8s - loss: 0.0254 - acc: 0.9908 - val_loss: 0.0225 - val_acc: 0.9912
Epoch 3/40
 - 8s - loss: 0.0216 - acc: 0.9917 - val_loss: 0.0231 - val_acc: 0.9912

==================================================================================================
	Training time : 0:01:11.978419
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 244
	25 : 1
	5 : 45

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15240

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15240 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15240
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 357)       5440680     input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 85)        14365       input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 1428)         0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 340)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 1768)         0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 36)           63684       concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 36)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            296         dropout_6[0][0]                  
==================================================================================================
Total params: 5,519,025
Trainable params: 5,519,025
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27166, 1.0: 26556, 2.0: 297, 6.0: 151, 5.0: 52, 4.0: 39})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0406 - acc: 0.9875 - val_loss: 0.0263 - val_acc: 0.9905
Epoch 2/40
 - 8s - loss: 0.0253 - acc: 0.9904 - val_loss: 0.0243 - val_acc: 0.9913
Epoch 3/40
 - 8s - loss: 0.0212 - acc: 0.9915 - val_loss: 0.0254 - val_acc: 0.9911

==================================================================================================
	Training time : 0:01:11.201334
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 244
	25 : 1
	5 : 45

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12580

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12580 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12580
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 357)       4491060     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 85)        9350        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 1428)         0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 340)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 1768)         0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 36)           63684       concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 36)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            296         dropout_7[0][0]                  
==================================================================================================
Total params: 4,564,390
Trainable params: 4,564,390
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27233, 1.0: 26294, 2.0: 494, 6.0: 235, 5.0: 230})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 7s - loss: 0.0702 - acc: 0.9764 - val_loss: 0.0426 - val_acc: 0.9828
Epoch 2/40
 - 7s - loss: 0.0487 - acc: 0.9798 - val_loss: 0.0437 - val_acc: 0.9830
Epoch 3/40
 - 7s - loss: 0.0442 - acc: 0.9810 - val_loss: 0.0437 - val_acc: 0.9829

==================================================================================================
	Training time : 0:01:10.347775
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 396
	25 : 17
	50 : 7
	5 : 106

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12618

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12618 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12618
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 357)       4504626     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 85)        9350        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 1428)         0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 340)          0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 1768)         0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 36)           63684       concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 36)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            296         dropout_8[0][0]                  
==================================================================================================
Total params: 4,577,956
Trainable params: 4,577,956
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27027, 1.0: 26530, 2.0: 466, 5.0: 242, 6.0: 221})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 7s - loss: 0.0609 - acc: 0.9785 - val_loss: 0.0429 - val_acc: 0.9828
Epoch 2/40
 - 7s - loss: 0.0443 - acc: 0.9815 - val_loss: 0.0424 - val_acc: 0.9821
Epoch 3/40
 - 7s - loss: 0.0403 - acc: 0.9821 - val_loss: 0.0416 - val_acc: 0.9827

==================================================================================================
	Training time : 0:01:10.098098
==================================================================================================
	Identification : 0.002
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 396
	25 : 17
	50 : 7
	5 : 106

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12514

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12514 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12514
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 357)       4467498     input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 85)        9350        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 1428)         0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 340)          0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 1768)         0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 36)           63684       concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 36)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            296         dropout_9[0][0]                  
==================================================================================================
Total params: 4,540,828
Trainable params: 4,540,828
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27180, 1.0: 26303, 2.0: 505, 5.0: 269, 6.0: 229})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 7s - loss: 0.0682 - acc: 0.9764 - val_loss: 0.0451 - val_acc: 0.9817
Epoch 2/40
 - 7s - loss: 0.0495 - acc: 0.9801 - val_loss: 0.0439 - val_acc: 0.9821
Epoch 3/40
 - 7s - loss: 0.0456 - acc: 0.9813 - val_loss: 0.0449 - val_acc: 0.9817

==================================================================================================
	Training time : 0:01:10.155877
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 396
	25 : 17
	50 : 7
	5 : 106

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14841

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14841 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14841
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 357)       5298237     input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 85)        14280       input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 1428)         0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 340)          0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 1768)         0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 36)           63684       concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 36)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            296         dropout_10[0][0]                 
==================================================================================================
Total params: 5,376,497
Trainable params: 5,376,497
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39777, 1.0: 39437, 2.0: 606, 4.0: 276, 6.0: 165, 5.0: 104, 3.0: 6})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 11s - loss: 0.0417 - acc: 0.9855 - val_loss: 0.0266 - val_acc: 0.9885
Epoch 2/40
 - 11s - loss: 0.0278 - acc: 0.9883 - val_loss: 0.0267 - val_acc: 0.9884
Epoch 3/40
 - 11s - loss: 0.0248 - acc: 0.9889 - val_loss: 0.0281 - val_acc: 0.9886

==================================================================================================
	Training time : 0:01:37.006295
==================================================================================================
	Identification : 0.344
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 18
	100 : 1
	5 : 28
	300 : 1
	50 : 5
	25 : 13

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 332
	25 : 12
	5 : 50

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14856

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14856 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14856
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 357)       5303592     input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 85)        14280       input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 1428)         0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 340)          0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 1768)         0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 36)           63684       concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 36)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            296         dropout_11[0][0]                 
==================================================================================================
Total params: 5,381,852
Trainable params: 5,381,852
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40003, 1.0: 39288, 2.0: 549, 4.0: 260, 6.0: 153, 5.0: 109, 3.0: 9})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 11s - loss: 0.0435 - acc: 0.9853 - val_loss: 0.0268 - val_acc: 0.9887
Epoch 2/40
 - 11s - loss: 0.0288 - acc: 0.9882 - val_loss: 0.0267 - val_acc: 0.9887
Epoch 3/40
 - 11s - loss: 0.0254 - acc: 0.9889 - val_loss: 0.0272 - val_acc: 0.9889

==================================================================================================
	Training time : 0:01:36.862045
==================================================================================================
	Identification : 0.029
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 2
	300 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 348
	100 : 1
	5 : 74
	300 : 1
	50 : 5
	25 : 21

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14774

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14774 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14774
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 357)       5274318     input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 85)        14280       input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 1428)         0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 340)          0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 1768)         0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 36)           63684       concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 36)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            296         dropout_12[0][0]                 
==================================================================================================
Total params: 5,352,578
Trainable params: 5,352,578
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39684, 1.0: 39543, 2.0: 587, 4.0: 275, 6.0: 164, 5.0: 111, 3.0: 7})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 11s - loss: 0.0456 - acc: 0.9846 - val_loss: 0.0281 - val_acc: 0.9882
Epoch 2/40
 - 10s - loss: 0.0301 - acc: 0.9879 - val_loss: 0.0270 - val_acc: 0.9884
Epoch 3/40
 - 11s - loss: 0.0264 - acc: 0.9885 - val_loss: 0.0278 - val_acc: 0.9881

==================================================================================================
	Training time : 0:01:37.172332
==================================================================================================
	Identification : 0.058
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 3
	300 : 1
	5 : 1
	100 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 349
	100 : 1
	5 : 74
	300 : 1
	50 : 5
	25 : 20

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 21100

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 21100 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 21100
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 357)       7532700     input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 85)        19040       input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 1428)         0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 340)          0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 1768)         0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 36)           63684       concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 36)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            296         dropout_13[0][0]                 
==================================================================================================
Total params: 7,615,720
Trainable params: 7,615,720
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47550, 1.0: 46602, 2.0: 543, 6.0: 276, 5.0: 85, 4.0: 67})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 15s - loss: 0.0372 - acc: 0.9880 - val_loss: 0.0227 - val_acc: 0.9912
Epoch 2/40
 - 15s - loss: 0.0240 - acc: 0.9908 - val_loss: 0.0216 - val_acc: 0.9913
Epoch 3/40
 - 15s - loss: 0.0208 - acc: 0.9916 - val_loss: 0.0227 - val_acc: 0.9914

==================================================================================================
	Training time : 0:01:59.564551
==================================================================================================
	Identification : 0.004
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 328
	25 : 8
	5 : 92

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 21122

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 21122 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 21122
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 357)       7540554     input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 85)        19040       input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 1428)         0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 340)          0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 1768)         0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 36)           63684       concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 36)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            296         dropout_14[0][0]                 
==================================================================================================
Total params: 7,623,574
Trainable params: 7,623,574
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47484, 1.0: 46649, 2.0: 533, 6.0: 303, 5.0: 87, 4.0: 67})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 15s - loss: 0.0371 - acc: 0.9879 - val_loss: 0.0219 - val_acc: 0.9909
Epoch 2/40
 - 15s - loss: 0.0241 - acc: 0.9905 - val_loss: 0.0214 - val_acc: 0.9911
Epoch 3/40
 - 15s - loss: 0.0209 - acc: 0.9915 - val_loss: 0.0213 - val_acc: 0.9913

==================================================================================================
	Training time : 0:02:01.634045
==================================================================================================
	Identification : 0.063
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 10
	5 : 7

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 320
	25 : 8
	5 : 89

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 20980

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20980 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 20980
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 357)       7489860     input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 85)        19040       input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 1428)         0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 340)          0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 1768)         0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 36)           63684       concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 36)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            296         dropout_15[0][0]                 
==================================================================================================
Total params: 7,572,880
Trainable params: 7,572,880
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47220, 1.0: 46978, 2.0: 512, 6.0: 263, 4.0: 76, 5.0: 74})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 15s - loss: 0.0365 - acc: 0.9881 - val_loss: 0.0222 - val_acc: 0.9911
Epoch 2/40
 - 15s - loss: 0.0234 - acc: 0.9910 - val_loss: 0.0209 - val_acc: 0.9915
Epoch 3/40
 - 15s - loss: 0.0204 - acc: 0.9916 - val_loss: 0.0209 - val_acc: 0.9916

==================================================================================================
	Training time : 0:01:59.543677
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 329
	25 : 8
	5 : 92

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13968

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13968 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13968
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 357)       4986576     input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 85)        10115       input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 1428)         0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 340)          0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 1768)         0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 36)           63684       concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 36)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            296         dropout_16[0][0]                 
==================================================================================================
Total params: 5,060,671
Trainable params: 5,060,671
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33520, 1.0: 32795, 2.0: 662, 5.0: 313, 6.0: 296})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 9s - loss: 0.0681 - acc: 0.9766 - val_loss: 0.0465 - val_acc: 0.9816
Epoch 2/40
 - 9s - loss: 0.0490 - acc: 0.9806 - val_loss: 0.0457 - val_acc: 0.9819
Epoch 3/40
 - 9s - loss: 0.0449 - acc: 0.9813 - val_loss: 0.0449 - val_acc: 0.9820

==================================================================================================
	Training time : 0:01:25.110074
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 244
	25 : 19
	50 : 10
	5 : 80

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13942

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13942 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13942
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 357)       4977294     input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 85)        10115       input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 1428)         0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 340)          0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 1768)         0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 36)           63684       concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 36)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            296         dropout_17[0][0]                 
==================================================================================================
Total params: 5,051,389
Trainable params: 5,051,389
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33405, 1.0: 32898, 2.0: 660, 5.0: 319, 6.0: 304})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 9s - loss: 0.0669 - acc: 0.9771 - val_loss: 0.0453 - val_acc: 0.9820
Epoch 2/40
 - 9s - loss: 0.0493 - acc: 0.9801 - val_loss: 0.0437 - val_acc: 0.9820
Epoch 3/40
 - 9s - loss: 0.0454 - acc: 0.9806 - val_loss: 0.0438 - val_acc: 0.9822

==================================================================================================
	Training time : 0:01:26.829347
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 244
	25 : 19
	50 : 10
	5 : 80

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13968

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13968 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13968
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 357
	POS = 85 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 357)       4986576     input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 85)        10115       input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 1428)         0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 340)          0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 1768)         0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 36)           63684       concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 36)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            296         dropout_18[0][0]                 
==================================================================================================
Total params: 5,060,671
Trainable params: 5,060,671
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33533, 1.0: 32862, 2.0: 607, 5.0: 299, 6.0: 285})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.032
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 9s - loss: 0.0611 - acc: 0.9779 - val_loss: 0.0428 - val_acc: 0.9823
Epoch 2/40
 - 9s - loss: 0.0450 - acc: 0.9812 - val_loss: 0.0421 - val_acc: 0.9825
Epoch 3/40
 - 9s - loss: 0.0406 - acc: 0.9819 - val_loss: 0.0418 - val_acc: 0.9823

==================================================================================================
	Training time : 0:01:24.563336
==================================================================================================
	Identification : 0.064
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	50 : 3

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 244
	25 : 19
	50 : 9
	5 : 80

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
21_False_True_218_76_compact_False_40_relu_0.324_False_512_relu_0.2_adagrad_0.075_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 218)       610400      input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 76)        11552       input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 872)          0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 304)          0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 1176)         0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 8)            9416        concatenate_19[0][0]             
==================================================================================================
Total params: 631,368
Trainable params: 631,368
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27040, 1.0: 26558, 2.0: 391, 4.0: 183, 6.0: 101, 5.0: 84, 3.0: 4})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0305 - acc: 0.9871 - val_loss: 0.0277 - val_acc: 0.9878
Epoch 2/40
 - 5s - loss: 0.0214 - acc: 0.9892 - val_loss: 0.0281 - val_acc: 0.9870
Epoch 3/40
 - 5s - loss: 0.0196 - acc: 0.9897 - val_loss: 0.0290 - val_acc: 0.9862

==================================================================================================
	Training time : 0:01:04.081823
==================================================================================================
	Identification : 0.238
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 27
	25 : 5
	100 : 1
	5 : 30

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 252
	100 : 1
	5 : 62
	25 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_39 (Embedding)        (None, 4, 218)       610400      input_39[0][0]                   
__________________________________________________________________________________________________
embedding_40 (Embedding)        (None, 4, 76)        11552       input_40[0][0]                   
__________________________________________________________________________________________________
flatten_39 (Flatten)            (None, 872)          0           embedding_39[0][0]               
__________________________________________________________________________________________________
flatten_40 (Flatten)            (None, 304)          0           embedding_40[0][0]               
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 1176)         0           flatten_39[0][0]                 
                                                                 flatten_40[0][0]                 
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            9416        concatenate_20[0][0]             
==================================================================================================
Total params: 631,368
Trainable params: 631,368
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27133, 1.0: 26493, 2.0: 386, 4.0: 182, 6.0: 95, 5.0: 68, 3.0: 4})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0314 - acc: 0.9868 - val_loss: 0.0253 - val_acc: 0.9889
Epoch 2/40
 - 5s - loss: 0.0215 - acc: 0.9892 - val_loss: 0.0260 - val_acc: 0.9872
Epoch 3/40
 - 5s - loss: 0.0197 - acc: 0.9896 - val_loss: 0.0264 - val_acc: 0.9871

==================================================================================================
	Training time : 0:01:01.340965
==================================================================================================
	Identification : 0.473
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 42
	100 : 1
	5 : 41
	25 : 12

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 239
	100 : 1
	5 : 50
	25 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_41 (Embedding)        (None, 4, 218)       610400      input_41[0][0]                   
__________________________________________________________________________________________________
embedding_42 (Embedding)        (None, 4, 76)        11552       input_42[0][0]                   
__________________________________________________________________________________________________
flatten_41 (Flatten)            (None, 872)          0           embedding_41[0][0]               
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 304)          0           embedding_42[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 1176)         0           flatten_41[0][0]                 
                                                                 flatten_42[0][0]                 
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 8)            9416        concatenate_21[0][0]             
==================================================================================================
Total params: 631,368
Trainable params: 631,368
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26956, 1.0: 26681, 2.0: 383, 4.0: 156, 6.0: 108, 5.0: 75, 3.0: 2})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0311 - acc: 0.9870 - val_loss: 0.0266 - val_acc: 0.9884
Epoch 2/40
 - 5s - loss: 0.0215 - acc: 0.9893 - val_loss: 0.0268 - val_acc: 0.9872
Epoch 3/40
 - 5s - loss: 0.0197 - acc: 0.9897 - val_loss: 0.0275 - val_acc: 0.9878

==================================================================================================
	Training time : 0:01:01.307428
==================================================================================================
	Identification : 0.248
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 26
	25 : 5
	5 : 27

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 256
	100 : 1
	5 : 66
	25 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 4, 218)       608874      input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 4, 76)        12844       input_44[0][0]                   
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 872)          0           embedding_43[0][0]               
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 304)          0           embedding_44[0][0]               
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 1176)         0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 8)            9416        concatenate_22[0][0]             
==================================================================================================
Total params: 631,134
Trainable params: 631,134
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27113, 1.0: 26599, 2.0: 275, 6.0: 185, 5.0: 46, 4.0: 43})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0259 - acc: 0.9896 - val_loss: 0.0218 - val_acc: 0.9909
Epoch 2/40
 - 5s - loss: 0.0173 - acc: 0.9918 - val_loss: 0.0215 - val_acc: 0.9896
Epoch 3/40
 - 5s - loss: 0.0157 - acc: 0.9921 - val_loss: 0.0214 - val_acc: 0.9898

==================================================================================================
	Training time : 0:01:01.115604
==================================================================================================
	Identification : 0.441
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 63
	5 : 25

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 185
	25 : 1
	5 : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_45 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 4, 218)       608874      input_45[0][0]                   
__________________________________________________________________________________________________
embedding_46 (Embedding)        (None, 4, 76)        12844       input_46[0][0]                   
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 872)          0           embedding_45[0][0]               
__________________________________________________________________________________________________
flatten_46 (Flatten)            (None, 304)          0           embedding_46[0][0]               
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 1176)         0           flatten_45[0][0]                 
                                                                 flatten_46[0][0]                 
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 8)            9416        concatenate_23[0][0]             
==================================================================================================
Total params: 631,134
Trainable params: 631,134
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 26911, 1.0: 26773, 2.0: 316, 6.0: 178, 5.0: 43, 4.0: 40})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0257 - acc: 0.9897 - val_loss: 0.0215 - val_acc: 0.9902
Epoch 2/40
 - 5s - loss: 0.0173 - acc: 0.9918 - val_loss: 0.0225 - val_acc: 0.9896
Epoch 3/40
 - 5s - loss: 0.0157 - acc: 0.9921 - val_loss: 0.0229 - val_acc: 0.9899

==================================================================================================
	Training time : 0:01:01.079609
==================================================================================================
	Identification : 0.324
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 51
	5 : 18

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 199
	25 : 1
	5 : 30

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_47 (Embedding)        (None, 4, 218)       608874      input_47[0][0]                   
__________________________________________________________________________________________________
embedding_48 (Embedding)        (None, 4, 76)        12844       input_48[0][0]                   
__________________________________________________________________________________________________
flatten_47 (Flatten)            (None, 872)          0           embedding_47[0][0]               
__________________________________________________________________________________________________
flatten_48 (Flatten)            (None, 304)          0           embedding_48[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 1176)         0           flatten_47[0][0]                 
                                                                 flatten_48[0][0]                 
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 8)            9416        concatenate_24[0][0]             
==================================================================================================
Total params: 631,134
Trainable params: 631,134
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({1.0: 26869, 0.0: 26851, 2.0: 288, 6.0: 173, 5.0: 45, 4.0: 35})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0258 - acc: 0.9898 - val_loss: 0.0220 - val_acc: 0.9905
Epoch 2/40
 - 5s - loss: 0.0172 - acc: 0.9919 - val_loss: 0.0219 - val_acc: 0.9897
Epoch 3/40
 - 5s - loss: 0.0157 - acc: 0.9922 - val_loss: 0.0221 - val_acc: 0.9893

==================================================================================================
	Training time : 0:01:01.830301
==================================================================================================
	Identification : 0.326
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 44
	5 : 19

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 204
	25 : 1
	5 : 31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_49 (Embedding)        (None, 4, 218)       693458      input_49[0][0]                   
__________________________________________________________________________________________________
embedding_50 (Embedding)        (None, 4, 76)        8360        input_50[0][0]                   
__________________________________________________________________________________________________
flatten_49 (Flatten)            (None, 872)          0           embedding_49[0][0]               
__________________________________________________________________________________________________
flatten_50 (Flatten)            (None, 304)          0           embedding_50[0][0]               
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 1176)         0           flatten_49[0][0]                 
                                                                 flatten_50[0][0]                 
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 8)            9416        concatenate_25[0][0]             
==================================================================================================
Total params: 711,234
Trainable params: 711,234
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27014, 1.0: 26406, 2.0: 526, 6.0: 272, 5.0: 268})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 5s - loss: 0.0477 - acc: 0.9797 - val_loss: 0.0417 - val_acc: 0.9810
Epoch 2/40
 - 5s - loss: 0.0367 - acc: 0.9822 - val_loss: 0.0428 - val_acc: 0.9797
Epoch 3/40
 - 5s - loss: 0.0347 - acc: 0.9826 - val_loss: 0.0782 - val_acc: 0.9675

==================================================================================================
	Training time : 0:01:01.440944
==================================================================================================
	Identification : 0.321
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 24
	25 : 10
	50 : 4
	5 : 32

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 373
	25 : 13
	50 : 4
	5 : 79

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_51 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_51 (Embedding)        (None, 4, 218)       693458      input_51[0][0]                   
__________________________________________________________________________________________________
embedding_52 (Embedding)        (None, 4, 76)        8360        input_52[0][0]                   
__________________________________________________________________________________________________
flatten_51 (Flatten)            (None, 872)          0           embedding_51[0][0]               
__________________________________________________________________________________________________
flatten_52 (Flatten)            (None, 304)          0           embedding_52[0][0]               
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 1176)         0           flatten_51[0][0]                 
                                                                 flatten_52[0][0]                 
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 8)            9416        concatenate_26[0][0]             
==================================================================================================
Total params: 711,234
Trainable params: 711,234
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26985, 1.0: 26519, 2.0: 481, 5.0: 258, 6.0: 243})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 5s - loss: 0.0467 - acc: 0.9799 - val_loss: 0.0642 - val_acc: 0.9780
Epoch 2/40
 - 5s - loss: 0.0366 - acc: 0.9819 - val_loss: 0.1492 - val_acc: 0.9413
Epoch 3/40
 - 5s - loss: 0.0347 - acc: 0.9824 - val_loss: 0.0503 - val_acc: 0.9792

==================================================================================================
	Training time : 0:01:01.312882
==================================================================================================
	Identification : 0.264
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 27
	25 : 11
	50 : 4
	5 : 32

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 374
	25 : 13
	50 : 7
	5 : 79

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_53 (Embedding)        (None, 4, 218)       693458      input_53[0][0]                   
__________________________________________________________________________________________________
embedding_54 (Embedding)        (None, 4, 76)        8360        input_54[0][0]                   
__________________________________________________________________________________________________
flatten_53 (Flatten)            (None, 872)          0           embedding_53[0][0]               
__________________________________________________________________________________________________
flatten_54 (Flatten)            (None, 304)          0           embedding_54[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 1176)         0           flatten_53[0][0]                 
                                                                 flatten_54[0][0]                 
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 8)            9416        concatenate_27[0][0]             
==================================================================================================
Total params: 711,234
Trainable params: 711,234
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26806, 1.0: 26635, 2.0: 554, 6.0: 249, 5.0: 242})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 5s - loss: 0.0471 - acc: 0.9801 - val_loss: 0.0422 - val_acc: 0.9815
Epoch 2/40
 - 5s - loss: 0.0365 - acc: 0.9822 - val_loss: 0.0441 - val_acc: 0.9798
Epoch 3/40
 - 5s - loss: 0.0347 - acc: 0.9825 - val_loss: 0.0437 - val_acc: 0.9775

==================================================================================================
	Training time : 0:01:01.485065
==================================================================================================
	Identification : 0.361
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 42
	25 : 7
	50 : 4
	5 : 35

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 356
	25 : 13
	50 : 6
	5 : 79

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_55 (Embedding)        (None, 4, 218)       771938      input_55[0][0]                   
__________________________________________________________________________________________________
embedding_56 (Embedding)        (None, 4, 76)        12768       input_56[0][0]                   
__________________________________________________________________________________________________
flatten_55 (Flatten)            (None, 872)          0           embedding_55[0][0]               
__________________________________________________________________________________________________
flatten_56 (Flatten)            (None, 304)          0           embedding_56[0][0]               
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 1176)         0           flatten_55[0][0]                 
                                                                 flatten_56[0][0]                 
__________________________________________________________________________________________________
dense_46 (Dense)                (None, 8)            9416        concatenate_28[0][0]             
==================================================================================================
Total params: 794,122
Trainable params: 794,122
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39686, 1.0: 39557, 2.0: 581, 4.0: 292, 6.0: 151, 5.0: 99, 3.0: 5})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 7s - loss: 0.0297 - acc: 0.9872 - val_loss: 0.0264 - val_acc: 0.9883
Epoch 2/40
 - 7s - loss: 0.0216 - acc: 0.9891 - val_loss: 0.0268 - val_acc: 0.9868
Epoch 3/40
 - 7s - loss: 0.0200 - acc: 0.9895 - val_loss: 0.0272 - val_acc: 0.9874

==================================================================================================
	Training time : 0:01:26.713900
==================================================================================================
	Identification : 0.366
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 37
	25 : 16
	50 : 2
	300 : 1
	5 : 38

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 317
	100 : 1
	5 : 41
	300 : 1
	50 : 3
	25 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_57 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_58 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_57 (Embedding)        (None, 4, 218)       771938      input_57[0][0]                   
__________________________________________________________________________________________________
embedding_58 (Embedding)        (None, 4, 76)        12768       input_58[0][0]                   
__________________________________________________________________________________________________
flatten_57 (Flatten)            (None, 872)          0           embedding_57[0][0]               
__________________________________________________________________________________________________
flatten_58 (Flatten)            (None, 304)          0           embedding_58[0][0]               
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 1176)         0           flatten_57[0][0]                 
                                                                 flatten_58[0][0]                 
__________________________________________________________________________________________________
dense_47 (Dense)                (None, 8)            9416        concatenate_29[0][0]             
==================================================================================================
Total params: 794,122
Trainable params: 794,122
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39989, 1.0: 39240, 2.0: 621, 4.0: 268, 6.0: 143, 5.0: 100, 3.0: 10})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 7s - loss: 0.0295 - acc: 0.9873 - val_loss: 0.0255 - val_acc: 0.9883
Epoch 2/40
 - 7s - loss: 0.0216 - acc: 0.9892 - val_loss: 0.0263 - val_acc: 0.9874
Epoch 3/40
 - 7s - loss: 0.0200 - acc: 0.9894 - val_loss: 0.0263 - val_acc: 0.9871

==================================================================================================
	Training time : 0:01:26.562638
==================================================================================================
	Identification : 0.197
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 17
	25 : 6
	50 : 2
	5 : 17

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 334
	100 : 1
	5 : 60
	300 : 1
	50 : 4
	25 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_59 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_60 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_59 (Embedding)        (None, 4, 218)       771938      input_59[0][0]                   
__________________________________________________________________________________________________
embedding_60 (Embedding)        (None, 4, 76)        12768       input_60[0][0]                   
__________________________________________________________________________________________________
flatten_59 (Flatten)            (None, 872)          0           embedding_59[0][0]               
__________________________________________________________________________________________________
flatten_60 (Flatten)            (None, 304)          0           embedding_60[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 1176)         0           flatten_59[0][0]                 
                                                                 flatten_60[0][0]                 
__________________________________________________________________________________________________
dense_48 (Dense)                (None, 8)            9416        concatenate_30[0][0]             
==================================================================================================
Total params: 794,122
Trainable params: 794,122
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40057, 1.0: 39215, 2.0: 581, 4.0: 258, 6.0: 154, 5.0: 95, 3.0: 11})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 7s - loss: 0.0301 - acc: 0.9872 - val_loss: 0.0242 - val_acc: 0.9888
Epoch 2/40
 - 7s - loss: 0.0218 - acc: 0.9891 - val_loss: 0.0240 - val_acc: 0.9882
Epoch 3/40
 - 7s - loss: 0.0202 - acc: 0.9895 - val_loss: 0.0246 - val_acc: 0.9873

==================================================================================================
	Training time : 0:01:25.831280
==================================================================================================
	Identification : 0.336
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 43
	25 : 10
	50 : 1
	300 : 1
	5 : 34

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 309
	100 : 1
	5 : 51
	300 : 1
	50 : 4
	25 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_61 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_62 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_61 (Embedding)        (None, 4, 218)       851726      input_61[0][0]                   
__________________________________________________________________________________________________
embedding_62 (Embedding)        (None, 4, 76)        17024       input_62[0][0]                   
__________________________________________________________________________________________________
flatten_61 (Flatten)            (None, 872)          0           embedding_61[0][0]               
__________________________________________________________________________________________________
flatten_62 (Flatten)            (None, 304)          0           embedding_62[0][0]               
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 1176)         0           flatten_61[0][0]                 
                                                                 flatten_62[0][0]                 
__________________________________________________________________________________________________
dense_49 (Dense)                (None, 8)            9416        concatenate_31[0][0]             
==================================================================================================
Total params: 878,166
Trainable params: 878,166
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47256, 1.0: 46887, 2.0: 540, 6.0: 293, 5.0: 83, 4.0: 64})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 8s - loss: 0.0239 - acc: 0.9901 - val_loss: 0.0207 - val_acc: 0.9909
Epoch 2/40
 - 8s - loss: 0.0172 - acc: 0.9918 - val_loss: 0.0202 - val_acc: 0.9911
Epoch 3/40
 - 8s - loss: 0.0160 - acc: 0.9921 - val_loss: 0.0203 - val_acc: 0.9901

==================================================================================================
	Training time : 0:01:39.911999
==================================================================================================
	Identification : 0.365
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 96
	25 : 5
	5 : 45

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 236
	25 : 4
	5 : 52

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_63 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_64 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_63 (Embedding)        (None, 4, 218)       851726      input_63[0][0]                   
__________________________________________________________________________________________________
embedding_64 (Embedding)        (None, 4, 76)        17024       input_64[0][0]                   
__________________________________________________________________________________________________
flatten_63 (Flatten)            (None, 872)          0           embedding_63[0][0]               
__________________________________________________________________________________________________
flatten_64 (Flatten)            (None, 304)          0           embedding_64[0][0]               
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 1176)         0           flatten_63[0][0]                 
                                                                 flatten_64[0][0]                 
__________________________________________________________________________________________________
dense_50 (Dense)                (None, 8)            9416        concatenate_32[0][0]             
==================================================================================================
Total params: 878,166
Trainable params: 878,166
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47202, 1.0: 47031, 2.0: 508, 6.0: 238, 5.0: 82, 4.0: 62})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 8s - loss: 0.0236 - acc: 0.9903 - val_loss: 0.0198 - val_acc: 0.9918
Epoch 2/40
 - 8s - loss: 0.0172 - acc: 0.9918 - val_loss: 0.0194 - val_acc: 0.9916
Epoch 3/40
 - 8s - loss: 0.0159 - acc: 0.9922 - val_loss: 0.0205 - val_acc: 0.9899

==================================================================================================
	Training time : 0:01:39.324112
==================================================================================================
	Identification : 0.274
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 46
	25 : 3
	5 : 24

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 285
	25 : 8
	5 : 70

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_65 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_66 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_65 (Embedding)        (None, 4, 218)       851726      input_65[0][0]                   
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 4, 76)        17024       input_66[0][0]                   
__________________________________________________________________________________________________
flatten_65 (Flatten)            (None, 872)          0           embedding_65[0][0]               
__________________________________________________________________________________________________
flatten_66 (Flatten)            (None, 304)          0           embedding_66[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 1176)         0           flatten_65[0][0]                 
                                                                 flatten_66[0][0]                 
__________________________________________________________________________________________________
dense_51 (Dense)                (None, 8)            9416        concatenate_33[0][0]             
==================================================================================================
Total params: 878,166
Trainable params: 878,166
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47390, 1.0: 46754, 2.0: 540, 6.0: 266, 5.0: 91, 4.0: 82})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 9s - loss: 0.0236 - acc: 0.9904 - val_loss: 0.0206 - val_acc: 0.9904
Epoch 2/40
 - 9s - loss: 0.0171 - acc: 0.9918 - val_loss: 0.0206 - val_acc: 0.9905
Epoch 3/40
 - 9s - loss: 0.0158 - acc: 0.9923 - val_loss: 0.0210 - val_acc: 0.9900

==================================================================================================
	Training time : 0:01:40.724205
==================================================================================================
	Identification : 0.354
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 48
	25 : 3
	5 : 35

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 284
	25 : 6
	5 : 63

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_67 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_68 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 4, 218)       800496      input_67[0][0]                   
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 4, 76)        9044        input_68[0][0]                   
__________________________________________________________________________________________________
flatten_67 (Flatten)            (None, 872)          0           embedding_67[0][0]               
__________________________________________________________________________________________________
flatten_68 (Flatten)            (None, 304)          0           embedding_68[0][0]               
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 1176)         0           flatten_67[0][0]                 
                                                                 flatten_68[0][0]                 
__________________________________________________________________________________________________
dense_52 (Dense)                (None, 8)            9416        concatenate_34[0][0]             
==================================================================================================
Total params: 818,956
Trainable params: 818,956
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33367, 1.0: 32972, 2.0: 674, 5.0: 300, 6.0: 273})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 6s - loss: 0.0467 - acc: 0.9800 - val_loss: 0.0413 - val_acc: 0.9814
Epoch 2/40
 - 6s - loss: 0.0369 - acc: 0.9820 - val_loss: 0.0429 - val_acc: 0.9823
Epoch 3/40
 - 6s - loss: 0.0351 - acc: 0.9824 - val_loss: 0.0425 - val_acc: 0.9788

==================================================================================================
	Training time : 0:01:14.986113
==================================================================================================
	Identification : 0.274
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 17
	25 : 6
	50 : 7
	5 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 228
	25 : 14
	50 : 7
	5 : 67

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_69 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_70 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_69 (Embedding)        (None, 4, 218)       800496      input_69[0][0]                   
__________________________________________________________________________________________________
embedding_70 (Embedding)        (None, 4, 76)        9044        input_70[0][0]                   
__________________________________________________________________________________________________
flatten_69 (Flatten)            (None, 872)          0           embedding_69[0][0]               
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 304)          0           embedding_70[0][0]               
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 1176)         0           flatten_69[0][0]                 
                                                                 flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_53 (Dense)                (None, 8)            9416        concatenate_35[0][0]             
==================================================================================================
Total params: 818,956
Trainable params: 818,956
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33256, 1.0: 33037, 2.0: 648, 5.0: 342, 6.0: 303})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 6s - loss: 0.0463 - acc: 0.9800 - val_loss: 0.0437 - val_acc: 0.9800
Epoch 2/40
 - 6s - loss: 0.0367 - acc: 0.9821 - val_loss: 0.0419 - val_acc: 0.9799
Epoch 3/40
 - 6s - loss: 0.0349 - acc: 0.9825 - val_loss: 0.0447 - val_acc: 0.9778

==================================================================================================
	Training time : 0:01:13.895538
==================================================================================================
	Identification : 0.175
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 9
	25 : 6
	50 : 4
	5 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 235
	25 : 15
	50 : 7
	5 : 66

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 218
	POS = 76 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_71 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_72 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_71 (Embedding)        (None, 4, 218)       800496      input_71[0][0]                   
__________________________________________________________________________________________________
embedding_72 (Embedding)        (None, 4, 76)        9044        input_72[0][0]                   
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 872)          0           embedding_71[0][0]               
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 304)          0           embedding_72[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 1176)         0           flatten_71[0][0]                 
                                                                 flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_54 (Dense)                (None, 8)            9416        concatenate_36[0][0]             
==================================================================================================
Total params: 818,956
Trainable params: 818,956
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33691, 1.0: 32623, 2.0: 653, 6.0: 311, 5.0: 308})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.075
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 6s - loss: 0.0463 - acc: 0.9799 - val_loss: 0.0424 - val_acc: 0.9808
Epoch 2/40
 - 6s - loss: 0.0368 - acc: 0.9820 - val_loss: 0.0458 - val_acc: 0.9810
Epoch 3/40
 - 6s - loss: 0.0349 - acc: 0.9826 - val_loss: 0.0429 - val_acc: 0.9802

==================================================================================================
	Training time : 0:01:13.568941
==================================================================================================
	Identification : 0.274
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 16
	25 : 9
	50 : 4
	5 : 26

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 228
	25 : 13
	50 : 9
	5 : 56

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
6_False_False_406_28_compact_False_7_relu_0.317_False_512_relu_0.2_adagrad_0.026_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_73 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_74 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_73 (Embedding)        (None, 4, 406)       1650390     input_73[0][0]                   
__________________________________________________________________________________________________
embedding_74 (Embedding)        (None, 4, 28)        4256        input_74[0][0]                   
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 1624)         0           embedding_73[0][0]               
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 112)          0           embedding_74[0][0]               
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 1736)         0           flatten_73[0][0]                 
                                                                 flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 8)            13896       concatenate_37[0][0]             
==================================================================================================
Total params: 1,668,542
Trainable params: 1,668,542
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27040, 1.0: 26558, 2.0: 391, 4.0: 183, 6.0: 101, 5.0: 84, 3.0: 4})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0290 - acc: 0.9877 - val_loss: 0.0259 - val_acc: 0.9882
Epoch 2/40
 - 5s - loss: 0.0191 - acc: 0.9899 - val_loss: 0.0275 - val_acc: 0.9864
Epoch 3/40
 - 5s - loss: 0.0171 - acc: 0.9905 - val_loss: 0.0288 - val_acc: 0.9849

==================================================================================================
	Training time : 0:01:03.725905
==================================================================================================
	Identification : 0.204
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 41
	50 : 2
	5 : 15

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 349
	25 : 7
	50 : 2
	100 : 1
	5 : 51

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_75 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_76 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_75 (Embedding)        (None, 4, 406)       1650390     input_75[0][0]                   
__________________________________________________________________________________________________
embedding_76 (Embedding)        (None, 4, 28)        4256        input_76[0][0]                   
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 1624)         0           embedding_75[0][0]               
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 112)          0           embedding_76[0][0]               
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 1736)         0           flatten_75[0][0]                 
                                                                 flatten_76[0][0]                 
__________________________________________________________________________________________________
dense_56 (Dense)                (None, 8)            13896       concatenate_38[0][0]             
==================================================================================================
Total params: 1,668,542
Trainable params: 1,668,542
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27133, 1.0: 26493, 2.0: 386, 4.0: 182, 6.0: 95, 5.0: 68, 3.0: 4})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0298 - acc: 0.9873 - val_loss: 0.0245 - val_acc: 0.9890
Epoch 2/40
 - 5s - loss: 0.0195 - acc: 0.9899 - val_loss: 0.0257 - val_acc: 0.9870
Epoch 3/40
 - 5s - loss: 0.0174 - acc: 0.9903 - val_loss: 0.0265 - val_acc: 0.9865

==================================================================================================
	Training time : 0:01:03.188630
==================================================================================================
	Identification : 0.485
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 79
	25 : 6
	50 : 3
	100 : 1
	5 : 38

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 313
	25 : 2
	50 : 2
	100 : 1
	5 : 31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_77 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_78 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_77 (Embedding)        (None, 4, 406)       1650390     input_77[0][0]                   
__________________________________________________________________________________________________
embedding_78 (Embedding)        (None, 4, 28)        4256        input_78[0][0]                   
__________________________________________________________________________________________________
flatten_77 (Flatten)            (None, 1624)         0           embedding_77[0][0]               
__________________________________________________________________________________________________
flatten_78 (Flatten)            (None, 112)          0           embedding_78[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 1736)         0           flatten_77[0][0]                 
                                                                 flatten_78[0][0]                 
__________________________________________________________________________________________________
dense_57 (Dense)                (None, 8)            13896       concatenate_39[0][0]             
==================================================================================================
Total params: 1,668,542
Trainable params: 1,668,542
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26956, 1.0: 26681, 2.0: 383, 4.0: 156, 6.0: 108, 5.0: 75, 3.0: 2})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0297 - acc: 0.9874 - val_loss: 0.0250 - val_acc: 0.9891
Epoch 2/40
 - 5s - loss: 0.0194 - acc: 0.9899 - val_loss: 0.0261 - val_acc: 0.9859
Epoch 3/40
 - 5s - loss: 0.0173 - acc: 0.9903 - val_loss: 0.0275 - val_acc: 0.9860

==================================================================================================
	Training time : 0:01:03.427779
==================================================================================================
	Identification : 0.185
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 32
	25 : 2
	5 : 14

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 359
	25 : 7
	50 : 3
	100 : 1
	5 : 51

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_79 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_80 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_79 (Embedding)        (None, 4, 406)       1640646     input_79[0][0]                   
__________________________________________________________________________________________________
embedding_80 (Embedding)        (None, 4, 28)        4732        input_80[0][0]                   
__________________________________________________________________________________________________
flatten_79 (Flatten)            (None, 1624)         0           embedding_79[0][0]               
__________________________________________________________________________________________________
flatten_80 (Flatten)            (None, 112)          0           embedding_80[0][0]               
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 1736)         0           flatten_79[0][0]                 
                                                                 flatten_80[0][0]                 
__________________________________________________________________________________________________
dense_58 (Dense)                (None, 8)            13896       concatenate_40[0][0]             
==================================================================================================
Total params: 1,659,274
Trainable params: 1,659,274
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27113, 1.0: 26599, 2.0: 275, 6.0: 185, 5.0: 46, 4.0: 43})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0254 - acc: 0.9899 - val_loss: 0.0211 - val_acc: 0.9905
Epoch 2/40
 - 5s - loss: 0.0161 - acc: 0.9922 - val_loss: 0.0220 - val_acc: 0.9893
Epoch 3/40
 - 5s - loss: 0.0143 - acc: 0.9927 - val_loss: 0.0231 - val_acc: 0.9880

==================================================================================================
	Training time : 0:01:02.367015
==================================================================================================
	Identification : 0.428
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 98
	5 : 5

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 224
	5 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_81 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_82 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_81 (Embedding)        (None, 4, 406)       1640646     input_81[0][0]                   
__________________________________________________________________________________________________
embedding_82 (Embedding)        (None, 4, 28)        4732        input_82[0][0]                   
__________________________________________________________________________________________________
flatten_81 (Flatten)            (None, 1624)         0           embedding_81[0][0]               
__________________________________________________________________________________________________
flatten_82 (Flatten)            (None, 112)          0           embedding_82[0][0]               
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 1736)         0           flatten_81[0][0]                 
                                                                 flatten_82[0][0]                 
__________________________________________________________________________________________________
dense_59 (Dense)                (None, 8)            13896       concatenate_41[0][0]             
==================================================================================================
Total params: 1,659,274
Trainable params: 1,659,274
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 26911, 1.0: 26773, 2.0: 316, 6.0: 178, 5.0: 43, 4.0: 40})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0256 - acc: 0.9897 - val_loss: 0.0206 - val_acc: 0.9904
Epoch 2/40
 - 5s - loss: 0.0160 - acc: 0.9922 - val_loss: 0.0215 - val_acc: 0.9896
Epoch 3/40
 - 5s - loss: 0.0142 - acc: 0.9926 - val_loss: 0.0231 - val_acc: 0.9878

==================================================================================================
	Training time : 0:01:02.454269
==================================================================================================
	Identification : 0.36
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 83
	5 : 5

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 239
	5 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_83 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_84 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_83 (Embedding)        (None, 4, 406)       1640646     input_83[0][0]                   
__________________________________________________________________________________________________
embedding_84 (Embedding)        (None, 4, 28)        4732        input_84[0][0]                   
__________________________________________________________________________________________________
flatten_83 (Flatten)            (None, 1624)         0           embedding_83[0][0]               
__________________________________________________________________________________________________
flatten_84 (Flatten)            (None, 112)          0           embedding_84[0][0]               
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 1736)         0           flatten_83[0][0]                 
                                                                 flatten_84[0][0]                 
__________________________________________________________________________________________________
dense_60 (Dense)                (None, 8)            13896       concatenate_42[0][0]             
==================================================================================================
Total params: 1,659,274
Trainable params: 1,659,274
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({1.0: 26869, 0.0: 26851, 2.0: 288, 6.0: 173, 5.0: 45, 4.0: 35})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0257 - acc: 0.9897 - val_loss: 0.0216 - val_acc: 0.9912
Epoch 2/40
 - 5s - loss: 0.0160 - acc: 0.9924 - val_loss: 0.0223 - val_acc: 0.9885
Epoch 3/40
 - 5s - loss: 0.0143 - acc: 0.9926 - val_loss: 0.0233 - val_acc: 0.9879

==================================================================================================
	Training time : 0:01:02.243904
==================================================================================================
	Identification : 0.378
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 82
	5 : 4

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 240
	5 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_85 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_86 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_85 (Embedding)        (None, 4, 406)       2981664     input_85[0][0]                   
__________________________________________________________________________________________________
embedding_86 (Embedding)        (None, 4, 28)        3080        input_86[0][0]                   
__________________________________________________________________________________________________
flatten_85 (Flatten)            (None, 1624)         0           embedding_85[0][0]               
__________________________________________________________________________________________________
flatten_86 (Flatten)            (None, 112)          0           embedding_86[0][0]               
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 1736)         0           flatten_85[0][0]                 
                                                                 flatten_86[0][0]                 
__________________________________________________________________________________________________
dense_61 (Dense)                (None, 8)            13896       concatenate_43[0][0]             
==================================================================================================
Total params: 2,998,640
Trainable params: 2,998,640
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27014, 1.0: 26406, 2.0: 526, 6.0: 272, 5.0: 268})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0393 - acc: 0.9825 - val_loss: 0.0346 - val_acc: 0.9834
Epoch 2/40
 - 6s - loss: 0.0265 - acc: 0.9850 - val_loss: 0.0373 - val_acc: 0.9802
Epoch 3/40
 - 6s - loss: 0.0234 - acc: 0.9858 - val_loss: 0.0419 - val_acc: 0.9779

==================================================================================================
	Training time : 0:01:04.753276
==================================================================================================
	Identification : 0.395
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 187
	25 : 1
	5 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 515
	25 : 1
	5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_87 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_88 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_87 (Embedding)        (None, 4, 406)       2981664     input_87[0][0]                   
__________________________________________________________________________________________________
embedding_88 (Embedding)        (None, 4, 28)        3080        input_88[0][0]                   
__________________________________________________________________________________________________
flatten_87 (Flatten)            (None, 1624)         0           embedding_87[0][0]               
__________________________________________________________________________________________________
flatten_88 (Flatten)            (None, 112)          0           embedding_88[0][0]               
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 1736)         0           flatten_87[0][0]                 
                                                                 flatten_88[0][0]                 
__________________________________________________________________________________________________
dense_62 (Dense)                (None, 8)            13896       concatenate_44[0][0]             
==================================================================================================
Total params: 2,998,640
Trainable params: 2,998,640
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26985, 1.0: 26519, 2.0: 481, 5.0: 258, 6.0: 243})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0389 - acc: 0.9826 - val_loss: 0.0439 - val_acc: 0.9802
Epoch 2/40
 - 6s - loss: 0.0262 - acc: 0.9852 - val_loss: 0.0452 - val_acc: 0.9789
Epoch 3/40
 - 6s - loss: 0.0232 - acc: 0.9859 - val_loss: 0.0466 - val_acc: 0.9809

==================================================================================================
	Training time : 0:01:05.012877
==================================================================================================
	Identification : 0.35
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 168
	25 : 1
	5 : 14

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 534
	25 : 1
	5 : 26

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_89 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_90 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_89 (Embedding)        (None, 4, 406)       2981664     input_89[0][0]                   
__________________________________________________________________________________________________
embedding_90 (Embedding)        (None, 4, 28)        3080        input_90[0][0]                   
__________________________________________________________________________________________________
flatten_89 (Flatten)            (None, 1624)         0           embedding_89[0][0]               
__________________________________________________________________________________________________
flatten_90 (Flatten)            (None, 112)          0           embedding_90[0][0]               
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 1736)         0           flatten_89[0][0]                 
                                                                 flatten_90[0][0]                 
__________________________________________________________________________________________________
dense_63 (Dense)                (None, 8)            13896       concatenate_45[0][0]             
==================================================================================================
Total params: 2,998,640
Trainable params: 2,998,640
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26806, 1.0: 26635, 2.0: 554, 6.0: 249, 5.0: 242})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0389 - acc: 0.9825 - val_loss: 0.0333 - val_acc: 0.9850
Epoch 2/40
 - 6s - loss: 0.0263 - acc: 0.9851 - val_loss: 0.0368 - val_acc: 0.9812
Epoch 3/40
 - 6s - loss: 0.0234 - acc: 0.9858 - val_loss: 0.0399 - val_acc: 0.9774

==================================================================================================
	Training time : 0:01:04.982090
==================================================================================================
	Identification : 0.387
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 216
	5 : 24

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 485
	25 : 2
	5 : 17

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_91 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_92 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_91 (Embedding)        (None, 4, 406)       2160326     input_91[0][0]                   
__________________________________________________________________________________________________
embedding_92 (Embedding)        (None, 4, 28)        4704        input_92[0][0]                   
__________________________________________________________________________________________________
flatten_91 (Flatten)            (None, 1624)         0           embedding_91[0][0]               
__________________________________________________________________________________________________
flatten_92 (Flatten)            (None, 112)          0           embedding_92[0][0]               
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 1736)         0           flatten_91[0][0]                 
                                                                 flatten_92[0][0]                 
__________________________________________________________________________________________________
dense_64 (Dense)                (None, 8)            13896       concatenate_46[0][0]             
==================================================================================================
Total params: 2,178,926
Trainable params: 2,178,926
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39686, 1.0: 39557, 2.0: 581, 4.0: 292, 6.0: 151, 5.0: 99, 3.0: 5})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 8s - loss: 0.0282 - acc: 0.9876 - val_loss: 0.0242 - val_acc: 0.9889
Epoch 2/40
 - 8s - loss: 0.0194 - acc: 0.9897 - val_loss: 0.0253 - val_acc: 0.9863
Epoch 3/40
 - 8s - loss: 0.0175 - acc: 0.9902 - val_loss: 0.0267 - val_acc: 0.9867

==================================================================================================
	Training time : 0:01:28.513812
==================================================================================================
	Identification : 0.405
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 82
	200 : 1
	50 : 3
	5 : 36
	25 : 10

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 373
	25 : 6
	50 : 5
	5 : 31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_93 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_94 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_93 (Embedding)        (None, 4, 406)       2160326     input_93[0][0]                   
__________________________________________________________________________________________________
embedding_94 (Embedding)        (None, 4, 28)        4704        input_94[0][0]                   
__________________________________________________________________________________________________
flatten_93 (Flatten)            (None, 1624)         0           embedding_93[0][0]               
__________________________________________________________________________________________________
flatten_94 (Flatten)            (None, 112)          0           embedding_94[0][0]               
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 1736)         0           flatten_93[0][0]                 
                                                                 flatten_94[0][0]                 
__________________________________________________________________________________________________
dense_65 (Dense)                (None, 8)            13896       concatenate_47[0][0]             
==================================================================================================
Total params: 2,178,926
Trainable params: 2,178,926
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39989, 1.0: 39240, 2.0: 621, 4.0: 268, 6.0: 143, 5.0: 100, 3.0: 10})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 8s - loss: 0.0279 - acc: 0.9875 - val_loss: 0.0238 - val_acc: 0.9890
Epoch 2/40
 - 8s - loss: 0.0193 - acc: 0.9898 - val_loss: 0.0247 - val_acc: 0.9865
Epoch 3/40
 - 8s - loss: 0.0175 - acc: 0.9902 - val_loss: 0.0255 - val_acc: 0.9858

==================================================================================================
	Training time : 0:01:28.508277
==================================================================================================
	Identification : 0.136
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 16
	25 : 4
	5 : 13

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 435
	200 : 1
	50 : 5
	5 : 52
	25 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_95 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_96 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_95 (Embedding)        (None, 4, 406)       2160326     input_95[0][0]                   
__________________________________________________________________________________________________
embedding_96 (Embedding)        (None, 4, 28)        4704        input_96[0][0]                   
__________________________________________________________________________________________________
flatten_95 (Flatten)            (None, 1624)         0           embedding_95[0][0]               
__________________________________________________________________________________________________
flatten_96 (Flatten)            (None, 112)          0           embedding_96[0][0]               
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 1736)         0           flatten_95[0][0]                 
                                                                 flatten_96[0][0]                 
__________________________________________________________________________________________________
dense_66 (Dense)                (None, 8)            13896       concatenate_48[0][0]             
==================================================================================================
Total params: 2,178,926
Trainable params: 2,178,926
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40057, 1.0: 39215, 2.0: 581, 4.0: 258, 6.0: 154, 5.0: 95, 3.0: 11})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 8s - loss: 0.0283 - acc: 0.9876 - val_loss: 0.0230 - val_acc: 0.9885
Epoch 2/40
 - 8s - loss: 0.0195 - acc: 0.9897 - val_loss: 0.0235 - val_acc: 0.9877
Epoch 3/40
 - 8s - loss: 0.0177 - acc: 0.9902 - val_loss: 0.0249 - val_acc: 0.9866

==================================================================================================
	Training time : 0:01:28.408541
==================================================================================================
	Identification : 0.35
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 72
	25 : 3
	50 : 3
	5 : 34

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 384
	200 : 1
	50 : 4
	5 : 35
	25 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_97 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_98 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_97 (Embedding)        (None, 4, 406)       2426662     input_97[0][0]                   
__________________________________________________________________________________________________
embedding_98 (Embedding)        (None, 4, 28)        6272        input_98[0][0]                   
__________________________________________________________________________________________________
flatten_97 (Flatten)            (None, 1624)         0           embedding_97[0][0]               
__________________________________________________________________________________________________
flatten_98 (Flatten)            (None, 112)          0           embedding_98[0][0]               
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 1736)         0           flatten_97[0][0]                 
                                                                 flatten_98[0][0]                 
__________________________________________________________________________________________________
dense_67 (Dense)                (None, 8)            13896       concatenate_49[0][0]             
==================================================================================================
Total params: 2,446,830
Trainable params: 2,446,830
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47256, 1.0: 46887, 2.0: 540, 6.0: 293, 5.0: 83, 4.0: 64})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 10s - loss: 0.0235 - acc: 0.9903 - val_loss: 0.0193 - val_acc: 0.9914
Epoch 2/40
 - 10s - loss: 0.0160 - acc: 0.9922 - val_loss: 0.0202 - val_acc: 0.9911
Epoch 3/40
 - 10s - loss: 0.0144 - acc: 0.9925 - val_loss: 0.0209 - val_acc: 0.9891

==================================================================================================
	Training time : 0:01:43.105998
==================================================================================================
	Identification : 0.43
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 155
	5 : 13

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 310
	25 : 1
	5 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_99 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_100 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_99 (Embedding)        (None, 4, 406)       2426662     input_99[0][0]                   
__________________________________________________________________________________________________
embedding_100 (Embedding)       (None, 4, 28)        6272        input_100[0][0]                  
__________________________________________________________________________________________________
flatten_99 (Flatten)            (None, 1624)         0           embedding_99[0][0]               
__________________________________________________________________________________________________
flatten_100 (Flatten)           (None, 112)          0           embedding_100[0][0]              
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 1736)         0           flatten_99[0][0]                 
                                                                 flatten_100[0][0]                
__________________________________________________________________________________________________
dense_68 (Dense)                (None, 8)            13896       concatenate_50[0][0]             
==================================================================================================
Total params: 2,446,830
Trainable params: 2,446,830
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47202, 1.0: 47031, 2.0: 508, 6.0: 238, 5.0: 82, 4.0: 62})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 10s - loss: 0.0233 - acc: 0.9902 - val_loss: 0.0187 - val_acc: 0.9919
Epoch 2/40
 - 10s - loss: 0.0160 - acc: 0.9922 - val_loss: 0.0188 - val_acc: 0.9910
Epoch 3/40
 - 10s - loss: 0.0145 - acc: 0.9926 - val_loss: 0.0199 - val_acc: 0.9896

==================================================================================================
	Training time : 0:01:42.846259
==================================================================================================
	Identification : 0.317
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 101
	5 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 365
	25 : 1
	5 : 22

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_101 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_102 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_101 (Embedding)       (None, 4, 406)       2426662     input_101[0][0]                  
__________________________________________________________________________________________________
embedding_102 (Embedding)       (None, 4, 28)        6272        input_102[0][0]                  
__________________________________________________________________________________________________
flatten_101 (Flatten)           (None, 1624)         0           embedding_101[0][0]              
__________________________________________________________________________________________________
flatten_102 (Flatten)           (None, 112)          0           embedding_102[0][0]              
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 1736)         0           flatten_101[0][0]                
                                                                 flatten_102[0][0]                
__________________________________________________________________________________________________
dense_69 (Dense)                (None, 8)            13896       concatenate_51[0][0]             
==================================================================================================
Total params: 2,446,830
Trainable params: 2,446,830
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47390, 1.0: 46754, 2.0: 540, 6.0: 266, 5.0: 91, 4.0: 82})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 10s - loss: 0.0233 - acc: 0.9903 - val_loss: 0.0199 - val_acc: 0.9910
Epoch 2/40
 - 10s - loss: 0.0158 - acc: 0.9923 - val_loss: 0.0203 - val_acc: 0.9895
Epoch 3/40
 - 10s - loss: 0.0143 - acc: 0.9927 - val_loss: 0.0213 - val_acc: 0.9888

==================================================================================================
	Training time : 0:01:43.002388
==================================================================================================
	Identification : 0.372
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 117
	25 : 1
	5 : 13

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 350
	25 : 1
	5 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_103 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_104 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_103 (Embedding)       (None, 4, 406)       3539508     input_103[0][0]                  
__________________________________________________________________________________________________
embedding_104 (Embedding)       (None, 4, 28)        3332        input_104[0][0]                  
__________________________________________________________________________________________________
flatten_103 (Flatten)           (None, 1624)         0           embedding_103[0][0]              
__________________________________________________________________________________________________
flatten_104 (Flatten)           (None, 112)          0           embedding_104[0][0]              
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 1736)         0           flatten_103[0][0]                
                                                                 flatten_104[0][0]                
__________________________________________________________________________________________________
dense_70 (Dense)                (None, 8)            13896       concatenate_52[0][0]             
==================================================================================================
Total params: 3,556,736
Trainable params: 3,556,736
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33367, 1.0: 32972, 2.0: 674, 5.0: 300, 6.0: 273})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0386 - acc: 0.9825 - val_loss: 0.0330 - val_acc: 0.9842
Epoch 2/40
 - 7s - loss: 0.0268 - acc: 0.9848 - val_loss: 0.0359 - val_acc: 0.9836
Epoch 3/40
 - 7s - loss: 0.0239 - acc: 0.9855 - val_loss: 0.0389 - val_acc: 0.9777

==================================================================================================
	Training time : 0:01:20.726047
==================================================================================================
	Identification : 0.361
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 109
	25 : 1
	5 : 10

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 327
	25 : 2
	5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_105 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_106 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_105 (Embedding)       (None, 4, 406)       3539508     input_105[0][0]                  
__________________________________________________________________________________________________
embedding_106 (Embedding)       (None, 4, 28)        3332        input_106[0][0]                  
__________________________________________________________________________________________________
flatten_105 (Flatten)           (None, 1624)         0           embedding_105[0][0]              
__________________________________________________________________________________________________
flatten_106 (Flatten)           (None, 112)          0           embedding_106[0][0]              
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 1736)         0           flatten_105[0][0]                
                                                                 flatten_106[0][0]                
__________________________________________________________________________________________________
dense_71 (Dense)                (None, 8)            13896       concatenate_53[0][0]             
==================================================================================================
Total params: 3,556,736
Trainable params: 3,556,736
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33256, 1.0: 33037, 2.0: 648, 5.0: 342, 6.0: 303})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0385 - acc: 0.9826 - val_loss: 0.0348 - val_acc: 0.9832
Epoch 2/40
 - 7s - loss: 0.0267 - acc: 0.9849 - val_loss: 0.0364 - val_acc: 0.9802
Epoch 3/40
 - 7s - loss: 0.0238 - acc: 0.9855 - val_loss: 0.0423 - val_acc: 0.9782

==================================================================================================
	Training time : 0:01:17.433518
==================================================================================================
	Identification : 0.361
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 124
	25 : 2
	5 : 9

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 315
	25 : 2
	5 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 406
	POS = 28 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_107 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_108 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_107 (Embedding)       (None, 4, 406)       3539508     input_107[0][0]                  
__________________________________________________________________________________________________
embedding_108 (Embedding)       (None, 4, 28)        3332        input_108[0][0]                  
__________________________________________________________________________________________________
flatten_107 (Flatten)           (None, 1624)         0           embedding_107[0][0]              
__________________________________________________________________________________________________
flatten_108 (Flatten)           (None, 112)          0           embedding_108[0][0]              
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 1736)         0           flatten_107[0][0]                
                                                                 flatten_108[0][0]                
__________________________________________________________________________________________________
dense_72 (Dense)                (None, 8)            13896       concatenate_54[0][0]             
==================================================================================================
Total params: 3,556,736
Trainable params: 3,556,736
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33691, 1.0: 32623, 2.0: 653, 6.0: 311, 5.0: 308})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.026
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0391 - acc: 0.9823 - val_loss: 0.0339 - val_acc: 0.9828
Epoch 2/40
 - 7s - loss: 0.0268 - acc: 0.9847 - val_loss: 0.0371 - val_acc: 0.9815
Epoch 3/40
 - 7s - loss: 0.0238 - acc: 0.9855 - val_loss: 0.0402 - val_acc: 0.9787

==================================================================================================
	Training time : 0:01:17.411326
==================================================================================================
	Identification : 0.362
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 111
	25 : 2
	5 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 325
	25 : 2
	5 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
29_False_False_334_37_compact_True_36_relu_0.405_False_512_relu_0.2_adagrad_0.01_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_109 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_110 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_109 (Embedding)       (None, 4, 334)       1357710     input_109[0][0]                  
__________________________________________________________________________________________________
embedding_110 (Embedding)       (None, 4, 37)        5624        input_110[0][0]                  
__________________________________________________________________________________________________
flatten_109 (Flatten)           (None, 1336)         0           embedding_109[0][0]              
__________________________________________________________________________________________________
flatten_110 (Flatten)           (None, 148)          0           embedding_110[0][0]              
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 1484)         0           flatten_109[0][0]                
                                                                 flatten_110[0][0]                
__________________________________________________________________________________________________
dense_73 (Dense)                (None, 36)           53460       concatenate_55[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 36)           0           dense_73[0][0]                   
__________________________________________________________________________________________________
dense_74 (Dense)                (None, 8)            296         dropout_19[0][0]                 
==================================================================================================
Total params: 1,417,090
Trainable params: 1,417,090
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27040, 1.0: 26558, 2.0: 391, 4.0: 183, 6.0: 101, 5.0: 84, 3.0: 4})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 6s - loss: 0.0395 - acc: 0.9868 - val_loss: 0.0279 - val_acc: 0.9886
Epoch 2/40
 - 6s - loss: 0.0260 - acc: 0.9894 - val_loss: 0.0279 - val_acc: 0.9882
Epoch 3/40
 - 6s - loss: 0.0236 - acc: 0.9901 - val_loss: 0.0273 - val_acc: 0.9885

==================================================================================================
	Training time : 0:01:06.218308
==================================================================================================
	Identification : 0.016
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 2
	25 : 1
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 388
	25 : 7
	50 : 3
	100 : 1
	5 : 61

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_111 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_112 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_111 (Embedding)       (None, 4, 334)       1357710     input_111[0][0]                  
__________________________________________________________________________________________________
embedding_112 (Embedding)       (None, 4, 37)        5624        input_112[0][0]                  
__________________________________________________________________________________________________
flatten_111 (Flatten)           (None, 1336)         0           embedding_111[0][0]              
__________________________________________________________________________________________________
flatten_112 (Flatten)           (None, 148)          0           embedding_112[0][0]              
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 1484)         0           flatten_111[0][0]                
                                                                 flatten_112[0][0]                
__________________________________________________________________________________________________
dense_75 (Dense)                (None, 36)           53460       concatenate_56[0][0]             
__________________________________________________________________________________________________
dropout_20 (Dropout)            (None, 36)           0           dense_75[0][0]                   
__________________________________________________________________________________________________
dense_76 (Dense)                (None, 8)            296         dropout_20[0][0]                 
==================================================================================================
Total params: 1,417,090
Trainable params: 1,417,090
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27133, 1.0: 26493, 2.0: 386, 4.0: 182, 6.0: 95, 5.0: 68, 3.0: 4})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 8s - loss: 0.0424 - acc: 0.9865 - val_loss: 0.0258 - val_acc: 0.9892
Epoch 2/40
 - 6s - loss: 0.0265 - acc: 0.9896 - val_loss: 0.0254 - val_acc: 0.9889
Epoch 3/40
 - 6s - loss: 0.0237 - acc: 0.9901 - val_loss: 0.0262 - val_acc: 0.9891

==================================================================================================
	Training time : 0:01:08.008218
==================================================================================================
	Identification : 0.357
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 48
	25 : 5
	50 : 3
	100 : 1
	5 : 32

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 344
	25 : 7
	50 : 3
	100 : 1
	5 : 41

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_113 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_114 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_113 (Embedding)       (None, 4, 334)       1357710     input_113[0][0]                  
__________________________________________________________________________________________________
embedding_114 (Embedding)       (None, 4, 37)        5624        input_114[0][0]                  
__________________________________________________________________________________________________
flatten_113 (Flatten)           (None, 1336)         0           embedding_113[0][0]              
__________________________________________________________________________________________________
flatten_114 (Flatten)           (None, 148)          0           embedding_114[0][0]              
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 1484)         0           flatten_113[0][0]                
                                                                 flatten_114[0][0]                
__________________________________________________________________________________________________
dense_77 (Dense)                (None, 36)           53460       concatenate_57[0][0]             
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 36)           0           dense_77[0][0]                   
__________________________________________________________________________________________________
dense_78 (Dense)                (None, 8)            296         dropout_21[0][0]                 
==================================================================================================
Total params: 1,417,090
Trainable params: 1,417,090
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26956, 1.0: 26681, 2.0: 383, 4.0: 156, 6.0: 108, 5.0: 75, 3.0: 2})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 6s - loss: 0.0400 - acc: 0.9869 - val_loss: 0.0265 - val_acc: 0.9889
Epoch 2/40
 - 6s - loss: 0.0263 - acc: 0.9895 - val_loss: 0.0264 - val_acc: 0.9878
Epoch 3/40
 - 6s - loss: 0.0232 - acc: 0.9901 - val_loss: 0.0272 - val_acc: 0.9880

==================================================================================================
	Training time : 0:01:05.497937
==================================================================================================
	Identification : 0.019
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 3
	25 : 1
	5 : 2

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 386
	25 : 7
	50 : 3
	100 : 1
	5 : 60

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_115 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_116 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_115 (Embedding)       (None, 4, 334)       1349694     input_115[0][0]                  
__________________________________________________________________________________________________
embedding_116 (Embedding)       (None, 4, 37)        6253        input_116[0][0]                  
__________________________________________________________________________________________________
flatten_115 (Flatten)           (None, 1336)         0           embedding_115[0][0]              
__________________________________________________________________________________________________
flatten_116 (Flatten)           (None, 148)          0           embedding_116[0][0]              
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 1484)         0           flatten_115[0][0]                
                                                                 flatten_116[0][0]                
__________________________________________________________________________________________________
dense_79 (Dense)                (None, 36)           53460       concatenate_58[0][0]             
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 36)           0           dense_79[0][0]                   
__________________________________________________________________________________________________
dense_80 (Dense)                (None, 8)            296         dropout_22[0][0]                 
==================================================================================================
Total params: 1,409,703
Trainable params: 1,409,703
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27113, 1.0: 26599, 2.0: 275, 6.0: 185, 5.0: 46, 4.0: 43})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.0378 - acc: 0.9887 - val_loss: 0.0242 - val_acc: 0.9908
Epoch 2/40
 - 6s - loss: 0.0233 - acc: 0.9917 - val_loss: 0.0234 - val_acc: 0.9911
Epoch 3/40
 - 6s - loss: 0.0205 - acc: 0.9922 - val_loss: 0.0242 - val_acc: 0.9911

==================================================================================================
	Training time : 0:01:07.967524
==================================================================================================
	Identification : 0.022
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 3
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 319
	5 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_117 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_118 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_117 (Embedding)       (None, 4, 334)       1349694     input_117[0][0]                  
__________________________________________________________________________________________________
embedding_118 (Embedding)       (None, 4, 37)        6253        input_118[0][0]                  
__________________________________________________________________________________________________
flatten_117 (Flatten)           (None, 1336)         0           embedding_117[0][0]              
__________________________________________________________________________________________________
flatten_118 (Flatten)           (None, 148)          0           embedding_118[0][0]              
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 1484)         0           flatten_117[0][0]                
                                                                 flatten_118[0][0]                
__________________________________________________________________________________________________
dense_81 (Dense)                (None, 36)           53460       concatenate_59[0][0]             
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 36)           0           dense_81[0][0]                   
__________________________________________________________________________________________________
dense_82 (Dense)                (None, 8)            296         dropout_23[0][0]                 
==================================================================================================
Total params: 1,409,703
Trainable params: 1,409,703
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 26911, 1.0: 26773, 2.0: 316, 6.0: 178, 5.0: 43, 4.0: 40})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.0381 - acc: 0.9884 - val_loss: 0.0237 - val_acc: 0.9910
Epoch 2/40
 - 6s - loss: 0.0238 - acc: 0.9915 - val_loss: 0.0240 - val_acc: 0.9912
Epoch 3/40
 - 6s - loss: 0.0212 - acc: 0.9922 - val_loss: 0.0231 - val_acc: 0.9912

==================================================================================================
	Training time : 0:01:05.700147
==================================================================================================
	Identification : 0.006
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 321
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_119 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_120 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_119 (Embedding)       (None, 4, 334)       1349694     input_119[0][0]                  
__________________________________________________________________________________________________
embedding_120 (Embedding)       (None, 4, 37)        6253        input_120[0][0]                  
__________________________________________________________________________________________________
flatten_119 (Flatten)           (None, 1336)         0           embedding_119[0][0]              
__________________________________________________________________________________________________
flatten_120 (Flatten)           (None, 148)          0           embedding_120[0][0]              
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 1484)         0           flatten_119[0][0]                
                                                                 flatten_120[0][0]                
__________________________________________________________________________________________________
dense_83 (Dense)                (None, 36)           53460       concatenate_60[0][0]             
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 36)           0           dense_83[0][0]                   
__________________________________________________________________________________________________
dense_84 (Dense)                (None, 8)            296         dropout_24[0][0]                 
==================================================================================================
Total params: 1,409,703
Trainable params: 1,409,703
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({1.0: 26869, 0.0: 26851, 2.0: 288, 6.0: 173, 5.0: 45, 4.0: 35})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.0362 - acc: 0.9885 - val_loss: 0.0230 - val_acc: 0.9914
Epoch 2/40
 - 6s - loss: 0.0226 - acc: 0.9916 - val_loss: 0.0229 - val_acc: 0.9911
Epoch 3/40
 - 6s - loss: 0.0200 - acc: 0.9922 - val_loss: 0.0234 - val_acc: 0.9901

==================================================================================================
	Training time : 0:01:06.397337
==================================================================================================
	Identification : 0.006
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 321
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_121 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_122 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_121 (Embedding)       (None, 4, 334)       2452896     input_121[0][0]                  
__________________________________________________________________________________________________
embedding_122 (Embedding)       (None, 4, 37)        4070        input_122[0][0]                  
__________________________________________________________________________________________________
flatten_121 (Flatten)           (None, 1336)         0           embedding_121[0][0]              
__________________________________________________________________________________________________
flatten_122 (Flatten)           (None, 148)          0           embedding_122[0][0]              
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 1484)         0           flatten_121[0][0]                
                                                                 flatten_122[0][0]                
__________________________________________________________________________________________________
dense_85 (Dense)                (None, 36)           53460       concatenate_61[0][0]             
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 36)           0           dense_85[0][0]                   
__________________________________________________________________________________________________
dense_86 (Dense)                (None, 8)            296         dropout_25[0][0]                 
==================================================================================================
Total params: 2,510,722
Trainable params: 2,510,722
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27014, 1.0: 26406, 2.0: 526, 6.0: 272, 5.0: 268})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0501 - acc: 0.9818 - val_loss: 0.0351 - val_acc: 0.9842
Epoch 2/40
 - 6s - loss: 0.0331 - acc: 0.9858 - val_loss: 0.0349 - val_acc: 0.9847
Epoch 3/40
 - 6s - loss: 0.0303 - acc: 0.9866 - val_loss: 0.0351 - val_acc: 0.9847

==================================================================================================
	Training time : 0:01:07.885954
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 698
	25 : 2
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_123 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_124 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_123 (Embedding)       (None, 4, 334)       2452896     input_123[0][0]                  
__________________________________________________________________________________________________
embedding_124 (Embedding)       (None, 4, 37)        4070        input_124[0][0]                  
__________________________________________________________________________________________________
flatten_123 (Flatten)           (None, 1336)         0           embedding_123[0][0]              
__________________________________________________________________________________________________
flatten_124 (Flatten)           (None, 148)          0           embedding_124[0][0]              
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 1484)         0           flatten_123[0][0]                
                                                                 flatten_124[0][0]                
__________________________________________________________________________________________________
dense_87 (Dense)                (None, 36)           53460       concatenate_62[0][0]             
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 36)           0           dense_87[0][0]                   
__________________________________________________________________________________________________
dense_88 (Dense)                (None, 8)            296         dropout_26[0][0]                 
==================================================================================================
Total params: 2,510,722
Trainable params: 2,510,722
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26985, 1.0: 26519, 2.0: 481, 5.0: 258, 6.0: 243})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0466 - acc: 0.9823 - val_loss: 0.0360 - val_acc: 0.9844
Epoch 2/40
 - 6s - loss: 0.0323 - acc: 0.9860 - val_loss: 0.0345 - val_acc: 0.9851
Epoch 3/40
 - 6s - loss: 0.0290 - acc: 0.9867 - val_loss: 0.0359 - val_acc: 0.9848

==================================================================================================
	Training time : 0:01:07.993736
==================================================================================================
	Identification : 0.005
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	5 : 2

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 698
	25 : 2
	5 : 33

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_125 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_126 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_125 (Embedding)       (None, 4, 334)       2452896     input_125[0][0]                  
__________________________________________________________________________________________________
embedding_126 (Embedding)       (None, 4, 37)        4070        input_126[0][0]                  
__________________________________________________________________________________________________
flatten_125 (Flatten)           (None, 1336)         0           embedding_125[0][0]              
__________________________________________________________________________________________________
flatten_126 (Flatten)           (None, 148)          0           embedding_126[0][0]              
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 1484)         0           flatten_125[0][0]                
                                                                 flatten_126[0][0]                
__________________________________________________________________________________________________
dense_89 (Dense)                (None, 36)           53460       concatenate_63[0][0]             
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 36)           0           dense_89[0][0]                   
__________________________________________________________________________________________________
dense_90 (Dense)                (None, 8)            296         dropout_27[0][0]                 
==================================================================================================
Total params: 2,510,722
Trainable params: 2,510,722
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26806, 1.0: 26635, 2.0: 554, 6.0: 249, 5.0: 242})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0522 - acc: 0.9808 - val_loss: 0.0351 - val_acc: 0.9852
Epoch 2/40
 - 6s - loss: 0.0348 - acc: 0.9853 - val_loss: 0.0349 - val_acc: 0.9849
Epoch 3/40
 - 6s - loss: 0.0318 - acc: 0.9861 - val_loss: 0.0352 - val_acc: 0.9850

==================================================================================================
	Training time : 0:01:08.229407
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 698
	25 : 2
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_127 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_128 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_127 (Embedding)       (None, 4, 334)       1777214     input_127[0][0]                  
__________________________________________________________________________________________________
embedding_128 (Embedding)       (None, 4, 37)        6216        input_128[0][0]                  
__________________________________________________________________________________________________
flatten_127 (Flatten)           (None, 1336)         0           embedding_127[0][0]              
__________________________________________________________________________________________________
flatten_128 (Flatten)           (None, 148)          0           embedding_128[0][0]              
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 1484)         0           flatten_127[0][0]                
                                                                 flatten_128[0][0]                
__________________________________________________________________________________________________
dense_91 (Dense)                (None, 36)           53460       concatenate_64[0][0]             
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 36)           0           dense_91[0][0]                   
__________________________________________________________________________________________________
dense_92 (Dense)                (None, 8)            296         dropout_28[0][0]                 
==================================================================================================
Total params: 1,837,186
Trainable params: 1,837,186
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39686, 1.0: 39557, 2.0: 581, 4.0: 292, 6.0: 151, 5.0: 99, 3.0: 5})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 9s - loss: 0.0367 - acc: 0.9871 - val_loss: 0.0254 - val_acc: 0.9890
Epoch 2/40
 - 9s - loss: 0.0249 - acc: 0.9895 - val_loss: 0.0251 - val_acc: 0.9885
Epoch 3/40
 - 9s - loss: 0.0224 - acc: 0.9900 - val_loss: 0.0257 - val_acc: 0.9884

==================================================================================================
	Training time : 0:01:33.798516
==================================================================================================
	Identification : 0.345
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 43
	50 : 4
	5 : 33
	25 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 409
	25 : 6
	50 : 2
	5 : 31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_129 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_130 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_129 (Embedding)       (None, 4, 334)       1777214     input_129[0][0]                  
__________________________________________________________________________________________________
embedding_130 (Embedding)       (None, 4, 37)        6216        input_130[0][0]                  
__________________________________________________________________________________________________
flatten_129 (Flatten)           (None, 1336)         0           embedding_129[0][0]              
__________________________________________________________________________________________________
flatten_130 (Flatten)           (None, 148)          0           embedding_130[0][0]              
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 1484)         0           flatten_129[0][0]                
                                                                 flatten_130[0][0]                
__________________________________________________________________________________________________
dense_93 (Dense)                (None, 36)           53460       concatenate_65[0][0]             
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 36)           0           dense_93[0][0]                   
__________________________________________________________________________________________________
dense_94 (Dense)                (None, 8)            296         dropout_29[0][0]                 
==================================================================================================
Total params: 1,837,186
Trainable params: 1,837,186
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39989, 1.0: 39240, 2.0: 621, 4.0: 268, 6.0: 143, 5.0: 100, 3.0: 10})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 9s - loss: 0.0373 - acc: 0.9868 - val_loss: 0.0254 - val_acc: 0.9886
Epoch 2/40
 - 9s - loss: 0.0248 - acc: 0.9894 - val_loss: 0.0253 - val_acc: 0.9882
Epoch 3/40
 - 9s - loss: 0.0224 - acc: 0.9899 - val_loss: 0.0257 - val_acc: 0.9881

==================================================================================================
	Training time : 0:01:33.011903
==================================================================================================
	Identification : 0.021
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 2
	25 : 1
	5 : 2

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 448
	200 : 1
	50 : 5
	5 : 63
	25 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_131 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_132 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_131 (Embedding)       (None, 4, 334)       1777214     input_131[0][0]                  
__________________________________________________________________________________________________
embedding_132 (Embedding)       (None, 4, 37)        6216        input_132[0][0]                  
__________________________________________________________________________________________________
flatten_131 (Flatten)           (None, 1336)         0           embedding_131[0][0]              
__________________________________________________________________________________________________
flatten_132 (Flatten)           (None, 148)          0           embedding_132[0][0]              
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 1484)         0           flatten_131[0][0]                
                                                                 flatten_132[0][0]                
__________________________________________________________________________________________________
dense_95 (Dense)                (None, 36)           53460       concatenate_66[0][0]             
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 36)           0           dense_95[0][0]                   
__________________________________________________________________________________________________
dense_96 (Dense)                (None, 8)            296         dropout_30[0][0]                 
==================================================================================================
Total params: 1,837,186
Trainable params: 1,837,186
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40057, 1.0: 39215, 2.0: 581, 4.0: 258, 6.0: 154, 5.0: 95, 3.0: 11})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 9s - loss: 0.0379 - acc: 0.9871 - val_loss: 0.0242 - val_acc: 0.9894
Epoch 2/40
 - 9s - loss: 0.0252 - acc: 0.9894 - val_loss: 0.0233 - val_acc: 0.9890
Epoch 3/40
 - 9s - loss: 0.0228 - acc: 0.9899 - val_loss: 0.0238 - val_acc: 0.9889

==================================================================================================
	Training time : 0:01:34.129797
==================================================================================================
	Identification : 0.006
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 450
	200 : 1
	50 : 5
	5 : 64
	25 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_133 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_134 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_133 (Embedding)       (None, 4, 334)       1996318     input_133[0][0]                  
__________________________________________________________________________________________________
embedding_134 (Embedding)       (None, 4, 37)        8288        input_134[0][0]                  
__________________________________________________________________________________________________
flatten_133 (Flatten)           (None, 1336)         0           embedding_133[0][0]              
__________________________________________________________________________________________________
flatten_134 (Flatten)           (None, 148)          0           embedding_134[0][0]              
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 1484)         0           flatten_133[0][0]                
                                                                 flatten_134[0][0]                
__________________________________________________________________________________________________
dense_97 (Dense)                (None, 36)           53460       concatenate_67[0][0]             
__________________________________________________________________________________________________
dropout_31 (Dropout)            (None, 36)           0           dense_97[0][0]                   
__________________________________________________________________________________________________
dense_98 (Dense)                (None, 8)            296         dropout_31[0][0]                 
==================================================================================================
Total params: 2,058,362
Trainable params: 2,058,362
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47256, 1.0: 46887, 2.0: 540, 6.0: 293, 5.0: 83, 4.0: 64})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.0329 - acc: 0.9893 - val_loss: 0.0213 - val_acc: 0.9917
Epoch 2/40
 - 11s - loss: 0.0223 - acc: 0.9916 - val_loss: 0.0207 - val_acc: 0.9918
Epoch 3/40
 - 11s - loss: 0.0200 - acc: 0.9922 - val_loss: 0.0209 - val_acc: 0.9916

==================================================================================================
	Training time : 0:01:52.015849
==================================================================================================
	Identification : 0.028
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 6
	5 : 2

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 457
	25 : 1
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_135 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_136 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_135 (Embedding)       (None, 4, 334)       1996318     input_135[0][0]                  
__________________________________________________________________________________________________
embedding_136 (Embedding)       (None, 4, 37)        8288        input_136[0][0]                  
__________________________________________________________________________________________________
flatten_135 (Flatten)           (None, 1336)         0           embedding_135[0][0]              
__________________________________________________________________________________________________
flatten_136 (Flatten)           (None, 148)          0           embedding_136[0][0]              
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 1484)         0           flatten_135[0][0]                
                                                                 flatten_136[0][0]                
__________________________________________________________________________________________________
dense_99 (Dense)                (None, 36)           53460       concatenate_68[0][0]             
__________________________________________________________________________________________________
dropout_32 (Dropout)            (None, 36)           0           dense_99[0][0]                   
__________________________________________________________________________________________________
dense_100 (Dense)               (None, 8)            296         dropout_32[0][0]                 
==================================================================================================
Total params: 2,058,362
Trainable params: 2,058,362
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47202, 1.0: 47031, 2.0: 508, 6.0: 238, 5.0: 82, 4.0: 62})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.0341 - acc: 0.9890 - val_loss: 0.0209 - val_acc: 0.9913
Epoch 2/40
 - 11s - loss: 0.0225 - acc: 0.9915 - val_loss: 0.0205 - val_acc: 0.9913
Epoch 3/40
 - 11s - loss: 0.0202 - acc: 0.9920 - val_loss: 0.0204 - val_acc: 0.9921

==================================================================================================
	Training time : 0:01:53.043482
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 463
	25 : 1
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_137 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_138 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_137 (Embedding)       (None, 4, 334)       1996318     input_137[0][0]                  
__________________________________________________________________________________________________
embedding_138 (Embedding)       (None, 4, 37)        8288        input_138[0][0]                  
__________________________________________________________________________________________________
flatten_137 (Flatten)           (None, 1336)         0           embedding_137[0][0]              
__________________________________________________________________________________________________
flatten_138 (Flatten)           (None, 148)          0           embedding_138[0][0]              
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 1484)         0           flatten_137[0][0]                
                                                                 flatten_138[0][0]                
__________________________________________________________________________________________________
dense_101 (Dense)               (None, 36)           53460       concatenate_69[0][0]             
__________________________________________________________________________________________________
dropout_33 (Dropout)            (None, 36)           0           dense_101[0][0]                  
__________________________________________________________________________________________________
dense_102 (Dense)               (None, 8)            296         dropout_33[0][0]                 
==================================================================================================
Total params: 2,058,362
Trainable params: 2,058,362
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47390, 1.0: 46754, 2.0: 540, 6.0: 266, 5.0: 91, 4.0: 82})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.0314 - acc: 0.9895 - val_loss: 0.0214 - val_acc: 0.9914
Epoch 2/40
 - 11s - loss: 0.0211 - acc: 0.9918 - val_loss: 0.0211 - val_acc: 0.9914
Epoch 3/40
 - 11s - loss: 0.0190 - acc: 0.9922 - val_loss: 0.0213 - val_acc: 0.9915

==================================================================================================
	Training time : 0:01:49.484135
==================================================================================================
	Identification : 0.007
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 463
	25 : 1
	5 : 28

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_139 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_140 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_139 (Embedding)       (None, 4, 334)       2911812     input_139[0][0]                  
__________________________________________________________________________________________________
embedding_140 (Embedding)       (None, 4, 37)        4403        input_140[0][0]                  
__________________________________________________________________________________________________
flatten_139 (Flatten)           (None, 1336)         0           embedding_139[0][0]              
__________________________________________________________________________________________________
flatten_140 (Flatten)           (None, 148)          0           embedding_140[0][0]              
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 1484)         0           flatten_139[0][0]                
                                                                 flatten_140[0][0]                
__________________________________________________________________________________________________
dense_103 (Dense)               (None, 36)           53460       concatenate_70[0][0]             
__________________________________________________________________________________________________
dropout_34 (Dropout)            (None, 36)           0           dense_103[0][0]                  
__________________________________________________________________________________________________
dense_104 (Dense)               (None, 8)            296         dropout_34[0][0]                 
==================================================================================================
Total params: 2,969,971
Trainable params: 2,969,971
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33367, 1.0: 32972, 2.0: 674, 5.0: 300, 6.0: 273})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 8s - loss: 0.0478 - acc: 0.9818 - val_loss: 0.0335 - val_acc: 0.9854
Epoch 2/40
 - 8s - loss: 0.0330 - acc: 0.9857 - val_loss: 0.0340 - val_acc: 0.9855
Epoch 3/40
 - 8s - loss: 0.0303 - acc: 0.9863 - val_loss: 0.0343 - val_acc: 0.9855

==================================================================================================
	Training time : 0:01:21.935228
==================================================================================================
	Identification : 0.016
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1
	25 : 1
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 435
	25 : 3
	5 : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_141 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_142 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_141 (Embedding)       (None, 4, 334)       2911812     input_141[0][0]                  
__________________________________________________________________________________________________
embedding_142 (Embedding)       (None, 4, 37)        4403        input_142[0][0]                  
__________________________________________________________________________________________________
flatten_141 (Flatten)           (None, 1336)         0           embedding_141[0][0]              
__________________________________________________________________________________________________
flatten_142 (Flatten)           (None, 148)          0           embedding_142[0][0]              
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 1484)         0           flatten_141[0][0]                
                                                                 flatten_142[0][0]                
__________________________________________________________________________________________________
dense_105 (Dense)               (None, 36)           53460       concatenate_71[0][0]             
__________________________________________________________________________________________________
dropout_35 (Dropout)            (None, 36)           0           dense_105[0][0]                  
__________________________________________________________________________________________________
dense_106 (Dense)               (None, 8)            296         dropout_35[0][0]                 
==================================================================================================
Total params: 2,969,971
Trainable params: 2,969,971
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33256, 1.0: 33037, 2.0: 648, 5.0: 342, 6.0: 303})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 8s - loss: 0.0510 - acc: 0.9807 - val_loss: 0.0362 - val_acc: 0.9840
Epoch 2/40
 - 8s - loss: 0.0350 - acc: 0.9850 - val_loss: 0.0358 - val_acc: 0.9843
Epoch 3/40
 - 8s - loss: 0.0314 - acc: 0.9858 - val_loss: 0.0373 - val_acc: 0.9840

==================================================================================================
	Training time : 0:01:21.752655
==================================================================================================
	Identification : 0.079
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1
	25 : 2
	5 : 4

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 435
	25 : 1
	5 : 21

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 334
	POS = 37 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_143 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_144 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_143 (Embedding)       (None, 4, 334)       2911812     input_143[0][0]                  
__________________________________________________________________________________________________
embedding_144 (Embedding)       (None, 4, 37)        4403        input_144[0][0]                  
__________________________________________________________________________________________________
flatten_143 (Flatten)           (None, 1336)         0           embedding_143[0][0]              
__________________________________________________________________________________________________
flatten_144 (Flatten)           (None, 148)          0           embedding_144[0][0]              
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 1484)         0           flatten_143[0][0]                
                                                                 flatten_144[0][0]                
__________________________________________________________________________________________________
dense_107 (Dense)               (None, 36)           53460       concatenate_72[0][0]             
__________________________________________________________________________________________________
dropout_36 (Dropout)            (None, 36)           0           dense_107[0][0]                  
__________________________________________________________________________________________________
dense_108 (Dense)               (None, 8)            296         dropout_36[0][0]                 
==================================================================================================
Total params: 2,969,971
Trainable params: 2,969,971
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33691, 1.0: 32623, 2.0: 653, 6.0: 311, 5.0: 308})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 8s - loss: 0.0472 - acc: 0.9814 - val_loss: 0.0342 - val_acc: 0.9850
Epoch 2/40
 - 8s - loss: 0.0335 - acc: 0.9854 - val_loss: 0.0340 - val_acc: 0.9851
Epoch 3/40
 - 8s - loss: 0.0304 - acc: 0.9862 - val_loss: 0.0346 - val_acc: 0.9850

==================================================================================================
	Training time : 0:01:22.257691
==================================================================================================
	Identification : 0.035
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 436
	25 : 2
	5 : 25

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
29_True_False_392_30_compact_False_5_relu_0.309_False_512_relu_0.2_adagrad_0.011_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_145 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_146 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_145 (Embedding)       (None, 4, 392)       1593480     input_145[0][0]                  
__________________________________________________________________________________________________
embedding_146 (Embedding)       (None, 4, 30)        4560        input_146[0][0]                  
__________________________________________________________________________________________________
flatten_145 (Flatten)           (None, 1568)         0           embedding_145[0][0]              
__________________________________________________________________________________________________
flatten_146 (Flatten)           (None, 120)          0           embedding_146[0][0]              
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 1688)         0           flatten_145[0][0]                
                                                                 flatten_146[0][0]                
__________________________________________________________________________________________________
dense_109 (Dense)               (None, 8)            13512       concatenate_73[0][0]             
==================================================================================================
Total params: 1,611,552
Trainable params: 1,611,552
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34614, 1.0: 26597, 2.0: 7676, 6.0: 2580, 4.0: 2172, 5.0: 1781, 3.0: 110})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 7s - loss: 0.0516 - acc: 0.9867 - val_loss: 0.0336 - val_acc: 0.9912
Epoch 2/40
 - 7s - loss: 0.0293 - acc: 0.9918 - val_loss: 0.0330 - val_acc: 0.9918
Epoch 3/40
 - 7s - loss: 0.0273 - acc: 0.9922 - val_loss: 0.0331 - val_acc: 0.9918

==================================================================================================
	Training time : 0:01:12.772593
==================================================================================================
	Identification : 0.629
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 147
	25 : 7
	50 : 3
	100 : 1
	5 : 57

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 245
	25 : 5
	50 : 1
	100 : 1
	5 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_147 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_148 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_147 (Embedding)       (None, 4, 392)       1593480     input_147[0][0]                  
__________________________________________________________________________________________________
embedding_148 (Embedding)       (None, 4, 30)        4560        input_148[0][0]                  
__________________________________________________________________________________________________
flatten_147 (Flatten)           (None, 1568)         0           embedding_147[0][0]              
__________________________________________________________________________________________________
flatten_148 (Flatten)           (None, 120)          0           embedding_148[0][0]              
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 1688)         0           flatten_147[0][0]                
                                                                 flatten_148[0][0]                
__________________________________________________________________________________________________
dense_110 (Dense)               (None, 8)            13512       concatenate_74[0][0]             
==================================================================================================
Total params: 1,611,552
Trainable params: 1,611,552
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34700, 1.0: 26548, 2.0: 7644, 6.0: 2596, 4.0: 2188, 5.0: 1731, 3.0: 123})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 7s - loss: 0.0517 - acc: 0.9866 - val_loss: 0.0331 - val_acc: 0.9917
Epoch 2/40
 - 7s - loss: 0.0293 - acc: 0.9918 - val_loss: 0.0329 - val_acc: 0.9918
Epoch 3/40
 - 7s - loss: 0.0274 - acc: 0.9922 - val_loss: 0.0329 - val_acc: 0.9920

==================================================================================================
	Training time : 0:01:13.929658
==================================================================================================
	Identification : 0.68
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 149
	25 : 7
	50 : 3
	100 : 1
	5 : 57

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 243
	25 : 5
	5 : 6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_149 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_150 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_149 (Embedding)       (None, 4, 392)       1593480     input_149[0][0]                  
__________________________________________________________________________________________________
embedding_150 (Embedding)       (None, 4, 30)        4560        input_150[0][0]                  
__________________________________________________________________________________________________
flatten_149 (Flatten)           (None, 1568)         0           embedding_149[0][0]              
__________________________________________________________________________________________________
flatten_150 (Flatten)           (None, 120)          0           embedding_150[0][0]              
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 1688)         0           flatten_149[0][0]                
                                                                 flatten_150[0][0]                
__________________________________________________________________________________________________
dense_111 (Dense)               (None, 8)            13512       concatenate_75[0][0]             
==================================================================================================
Total params: 1,611,552
Trainable params: 1,611,552
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34383, 1.0: 26780, 2.0: 7698, 6.0: 2665, 4.0: 2049, 5.0: 1826, 3.0: 129})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 7s - loss: 0.0513 - acc: 0.9868 - val_loss: 0.0347 - val_acc: 0.9912
Epoch 2/40
 - 7s - loss: 0.0292 - acc: 0.9917 - val_loss: 0.0341 - val_acc: 0.9918
Epoch 3/40
 - 7s - loss: 0.0273 - acc: 0.9922 - val_loss: 0.0342 - val_acc: 0.9918

==================================================================================================
	Training time : 0:01:14.092475
==================================================================================================
	Identification : 0.62
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 145
	25 : 6
	50 : 3
	5 : 55

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 247
	25 : 4
	50 : 3
	100 : 1
	5 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_151 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_152 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_151 (Embedding)       (None, 4, 392)       1584072     input_151[0][0]                  
__________________________________________________________________________________________________
embedding_152 (Embedding)       (None, 4, 30)        5070        input_152[0][0]                  
__________________________________________________________________________________________________
flatten_151 (Flatten)           (None, 1568)         0           embedding_151[0][0]              
__________________________________________________________________________________________________
flatten_152 (Flatten)           (None, 120)          0           embedding_152[0][0]              
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 1688)         0           flatten_151[0][0]                
                                                                 flatten_152[0][0]                
__________________________________________________________________________________________________
dense_112 (Dense)               (None, 8)            13512       concatenate_76[0][0]             
==================================================================================================
Total params: 1,602,654
Trainable params: 1,602,654
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 34831, 1.0: 26916, 2.0: 8387, 6.0: 4701, 5.0: 1408, 4.0: 875})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 8s - loss: 0.0462 - acc: 0.9884 - val_loss: 0.0312 - val_acc: 0.9922
Epoch 2/40
 - 7s - loss: 0.0262 - acc: 0.9931 - val_loss: 0.0312 - val_acc: 0.9928
Epoch 3/40
 - 7s - loss: 0.0245 - acc: 0.9936 - val_loss: 0.0315 - val_acc: 0.9930

==================================================================================================
	Training time : 0:01:18.189619
==================================================================================================
	Identification : 0.406
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 74
	5 : 10

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 248
	5 : 5

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_153 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_154 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_153 (Embedding)       (None, 4, 392)       1584072     input_153[0][0]                  
__________________________________________________________________________________________________
embedding_154 (Embedding)       (None, 4, 30)        5070        input_154[0][0]                  
__________________________________________________________________________________________________
flatten_153 (Flatten)           (None, 1568)         0           embedding_153[0][0]              
__________________________________________________________________________________________________
flatten_154 (Flatten)           (None, 120)          0           embedding_154[0][0]              
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 1688)         0           flatten_153[0][0]                
                                                                 flatten_154[0][0]                
__________________________________________________________________________________________________
dense_113 (Dense)               (None, 8)            13512       concatenate_77[0][0]             
==================================================================================================
Total params: 1,602,654
Trainable params: 1,602,654
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 35012, 1.0: 26875, 2.0: 8255, 6.0: 4666, 5.0: 1370, 4.0: 940})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 7s - loss: 0.0458 - acc: 0.9884 - val_loss: 0.0307 - val_acc: 0.9925
Epoch 2/40
 - 7s - loss: 0.0261 - acc: 0.9932 - val_loss: 0.0307 - val_acc: 0.9931
Epoch 3/40
 - 7s - loss: 0.0244 - acc: 0.9936 - val_loss: 0.0312 - val_acc: 0.9931

==================================================================================================
	Training time : 0:01:13.519447
==================================================================================================
	Identification : 0.425
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 76
	5 : 12

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 245
	5 : 3

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_155 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_156 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_155 (Embedding)       (None, 4, 392)       1584072     input_155[0][0]                  
__________________________________________________________________________________________________
embedding_156 (Embedding)       (None, 4, 30)        5070        input_156[0][0]                  
__________________________________________________________________________________________________
flatten_155 (Flatten)           (None, 1568)         0           embedding_155[0][0]              
__________________________________________________________________________________________________
flatten_156 (Flatten)           (None, 120)          0           embedding_156[0][0]              
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 1688)         0           flatten_155[0][0]                
                                                                 flatten_156[0][0]                
__________________________________________________________________________________________________
dense_114 (Dense)               (None, 8)            13512       concatenate_78[0][0]             
==================================================================================================
Total params: 1,602,654
Trainable params: 1,602,654
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 34910, 1.0: 26928, 2.0: 8424, 6.0: 4544, 5.0: 1403, 4.0: 909})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 7s - loss: 0.0462 - acc: 0.9884 - val_loss: 0.0311 - val_acc: 0.9924
Epoch 2/40
 - 7s - loss: 0.0262 - acc: 0.9931 - val_loss: 0.0309 - val_acc: 0.9928
Epoch 3/40
 - 7s - loss: 0.0244 - acc: 0.9935 - val_loss: 0.0314 - val_acc: 0.9930

==================================================================================================
	Training time : 0:01:14.095495
==================================================================================================
	Identification : 0.409
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 72
	5 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 249
	5 : 4

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_157 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_158 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_157 (Embedding)       (None, 4, 392)       2878848     input_157[0][0]                  
__________________________________________________________________________________________________
embedding_158 (Embedding)       (None, 4, 30)        3300        input_158[0][0]                  
__________________________________________________________________________________________________
flatten_157 (Flatten)           (None, 1568)         0           embedding_157[0][0]              
__________________________________________________________________________________________________
flatten_158 (Flatten)           (None, 120)          0           embedding_158[0][0]              
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 1688)         0           flatten_157[0][0]                
                                                                 flatten_158[0][0]                
__________________________________________________________________________________________________
dense_115 (Dense)               (None, 8)            13512       concatenate_79[0][0]             
==================================================================================================
Total params: 2,895,660
Trainable params: 2,895,660
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40694, 1.0: 26871, 2.0: 13587, 5.0: 6787, 6.0: 6427})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 10s - loss: 0.0501 - acc: 0.9876 - val_loss: 0.0342 - val_acc: 0.9923
Epoch 2/40
 - 10s - loss: 0.0320 - acc: 0.9921 - val_loss: 0.0353 - val_acc: 0.9925
Epoch 3/40
 - 10s - loss: 0.0305 - acc: 0.9923 - val_loss: 0.0363 - val_acc: 0.9926

==================================================================================================
	Training time : 0:01:25.422428
==================================================================================================
	Identification : 0.412
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 147
	25 : 2
	5 : 32

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 556
	25 : 1
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_159 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_160 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_159 (Embedding)       (None, 4, 392)       2878848     input_159[0][0]                  
__________________________________________________________________________________________________
embedding_160 (Embedding)       (None, 4, 30)        3300        input_160[0][0]                  
__________________________________________________________________________________________________
flatten_159 (Flatten)           (None, 1568)         0           embedding_159[0][0]              
__________________________________________________________________________________________________
flatten_160 (Flatten)           (None, 120)          0           embedding_160[0][0]              
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 1688)         0           flatten_159[0][0]                
                                                                 flatten_160[0][0]                
__________________________________________________________________________________________________
dense_116 (Dense)               (None, 8)            13512       concatenate_80[0][0]             
==================================================================================================
Total params: 2,895,660
Trainable params: 2,895,660
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40535, 1.0: 26783, 2.0: 13925, 5.0: 6771, 6.0: 6352})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 10s - loss: 0.0500 - acc: 0.9877 - val_loss: 0.0354 - val_acc: 0.9919
Epoch 2/40
 - 10s - loss: 0.0319 - acc: 0.9921 - val_loss: 0.0364 - val_acc: 0.9920
Epoch 3/40
 - 10s - loss: 0.0305 - acc: 0.9924 - val_loss: 0.0375 - val_acc: 0.9920

==================================================================================================
	Training time : 0:01:24.821615
==================================================================================================
	Identification : 0.416
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 151
	25 : 2
	5 : 32

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 552
	25 : 1
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_161 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_162 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_161 (Embedding)       (None, 4, 392)       2878848     input_161[0][0]                  
__________________________________________________________________________________________________
embedding_162 (Embedding)       (None, 4, 30)        3300        input_162[0][0]                  
__________________________________________________________________________________________________
flatten_161 (Flatten)           (None, 1568)         0           embedding_161[0][0]              
__________________________________________________________________________________________________
flatten_162 (Flatten)           (None, 120)          0           embedding_162[0][0]              
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 1688)         0           flatten_161[0][0]                
                                                                 flatten_162[0][0]                
__________________________________________________________________________________________________
dense_117 (Dense)               (None, 8)            13512       concatenate_81[0][0]             
==================================================================================================
Total params: 2,895,660
Trainable params: 2,895,660
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40547, 1.0: 26765, 2.0: 13742, 5.0: 6926, 6.0: 6386})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 10s - loss: 0.0503 - acc: 0.9876 - val_loss: 0.0353 - val_acc: 0.9918
Epoch 2/40
 - 10s - loss: 0.0319 - acc: 0.9921 - val_loss: 0.0366 - val_acc: 0.9920
Epoch 3/40
 - 10s - loss: 0.0304 - acc: 0.9924 - val_loss: 0.0375 - val_acc: 0.9921

==================================================================================================
	Training time : 0:01:25.683544
==================================================================================================
	Identification : 0.412
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 149
	25 : 2
	5 : 31

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 555
	25 : 1
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_163 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_164 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_163 (Embedding)       (None, 4, 392)       2085832     input_163[0][0]                  
__________________________________________________________________________________________________
embedding_164 (Embedding)       (None, 4, 30)        5040        input_164[0][0]                  
__________________________________________________________________________________________________
flatten_163 (Flatten)           (None, 1568)         0           embedding_163[0][0]              
__________________________________________________________________________________________________
flatten_164 (Flatten)           (None, 120)          0           embedding_164[0][0]              
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 1688)         0           flatten_163[0][0]                
                                                                 flatten_164[0][0]                
__________________________________________________________________________________________________
dense_118 (Dense)               (None, 8)            13512       concatenate_82[0][0]             
==================================================================================================
Total params: 2,104,384
Trainable params: 2,104,384
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 49807, 1.0: 39705, 2.0: 10488, 6.0: 3535, 4.0: 2659, 5.0: 2534, 3.0: 146})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 11s - loss: 0.0490 - acc: 0.9867 - val_loss: 0.0353 - val_acc: 0.9905
Epoch 2/40
 - 11s - loss: 0.0296 - acc: 0.9913 - val_loss: 0.0348 - val_acc: 0.9908
Epoch 3/40
 - 11s - loss: 0.0279 - acc: 0.9917 - val_loss: 0.0351 - val_acc: 0.9910

==================================================================================================
	Training time : 0:01:43.578977
==================================================================================================
	Identification : 0.481
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 111
	200 : 1
	50 : 5
	5 : 55
	25 : 7

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 345
	25 : 7
	50 : 2
	5 : 20

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_165 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_166 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_165 (Embedding)       (None, 4, 392)       2085832     input_165[0][0]                  
__________________________________________________________________________________________________
embedding_166 (Embedding)       (None, 4, 30)        5040        input_166[0][0]                  
__________________________________________________________________________________________________
flatten_165 (Flatten)           (None, 1568)         0           embedding_165[0][0]              
__________________________________________________________________________________________________
flatten_166 (Flatten)           (None, 120)          0           embedding_166[0][0]              
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 1688)         0           flatten_165[0][0]                
                                                                 flatten_166[0][0]                
__________________________________________________________________________________________________
dense_119 (Dense)               (None, 8)            13512       concatenate_83[0][0]             
==================================================================================================
Total params: 2,104,384
Trainable params: 2,104,384
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 49990, 1.0: 39585, 2.0: 10463, 6.0: 3521, 4.0: 2666, 5.0: 2509, 3.0: 140})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 11s - loss: 0.0491 - acc: 0.9867 - val_loss: 0.0355 - val_acc: 0.9901
Epoch 2/40
 - 11s - loss: 0.0298 - acc: 0.9914 - val_loss: 0.0355 - val_acc: 0.9905
Epoch 3/40
 - 11s - loss: 0.0279 - acc: 0.9917 - val_loss: 0.0354 - val_acc: 0.9906

==================================================================================================
	Training time : 0:01:44.079402
==================================================================================================
	Identification : 0.431
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 110
	25 : 4
	50 : 1
	5 : 49

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 347
	200 : 1
	50 : 5
	5 : 26
	25 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_167 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_168 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_167 (Embedding)       (None, 4, 392)       2085832     input_167[0][0]                  
__________________________________________________________________________________________________
embedding_168 (Embedding)       (None, 4, 30)        5040        input_168[0][0]                  
__________________________________________________________________________________________________
flatten_167 (Flatten)           (None, 1568)         0           embedding_167[0][0]              
__________________________________________________________________________________________________
flatten_168 (Flatten)           (None, 120)          0           embedding_168[0][0]              
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 1688)         0           flatten_167[0][0]                
                                                                 flatten_168[0][0]                
__________________________________________________________________________________________________
dense_120 (Dense)               (None, 8)            13512       concatenate_84[0][0]             
==================================================================================================
Total params: 2,104,384
Trainable params: 2,104,384
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 50358, 1.0: 39454, 2.0: 10378, 6.0: 3406, 4.0: 2705, 5.0: 2431, 3.0: 142})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 12s - loss: 0.0489 - acc: 0.9868 - val_loss: 0.0342 - val_acc: 0.9905
Epoch 2/40
 - 11s - loss: 0.0297 - acc: 0.9914 - val_loss: 0.0335 - val_acc: 0.9907
Epoch 3/40
 - 11s - loss: 0.0280 - acc: 0.9917 - val_loss: 0.0338 - val_acc: 0.9909

==================================================================================================
	Training time : 0:01:43.837227
==================================================================================================
	Identification : 0.462
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 109
	200 : 1
	50 : 5
	5 : 54
	25 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 348
	200 : 1
	50 : 1
	5 : 22
	25 : 6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_169 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_170 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_169 (Embedding)       (None, 4, 392)       2342984     input_169[0][0]                  
__________________________________________________________________________________________________
embedding_170 (Embedding)       (None, 4, 30)        6720        input_170[0][0]                  
__________________________________________________________________________________________________
flatten_169 (Flatten)           (None, 1568)         0           embedding_169[0][0]              
__________________________________________________________________________________________________
flatten_170 (Flatten)           (None, 120)          0           embedding_170[0][0]              
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 1688)         0           flatten_169[0][0]                
                                                                 flatten_170[0][0]                
__________________________________________________________________________________________________
dense_121 (Dense)               (None, 8)            13512       concatenate_85[0][0]             
==================================================================================================
Total params: 2,363,216
Trainable params: 2,363,216
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 60139, 1.0: 46956, 2.0: 13150, 6.0: 7383, 5.0: 2101, 4.0: 1366})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 13s - loss: 0.0422 - acc: 0.9890 - val_loss: 0.0311 - val_acc: 0.9922
Epoch 2/40
 - 13s - loss: 0.0265 - acc: 0.9929 - val_loss: 0.0313 - val_acc: 0.9926
Epoch 3/40
 - 13s - loss: 0.0250 - acc: 0.9932 - val_loss: 0.0316 - val_acc: 0.9927

==================================================================================================
	Training time : 0:02:00.316413
==================================================================================================
	Identification : 0.516
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 129
	25 : 1
	5 : 28

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 336
	25 : 1
	5 : 4

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_171 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_172 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_171 (Embedding)       (None, 4, 392)       2342984     input_171[0][0]                  
__________________________________________________________________________________________________
embedding_172 (Embedding)       (None, 4, 30)        6720        input_172[0][0]                  
__________________________________________________________________________________________________
flatten_171 (Flatten)           (None, 1568)         0           embedding_171[0][0]              
__________________________________________________________________________________________________
flatten_172 (Flatten)           (None, 120)          0           embedding_172[0][0]              
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 1688)         0           flatten_171[0][0]                
                                                                 flatten_172[0][0]                
__________________________________________________________________________________________________
dense_122 (Dense)               (None, 8)            13512       concatenate_86[0][0]             
==================================================================================================
Total params: 2,363,216
Trainable params: 2,363,216
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 60034, 1.0: 46992, 2.0: 13114, 6.0: 7470, 5.0: 2067, 4.0: 1418})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 13s - loss: 0.0425 - acc: 0.9889 - val_loss: 0.0298 - val_acc: 0.9926
Epoch 2/40
 - 13s - loss: 0.0266 - acc: 0.9929 - val_loss: 0.0297 - val_acc: 0.9928
Epoch 3/40
 - 13s - loss: 0.0251 - acc: 0.9932 - val_loss: 0.0300 - val_acc: 0.9930

==================================================================================================
	Training time : 0:01:59.549095
==================================================================================================
	Identification : 0.52
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 128
	25 : 1
	5 : 28

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 337
	5 : 3

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_173 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_174 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_173 (Embedding)       (None, 4, 392)       2342984     input_173[0][0]                  
__________________________________________________________________________________________________
embedding_174 (Embedding)       (None, 4, 30)        6720        input_174[0][0]                  
__________________________________________________________________________________________________
flatten_173 (Flatten)           (None, 1568)         0           embedding_173[0][0]              
__________________________________________________________________________________________________
flatten_174 (Flatten)           (None, 120)          0           embedding_174[0][0]              
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 1688)         0           flatten_173[0][0]                
                                                                 flatten_174[0][0]                
__________________________________________________________________________________________________
dense_123 (Dense)               (None, 8)            13512       concatenate_87[0][0]             
==================================================================================================
Total params: 2,363,216
Trainable params: 2,363,216
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 60037, 1.0: 46898, 2.0: 13080, 6.0: 7450, 5.0: 2192, 4.0: 1438})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 13s - loss: 0.0424 - acc: 0.9890 - val_loss: 0.0292 - val_acc: 0.9927
Epoch 2/40
 - 13s - loss: 0.0267 - acc: 0.9929 - val_loss: 0.0292 - val_acc: 0.9929
Epoch 3/40
 - 13s - loss: 0.0251 - acc: 0.9933 - val_loss: 0.0295 - val_acc: 0.9930

==================================================================================================
	Training time : 0:02:01.572835
==================================================================================================
	Identification : 0.516
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 127
	25 : 1
	5 : 28

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 338
	5 : 3

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_175 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_176 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_175 (Embedding)       (None, 4, 392)       3417456     input_175[0][0]                  
__________________________________________________________________________________________________
embedding_176 (Embedding)       (None, 4, 30)        3570        input_176[0][0]                  
__________________________________________________________________________________________________
flatten_175 (Flatten)           (None, 1568)         0           embedding_175[0][0]              
__________________________________________________________________________________________________
flatten_176 (Flatten)           (None, 120)          0           embedding_176[0][0]              
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 1688)         0           flatten_175[0][0]                
                                                                 flatten_176[0][0]                
__________________________________________________________________________________________________
dense_124 (Dense)               (None, 8)            13512       concatenate_88[0][0]             
==================================================================================================
Total params: 3,434,538
Trainable params: 3,434,538
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 1159213
	5 Labels in train : Counter({0.0: 499647, 1.0: 333227, 2.0: 166420, 5.0: 83436, 6.0: 76483})
	5 Labels in valid : Counter({0.0: 49903, 1.0: 33357, 2.0: 16598, 5.0: 8374, 6.0: 7690})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1043291 samples, validate on 115922 samples
Epoch 1/40
 - 12s - loss: 0.0496 - acc: 0.9875 - val_loss: 0.0371 - val_acc: 0.9914
Epoch 2/40
 - 13s - loss: 0.0325 - acc: 0.9919 - val_loss: 0.0389 - val_acc: 0.9914
Epoch 3/40
 - 13s - loss: 0.0310 - acc: 0.9921 - val_loss: 0.0400 - val_acc: 0.9915

==================================================================================================
	Training time : 0:01:42.508969
==================================================================================================
	Identification : 0.326
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 67
	25 : 2
	5 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 369
	25 : 2
	5 : 4

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_177 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_178 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_177 (Embedding)       (None, 4, 392)       3417456     input_177[0][0]                  
__________________________________________________________________________________________________
embedding_178 (Embedding)       (None, 4, 30)        3570        input_178[0][0]                  
__________________________________________________________________________________________________
flatten_177 (Flatten)           (None, 1568)         0           embedding_177[0][0]              
__________________________________________________________________________________________________
flatten_178 (Flatten)           (None, 120)          0           embedding_178[0][0]              
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 1688)         0           flatten_177[0][0]                
                                                                 flatten_178[0][0]                
__________________________________________________________________________________________________
dense_125 (Dense)               (None, 8)            13512       concatenate_89[0][0]             
==================================================================================================
Total params: 3,434,538
Trainable params: 3,434,538
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 1159213
	5 Labels in train : Counter({0.0: 499647, 1.0: 333227, 2.0: 166420, 5.0: 83436, 6.0: 76483})
	5 Labels in valid : Counter({0.0: 50049, 1.0: 33328, 2.0: 16585, 5.0: 8392, 6.0: 7568})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1043291 samples, validate on 115922 samples
Epoch 1/40
 - 12s - loss: 0.0495 - acc: 0.9875 - val_loss: 0.0379 - val_acc: 0.9912
Epoch 2/40
 - 12s - loss: 0.0324 - acc: 0.9919 - val_loss: 0.0395 - val_acc: 0.9913
Epoch 3/40
 - 12s - loss: 0.0309 - acc: 0.9921 - val_loss: 0.0410 - val_acc: 0.9913

==================================================================================================
	Training time : 0:01:40.528681
==================================================================================================
	Identification : 0.332
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 61
	25 : 3
	5 : 20

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 375
	25 : 1
	5 : 6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 392
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_179 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_180 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_179 (Embedding)       (None, 4, 392)       3417456     input_179[0][0]                  
__________________________________________________________________________________________________
embedding_180 (Embedding)       (None, 4, 30)        3570        input_180[0][0]                  
__________________________________________________________________________________________________
flatten_179 (Flatten)           (None, 1568)         0           embedding_179[0][0]              
__________________________________________________________________________________________________
flatten_180 (Flatten)           (None, 120)          0           embedding_180[0][0]              
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 1688)         0           flatten_179[0][0]                
                                                                 flatten_180[0][0]                
__________________________________________________________________________________________________
dense_126 (Dense)               (None, 8)            13512       concatenate_90[0][0]             
==================================================================================================
Total params: 3,434,538
Trainable params: 3,434,538
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 1159213
	5 Labels in train : Counter({0.0: 499647, 1.0: 333227, 2.0: 166420, 5.0: 83436, 6.0: 76483})
	5 Labels in valid : Counter({0.0: 49837, 1.0: 33492, 2.0: 16546, 5.0: 8378, 6.0: 7669})
	Favorisation Coeff : 29

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1043291 samples, validate on 115922 samples
Epoch 1/40
 - 12s - loss: 0.0494 - acc: 0.9875 - val_loss: 0.0372 - val_acc: 0.9916
Epoch 2/40
 - 12s - loss: 0.0325 - acc: 0.9919 - val_loss: 0.0389 - val_acc: 0.9915
Epoch 3/40
 - 12s - loss: 0.0310 - acc: 0.9921 - val_loss: 0.0401 - val_acc: 0.9917

==================================================================================================
	Training time : 0:01:41.417149
==================================================================================================
	Identification : 0.324
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 61
	25 : 3
	5 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 375
	5 : 6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
23_True_True_293_68_compact_True_17_relu_0.326_False_512_relu_0.2_adagrad_0.08_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_181 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_182 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_181 (Embedding)       (None, 4, 293)       820400      input_181[0][0]                  
__________________________________________________________________________________________________
embedding_182 (Embedding)       (None, 4, 68)        10336       input_182[0][0]                  
__________________________________________________________________________________________________
flatten_181 (Flatten)           (None, 1172)         0           embedding_181[0][0]              
__________________________________________________________________________________________________
flatten_182 (Flatten)           (None, 272)          0           embedding_182[0][0]              
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 1444)         0           flatten_181[0][0]                
                                                                 flatten_182[0][0]                
__________________________________________________________________________________________________
dense_127 (Dense)               (None, 17)           24565       concatenate_91[0][0]             
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 17)           0           dense_127[0][0]                  
__________________________________________________________________________________________________
dense_128 (Dense)               (None, 8)            144         dropout_37[0][0]                 
==================================================================================================
Total params: 855,445
Trainable params: 855,445
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32401, 1.0: 26794, 2.0: 6027, 6.0: 1846, 5.0: 1482, 4.0: 1381, 3.0: 89})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 8s - loss: 0.0798 - acc: 0.9736 - val_loss: 0.0371 - val_acc: 0.9890
Epoch 2/40
 - 8s - loss: 0.0505 - acc: 0.9838 - val_loss: 0.0350 - val_acc: 0.9895
Epoch 3/40
 - 8s - loss: 0.0461 - acc: 0.9852 - val_loss: 0.0347 - val_acc: 0.9899

==================================================================================================
	Training time : 0:01:15.308091
==================================================================================================
	Identification : 0.705
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	25 : 16
	100 : 1
	5 : 75
	0 : 105

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 177
	25 : 4
	200 : 1
	5 : 19
	100 : 1

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_183 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_184 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_183 (Embedding)       (None, 4, 293)       820400      input_183[0][0]                  
__________________________________________________________________________________________________
embedding_184 (Embedding)       (None, 4, 68)        10336       input_184[0][0]                  
__________________________________________________________________________________________________
flatten_183 (Flatten)           (None, 1172)         0           embedding_183[0][0]              
__________________________________________________________________________________________________
flatten_184 (Flatten)           (None, 272)          0           embedding_184[0][0]              
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 1444)         0           flatten_183[0][0]                
                                                                 flatten_184[0][0]                
__________________________________________________________________________________________________
dense_129 (Dense)               (None, 17)           24565       concatenate_92[0][0]             
__________________________________________________________________________________________________
dropout_38 (Dropout)            (None, 17)           0           dense_129[0][0]                  
__________________________________________________________________________________________________
dense_130 (Dense)               (None, 8)            144         dropout_38[0][0]                 
==================================================================================================
Total params: 855,445
Trainable params: 855,445
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32471, 1.0: 26785, 2.0: 5918, 6.0: 1932, 5.0: 1440, 4.0: 1368, 3.0: 106})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 7s - loss: 0.0890 - acc: 0.9700 - val_loss: 0.0378 - val_acc: 0.9891
Epoch 2/40
 - 7s - loss: 0.0610 - acc: 0.9794 - val_loss: 0.0352 - val_acc: 0.9901
Epoch 3/40
 - 7s - loss: 0.0562 - acc: 0.9806 - val_loss: 0.0352 - val_acc: 0.9897

==================================================================================================
	Training time : 0:01:13.744610
==================================================================================================
	Identification : 0.73
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 104
	100 : 1
	5 : 75
	25 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 178
	25 : 4
	200 : 1
	5 : 22
	100 : 1

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_185 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_186 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_185 (Embedding)       (None, 4, 293)       820400      input_185[0][0]                  
__________________________________________________________________________________________________
embedding_186 (Embedding)       (None, 4, 68)        10336       input_186[0][0]                  
__________________________________________________________________________________________________
flatten_185 (Flatten)           (None, 1172)         0           embedding_185[0][0]              
__________________________________________________________________________________________________
flatten_186 (Flatten)           (None, 272)          0           embedding_186[0][0]              
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 1444)         0           flatten_185[0][0]                
                                                                 flatten_186[0][0]                
__________________________________________________________________________________________________
dense_131 (Dense)               (None, 17)           24565       concatenate_93[0][0]             
__________________________________________________________________________________________________
dropout_39 (Dropout)            (None, 17)           0           dense_131[0][0]                  
__________________________________________________________________________________________________
dense_132 (Dense)               (None, 8)            144         dropout_39[0][0]                 
==================================================================================================
Total params: 855,445
Trainable params: 855,445
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32675, 1.0: 26790, 2.0: 5756, 6.0: 1815, 5.0: 1546, 4.0: 1335, 3.0: 103})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 7s - loss: 0.0965 - acc: 0.9654 - val_loss: 0.0389 - val_acc: 0.9887
Epoch 2/40
 - 7s - loss: 0.0578 - acc: 0.9829 - val_loss: 0.0358 - val_acc: 0.9898
Epoch 3/40
 - 7s - loss: 0.0519 - acc: 0.9847 - val_loss: 0.0343 - val_acc: 0.9901

==================================================================================================
	Training time : 0:01:14.019590
==================================================================================================
	Identification : 0.728
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	25 : 16
	100 : 1
	5 : 78
	0 : 105

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 177
	200 : 1
	100 : 1
	5 : 16
	25 : 5

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_187 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_188 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_187 (Embedding)       (None, 4, 293)       818349      input_187[0][0]                  
__________________________________________________________________________________________________
embedding_188 (Embedding)       (None, 4, 68)        11492       input_188[0][0]                  
__________________________________________________________________________________________________
flatten_187 (Flatten)           (None, 1172)         0           embedding_187[0][0]              
__________________________________________________________________________________________________
flatten_188 (Flatten)           (None, 272)          0           embedding_188[0][0]              
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 1444)         0           flatten_187[0][0]                
                                                                 flatten_188[0][0]                
__________________________________________________________________________________________________
dense_133 (Dense)               (None, 17)           24565       concatenate_94[0][0]             
__________________________________________________________________________________________________
dropout_40 (Dropout)            (None, 17)           0           dense_133[0][0]                  
__________________________________________________________________________________________________
dense_134 (Dense)               (None, 8)            144         dropout_40[0][0]                 
==================================================================================================
Total params: 854,550
Trainable params: 854,550
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32686, 1.0: 26923, 2.0: 6046, 6.0: 3270, 5.0: 975, 4.0: 613})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 8s - loss: 0.0778 - acc: 0.9741 - val_loss: 0.0294 - val_acc: 0.9912
Epoch 2/40
 - 8s - loss: 0.0490 - acc: 0.9841 - val_loss: 0.0283 - val_acc: 0.9916
Epoch 3/40
 - 8s - loss: 0.0448 - acc: 0.9856 - val_loss: 0.0275 - val_acc: 0.9922

==================================================================================================
	Training time : 0:01:14.999806
==================================================================================================
	Identification : 0.615
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 90
	25 : 1
	5 : 37

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 160
	25 : 1
	5 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_189 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_190 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_189 (Embedding)       (None, 4, 293)       818349      input_189[0][0]                  
__________________________________________________________________________________________________
embedding_190 (Embedding)       (None, 4, 68)        11492       input_190[0][0]                  
__________________________________________________________________________________________________
flatten_189 (Flatten)           (None, 1172)         0           embedding_189[0][0]              
__________________________________________________________________________________________________
flatten_190 (Flatten)           (None, 272)          0           embedding_190[0][0]              
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 1444)         0           flatten_189[0][0]                
                                                                 flatten_190[0][0]                
__________________________________________________________________________________________________
dense_135 (Dense)               (None, 17)           24565       concatenate_95[0][0]             
__________________________________________________________________________________________________
dropout_41 (Dropout)            (None, 17)           0           dense_135[0][0]                  
__________________________________________________________________________________________________
dense_136 (Dense)               (None, 8)            144         dropout_41[0][0]                 
==================================================================================================
Total params: 854,550
Trainable params: 854,550
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32642, 1.0: 26829, 2.0: 6066, 6.0: 3348, 5.0: 990, 4.0: 638})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 8s - loss: 0.0673 - acc: 0.9770 - val_loss: 0.0294 - val_acc: 0.9918
Epoch 2/40
 - 8s - loss: 0.0427 - acc: 0.9861 - val_loss: 0.0273 - val_acc: 0.9927
Epoch 3/40
 - 8s - loss: 0.0380 - acc: 0.9887 - val_loss: 0.0272 - val_acc: 0.9931

==================================================================================================
	Training time : 0:01:14.802264
==================================================================================================
	Identification : 0.593
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 84
	25 : 1
	5 : 38

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 167
	5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_191 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_192 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_191 (Embedding)       (None, 4, 293)       818349      input_191[0][0]                  
__________________________________________________________________________________________________
embedding_192 (Embedding)       (None, 4, 68)        11492       input_192[0][0]                  
__________________________________________________________________________________________________
flatten_191 (Flatten)           (None, 1172)         0           embedding_191[0][0]              
__________________________________________________________________________________________________
flatten_192 (Flatten)           (None, 272)          0           embedding_192[0][0]              
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 1444)         0           flatten_191[0][0]                
                                                                 flatten_192[0][0]                
__________________________________________________________________________________________________
dense_137 (Dense)               (None, 17)           24565       concatenate_96[0][0]             
__________________________________________________________________________________________________
dropout_42 (Dropout)            (None, 17)           0           dense_137[0][0]                  
__________________________________________________________________________________________________
dense_138 (Dense)               (None, 8)            144         dropout_42[0][0]                 
==================================================================================================
Total params: 854,550
Trainable params: 854,550
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32693, 1.0: 26838, 2.0: 6111, 6.0: 3207, 5.0: 1065, 4.0: 599})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 8s - loss: 0.0724 - acc: 0.9765 - val_loss: 0.0298 - val_acc: 0.9922
Epoch 2/40
 - 8s - loss: 0.0421 - acc: 0.9872 - val_loss: 0.0282 - val_acc: 0.9927
Epoch 3/40
 - 8s - loss: 0.0376 - acc: 0.9886 - val_loss: 0.0274 - val_acc: 0.9927

==================================================================================================
	Training time : 0:01:14.535385
==================================================================================================
	Identification : 0.608
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 89
	25 : 1
	5 : 39

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 164
	5 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_193 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_194 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_193 (Embedding)       (None, 4, 293)       932033      input_193[0][0]                  
__________________________________________________________________________________________________
embedding_194 (Embedding)       (None, 4, 68)        7480        input_194[0][0]                  
__________________________________________________________________________________________________
flatten_193 (Flatten)           (None, 1172)         0           embedding_193[0][0]              
__________________________________________________________________________________________________
flatten_194 (Flatten)           (None, 272)          0           embedding_194[0][0]              
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 1444)         0           flatten_193[0][0]                
                                                                 flatten_194[0][0]                
__________________________________________________________________________________________________
dense_139 (Dense)               (None, 17)           24565       concatenate_97[0][0]             
__________________________________________________________________________________________________
dropout_43 (Dropout)            (None, 17)           0           dense_139[0][0]                  
__________________________________________________________________________________________________
dense_140 (Dense)               (None, 8)            144         dropout_43[0][0]                 
==================================================================================================
Total params: 964,222
Trainable params: 964,222
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33370, 1.0: 26874, 2.0: 6754, 5.0: 3658, 6.0: 2709})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 8s - loss: 0.1056 - acc: 0.9653 - val_loss: 0.0510 - val_acc: 0.9844
Epoch 2/40
 - 8s - loss: 0.0695 - acc: 0.9779 - val_loss: 0.0486 - val_acc: 0.9850
Epoch 3/40
 - 8s - loss: 0.0631 - acc: 0.9797 - val_loss: 0.0464 - val_acc: 0.9861

==================================================================================================
	Training time : 0:01:16.610344
==================================================================================================
	Identification : 0.541
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 138
	25 : 16
	50 : 6
	5 : 79

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 264
	25 : 9
	50 : 5
	5 : 43

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_195 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_196 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_195 (Embedding)       (None, 4, 293)       932033      input_195[0][0]                  
__________________________________________________________________________________________________
embedding_196 (Embedding)       (None, 4, 68)        7480        input_196[0][0]                  
__________________________________________________________________________________________________
flatten_195 (Flatten)           (None, 1172)         0           embedding_195[0][0]              
__________________________________________________________________________________________________
flatten_196 (Flatten)           (None, 272)          0           embedding_196[0][0]              
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 1444)         0           flatten_195[0][0]                
                                                                 flatten_196[0][0]                
__________________________________________________________________________________________________
dense_141 (Dense)               (None, 17)           24565       concatenate_98[0][0]             
__________________________________________________________________________________________________
dropout_44 (Dropout)            (None, 17)           0           dense_141[0][0]                  
__________________________________________________________________________________________________
dense_142 (Dense)               (None, 8)            144         dropout_44[0][0]                 
==================================================================================================
Total params: 964,222
Trainable params: 964,222
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33607, 1.0: 26717, 2.0: 6781, 5.0: 3546, 6.0: 2714})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 8s - loss: 0.0954 - acc: 0.9667 - val_loss: 0.0537 - val_acc: 0.9839
Epoch 2/40
 - 8s - loss: 0.0657 - acc: 0.9795 - val_loss: 0.0512 - val_acc: 0.9849
Epoch 3/40
 - 8s - loss: 0.0601 - acc: 0.9814 - val_loss: 0.0518 - val_acc: 0.9840

==================================================================================================
	Training time : 0:01:18.306654
==================================================================================================
	Identification : 0.531
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 148
	25 : 17
	50 : 6
	5 : 86

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 256
	25 : 10
	50 : 5
	5 : 35

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_197 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_198 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_197 (Embedding)       (None, 4, 293)       932033      input_197[0][0]                  
__________________________________________________________________________________________________
embedding_198 (Embedding)       (None, 4, 68)        7480        input_198[0][0]                  
__________________________________________________________________________________________________
flatten_197 (Flatten)           (None, 1172)         0           embedding_197[0][0]              
__________________________________________________________________________________________________
flatten_198 (Flatten)           (None, 272)          0           embedding_198[0][0]              
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 1444)         0           flatten_197[0][0]                
                                                                 flatten_198[0][0]                
__________________________________________________________________________________________________
dense_143 (Dense)               (None, 17)           24565       concatenate_99[0][0]             
__________________________________________________________________________________________________
dropout_45 (Dropout)            (None, 17)           0           dense_143[0][0]                  
__________________________________________________________________________________________________
dense_144 (Dense)               (None, 8)            144         dropout_45[0][0]                 
==================================================================================================
Total params: 964,222
Trainable params: 964,222
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33506, 1.0: 26881, 2.0: 6678, 5.0: 3602, 6.0: 2698})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 8s - loss: 0.0964 - acc: 0.9684 - val_loss: 0.0532 - val_acc: 0.9836
Epoch 2/40
 - 8s - loss: 0.0666 - acc: 0.9779 - val_loss: 0.0497 - val_acc: 0.9851
Epoch 3/40
 - 8s - loss: 0.0622 - acc: 0.9791 - val_loss: 0.0499 - val_acc: 0.9852

==================================================================================================
	Training time : 0:01:17.948175
==================================================================================================
	Identification : 0.521
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 126
	25 : 12
	50 : 5
	5 : 72

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 275
	25 : 8
	50 : 5
	5 : 46

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_199 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_200 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_199 (Embedding)       (None, 4, 293)       1037513     input_199[0][0]                  
__________________________________________________________________________________________________
embedding_200 (Embedding)       (None, 4, 68)        11424       input_200[0][0]                  
__________________________________________________________________________________________________
flatten_199 (Flatten)           (None, 1172)         0           embedding_199[0][0]              
__________________________________________________________________________________________________
flatten_200 (Flatten)           (None, 272)          0           embedding_200[0][0]              
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 1444)         0           flatten_199[0][0]                
                                                                 flatten_200[0][0]                
__________________________________________________________________________________________________
dense_145 (Dense)               (None, 17)           24565       concatenate_100[0][0]            
__________________________________________________________________________________________________
dropout_46 (Dropout)            (None, 17)           0           dense_145[0][0]                  
__________________________________________________________________________________________________
dense_146 (Dense)               (None, 8)            144         dropout_46[0][0]                 
==================================================================================================
Total params: 1,073,646
Trainable params: 1,073,646
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47142, 1.0: 39547, 2.0: 7727, 6.0: 2465, 5.0: 2053, 4.0: 1739, 3.0: 114})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 11s - loss: 0.0774 - acc: 0.9753 - val_loss: 0.0363 - val_acc: 0.9881
Epoch 2/40
 - 11s - loss: 0.0470 - acc: 0.9852 - val_loss: 0.0348 - val_acc: 0.9893
Epoch 3/40
 - 11s - loss: 0.0429 - acc: 0.9863 - val_loss: 0.0329 - val_acc: 0.9898

==================================================================================================
	Training time : 0:01:41.905547
==================================================================================================
	Identification : 0.565
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 88
	100 : 1
	5 : 68
	300 : 1
	50 : 5
	25 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 267
	25 : 6
	5 : 20

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_201 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_202 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_201 (Embedding)       (None, 4, 293)       1037513     input_201[0][0]                  
__________________________________________________________________________________________________
embedding_202 (Embedding)       (None, 4, 68)        11424       input_202[0][0]                  
__________________________________________________________________________________________________
flatten_201 (Flatten)           (None, 1172)         0           embedding_201[0][0]              
__________________________________________________________________________________________________
flatten_202 (Flatten)           (None, 272)          0           embedding_202[0][0]              
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 1444)         0           flatten_201[0][0]                
                                                                 flatten_202[0][0]                
__________________________________________________________________________________________________
dense_147 (Dense)               (None, 17)           24565       concatenate_101[0][0]            
__________________________________________________________________________________________________
dropout_47 (Dropout)            (None, 17)           0           dense_147[0][0]                  
__________________________________________________________________________________________________
dense_148 (Dense)               (None, 8)            144         dropout_47[0][0]                 
==================================================================================================
Total params: 1,073,646
Trainable params: 1,073,646
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47234, 1.0: 39414, 2.0: 7846, 6.0: 2448, 5.0: 2057, 4.0: 1675, 3.0: 113})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 11s - loss: 0.0918 - acc: 0.9662 - val_loss: 0.0378 - val_acc: 0.9883
Epoch 2/40
 - 11s - loss: 0.0532 - acc: 0.9831 - val_loss: 0.0350 - val_acc: 0.9892
Epoch 3/40
 - 11s - loss: 0.0460 - acc: 0.9856 - val_loss: 0.0338 - val_acc: 0.9896

==================================================================================================
	Training time : 0:01:41.871518
==================================================================================================
	Identification : 0.557
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 86
	100 : 1
	5 : 68
	300 : 1
	50 : 5
	25 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 266
	25 : 5
	100 : 1
	5 : 20

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_203 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_204 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_203 (Embedding)       (None, 4, 293)       1037513     input_203[0][0]                  
__________________________________________________________________________________________________
embedding_204 (Embedding)       (None, 4, 68)        11424       input_204[0][0]                  
__________________________________________________________________________________________________
flatten_203 (Flatten)           (None, 1172)         0           embedding_203[0][0]              
__________________________________________________________________________________________________
flatten_204 (Flatten)           (None, 272)          0           embedding_204[0][0]              
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 1444)         0           flatten_203[0][0]                
                                                                 flatten_204[0][0]                
__________________________________________________________________________________________________
dense_149 (Dense)               (None, 17)           24565       concatenate_102[0][0]            
__________________________________________________________________________________________________
dropout_48 (Dropout)            (None, 17)           0           dense_149[0][0]                  
__________________________________________________________________________________________________
dense_150 (Dense)               (None, 8)            144         dropout_48[0][0]                 
==================================================================================================
Total params: 1,073,646
Trainable params: 1,073,646
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47263, 1.0: 39408, 2.0: 7744, 6.0: 2418, 5.0: 2076, 4.0: 1763, 3.0: 115})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 11s - loss: 0.0795 - acc: 0.9740 - val_loss: 0.0350 - val_acc: 0.9888
Epoch 2/40
 - 11s - loss: 0.0526 - acc: 0.9836 - val_loss: 0.0342 - val_acc: 0.9903
Epoch 3/40
 - 11s - loss: 0.0475 - acc: 0.9853 - val_loss: 0.0331 - val_acc: 0.9898

==================================================================================================
	Training time : 0:01:41.922588
==================================================================================================
	Identification : 0.557
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 83
	100 : 1
	5 : 64
	300 : 1
	50 : 4
	25 : 20

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 272
	25 : 9
	50 : 2
	300 : 1
	5 : 22

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_205 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_206 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_205 (Embedding)       (None, 4, 293)       1144751     input_205[0][0]                  
__________________________________________________________________________________________________
embedding_206 (Embedding)       (None, 4, 68)        15232       input_206[0][0]                  
__________________________________________________________________________________________________
flatten_205 (Flatten)           (None, 1172)         0           embedding_205[0][0]              
__________________________________________________________________________________________________
flatten_206 (Flatten)           (None, 272)          0           embedding_206[0][0]              
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 1444)         0           flatten_205[0][0]                
                                                                 flatten_206[0][0]                
__________________________________________________________________________________________________
dense_151 (Dense)               (None, 17)           24565       concatenate_103[0][0]            
__________________________________________________________________________________________________
dropout_49 (Dropout)            (None, 17)           0           dense_151[0][0]                  
__________________________________________________________________________________________________
dense_152 (Dense)               (None, 8)            144         dropout_49[0][0]                 
==================================================================================================
Total params: 1,184,692
Trainable params: 1,184,692
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 55845, 1.0: 46982, 2.0: 8678, 6.0: 4740, 5.0: 1461, 4.0: 820})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 13s - loss: 0.0635 - acc: 0.9786 - val_loss: 0.0283 - val_acc: 0.9917
Epoch 2/40
 - 13s - loss: 0.0424 - acc: 0.9861 - val_loss: 0.0271 - val_acc: 0.9918
Epoch 3/40
 - 13s - loss: 0.0391 - acc: 0.9871 - val_loss: 0.0266 - val_acc: 0.9922

==================================================================================================
	Training time : 0:01:59.836247
==================================================================================================
	Identification : 0.668
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 135
	25 : 8
	5 : 81

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 198
	25 : 3
	5 : 19

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_207 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_208 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_207 (Embedding)       (None, 4, 293)       1144751     input_207[0][0]                  
__________________________________________________________________________________________________
embedding_208 (Embedding)       (None, 4, 68)        15232       input_208[0][0]                  
__________________________________________________________________________________________________
flatten_207 (Flatten)           (None, 1172)         0           embedding_207[0][0]              
__________________________________________________________________________________________________
flatten_208 (Flatten)           (None, 272)          0           embedding_208[0][0]              
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 1444)         0           flatten_207[0][0]                
                                                                 flatten_208[0][0]                
__________________________________________________________________________________________________
dense_153 (Dense)               (None, 17)           24565       concatenate_104[0][0]            
__________________________________________________________________________________________________
dropout_50 (Dropout)            (None, 17)           0           dense_153[0][0]                  
__________________________________________________________________________________________________
dense_154 (Dense)               (None, 8)            144         dropout_50[0][0]                 
==================================================================================================
Total params: 1,184,692
Trainable params: 1,184,692
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 56182, 1.0: 46641, 2.0: 8678, 6.0: 4719, 5.0: 1470, 4.0: 836})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 13s - loss: 0.0729 - acc: 0.9741 - val_loss: 0.0328 - val_acc: 0.9903
Epoch 2/40
 - 13s - loss: 0.0470 - acc: 0.9832 - val_loss: 0.0311 - val_acc: 0.9911
Epoch 3/40
 - 13s - loss: 0.0398 - acc: 0.9871 - val_loss: 0.0293 - val_acc: 0.9918

==================================================================================================
	Training time : 0:02:00.997025
==================================================================================================
	Identification : 0.662
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 124
	25 : 7
	5 : 81

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 210
	25 : 2
	5 : 20

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_209 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_210 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_209 (Embedding)       (None, 4, 293)       1144751     input_209[0][0]                  
__________________________________________________________________________________________________
embedding_210 (Embedding)       (None, 4, 68)        15232       input_210[0][0]                  
__________________________________________________________________________________________________
flatten_209 (Flatten)           (None, 1172)         0           embedding_209[0][0]              
__________________________________________________________________________________________________
flatten_210 (Flatten)           (None, 272)          0           embedding_210[0][0]              
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 1444)         0           flatten_209[0][0]                
                                                                 flatten_210[0][0]                
__________________________________________________________________________________________________
dense_155 (Dense)               (None, 17)           24565       concatenate_105[0][0]            
__________________________________________________________________________________________________
dropout_51 (Dropout)            (None, 17)           0           dense_155[0][0]                  
__________________________________________________________________________________________________
dense_156 (Dense)               (None, 8)            144         dropout_51[0][0]                 
==================================================================================================
Total params: 1,184,692
Trainable params: 1,184,692
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 55528, 1.0: 47006, 2.0: 8890, 6.0: 4766, 5.0: 1476, 4.0: 860})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 13s - loss: 0.0600 - acc: 0.9804 - val_loss: 0.0291 - val_acc: 0.9915
Epoch 2/40
 - 13s - loss: 0.0402 - acc: 0.9873 - val_loss: 0.0275 - val_acc: 0.9923
Epoch 3/40
 - 13s - loss: 0.0369 - acc: 0.9882 - val_loss: 0.0272 - val_acc: 0.9924

==================================================================================================
	Training time : 0:02:00.193300
==================================================================================================
	Identification : 0.645
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 148
	25 : 8
	5 : 83

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 186
	25 : 3
	5 : 17

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_211 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_212 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_211 (Embedding)       (None, 4, 293)       1075896     input_211[0][0]                  
__________________________________________________________________________________________________
embedding_212 (Embedding)       (None, 4, 68)        8092        input_212[0][0]                  
__________________________________________________________________________________________________
flatten_211 (Flatten)           (None, 1172)         0           embedding_211[0][0]              
__________________________________________________________________________________________________
flatten_212 (Flatten)           (None, 272)          0           embedding_212[0][0]              
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 1444)         0           flatten_211[0][0]                
                                                                 flatten_212[0][0]                
__________________________________________________________________________________________________
dense_157 (Dense)               (None, 17)           24565       concatenate_106[0][0]            
__________________________________________________________________________________________________
dropout_52 (Dropout)            (None, 17)           0           dense_157[0][0]                  
__________________________________________________________________________________________________
dense_158 (Dense)               (None, 8)            144         dropout_52[0][0]                 
==================================================================================================
Total params: 1,108,697
Trainable params: 1,108,697
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 41077, 1.0: 33178, 2.0: 7843, 5.0: 4304, 6.0: 3124})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 10s - loss: 0.1042 - acc: 0.9631 - val_loss: 0.0542 - val_acc: 0.9834
Epoch 2/40
 - 10s - loss: 0.0716 - acc: 0.9759 - val_loss: 0.0524 - val_acc: 0.9841
Epoch 3/40
 - 10s - loss: 0.0670 - acc: 0.9769 - val_loss: 0.0513 - val_acc: 0.9849

==================================================================================================
	Training time : 0:01:30.125600
==================================================================================================
	Identification : 0.449
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 67
	25 : 12
	50 : 9
	5 : 48

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 178
	25 : 12
	50 : 7
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_213 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_214 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_213 (Embedding)       (None, 4, 293)       1075896     input_213[0][0]                  
__________________________________________________________________________________________________
embedding_214 (Embedding)       (None, 4, 68)        8092        input_214[0][0]                  
__________________________________________________________________________________________________
flatten_213 (Flatten)           (None, 1172)         0           embedding_213[0][0]              
__________________________________________________________________________________________________
flatten_214 (Flatten)           (None, 272)          0           embedding_214[0][0]              
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 1444)         0           flatten_213[0][0]                
                                                                 flatten_214[0][0]                
__________________________________________________________________________________________________
dense_159 (Dense)               (None, 17)           24565       concatenate_107[0][0]            
__________________________________________________________________________________________________
dropout_53 (Dropout)            (None, 17)           0           dense_159[0][0]                  
__________________________________________________________________________________________________
dense_160 (Dense)               (None, 8)            144         dropout_53[0][0]                 
==================================================================================================
Total params: 1,108,697
Trainable params: 1,108,697
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 41018, 1.0: 33279, 2.0: 7992, 5.0: 4136, 6.0: 3101})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 10s - loss: 0.1156 - acc: 0.9595 - val_loss: 0.0549 - val_acc: 0.9828
Epoch 2/40
 - 10s - loss: 0.0795 - acc: 0.9733 - val_loss: 0.0524 - val_acc: 0.9841
Epoch 3/40
 - 10s - loss: 0.0744 - acc: 0.9746 - val_loss: 0.0515 - val_acc: 0.9847

==================================================================================================
	Training time : 0:01:33.187672
==================================================================================================
	Identification : 0.487
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 75
	25 : 15
	50 : 9
	5 : 52

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 172
	25 : 10
	50 : 5
	5 : 32

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 293
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_215 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_216 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_215 (Embedding)       (None, 4, 293)       1075896     input_215[0][0]                  
__________________________________________________________________________________________________
embedding_216 (Embedding)       (None, 4, 68)        8092        input_216[0][0]                  
__________________________________________________________________________________________________
flatten_215 (Flatten)           (None, 1172)         0           embedding_215[0][0]              
__________________________________________________________________________________________________
flatten_216 (Flatten)           (None, 272)          0           embedding_216[0][0]              
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 1444)         0           flatten_215[0][0]                
                                                                 flatten_216[0][0]                
__________________________________________________________________________________________________
dense_161 (Dense)               (None, 17)           24565       concatenate_108[0][0]            
__________________________________________________________________________________________________
dropout_54 (Dropout)            (None, 17)           0           dense_161[0][0]                  
__________________________________________________________________________________________________
dense_162 (Dense)               (None, 8)            144         dropout_54[0][0]                 
==================================================================================================
Total params: 1,108,697
Trainable params: 1,108,697
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 40955, 1.0: 33291, 2.0: 7962, 5.0: 4134, 6.0: 3184})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.08
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 10s - loss: 0.0958 - acc: 0.9676 - val_loss: 0.0541 - val_acc: 0.9830
Epoch 2/40
 - 10s - loss: 0.0693 - acc: 0.9773 - val_loss: 0.0520 - val_acc: 0.9845
Epoch 3/40
 - 10s - loss: 0.0644 - acc: 0.9788 - val_loss: 0.0502 - val_acc: 0.9852

==================================================================================================
	Training time : 0:01:29.931525
==================================================================================================
	Identification : 0.477
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 73
	25 : 12
	50 : 9
	5 : 54

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 174
	25 : 12
	50 : 7
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
10_True_True_336_103_frequent_True_7_relu_0.287_False_512_relu_0.2_adagrad_0.087_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12174

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12174 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12174
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_217 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_218 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_217 (Embedding)       (None, 4, 336)       4090464     input_217[0][0]                  
__________________________________________________________________________________________________
embedding_218 (Embedding)       (None, 4, 103)       15656       input_218[0][0]                  
__________________________________________________________________________________________________
flatten_217 (Flatten)           (None, 1344)         0           embedding_217[0][0]              
__________________________________________________________________________________________________
flatten_218 (Flatten)           (None, 412)          0           embedding_218[0][0]              
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 1756)         0           flatten_217[0][0]                
                                                                 flatten_218[0][0]                
__________________________________________________________________________________________________
dense_163 (Dense)               (None, 7)            12299       concatenate_109[0][0]            
__________________________________________________________________________________________________
dropout_55 (Dropout)            (None, 7)            0           dense_163[0][0]                  
__________________________________________________________________________________________________
dense_164 (Dense)               (None, 8)            64          dropout_55[0][0]                 
==================================================================================================
Total params: 4,118,483
Trainable params: 4,118,483
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32723, 1.0: 26723, 2.0: 5791, 6.0: 1863, 5.0: 1480, 4.0: 1346, 3.0: 94})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 9s - loss: 0.1807 - acc: 0.9330 - val_loss: 0.0704 - val_acc: 0.9851
Epoch 2/40
 - 9s - loss: 0.1434 - acc: 0.9453 - val_loss: 0.0635 - val_acc: 0.9866
Epoch 3/40
 - 9s - loss: 0.1375 - acc: 0.9469 - val_loss: 0.0638 - val_acc: 0.9861

==================================================================================================
	Training time : 0:01:22.138680
==================================================================================================
	Identification : 0.696
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 101
	100 : 1
	5 : 73
	25 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 181
	25 : 6
	200 : 1
	5 : 22
	100 : 1

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12109

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12109 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12109
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_219 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_220 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_219 (Embedding)       (None, 4, 336)       4068624     input_219[0][0]                  
__________________________________________________________________________________________________
embedding_220 (Embedding)       (None, 4, 103)       15656       input_220[0][0]                  
__________________________________________________________________________________________________
flatten_219 (Flatten)           (None, 1344)         0           embedding_219[0][0]              
__________________________________________________________________________________________________
flatten_220 (Flatten)           (None, 412)          0           embedding_220[0][0]              
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 1756)         0           flatten_219[0][0]                
                                                                 flatten_220[0][0]                
__________________________________________________________________________________________________
dense_165 (Dense)               (None, 7)            12299       concatenate_110[0][0]            
__________________________________________________________________________________________________
dropout_56 (Dropout)            (None, 7)            0           dense_165[0][0]                  
__________________________________________________________________________________________________
dense_166 (Dense)               (None, 8)            64          dropout_56[0][0]                 
==================================================================================================
Total params: 4,096,643
Trainable params: 4,096,643
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32589, 1.0: 26713, 2.0: 5865, 6.0: 1850, 5.0: 1494, 4.0: 1386, 3.0: 123})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 9s - loss: 0.1937 - acc: 0.9272 - val_loss: 0.0976 - val_acc: 0.9644
Epoch 2/40
 - 9s - loss: 0.1615 - acc: 0.9400 - val_loss: 0.0922 - val_acc: 0.9672
Epoch 3/40
 - 9s - loss: 0.1518 - acc: 0.9434 - val_loss: 0.0896 - val_acc: 0.9676

==================================================================================================
	Training time : 0:01:19.073467
==================================================================================================
	Identification : 0.714
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 102
	100 : 1
	5 : 78
	25 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 182
	25 : 6
	100 : 1
	5 : 19

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12163

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12163 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12163
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_221 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_222 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_221 (Embedding)       (None, 4, 336)       4086768     input_221[0][0]                  
__________________________________________________________________________________________________
embedding_222 (Embedding)       (None, 4, 103)       15656       input_222[0][0]                  
__________________________________________________________________________________________________
flatten_221 (Flatten)           (None, 1344)         0           embedding_221[0][0]              
__________________________________________________________________________________________________
flatten_222 (Flatten)           (None, 412)          0           embedding_222[0][0]              
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 1756)         0           flatten_221[0][0]                
                                                                 flatten_222[0][0]                
__________________________________________________________________________________________________
dense_167 (Dense)               (None, 7)            12299       concatenate_111[0][0]            
__________________________________________________________________________________________________
dropout_57 (Dropout)            (None, 7)            0           dense_167[0][0]                  
__________________________________________________________________________________________________
dense_168 (Dense)               (None, 8)            64          dropout_57[0][0]                 
==================================================================================================
Total params: 4,114,787
Trainable params: 4,114,787
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32498, 1.0: 26861, 2.0: 5811, 6.0: 1861, 5.0: 1545, 4.0: 1356, 3.0: 88})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 9s - loss: 0.2388 - acc: 0.9064 - val_loss: 0.1397 - val_acc: 0.9443
Epoch 2/40
 - 9s - loss: 0.1817 - acc: 0.9333 - val_loss: 0.1361 - val_acc: 0.9451
Epoch 3/40
 - 9s - loss: 0.1722 - acc: 0.9359 - val_loss: 0.1337 - val_acc: 0.9462

==================================================================================================
	Training time : 0:01:18.757509
==================================================================================================
	Identification : 0.716
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 107
	100 : 1
	5 : 76
	25 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 175
	200 : 1
	100 : 1
	5 : 20
	25 : 6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15302

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15302 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15302
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_223 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_224 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_223 (Embedding)       (None, 4, 336)       5141472     input_223[0][0]                  
__________________________________________________________________________________________________
embedding_224 (Embedding)       (None, 4, 103)       17407       input_224[0][0]                  
__________________________________________________________________________________________________
flatten_223 (Flatten)           (None, 1344)         0           embedding_223[0][0]              
__________________________________________________________________________________________________
flatten_224 (Flatten)           (None, 412)          0           embedding_224[0][0]              
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 1756)         0           flatten_223[0][0]                
                                                                 flatten_224[0][0]                
__________________________________________________________________________________________________
dense_169 (Dense)               (None, 7)            12299       concatenate_112[0][0]            
__________________________________________________________________________________________________
dropout_58 (Dropout)            (None, 7)            0           dense_169[0][0]                  
__________________________________________________________________________________________________
dense_170 (Dense)               (None, 8)            64          dropout_58[0][0]                 
==================================================================================================
Total params: 5,171,242
Trainable params: 5,171,242
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32566, 1.0: 26984, 2.0: 6037, 6.0: 3283, 5.0: 984, 4.0: 659})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 10s - loss: 0.1712 - acc: 0.9366 - val_loss: 0.0651 - val_acc: 0.9746
Epoch 2/40
 - 10s - loss: 0.1297 - acc: 0.9553 - val_loss: 0.0599 - val_acc: 0.9803
Epoch 3/40
 - 10s - loss: 0.1249 - acc: 0.9563 - val_loss: 0.0579 - val_acc: 0.9825

==================================================================================================
	Training time : 0:01:21.273029
==================================================================================================
	Identification : 0.48
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 67
	5 : 25

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 184
	25 : 1
	5 : 26

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15324

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15324 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15324
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_225 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_226 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_225 (Embedding)       (None, 4, 336)       5148864     input_225[0][0]                  
__________________________________________________________________________________________________
embedding_226 (Embedding)       (None, 4, 103)       17407       input_226[0][0]                  
__________________________________________________________________________________________________
flatten_225 (Flatten)           (None, 1344)         0           embedding_225[0][0]              
__________________________________________________________________________________________________
flatten_226 (Flatten)           (None, 412)          0           embedding_226[0][0]              
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 1756)         0           flatten_225[0][0]                
                                                                 flatten_226[0][0]                
__________________________________________________________________________________________________
dense_171 (Dense)               (None, 7)            12299       concatenate_113[0][0]            
__________________________________________________________________________________________________
dropout_59 (Dropout)            (None, 7)            0           dense_171[0][0]                  
__________________________________________________________________________________________________
dense_172 (Dense)               (None, 8)            64          dropout_59[0][0]                 
==================================================================================================
Total params: 5,178,634
Trainable params: 5,178,634
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32780, 1.0: 26749, 2.0: 6104, 6.0: 3357, 5.0: 976, 4.0: 547})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 10s - loss: 0.1740 - acc: 0.9350 - val_loss: 0.0716 - val_acc: 0.9863
Epoch 2/40
 - 10s - loss: 0.1390 - acc: 0.9475 - val_loss: 0.0702 - val_acc: 0.9868
Epoch 3/40
 - 10s - loss: 0.1331 - acc: 0.9500 - val_loss: 0.0649 - val_acc: 0.9878

==================================================================================================
	Training time : 0:01:21.285231
==================================================================================================
	Identification : 0.565
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 101
	25 : 1
	5 : 34

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 151
	5 : 22

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15316

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15316 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15316
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_227 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_228 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_227 (Embedding)       (None, 4, 336)       5146176     input_227[0][0]                  
__________________________________________________________________________________________________
embedding_228 (Embedding)       (None, 4, 103)       17407       input_228[0][0]                  
__________________________________________________________________________________________________
flatten_227 (Flatten)           (None, 1344)         0           embedding_227[0][0]              
__________________________________________________________________________________________________
flatten_228 (Flatten)           (None, 412)          0           embedding_228[0][0]              
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 1756)         0           flatten_227[0][0]                
                                                                 flatten_228[0][0]                
__________________________________________________________________________________________________
dense_173 (Dense)               (None, 7)            12299       concatenate_114[0][0]            
__________________________________________________________________________________________________
dropout_60 (Dropout)            (None, 7)            0           dense_173[0][0]                  
__________________________________________________________________________________________________
dense_174 (Dense)               (None, 8)            64          dropout_60[0][0]                 
==================================================================================================
Total params: 5,175,946
Trainable params: 5,175,946
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32928, 1.0: 26640, 2.0: 6030, 6.0: 3270, 5.0: 1018, 4.0: 627})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 10s - loss: 0.1815 - acc: 0.9339 - val_loss: 0.1015 - val_acc: 0.9656
Epoch 2/40
 - 10s - loss: 0.1465 - acc: 0.9429 - val_loss: 0.0989 - val_acc: 0.9667
Epoch 3/40
 - 10s - loss: 0.1401 - acc: 0.9450 - val_loss: 0.0996 - val_acc: 0.9669

==================================================================================================
	Training time : 0:01:21.483177
==================================================================================================
	Identification : 0.635
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 94
	25 : 1
	5 : 36

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 155
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12575

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12575 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12575
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_229 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_230 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_229 (Embedding)       (None, 4, 336)       4225200     input_229[0][0]                  
__________________________________________________________________________________________________
embedding_230 (Embedding)       (None, 4, 103)       11330       input_230[0][0]                  
__________________________________________________________________________________________________
flatten_229 (Flatten)           (None, 1344)         0           embedding_229[0][0]              
__________________________________________________________________________________________________
flatten_230 (Flatten)           (None, 412)          0           embedding_230[0][0]              
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 1756)         0           flatten_229[0][0]                
                                                                 flatten_230[0][0]                
__________________________________________________________________________________________________
dense_175 (Dense)               (None, 7)            12299       concatenate_115[0][0]            
__________________________________________________________________________________________________
dropout_61 (Dropout)            (None, 7)            0           dense_175[0][0]                  
__________________________________________________________________________________________________
dense_176 (Dense)               (None, 8)            64          dropout_61[0][0]                 
==================================================================================================
Total params: 4,248,893
Trainable params: 4,248,893
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33626, 1.0: 26738, 2.0: 6662, 5.0: 3572, 6.0: 2767})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 10s - loss: 0.2080 - acc: 0.9157 - val_loss: 0.0678 - val_acc: 0.9813
Epoch 2/40
 - 10s - loss: 0.1697 - acc: 0.9346 - val_loss: 0.0628 - val_acc: 0.9832
Epoch 3/40
 - 10s - loss: 0.1622 - acc: 0.9387 - val_loss: 0.0610 - val_acc: 0.9832

==================================================================================================
	Training time : 0:01:23.760473
==================================================================================================
	Identification : 0.463
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 179
	25 : 14
	50 : 5
	5 : 71

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 223
	25 : 8
	50 : 4
	5 : 49

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12679

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12679 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12679
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_231 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_232 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_231 (Embedding)       (None, 4, 336)       4260144     input_231[0][0]                  
__________________________________________________________________________________________________
embedding_232 (Embedding)       (None, 4, 103)       11330       input_232[0][0]                  
__________________________________________________________________________________________________
flatten_231 (Flatten)           (None, 1344)         0           embedding_231[0][0]              
__________________________________________________________________________________________________
flatten_232 (Flatten)           (None, 412)          0           embedding_232[0][0]              
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 1756)         0           flatten_231[0][0]                
                                                                 flatten_232[0][0]                
__________________________________________________________________________________________________
dense_177 (Dense)               (None, 7)            12299       concatenate_116[0][0]            
__________________________________________________________________________________________________
dropout_62 (Dropout)            (None, 7)            0           dense_177[0][0]                  
__________________________________________________________________________________________________
dense_178 (Dense)               (None, 8)            64          dropout_62[0][0]                 
==================================================================================================
Total params: 4,283,837
Trainable params: 4,283,837
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33736, 1.0: 26628, 2.0: 6715, 5.0: 3471, 6.0: 2815})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 10s - loss: 0.2057 - acc: 0.9259 - val_loss: 0.0829 - val_acc: 0.9802
Epoch 2/40
 - 9s - loss: 0.1677 - acc: 0.9440 - val_loss: 0.0757 - val_acc: 0.9822
Epoch 3/40
 - 9s - loss: 0.1635 - acc: 0.9457 - val_loss: 0.0769 - val_acc: 0.9815

==================================================================================================
	Training time : 0:01:20.645767
==================================================================================================
	Identification : 0.459
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 138
	25 : 8
	50 : 5
	5 : 68

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 262
	25 : 15
	50 : 5
	5 : 52

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12544

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12544 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12544
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_233 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_234 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_233 (Embedding)       (None, 4, 336)       4214784     input_233[0][0]                  
__________________________________________________________________________________________________
embedding_234 (Embedding)       (None, 4, 103)       11330       input_234[0][0]                  
__________________________________________________________________________________________________
flatten_233 (Flatten)           (None, 1344)         0           embedding_233[0][0]              
__________________________________________________________________________________________________
flatten_234 (Flatten)           (None, 412)          0           embedding_234[0][0]              
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 1756)         0           flatten_233[0][0]                
                                                                 flatten_234[0][0]                
__________________________________________________________________________________________________
dense_179 (Dense)               (None, 7)            12299       concatenate_117[0][0]            
__________________________________________________________________________________________________
dropout_63 (Dropout)            (None, 7)            0           dense_179[0][0]                  
__________________________________________________________________________________________________
dense_180 (Dense)               (None, 8)            64          dropout_63[0][0]                 
==================================================================================================
Total params: 4,238,477
Trainable params: 4,238,477
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33523, 1.0: 26813, 2.0: 6827, 5.0: 3561, 6.0: 2641})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 9s - loss: 0.1734 - acc: 0.9362 - val_loss: 0.0705 - val_acc: 0.9815
Epoch 2/40
 - 9s - loss: 0.1287 - acc: 0.9520 - val_loss: 0.0660 - val_acc: 0.9829
Epoch 3/40
 - 9s - loss: 0.1179 - acc: 0.9562 - val_loss: 0.0647 - val_acc: 0.9827

==================================================================================================
	Training time : 0:01:20.981658
==================================================================================================
	Identification : 0.485
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 197
	25 : 17
	50 : 6
	5 : 82

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 207
	25 : 11
	50 : 4
	5 : 41

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14841

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14841 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14841
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_235 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_236 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_235 (Embedding)       (None, 4, 336)       4986576     input_235[0][0]                  
__________________________________________________________________________________________________
embedding_236 (Embedding)       (None, 4, 103)       17304       input_236[0][0]                  
__________________________________________________________________________________________________
flatten_235 (Flatten)           (None, 1344)         0           embedding_235[0][0]              
__________________________________________________________________________________________________
flatten_236 (Flatten)           (None, 412)          0           embedding_236[0][0]              
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 1756)         0           flatten_235[0][0]                
                                                                 flatten_236[0][0]                
__________________________________________________________________________________________________
dense_181 (Dense)               (None, 7)            12299       concatenate_118[0][0]            
__________________________________________________________________________________________________
dropout_64 (Dropout)            (None, 7)            0           dense_181[0][0]                  
__________________________________________________________________________________________________
dense_182 (Dense)               (None, 8)            64          dropout_64[0][0]                 
==================================================================================================
Total params: 5,016,243
Trainable params: 5,016,243
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47323, 1.0: 39414, 2.0: 7735, 6.0: 2381, 5.0: 2125, 4.0: 1703, 3.0: 106})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 13s - loss: 0.2136 - acc: 0.9139 - val_loss: 0.1134 - val_acc: 0.9605
Epoch 2/40
 - 13s - loss: 0.1776 - acc: 0.9292 - val_loss: 0.1092 - val_acc: 0.9625
Epoch 3/40
 - 13s - loss: 0.1723 - acc: 0.9306 - val_loss: 0.1098 - val_acc: 0.9629

==================================================================================================
	Training time : 0:01:50.330583
==================================================================================================
	Identification : 0.556
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 74
	100 : 1
	5 : 61
	300 : 1
	50 : 5
	25 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 281
	25 : 7
	50 : 1
	300 : 1
	5 : 26

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14815

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14815 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14815
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_237 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_238 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_237 (Embedding)       (None, 4, 336)       4977840     input_237[0][0]                  
__________________________________________________________________________________________________
embedding_238 (Embedding)       (None, 4, 103)       17304       input_238[0][0]                  
__________________________________________________________________________________________________
flatten_237 (Flatten)           (None, 1344)         0           embedding_237[0][0]              
__________________________________________________________________________________________________
flatten_238 (Flatten)           (None, 412)          0           embedding_238[0][0]              
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 1756)         0           flatten_237[0][0]                
                                                                 flatten_238[0][0]                
__________________________________________________________________________________________________
dense_183 (Dense)               (None, 7)            12299       concatenate_119[0][0]            
__________________________________________________________________________________________________
dropout_65 (Dropout)            (None, 7)            0           dense_183[0][0]                  
__________________________________________________________________________________________________
dense_184 (Dense)               (None, 8)            64          dropout_65[0][0]                 
==================================================================================================
Total params: 5,007,507
Trainable params: 5,007,507
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47356, 1.0: 39411, 2.0: 7830, 6.0: 2400, 5.0: 1988, 4.0: 1686, 3.0: 116})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 13s - loss: 0.1793 - acc: 0.9349 - val_loss: 0.0825 - val_acc: 0.9841
Epoch 2/40
 - 13s - loss: 0.1450 - acc: 0.9518 - val_loss: 0.0785 - val_acc: 0.9853
Epoch 3/40
 - 13s - loss: 0.1401 - acc: 0.9537 - val_loss: 0.0764 - val_acc: 0.9856

==================================================================================================
	Training time : 0:01:49.557880
==================================================================================================
	Identification : 0.571
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 77
	100 : 1
	5 : 65
	300 : 1
	50 : 5
	25 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 278
	25 : 7
	50 : 1
	300 : 1
	5 : 20

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14753

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14753 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14753
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_239 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_240 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_239 (Embedding)       (None, 4, 336)       4957008     input_239[0][0]                  
__________________________________________________________________________________________________
embedding_240 (Embedding)       (None, 4, 103)       17304       input_240[0][0]                  
__________________________________________________________________________________________________
flatten_239 (Flatten)           (None, 1344)         0           embedding_239[0][0]              
__________________________________________________________________________________________________
flatten_240 (Flatten)           (None, 412)          0           embedding_240[0][0]              
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 1756)         0           flatten_239[0][0]                
                                                                 flatten_240[0][0]                
__________________________________________________________________________________________________
dense_185 (Dense)               (None, 7)            12299       concatenate_120[0][0]            
__________________________________________________________________________________________________
dropout_66 (Dropout)            (None, 7)            0           dense_185[0][0]                  
__________________________________________________________________________________________________
dense_186 (Dense)               (None, 8)            64          dropout_66[0][0]                 
==================================================================================================
Total params: 4,986,675
Trainable params: 4,986,675
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47400, 1.0: 39369, 2.0: 7713, 6.0: 2454, 5.0: 2084, 4.0: 1654, 3.0: 113})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 13s - loss: 0.2005 - acc: 0.9147 - val_loss: 0.1179 - val_acc: 0.9498
Epoch 2/40
 - 13s - loss: 0.1666 - acc: 0.9259 - val_loss: 0.1154 - val_acc: 0.9505
Epoch 3/40
 - 13s - loss: 0.1611 - acc: 0.9282 - val_loss: 0.1141 - val_acc: 0.9511

==================================================================================================
	Training time : 0:01:49.406956
==================================================================================================
	Identification : 0.517
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 90
	100 : 1
	5 : 63
	300 : 1
	50 : 5
	25 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 266
	25 : 5
	50 : 1
	300 : 1
	5 : 26

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 20983

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20983 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 20983
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_241 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_242 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_241 (Embedding)       (None, 4, 336)       7050288     input_241[0][0]                  
__________________________________________________________________________________________________
embedding_242 (Embedding)       (None, 4, 103)       23072       input_242[0][0]                  
__________________________________________________________________________________________________
flatten_241 (Flatten)           (None, 1344)         0           embedding_241[0][0]              
__________________________________________________________________________________________________
flatten_242 (Flatten)           (None, 412)          0           embedding_242[0][0]              
__________________________________________________________________________________________________
concatenate_121 (Concatenate)   (None, 1756)         0           flatten_241[0][0]                
                                                                 flatten_242[0][0]                
__________________________________________________________________________________________________
dense_187 (Dense)               (None, 7)            12299       concatenate_121[0][0]            
__________________________________________________________________________________________________
dropout_67 (Dropout)            (None, 7)            0           dense_187[0][0]                  
__________________________________________________________________________________________________
dense_188 (Dense)               (None, 8)            64          dropout_67[0][0]                 
==================================================================================================
Total params: 7,085,723
Trainable params: 7,085,723
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 55872, 1.0: 46741, 2.0: 8856, 6.0: 4741, 5.0: 1470, 4.0: 846})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 17s - loss: 0.1502 - acc: 0.9374 - val_loss: 0.0589 - val_acc: 0.9768
Epoch 2/40
 - 17s - loss: 0.1224 - acc: 0.9482 - val_loss: 0.0575 - val_acc: 0.9810
Epoch 3/40
 - 17s - loss: 0.1188 - acc: 0.9493 - val_loss: 0.0565 - val_acc: 0.9814

==================================================================================================
	Training time : 0:02:14.129073
==================================================================================================
	Identification : 0.671
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 133
	25 : 8
	5 : 81

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 202
	25 : 1
	5 : 19

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 21038

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 21038 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 21038
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_243 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_244 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_243 (Embedding)       (None, 4, 336)       7068768     input_243[0][0]                  
__________________________________________________________________________________________________
embedding_244 (Embedding)       (None, 4, 103)       23072       input_244[0][0]                  
__________________________________________________________________________________________________
flatten_243 (Flatten)           (None, 1344)         0           embedding_243[0][0]              
__________________________________________________________________________________________________
flatten_244 (Flatten)           (None, 412)          0           embedding_244[0][0]              
__________________________________________________________________________________________________
concatenate_122 (Concatenate)   (None, 1756)         0           flatten_243[0][0]                
                                                                 flatten_244[0][0]                
__________________________________________________________________________________________________
dense_189 (Dense)               (None, 7)            12299       concatenate_122[0][0]            
__________________________________________________________________________________________________
dropout_68 (Dropout)            (None, 7)            0           dense_189[0][0]                  
__________________________________________________________________________________________________
dense_190 (Dense)               (None, 8)            64          dropout_68[0][0]                 
==================================================================================================
Total params: 7,104,203
Trainable params: 7,104,203
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 55695, 1.0: 47111, 2.0: 8666, 6.0: 4732, 5.0: 1494, 4.0: 828})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 17s - loss: 0.1489 - acc: 0.9345 - val_loss: 0.0427 - val_acc: 0.9833
Epoch 2/40
 - 17s - loss: 0.1224 - acc: 0.9454 - val_loss: 0.0420 - val_acc: 0.9836
Epoch 3/40
 - 17s - loss: 0.1186 - acc: 0.9483 - val_loss: 0.0421 - val_acc: 0.9835

==================================================================================================
	Training time : 0:02:13.003127
==================================================================================================
	Identification : 0.616
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 114
	25 : 8
	5 : 73

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 223
	25 : 2
	5 : 31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 21043

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 21043 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 21043
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_245 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_246 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_245 (Embedding)       (None, 4, 336)       7070448     input_245[0][0]                  
__________________________________________________________________________________________________
embedding_246 (Embedding)       (None, 4, 103)       23072       input_246[0][0]                  
__________________________________________________________________________________________________
flatten_245 (Flatten)           (None, 1344)         0           embedding_245[0][0]              
__________________________________________________________________________________________________
flatten_246 (Flatten)           (None, 412)          0           embedding_246[0][0]              
__________________________________________________________________________________________________
concatenate_123 (Concatenate)   (None, 1756)         0           flatten_245[0][0]                
                                                                 flatten_246[0][0]                
__________________________________________________________________________________________________
dense_191 (Dense)               (None, 7)            12299       concatenate_123[0][0]            
__________________________________________________________________________________________________
dropout_69 (Dropout)            (None, 7)            0           dense_191[0][0]                  
__________________________________________________________________________________________________
dense_192 (Dense)               (None, 8)            64          dropout_69[0][0]                 
==================================================================================================
Total params: 7,105,883
Trainable params: 7,105,883
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 55617, 1.0: 46900, 2.0: 8882, 6.0: 4790, 5.0: 1517, 4.0: 820})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 17s - loss: 0.1372 - acc: 0.9505 - val_loss: 0.0505 - val_acc: 0.9892
Epoch 2/40
 - 17s - loss: 0.1060 - acc: 0.9652 - val_loss: 0.0494 - val_acc: 0.9900
Epoch 3/40
 - 17s - loss: 0.1004 - acc: 0.9674 - val_loss: 0.0472 - val_acc: 0.9901

==================================================================================================
	Training time : 0:02:13.836883
==================================================================================================
	Identification : 0.661
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 127
	25 : 8
	5 : 81

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 209
	25 : 2
	5 : 21

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 14048

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14048 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 14048
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_247 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_248 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_247 (Embedding)       (None, 4, 336)       4720128     input_247[0][0]                  
__________________________________________________________________________________________________
embedding_248 (Embedding)       (None, 4, 103)       12257       input_248[0][0]                  
__________________________________________________________________________________________________
flatten_247 (Flatten)           (None, 1344)         0           embedding_247[0][0]              
__________________________________________________________________________________________________
flatten_248 (Flatten)           (None, 412)          0           embedding_248[0][0]              
__________________________________________________________________________________________________
concatenate_124 (Concatenate)   (None, 1756)         0           flatten_247[0][0]                
                                                                 flatten_248[0][0]                
__________________________________________________________________________________________________
dense_193 (Dense)               (None, 7)            12299       concatenate_124[0][0]            
__________________________________________________________________________________________________
dropout_70 (Dropout)            (None, 7)            0           dense_193[0][0]                  
__________________________________________________________________________________________________
dense_194 (Dense)               (None, 8)            64          dropout_70[0][0]                 
==================================================================================================
Total params: 4,744,748
Trainable params: 4,744,748
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 41126, 1.0: 33176, 2.0: 7862, 5.0: 4217, 6.0: 3145})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 12s - loss: 0.2935 - acc: 0.8643 - val_loss: 0.1328 - val_acc: 0.9458
Epoch 2/40
 - 12s - loss: 0.2402 - acc: 0.8738 - val_loss: 0.1320 - val_acc: 0.9464
Epoch 3/40
 - 12s - loss: 0.2356 - acc: 0.8749 - val_loss: 0.1289 - val_acc: 0.9467

==================================================================================================
	Training time : 0:01:42.341488
==================================================================================================
	Identification : 0.49
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 66
	25 : 16
	50 : 9
	5 : 47

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 179
	25 : 7
	50 : 3
	5 : 35

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 14084

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14084 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 14084
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_249 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_250 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_249 (Embedding)       (None, 4, 336)       4732224     input_249[0][0]                  
__________________________________________________________________________________________________
embedding_250 (Embedding)       (None, 4, 103)       12257       input_250[0][0]                  
__________________________________________________________________________________________________
flatten_249 (Flatten)           (None, 1344)         0           embedding_249[0][0]              
__________________________________________________________________________________________________
flatten_250 (Flatten)           (None, 412)          0           embedding_250[0][0]              
__________________________________________________________________________________________________
concatenate_125 (Concatenate)   (None, 1756)         0           flatten_249[0][0]                
                                                                 flatten_250[0][0]                
__________________________________________________________________________________________________
dense_195 (Dense)               (None, 7)            12299       concatenate_125[0][0]            
__________________________________________________________________________________________________
dropout_71 (Dropout)            (None, 7)            0           dense_195[0][0]                  
__________________________________________________________________________________________________
dense_196 (Dense)               (None, 8)            64          dropout_71[0][0]                 
==================================================================================================
Total params: 4,756,844
Trainable params: 4,756,844
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 40824, 1.0: 33341, 2.0: 7955, 5.0: 4300, 6.0: 3106})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 12s - loss: 0.2020 - acc: 0.9261 - val_loss: 0.0873 - val_acc: 0.9780
Epoch 2/40
 - 12s - loss: 0.1676 - acc: 0.9377 - val_loss: 0.0860 - val_acc: 0.9782
Epoch 3/40
 - 12s - loss: 0.1616 - acc: 0.9396 - val_loss: 0.0846 - val_acc: 0.9786

==================================================================================================
	Training time : 0:01:38.496807
==================================================================================================
	Identification : 0.469
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 69
	25 : 11
	50 : 9
	5 : 49

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 176
	25 : 10
	50 : 6
	5 : 33

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13995

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13995 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13995
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 336
	POS = 103 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_251 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_252 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_251 (Embedding)       (None, 4, 336)       4702320     input_251[0][0]                  
__________________________________________________________________________________________________
embedding_252 (Embedding)       (None, 4, 103)       12257       input_252[0][0]                  
__________________________________________________________________________________________________
flatten_251 (Flatten)           (None, 1344)         0           embedding_251[0][0]              
__________________________________________________________________________________________________
flatten_252 (Flatten)           (None, 412)          0           embedding_252[0][0]              
__________________________________________________________________________________________________
concatenate_126 (Concatenate)   (None, 1756)         0           flatten_251[0][0]                
                                                                 flatten_252[0][0]                
__________________________________________________________________________________________________
dense_197 (Dense)               (None, 7)            12299       concatenate_126[0][0]            
__________________________________________________________________________________________________
dropout_72 (Dropout)            (None, 7)            0           dense_197[0][0]                  
__________________________________________________________________________________________________
dense_198 (Dense)               (None, 8)            64          dropout_72[0][0]                 
==================================================================================================
Total params: 4,726,940
Trainable params: 4,726,940
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 41064, 1.0: 33292, 2.0: 7804, 5.0: 4236, 6.0: 3130})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.087
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 12s - loss: 0.2313 - acc: 0.9085 - val_loss: 0.1315 - val_acc: 0.9461
Epoch 2/40
 - 12s - loss: 0.1991 - acc: 0.9203 - val_loss: 0.1288 - val_acc: 0.9471
Epoch 3/40
 - 12s - loss: 0.1957 - acc: 0.9212 - val_loss: 0.1301 - val_acc: 0.9473

==================================================================================================
	Training time : 0:01:37.018065
==================================================================================================
	Identification : 0.474
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 74
	25 : 16
	50 : 9
	5 : 57

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 173
	25 : 8
	50 : 4
	5 : 27

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
19_False_True_462_54_frequent_True_56_relu_0.463_False_512_relu_0.2_adagrad_0.029_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12174

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12174 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12174
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_253 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_254 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_253 (Embedding)       (None, 4, 462)       5624388     input_253[0][0]                  
__________________________________________________________________________________________________
embedding_254 (Embedding)       (None, 4, 54)        8208        input_254[0][0]                  
__________________________________________________________________________________________________
flatten_253 (Flatten)           (None, 1848)         0           embedding_253[0][0]              
__________________________________________________________________________________________________
flatten_254 (Flatten)           (None, 216)          0           embedding_254[0][0]              
__________________________________________________________________________________________________
concatenate_127 (Concatenate)   (None, 2064)         0           flatten_253[0][0]                
                                                                 flatten_254[0][0]                
__________________________________________________________________________________________________
dense_199 (Dense)               (None, 56)           115640      concatenate_127[0][0]            
__________________________________________________________________________________________________
dropout_73 (Dropout)            (None, 56)           0           dense_199[0][0]                  
__________________________________________________________________________________________________
dense_200 (Dense)               (None, 8)            456         dropout_73[0][0]                 
==================================================================================================
Total params: 5,748,692
Trainable params: 5,748,692
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27106, 1.0: 26494, 2.0: 407, 4.0: 181, 6.0: 103, 5.0: 67, 3.0: 3})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 8s - loss: 0.0365 - acc: 0.9864 - val_loss: 0.0260 - val_acc: 0.9882
Epoch 2/40
 - 8s - loss: 0.0225 - acc: 0.9894 - val_loss: 0.0261 - val_acc: 0.9879
Epoch 3/40
 - 8s - loss: 0.0192 - acc: 0.9901 - val_loss: 0.0274 - val_acc: 0.9873

==================================================================================================
	Training time : 0:01:12.455606
==================================================================================================
	Identification : 0.135
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 5
	100 : 1
	5 : 12
	25 : 4

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 275
	100 : 1
	5 : 79
	25 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12139

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12139 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12139
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_255 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_256 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_255 (Embedding)       (None, 4, 462)       5608218     input_255[0][0]                  
__________________________________________________________________________________________________
embedding_256 (Embedding)       (None, 4, 54)        8208        input_256[0][0]                  
__________________________________________________________________________________________________
flatten_255 (Flatten)           (None, 1848)         0           embedding_255[0][0]              
__________________________________________________________________________________________________
flatten_256 (Flatten)           (None, 216)          0           embedding_256[0][0]              
__________________________________________________________________________________________________
concatenate_128 (Concatenate)   (None, 2064)         0           flatten_255[0][0]                
                                                                 flatten_256[0][0]                
__________________________________________________________________________________________________
dense_201 (Dense)               (None, 56)           115640      concatenate_128[0][0]            
__________________________________________________________________________________________________
dropout_74 (Dropout)            (None, 56)           0           dense_201[0][0]                  
__________________________________________________________________________________________________
dense_202 (Dense)               (None, 8)            456         dropout_74[0][0]                 
==================================================================================================
Total params: 5,732,522
Trainable params: 5,732,522
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27001, 1.0: 26618, 2.0: 384, 4.0: 160, 6.0: 112, 5.0: 82, 3.0: 4})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 8s - loss: 0.0376 - acc: 0.9864 - val_loss: 0.0255 - val_acc: 0.9885
Epoch 2/40
 - 8s - loss: 0.0232 - acc: 0.9894 - val_loss: 0.0261 - val_acc: 0.9880
Epoch 3/40
 - 8s - loss: 0.0198 - acc: 0.9899 - val_loss: 0.0266 - val_acc: 0.9877

==================================================================================================
	Training time : 0:01:14.302459
==================================================================================================
	Identification : 0.528
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	25 : 14
	100 : 1
	5 : 37
	0 : 20

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 261
	25 : 8
	5 : 60

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12111

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12111 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12111
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_257 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_258 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_257 (Embedding)       (None, 4, 462)       5595282     input_257[0][0]                  
__________________________________________________________________________________________________
embedding_258 (Embedding)       (None, 4, 54)        8208        input_258[0][0]                  
__________________________________________________________________________________________________
flatten_257 (Flatten)           (None, 1848)         0           embedding_257[0][0]              
__________________________________________________________________________________________________
flatten_258 (Flatten)           (None, 216)          0           embedding_258[0][0]              
__________________________________________________________________________________________________
concatenate_129 (Concatenate)   (None, 2064)         0           flatten_257[0][0]                
                                                                 flatten_258[0][0]                
__________________________________________________________________________________________________
dense_203 (Dense)               (None, 56)           115640      concatenate_129[0][0]            
__________________________________________________________________________________________________
dropout_75 (Dropout)            (None, 56)           0           dense_203[0][0]                  
__________________________________________________________________________________________________
dense_204 (Dense)               (None, 8)            456         dropout_75[0][0]                 
==================================================================================================
Total params: 5,719,586
Trainable params: 5,719,586
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26846, 1.0: 26765, 2.0: 387, 4.0: 173, 6.0: 115, 5.0: 68, 3.0: 7})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 11s - loss: 0.0368 - acc: 0.9865 - val_loss: 0.0284 - val_acc: 0.9873
Epoch 2/40
 - 8s - loss: 0.0227 - acc: 0.9895 - val_loss: 0.0291 - val_acc: 0.9871
Epoch 3/40
 - 8s - loss: 0.0194 - acc: 0.9901 - val_loss: 0.0300 - val_acc: 0.9870

==================================================================================================
	Training time : 0:01:16.302388
==================================================================================================
	Identification : 0.158
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 9
	25 : 2
	5 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 271
	100 : 1
	5 : 68
	25 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15313

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15313 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15313
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_259 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_260 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_259 (Embedding)       (None, 4, 462)       7074606     input_259[0][0]                  
__________________________________________________________________________________________________
embedding_260 (Embedding)       (None, 4, 54)        9126        input_260[0][0]                  
__________________________________________________________________________________________________
flatten_259 (Flatten)           (None, 1848)         0           embedding_259[0][0]              
__________________________________________________________________________________________________
flatten_260 (Flatten)           (None, 216)          0           embedding_260[0][0]              
__________________________________________________________________________________________________
concatenate_130 (Concatenate)   (None, 2064)         0           flatten_259[0][0]                
                                                                 flatten_260[0][0]                
__________________________________________________________________________________________________
dense_205 (Dense)               (None, 56)           115640      concatenate_130[0][0]            
__________________________________________________________________________________________________
dropout_76 (Dropout)            (None, 56)           0           dense_205[0][0]                  
__________________________________________________________________________________________________
dense_206 (Dense)               (None, 8)            456         dropout_76[0][0]                 
==================================================================================================
Total params: 7,199,828
Trainable params: 7,199,828
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({1.0: 26937, 0.0: 26782, 2.0: 284, 6.0: 169, 5.0: 51, 4.0: 38})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0321 - acc: 0.9888 - val_loss: 0.0227 - val_acc: 0.9907
Epoch 2/40
 - 8s - loss: 0.0191 - acc: 0.9918 - val_loss: 0.0218 - val_acc: 0.9909
Epoch 3/40
 - 8s - loss: 0.0157 - acc: 0.9927 - val_loss: 0.0223 - val_acc: 0.9908

==================================================================================================
	Training time : 0:01:14.974458
==================================================================================================
	Identification : 0.488
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 68
	5 : 27

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 184
	25 : 1
	5 : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15281

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15281 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15281
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_261 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_262 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_261 (Embedding)       (None, 4, 462)       7059822     input_261[0][0]                  
__________________________________________________________________________________________________
embedding_262 (Embedding)       (None, 4, 54)        9126        input_262[0][0]                  
__________________________________________________________________________________________________
flatten_261 (Flatten)           (None, 1848)         0           embedding_261[0][0]              
__________________________________________________________________________________________________
flatten_262 (Flatten)           (None, 216)          0           embedding_262[0][0]              
__________________________________________________________________________________________________
concatenate_131 (Concatenate)   (None, 2064)         0           flatten_261[0][0]                
                                                                 flatten_262[0][0]                
__________________________________________________________________________________________________
dense_207 (Dense)               (None, 56)           115640      concatenate_131[0][0]            
__________________________________________________________________________________________________
dropout_77 (Dropout)            (None, 56)           0           dense_207[0][0]                  
__________________________________________________________________________________________________
dense_208 (Dense)               (None, 8)            456         dropout_77[0][0]                 
==================================================================================================
Total params: 7,185,044
Trainable params: 7,185,044
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 26975, 1.0: 26725, 2.0: 301, 6.0: 167, 5.0: 49, 4.0: 44})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0333 - acc: 0.9884 - val_loss: 0.0219 - val_acc: 0.9905
Epoch 2/40
 - 8s - loss: 0.0188 - acc: 0.9919 - val_loss: 0.0204 - val_acc: 0.9897
Epoch 3/40
 - 8s - loss: 0.0155 - acc: 0.9927 - val_loss: 0.0209 - val_acc: 0.9904

==================================================================================================
	Training time : 0:01:14.087061
==================================================================================================
	Identification : 0.344
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 38
	5 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 211
	25 : 1
	5 : 32

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15240

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15240 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15240
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_263 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_264 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_263 (Embedding)       (None, 4, 462)       7040880     input_263[0][0]                  
__________________________________________________________________________________________________
embedding_264 (Embedding)       (None, 4, 54)        9126        input_264[0][0]                  
__________________________________________________________________________________________________
flatten_263 (Flatten)           (None, 1848)         0           embedding_263[0][0]              
__________________________________________________________________________________________________
flatten_264 (Flatten)           (None, 216)          0           embedding_264[0][0]              
__________________________________________________________________________________________________
concatenate_132 (Concatenate)   (None, 2064)         0           flatten_263[0][0]                
                                                                 flatten_264[0][0]                
__________________________________________________________________________________________________
dense_209 (Dense)               (None, 56)           115640      concatenate_132[0][0]            
__________________________________________________________________________________________________
dropout_78 (Dropout)            (None, 56)           0           dense_209[0][0]                  
__________________________________________________________________________________________________
dense_210 (Dense)               (None, 8)            456         dropout_78[0][0]                 
==================================================================================================
Total params: 7,166,102
Trainable params: 7,166,102
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27166, 1.0: 26556, 2.0: 297, 6.0: 151, 5.0: 52, 4.0: 39})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0327 - acc: 0.9888 - val_loss: 0.0226 - val_acc: 0.9912
Epoch 2/40
 - 8s - loss: 0.0188 - acc: 0.9918 - val_loss: 0.0222 - val_acc: 0.9914
Epoch 3/40
 - 8s - loss: 0.0155 - acc: 0.9926 - val_loss: 0.0232 - val_acc: 0.9899

==================================================================================================
	Training time : 0:01:12.890871
==================================================================================================
	Identification : 0.234
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 29
	5 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 218
	25 : 1
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12580

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12580 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12580
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_265 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_266 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_265 (Embedding)       (None, 4, 462)       5811960     input_265[0][0]                  
__________________________________________________________________________________________________
embedding_266 (Embedding)       (None, 4, 54)        5940        input_266[0][0]                  
__________________________________________________________________________________________________
flatten_265 (Flatten)           (None, 1848)         0           embedding_265[0][0]              
__________________________________________________________________________________________________
flatten_266 (Flatten)           (None, 216)          0           embedding_266[0][0]              
__________________________________________________________________________________________________
concatenate_133 (Concatenate)   (None, 2064)         0           flatten_265[0][0]                
                                                                 flatten_266[0][0]                
__________________________________________________________________________________________________
dense_211 (Dense)               (None, 56)           115640      concatenate_133[0][0]            
__________________________________________________________________________________________________
dropout_79 (Dropout)            (None, 56)           0           dense_211[0][0]                  
__________________________________________________________________________________________________
dense_212 (Dense)               (None, 8)            456         dropout_79[0][0]                 
==================================================================================================
Total params: 5,933,996
Trainable params: 5,933,996
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27233, 1.0: 26294, 2.0: 494, 6.0: 235, 5.0: 230})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 8s - loss: 0.0538 - acc: 0.9799 - val_loss: 0.0404 - val_acc: 0.9830
Epoch 2/40
 - 8s - loss: 0.0378 - acc: 0.9827 - val_loss: 0.0394 - val_acc: 0.9827
Epoch 3/40
 - 8s - loss: 0.0333 - acc: 0.9834 - val_loss: 0.0396 - val_acc: 0.9832

==================================================================================================
	Training time : 0:01:13.099724
==================================================================================================
	Identification : 0.288
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 16
	25 : 7
	50 : 4
	5 : 26

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 383
	25 : 12
	50 : 6
	5 : 89

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12618

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12618 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12618
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_267 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_268 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_267 (Embedding)       (None, 4, 462)       5829516     input_267[0][0]                  
__________________________________________________________________________________________________
embedding_268 (Embedding)       (None, 4, 54)        5940        input_268[0][0]                  
__________________________________________________________________________________________________
flatten_267 (Flatten)           (None, 1848)         0           embedding_267[0][0]              
__________________________________________________________________________________________________
flatten_268 (Flatten)           (None, 216)          0           embedding_268[0][0]              
__________________________________________________________________________________________________
concatenate_134 (Concatenate)   (None, 2064)         0           flatten_267[0][0]                
                                                                 flatten_268[0][0]                
__________________________________________________________________________________________________
dense_213 (Dense)               (None, 56)           115640      concatenate_134[0][0]            
__________________________________________________________________________________________________
dropout_80 (Dropout)            (None, 56)           0           dense_213[0][0]                  
__________________________________________________________________________________________________
dense_214 (Dense)               (None, 8)            456         dropout_80[0][0]                 
==================================================================================================
Total params: 5,951,552
Trainable params: 5,951,552
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27027, 1.0: 26530, 2.0: 466, 5.0: 242, 6.0: 221})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 8s - loss: 0.0540 - acc: 0.9799 - val_loss: 0.0399 - val_acc: 0.9827
Epoch 2/40
 - 8s - loss: 0.0367 - acc: 0.9829 - val_loss: 0.0405 - val_acc: 0.9809
Epoch 3/40
 - 8s - loss: 0.0327 - acc: 0.9836 - val_loss: 0.0407 - val_acc: 0.9816

==================================================================================================
	Training time : 0:01:13.825436
==================================================================================================
	Identification : 0.137
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 2
	25 : 5
	50 : 5
	5 : 4

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 395
	25 : 16
	50 : 7
	5 : 104

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12514

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12514 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12514
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_269 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_270 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_269 (Embedding)       (None, 4, 462)       5781468     input_269[0][0]                  
__________________________________________________________________________________________________
embedding_270 (Embedding)       (None, 4, 54)        5940        input_270[0][0]                  
__________________________________________________________________________________________________
flatten_269 (Flatten)           (None, 1848)         0           embedding_269[0][0]              
__________________________________________________________________________________________________
flatten_270 (Flatten)           (None, 216)          0           embedding_270[0][0]              
__________________________________________________________________________________________________
concatenate_135 (Concatenate)   (None, 2064)         0           flatten_269[0][0]                
                                                                 flatten_270[0][0]                
__________________________________________________________________________________________________
dense_215 (Dense)               (None, 56)           115640      concatenate_135[0][0]            
__________________________________________________________________________________________________
dropout_81 (Dropout)            (None, 56)           0           dense_215[0][0]                  
__________________________________________________________________________________________________
dense_216 (Dense)               (None, 8)            456         dropout_81[0][0]                 
==================================================================================================
Total params: 5,903,504
Trainable params: 5,903,504
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27180, 1.0: 26303, 2.0: 505, 5.0: 269, 6.0: 229})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 8s - loss: 0.0529 - acc: 0.9799 - val_loss: 0.0410 - val_acc: 0.9821
Epoch 2/40
 - 8s - loss: 0.0367 - acc: 0.9831 - val_loss: 0.0406 - val_acc: 0.9809
Epoch 3/40
 - 8s - loss: 0.0326 - acc: 0.9835 - val_loss: 0.0424 - val_acc: 0.9811

==================================================================================================
	Training time : 0:01:15.319185
==================================================================================================
	Identification : 0.196
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 16
	25 : 6
	50 : 4
	5 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 383
	25 : 16
	50 : 7
	5 : 92

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14841

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14841 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14841
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_271 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_272 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_271 (Embedding)       (None, 4, 462)       6856542     input_271[0][0]                  
__________________________________________________________________________________________________
embedding_272 (Embedding)       (None, 4, 54)        9072        input_272[0][0]                  
__________________________________________________________________________________________________
flatten_271 (Flatten)           (None, 1848)         0           embedding_271[0][0]              
__________________________________________________________________________________________________
flatten_272 (Flatten)           (None, 216)          0           embedding_272[0][0]              
__________________________________________________________________________________________________
concatenate_136 (Concatenate)   (None, 2064)         0           flatten_271[0][0]                
                                                                 flatten_272[0][0]                
__________________________________________________________________________________________________
dense_217 (Dense)               (None, 56)           115640      concatenate_136[0][0]            
__________________________________________________________________________________________________
dropout_82 (Dropout)            (None, 56)           0           dense_217[0][0]                  
__________________________________________________________________________________________________
dense_218 (Dense)               (None, 8)            456         dropout_82[0][0]                 
==================================================================================================
Total params: 6,981,710
Trainable params: 6,981,710
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39777, 1.0: 39437, 2.0: 606, 4.0: 276, 6.0: 165, 5.0: 104, 3.0: 6})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 12s - loss: 0.0342 - acc: 0.9867 - val_loss: 0.0251 - val_acc: 0.9884
Epoch 2/40
 - 12s - loss: 0.0222 - acc: 0.9891 - val_loss: 0.0254 - val_acc: 0.9881
Epoch 3/40
 - 12s - loss: 0.0194 - acc: 0.9899 - val_loss: 0.0266 - val_acc: 0.9881

==================================================================================================
	Training time : 0:01:43.422430
==================================================================================================
	Identification : 0.503
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 42
	100 : 1
	5 : 50
	300 : 1
	50 : 5
	25 : 19

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 309
	25 : 7
	5 : 30

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14856

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14856 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14856
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_273 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_274 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_273 (Embedding)       (None, 4, 462)       6863472     input_273[0][0]                  
__________________________________________________________________________________________________
embedding_274 (Embedding)       (None, 4, 54)        9072        input_274[0][0]                  
__________________________________________________________________________________________________
flatten_273 (Flatten)           (None, 1848)         0           embedding_273[0][0]              
__________________________________________________________________________________________________
flatten_274 (Flatten)           (None, 216)          0           embedding_274[0][0]              
__________________________________________________________________________________________________
concatenate_137 (Concatenate)   (None, 2064)         0           flatten_273[0][0]                
                                                                 flatten_274[0][0]                
__________________________________________________________________________________________________
dense_219 (Dense)               (None, 56)           115640      concatenate_137[0][0]            
__________________________________________________________________________________________________
dropout_83 (Dropout)            (None, 56)           0           dense_219[0][0]                  
__________________________________________________________________________________________________
dense_220 (Dense)               (None, 8)            456         dropout_83[0][0]                 
==================================================================================================
Total params: 6,988,640
Trainable params: 6,988,640
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40003, 1.0: 39288, 2.0: 549, 4.0: 260, 6.0: 153, 5.0: 109, 3.0: 9})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 12s - loss: 0.0346 - acc: 0.9866 - val_loss: 0.0249 - val_acc: 0.9890
Epoch 2/40
 - 12s - loss: 0.0229 - acc: 0.9893 - val_loss: 0.0248 - val_acc: 0.9890
Epoch 3/40
 - 12s - loss: 0.0200 - acc: 0.9897 - val_loss: 0.0257 - val_acc: 0.9889

==================================================================================================
	Training time : 0:01:42.689398
==================================================================================================
	Identification : 0.398
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 26
	100 : 1
	5 : 35
	300 : 1
	50 : 5
	25 : 15

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 324
	25 : 10
	5 : 46

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14774

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14774 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14774
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_275 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_276 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_275 (Embedding)       (None, 4, 462)       6825588     input_275[0][0]                  
__________________________________________________________________________________________________
embedding_276 (Embedding)       (None, 4, 54)        9072        input_276[0][0]                  
__________________________________________________________________________________________________
flatten_275 (Flatten)           (None, 1848)         0           embedding_275[0][0]              
__________________________________________________________________________________________________
flatten_276 (Flatten)           (None, 216)          0           embedding_276[0][0]              
__________________________________________________________________________________________________
concatenate_138 (Concatenate)   (None, 2064)         0           flatten_275[0][0]                
                                                                 flatten_276[0][0]                
__________________________________________________________________________________________________
dense_221 (Dense)               (None, 56)           115640      concatenate_138[0][0]            
__________________________________________________________________________________________________
dropout_84 (Dropout)            (None, 56)           0           dense_221[0][0]                  
__________________________________________________________________________________________________
dense_222 (Dense)               (None, 8)            456         dropout_84[0][0]                 
==================================================================================================
Total params: 6,950,756
Trainable params: 6,950,756
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39684, 1.0: 39543, 2.0: 587, 4.0: 275, 6.0: 164, 5.0: 111, 3.0: 7})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 12s - loss: 0.0331 - acc: 0.9869 - val_loss: 0.0252 - val_acc: 0.9882
Epoch 2/40
 - 12s - loss: 0.0221 - acc: 0.9893 - val_loss: 0.0248 - val_acc: 0.9885
Epoch 3/40
 - 12s - loss: 0.0192 - acc: 0.9900 - val_loss: 0.0256 - val_acc: 0.9874

==================================================================================================
	Training time : 0:01:46.013097
==================================================================================================
	Identification : 0.543
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 49
	100 : 1
	5 : 52
	300 : 1
	50 : 5
	25 : 20

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 308
	25 : 4
	5 : 31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 21100

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 21100 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 21100
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_277 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_278 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_277 (Embedding)       (None, 4, 462)       9748200     input_277[0][0]                  
__________________________________________________________________________________________________
embedding_278 (Embedding)       (None, 4, 54)        12096       input_278[0][0]                  
__________________________________________________________________________________________________
flatten_277 (Flatten)           (None, 1848)         0           embedding_277[0][0]              
__________________________________________________________________________________________________
flatten_278 (Flatten)           (None, 216)          0           embedding_278[0][0]              
__________________________________________________________________________________________________
concatenate_139 (Concatenate)   (None, 2064)         0           flatten_277[0][0]                
                                                                 flatten_278[0][0]                
__________________________________________________________________________________________________
dense_223 (Dense)               (None, 56)           115640      concatenate_139[0][0]            
__________________________________________________________________________________________________
dropout_85 (Dropout)            (None, 56)           0           dense_223[0][0]                  
__________________________________________________________________________________________________
dense_224 (Dense)               (None, 8)            456         dropout_85[0][0]                 
==================================================================================================
Total params: 9,876,392
Trainable params: 9,876,392
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47550, 1.0: 46602, 2.0: 543, 6.0: 276, 5.0: 85, 4.0: 67})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 17s - loss: 0.0282 - acc: 0.9896 - val_loss: 0.0198 - val_acc: 0.9916
Epoch 2/40
 - 17s - loss: 0.0176 - acc: 0.9920 - val_loss: 0.0190 - val_acc: 0.9917
Epoch 3/40
 - 17s - loss: 0.0147 - acc: 0.9927 - val_loss: 0.0200 - val_acc: 0.9907

==================================================================================================
	Training time : 0:02:09.320095
==================================================================================================
	Identification : 0.171
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 24
	25 : 4
	5 : 17

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 308
	25 : 7
	5 : 81

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 21122

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 21122 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 21122
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_279 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_280 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_279 (Embedding)       (None, 4, 462)       9758364     input_279[0][0]                  
__________________________________________________________________________________________________
embedding_280 (Embedding)       (None, 4, 54)        12096       input_280[0][0]                  
__________________________________________________________________________________________________
flatten_279 (Flatten)           (None, 1848)         0           embedding_279[0][0]              
__________________________________________________________________________________________________
flatten_280 (Flatten)           (None, 216)          0           embedding_280[0][0]              
__________________________________________________________________________________________________
concatenate_140 (Concatenate)   (None, 2064)         0           flatten_279[0][0]                
                                                                 flatten_280[0][0]                
__________________________________________________________________________________________________
dense_225 (Dense)               (None, 56)           115640      concatenate_140[0][0]            
__________________________________________________________________________________________________
dropout_86 (Dropout)            (None, 56)           0           dense_225[0][0]                  
__________________________________________________________________________________________________
dense_226 (Dense)               (None, 8)            456         dropout_86[0][0]                 
==================================================================================================
Total params: 9,886,556
Trainable params: 9,886,556
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47484, 1.0: 46649, 2.0: 533, 6.0: 303, 5.0: 87, 4.0: 67})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 17s - loss: 0.0290 - acc: 0.9894 - val_loss: 0.0199 - val_acc: 0.9910
Epoch 2/40
 - 17s - loss: 0.0175 - acc: 0.9921 - val_loss: 0.0194 - val_acc: 0.9913
Epoch 3/40
 - 17s - loss: 0.0148 - acc: 0.9926 - val_loss: 0.0192 - val_acc: 0.9909

==================================================================================================
	Training time : 0:02:06.946621
==================================================================================================
	Identification : 0.581
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 99
	25 : 6
	5 : 67

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 233
	25 : 2
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 20980

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20980 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 20980
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_281 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_282 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_281 (Embedding)       (None, 4, 462)       9692760     input_281[0][0]                  
__________________________________________________________________________________________________
embedding_282 (Embedding)       (None, 4, 54)        12096       input_282[0][0]                  
__________________________________________________________________________________________________
flatten_281 (Flatten)           (None, 1848)         0           embedding_281[0][0]              
__________________________________________________________________________________________________
flatten_282 (Flatten)           (None, 216)          0           embedding_282[0][0]              
__________________________________________________________________________________________________
concatenate_141 (Concatenate)   (None, 2064)         0           flatten_281[0][0]                
                                                                 flatten_282[0][0]                
__________________________________________________________________________________________________
dense_227 (Dense)               (None, 56)           115640      concatenate_141[0][0]            
__________________________________________________________________________________________________
dropout_87 (Dropout)            (None, 56)           0           dense_227[0][0]                  
__________________________________________________________________________________________________
dense_228 (Dense)               (None, 8)            456         dropout_87[0][0]                 
==================================================================================================
Total params: 9,820,952
Trainable params: 9,820,952
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47220, 1.0: 46978, 2.0: 512, 6.0: 263, 4.0: 76, 5.0: 74})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 16s - loss: 0.0286 - acc: 0.9896 - val_loss: 0.0197 - val_acc: 0.9914
Epoch 2/40
 - 17s - loss: 0.0173 - acc: 0.9920 - val_loss: 0.0183 - val_acc: 0.9900
Epoch 3/40
 - 16s - loss: 0.0147 - acc: 0.9926 - val_loss: 0.0193 - val_acc: 0.9913

==================================================================================================
	Training time : 0:02:07.282678
==================================================================================================
	Identification : 0.212
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 42
	25 : 1
	5 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 290
	25 : 7
	5 : 80

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13968

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13968 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13968
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_283 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_284 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_283 (Embedding)       (None, 4, 462)       6453216     input_283[0][0]                  
__________________________________________________________________________________________________
embedding_284 (Embedding)       (None, 4, 54)        6426        input_284[0][0]                  
__________________________________________________________________________________________________
flatten_283 (Flatten)           (None, 1848)         0           embedding_283[0][0]              
__________________________________________________________________________________________________
flatten_284 (Flatten)           (None, 216)          0           embedding_284[0][0]              
__________________________________________________________________________________________________
concatenate_142 (Concatenate)   (None, 2064)         0           flatten_283[0][0]                
                                                                 flatten_284[0][0]                
__________________________________________________________________________________________________
dense_229 (Dense)               (None, 56)           115640      concatenate_142[0][0]            
__________________________________________________________________________________________________
dropout_88 (Dropout)            (None, 56)           0           dense_229[0][0]                  
__________________________________________________________________________________________________
dense_230 (Dense)               (None, 8)            456         dropout_88[0][0]                 
==================================================================================================
Total params: 6,575,738
Trainable params: 6,575,738
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33520, 1.0: 32795, 2.0: 662, 5.0: 313, 6.0: 296})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 10s - loss: 0.0514 - acc: 0.9800 - val_loss: 0.0409 - val_acc: 0.9822
Epoch 2/40
 - 9s - loss: 0.0362 - acc: 0.9828 - val_loss: 0.0408 - val_acc: 0.9823
Epoch 3/40
 - 10s - loss: 0.0325 - acc: 0.9835 - val_loss: 0.0407 - val_acc: 0.9825

==================================================================================================
	Training time : 0:01:27.971001
==================================================================================================
	Identification : 0.178
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 6
	50 : 5
	5 : 12

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 244
	25 : 17
	50 : 8
	5 : 71

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13942

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13942 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13942
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_285 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_286 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_285 (Embedding)       (None, 4, 462)       6441204     input_285[0][0]                  
__________________________________________________________________________________________________
embedding_286 (Embedding)       (None, 4, 54)        6426        input_286[0][0]                  
__________________________________________________________________________________________________
flatten_285 (Flatten)           (None, 1848)         0           embedding_285[0][0]              
__________________________________________________________________________________________________
flatten_286 (Flatten)           (None, 216)          0           embedding_286[0][0]              
__________________________________________________________________________________________________
concatenate_143 (Concatenate)   (None, 2064)         0           flatten_285[0][0]                
                                                                 flatten_286[0][0]                
__________________________________________________________________________________________________
dense_231 (Dense)               (None, 56)           115640      concatenate_143[0][0]            
__________________________________________________________________________________________________
dropout_89 (Dropout)            (None, 56)           0           dense_231[0][0]                  
__________________________________________________________________________________________________
dense_232 (Dense)               (None, 8)            456         dropout_89[0][0]                 
==================================================================================================
Total params: 6,563,726
Trainable params: 6,563,726
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33405, 1.0: 32898, 2.0: 660, 5.0: 319, 6.0: 304})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 9s - loss: 0.0513 - acc: 0.9802 - val_loss: 0.0411 - val_acc: 0.9826
Epoch 2/40
 - 9s - loss: 0.0361 - acc: 0.9828 - val_loss: 0.0395 - val_acc: 0.9826
Epoch 3/40
 - 9s - loss: 0.0323 - acc: 0.9835 - val_loss: 0.0402 - val_acc: 0.9826

==================================================================================================
	Training time : 0:01:26.109386
==================================================================================================
	Identification : 0.067
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 3
	50 : 2
	5 : 5

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 244
	25 : 16
	50 : 9
	5 : 75

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13968

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13968 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13968
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 462
	POS = 54 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_287 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_288 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_287 (Embedding)       (None, 4, 462)       6453216     input_287[0][0]                  
__________________________________________________________________________________________________
embedding_288 (Embedding)       (None, 4, 54)        6426        input_288[0][0]                  
__________________________________________________________________________________________________
flatten_287 (Flatten)           (None, 1848)         0           embedding_287[0][0]              
__________________________________________________________________________________________________
flatten_288 (Flatten)           (None, 216)          0           embedding_288[0][0]              
__________________________________________________________________________________________________
concatenate_144 (Concatenate)   (None, 2064)         0           flatten_287[0][0]                
                                                                 flatten_288[0][0]                
__________________________________________________________________________________________________
dense_233 (Dense)               (None, 56)           115640      concatenate_144[0][0]            
__________________________________________________________________________________________________
dropout_90 (Dropout)            (None, 56)           0           dense_233[0][0]                  
__________________________________________________________________________________________________
dense_234 (Dense)               (None, 8)            456         dropout_90[0][0]                 
==================================================================================================
Total params: 6,575,738
Trainable params: 6,575,738
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33533, 1.0: 32862, 2.0: 607, 5.0: 299, 6.0: 285})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.029
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 9s - loss: 0.0510 - acc: 0.9800 - val_loss: 0.0396 - val_acc: 0.9826
Epoch 2/40
 - 9s - loss: 0.0359 - acc: 0.9828 - val_loss: 0.0389 - val_acc: 0.9816
Epoch 3/40
 - 9s - loss: 0.0322 - acc: 0.9835 - val_loss: 0.0397 - val_acc: 0.9813

==================================================================================================
	Training time : 0:01:26.524401
==================================================================================================
	Identification : 0.233
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 13
	25 : 7
	50 : 4
	5 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 231
	25 : 14
	50 : 8
	5 : 71

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
31_True_True_417_46_compact_False_40_relu_0.52_False_512_relu_0.2_adagrad_0.09_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_289 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_290 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_289 (Embedding)       (None, 4, 417)       1167600     input_289[0][0]                  
__________________________________________________________________________________________________
embedding_290 (Embedding)       (None, 4, 46)        6992        input_290[0][0]                  
__________________________________________________________________________________________________
flatten_289 (Flatten)           (None, 1668)         0           embedding_289[0][0]              
__________________________________________________________________________________________________
flatten_290 (Flatten)           (None, 184)          0           embedding_290[0][0]              
__________________________________________________________________________________________________
concatenate_145 (Concatenate)   (None, 1852)         0           flatten_289[0][0]                
                                                                 flatten_290[0][0]                
__________________________________________________________________________________________________
dense_235 (Dense)               (None, 8)            14824       concatenate_145[0][0]            
==================================================================================================
Total params: 1,189,416
Trainable params: 1,189,416
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32401, 1.0: 26794, 2.0: 6027, 6.0: 1846, 5.0: 1482, 4.0: 1381, 3.0: 89})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 7s - loss: 0.0509 - acc: 0.9855 - val_loss: 0.0395 - val_acc: 0.9883
Epoch 2/40
 - 7s - loss: 0.0332 - acc: 0.9900 - val_loss: 0.0397 - val_acc: 0.9886
Epoch 3/40
 - 7s - loss: 0.0314 - acc: 0.9905 - val_loss: 0.0396 - val_acc: 0.9886

==================================================================================================
	Training time : 0:01:11.339270
==================================================================================================
	Identification : 0.487
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 109
	25 : 9
	100 : 1
	5 : 64

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 176
	100 : 1
	5 : 39
	25 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_291 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_292 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_291 (Embedding)       (None, 4, 417)       1167600     input_291[0][0]                  
__________________________________________________________________________________________________
embedding_292 (Embedding)       (None, 4, 46)        6992        input_292[0][0]                  
__________________________________________________________________________________________________
flatten_291 (Flatten)           (None, 1668)         0           embedding_291[0][0]              
__________________________________________________________________________________________________
flatten_292 (Flatten)           (None, 184)          0           embedding_292[0][0]              
__________________________________________________________________________________________________
concatenate_146 (Concatenate)   (None, 1852)         0           flatten_291[0][0]                
                                                                 flatten_292[0][0]                
__________________________________________________________________________________________________
dense_236 (Dense)               (None, 8)            14824       concatenate_146[0][0]            
==================================================================================================
Total params: 1,189,416
Trainable params: 1,189,416
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32471, 1.0: 26785, 2.0: 5918, 6.0: 1932, 5.0: 1440, 4.0: 1368, 3.0: 106})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 7s - loss: 0.0519 - acc: 0.9854 - val_loss: 0.0374 - val_acc: 0.9894
Epoch 2/40
 - 7s - loss: 0.0334 - acc: 0.9899 - val_loss: 0.0365 - val_acc: 0.9899
Epoch 3/40
 - 7s - loss: 0.0316 - acc: 0.9904 - val_loss: 0.0364 - val_acc: 0.9895

==================================================================================================
	Training time : 0:01:11.745703
==================================================================================================
	Identification : 0.56
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 107
	100 : 1
	5 : 66
	25 : 15

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 176
	200 : 1
	100 : 1
	5 : 35
	25 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15258
	After : 2800

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2800 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 2800
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_293 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_294 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_293 (Embedding)       (None, 4, 417)       1167600     input_293[0][0]                  
__________________________________________________________________________________________________
embedding_294 (Embedding)       (None, 4, 46)        6992        input_294[0][0]                  
__________________________________________________________________________________________________
flatten_293 (Flatten)           (None, 1668)         0           embedding_293[0][0]              
__________________________________________________________________________________________________
flatten_294 (Flatten)           (None, 184)          0           embedding_294[0][0]              
__________________________________________________________________________________________________
concatenate_147 (Concatenate)   (None, 1852)         0           flatten_293[0][0]                
                                                                 flatten_294[0][0]                
__________________________________________________________________________________________________
dense_237 (Dense)               (None, 8)            14824       concatenate_147[0][0]            
==================================================================================================
Total params: 1,189,416
Trainable params: 1,189,416
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 700197
	7 Labels in train : Counter({0.0: 325933, 1.0: 267270, 2.0: 58663, 6.0: 18728, 5.0: 14907, 4.0: 13647, 3.0: 1049})
	7 Labels in valid : Counter({0.0: 32675, 1.0: 26790, 2.0: 5756, 6.0: 1815, 5.0: 1546, 4.0: 1335, 3.0: 103})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 630177 samples, validate on 70020 samples
Epoch 1/40
 - 7s - loss: 0.0520 - acc: 0.9852 - val_loss: 0.0382 - val_acc: 0.9891
Epoch 2/40
 - 7s - loss: 0.0333 - acc: 0.9899 - val_loss: 0.0379 - val_acc: 0.9893
Epoch 3/40
 - 7s - loss: 0.0315 - acc: 0.9905 - val_loss: 0.0367 - val_acc: 0.9900

==================================================================================================
	Training time : 0:01:11.718610
==================================================================================================
	Identification : 0.552
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 103
	100 : 1
	5 : 69
	25 : 15

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 179
	200 : 1
	100 : 1
	5 : 33
	25 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_295 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_296 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_295 (Embedding)       (None, 4, 417)       1164681     input_295[0][0]                  
__________________________________________________________________________________________________
embedding_296 (Embedding)       (None, 4, 46)        7774        input_296[0][0]                  
__________________________________________________________________________________________________
flatten_295 (Flatten)           (None, 1668)         0           embedding_295[0][0]              
__________________________________________________________________________________________________
flatten_296 (Flatten)           (None, 184)          0           embedding_296[0][0]              
__________________________________________________________________________________________________
concatenate_148 (Concatenate)   (None, 1852)         0           flatten_295[0][0]                
                                                                 flatten_296[0][0]                
__________________________________________________________________________________________________
dense_238 (Dense)               (None, 8)            14824       concatenate_148[0][0]            
==================================================================================================
Total params: 1,187,279
Trainable params: 1,187,279
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32686, 1.0: 26923, 2.0: 6046, 6.0: 3270, 5.0: 975, 4.0: 613})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 7s - loss: 0.0458 - acc: 0.9874 - val_loss: 0.0340 - val_acc: 0.9910
Epoch 2/40
 - 7s - loss: 0.0297 - acc: 0.9913 - val_loss: 0.0336 - val_acc: 0.9907
Epoch 3/40
 - 7s - loss: 0.0281 - acc: 0.9917 - val_loss: 0.0339 - val_acc: 0.9911

==================================================================================================
	Training time : 0:01:11.295264
==================================================================================================
	Identification : 0.492
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 105
	25 : 1
	5 : 33

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 148
	25 : 1
	5 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_297 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_298 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_297 (Embedding)       (None, 4, 417)       1164681     input_297[0][0]                  
__________________________________________________________________________________________________
embedding_298 (Embedding)       (None, 4, 46)        7774        input_298[0][0]                  
__________________________________________________________________________________________________
flatten_297 (Flatten)           (None, 1668)         0           embedding_297[0][0]              
__________________________________________________________________________________________________
flatten_298 (Flatten)           (None, 184)          0           embedding_298[0][0]              
__________________________________________________________________________________________________
concatenate_149 (Concatenate)   (None, 1852)         0           flatten_297[0][0]                
                                                                 flatten_298[0][0]                
__________________________________________________________________________________________________
dense_239 (Dense)               (None, 8)            14824       concatenate_149[0][0]            
==================================================================================================
Total params: 1,187,279
Trainable params: 1,187,279
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32642, 1.0: 26829, 2.0: 6066, 6.0: 3348, 5.0: 990, 4.0: 638})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 7s - loss: 0.0457 - acc: 0.9875 - val_loss: 0.0332 - val_acc: 0.9911
Epoch 2/40
 - 7s - loss: 0.0298 - acc: 0.9914 - val_loss: 0.0337 - val_acc: 0.9910
Epoch 3/40
 - 6s - loss: 0.0282 - acc: 0.9917 - val_loss: 0.0342 - val_acc: 0.9913

==================================================================================================
	Training time : 0:01:11.620280
==================================================================================================
	Identification : 0.451
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 99
	5 : 33

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 151
	25 : 1
	5 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 20014
	After : 2793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2793 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 2793
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_299 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_300 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_299 (Embedding)       (None, 4, 417)       1164681     input_299[0][0]                  
__________________________________________________________________________________________________
embedding_300 (Embedding)       (None, 4, 46)        7774        input_300[0][0]                  
__________________________________________________________________________________________________
flatten_299 (Flatten)           (None, 1668)         0           embedding_299[0][0]              
__________________________________________________________________________________________________
flatten_300 (Flatten)           (None, 184)          0           embedding_300[0][0]              
__________________________________________________________________________________________________
concatenate_150 (Concatenate)   (None, 1852)         0           flatten_299[0][0]                
                                                                 flatten_300[0][0]                
__________________________________________________________________________________________________
dense_240 (Dense)               (None, 8)            14824       concatenate_150[0][0]            
==================================================================================================
Total params: 1,187,279
Trainable params: 1,187,279
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 705130
	6 Labels in train : Counter({0.0: 328050, 1.0: 267783, 2.0: 60369, 6.0: 32896, 5.0: 9977, 4.0: 6055})
	6 Labels in valid : Counter({0.0: 32693, 1.0: 26838, 2.0: 6111, 6.0: 3207, 5.0: 1065, 4.0: 599})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 634617 samples, validate on 70513 samples
Epoch 1/40
 - 7s - loss: 0.0453 - acc: 0.9875 - val_loss: 0.0324 - val_acc: 0.9911
Epoch 2/40
 - 6s - loss: 0.0298 - acc: 0.9913 - val_loss: 0.0327 - val_acc: 0.9912
Epoch 3/40
 - 6s - loss: 0.0282 - acc: 0.9917 - val_loss: 0.0325 - val_acc: 0.9916

==================================================================================================
	Training time : 0:01:14.078511
==================================================================================================
	Identification : 0.472
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 102
	5 : 37

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 150
	25 : 1
	5 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_301 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_302 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_301 (Embedding)       (None, 4, 417)       1326477     input_301[0][0]                  
__________________________________________________________________________________________________
embedding_302 (Embedding)       (None, 4, 46)        5060        input_302[0][0]                  
__________________________________________________________________________________________________
flatten_301 (Flatten)           (None, 1668)         0           embedding_301[0][0]              
__________________________________________________________________________________________________
flatten_302 (Flatten)           (None, 184)          0           embedding_302[0][0]              
__________________________________________________________________________________________________
concatenate_151 (Concatenate)   (None, 1852)         0           flatten_301[0][0]                
                                                                 flatten_302[0][0]                
__________________________________________________________________________________________________
dense_241 (Dense)               (None, 8)            14824       concatenate_151[0][0]            
==================================================================================================
Total params: 1,346,361
Trainable params: 1,346,361
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33370, 1.0: 26874, 2.0: 6754, 5.0: 3658, 6.0: 2709})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 7s - loss: 0.0726 - acc: 0.9768 - val_loss: 0.0564 - val_acc: 0.9827
Epoch 2/40
 - 7s - loss: 0.0552 - acc: 0.9819 - val_loss: 0.0550 - val_acc: 0.9825
Epoch 3/40
 - 7s - loss: 0.0532 - acc: 0.9822 - val_loss: 0.0562 - val_acc: 0.9823

==================================================================================================
	Training time : 0:01:14.412354
==================================================================================================
	Identification : 0.392
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 143
	25 : 11
	50 : 4
	5 : 63

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 261
	25 : 13
	50 : 6
	5 : 53

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_303 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_304 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_303 (Embedding)       (None, 4, 417)       1326477     input_303[0][0]                  
__________________________________________________________________________________________________
embedding_304 (Embedding)       (None, 4, 46)        5060        input_304[0][0]                  
__________________________________________________________________________________________________
flatten_303 (Flatten)           (None, 1668)         0           embedding_303[0][0]              
__________________________________________________________________________________________________
flatten_304 (Flatten)           (None, 184)          0           embedding_304[0][0]              
__________________________________________________________________________________________________
concatenate_152 (Concatenate)   (None, 1852)         0           flatten_303[0][0]                
                                                                 flatten_304[0][0]                
__________________________________________________________________________________________________
dense_242 (Dense)               (None, 8)            14824       concatenate_152[0][0]            
==================================================================================================
Total params: 1,346,361
Trainable params: 1,346,361
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33607, 1.0: 26717, 2.0: 6781, 5.0: 3546, 6.0: 2714})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 7s - loss: 0.0727 - acc: 0.9768 - val_loss: 0.0610 - val_acc: 0.9807
Epoch 2/40
 - 7s - loss: 0.0550 - acc: 0.9820 - val_loss: 0.0596 - val_acc: 0.9818
Epoch 3/40
 - 7s - loss: 0.0531 - acc: 0.9823 - val_loss: 0.0594 - val_acc: 0.9817

==================================================================================================
	Training time : 0:01:15.362643
==================================================================================================
	Identification : 0.404
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 122
	25 : 9
	50 : 3
	5 : 58

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 278
	25 : 12
	50 : 6
	5 : 55

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 15500
	After : 3181

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3181 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 3181
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_305 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_306 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_305 (Embedding)       (None, 4, 417)       1326477     input_305[0][0]                  
__________________________________________________________________________________________________
embedding_306 (Embedding)       (None, 4, 46)        5060        input_306[0][0]                  
__________________________________________________________________________________________________
flatten_305 (Flatten)           (None, 1668)         0           embedding_305[0][0]              
__________________________________________________________________________________________________
flatten_306 (Flatten)           (None, 184)          0           embedding_306[0][0]              
__________________________________________________________________________________________________
concatenate_153 (Concatenate)   (None, 1852)         0           flatten_305[0][0]                
                                                                 flatten_306[0][0]                
__________________________________________________________________________________________________
dense_243 (Dense)               (None, 8)            14824       concatenate_153[0][0]            
==================================================================================================
Total params: 1,346,361
Trainable params: 1,346,361
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 733650
	5 Labels in train : Counter({0.0: 335379, 1.0: 267597, 2.0: 67782, 5.0: 35654, 6.0: 27238})
	5 Labels in valid : Counter({0.0: 33506, 1.0: 26881, 2.0: 6678, 5.0: 3602, 6.0: 2698})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 660285 samples, validate on 73365 samples
Epoch 1/40
 - 7s - loss: 0.0744 - acc: 0.9766 - val_loss: 0.0589 - val_acc: 0.9814
Epoch 2/40
 - 7s - loss: 0.0550 - acc: 0.9821 - val_loss: 0.0589 - val_acc: 0.9823
Epoch 3/40
 - 7s - loss: 0.0531 - acc: 0.9823 - val_loss: 0.0577 - val_acc: 0.9824

==================================================================================================
	Training time : 0:01:14.711558
==================================================================================================
	Identification : 0.415
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 128
	25 : 10
	50 : 3
	5 : 66

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 271
	25 : 13
	50 : 7
	5 : 54

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_307 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_308 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_307 (Embedding)       (None, 4, 417)       1476597     input_307[0][0]                  
__________________________________________________________________________________________________
embedding_308 (Embedding)       (None, 4, 46)        7728        input_308[0][0]                  
__________________________________________________________________________________________________
flatten_307 (Flatten)           (None, 1668)         0           embedding_307[0][0]              
__________________________________________________________________________________________________
flatten_308 (Flatten)           (None, 184)          0           embedding_308[0][0]              
__________________________________________________________________________________________________
concatenate_154 (Concatenate)   (None, 1852)         0           flatten_307[0][0]                
                                                                 flatten_308[0][0]                
__________________________________________________________________________________________________
dense_244 (Dense)               (None, 8)            14824       concatenate_154[0][0]            
==================================================================================================
Total params: 1,499,149
Trainable params: 1,499,149
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47142, 1.0: 39547, 2.0: 7727, 6.0: 2465, 5.0: 2053, 4.0: 1739, 3.0: 114})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 10s - loss: 0.0490 - acc: 0.9852 - val_loss: 0.0389 - val_acc: 0.9880
Epoch 2/40
 - 10s - loss: 0.0338 - acc: 0.9893 - val_loss: 0.0380 - val_acc: 0.9885
Epoch 3/40
 - 10s - loss: 0.0322 - acc: 0.9897 - val_loss: 0.0379 - val_acc: 0.9882

==================================================================================================
	Training time : 0:01:37.214579
==================================================================================================
	Identification : 0.434
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 84
	25 : 11
	50 : 4
	300 : 1
	5 : 49

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 271
	100 : 1
	5 : 40
	300 : 1
	50 : 2
	25 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_309 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_310 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_309 (Embedding)       (None, 4, 417)       1476597     input_309[0][0]                  
__________________________________________________________________________________________________
embedding_310 (Embedding)       (None, 4, 46)        7728        input_310[0][0]                  
__________________________________________________________________________________________________
flatten_309 (Flatten)           (None, 1668)         0           embedding_309[0][0]              
__________________________________________________________________________________________________
flatten_310 (Flatten)           (None, 184)          0           embedding_310[0][0]              
__________________________________________________________________________________________________
concatenate_155 (Concatenate)   (None, 1852)         0           flatten_309[0][0]                
                                                                 flatten_310[0][0]                
__________________________________________________________________________________________________
dense_245 (Dense)               (None, 8)            14824       concatenate_155[0][0]            
==================================================================================================
Total params: 1,499,149
Trainable params: 1,499,149
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47234, 1.0: 39414, 2.0: 7846, 6.0: 2448, 5.0: 2057, 4.0: 1675, 3.0: 113})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 9s - loss: 0.0486 - acc: 0.9852 - val_loss: 0.0363 - val_acc: 0.9891
Epoch 2/40
 - 9s - loss: 0.0339 - acc: 0.9892 - val_loss: 0.0369 - val_acc: 0.9885
Epoch 3/40
 - 9s - loss: 0.0323 - acc: 0.9897 - val_loss: 0.0366 - val_acc: 0.9890

==================================================================================================
	Training time : 0:01:38.078894
==================================================================================================
	Identification : 0.496
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 86
	100 : 1
	5 : 60
	300 : 1
	50 : 3
	25 : 18

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 269
	25 : 12
	50 : 5
	5 : 27

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 18410
	After : 3541

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3541 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 3541
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_311 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_312 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_311 (Embedding)       (None, 4, 417)       1476597     input_311[0][0]                  
__________________________________________________________________________________________________
embedding_312 (Embedding)       (None, 4, 46)        7728        input_312[0][0]                  
__________________________________________________________________________________________________
flatten_311 (Flatten)           (None, 1668)         0           embedding_311[0][0]              
__________________________________________________________________________________________________
flatten_312 (Flatten)           (None, 184)          0           embedding_312[0][0]              
__________________________________________________________________________________________________
concatenate_156 (Concatenate)   (None, 1852)         0           flatten_311[0][0]                
                                                                 flatten_312[0][0]                
__________________________________________________________________________________________________
dense_246 (Dense)               (None, 8)            14824       concatenate_156[0][0]            
==================================================================================================
Total params: 1,499,149
Trainable params: 1,499,149
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1007865
	7 Labels in train : Counter({0.0: 472451, 1.0: 395105, 2.0: 77346, 6.0: 24069, 5.0: 20853, 4.0: 16887, 3.0: 1154})
	7 Labels in valid : Counter({0.0: 47263, 1.0: 39408, 2.0: 7744, 6.0: 2418, 5.0: 2076, 4.0: 1763, 3.0: 115})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 907078 samples, validate on 100787 samples
Epoch 1/40
 - 9s - loss: 0.0497 - acc: 0.9853 - val_loss: 0.0370 - val_acc: 0.9887
Epoch 2/40
 - 10s - loss: 0.0338 - acc: 0.9891 - val_loss: 0.0371 - val_acc: 0.9894
Epoch 3/40
 - 10s - loss: 0.0322 - acc: 0.9896 - val_loss: 0.0377 - val_acc: 0.9894

==================================================================================================
	Training time : 0:01:41.359329
==================================================================================================
	Identification : 0.475
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 81
	25 : 20
	50 : 4
	100 : 1
	5 : 60

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 274
	25 : 11
	50 : 2
	300 : 1
	5 : 26

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_313 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_314 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_313 (Embedding)       (None, 4, 417)       1629219     input_313[0][0]                  
__________________________________________________________________________________________________
embedding_314 (Embedding)       (None, 4, 46)        10304       input_314[0][0]                  
__________________________________________________________________________________________________
flatten_313 (Flatten)           (None, 1668)         0           embedding_313[0][0]              
__________________________________________________________________________________________________
flatten_314 (Flatten)           (None, 184)          0           embedding_314[0][0]              
__________________________________________________________________________________________________
concatenate_157 (Concatenate)   (None, 1852)         0           flatten_313[0][0]                
                                                                 flatten_314[0][0]                
__________________________________________________________________________________________________
dense_247 (Dense)               (None, 8)            14824       concatenate_157[0][0]            
==================================================================================================
Total params: 1,654,347
Trainable params: 1,654,347
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 55845, 1.0: 46982, 2.0: 8678, 6.0: 4740, 5.0: 1461, 4.0: 820})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 11s - loss: 0.0419 - acc: 0.9878 - val_loss: 0.0326 - val_acc: 0.9909
Epoch 2/40
 - 11s - loss: 0.0299 - acc: 0.9908 - val_loss: 0.0335 - val_acc: 0.9901
Epoch 3/40
 - 11s - loss: 0.0286 - acc: 0.9912 - val_loss: 0.0331 - val_acc: 0.9907

==================================================================================================
	Training time : 0:01:55.156450
==================================================================================================
	Identification : 0.521
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 128
	25 : 5
	5 : 64

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 207
	25 : 5
	5 : 38

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_315 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_316 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_315 (Embedding)       (None, 4, 417)       1629219     input_315[0][0]                  
__________________________________________________________________________________________________
embedding_316 (Embedding)       (None, 4, 46)        10304       input_316[0][0]                  
__________________________________________________________________________________________________
flatten_315 (Flatten)           (None, 1668)         0           embedding_315[0][0]              
__________________________________________________________________________________________________
flatten_316 (Flatten)           (None, 184)          0           embedding_316[0][0]              
__________________________________________________________________________________________________
concatenate_158 (Concatenate)   (None, 1852)         0           flatten_315[0][0]                
                                                                 flatten_316[0][0]                
__________________________________________________________________________________________________
dense_248 (Dense)               (None, 8)            14824       concatenate_158[0][0]            
==================================================================================================
Total params: 1,654,347
Trainable params: 1,654,347
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 56182, 1.0: 46641, 2.0: 8678, 6.0: 4719, 5.0: 1470, 4.0: 836})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 12s - loss: 0.0411 - acc: 0.9879 - val_loss: 0.0332 - val_acc: 0.9902
Epoch 2/40
 - 12s - loss: 0.0298 - acc: 0.9909 - val_loss: 0.0349 - val_acc: 0.9901
Epoch 3/40
 - 12s - loss: 0.0285 - acc: 0.9912 - val_loss: 0.0333 - val_acc: 0.9905

==================================================================================================
	Training time : 0:01:59.433834
==================================================================================================
	Identification : 0.503
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 133
	25 : 6
	5 : 63

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 204
	25 : 4
	5 : 42

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 27332
	After : 3907

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3907 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 3907
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_317 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_318 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_317 (Embedding)       (None, 4, 417)       1629219     input_317[0][0]                  
__________________________________________________________________________________________________
embedding_318 (Embedding)       (None, 4, 46)        10304       input_318[0][0]                  
__________________________________________________________________________________________________
flatten_317 (Flatten)           (None, 1668)         0           embedding_317[0][0]              
__________________________________________________________________________________________________
flatten_318 (Flatten)           (None, 184)          0           embedding_318[0][0]              
__________________________________________________________________________________________________
concatenate_159 (Concatenate)   (None, 1852)         0           flatten_317[0][0]                
                                                                 flatten_318[0][0]                
__________________________________________________________________________________________________
dense_249 (Dense)               (None, 8)            14824       concatenate_159[0][0]            
==================================================================================================
Total params: 1,654,347
Trainable params: 1,654,347
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1185258
	6 Labels in train : Counter({0.0: 557071, 1.0: 469336, 2.0: 87939, 6.0: 47736, 5.0: 14777, 4.0: 8399})
	6 Labels in valid : Counter({0.0: 55528, 1.0: 47006, 2.0: 8890, 6.0: 4766, 5.0: 1476, 4.0: 860})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 1066732 samples, validate on 118526 samples
Epoch 1/40
 - 12s - loss: 0.0410 - acc: 0.9878 - val_loss: 0.0324 - val_acc: 0.9902
Epoch 2/40
 - 12s - loss: 0.0299 - acc: 0.9908 - val_loss: 0.0323 - val_acc: 0.9906
Epoch 3/40
 - 12s - loss: 0.0286 - acc: 0.9911 - val_loss: 0.0328 - val_acc: 0.9905

==================================================================================================
	Training time : 0:01:56.821544
==================================================================================================
	Identification : 0.524
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 152
	25 : 8
	5 : 74

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 183
	25 : 5
	5 : 33

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_319 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_320 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_319 (Embedding)       (None, 4, 417)       1531224     input_319[0][0]                  
__________________________________________________________________________________________________
embedding_320 (Embedding)       (None, 4, 46)        5474        input_320[0][0]                  
__________________________________________________________________________________________________
flatten_319 (Flatten)           (None, 1668)         0           embedding_319[0][0]              
__________________________________________________________________________________________________
flatten_320 (Flatten)           (None, 184)          0           embedding_320[0][0]              
__________________________________________________________________________________________________
concatenate_160 (Concatenate)   (None, 1852)         0           flatten_319[0][0]                
                                                                 flatten_320[0][0]                
__________________________________________________________________________________________________
dense_250 (Dense)               (None, 8)            14824       concatenate_160[0][0]            
==================================================================================================
Total params: 1,551,522
Trainable params: 1,551,522
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 41077, 1.0: 33178, 2.0: 7843, 5.0: 4304, 6.0: 3124})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 9s - loss: 0.0729 - acc: 0.9762 - val_loss: 0.0597 - val_acc: 0.9805
Epoch 2/40
 - 9s - loss: 0.0559 - acc: 0.9810 - val_loss: 0.0606 - val_acc: 0.9804
Epoch 3/40
 - 9s - loss: 0.0541 - acc: 0.9816 - val_loss: 0.0591 - val_acc: 0.9813

==================================================================================================
	Training time : 0:01:36.386733
==================================================================================================
	Identification : 0.36
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 71
	25 : 8
	50 : 3
	5 : 44

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 174
	25 : 16
	50 : 8
	5 : 41

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_321 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_322 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_321 (Embedding)       (None, 4, 417)       1531224     input_321[0][0]                  
__________________________________________________________________________________________________
embedding_322 (Embedding)       (None, 4, 46)        5474        input_322[0][0]                  
__________________________________________________________________________________________________
flatten_321 (Flatten)           (None, 1668)         0           embedding_321[0][0]              
__________________________________________________________________________________________________
flatten_322 (Flatten)           (None, 184)          0           embedding_322[0][0]              
__________________________________________________________________________________________________
concatenate_161 (Concatenate)   (None, 1852)         0           flatten_321[0][0]                
                                                                 flatten_322[0][0]                
__________________________________________________________________________________________________
dense_251 (Dense)               (None, 8)            14824       concatenate_161[0][0]            
==================================================================================================
Total params: 1,551,522
Trainable params: 1,551,522
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 41018, 1.0: 33279, 2.0: 7992, 5.0: 4136, 6.0: 3101})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 9s - loss: 0.0731 - acc: 0.9762 - val_loss: 0.0599 - val_acc: 0.9804
Epoch 2/40
 - 9s - loss: 0.0560 - acc: 0.9812 - val_loss: 0.0604 - val_acc: 0.9806
Epoch 3/40
 - 9s - loss: 0.0541 - acc: 0.9817 - val_loss: 0.0603 - val_acc: 0.9808

==================================================================================================
	Training time : 0:01:26.845402
==================================================================================================
	Identification : 0.402
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 72
	25 : 10
	50 : 8
	5 : 43

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 173
	25 : 14
	50 : 6
	5 : 38

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17167
	After : 3672

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3672 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 3672
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 417
	POS = 46 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_323 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_324 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_323 (Embedding)       (None, 4, 417)       1531224     input_323[0][0]                  
__________________________________________________________________________________________________
embedding_324 (Embedding)       (None, 4, 46)        5474        input_324[0][0]                  
__________________________________________________________________________________________________
flatten_323 (Flatten)           (None, 1668)         0           embedding_323[0][0]              
__________________________________________________________________________________________________
flatten_324 (Flatten)           (None, 184)          0           embedding_324[0][0]              
__________________________________________________________________________________________________
concatenate_162 (Concatenate)   (None, 1852)         0           flatten_323[0][0]                
                                                                 flatten_324[0][0]                
__________________________________________________________________________________________________
dense_252 (Dense)               (None, 8)            14824       concatenate_162[0][0]            
==================================================================================================
Total params: 1,551,522
Trainable params: 1,551,522
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 895260
	5 Labels in train : Counter({0.0: 410951, 1.0: 331861, 2.0: 79090, 5.0: 42225, 6.0: 31133})
	5 Labels in valid : Counter({0.0: 40955, 1.0: 33291, 2.0: 7962, 5.0: 4134, 6.0: 3184})
	Favorisation Coeff : 31

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.09
__________________________________________________________________________________________________
Train on 805734 samples, validate on 89526 samples
Epoch 1/40
 - 9s - loss: 0.0722 - acc: 0.9763 - val_loss: 0.0599 - val_acc: 0.9809
Epoch 2/40
 - 9s - loss: 0.0557 - acc: 0.9813 - val_loss: 0.0594 - val_acc: 0.9813
Epoch 3/40
 - 9s - loss: 0.0539 - acc: 0.9815 - val_loss: 0.0601 - val_acc: 0.9808

==================================================================================================
	Training time : 0:01:28.751207
==================================================================================================
	Identification : 0.333
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 68
	25 : 6
	50 : 4
	5 : 40

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 176
	25 : 17
	50 : 8
	5 : 44

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
14_False_False_224_114_compact_True_5_relu_0.336_False_512_relu_0.2_adagrad_0.012_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_325 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_326 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_325 (Embedding)       (None, 4, 224)       910560      input_325[0][0]                  
__________________________________________________________________________________________________
embedding_326 (Embedding)       (None, 4, 114)       17328       input_326[0][0]                  
__________________________________________________________________________________________________
flatten_325 (Flatten)           (None, 896)          0           embedding_325[0][0]              
__________________________________________________________________________________________________
flatten_326 (Flatten)           (None, 456)          0           embedding_326[0][0]              
__________________________________________________________________________________________________
concatenate_163 (Concatenate)   (None, 1352)         0           flatten_325[0][0]                
                                                                 flatten_326[0][0]                
__________________________________________________________________________________________________
dense_253 (Dense)               (None, 5)            6765        concatenate_163[0][0]            
__________________________________________________________________________________________________
dropout_91 (Dropout)            (None, 5)            0           dense_253[0][0]                  
__________________________________________________________________________________________________
dense_254 (Dense)               (None, 8)            48          dropout_91[0][0]                 
==================================================================================================
Total params: 934,701
Trainable params: 934,701
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27040, 1.0: 26558, 2.0: 391, 4.0: 183, 6.0: 101, 5.0: 84, 3.0: 4})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 6s - loss: 0.1878 - acc: 0.9277 - val_loss: 0.0537 - val_acc: 0.9817
Epoch 2/40
 - 6s - loss: 0.1280 - acc: 0.9633 - val_loss: 0.0522 - val_acc: 0.9820
Epoch 3/40
 - 6s - loss: 0.1184 - acc: 0.9634 - val_loss: 0.0516 - val_acc: 0.9821

==================================================================================================
	Training time : 0:01:06.960768
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 389
	25 : 7
	50 : 3
	100 : 1
	5 : 61

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_327 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_328 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_327 (Embedding)       (None, 4, 224)       910560      input_327[0][0]                  
__________________________________________________________________________________________________
embedding_328 (Embedding)       (None, 4, 114)       17328       input_328[0][0]                  
__________________________________________________________________________________________________
flatten_327 (Flatten)           (None, 896)          0           embedding_327[0][0]              
__________________________________________________________________________________________________
flatten_328 (Flatten)           (None, 456)          0           embedding_328[0][0]              
__________________________________________________________________________________________________
concatenate_164 (Concatenate)   (None, 1352)         0           flatten_327[0][0]                
                                                                 flatten_328[0][0]                
__________________________________________________________________________________________________
dense_255 (Dense)               (None, 5)            6765        concatenate_164[0][0]            
__________________________________________________________________________________________________
dropout_92 (Dropout)            (None, 5)            0           dense_255[0][0]                  
__________________________________________________________________________________________________
dense_256 (Dense)               (None, 8)            48          dropout_92[0][0]                 
==================================================================================================
Total params: 934,701
Trainable params: 934,701
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27133, 1.0: 26493, 2.0: 386, 4.0: 182, 6.0: 95, 5.0: 68, 3.0: 4})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 6s - loss: 0.2105 - acc: 0.8993 - val_loss: 0.0452 - val_acc: 0.9825
Epoch 2/40
 - 6s - loss: 0.1338 - acc: 0.9626 - val_loss: 0.0457 - val_acc: 0.9823
Epoch 3/40
 - 6s - loss: 0.1205 - acc: 0.9636 - val_loss: 0.0459 - val_acc: 0.9820

==================================================================================================
	Training time : 0:01:06.819021
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 389
	25 : 7
	50 : 3
	100 : 1
	5 : 61

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_329 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_330 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_329 (Embedding)       (None, 4, 224)       910560      input_329[0][0]                  
__________________________________________________________________________________________________
embedding_330 (Embedding)       (None, 4, 114)       17328       input_330[0][0]                  
__________________________________________________________________________________________________
flatten_329 (Flatten)           (None, 896)          0           embedding_329[0][0]              
__________________________________________________________________________________________________
flatten_330 (Flatten)           (None, 456)          0           embedding_330[0][0]              
__________________________________________________________________________________________________
concatenate_165 (Concatenate)   (None, 1352)         0           flatten_329[0][0]                
                                                                 flatten_330[0][0]                
__________________________________________________________________________________________________
dense_257 (Dense)               (None, 5)            6765        concatenate_165[0][0]            
__________________________________________________________________________________________________
dropout_93 (Dropout)            (None, 5)            0           dense_257[0][0]                  
__________________________________________________________________________________________________
dense_258 (Dense)               (None, 8)            48          dropout_93[0][0]                 
==================================================================================================
Total params: 934,701
Trainable params: 934,701
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26956, 1.0: 26681, 2.0: 383, 4.0: 156, 6.0: 108, 5.0: 75, 3.0: 2})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 6s - loss: 0.3490 - acc: 0.9251 - val_loss: 0.0434 - val_acc: 0.9893
Epoch 2/40
 - 6s - loss: 0.2403 - acc: 0.9324 - val_loss: 0.0409 - val_acc: 0.9893
Epoch 3/40
 - 6s - loss: 0.2126 - acc: 0.9336 - val_loss: 0.0405 - val_acc: 0.9895

==================================================================================================
	Training time : 0:01:09.403518
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 389
	25 : 7
	50 : 3
	100 : 1
	5 : 61

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_331 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_332 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_331 (Embedding)       (None, 4, 224)       905184      input_331[0][0]                  
__________________________________________________________________________________________________
embedding_332 (Embedding)       (None, 4, 114)       19266       input_332[0][0]                  
__________________________________________________________________________________________________
flatten_331 (Flatten)           (None, 896)          0           embedding_331[0][0]              
__________________________________________________________________________________________________
flatten_332 (Flatten)           (None, 456)          0           embedding_332[0][0]              
__________________________________________________________________________________________________
concatenate_166 (Concatenate)   (None, 1352)         0           flatten_331[0][0]                
                                                                 flatten_332[0][0]                
__________________________________________________________________________________________________
dense_259 (Dense)               (None, 5)            6765        concatenate_166[0][0]            
__________________________________________________________________________________________________
dropout_94 (Dropout)            (None, 5)            0           dense_259[0][0]                  
__________________________________________________________________________________________________
dense_260 (Dense)               (None, 8)            48          dropout_94[0][0]                 
==================================================================================================
Total params: 931,263
Trainable params: 931,263
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27113, 1.0: 26599, 2.0: 275, 6.0: 185, 5.0: 46, 4.0: 43})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.1813 - acc: 0.9270 - val_loss: 0.0338 - val_acc: 0.9894
Epoch 2/40
 - 6s - loss: 0.1114 - acc: 0.9683 - val_loss: 0.0332 - val_acc: 0.9901
Epoch 3/40
 - 6s - loss: 0.1008 - acc: 0.9688 - val_loss: 0.0331 - val_acc: 0.9901

==================================================================================================
	Training time : 0:01:06.942169
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 321
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_333 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_334 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_333 (Embedding)       (None, 4, 224)       905184      input_333[0][0]                  
__________________________________________________________________________________________________
embedding_334 (Embedding)       (None, 4, 114)       19266       input_334[0][0]                  
__________________________________________________________________________________________________
flatten_333 (Flatten)           (None, 896)          0           embedding_333[0][0]              
__________________________________________________________________________________________________
flatten_334 (Flatten)           (None, 456)          0           embedding_334[0][0]              
__________________________________________________________________________________________________
concatenate_167 (Concatenate)   (None, 1352)         0           flatten_333[0][0]                
                                                                 flatten_334[0][0]                
__________________________________________________________________________________________________
dense_261 (Dense)               (None, 5)            6765        concatenate_167[0][0]            
__________________________________________________________________________________________________
dropout_95 (Dropout)            (None, 5)            0           dense_261[0][0]                  
__________________________________________________________________________________________________
dense_262 (Dense)               (None, 8)            48          dropout_95[0][0]                 
==================================================================================================
Total params: 931,263
Trainable params: 931,263
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 26911, 1.0: 26773, 2.0: 316, 6.0: 178, 5.0: 43, 4.0: 40})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.1995 - acc: 0.9469 - val_loss: 0.0457 - val_acc: 0.9852
Epoch 2/40
 - 6s - loss: 0.1259 - acc: 0.9673 - val_loss: 0.0464 - val_acc: 0.9855
Epoch 3/40
 - 6s - loss: 0.1136 - acc: 0.9679 - val_loss: 0.0467 - val_acc: 0.9859

==================================================================================================
	Training time : 0:01:07.931706
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 321
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_335 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_336 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_335 (Embedding)       (None, 4, 224)       905184      input_335[0][0]                  
__________________________________________________________________________________________________
embedding_336 (Embedding)       (None, 4, 114)       19266       input_336[0][0]                  
__________________________________________________________________________________________________
flatten_335 (Flatten)           (None, 896)          0           embedding_335[0][0]              
__________________________________________________________________________________________________
flatten_336 (Flatten)           (None, 456)          0           embedding_336[0][0]              
__________________________________________________________________________________________________
concatenate_168 (Concatenate)   (None, 1352)         0           flatten_335[0][0]                
                                                                 flatten_336[0][0]                
__________________________________________________________________________________________________
dense_263 (Dense)               (None, 5)            6765        concatenate_168[0][0]            
__________________________________________________________________________________________________
dropout_96 (Dropout)            (None, 5)            0           dense_263[0][0]                  
__________________________________________________________________________________________________
dense_264 (Dense)               (None, 8)            48          dropout_96[0][0]                 
==================================================================================================
Total params: 931,263
Trainable params: 931,263
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({1.0: 26869, 0.0: 26851, 2.0: 288, 6.0: 173, 5.0: 45, 4.0: 35})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.2115 - acc: 0.9295 - val_loss: 0.0334 - val_acc: 0.9906
Epoch 2/40
 - 6s - loss: 0.1536 - acc: 0.9340 - val_loss: 0.0325 - val_acc: 0.9910
Epoch 3/40
 - 6s - loss: 0.1391 - acc: 0.9344 - val_loss: 0.0329 - val_acc: 0.9912

==================================================================================================
	Training time : 0:01:07.253317
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 321
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_337 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_338 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_337 (Embedding)       (None, 4, 224)       1645056     input_337[0][0]                  
__________________________________________________________________________________________________
embedding_338 (Embedding)       (None, 4, 114)       12540       input_338[0][0]                  
__________________________________________________________________________________________________
flatten_337 (Flatten)           (None, 896)          0           embedding_337[0][0]              
__________________________________________________________________________________________________
flatten_338 (Flatten)           (None, 456)          0           embedding_338[0][0]              
__________________________________________________________________________________________________
concatenate_169 (Concatenate)   (None, 1352)         0           flatten_337[0][0]                
                                                                 flatten_338[0][0]                
__________________________________________________________________________________________________
dense_265 (Dense)               (None, 5)            6765        concatenate_169[0][0]            
__________________________________________________________________________________________________
dropout_97 (Dropout)            (None, 5)            0           dense_265[0][0]                  
__________________________________________________________________________________________________
dense_266 (Dense)               (None, 8)            48          dropout_97[0][0]                 
==================================================================================================
Total params: 1,664,409
Trainable params: 1,664,409
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27014, 1.0: 26406, 2.0: 526, 6.0: 272, 5.0: 268})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.2567 - acc: 0.9651 - val_loss: 0.0776 - val_acc: 0.9742
Epoch 2/40
 - 6s - loss: 0.1410 - acc: 0.9693 - val_loss: 0.0734 - val_acc: 0.9747
Epoch 3/40
 - 6s - loss: 0.1257 - acc: 0.9699 - val_loss: 0.0725 - val_acc: 0.9746

==================================================================================================
	Training time : 0:01:10.686496
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 698
	25 : 2
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_339 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_340 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_339 (Embedding)       (None, 4, 224)       1645056     input_339[0][0]                  
__________________________________________________________________________________________________
embedding_340 (Embedding)       (None, 4, 114)       12540       input_340[0][0]                  
__________________________________________________________________________________________________
flatten_339 (Flatten)           (None, 896)          0           embedding_339[0][0]              
__________________________________________________________________________________________________
flatten_340 (Flatten)           (None, 456)          0           embedding_340[0][0]              
__________________________________________________________________________________________________
concatenate_170 (Concatenate)   (None, 1352)         0           flatten_339[0][0]                
                                                                 flatten_340[0][0]                
__________________________________________________________________________________________________
dense_267 (Dense)               (None, 5)            6765        concatenate_170[0][0]            
__________________________________________________________________________________________________
dropout_98 (Dropout)            (None, 5)            0           dense_267[0][0]                  
__________________________________________________________________________________________________
dense_268 (Dense)               (None, 8)            48          dropout_98[0][0]                 
==================================================================================================
Total params: 1,664,409
Trainable params: 1,664,409
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26985, 1.0: 26519, 2.0: 481, 5.0: 258, 6.0: 243})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 7s - loss: 0.3563 - acc: 0.9160 - val_loss: 0.0779 - val_acc: 0.9748
Epoch 2/40
 - 7s - loss: 0.2637 - acc: 0.9201 - val_loss: 0.0741 - val_acc: 0.9754
Epoch 3/40
 - 7s - loss: 0.2406 - acc: 0.9214 - val_loss: 0.0744 - val_acc: 0.9755

==================================================================================================
	Training time : 0:01:10.840981
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 698
	25 : 2
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_341 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_342 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_341 (Embedding)       (None, 4, 224)       1645056     input_341[0][0]                  
__________________________________________________________________________________________________
embedding_342 (Embedding)       (None, 4, 114)       12540       input_342[0][0]                  
__________________________________________________________________________________________________
flatten_341 (Flatten)           (None, 896)          0           embedding_341[0][0]              
__________________________________________________________________________________________________
flatten_342 (Flatten)           (None, 456)          0           embedding_342[0][0]              
__________________________________________________________________________________________________
concatenate_171 (Concatenate)   (None, 1352)         0           flatten_341[0][0]                
                                                                 flatten_342[0][0]                
__________________________________________________________________________________________________
dense_269 (Dense)               (None, 5)            6765        concatenate_171[0][0]            
__________________________________________________________________________________________________
dropout_99 (Dropout)            (None, 5)            0           dense_269[0][0]                  
__________________________________________________________________________________________________
dense_270 (Dense)               (None, 8)            48          dropout_99[0][0]                 
==================================================================================================
Total params: 1,664,409
Trainable params: 1,664,409
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26806, 1.0: 26635, 2.0: 554, 6.0: 249, 5.0: 242})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.3176 - acc: 0.8903 - val_loss: 0.0587 - val_acc: 0.9756
Epoch 2/40
 - 6s - loss: 0.2424 - acc: 0.9217 - val_loss: 0.0538 - val_acc: 0.9821
Epoch 3/40
 - 6s - loss: 0.2030 - acc: 0.9368 - val_loss: 0.0500 - val_acc: 0.9839

==================================================================================================
	Training time : 0:01:08.494344
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 698
	25 : 2
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_343 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_344 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_343 (Embedding)       (None, 4, 224)       1191904     input_343[0][0]                  
__________________________________________________________________________________________________
embedding_344 (Embedding)       (None, 4, 114)       19152       input_344[0][0]                  
__________________________________________________________________________________________________
flatten_343 (Flatten)           (None, 896)          0           embedding_343[0][0]              
__________________________________________________________________________________________________
flatten_344 (Flatten)           (None, 456)          0           embedding_344[0][0]              
__________________________________________________________________________________________________
concatenate_172 (Concatenate)   (None, 1352)         0           flatten_343[0][0]                
                                                                 flatten_344[0][0]                
__________________________________________________________________________________________________
dense_271 (Dense)               (None, 5)            6765        concatenate_172[0][0]            
__________________________________________________________________________________________________
dropout_100 (Dropout)           (None, 5)            0           dense_271[0][0]                  
__________________________________________________________________________________________________
dense_272 (Dense)               (None, 8)            48          dropout_100[0][0]                
==================================================================================================
Total params: 1,217,869
Trainable params: 1,217,869
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39686, 1.0: 39557, 2.0: 581, 4.0: 292, 6.0: 151, 5.0: 99, 3.0: 5})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 9s - loss: 0.1743 - acc: 0.9572 - val_loss: 0.0543 - val_acc: 0.9819
Epoch 2/40
 - 9s - loss: 0.1305 - acc: 0.9629 - val_loss: 0.0555 - val_acc: 0.9821
Epoch 3/40
 - 9s - loss: 0.1210 - acc: 0.9636 - val_loss: 0.0564 - val_acc: 0.9821

==================================================================================================
	Training time : 0:01:32.308612
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 450
	200 : 1
	50 : 5
	5 : 64
	25 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_345 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_346 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_345 (Embedding)       (None, 4, 224)       1191904     input_345[0][0]                  
__________________________________________________________________________________________________
embedding_346 (Embedding)       (None, 4, 114)       19152       input_346[0][0]                  
__________________________________________________________________________________________________
flatten_345 (Flatten)           (None, 896)          0           embedding_345[0][0]              
__________________________________________________________________________________________________
flatten_346 (Flatten)           (None, 456)          0           embedding_346[0][0]              
__________________________________________________________________________________________________
concatenate_173 (Concatenate)   (None, 1352)         0           flatten_345[0][0]                
                                                                 flatten_346[0][0]                
__________________________________________________________________________________________________
dense_273 (Dense)               (None, 5)            6765        concatenate_173[0][0]            
__________________________________________________________________________________________________
dropout_101 (Dropout)           (None, 5)            0           dense_273[0][0]                  
__________________________________________________________________________________________________
dense_274 (Dense)               (None, 8)            48          dropout_101[0][0]                
==================================================================================================
Total params: 1,217,869
Trainable params: 1,217,869
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39989, 1.0: 39240, 2.0: 621, 4.0: 268, 6.0: 143, 5.0: 100, 3.0: 10})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 9s - loss: 0.1746 - acc: 0.9134 - val_loss: 0.0393 - val_acc: 0.9866
Epoch 2/40
 - 9s - loss: 0.1193 - acc: 0.9645 - val_loss: 0.0385 - val_acc: 0.9876
Epoch 3/40
 - 9s - loss: 0.1096 - acc: 0.9657 - val_loss: 0.0376 - val_acc: 0.9886

==================================================================================================
	Training time : 0:01:31.804478
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 450
	200 : 1
	50 : 5
	5 : 64
	25 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_347 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_348 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_347 (Embedding)       (None, 4, 224)       1191904     input_347[0][0]                  
__________________________________________________________________________________________________
embedding_348 (Embedding)       (None, 4, 114)       19152       input_348[0][0]                  
__________________________________________________________________________________________________
flatten_347 (Flatten)           (None, 896)          0           embedding_347[0][0]              
__________________________________________________________________________________________________
flatten_348 (Flatten)           (None, 456)          0           embedding_348[0][0]              
__________________________________________________________________________________________________
concatenate_174 (Concatenate)   (None, 1352)         0           flatten_347[0][0]                
                                                                 flatten_348[0][0]                
__________________________________________________________________________________________________
dense_275 (Dense)               (None, 5)            6765        concatenate_174[0][0]            
__________________________________________________________________________________________________
dropout_102 (Dropout)           (None, 5)            0           dense_275[0][0]                  
__________________________________________________________________________________________________
dense_276 (Dense)               (None, 8)            48          dropout_102[0][0]                
==================================================================================================
Total params: 1,217,869
Trainable params: 1,217,869
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40057, 1.0: 39215, 2.0: 581, 4.0: 258, 6.0: 154, 5.0: 95, 3.0: 11})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 9s - loss: 0.2426 - acc: 0.8723 - val_loss: 0.0371 - val_acc: 0.9890
Epoch 2/40
 - 9s - loss: 0.1564 - acc: 0.8953 - val_loss: 0.0365 - val_acc: 0.9894
Epoch 3/40
 - 9s - loss: 0.1456 - acc: 0.8962 - val_loss: 0.0365 - val_acc: 0.9893

==================================================================================================
	Training time : 0:01:34.227432
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 450
	200 : 1
	50 : 5
	5 : 64
	25 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_349 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_350 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_349 (Embedding)       (None, 4, 224)       1338848     input_349[0][0]                  
__________________________________________________________________________________________________
embedding_350 (Embedding)       (None, 4, 114)       25536       input_350[0][0]                  
__________________________________________________________________________________________________
flatten_349 (Flatten)           (None, 896)          0           embedding_349[0][0]              
__________________________________________________________________________________________________
flatten_350 (Flatten)           (None, 456)          0           embedding_350[0][0]              
__________________________________________________________________________________________________
concatenate_175 (Concatenate)   (None, 1352)         0           flatten_349[0][0]                
                                                                 flatten_350[0][0]                
__________________________________________________________________________________________________
dense_277 (Dense)               (None, 5)            6765        concatenate_175[0][0]            
__________________________________________________________________________________________________
dropout_103 (Dropout)           (None, 5)            0           dense_277[0][0]                  
__________________________________________________________________________________________________
dense_278 (Dense)               (None, 8)            48          dropout_103[0][0]                
==================================================================================================
Total params: 1,371,197
Trainable params: 1,371,197
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47256, 1.0: 46887, 2.0: 540, 6.0: 293, 5.0: 83, 4.0: 64})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.3194 - acc: 0.8495 - val_loss: 0.0537 - val_acc: 0.9850
Epoch 2/40
 - 11s - loss: 0.1529 - acc: 0.9307 - val_loss: 0.0351 - val_acc: 0.9894
Epoch 3/40
 - 11s - loss: 0.1403 - acc: 0.9214 - val_loss: 0.0344 - val_acc: 0.9901

==================================================================================================
	Training time : 0:01:47.023928
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 463
	25 : 1
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_351 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_352 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_351 (Embedding)       (None, 4, 224)       1338848     input_351[0][0]                  
__________________________________________________________________________________________________
embedding_352 (Embedding)       (None, 4, 114)       25536       input_352[0][0]                  
__________________________________________________________________________________________________
flatten_351 (Flatten)           (None, 896)          0           embedding_351[0][0]              
__________________________________________________________________________________________________
flatten_352 (Flatten)           (None, 456)          0           embedding_352[0][0]              
__________________________________________________________________________________________________
concatenate_176 (Concatenate)   (None, 1352)         0           flatten_351[0][0]                
                                                                 flatten_352[0][0]                
__________________________________________________________________________________________________
dense_279 (Dense)               (None, 5)            6765        concatenate_176[0][0]            
__________________________________________________________________________________________________
dropout_104 (Dropout)           (None, 5)            0           dense_279[0][0]                  
__________________________________________________________________________________________________
dense_280 (Dense)               (None, 8)            48          dropout_104[0][0]                
==================================================================================================
Total params: 1,371,197
Trainable params: 1,371,197
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47202, 1.0: 47031, 2.0: 508, 6.0: 238, 5.0: 82, 4.0: 62})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.2887 - acc: 0.9067 - val_loss: 0.0368 - val_acc: 0.9869
Epoch 2/40
 - 11s - loss: 0.1533 - acc: 0.9638 - val_loss: 0.0339 - val_acc: 0.9868
Epoch 3/40
 - 11s - loss: 0.1023 - acc: 0.9798 - val_loss: 0.0323 - val_acc: 0.9897

==================================================================================================
	Training time : 0:01:47.026928
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 463
	25 : 1
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_353 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_354 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_353 (Embedding)       (None, 4, 224)       1338848     input_353[0][0]                  
__________________________________________________________________________________________________
embedding_354 (Embedding)       (None, 4, 114)       25536       input_354[0][0]                  
__________________________________________________________________________________________________
flatten_353 (Flatten)           (None, 896)          0           embedding_353[0][0]              
__________________________________________________________________________________________________
flatten_354 (Flatten)           (None, 456)          0           embedding_354[0][0]              
__________________________________________________________________________________________________
concatenate_177 (Concatenate)   (None, 1352)         0           flatten_353[0][0]                
                                                                 flatten_354[0][0]                
__________________________________________________________________________________________________
dense_281 (Dense)               (None, 5)            6765        concatenate_177[0][0]            
__________________________________________________________________________________________________
dropout_105 (Dropout)           (None, 5)            0           dense_281[0][0]                  
__________________________________________________________________________________________________
dense_282 (Dense)               (None, 8)            48          dropout_105[0][0]                
==================================================================================================
Total params: 1,371,197
Trainable params: 1,371,197
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47390, 1.0: 46754, 2.0: 540, 6.0: 266, 5.0: 91, 4.0: 82})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.1996 - acc: 0.9194 - val_loss: 0.0316 - val_acc: 0.9904
Epoch 2/40
 - 11s - loss: 0.1423 - acc: 0.8986 - val_loss: 0.0314 - val_acc: 0.9909
Epoch 3/40
 - 11s - loss: 0.1326 - acc: 0.8989 - val_loss: 0.0315 - val_acc: 0.9911

==================================================================================================
	Training time : 0:01:47.577522
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 463
	25 : 1
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_355 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_356 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_355 (Embedding)       (None, 4, 224)       1952832     input_355[0][0]                  
__________________________________________________________________________________________________
embedding_356 (Embedding)       (None, 4, 114)       13566       input_356[0][0]                  
__________________________________________________________________________________________________
flatten_355 (Flatten)           (None, 896)          0           embedding_355[0][0]              
__________________________________________________________________________________________________
flatten_356 (Flatten)           (None, 456)          0           embedding_356[0][0]              
__________________________________________________________________________________________________
concatenate_178 (Concatenate)   (None, 1352)         0           flatten_355[0][0]                
                                                                 flatten_356[0][0]                
__________________________________________________________________________________________________
dense_283 (Dense)               (None, 5)            6765        concatenate_178[0][0]            
__________________________________________________________________________________________________
dropout_106 (Dropout)           (None, 5)            0           dense_283[0][0]                  
__________________________________________________________________________________________________
dense_284 (Dense)               (None, 8)            48          dropout_106[0][0]                
==================================================================================================
Total params: 1,973,211
Trainable params: 1,973,211
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33367, 1.0: 32972, 2.0: 674, 5.0: 300, 6.0: 273})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 8s - loss: 0.2192 - acc: 0.8816 - val_loss: 0.0667 - val_acc: 0.9742
Epoch 2/40
 - 8s - loss: 0.1775 - acc: 0.8849 - val_loss: 0.0631 - val_acc: 0.9748
Epoch 3/40
 - 8s - loss: 0.1679 - acc: 0.8858 - val_loss: 0.0623 - val_acc: 0.9752

==================================================================================================
	Training time : 0:01:27.098418
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 436
	25 : 3
	5 : 25

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_357 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_358 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_357 (Embedding)       (None, 4, 224)       1952832     input_357[0][0]                  
__________________________________________________________________________________________________
embedding_358 (Embedding)       (None, 4, 114)       13566       input_358[0][0]                  
__________________________________________________________________________________________________
flatten_357 (Flatten)           (None, 896)          0           embedding_357[0][0]              
__________________________________________________________________________________________________
flatten_358 (Flatten)           (None, 456)          0           embedding_358[0][0]              
__________________________________________________________________________________________________
concatenate_179 (Concatenate)   (None, 1352)         0           flatten_357[0][0]                
                                                                 flatten_358[0][0]                
__________________________________________________________________________________________________
dense_285 (Dense)               (None, 5)            6765        concatenate_179[0][0]            
__________________________________________________________________________________________________
dropout_107 (Dropout)           (None, 5)            0           dense_285[0][0]                  
__________________________________________________________________________________________________
dense_286 (Dense)               (None, 8)            48          dropout_107[0][0]                
==================================================================================================
Total params: 1,973,211
Trainable params: 1,973,211
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33256, 1.0: 33037, 2.0: 648, 5.0: 342, 6.0: 303})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 8s - loss: 0.3711 - acc: 0.9053 - val_loss: 0.0551 - val_acc: 0.9823
Epoch 2/40
 - 8s - loss: 0.2489 - acc: 0.9257 - val_loss: 0.0498 - val_acc: 0.9834
Epoch 3/40
 - 8s - loss: 0.2251 - acc: 0.9261 - val_loss: 0.0499 - val_acc: 0.9837

==================================================================================================
	Training time : 0:01:21.040001
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 436
	25 : 3
	5 : 25

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 224
	POS = 114 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_359 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_360 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_359 (Embedding)       (None, 4, 224)       1952832     input_359[0][0]                  
__________________________________________________________________________________________________
embedding_360 (Embedding)       (None, 4, 114)       13566       input_360[0][0]                  
__________________________________________________________________________________________________
flatten_359 (Flatten)           (None, 896)          0           embedding_359[0][0]              
__________________________________________________________________________________________________
flatten_360 (Flatten)           (None, 456)          0           embedding_360[0][0]              
__________________________________________________________________________________________________
concatenate_180 (Concatenate)   (None, 1352)         0           flatten_359[0][0]                
                                                                 flatten_360[0][0]                
__________________________________________________________________________________________________
dense_287 (Dense)               (None, 5)            6765        concatenate_180[0][0]            
__________________________________________________________________________________________________
dropout_108 (Dropout)           (None, 5)            0           dense_287[0][0]                  
__________________________________________________________________________________________________
dense_288 (Dense)               (None, 8)            48          dropout_108[0][0]                
==================================================================================================
Total params: 1,973,211
Trainable params: 1,973,211
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33691, 1.0: 32623, 2.0: 653, 6.0: 311, 5.0: 308})
	Favorisation Coeff : 14

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.012
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 8s - loss: 0.2279 - acc: 0.9055 - val_loss: 0.0724 - val_acc: 0.9745
Epoch 2/40
 - 8s - loss: 0.1845 - acc: 0.8836 - val_loss: 0.0712 - val_acc: 0.9748
Epoch 3/40
 - 8s - loss: 0.1755 - acc: 0.8831 - val_loss: 0.0702 - val_acc: 0.9751

==================================================================================================
	Training time : 0:01:20.903890
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 436
	25 : 3
	5 : 25

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
17_False_True_239_115_frequent_False_22_relu_0.353_False_512_relu_0.2_adagrad_0.068_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12174

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12174 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12174
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_361 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_362 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_361 (Embedding)       (None, 4, 239)       2909586     input_361[0][0]                  
__________________________________________________________________________________________________
embedding_362 (Embedding)       (None, 4, 115)       17480       input_362[0][0]                  
__________________________________________________________________________________________________
flatten_361 (Flatten)           (None, 956)          0           embedding_361[0][0]              
__________________________________________________________________________________________________
flatten_362 (Flatten)           (None, 460)          0           embedding_362[0][0]              
__________________________________________________________________________________________________
concatenate_181 (Concatenate)   (None, 1416)         0           flatten_361[0][0]                
                                                                 flatten_362[0][0]                
__________________________________________________________________________________________________
dense_289 (Dense)               (None, 8)            11336       concatenate_181[0][0]            
==================================================================================================
Total params: 2,938,402
Trainable params: 2,938,402
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27106, 1.0: 26494, 2.0: 407, 4.0: 181, 6.0: 103, 5.0: 67, 3.0: 3})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 6s - loss: 0.0329 - acc: 0.9868 - val_loss: 0.0264 - val_acc: 0.9879
Epoch 2/40
 - 6s - loss: 0.0211 - acc: 0.9893 - val_loss: 0.0273 - val_acc: 0.9876
Epoch 3/40
 - 6s - loss: 0.0188 - acc: 0.9900 - val_loss: 0.0284 - val_acc: 0.9864

==================================================================================================
	Training time : 0:01:06.345511
==================================================================================================
	Identification : 0.41
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 33
	200 : 1
	100 : 1
	5 : 34
	25 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 250
	100 : 1
	5 : 62
	25 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12139

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12139 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12139
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_363 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_364 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_363 (Embedding)       (None, 4, 239)       2901221     input_363[0][0]                  
__________________________________________________________________________________________________
embedding_364 (Embedding)       (None, 4, 115)       17480       input_364[0][0]                  
__________________________________________________________________________________________________
flatten_363 (Flatten)           (None, 956)          0           embedding_363[0][0]              
__________________________________________________________________________________________________
flatten_364 (Flatten)           (None, 460)          0           embedding_364[0][0]              
__________________________________________________________________________________________________
concatenate_182 (Concatenate)   (None, 1416)         0           flatten_363[0][0]                
                                                                 flatten_364[0][0]                
__________________________________________________________________________________________________
dense_290 (Dense)               (None, 8)            11336       concatenate_182[0][0]            
==================================================================================================
Total params: 2,930,037
Trainable params: 2,930,037
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27001, 1.0: 26618, 2.0: 384, 4.0: 160, 6.0: 112, 5.0: 82, 3.0: 4})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 6s - loss: 0.0326 - acc: 0.9868 - val_loss: 0.0268 - val_acc: 0.9879
Epoch 2/40
 - 6s - loss: 0.0212 - acc: 0.9893 - val_loss: 0.0289 - val_acc: 0.9871
Epoch 3/40
 - 6s - loss: 0.0190 - acc: 0.9899 - val_loss: 0.0285 - val_acc: 0.9870

==================================================================================================
	Training time : 0:01:05.686891
==================================================================================================
	Identification : 0.415
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 34
	5 : 38
	25 : 10

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 246
	200 : 1
	100 : 1
	5 : 61
	25 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15258
	After : 12111

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12111 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 12111
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_365 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_366 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_365 (Embedding)       (None, 4, 239)       2894529     input_365[0][0]                  
__________________________________________________________________________________________________
embedding_366 (Embedding)       (None, 4, 115)       17480       input_366[0][0]                  
__________________________________________________________________________________________________
flatten_365 (Flatten)           (None, 956)          0           embedding_365[0][0]              
__________________________________________________________________________________________________
flatten_366 (Flatten)           (None, 460)          0           embedding_366[0][0]              
__________________________________________________________________________________________________
concatenate_183 (Concatenate)   (None, 1416)         0           flatten_365[0][0]                
                                                                 flatten_366[0][0]                
__________________________________________________________________________________________________
dense_291 (Dense)               (None, 8)            11336       concatenate_183[0][0]            
==================================================================================================
Total params: 2,923,345
Trainable params: 2,923,345
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26846, 1.0: 26765, 2.0: 387, 4.0: 173, 6.0: 115, 5.0: 68, 3.0: 7})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 6s - loss: 0.0325 - acc: 0.9869 - val_loss: 0.0299 - val_acc: 0.9867
Epoch 2/40
 - 6s - loss: 0.0212 - acc: 0.9894 - val_loss: 0.0293 - val_acc: 0.9867
Epoch 3/40
 - 6s - loss: 0.0188 - acc: 0.9899 - val_loss: 0.0310 - val_acc: 0.9857

==================================================================================================
	Training time : 0:01:05.944097
==================================================================================================
	Identification : 0.4
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 43
	200 : 1
	100 : 1
	5 : 44
	25 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	200 : 1
	0 : 241
	100 : 1
	5 : 57
	25 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15313

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15313 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15313
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_367 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_368 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_367 (Embedding)       (None, 4, 239)       3659807     input_367[0][0]                  
__________________________________________________________________________________________________
embedding_368 (Embedding)       (None, 4, 115)       19435       input_368[0][0]                  
__________________________________________________________________________________________________
flatten_367 (Flatten)           (None, 956)          0           embedding_367[0][0]              
__________________________________________________________________________________________________
flatten_368 (Flatten)           (None, 460)          0           embedding_368[0][0]              
__________________________________________________________________________________________________
concatenate_184 (Concatenate)   (None, 1416)         0           flatten_367[0][0]                
                                                                 flatten_368[0][0]                
__________________________________________________________________________________________________
dense_292 (Dense)               (None, 8)            11336       concatenate_184[0][0]            
==================================================================================================
Total params: 3,690,578
Trainable params: 3,690,578
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({1.0: 26937, 0.0: 26782, 2.0: 284, 6.0: 169, 5.0: 51, 4.0: 38})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.0272 - acc: 0.9892 - val_loss: 0.0231 - val_acc: 0.9898
Epoch 2/40
 - 6s - loss: 0.0167 - acc: 0.9921 - val_loss: 0.0217 - val_acc: 0.9896
Epoch 3/40
 - 6s - loss: 0.0146 - acc: 0.9927 - val_loss: 0.0227 - val_acc: 0.9896

==================================================================================================
	Training time : 0:01:06.682241
==================================================================================================
	Identification : 0.274
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 25
	5 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 221
	25 : 1
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15281

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15281 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15281
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_369 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_370 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_369 (Embedding)       (None, 4, 239)       3652159     input_369[0][0]                  
__________________________________________________________________________________________________
embedding_370 (Embedding)       (None, 4, 115)       19435       input_370[0][0]                  
__________________________________________________________________________________________________
flatten_369 (Flatten)           (None, 956)          0           embedding_369[0][0]              
__________________________________________________________________________________________________
flatten_370 (Flatten)           (None, 460)          0           embedding_370[0][0]              
__________________________________________________________________________________________________
concatenate_185 (Concatenate)   (None, 1416)         0           flatten_369[0][0]                
                                                                 flatten_370[0][0]                
__________________________________________________________________________________________________
dense_293 (Dense)               (None, 8)            11336       concatenate_185[0][0]            
==================================================================================================
Total params: 3,682,930
Trainable params: 3,682,930
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 26975, 1.0: 26725, 2.0: 301, 6.0: 167, 5.0: 49, 4.0: 44})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.0277 - acc: 0.9893 - val_loss: 0.0221 - val_acc: 0.9910
Epoch 2/40
 - 6s - loss: 0.0168 - acc: 0.9921 - val_loss: 0.0233 - val_acc: 0.9900
Epoch 3/40
 - 6s - loss: 0.0147 - acc: 0.9925 - val_loss: 0.0215 - val_acc: 0.9898

==================================================================================================
	Training time : 0:01:06.689430
==================================================================================================
	Identification : 0.384
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 58
	5 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 192
	25 : 1
	5 : 28

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 20014
	After : 15240

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 15240 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 15240
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_371 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_372 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_371 (Embedding)       (None, 4, 239)       3642360     input_371[0][0]                  
__________________________________________________________________________________________________
embedding_372 (Embedding)       (None, 4, 115)       19435       input_372[0][0]                  
__________________________________________________________________________________________________
flatten_371 (Flatten)           (None, 956)          0           embedding_371[0][0]              
__________________________________________________________________________________________________
flatten_372 (Flatten)           (None, 460)          0           embedding_372[0][0]              
__________________________________________________________________________________________________
concatenate_186 (Concatenate)   (None, 1416)         0           flatten_371[0][0]                
                                                                 flatten_372[0][0]                
__________________________________________________________________________________________________
dense_294 (Dense)               (None, 8)            11336       concatenate_186[0][0]            
==================================================================================================
Total params: 3,673,131
Trainable params: 3,673,131
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27166, 1.0: 26556, 2.0: 297, 6.0: 151, 5.0: 52, 4.0: 39})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 6s - loss: 0.0275 - acc: 0.9893 - val_loss: 0.0218 - val_acc: 0.9903
Epoch 2/40
 - 6s - loss: 0.0166 - acc: 0.9920 - val_loss: 0.0214 - val_acc: 0.9906
Epoch 3/40
 - 6s - loss: 0.0146 - acc: 0.9926 - val_loss: 0.0220 - val_acc: 0.9899

==================================================================================================
	Training time : 0:01:06.378361
==================================================================================================
	Identification : 0.333
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 56
	5 : 14

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 193
	25 : 1
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12580

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12580 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12580
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_373 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_374 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_373 (Embedding)       (None, 4, 239)       3006620     input_373[0][0]                  
__________________________________________________________________________________________________
embedding_374 (Embedding)       (None, 4, 115)       12650       input_374[0][0]                  
__________________________________________________________________________________________________
flatten_373 (Flatten)           (None, 956)          0           embedding_373[0][0]              
__________________________________________________________________________________________________
flatten_374 (Flatten)           (None, 460)          0           embedding_374[0][0]              
__________________________________________________________________________________________________
concatenate_187 (Concatenate)   (None, 1416)         0           flatten_373[0][0]                
                                                                 flatten_374[0][0]                
__________________________________________________________________________________________________
dense_295 (Dense)               (None, 8)            11336       concatenate_187[0][0]            
==================================================================================================
Total params: 3,030,606
Trainable params: 3,030,606
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27233, 1.0: 26294, 2.0: 494, 6.0: 235, 5.0: 230})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0501 - acc: 0.9794 - val_loss: 0.0415 - val_acc: 0.9817
Epoch 2/40
 - 6s - loss: 0.0366 - acc: 0.9822 - val_loss: 0.0421 - val_acc: 0.9802
Epoch 3/40
 - 6s - loss: 0.0338 - acc: 0.9828 - val_loss: 0.0435 - val_acc: 0.9795

==================================================================================================
	Training time : 0:01:09.861050
==================================================================================================
	Identification : 0.249
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 30
	25 : 6
	50 : 4
	5 : 31

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 369
	25 : 17
	50 : 6
	5 : 89

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12618

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12618 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12618
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_375 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_376 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_375 (Embedding)       (None, 4, 239)       3015702     input_375[0][0]                  
__________________________________________________________________________________________________
embedding_376 (Embedding)       (None, 4, 115)       12650       input_376[0][0]                  
__________________________________________________________________________________________________
flatten_375 (Flatten)           (None, 956)          0           embedding_375[0][0]              
__________________________________________________________________________________________________
flatten_376 (Flatten)           (None, 460)          0           embedding_376[0][0]              
__________________________________________________________________________________________________
concatenate_188 (Concatenate)   (None, 1416)         0           flatten_375[0][0]                
                                                                 flatten_376[0][0]                
__________________________________________________________________________________________________
dense_296 (Dense)               (None, 8)            11336       concatenate_188[0][0]            
==================================================================================================
Total params: 3,039,688
Trainable params: 3,039,688
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27027, 1.0: 26530, 2.0: 466, 5.0: 242, 6.0: 221})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0502 - acc: 0.9793 - val_loss: 0.0429 - val_acc: 0.9804
Epoch 2/40
 - 6s - loss: 0.0366 - acc: 0.9822 - val_loss: 0.0473 - val_acc: 0.9812
Epoch 3/40
 - 6s - loss: 0.0338 - acc: 0.9825 - val_loss: 0.0433 - val_acc: 0.9785

==================================================================================================
	Training time : 0:01:10.143035
==================================================================================================
	Identification : 0.375
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 86
	25 : 11
	50 : 7
	5 : 52

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 320
	25 : 13
	50 : 5
	5 : 68

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15500
	After : 12514

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 12514 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1979
	One occurrence keys in vocabulary 1979 / 12514
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_377 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_378 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_377 (Embedding)       (None, 4, 239)       2990846     input_377[0][0]                  
__________________________________________________________________________________________________
embedding_378 (Embedding)       (None, 4, 115)       12650       input_378[0][0]                  
__________________________________________________________________________________________________
flatten_377 (Flatten)           (None, 956)          0           embedding_377[0][0]              
__________________________________________________________________________________________________
flatten_378 (Flatten)           (None, 460)          0           embedding_378[0][0]              
__________________________________________________________________________________________________
concatenate_189 (Concatenate)   (None, 1416)         0           flatten_377[0][0]                
                                                                 flatten_378[0][0]                
__________________________________________________________________________________________________
dense_297 (Dense)               (None, 8)            11336       concatenate_189[0][0]            
==================================================================================================
Total params: 3,014,832
Trainable params: 3,014,832
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27180, 1.0: 26303, 2.0: 505, 5.0: 269, 6.0: 229})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 6s - loss: 0.0498 - acc: 0.9797 - val_loss: 0.0435 - val_acc: 0.9812
Epoch 2/40
 - 6s - loss: 0.0364 - acc: 0.9822 - val_loss: 0.0436 - val_acc: 0.9792
Epoch 3/40
 - 6s - loss: 0.0336 - acc: 0.9828 - val_loss: 0.0445 - val_acc: 0.9792

==================================================================================================
	Training time : 0:01:07.463695
==================================================================================================
	Identification : 0.279
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 31
	25 : 10
	50 : 4
	5 : 32

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 371
	25 : 17
	50 : 6
	5 : 85

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14841

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14841 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14841
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_379 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_380 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_379 (Embedding)       (None, 4, 239)       3546999     input_379[0][0]                  
__________________________________________________________________________________________________
embedding_380 (Embedding)       (None, 4, 115)       19320       input_380[0][0]                  
__________________________________________________________________________________________________
flatten_379 (Flatten)           (None, 956)          0           embedding_379[0][0]              
__________________________________________________________________________________________________
flatten_380 (Flatten)           (None, 460)          0           embedding_380[0][0]              
__________________________________________________________________________________________________
concatenate_190 (Concatenate)   (None, 1416)         0           flatten_379[0][0]                
                                                                 flatten_380[0][0]                
__________________________________________________________________________________________________
dense_298 (Dense)               (None, 8)            11336       concatenate_190[0][0]            
==================================================================================================
Total params: 3,577,655
Trainable params: 3,577,655
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39777, 1.0: 39437, 2.0: 606, 4.0: 276, 6.0: 165, 5.0: 104, 3.0: 6})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 9s - loss: 0.0313 - acc: 0.9869 - val_loss: 0.0266 - val_acc: 0.9874
Epoch 2/40
 - 9s - loss: 0.0211 - acc: 0.9893 - val_loss: 0.0260 - val_acc: 0.9873
Epoch 3/40
 - 9s - loss: 0.0191 - acc: 0.9897 - val_loss: 0.0276 - val_acc: 0.9870

==================================================================================================
	Training time : 0:01:30.749899
==================================================================================================
	Identification : 0.358
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 45
	100 : 1
	5 : 33
	300 : 1
	50 : 4
	25 : 14

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 308
	100 : 1
	5 : 50
	300 : 1
	50 : 3
	25 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14856

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14856 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14856
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_381 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_382 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_381 (Embedding)       (None, 4, 239)       3550584     input_381[0][0]                  
__________________________________________________________________________________________________
embedding_382 (Embedding)       (None, 4, 115)       19320       input_382[0][0]                  
__________________________________________________________________________________________________
flatten_381 (Flatten)           (None, 956)          0           embedding_381[0][0]              
__________________________________________________________________________________________________
flatten_382 (Flatten)           (None, 460)          0           embedding_382[0][0]              
__________________________________________________________________________________________________
concatenate_191 (Concatenate)   (None, 1416)         0           flatten_381[0][0]                
                                                                 flatten_382[0][0]                
__________________________________________________________________________________________________
dense_299 (Dense)               (None, 8)            11336       concatenate_191[0][0]            
==================================================================================================
Total params: 3,581,240
Trainable params: 3,581,240
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40003, 1.0: 39288, 2.0: 549, 4.0: 260, 6.0: 153, 5.0: 109, 3.0: 9})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 9s - loss: 0.0310 - acc: 0.9871 - val_loss: 0.0287 - val_acc: 0.9884
Epoch 2/40
 - 9s - loss: 0.0213 - acc: 0.9893 - val_loss: 0.0264 - val_acc: 0.9882
Epoch 3/40
 - 9s - loss: 0.0192 - acc: 0.9896 - val_loss: 0.0270 - val_acc: 0.9868

==================================================================================================
	Training time : 0:01:30.608652
==================================================================================================
	Identification : 0.39
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 55
	100 : 1
	5 : 47
	300 : 1
	50 : 3
	25 : 19

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 301
	25 : 11
	50 : 3
	300 : 1
	5 : 39

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 1780, occurrences : 5364
	Impotant words in tain : 1424
	MWE length mean : 2.13
	Seen MWEs : 405 (60 %)
	New MWEs : 265 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 18410
	After : 14774

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 14774 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2114
	One occurrence keys in vocabulary 2114 / 14774
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_383 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_384 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_383 (Embedding)       (None, 4, 239)       3530986     input_383[0][0]                  
__________________________________________________________________________________________________
embedding_384 (Embedding)       (None, 4, 115)       19320       input_384[0][0]                  
__________________________________________________________________________________________________
flatten_383 (Flatten)           (None, 956)          0           embedding_383[0][0]              
__________________________________________________________________________________________________
flatten_384 (Flatten)           (None, 460)          0           embedding_384[0][0]              
__________________________________________________________________________________________________
concatenate_192 (Concatenate)   (None, 1416)         0           flatten_383[0][0]                
                                                                 flatten_384[0][0]                
__________________________________________________________________________________________________
dense_300 (Dense)               (None, 8)            11336       concatenate_192[0][0]            
==================================================================================================
Total params: 3,561,642
Trainable params: 3,561,642
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39684, 1.0: 39543, 2.0: 587, 4.0: 275, 6.0: 164, 5.0: 111, 3.0: 7})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 8s - loss: 0.0311 - acc: 0.9870 - val_loss: 0.0260 - val_acc: 0.9883
Epoch 2/40
 - 8s - loss: 0.0212 - acc: 0.9893 - val_loss: 0.0268 - val_acc: 0.9875
Epoch 3/40
 - 8s - loss: 0.0193 - acc: 0.9895 - val_loss: 0.0272 - val_acc: 0.9873

==================================================================================================
	Training time : 0:01:31.627897
==================================================================================================
	Identification : 0.361
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 28
	100 : 1
	5 : 37
	300 : 1
	50 : 3
	25 : 15

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 325
	100 : 1
	5 : 49
	300 : 1
	50 : 3
	25 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 21100

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 21100 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 21100
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_385 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_386 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_385 (Embedding)       (None, 4, 239)       5042900     input_385[0][0]                  
__________________________________________________________________________________________________
embedding_386 (Embedding)       (None, 4, 115)       25760       input_386[0][0]                  
__________________________________________________________________________________________________
flatten_385 (Flatten)           (None, 956)          0           embedding_385[0][0]              
__________________________________________________________________________________________________
flatten_386 (Flatten)           (None, 460)          0           embedding_386[0][0]              
__________________________________________________________________________________________________
concatenate_193 (Concatenate)   (None, 1416)         0           flatten_385[0][0]                
                                                                 flatten_386[0][0]                
__________________________________________________________________________________________________
dense_301 (Dense)               (None, 8)            11336       concatenate_193[0][0]            
==================================================================================================
Total params: 5,079,996
Trainable params: 5,079,996
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47550, 1.0: 46602, 2.0: 543, 6.0: 276, 5.0: 85, 4.0: 67})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.0249 - acc: 0.9899 - val_loss: 0.0209 - val_acc: 0.9910
Epoch 2/40
 - 11s - loss: 0.0166 - acc: 0.9921 - val_loss: 0.0199 - val_acc: 0.9911
Epoch 3/40
 - 11s - loss: 0.0149 - acc: 0.9925 - val_loss: 0.0208 - val_acc: 0.9906

==================================================================================================
	Training time : 0:01:49.833636
==================================================================================================
	Identification : 0.324
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 48
	25 : 3
	5 : 29

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 285
	25 : 6
	5 : 68

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 21122

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 21122 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 21122
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_387 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_388 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_387 (Embedding)       (None, 4, 239)       5048158     input_387[0][0]                  
__________________________________________________________________________________________________
embedding_388 (Embedding)       (None, 4, 115)       25760       input_388[0][0]                  
__________________________________________________________________________________________________
flatten_387 (Flatten)           (None, 956)          0           embedding_387[0][0]              
__________________________________________________________________________________________________
flatten_388 (Flatten)           (None, 460)          0           embedding_388[0][0]              
__________________________________________________________________________________________________
concatenate_194 (Concatenate)   (None, 1416)         0           flatten_387[0][0]                
                                                                 flatten_388[0][0]                
__________________________________________________________________________________________________
dense_302 (Dense)               (None, 8)            11336       concatenate_194[0][0]            
==================================================================================================
Total params: 5,085,254
Trainable params: 5,085,254
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47484, 1.0: 46649, 2.0: 533, 6.0: 303, 5.0: 87, 4.0: 67})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.0252 - acc: 0.9898 - val_loss: 0.0203 - val_acc: 0.9912
Epoch 2/40
 - 11s - loss: 0.0165 - acc: 0.9922 - val_loss: 0.0214 - val_acc: 0.9907
Epoch 3/40
 - 11s - loss: 0.0149 - acc: 0.9925 - val_loss: 0.0210 - val_acc: 0.9900

==================================================================================================
	Training time : 0:01:50.298805
==================================================================================================
	Identification : 0.373
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 75
	25 : 4
	5 : 35

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 259
	25 : 5
	5 : 63

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 2021, occurrences : 4337
	Impotant words in tain : 1512
	MWE length mean : 2.22
	Seen MWEs : 387 (69 %)
	New MWEs : 166 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 27332
	After : 20980

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20980 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2393
	One occurrence keys in vocabulary 2393 / 20980
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_389 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_390 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_389 (Embedding)       (None, 4, 239)       5014220     input_389[0][0]                  
__________________________________________________________________________________________________
embedding_390 (Embedding)       (None, 4, 115)       25760       input_390[0][0]                  
__________________________________________________________________________________________________
flatten_389 (Flatten)           (None, 956)          0           embedding_389[0][0]              
__________________________________________________________________________________________________
flatten_390 (Flatten)           (None, 460)          0           embedding_390[0][0]              
__________________________________________________________________________________________________
concatenate_195 (Concatenate)   (None, 1416)         0           flatten_389[0][0]                
                                                                 flatten_390[0][0]                
__________________________________________________________________________________________________
dense_303 (Dense)               (None, 8)            11336       concatenate_195[0][0]            
==================================================================================================
Total params: 5,051,316
Trainable params: 5,051,316
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47220, 1.0: 46978, 2.0: 512, 6.0: 263, 4.0: 76, 5.0: 74})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 11s - loss: 0.0251 - acc: 0.9899 - val_loss: 0.0207 - val_acc: 0.9910
Epoch 2/40
 - 11s - loss: 0.0165 - acc: 0.9921 - val_loss: 0.0209 - val_acc: 0.9899
Epoch 3/40
 - 11s - loss: 0.0149 - acc: 0.9925 - val_loss: 0.0221 - val_acc: 0.9900

==================================================================================================
	Training time : 0:01:50.702996
==================================================================================================
	Identification : 0.359
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 77
	25 : 2
	5 : 35

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 257
	25 : 7
	5 : 64

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13968

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13968 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13968
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_391 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_392 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_391 (Embedding)       (None, 4, 239)       3338352     input_391[0][0]                  
__________________________________________________________________________________________________
embedding_392 (Embedding)       (None, 4, 115)       13685       input_392[0][0]                  
__________________________________________________________________________________________________
flatten_391 (Flatten)           (None, 956)          0           embedding_391[0][0]              
__________________________________________________________________________________________________
flatten_392 (Flatten)           (None, 460)          0           embedding_392[0][0]              
__________________________________________________________________________________________________
concatenate_196 (Concatenate)   (None, 1416)         0           flatten_391[0][0]                
                                                                 flatten_392[0][0]                
__________________________________________________________________________________________________
dense_304 (Dense)               (None, 8)            11336       concatenate_196[0][0]            
==================================================================================================
Total params: 3,363,373
Trainable params: 3,363,373
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33520, 1.0: 32795, 2.0: 662, 5.0: 313, 6.0: 296})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0488 - acc: 0.9797 - val_loss: 0.0446 - val_acc: 0.9803
Epoch 2/40
 - 7s - loss: 0.0364 - acc: 0.9822 - val_loss: 0.0438 - val_acc: 0.9792
Epoch 3/40
 - 7s - loss: 0.0339 - acc: 0.9824 - val_loss: 0.0450 - val_acc: 0.9776

==================================================================================================
	Training time : 0:01:23.023452
==================================================================================================
	Identification : 0.267
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 16
	25 : 10
	50 : 9
	5 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 230
	25 : 11
	50 : 8
	5 : 61

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13942

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13942 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13942
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_393 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_394 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_393 (Embedding)       (None, 4, 239)       3332138     input_393[0][0]                  
__________________________________________________________________________________________________
embedding_394 (Embedding)       (None, 4, 115)       13685       input_394[0][0]                  
__________________________________________________________________________________________________
flatten_393 (Flatten)           (None, 956)          0           embedding_393[0][0]              
__________________________________________________________________________________________________
flatten_394 (Flatten)           (None, 460)          0           embedding_394[0][0]              
__________________________________________________________________________________________________
concatenate_197 (Concatenate)   (None, 1416)         0           flatten_393[0][0]                
                                                                 flatten_394[0][0]                
__________________________________________________________________________________________________
dense_305 (Dense)               (None, 8)            11336       concatenate_197[0][0]            
==================================================================================================
Total params: 3,357,159
Trainable params: 3,357,159
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33405, 1.0: 32898, 2.0: 660, 5.0: 319, 6.0: 304})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0490 - acc: 0.9797 - val_loss: 0.0431 - val_acc: 0.9813
Epoch 2/40
 - 7s - loss: 0.0365 - acc: 0.9821 - val_loss: 0.0431 - val_acc: 0.9794
Epoch 3/40
 - 7s - loss: 0.0339 - acc: 0.9826 - val_loss: 0.0439 - val_acc: 0.9784

==================================================================================================
	Training time : 0:01:19.608208
==================================================================================================
	Identification : 0.163
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 13
	25 : 2
	50 : 5
	5 : 15

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 233
	25 : 18
	50 : 9
	5 : 67

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 2083, occurrences : 6091
	Impotant words in tain : 1362
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 156 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17167
	After : 13968

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13968 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2308
	One occurrence keys in vocabulary 2308 / 13968
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 239
	POS = 115 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_395 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_396 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_395 (Embedding)       (None, 4, 239)       3338352     input_395[0][0]                  
__________________________________________________________________________________________________
embedding_396 (Embedding)       (None, 4, 115)       13685       input_396[0][0]                  
__________________________________________________________________________________________________
flatten_395 (Flatten)           (None, 956)          0           embedding_395[0][0]              
__________________________________________________________________________________________________
flatten_396 (Flatten)           (None, 460)          0           embedding_396[0][0]              
__________________________________________________________________________________________________
concatenate_198 (Concatenate)   (None, 1416)         0           flatten_395[0][0]                
                                                                 flatten_396[0][0]                
__________________________________________________________________________________________________
dense_306 (Dense)               (None, 8)            11336       concatenate_198[0][0]            
==================================================================================================
Total params: 3,363,373
Trainable params: 3,363,373
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33533, 1.0: 32862, 2.0: 607, 5.0: 299, 6.0: 285})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.068
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0488 - acc: 0.9796 - val_loss: 0.0447 - val_acc: 0.9800
Epoch 2/40
 - 7s - loss: 0.0365 - acc: 0.9821 - val_loss: 0.0424 - val_acc: 0.9802
Epoch 3/40
 - 7s - loss: 0.0339 - acc: 0.9827 - val_loss: 0.0442 - val_acc: 0.9784

==================================================================================================
	Training time : 0:01:20.382902
==================================================================================================
	Identification : 0.258
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 19
	25 : 6
	50 : 6
	5 : 18

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 225
	25 : 17
	50 : 7
	5 : 63

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
15_False_False_307_107_frequent_True_52_relu_0.556_False_512_relu_0.2_adagrad_0.055_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19950

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19950 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19950
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_397 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_398 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_397 (Embedding)       (None, 4, 307)       6124650     input_397[0][0]                  
__________________________________________________________________________________________________
embedding_398 (Embedding)       (None, 4, 107)       16264       input_398[0][0]                  
__________________________________________________________________________________________________
flatten_397 (Flatten)           (None, 1228)         0           embedding_397[0][0]              
__________________________________________________________________________________________________
flatten_398 (Flatten)           (None, 428)          0           embedding_398[0][0]              
__________________________________________________________________________________________________
concatenate_199 (Concatenate)   (None, 1656)         0           flatten_397[0][0]                
                                                                 flatten_398[0][0]                
__________________________________________________________________________________________________
dense_307 (Dense)               (None, 52)           86164       concatenate_199[0][0]            
__________________________________________________________________________________________________
dropout_109 (Dropout)           (None, 52)           0           dense_307[0][0]                  
__________________________________________________________________________________________________
dense_308 (Dense)               (None, 8)            424         dropout_109[0][0]                
==================================================================================================
Total params: 6,227,502
Trainable params: 6,227,502
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27067, 1.0: 26489, 2.0: 398, 4.0: 204, 6.0: 119, 5.0: 78, 3.0: 6})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 8s - loss: 0.0398 - acc: 0.9862 - val_loss: 0.0281 - val_acc: 0.9877
Epoch 2/40
 - 8s - loss: 0.0226 - acc: 0.9899 - val_loss: 0.0280 - val_acc: 0.9881
Epoch 3/40
 - 8s - loss: 0.0189 - acc: 0.9908 - val_loss: 0.0300 - val_acc: 0.9877

==================================================================================================
	Training time : 0:01:15.847246
==================================================================================================
	Identification : 0.619
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 105
	25 : 7
	50 : 3
	100 : 1
	5 : 47

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 291
	5 : 19

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19943

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19943 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19943
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_399 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_400 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_399 (Embedding)       (None, 4, 307)       6122501     input_399[0][0]                  
__________________________________________________________________________________________________
embedding_400 (Embedding)       (None, 4, 107)       16264       input_400[0][0]                  
__________________________________________________________________________________________________
flatten_399 (Flatten)           (None, 1228)         0           embedding_399[0][0]              
__________________________________________________________________________________________________
flatten_400 (Flatten)           (None, 428)          0           embedding_400[0][0]              
__________________________________________________________________________________________________
concatenate_200 (Concatenate)   (None, 1656)         0           flatten_399[0][0]                
                                                                 flatten_400[0][0]                
__________________________________________________________________________________________________
dense_309 (Dense)               (None, 52)           86164       concatenate_200[0][0]            
__________________________________________________________________________________________________
dropout_110 (Dropout)           (None, 52)           0           dense_309[0][0]                  
__________________________________________________________________________________________________
dense_310 (Dense)               (None, 8)            424         dropout_110[0][0]                
==================================================================================================
Total params: 6,225,353
Trainable params: 6,225,353
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26894, 1.0: 26705, 2.0: 398, 4.0: 188, 6.0: 97, 5.0: 74, 3.0: 5})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 8s - loss: 0.0418 - acc: 0.9856 - val_loss: 0.0268 - val_acc: 0.9886
Epoch 2/40
 - 8s - loss: 0.0242 - acc: 0.9895 - val_loss: 0.0270 - val_acc: 0.9884
Epoch 3/40
 - 8s - loss: 0.0201 - acc: 0.9905 - val_loss: 0.0292 - val_acc: 0.9878

==================================================================================================
	Training time : 0:01:13.935681
==================================================================================================
	Identification : 0.33
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 38
	25 : 5
	50 : 3
	100 : 1
	5 : 24

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 355
	25 : 5
	50 : 2
	100 : 1
	5 : 48

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19887

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19887 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19887
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_401 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_402 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_401 (Embedding)       (None, 4, 307)       6105309     input_401[0][0]                  
__________________________________________________________________________________________________
embedding_402 (Embedding)       (None, 4, 107)       16264       input_402[0][0]                  
__________________________________________________________________________________________________
flatten_401 (Flatten)           (None, 1228)         0           embedding_401[0][0]              
__________________________________________________________________________________________________
flatten_402 (Flatten)           (None, 428)          0           embedding_402[0][0]              
__________________________________________________________________________________________________
concatenate_201 (Concatenate)   (None, 1656)         0           flatten_401[0][0]                
                                                                 flatten_402[0][0]                
__________________________________________________________________________________________________
dense_311 (Dense)               (None, 52)           86164       concatenate_201[0][0]            
__________________________________________________________________________________________________
dropout_111 (Dropout)           (None, 52)           0           dense_311[0][0]                  
__________________________________________________________________________________________________
dense_312 (Dense)               (None, 8)            424         dropout_111[0][0]                
==================================================================================================
Total params: 6,208,161
Trainable params: 6,208,161
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26858, 1.0: 26704, 2.0: 436, 4.0: 171, 6.0: 104, 5.0: 81, 3.0: 7})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 8s - loss: 0.0403 - acc: 0.9857 - val_loss: 0.0297 - val_acc: 0.9878
Epoch 2/40
 - 8s - loss: 0.0234 - acc: 0.9896 - val_loss: 0.0305 - val_acc: 0.9880
Epoch 3/40
 - 8s - loss: 0.0196 - acc: 0.9905 - val_loss: 0.0329 - val_acc: 0.9874

==================================================================================================
	Training time : 0:01:15.571572
==================================================================================================
	Identification : 0.003
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 389
	25 : 7
	50 : 3
	100 : 1
	5 : 60

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20434

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20434 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20434
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_403 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_404 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_403 (Embedding)       (None, 4, 307)       6273238     input_403[0][0]                  
__________________________________________________________________________________________________
embedding_404 (Embedding)       (None, 4, 107)       18083       input_404[0][0]                  
__________________________________________________________________________________________________
flatten_403 (Flatten)           (None, 1228)         0           embedding_403[0][0]              
__________________________________________________________________________________________________
flatten_404 (Flatten)           (None, 428)          0           embedding_404[0][0]              
__________________________________________________________________________________________________
concatenate_202 (Concatenate)   (None, 1656)         0           flatten_403[0][0]                
                                                                 flatten_404[0][0]                
__________________________________________________________________________________________________
dense_313 (Dense)               (None, 52)           86164       concatenate_202[0][0]            
__________________________________________________________________________________________________
dropout_112 (Dropout)           (None, 52)           0           dense_313[0][0]                  
__________________________________________________________________________________________________
dense_314 (Dense)               (None, 8)            424         dropout_112[0][0]                
==================================================================================================
Total params: 6,377,909
Trainable params: 6,377,909
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27173, 1.0: 26565, 2.0: 289, 6.0: 143, 5.0: 48, 4.0: 43})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0379 - acc: 0.9876 - val_loss: 0.0216 - val_acc: 0.9912
Epoch 2/40
 - 8s - loss: 0.0192 - acc: 0.9919 - val_loss: 0.0216 - val_acc: 0.9914
Epoch 3/40
 - 8s - loss: 0.0151 - acc: 0.9930 - val_loss: 0.0223 - val_acc: 0.9903

==================================================================================================
	Training time : 0:01:13.421130
==================================================================================================
	Identification : 0.044
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 7
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 315
	5 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20432

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20432 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20432
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_405 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_406 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_405 (Embedding)       (None, 4, 307)       6272624     input_405[0][0]                  
__________________________________________________________________________________________________
embedding_406 (Embedding)       (None, 4, 107)       18083       input_406[0][0]                  
__________________________________________________________________________________________________
flatten_405 (Flatten)           (None, 1228)         0           embedding_405[0][0]              
__________________________________________________________________________________________________
flatten_406 (Flatten)           (None, 428)          0           embedding_406[0][0]              
__________________________________________________________________________________________________
concatenate_203 (Concatenate)   (None, 1656)         0           flatten_405[0][0]                
                                                                 flatten_406[0][0]                
__________________________________________________________________________________________________
dense_315 (Dense)               (None, 52)           86164       concatenate_203[0][0]            
__________________________________________________________________________________________________
dropout_113 (Dropout)           (None, 52)           0           dense_315[0][0]                  
__________________________________________________________________________________________________
dense_316 (Dense)               (None, 8)            424         dropout_113[0][0]                
==================================================================================================
Total params: 6,377,295
Trainable params: 6,377,295
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27012, 1.0: 26706, 2.0: 323, 6.0: 138, 5.0: 44, 4.0: 38})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0372 - acc: 0.9876 - val_loss: 0.0218 - val_acc: 0.9914
Epoch 2/40
 - 8s - loss: 0.0198 - acc: 0.9917 - val_loss: 0.0228 - val_acc: 0.9911
Epoch 3/40
 - 8s - loss: 0.0157 - acc: 0.9928 - val_loss: 0.0247 - val_acc: 0.9903

==================================================================================================
	Training time : 0:01:15.048146
==================================================================================================
	Identification : 0.061
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 310
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20398

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20398 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20398
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_407 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_408 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_407 (Embedding)       (None, 4, 307)       6262186     input_407[0][0]                  
__________________________________________________________________________________________________
embedding_408 (Embedding)       (None, 4, 107)       18083       input_408[0][0]                  
__________________________________________________________________________________________________
flatten_407 (Flatten)           (None, 1228)         0           embedding_407[0][0]              
__________________________________________________________________________________________________
flatten_408 (Flatten)           (None, 428)          0           embedding_408[0][0]              
__________________________________________________________________________________________________
concatenate_204 (Concatenate)   (None, 1656)         0           flatten_407[0][0]                
                                                                 flatten_408[0][0]                
__________________________________________________________________________________________________
dense_317 (Dense)               (None, 52)           86164       concatenate_204[0][0]            
__________________________________________________________________________________________________
dropout_114 (Dropout)           (None, 52)           0           dense_317[0][0]                  
__________________________________________________________________________________________________
dense_318 (Dense)               (None, 8)            424         dropout_114[0][0]                
==================================================================================================
Total params: 6,366,857
Trainable params: 6,366,857
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27064, 1.0: 26655, 2.0: 298, 6.0: 161, 5.0: 50, 4.0: 33})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 8s - loss: 0.0363 - acc: 0.9880 - val_loss: 0.0222 - val_acc: 0.9910
Epoch 2/40
 - 8s - loss: 0.0193 - acc: 0.9918 - val_loss: 0.0219 - val_acc: 0.9911
Epoch 3/40
 - 8s - loss: 0.0156 - acc: 0.9928 - val_loss: 0.0233 - val_acc: 0.9913

==================================================================================================
	Training time : 0:01:13.427299
==================================================================================================
	Identification : 0
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 321
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35898

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35898 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35898
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_409 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_410 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_409 (Embedding)       (None, 4, 307)       11020686    input_409[0][0]                  
__________________________________________________________________________________________________
embedding_410 (Embedding)       (None, 4, 107)       11770       input_410[0][0]                  
__________________________________________________________________________________________________
flatten_409 (Flatten)           (None, 1228)         0           embedding_409[0][0]              
__________________________________________________________________________________________________
flatten_410 (Flatten)           (None, 428)          0           embedding_410[0][0]              
__________________________________________________________________________________________________
concatenate_205 (Concatenate)   (None, 1656)         0           flatten_409[0][0]                
                                                                 flatten_410[0][0]                
__________________________________________________________________________________________________
dense_319 (Dense)               (None, 52)           86164       concatenate_205[0][0]            
__________________________________________________________________________________________________
dropout_115 (Dropout)           (None, 52)           0           dense_319[0][0]                  
__________________________________________________________________________________________________
dense_320 (Dense)               (None, 8)            424         dropout_115[0][0]                
==================================================================================================
Total params: 11,119,044
Trainable params: 11,119,044
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27191, 1.0: 26298, 2.0: 496, 5.0: 269, 6.0: 232})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 10s - loss: 0.0557 - acc: 0.9803 - val_loss: 0.0404 - val_acc: 0.9835
Epoch 2/40
 - 10s - loss: 0.0319 - acc: 0.9860 - val_loss: 0.0427 - val_acc: 0.9839
Epoch 3/40
 - 10s - loss: 0.0284 - acc: 0.9870 - val_loss: 0.0439 - val_acc: 0.9838

==================================================================================================
	Training time : 0:01:23.212903
==================================================================================================
	Identification : 0.306
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 142
	5 : 7

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 560
	25 : 2
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35927

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35927 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35927
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_411 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_412 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_411 (Embedding)       (None, 4, 307)       11029589    input_411[0][0]                  
__________________________________________________________________________________________________
embedding_412 (Embedding)       (None, 4, 107)       11770       input_412[0][0]                  
__________________________________________________________________________________________________
flatten_411 (Flatten)           (None, 1228)         0           embedding_411[0][0]              
__________________________________________________________________________________________________
flatten_412 (Flatten)           (None, 428)          0           embedding_412[0][0]              
__________________________________________________________________________________________________
concatenate_206 (Concatenate)   (None, 1656)         0           flatten_411[0][0]                
                                                                 flatten_412[0][0]                
__________________________________________________________________________________________________
dense_321 (Dense)               (None, 52)           86164       concatenate_206[0][0]            
__________________________________________________________________________________________________
dropout_116 (Dropout)           (None, 52)           0           dense_321[0][0]                  
__________________________________________________________________________________________________
dense_322 (Dense)               (None, 8)            424         dropout_116[0][0]                
==================================================================================================
Total params: 11,127,947
Trainable params: 11,127,947
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26955, 1.0: 26567, 2.0: 488, 5.0: 239, 6.0: 237})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 10s - loss: 0.0562 - acc: 0.9800 - val_loss: 0.0376 - val_acc: 0.9836
Epoch 2/40
 - 10s - loss: 0.0321 - acc: 0.9859 - val_loss: 0.0380 - val_acc: 0.9839
Epoch 3/40
 - 10s - loss: 0.0284 - acc: 0.9871 - val_loss: 0.0415 - val_acc: 0.9836

==================================================================================================
	Training time : 0:01:26.194971
==================================================================================================
	Identification : 0.196
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 73
	25 : 1
	5 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 628
	25 : 1
	5 : 28

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35853

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35853 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35853
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_413 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_414 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_413 (Embedding)       (None, 4, 307)       11006871    input_413[0][0]                  
__________________________________________________________________________________________________
embedding_414 (Embedding)       (None, 4, 107)       11770       input_414[0][0]                  
__________________________________________________________________________________________________
flatten_413 (Flatten)           (None, 1228)         0           embedding_413[0][0]              
__________________________________________________________________________________________________
flatten_414 (Flatten)           (None, 428)          0           embedding_414[0][0]              
__________________________________________________________________________________________________
concatenate_207 (Concatenate)   (None, 1656)         0           flatten_413[0][0]                
                                                                 flatten_414[0][0]                
__________________________________________________________________________________________________
dense_323 (Dense)               (None, 52)           86164       concatenate_207[0][0]            
__________________________________________________________________________________________________
dropout_117 (Dropout)           (None, 52)           0           dense_323[0][0]                  
__________________________________________________________________________________________________
dense_324 (Dense)               (None, 8)            424         dropout_117[0][0]                
==================================================================================================
Total params: 11,105,229
Trainable params: 11,105,229
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27087, 1.0: 26418, 2.0: 512, 5.0: 252, 6.0: 217})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 10s - loss: 0.0559 - acc: 0.9797 - val_loss: 0.0380 - val_acc: 0.9843
Epoch 2/40
 - 10s - loss: 0.0319 - acc: 0.9858 - val_loss: 0.0382 - val_acc: 0.9845
Epoch 3/40
 - 10s - loss: 0.0274 - acc: 0.9869 - val_loss: 0.0402 - val_acc: 0.9846

==================================================================================================
	Training time : 0:01:20.994832
==================================================================================================
	Identification : 0.469
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 232
	25 : 2
	5 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 468
	5 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24724

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24724 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24724
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_415 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_416 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_415 (Embedding)       (None, 4, 307)       7590268     input_415[0][0]                  
__________________________________________________________________________________________________
embedding_416 (Embedding)       (None, 4, 107)       17976       input_416[0][0]                  
__________________________________________________________________________________________________
flatten_415 (Flatten)           (None, 1228)         0           embedding_415[0][0]              
__________________________________________________________________________________________________
flatten_416 (Flatten)           (None, 428)          0           embedding_416[0][0]              
__________________________________________________________________________________________________
concatenate_208 (Concatenate)   (None, 1656)         0           flatten_415[0][0]                
                                                                 flatten_416[0][0]                
__________________________________________________________________________________________________
dense_325 (Dense)               (None, 52)           86164       concatenate_208[0][0]            
__________________________________________________________________________________________________
dropout_118 (Dropout)           (None, 52)           0           dense_325[0][0]                  
__________________________________________________________________________________________________
dense_326 (Dense)               (None, 8)            424         dropout_118[0][0]                
==================================================================================================
Total params: 7,694,832
Trainable params: 7,694,832
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39840, 1.0: 39405, 2.0: 599, 4.0: 265, 6.0: 164, 5.0: 90, 3.0: 8})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 12s - loss: 0.0388 - acc: 0.9861 - val_loss: 0.0239 - val_acc: 0.9892
Epoch 2/40
 - 12s - loss: 0.0232 - acc: 0.9896 - val_loss: 0.0239 - val_acc: 0.9893
Epoch 3/40
 - 12s - loss: 0.0199 - acc: 0.9902 - val_loss: 0.0249 - val_acc: 0.9893

==================================================================================================
	Training time : 0:01:43.509527
==================================================================================================
	Identification : 0.35
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 56
	50 : 5
	5 : 29
	25 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 400
	200 : 1
	50 : 1
	5 : 40
	25 : 4

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24782

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24782 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24782
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_417 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_418 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_417 (Embedding)       (None, 4, 307)       7608074     input_417[0][0]                  
__________________________________________________________________________________________________
embedding_418 (Embedding)       (None, 4, 107)       17976       input_418[0][0]                  
__________________________________________________________________________________________________
flatten_417 (Flatten)           (None, 1228)         0           embedding_417[0][0]              
__________________________________________________________________________________________________
flatten_418 (Flatten)           (None, 428)          0           embedding_418[0][0]              
__________________________________________________________________________________________________
concatenate_209 (Concatenate)   (None, 1656)         0           flatten_417[0][0]                
                                                                 flatten_418[0][0]                
__________________________________________________________________________________________________
dense_327 (Dense)               (None, 52)           86164       concatenate_209[0][0]            
__________________________________________________________________________________________________
dropout_119 (Dropout)           (None, 52)           0           dense_327[0][0]                  
__________________________________________________________________________________________________
dense_328 (Dense)               (None, 8)            424         dropout_119[0][0]                
==================================================================================================
Total params: 7,712,638
Trainable params: 7,712,638
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39912, 1.0: 39266, 2.0: 615, 4.0: 290, 6.0: 176, 5.0: 105, 3.0: 7})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 12s - loss: 0.0372 - acc: 0.9865 - val_loss: 0.0307 - val_acc: 0.9875
Epoch 2/40
 - 12s - loss: 0.0229 - acc: 0.9898 - val_loss: 0.0304 - val_acc: 0.9879
Epoch 3/40
 - 12s - loss: 0.0196 - acc: 0.9905 - val_loss: 0.0319 - val_acc: 0.9875

==================================================================================================
	Training time : 0:01:43.696288
==================================================================================================
	Identification : 0.232
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 64
	25 : 1
	5 : 10

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 388
	200 : 1
	50 : 5
	5 : 57
	25 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24698

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24698 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24698
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_419 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_420 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_419 (Embedding)       (None, 4, 307)       7582286     input_419[0][0]                  
__________________________________________________________________________________________________
embedding_420 (Embedding)       (None, 4, 107)       17976       input_420[0][0]                  
__________________________________________________________________________________________________
flatten_419 (Flatten)           (None, 1228)         0           embedding_419[0][0]              
__________________________________________________________________________________________________
flatten_420 (Flatten)           (None, 428)          0           embedding_420[0][0]              
__________________________________________________________________________________________________
concatenate_210 (Concatenate)   (None, 1656)         0           flatten_419[0][0]                
                                                                 flatten_420[0][0]                
__________________________________________________________________________________________________
dense_329 (Dense)               (None, 52)           86164       concatenate_210[0][0]            
__________________________________________________________________________________________________
dropout_120 (Dropout)           (None, 52)           0           dense_329[0][0]                  
__________________________________________________________________________________________________
dense_330 (Dense)               (None, 8)            424         dropout_120[0][0]                
==================================================================================================
Total params: 7,686,850
Trainable params: 7,686,850
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40029, 1.0: 39217, 2.0: 570, 4.0: 288, 6.0: 159, 5.0: 100, 3.0: 8})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 12s - loss: 0.0393 - acc: 0.9863 - val_loss: 0.0266 - val_acc: 0.9876
Epoch 2/40
 - 12s - loss: 0.0234 - acc: 0.9897 - val_loss: 0.0280 - val_acc: 0.9884
Epoch 3/40
 - 12s - loss: 0.0201 - acc: 0.9905 - val_loss: 0.0301 - val_acc: 0.9872

==================================================================================================
	Training time : 0:01:43.962219
==================================================================================================
	Identification : 0.128
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 33
	25 : 2
	5 : 4

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 420
	200 : 1
	50 : 5
	5 : 62
	25 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28516

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28516 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28516
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_421 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_422 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_421 (Embedding)       (None, 4, 307)       8754412     input_421[0][0]                  
__________________________________________________________________________________________________
embedding_422 (Embedding)       (None, 4, 107)       23968       input_422[0][0]                  
__________________________________________________________________________________________________
flatten_421 (Flatten)           (None, 1228)         0           embedding_421[0][0]              
__________________________________________________________________________________________________
flatten_422 (Flatten)           (None, 428)          0           embedding_422[0][0]              
__________________________________________________________________________________________________
concatenate_211 (Concatenate)   (None, 1656)         0           flatten_421[0][0]                
                                                                 flatten_422[0][0]                
__________________________________________________________________________________________________
dense_331 (Dense)               (None, 52)           86164       concatenate_211[0][0]            
__________________________________________________________________________________________________
dropout_121 (Dropout)           (None, 52)           0           dense_331[0][0]                  
__________________________________________________________________________________________________
dense_332 (Dense)               (None, 8)            424         dropout_121[0][0]                
==================================================================================================
Total params: 8,864,968
Trainable params: 8,864,968
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47346, 1.0: 46810, 2.0: 514, 6.0: 271, 5.0: 107, 4.0: 75})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 14s - loss: 0.0333 - acc: 0.9887 - val_loss: 0.0211 - val_acc: 0.9911
Epoch 2/40
 - 14s - loss: 0.0186 - acc: 0.9921 - val_loss: 0.0215 - val_acc: 0.9915
Epoch 3/40
 - 14s - loss: 0.0152 - acc: 0.9929 - val_loss: 0.0221 - val_acc: 0.9914

==================================================================================================
	Training time : 0:02:00.481977
==================================================================================================
	Identification : 0.481
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 167
	5 : 14

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 297
	25 : 1
	5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28557

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28557 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28557
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_423 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_424 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_423 (Embedding)       (None, 4, 307)       8766999     input_423[0][0]                  
__________________________________________________________________________________________________
embedding_424 (Embedding)       (None, 4, 107)       23968       input_424[0][0]                  
__________________________________________________________________________________________________
flatten_423 (Flatten)           (None, 1228)         0           embedding_423[0][0]              
__________________________________________________________________________________________________
flatten_424 (Flatten)           (None, 428)          0           embedding_424[0][0]              
__________________________________________________________________________________________________
concatenate_212 (Concatenate)   (None, 1656)         0           flatten_423[0][0]                
                                                                 flatten_424[0][0]                
__________________________________________________________________________________________________
dense_333 (Dense)               (None, 52)           86164       concatenate_212[0][0]            
__________________________________________________________________________________________________
dropout_122 (Dropout)           (None, 52)           0           dense_333[0][0]                  
__________________________________________________________________________________________________
dense_334 (Dense)               (None, 8)            424         dropout_122[0][0]                
==================================================================================================
Total params: 8,877,555
Trainable params: 8,877,555
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47498, 1.0: 46673, 2.0: 547, 6.0: 264, 5.0: 84, 4.0: 57})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 15s - loss: 0.0326 - acc: 0.9888 - val_loss: 0.0203 - val_acc: 0.9915
Epoch 2/40
 - 15s - loss: 0.0193 - acc: 0.9919 - val_loss: 0.0217 - val_acc: 0.9915
Epoch 3/40
 - 15s - loss: 0.0162 - acc: 0.9927 - val_loss: 0.0227 - val_acc: 0.9917

==================================================================================================
	Training time : 0:02:01.942830
==================================================================================================
	Identification : 0.007
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 2

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 461
	25 : 1
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28519

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28519 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28519
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_425 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_426 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_425 (Embedding)       (None, 4, 307)       8755333     input_425[0][0]                  
__________________________________________________________________________________________________
embedding_426 (Embedding)       (None, 4, 107)       23968       input_426[0][0]                  
__________________________________________________________________________________________________
flatten_425 (Flatten)           (None, 1228)         0           embedding_425[0][0]              
__________________________________________________________________________________________________
flatten_426 (Flatten)           (None, 428)          0           embedding_426[0][0]              
__________________________________________________________________________________________________
concatenate_213 (Concatenate)   (None, 1656)         0           flatten_425[0][0]                
                                                                 flatten_426[0][0]                
__________________________________________________________________________________________________
dense_335 (Dense)               (None, 52)           86164       concatenate_213[0][0]            
__________________________________________________________________________________________________
dropout_123 (Dropout)           (None, 52)           0           dense_335[0][0]                  
__________________________________________________________________________________________________
dense_336 (Dense)               (None, 8)            424         dropout_123[0][0]                
==================================================================================================
Total params: 8,865,889
Trainable params: 8,865,889
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47390, 1.0: 46810, 2.0: 516, 6.0: 266, 5.0: 78, 4.0: 63})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 14s - loss: 0.0332 - acc: 0.9888 - val_loss: 0.0203 - val_acc: 0.9917
Epoch 2/40
 - 14s - loss: 0.0189 - acc: 0.9920 - val_loss: 0.0199 - val_acc: 0.9921
Epoch 3/40
 - 14s - loss: 0.0161 - acc: 0.9927 - val_loss: 0.0212 - val_acc: 0.9918

==================================================================================================
	Training time : 0:02:01.181001
==================================================================================================
	Identification : 0.294
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 85
	5 : 7

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 381
	25 : 1
	5 : 23

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 55011
	After : 41378

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 41378 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 41378
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_427 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_428 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_427 (Embedding)       (None, 4, 307)       12703046    input_427[0][0]                  
__________________________________________________________________________________________________
embedding_428 (Embedding)       (None, 4, 107)       12733       input_428[0][0]                  
__________________________________________________________________________________________________
flatten_427 (Flatten)           (None, 1228)         0           embedding_427[0][0]              
__________________________________________________________________________________________________
flatten_428 (Flatten)           (None, 428)          0           embedding_428[0][0]              
__________________________________________________________________________________________________
concatenate_214 (Concatenate)   (None, 1656)         0           flatten_427[0][0]                
                                                                 flatten_428[0][0]                
__________________________________________________________________________________________________
dense_337 (Dense)               (None, 52)           86164       concatenate_214[0][0]            
__________________________________________________________________________________________________
dropout_124 (Dropout)           (None, 52)           0           dense_337[0][0]                  
__________________________________________________________________________________________________
dense_338 (Dense)               (None, 8)            424         dropout_124[0][0]                
==================================================================================================
Total params: 12,802,367
Trainable params: 12,802,367
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33500, 1.0: 32812, 2.0: 645, 5.0: 333, 6.0: 296})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 13s - loss: 0.0546 - acc: 0.9800 - val_loss: 0.0377 - val_acc: 0.9837
Epoch 2/40
 - 13s - loss: 0.0327 - acc: 0.9856 - val_loss: 0.0382 - val_acc: 0.9840
Epoch 3/40
 - 13s - loss: 0.0286 - acc: 0.9865 - val_loss: 0.0389 - val_acc: 0.9839

==================================================================================================
	Training time : 0:01:39.056158
==================================================================================================
	Identification : 0.228
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 65
	25 : 1
	5 : 2

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 372
	25 : 2
	5 : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 55011
	After : 41357

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 41357 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 41357
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_429 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_430 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_429 (Embedding)       (None, 4, 307)       12696599    input_429[0][0]                  
__________________________________________________________________________________________________
embedding_430 (Embedding)       (None, 4, 107)       12733       input_430[0][0]                  
__________________________________________________________________________________________________
flatten_429 (Flatten)           (None, 1228)         0           embedding_429[0][0]              
__________________________________________________________________________________________________
flatten_430 (Flatten)           (None, 428)          0           embedding_430[0][0]              
__________________________________________________________________________________________________
concatenate_215 (Concatenate)   (None, 1656)         0           flatten_429[0][0]                
                                                                 flatten_430[0][0]                
__________________________________________________________________________________________________
dense_339 (Dense)               (None, 52)           86164       concatenate_215[0][0]            
__________________________________________________________________________________________________
dropout_125 (Dropout)           (None, 52)           0           dense_339[0][0]                  
__________________________________________________________________________________________________
dense_340 (Dense)               (None, 8)            424         dropout_125[0][0]                
==================================================================================================
Total params: 12,795,920
Trainable params: 12,795,920
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33580, 1.0: 32803, 2.0: 597, 5.0: 310, 6.0: 296})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 13s - loss: 0.0574 - acc: 0.9797 - val_loss: 0.0385 - val_acc: 0.9815
Epoch 2/40
 - 13s - loss: 0.0334 - acc: 0.9854 - val_loss: 0.0386 - val_acc: 0.9841
Epoch 3/40
 - 13s - loss: 0.0292 - acc: 0.9864 - val_loss: 0.0412 - val_acc: 0.9838

==================================================================================================
	Training time : 0:01:42.067097
==================================================================================================
	Identification : 0.075
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 8
	25 : 1
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 428
	25 : 2
	5 : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 55011
	After : 41331

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 41331 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 41331
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 307
	POS = 107 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_431 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_432 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_431 (Embedding)       (None, 4, 307)       12688617    input_431[0][0]                  
__________________________________________________________________________________________________
embedding_432 (Embedding)       (None, 4, 107)       12733       input_432[0][0]                  
__________________________________________________________________________________________________
flatten_431 (Flatten)           (None, 1228)         0           embedding_431[0][0]              
__________________________________________________________________________________________________
flatten_432 (Flatten)           (None, 428)          0           embedding_432[0][0]              
__________________________________________________________________________________________________
concatenate_216 (Concatenate)   (None, 1656)         0           flatten_431[0][0]                
                                                                 flatten_432[0][0]                
__________________________________________________________________________________________________
dense_341 (Dense)               (None, 52)           86164       concatenate_216[0][0]            
__________________________________________________________________________________________________
dropout_126 (Dropout)           (None, 52)           0           dense_341[0][0]                  
__________________________________________________________________________________________________
dense_342 (Dense)               (None, 8)            424         dropout_126[0][0]                
==================================================================================================
Total params: 12,787,938
Trainable params: 12,787,938
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33591, 1.0: 32794, 2.0: 632, 6.0: 287, 5.0: 282})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.055
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 13s - loss: 0.0550 - acc: 0.9798 - val_loss: 0.0380 - val_acc: 0.9837
Epoch 2/40
 - 13s - loss: 0.0327 - acc: 0.9854 - val_loss: 0.0378 - val_acc: 0.9845
Epoch 3/40
 - 13s - loss: 0.0289 - acc: 0.9866 - val_loss: 0.0408 - val_acc: 0.9825

==================================================================================================
	Training time : 0:01:38.610121
==================================================================================================
	Identification : 0.05
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 3
	25 : 1
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 433
	25 : 2
	5 : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
7_False_False_233_96_compact_False_47_relu_0.248_False_512_relu_0.2_adagrad_0.022_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_433 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_434 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_433 (Embedding)       (None, 4, 233)       947145      input_433[0][0]                  
__________________________________________________________________________________________________
embedding_434 (Embedding)       (None, 4, 96)        14592       input_434[0][0]                  
__________________________________________________________________________________________________
flatten_433 (Flatten)           (None, 932)          0           embedding_433[0][0]              
__________________________________________________________________________________________________
flatten_434 (Flatten)           (None, 384)          0           embedding_434[0][0]              
__________________________________________________________________________________________________
concatenate_217 (Concatenate)   (None, 1316)         0           flatten_433[0][0]                
                                                                 flatten_434[0][0]                
__________________________________________________________________________________________________
dense_343 (Dense)               (None, 8)            10536       concatenate_217[0][0]            
==================================================================================================
Total params: 972,273
Trainable params: 972,273
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27040, 1.0: 26558, 2.0: 391, 4.0: 183, 6.0: 101, 5.0: 84, 3.0: 4})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0287 - acc: 0.9878 - val_loss: 0.0255 - val_acc: 0.9882
Epoch 2/40
 - 5s - loss: 0.0195 - acc: 0.9899 - val_loss: 0.0264 - val_acc: 0.9863
Epoch 3/40
 - 5s - loss: 0.0174 - acc: 0.9904 - val_loss: 0.0276 - val_acc: 0.9849

==================================================================================================
	Training time : 0:01:06.351712
==================================================================================================
	Identification : 0.164
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 37
	50 : 2
	100 : 1
	5 : 14

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 355
	25 : 7
	50 : 3
	100 : 1
	5 : 52

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_435 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_436 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_435 (Embedding)       (None, 4, 233)       947145      input_435[0][0]                  
__________________________________________________________________________________________________
embedding_436 (Embedding)       (None, 4, 96)        14592       input_436[0][0]                  
__________________________________________________________________________________________________
flatten_435 (Flatten)           (None, 932)          0           embedding_435[0][0]              
__________________________________________________________________________________________________
flatten_436 (Flatten)           (None, 384)          0           embedding_436[0][0]              
__________________________________________________________________________________________________
concatenate_218 (Concatenate)   (None, 1316)         0           flatten_435[0][0]                
                                                                 flatten_436[0][0]                
__________________________________________________________________________________________________
dense_344 (Dense)               (None, 8)            10536       concatenate_218[0][0]            
==================================================================================================
Total params: 972,273
Trainable params: 972,273
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 27133, 1.0: 26493, 2.0: 386, 4.0: 182, 6.0: 95, 5.0: 68, 3.0: 4})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0289 - acc: 0.9875 - val_loss: 0.0237 - val_acc: 0.9891
Epoch 2/40
 - 5s - loss: 0.0197 - acc: 0.9899 - val_loss: 0.0242 - val_acc: 0.9878
Epoch 3/40
 - 5s - loss: 0.0177 - acc: 0.9902 - val_loss: 0.0248 - val_acc: 0.9868

==================================================================================================
	Training time : 0:01:04.105884
==================================================================================================
	Identification : 0.586
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 86
	25 : 6
	50 : 3
	100 : 1
	5 : 44

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 305
	25 : 2
	50 : 1
	100 : 1
	5 : 22

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 25528
	After : 4065

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4065 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 4065
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_437 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_438 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_437 (Embedding)       (None, 4, 233)       947145      input_437[0][0]                  
__________________________________________________________________________________________________
embedding_438 (Embedding)       (None, 4, 96)        14592       input_438[0][0]                  
__________________________________________________________________________________________________
flatten_437 (Flatten)           (None, 932)          0           embedding_437[0][0]              
__________________________________________________________________________________________________
flatten_438 (Flatten)           (None, 384)          0           embedding_438[0][0]              
__________________________________________________________________________________________________
concatenate_219 (Concatenate)   (None, 1316)         0           flatten_437[0][0]                
                                                                 flatten_438[0][0]                
__________________________________________________________________________________________________
dense_345 (Dense)               (None, 8)            10536       concatenate_219[0][0]            
==================================================================================================
Total params: 972,273
Trainable params: 972,273
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 270011, 1.0: 265982, 2.0: 4029, 4.0: 1813, 6.0: 1062, 5.0: 665, 3.0: 43})
	7 Labels in valid : Counter({0.0: 26956, 1.0: 26681, 2.0: 383, 4.0: 156, 6.0: 108, 5.0: 75, 3.0: 2})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 489244 samples, validate on 54361 samples
Epoch 1/40
 - 5s - loss: 0.0289 - acc: 0.9878 - val_loss: 0.0241 - val_acc: 0.9891
Epoch 2/40
 - 5s - loss: 0.0197 - acc: 0.9899 - val_loss: 0.0247 - val_acc: 0.9863
Epoch 3/40
 - 5s - loss: 0.0176 - acc: 0.9903 - val_loss: 0.0258 - val_acc: 0.9867

==================================================================================================
	Training time : 0:01:04.736139
==================================================================================================
	Identification : 0.152
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 28
	25 : 1
	5 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 362
	25 : 6
	50 : 3
	100 : 1
	5 : 53

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_439 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_440 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_439 (Embedding)       (None, 4, 233)       941553      input_439[0][0]                  
__________________________________________________________________________________________________
embedding_440 (Embedding)       (None, 4, 96)        16224       input_440[0][0]                  
__________________________________________________________________________________________________
flatten_439 (Flatten)           (None, 932)          0           embedding_439[0][0]              
__________________________________________________________________________________________________
flatten_440 (Flatten)           (None, 384)          0           embedding_440[0][0]              
__________________________________________________________________________________________________
concatenate_220 (Concatenate)   (None, 1316)         0           flatten_439[0][0]                
                                                                 flatten_440[0][0]                
__________________________________________________________________________________________________
dense_346 (Dense)               (None, 8)            10536       concatenate_220[0][0]            
==================================================================================================
Total params: 968,313
Trainable params: 968,313
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 27113, 1.0: 26599, 2.0: 275, 6.0: 185, 5.0: 46, 4.0: 43})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0246 - acc: 0.9901 - val_loss: 0.0201 - val_acc: 0.9909
Epoch 2/40
 - 5s - loss: 0.0163 - acc: 0.9922 - val_loss: 0.0208 - val_acc: 0.9897
Epoch 3/40
 - 5s - loss: 0.0145 - acc: 0.9927 - val_loss: 0.0216 - val_acc: 0.9882

==================================================================================================
	Training time : 0:01:05.902160
==================================================================================================
	Identification : 0.448
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 109
	5 : 7

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 216
	5 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_441 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_442 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_441 (Embedding)       (None, 4, 233)       941553      input_441[0][0]                  
__________________________________________________________________________________________________
embedding_442 (Embedding)       (None, 4, 96)        16224       input_442[0][0]                  
__________________________________________________________________________________________________
flatten_441 (Flatten)           (None, 932)          0           embedding_441[0][0]              
__________________________________________________________________________________________________
flatten_442 (Flatten)           (None, 384)          0           embedding_442[0][0]              
__________________________________________________________________________________________________
concatenate_221 (Concatenate)   (None, 1316)         0           flatten_441[0][0]                
                                                                 flatten_442[0][0]                
__________________________________________________________________________________________________
dense_347 (Dense)               (None, 8)            10536       concatenate_221[0][0]            
==================================================================================================
Total params: 968,313
Trainable params: 968,313
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({0.0: 26911, 1.0: 26773, 2.0: 316, 6.0: 178, 5.0: 43, 4.0: 40})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0247 - acc: 0.9899 - val_loss: 0.0198 - val_acc: 0.9908
Epoch 2/40
 - 5s - loss: 0.0162 - acc: 0.9923 - val_loss: 0.0203 - val_acc: 0.9899
Epoch 3/40
 - 5s - loss: 0.0145 - acc: 0.9926 - val_loss: 0.0214 - val_acc: 0.9884

==================================================================================================
	Training time : 0:01:07.085326
==================================================================================================
	Identification : 0.401
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 97
	5 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 227
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 26651
	After : 4041

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4041 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 4041
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_443 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_444 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_443 (Embedding)       (None, 4, 233)       941553      input_443[0][0]                  
__________________________________________________________________________________________________
embedding_444 (Embedding)       (None, 4, 96)        16224       input_444[0][0]                  
__________________________________________________________________________________________________
flatten_443 (Flatten)           (None, 932)          0           embedding_443[0][0]              
__________________________________________________________________________________________________
flatten_444 (Flatten)           (None, 384)          0           embedding_444[0][0]              
__________________________________________________________________________________________________
concatenate_222 (Concatenate)   (None, 1316)         0           flatten_443[0][0]                
                                                                 flatten_444[0][0]                
__________________________________________________________________________________________________
dense_348 (Dense)               (None, 8)            10536       concatenate_222[0][0]            
==================================================================================================
Total params: 968,313
Trainable params: 968,313
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 270045, 1.0: 266973, 2.0: 3072, 6.0: 1609, 5.0: 521, 4.0: 388})
	6 Labels in valid : Counter({1.0: 26869, 0.0: 26851, 2.0: 288, 6.0: 173, 5.0: 45, 4.0: 35})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 488347 samples, validate on 54261 samples
Epoch 1/40
 - 5s - loss: 0.0246 - acc: 0.9900 - val_loss: 0.0205 - val_acc: 0.9913
Epoch 2/40
 - 5s - loss: 0.0161 - acc: 0.9924 - val_loss: 0.0209 - val_acc: 0.9891
Epoch 3/40
 - 5s - loss: 0.0145 - acc: 0.9927 - val_loss: 0.0217 - val_acc: 0.9884

==================================================================================================
	Training time : 0:01:05.411360
==================================================================================================
	Identification : 0.368
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 86
	5 : 4

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 237
	5 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_445 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_446 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_445 (Embedding)       (None, 4, 233)       1711152     input_445[0][0]                  
__________________________________________________________________________________________________
embedding_446 (Embedding)       (None, 4, 96)        10560       input_446[0][0]                  
__________________________________________________________________________________________________
flatten_445 (Flatten)           (None, 932)          0           embedding_445[0][0]              
__________________________________________________________________________________________________
flatten_446 (Flatten)           (None, 384)          0           embedding_446[0][0]              
__________________________________________________________________________________________________
concatenate_223 (Concatenate)   (None, 1316)         0           flatten_445[0][0]                
                                                                 flatten_446[0][0]                
__________________________________________________________________________________________________
dense_349 (Dense)               (None, 8)            10536       concatenate_223[0][0]            
==================================================================================================
Total params: 1,732,248
Trainable params: 1,732,248
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 27014, 1.0: 26406, 2.0: 526, 6.0: 272, 5.0: 268})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 5s - loss: 0.0388 - acc: 0.9829 - val_loss: 0.0341 - val_acc: 0.9836
Epoch 2/40
 - 5s - loss: 0.0269 - acc: 0.9852 - val_loss: 0.0362 - val_acc: 0.9805
Epoch 3/40
 - 5s - loss: 0.0237 - acc: 0.9858 - val_loss: 0.0398 - val_acc: 0.9780

==================================================================================================
	Training time : 0:01:05.425566
==================================================================================================
	Identification : 0.414
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 207
	25 : 2
	5 : 24

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 494
	5 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_447 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_448 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_447 (Embedding)       (None, 4, 233)       1711152     input_447[0][0]                  
__________________________________________________________________________________________________
embedding_448 (Embedding)       (None, 4, 96)        10560       input_448[0][0]                  
__________________________________________________________________________________________________
flatten_447 (Flatten)           (None, 932)          0           embedding_447[0][0]              
__________________________________________________________________________________________________
flatten_448 (Flatten)           (None, 384)          0           embedding_448[0][0]              
__________________________________________________________________________________________________
concatenate_224 (Concatenate)   (None, 1316)         0           flatten_447[0][0]                
                                                                 flatten_448[0][0]                
__________________________________________________________________________________________________
dense_350 (Dense)               (None, 8)            10536       concatenate_224[0][0]            
==================================================================================================
Total params: 1,732,248
Trainable params: 1,732,248
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26985, 1.0: 26519, 2.0: 481, 5.0: 258, 6.0: 243})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 5s - loss: 0.0386 - acc: 0.9827 - val_loss: 0.0413 - val_acc: 0.9795
Epoch 2/40
 - 5s - loss: 0.0268 - acc: 0.9853 - val_loss: 0.0405 - val_acc: 0.9792
Epoch 3/40
 - 5s - loss: 0.0237 - acc: 0.9858 - val_loss: 0.0426 - val_acc: 0.9804

==================================================================================================
	Training time : 0:01:05.103003
==================================================================================================
	Identification : 0.346
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 172
	25 : 1
	5 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 528
	25 : 1
	5 : 21

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 48374
	After : 7344

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7344 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 7344
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_449 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_450 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_449 (Embedding)       (None, 4, 233)       1711152     input_449[0][0]                  
__________________________________________________________________________________________________
embedding_450 (Embedding)       (None, 4, 96)        10560       input_450[0][0]                  
__________________________________________________________________________________________________
flatten_449 (Flatten)           (None, 932)          0           embedding_449[0][0]              
__________________________________________________________________________________________________
flatten_450 (Flatten)           (None, 384)          0           embedding_450[0][0]              
__________________________________________________________________________________________________
concatenate_225 (Concatenate)   (None, 1316)         0           flatten_449[0][0]                
                                                                 flatten_450[0][0]                
__________________________________________________________________________________________________
dense_351 (Dense)               (None, 8)            10536       concatenate_225[0][0]            
==================================================================================================
Total params: 1,732,248
Trainable params: 1,732,248
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 270001, 1.0: 264857, 2.0: 5144, 5.0: 2513, 6.0: 2343})
	5 Labels in valid : Counter({0.0: 26806, 1.0: 26635, 2.0: 554, 6.0: 249, 5.0: 242})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 490372 samples, validate on 54486 samples
Epoch 1/40
 - 5s - loss: 0.0386 - acc: 0.9828 - val_loss: 0.0331 - val_acc: 0.9847
Epoch 2/40
 - 5s - loss: 0.0269 - acc: 0.9853 - val_loss: 0.0356 - val_acc: 0.9815
Epoch 3/40
 - 5s - loss: 0.0238 - acc: 0.9857 - val_loss: 0.0385 - val_acc: 0.9776

==================================================================================================
	Training time : 0:01:07.024173
==================================================================================================
	Identification : 0.422
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 239
	5 : 26

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 464
	25 : 2
	5 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_451 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_452 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_451 (Embedding)       (None, 4, 233)       1239793     input_451[0][0]                  
__________________________________________________________________________________________________
embedding_452 (Embedding)       (None, 4, 96)        16128       input_452[0][0]                  
__________________________________________________________________________________________________
flatten_451 (Flatten)           (None, 932)          0           embedding_451[0][0]              
__________________________________________________________________________________________________
flatten_452 (Flatten)           (None, 384)          0           embedding_452[0][0]              
__________________________________________________________________________________________________
concatenate_226 (Concatenate)   (None, 1316)         0           flatten_451[0][0]                
                                                                 flatten_452[0][0]                
__________________________________________________________________________________________________
dense_352 (Dense)               (None, 8)            10536       concatenate_226[0][0]            
==================================================================================================
Total params: 1,266,457
Trainable params: 1,266,457
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39686, 1.0: 39557, 2.0: 581, 4.0: 292, 6.0: 151, 5.0: 99, 3.0: 5})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 7s - loss: 0.0276 - acc: 0.9877 - val_loss: 0.0239 - val_acc: 0.9888
Epoch 2/40
 - 8s - loss: 0.0196 - acc: 0.9897 - val_loss: 0.0247 - val_acc: 0.9862
Epoch 3/40
 - 8s - loss: 0.0178 - acc: 0.9902 - val_loss: 0.0259 - val_acc: 0.9867

==================================================================================================
	Training time : 0:01:27.045898
==================================================================================================
	Identification : 0.434
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 90
	200 : 1
	50 : 4
	5 : 41
	25 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 365
	25 : 5
	50 : 4
	5 : 31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_453 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_454 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_453 (Embedding)       (None, 4, 233)       1239793     input_453[0][0]                  
__________________________________________________________________________________________________
embedding_454 (Embedding)       (None, 4, 96)        16128       input_454[0][0]                  
__________________________________________________________________________________________________
flatten_453 (Flatten)           (None, 932)          0           embedding_453[0][0]              
__________________________________________________________________________________________________
flatten_454 (Flatten)           (None, 384)          0           embedding_454[0][0]              
__________________________________________________________________________________________________
concatenate_227 (Concatenate)   (None, 1316)         0           flatten_453[0][0]                
                                                                 flatten_454[0][0]                
__________________________________________________________________________________________________
dense_353 (Dense)               (None, 8)            10536       concatenate_227[0][0]            
==================================================================================================
Total params: 1,266,457
Trainable params: 1,266,457
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 39989, 1.0: 39240, 2.0: 621, 4.0: 268, 6.0: 143, 5.0: 100, 3.0: 10})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 8s - loss: 0.0274 - acc: 0.9877 - val_loss: 0.0234 - val_acc: 0.9890
Epoch 2/40
 - 8s - loss: 0.0196 - acc: 0.9897 - val_loss: 0.0240 - val_acc: 0.9866
Epoch 3/40
 - 8s - loss: 0.0178 - acc: 0.9901 - val_loss: 0.0248 - val_acc: 0.9861

==================================================================================================
	Training time : 0:01:30.784976
==================================================================================================
	Identification : 0.094
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 13
	25 : 3
	5 : 12

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 439
	200 : 1
	50 : 5
	5 : 55
	25 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 31203
	After : 5321

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5321 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 5321
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_455 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_456 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_455 (Embedding)       (None, 4, 233)       1239793     input_455[0][0]                  
__________________________________________________________________________________________________
embedding_456 (Embedding)       (None, 4, 96)        16128       input_456[0][0]                  
__________________________________________________________________________________________________
flatten_455 (Flatten)           (None, 932)          0           embedding_455[0][0]              
__________________________________________________________________________________________________
flatten_456 (Flatten)           (None, 384)          0           embedding_456[0][0]              
__________________________________________________________________________________________________
concatenate_228 (Concatenate)   (None, 1316)         0           flatten_455[0][0]                
                                                                 flatten_456[0][0]                
__________________________________________________________________________________________________
dense_354 (Dense)               (None, 8)            10536       concatenate_228[0][0]            
==================================================================================================
Total params: 1,266,457
Trainable params: 1,266,457
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	7 Labels in train : Counter({0.0: 399173, 1.0: 393143, 2.0: 6030, 4.0: 2729, 6.0: 1556, 5.0: 1005, 3.0: 74})
	7 Labels in valid : Counter({0.0: 40057, 1.0: 39215, 2.0: 581, 4.0: 258, 6.0: 154, 5.0: 95, 3.0: 11})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 723339 samples, validate on 80371 samples
Epoch 1/40
 - 8s - loss: 0.0276 - acc: 0.9879 - val_loss: 0.0225 - val_acc: 0.9888
Epoch 2/40
 - 8s - loss: 0.0197 - acc: 0.9898 - val_loss: 0.0228 - val_acc: 0.9878
Epoch 3/40
 - 8s - loss: 0.0179 - acc: 0.9902 - val_loss: 0.0239 - val_acc: 0.9871

==================================================================================================
	Training time : 0:01:29.219475
==================================================================================================
	Identification : 0.363
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 84
	25 : 4
	50 : 3
	5 : 31

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 373
	200 : 1
	50 : 4
	5 : 40
	25 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_457 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_458 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_457 (Embedding)       (None, 4, 233)       1392641     input_457[0][0]                  
__________________________________________________________________________________________________
embedding_458 (Embedding)       (None, 4, 96)        21504       input_458[0][0]                  
__________________________________________________________________________________________________
flatten_457 (Flatten)           (None, 932)          0           embedding_457[0][0]              
__________________________________________________________________________________________________
flatten_458 (Flatten)           (None, 384)          0           embedding_458[0][0]              
__________________________________________________________________________________________________
concatenate_229 (Concatenate)   (None, 1316)         0           flatten_457[0][0]                
                                                                 flatten_458[0][0]                
__________________________________________________________________________________________________
dense_355 (Dense)               (None, 8)            10536       concatenate_229[0][0]            
==================================================================================================
Total params: 1,424,681
Trainable params: 1,424,681
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47256, 1.0: 46887, 2.0: 540, 6.0: 293, 5.0: 83, 4.0: 64})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 10s - loss: 0.0226 - acc: 0.9905 - val_loss: 0.0186 - val_acc: 0.9917
Epoch 2/40
 - 9s - loss: 0.0161 - acc: 0.9923 - val_loss: 0.0193 - val_acc: 0.9913
Epoch 3/40
 - 9s - loss: 0.0146 - acc: 0.9925 - val_loss: 0.0198 - val_acc: 0.9895

==================================================================================================
	Training time : 0:01:43.250601
==================================================================================================
	Identification : 0.365
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 171
	5 : 17

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 295
	25 : 1
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_459 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_460 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_459 (Embedding)       (None, 4, 233)       1392641     input_459[0][0]                  
__________________________________________________________________________________________________
embedding_460 (Embedding)       (None, 4, 96)        21504       input_460[0][0]                  
__________________________________________________________________________________________________
flatten_459 (Flatten)           (None, 932)          0           embedding_459[0][0]              
__________________________________________________________________________________________________
flatten_460 (Flatten)           (None, 384)          0           embedding_460[0][0]              
__________________________________________________________________________________________________
concatenate_230 (Concatenate)   (None, 1316)         0           flatten_459[0][0]                
                                                                 flatten_460[0][0]                
__________________________________________________________________________________________________
dense_356 (Dense)               (None, 8)            10536       concatenate_230[0][0]            
==================================================================================================
Total params: 1,424,681
Trainable params: 1,424,681
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47202, 1.0: 47031, 2.0: 508, 6.0: 238, 5.0: 82, 4.0: 62})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 10s - loss: 0.0226 - acc: 0.9904 - val_loss: 0.0184 - val_acc: 0.9919
Epoch 2/40
 - 10s - loss: 0.0161 - acc: 0.9922 - val_loss: 0.0185 - val_acc: 0.9910
Epoch 3/40
 - 10s - loss: 0.0147 - acc: 0.9925 - val_loss: 0.0193 - val_acc: 0.9897

==================================================================================================
	Training time : 0:01:43.316653
==================================================================================================
	Identification : 0.319
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 106
	5 : 9

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 359
	25 : 1
	5 : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 36575
	After : 5977

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5977 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 5977
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_461 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_462 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_461 (Embedding)       (None, 4, 233)       1392641     input_461[0][0]                  
__________________________________________________________________________________________________
embedding_462 (Embedding)       (None, 4, 96)        21504       input_462[0][0]                  
__________________________________________________________________________________________________
flatten_461 (Flatten)           (None, 932)          0           embedding_461[0][0]              
__________________________________________________________________________________________________
flatten_462 (Flatten)           (None, 384)          0           embedding_462[0][0]              
__________________________________________________________________________________________________
concatenate_231 (Concatenate)   (None, 1316)         0           flatten_461[0][0]                
                                                                 flatten_462[0][0]                
__________________________________________________________________________________________________
dense_357 (Dense)               (None, 8)            10536       concatenate_231[0][0]            
==================================================================================================
Total params: 1,424,681
Trainable params: 1,424,681
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	6 Labels in train : Counter({0.0: 473446, 1.0: 468193, 2.0: 5253, 6.0: 2770, 5.0: 879, 4.0: 688})
	6 Labels in valid : Counter({0.0: 47390, 1.0: 46754, 2.0: 540, 6.0: 266, 5.0: 91, 4.0: 82})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 856106 samples, validate on 95123 samples
Epoch 1/40
 - 10s - loss: 0.0225 - acc: 0.9906 - val_loss: 0.0194 - val_acc: 0.9909
Epoch 2/40
 - 10s - loss: 0.0160 - acc: 0.9923 - val_loss: 0.0195 - val_acc: 0.9898
Epoch 3/40
 - 10s - loss: 0.0145 - acc: 0.9926 - val_loss: 0.0204 - val_acc: 0.9889

==================================================================================================
	Training time : 0:01:43.228522
==================================================================================================
	Identification : 0.397
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 132
	25 : 1
	5 : 14

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 334
	5 : 17

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_463 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_464 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_463 (Embedding)       (None, 4, 233)       2031294     input_463[0][0]                  
__________________________________________________________________________________________________
embedding_464 (Embedding)       (None, 4, 96)        11424       input_464[0][0]                  
__________________________________________________________________________________________________
flatten_463 (Flatten)           (None, 932)          0           embedding_463[0][0]              
__________________________________________________________________________________________________
flatten_464 (Flatten)           (None, 384)          0           embedding_464[0][0]              
__________________________________________________________________________________________________
concatenate_232 (Concatenate)   (None, 1316)         0           flatten_463[0][0]                
                                                                 flatten_464[0][0]                
__________________________________________________________________________________________________
dense_358 (Dense)               (None, 8)            10536       concatenate_232[0][0]            
==================================================================================================
Total params: 2,053,254
Trainable params: 2,053,254
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33367, 1.0: 32972, 2.0: 674, 5.0: 300, 6.0: 273})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0383 - acc: 0.9827 - val_loss: 0.0327 - val_acc: 0.9844
Epoch 2/40
 - 7s - loss: 0.0273 - acc: 0.9848 - val_loss: 0.0349 - val_acc: 0.9835
Epoch 3/40
 - 6s - loss: 0.0243 - acc: 0.9855 - val_loss: 0.0376 - val_acc: 0.9778

==================================================================================================
	Training time : 0:01:25.331043
==================================================================================================
	Identification : 0.366
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 115
	25 : 1
	5 : 10

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 321
	25 : 2
	5 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_465 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_466 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_465 (Embedding)       (None, 4, 233)       2031294     input_465[0][0]                  
__________________________________________________________________________________________________
embedding_466 (Embedding)       (None, 4, 96)        11424       input_466[0][0]                  
__________________________________________________________________________________________________
flatten_465 (Flatten)           (None, 932)          0           embedding_465[0][0]              
__________________________________________________________________________________________________
flatten_466 (Flatten)           (None, 384)          0           embedding_466[0][0]              
__________________________________________________________________________________________________
concatenate_233 (Concatenate)   (None, 1316)         0           flatten_465[0][0]                
                                                                 flatten_466[0][0]                
__________________________________________________________________________________________________
dense_359 (Dense)               (None, 8)            10536       concatenate_233[0][0]            
==================================================================================================
Total params: 2,053,254
Trainable params: 2,053,254
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33256, 1.0: 33037, 2.0: 648, 5.0: 342, 6.0: 303})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0382 - acc: 0.9829 - val_loss: 0.0344 - val_acc: 0.9837
Epoch 2/40
 - 7s - loss: 0.0272 - acc: 0.9850 - val_loss: 0.0358 - val_acc: 0.9812
Epoch 3/40
 - 7s - loss: 0.0242 - acc: 0.9855 - val_loss: 0.0403 - val_acc: 0.9782

==================================================================================================
	Training time : 0:01:17.419679
==================================================================================================
	Identification : 0.362
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 124
	25 : 2
	5 : 10

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 312
	25 : 2
	5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 55011
	After : 8718

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8718 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 8718
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 233
	POS = 96 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_467 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_468 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_467 (Embedding)       (None, 4, 233)       2031294     input_467[0][0]                  
__________________________________________________________________________________________________
embedding_468 (Embedding)       (None, 4, 96)        11424       input_468[0][0]                  
__________________________________________________________________________________________________
flatten_467 (Flatten)           (None, 932)          0           embedding_467[0][0]              
__________________________________________________________________________________________________
flatten_468 (Flatten)           (None, 384)          0           embedding_468[0][0]              
__________________________________________________________________________________________________
concatenate_234 (Concatenate)   (None, 1316)         0           flatten_467[0][0]                
                                                                 flatten_468[0][0]                
__________________________________________________________________________________________________
dense_360 (Dense)               (None, 8)            10536       concatenate_234[0][0]            
==================================================================================================
Total params: 2,053,254
Trainable params: 2,053,254
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5 Labels in train : Counter({0.0: 334880, 1.0: 328423, 2.0: 6457, 5.0: 3157, 6.0: 2934})
	5 Labels in valid : Counter({0.0: 33691, 1.0: 32623, 2.0: 653, 6.0: 311, 5.0: 308})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.022
__________________________________________________________________________________________________
Train on 608265 samples, validate on 67586 samples
Epoch 1/40
 - 7s - loss: 0.0385 - acc: 0.9827 - val_loss: 0.0336 - val_acc: 0.9834
Epoch 2/40
 - 7s - loss: 0.0273 - acc: 0.9848 - val_loss: 0.0360 - val_acc: 0.9819
Epoch 3/40
 - 7s - loss: 0.0243 - acc: 0.9855 - val_loss: 0.0386 - val_acc: 0.9787

==================================================================================================
	Training time : 0:01:17.386525
==================================================================================================
	Identification : 0.4
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 124
	25 : 3
	5 : 13

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 313
	25 : 2
	5 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
26_True_False_208_30_frequent_False_37_relu_0.229_False_512_relu_0.2_adagrad_0.011_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19950

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19950 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19950
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_469 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_470 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_469 (Embedding)       (None, 4, 208)       4149600     input_469[0][0]                  
__________________________________________________________________________________________________
embedding_470 (Embedding)       (None, 4, 30)        4560        input_470[0][0]                  
__________________________________________________________________________________________________
flatten_469 (Flatten)           (None, 832)          0           embedding_469[0][0]              
__________________________________________________________________________________________________
flatten_470 (Flatten)           (None, 120)          0           embedding_470[0][0]              
__________________________________________________________________________________________________
concatenate_235 (Concatenate)   (None, 952)          0           flatten_469[0][0]                
                                                                 flatten_470[0][0]                
__________________________________________________________________________________________________
dense_361 (Dense)               (None, 8)            7624        concatenate_235[0][0]            
==================================================================================================
Total params: 4,161,784
Trainable params: 4,161,784
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34236, 1.0: 26917, 2.0: 7788, 6.0: 2559, 4.0: 2141, 5.0: 1769, 3.0: 120})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 8s - loss: 0.0638 - acc: 0.9834 - val_loss: 0.0333 - val_acc: 0.9915
Epoch 2/40
 - 8s - loss: 0.0284 - acc: 0.9922 - val_loss: 0.0320 - val_acc: 0.9917
Epoch 3/40
 - 8s - loss: 0.0255 - acc: 0.9927 - val_loss: 0.0322 - val_acc: 0.9920

==================================================================================================
	Training time : 0:01:19.182419
==================================================================================================
	Identification : 0.644
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 137
	25 : 6
	50 : 3
	100 : 1
	5 : 57

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 255
	25 : 7
	50 : 1
	100 : 1
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19870

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19870 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19870
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_471 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_472 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_471 (Embedding)       (None, 4, 208)       4132960     input_471[0][0]                  
__________________________________________________________________________________________________
embedding_472 (Embedding)       (None, 4, 30)        4560        input_472[0][0]                  
__________________________________________________________________________________________________
flatten_471 (Flatten)           (None, 832)          0           embedding_471[0][0]              
__________________________________________________________________________________________________
flatten_472 (Flatten)           (None, 120)          0           embedding_472[0][0]              
__________________________________________________________________________________________________
concatenate_236 (Concatenate)   (None, 952)          0           flatten_471[0][0]                
                                                                 flatten_472[0][0]                
__________________________________________________________________________________________________
dense_362 (Dense)               (None, 8)            7624        concatenate_236[0][0]            
==================================================================================================
Total params: 4,145,144
Trainable params: 4,145,144
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34574, 1.0: 26767, 2.0: 7679, 6.0: 2579, 4.0: 2101, 5.0: 1713, 3.0: 117})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 8s - loss: 0.0642 - acc: 0.9830 - val_loss: 0.0349 - val_acc: 0.9909
Epoch 2/40
 - 8s - loss: 0.0283 - acc: 0.9922 - val_loss: 0.0336 - val_acc: 0.9913
Epoch 3/40
 - 8s - loss: 0.0254 - acc: 0.9928 - val_loss: 0.0340 - val_acc: 0.9914

==================================================================================================
	Training time : 0:01:18.686253
==================================================================================================
	Identification : 0.612
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 138
	25 : 7
	50 : 3
	100 : 1
	5 : 55

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 252
	25 : 5
	50 : 3
	100 : 1
	5 : 19

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19918

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19918 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19918
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_473 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_474 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_473 (Embedding)       (None, 4, 208)       4142944     input_473[0][0]                  
__________________________________________________________________________________________________
embedding_474 (Embedding)       (None, 4, 30)        4560        input_474[0][0]                  
__________________________________________________________________________________________________
flatten_473 (Flatten)           (None, 832)          0           embedding_473[0][0]              
__________________________________________________________________________________________________
flatten_474 (Flatten)           (None, 120)          0           embedding_474[0][0]              
__________________________________________________________________________________________________
concatenate_237 (Concatenate)   (None, 952)          0           flatten_473[0][0]                
                                                                 flatten_474[0][0]                
__________________________________________________________________________________________________
dense_363 (Dense)               (None, 8)            7624        concatenate_237[0][0]            
==================================================================================================
Total params: 4,155,128
Trainable params: 4,155,128
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34326, 1.0: 26788, 2.0: 7816, 6.0: 2626, 4.0: 2111, 5.0: 1743, 3.0: 120})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 8s - loss: 0.0640 - acc: 0.9831 - val_loss: 0.0321 - val_acc: 0.9916
Epoch 2/40
 - 8s - loss: 0.0285 - acc: 0.9921 - val_loss: 0.0309 - val_acc: 0.9918
Epoch 3/40
 - 8s - loss: 0.0256 - acc: 0.9927 - val_loss: 0.0309 - val_acc: 0.9920

==================================================================================================
	Training time : 0:01:18.188667
==================================================================================================
	Identification : 0.644
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 138
	25 : 6
	50 : 3
	100 : 1
	5 : 54

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 252
	25 : 6
	50 : 2
	100 : 1
	5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20259

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20259 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20259
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_475 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_476 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_475 (Embedding)       (None, 4, 208)       4213872     input_475[0][0]                  
__________________________________________________________________________________________________
embedding_476 (Embedding)       (None, 4, 30)        5070        input_476[0][0]                  
__________________________________________________________________________________________________
flatten_475 (Flatten)           (None, 832)          0           embedding_475[0][0]              
__________________________________________________________________________________________________
flatten_476 (Flatten)           (None, 120)          0           embedding_476[0][0]              
__________________________________________________________________________________________________
concatenate_238 (Concatenate)   (None, 952)          0           flatten_475[0][0]                
                                                                 flatten_476[0][0]                
__________________________________________________________________________________________________
dense_364 (Dense)               (None, 8)            7624        concatenate_238[0][0]            
==================================================================================================
Total params: 4,226,566
Trainable params: 4,226,566
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 34868, 1.0: 26793, 2.0: 8392, 6.0: 4738, 5.0: 1403, 4.0: 924})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 8s - loss: 0.0582 - acc: 0.9849 - val_loss: 0.0333 - val_acc: 0.9922
Epoch 2/40
 - 8s - loss: 0.0264 - acc: 0.9932 - val_loss: 0.0326 - val_acc: 0.9924
Epoch 3/40
 - 8s - loss: 0.0238 - acc: 0.9938 - val_loss: 0.0328 - val_acc: 0.9926

==================================================================================================
	Training time : 0:01:18.845747
==================================================================================================
	Identification : 0.393
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 71
	5 : 9

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 251
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20424

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20424 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20424
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_477 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_478 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_477 (Embedding)       (None, 4, 208)       4248192     input_477[0][0]                  
__________________________________________________________________________________________________
embedding_478 (Embedding)       (None, 4, 30)        5070        input_478[0][0]                  
__________________________________________________________________________________________________
flatten_477 (Flatten)           (None, 832)          0           embedding_477[0][0]              
__________________________________________________________________________________________________
flatten_478 (Flatten)           (None, 120)          0           embedding_478[0][0]              
__________________________________________________________________________________________________
concatenate_239 (Concatenate)   (None, 952)          0           flatten_477[0][0]                
                                                                 flatten_478[0][0]                
__________________________________________________________________________________________________
dense_365 (Dense)               (None, 8)            7624        concatenate_239[0][0]            
==================================================================================================
Total params: 4,260,886
Trainable params: 4,260,886
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 35252, 1.0: 26683, 2.0: 8341, 6.0: 4538, 5.0: 1388, 4.0: 916})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 9s - loss: 0.0586 - acc: 0.9848 - val_loss: 0.0319 - val_acc: 0.9922
Epoch 2/40
 - 9s - loss: 0.0267 - acc: 0.9932 - val_loss: 0.0305 - val_acc: 0.9926
Epoch 3/40
 - 9s - loss: 0.0240 - acc: 0.9937 - val_loss: 0.0306 - val_acc: 0.9928

==================================================================================================
	Training time : 0:01:19.127563
==================================================================================================
	Identification : 0.394
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 70
	5 : 9

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 251
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20435

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20435 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20435
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_479 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_480 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_479 (Embedding)       (None, 4, 208)       4250480     input_479[0][0]                  
__________________________________________________________________________________________________
embedding_480 (Embedding)       (None, 4, 30)        5070        input_480[0][0]                  
__________________________________________________________________________________________________
flatten_479 (Flatten)           (None, 832)          0           embedding_479[0][0]              
__________________________________________________________________________________________________
flatten_480 (Flatten)           (None, 120)          0           embedding_480[0][0]              
__________________________________________________________________________________________________
concatenate_240 (Concatenate)   (None, 952)          0           flatten_479[0][0]                
                                                                 flatten_480[0][0]                
__________________________________________________________________________________________________
dense_366 (Dense)               (None, 8)            7624        concatenate_240[0][0]            
==================================================================================================
Total params: 4,263,174
Trainable params: 4,263,174
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 35023, 1.0: 26847, 2.0: 8321, 6.0: 4636, 5.0: 1389, 4.0: 902})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 8s - loss: 0.0587 - acc: 0.9848 - val_loss: 0.0328 - val_acc: 0.9919
Epoch 2/40
 - 8s - loss: 0.0266 - acc: 0.9931 - val_loss: 0.0317 - val_acc: 0.9924
Epoch 3/40
 - 8s - loss: 0.0240 - acc: 0.9938 - val_loss: 0.0319 - val_acc: 0.9926

==================================================================================================
	Training time : 0:01:18.971176
==================================================================================================
	Identification : 0.407
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 74
	5 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 247
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35965

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35965 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35965
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_481 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_482 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_481 (Embedding)       (None, 4, 208)       7480720     input_481[0][0]                  
__________________________________________________________________________________________________
embedding_482 (Embedding)       (None, 4, 30)        3300        input_482[0][0]                  
__________________________________________________________________________________________________
flatten_481 (Flatten)           (None, 832)          0           embedding_481[0][0]              
__________________________________________________________________________________________________
flatten_482 (Flatten)           (None, 120)          0           embedding_482[0][0]              
__________________________________________________________________________________________________
concatenate_241 (Concatenate)   (None, 952)          0           flatten_481[0][0]                
                                                                 flatten_482[0][0]                
__________________________________________________________________________________________________
dense_367 (Dense)               (None, 8)            7624        concatenate_241[0][0]            
==================================================================================================
Total params: 7,491,644
Trainable params: 7,491,644
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40372, 1.0: 26774, 2.0: 13875, 5.0: 7046, 6.0: 6299})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 13s - loss: 0.0619 - acc: 0.9848 - val_loss: 0.0373 - val_acc: 0.9915
Epoch 2/40
 - 13s - loss: 0.0321 - acc: 0.9920 - val_loss: 0.0384 - val_acc: 0.9916
Epoch 3/40
 - 13s - loss: 0.0301 - acc: 0.9924 - val_loss: 0.0394 - val_acc: 0.9917

==================================================================================================
	Training time : 0:01:35.752386
==================================================================================================
	Identification : 0.423
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 155
	25 : 1
	5 : 30

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 546
	25 : 2
	5 : 6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35818

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35818 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35818
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_483 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_484 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_483 (Embedding)       (None, 4, 208)       7450144     input_483[0][0]                  
__________________________________________________________________________________________________
embedding_484 (Embedding)       (None, 4, 30)        3300        input_484[0][0]                  
__________________________________________________________________________________________________
flatten_483 (Flatten)           (None, 832)          0           embedding_483[0][0]              
__________________________________________________________________________________________________
flatten_484 (Flatten)           (None, 120)          0           embedding_484[0][0]              
__________________________________________________________________________________________________
concatenate_242 (Concatenate)   (None, 952)          0           flatten_483[0][0]                
                                                                 flatten_484[0][0]                
__________________________________________________________________________________________________
dense_368 (Dense)               (None, 8)            7624        concatenate_242[0][0]            
==================================================================================================
Total params: 7,461,068
Trainable params: 7,461,068
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40512, 1.0: 26847, 2.0: 13880, 5.0: 6775, 6.0: 6352})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 13s - loss: 0.0618 - acc: 0.9847 - val_loss: 0.0395 - val_acc: 0.9910
Epoch 2/40
 - 13s - loss: 0.0319 - acc: 0.9921 - val_loss: 0.0406 - val_acc: 0.9908
Epoch 3/40
 - 13s - loss: 0.0299 - acc: 0.9925 - val_loss: 0.0418 - val_acc: 0.9909

==================================================================================================
	Training time : 0:01:36.086264
==================================================================================================
	Identification : 0.412
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 149
	25 : 1
	5 : 30

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 550
	25 : 2
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35723

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35723 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35723
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_485 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_486 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_485 (Embedding)       (None, 4, 208)       7430384     input_485[0][0]                  
__________________________________________________________________________________________________
embedding_486 (Embedding)       (None, 4, 30)        3300        input_486[0][0]                  
__________________________________________________________________________________________________
flatten_485 (Flatten)           (None, 832)          0           embedding_485[0][0]              
__________________________________________________________________________________________________
flatten_486 (Flatten)           (None, 120)          0           embedding_486[0][0]              
__________________________________________________________________________________________________
concatenate_243 (Concatenate)   (None, 952)          0           flatten_485[0][0]                
                                                                 flatten_486[0][0]                
__________________________________________________________________________________________________
dense_369 (Dense)               (None, 8)            7624        concatenate_243[0][0]            
==================================================================================================
Total params: 7,441,308
Trainable params: 7,441,308
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40890, 1.0: 26610, 2.0: 13804, 5.0: 6808, 6.0: 6254})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 13s - loss: 0.0621 - acc: 0.9847 - val_loss: 0.0385 - val_acc: 0.9913
Epoch 2/40
 - 13s - loss: 0.0320 - acc: 0.9920 - val_loss: 0.0399 - val_acc: 0.9914
Epoch 3/40
 - 13s - loss: 0.0300 - acc: 0.9925 - val_loss: 0.0413 - val_acc: 0.9914

==================================================================================================
	Training time : 0:01:38.529594
==================================================================================================
	Identification : 0.421
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 150
	25 : 1
	5 : 31

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 549
	25 : 2
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24724

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24724 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24724
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_487 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_488 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_487 (Embedding)       (None, 4, 208)       5142592     input_487[0][0]                  
__________________________________________________________________________________________________
embedding_488 (Embedding)       (None, 4, 30)        5040        input_488[0][0]                  
__________________________________________________________________________________________________
flatten_487 (Flatten)           (None, 832)          0           embedding_487[0][0]              
__________________________________________________________________________________________________
flatten_488 (Flatten)           (None, 120)          0           embedding_488[0][0]              
__________________________________________________________________________________________________
concatenate_244 (Concatenate)   (None, 952)          0           flatten_487[0][0]                
                                                                 flatten_488[0][0]                
__________________________________________________________________________________________________
dense_370 (Dense)               (None, 8)            7624        concatenate_244[0][0]            
==================================================================================================
Total params: 5,155,256
Trainable params: 5,155,256
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 50000, 1.0: 39471, 2.0: 10440, 6.0: 3502, 4.0: 2736, 5.0: 2576, 3.0: 149})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 13s - loss: 0.0594 - acc: 0.9837 - val_loss: 0.0327 - val_acc: 0.9913
Epoch 2/40
 - 13s - loss: 0.0288 - acc: 0.9917 - val_loss: 0.0315 - val_acc: 0.9916
Epoch 3/40
 - 13s - loss: 0.0261 - acc: 0.9922 - val_loss: 0.0317 - val_acc: 0.9915

==================================================================================================
	Training time : 0:01:50.199619
==================================================================================================
	Identification : 0.465
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 108
	200 : 1
	50 : 3
	5 : 50
	25 : 9

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 347
	25 : 7
	50 : 4
	200 : 1
	5 : 23

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24666

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24666 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24666
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_489 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_490 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_489 (Embedding)       (None, 4, 208)       5130528     input_489[0][0]                  
__________________________________________________________________________________________________
embedding_490 (Embedding)       (None, 4, 30)        5040        input_490[0][0]                  
__________________________________________________________________________________________________
flatten_489 (Flatten)           (None, 832)          0           embedding_489[0][0]              
__________________________________________________________________________________________________
flatten_490 (Flatten)           (None, 120)          0           embedding_490[0][0]              
__________________________________________________________________________________________________
concatenate_245 (Concatenate)   (None, 952)          0           flatten_489[0][0]                
                                                                 flatten_490[0][0]                
__________________________________________________________________________________________________
dense_371 (Dense)               (None, 8)            7624        concatenate_245[0][0]            
==================================================================================================
Total params: 5,143,192
Trainable params: 5,143,192
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 50148, 1.0: 39495, 2.0: 10498, 6.0: 3419, 4.0: 2677, 5.0: 2515, 3.0: 122})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 13s - loss: 0.0592 - acc: 0.9837 - val_loss: 0.0331 - val_acc: 0.9908
Epoch 2/40
 - 13s - loss: 0.0288 - acc: 0.9917 - val_loss: 0.0318 - val_acc: 0.9911
Epoch 3/40
 - 13s - loss: 0.0261 - acc: 0.9923 - val_loss: 0.0319 - val_acc: 0.9910

==================================================================================================
	Training time : 0:01:53.642929
==================================================================================================
	Identification : 0.474
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 107
	25 : 9
	50 : 5
	200 : 1
	5 : 50

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 347
	200 : 1
	50 : 1
	5 : 25
	25 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24727

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24727 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24727
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_491 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_492 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_491 (Embedding)       (None, 4, 208)       5143216     input_491[0][0]                  
__________________________________________________________________________________________________
embedding_492 (Embedding)       (None, 4, 30)        5040        input_492[0][0]                  
__________________________________________________________________________________________________
flatten_491 (Flatten)           (None, 832)          0           embedding_491[0][0]              
__________________________________________________________________________________________________
flatten_492 (Flatten)           (None, 120)          0           embedding_492[0][0]              
__________________________________________________________________________________________________
concatenate_246 (Concatenate)   (None, 952)          0           flatten_491[0][0]                
                                                                 flatten_492[0][0]                
__________________________________________________________________________________________________
dense_372 (Dense)               (None, 8)            7624        concatenate_246[0][0]            
==================================================================================================
Total params: 5,155,880
Trainable params: 5,155,880
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 50049, 1.0: 39464, 2.0: 10551, 6.0: 3493, 4.0: 2684, 5.0: 2498, 3.0: 135})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 14s - loss: 0.0591 - acc: 0.9837 - val_loss: 0.0342 - val_acc: 0.9907
Epoch 2/40
 - 13s - loss: 0.0286 - acc: 0.9917 - val_loss: 0.0330 - val_acc: 0.9909
Epoch 3/40
 - 13s - loss: 0.0260 - acc: 0.9923 - val_loss: 0.0331 - val_acc: 0.9910

==================================================================================================
	Training time : 0:01:52.824402
==================================================================================================
	Identification : 0.478
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 110
	200 : 1
	50 : 3
	5 : 53
	25 : 7

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 346
	25 : 8
	50 : 4
	200 : 1
	5 : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28435

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28435 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28435
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_493 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_494 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_493 (Embedding)       (None, 4, 208)       5914480     input_493[0][0]                  
__________________________________________________________________________________________________
embedding_494 (Embedding)       (None, 4, 30)        6720        input_494[0][0]                  
__________________________________________________________________________________________________
flatten_493 (Flatten)           (None, 832)          0           embedding_493[0][0]              
__________________________________________________________________________________________________
flatten_494 (Flatten)           (None, 120)          0           embedding_494[0][0]              
__________________________________________________________________________________________________
concatenate_247 (Concatenate)   (None, 952)          0           flatten_493[0][0]                
                                                                 flatten_494[0][0]                
__________________________________________________________________________________________________
dense_373 (Dense)               (None, 8)            7624        concatenate_247[0][0]            
==================================================================================================
Total params: 5,928,824
Trainable params: 5,928,824
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 60269, 1.0: 46811, 2.0: 13080, 6.0: 7402, 5.0: 2097, 4.0: 1436})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 15s - loss: 0.0512 - acc: 0.9864 - val_loss: 0.0318 - val_acc: 0.9921
Epoch 2/40
 - 15s - loss: 0.0267 - acc: 0.9929 - val_loss: 0.0311 - val_acc: 0.9925
Epoch 3/40
 - 15s - loss: 0.0243 - acc: 0.9935 - val_loss: 0.0310 - val_acc: 0.9929

==================================================================================================
	Training time : 0:02:08.750093
==================================================================================================
	Identification : 0.505
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 122
	25 : 1
	5 : 29

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 343
	25 : 1
	5 : 5

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28458

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28458 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28458
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_495 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_496 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_495 (Embedding)       (None, 4, 208)       5919264     input_495[0][0]                  
__________________________________________________________________________________________________
embedding_496 (Embedding)       (None, 4, 30)        6720        input_496[0][0]                  
__________________________________________________________________________________________________
flatten_495 (Flatten)           (None, 832)          0           embedding_495[0][0]              
__________________________________________________________________________________________________
flatten_496 (Flatten)           (None, 120)          0           embedding_496[0][0]              
__________________________________________________________________________________________________
concatenate_248 (Concatenate)   (None, 952)          0           flatten_495[0][0]                
                                                                 flatten_496[0][0]                
__________________________________________________________________________________________________
dense_374 (Dense)               (None, 8)            7624        concatenate_248[0][0]            
==================================================================================================
Total params: 5,933,608
Trainable params: 5,933,608
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 59898, 1.0: 46925, 2.0: 13115, 6.0: 7547, 5.0: 2166, 4.0: 1444})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 15s - loss: 0.0510 - acc: 0.9864 - val_loss: 0.0332 - val_acc: 0.9916
Epoch 2/40
 - 15s - loss: 0.0266 - acc: 0.9929 - val_loss: 0.0324 - val_acc: 0.9924
Epoch 3/40
 - 15s - loss: 0.0243 - acc: 0.9935 - val_loss: 0.0326 - val_acc: 0.9923

==================================================================================================
	Training time : 0:02:08.969899
==================================================================================================
	Identification : 0.493
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 120
	5 : 28

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 346
	25 : 1
	5 : 5

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28422

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28422 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28422
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_497 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_498 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_497 (Embedding)       (None, 4, 208)       5911776     input_497[0][0]                  
__________________________________________________________________________________________________
embedding_498 (Embedding)       (None, 4, 30)        6720        input_498[0][0]                  
__________________________________________________________________________________________________
flatten_497 (Flatten)           (None, 832)          0           embedding_497[0][0]              
__________________________________________________________________________________________________
flatten_498 (Flatten)           (None, 120)          0           embedding_498[0][0]              
__________________________________________________________________________________________________
concatenate_249 (Concatenate)   (None, 952)          0           flatten_497[0][0]                
                                                                 flatten_498[0][0]                
__________________________________________________________________________________________________
dense_375 (Dense)               (None, 8)            7624        concatenate_249[0][0]            
==================================================================================================
Total params: 5,926,120
Trainable params: 5,926,120
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 60006, 1.0: 47094, 2.0: 12997, 6.0: 7464, 5.0: 2060, 4.0: 1474})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 15s - loss: 0.0511 - acc: 0.9865 - val_loss: 0.0319 - val_acc: 0.9918
Epoch 2/40
 - 15s - loss: 0.0267 - acc: 0.9930 - val_loss: 0.0312 - val_acc: 0.9922
Epoch 3/40
 - 15s - loss: 0.0244 - acc: 0.9935 - val_loss: 0.0313 - val_acc: 0.9924

==================================================================================================
	Training time : 0:02:10.567889
==================================================================================================
	Identification : 0.512
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 125
	25 : 1
	5 : 29

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 340
	25 : 1
	5 : 3

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 55011
	After : 41352

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 41352 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 41352
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_499 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_500 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_499 (Embedding)       (None, 4, 208)       8601216     input_499[0][0]                  
__________________________________________________________________________________________________
embedding_500 (Embedding)       (None, 4, 30)        3570        input_500[0][0]                  
__________________________________________________________________________________________________
flatten_499 (Flatten)           (None, 832)          0           embedding_499[0][0]              
__________________________________________________________________________________________________
flatten_500 (Flatten)           (None, 120)          0           embedding_500[0][0]              
__________________________________________________________________________________________________
concatenate_250 (Concatenate)   (None, 952)          0           flatten_499[0][0]                
                                                                 flatten_500[0][0]                
__________________________________________________________________________________________________
dense_376 (Dense)               (None, 8)            7624        concatenate_250[0][0]            
==================================================================================================
Total params: 8,612,410
Trainable params: 8,612,410
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 1159213
	5 Labels in train : Counter({0.0: 499647, 1.0: 333227, 2.0: 166420, 5.0: 83436, 6.0: 76483})
	5 Labels in valid : Counter({0.0: 49736, 1.0: 33469, 2.0: 16624, 5.0: 8409, 6.0: 7684})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1043291 samples, validate on 115922 samples
Epoch 1/40
 - 15s - loss: 0.0605 - acc: 0.9848 - val_loss: 0.0388 - val_acc: 0.9907
Epoch 2/40
 - 15s - loss: 0.0326 - acc: 0.9918 - val_loss: 0.0400 - val_acc: 0.9909
Epoch 3/40
 - 15s - loss: 0.0306 - acc: 0.9921 - val_loss: 0.0409 - val_acc: 0.9910

==================================================================================================
	Training time : 0:01:57.492564
==================================================================================================
	Identification : 0.286
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 62
	25 : 2
	5 : 18

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 374
	25 : 2
	5 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 55011
	After : 41322

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 41322 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 41322
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_501 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_502 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_501 (Embedding)       (None, 4, 208)       8594976     input_501[0][0]                  
__________________________________________________________________________________________________
embedding_502 (Embedding)       (None, 4, 30)        3570        input_502[0][0]                  
__________________________________________________________________________________________________
flatten_501 (Flatten)           (None, 832)          0           embedding_501[0][0]              
__________________________________________________________________________________________________
flatten_502 (Flatten)           (None, 120)          0           embedding_502[0][0]              
__________________________________________________________________________________________________
concatenate_251 (Concatenate)   (None, 952)          0           flatten_501[0][0]                
                                                                 flatten_502[0][0]                
__________________________________________________________________________________________________
dense_377 (Dense)               (None, 8)            7624        concatenate_251[0][0]            
==================================================================================================
Total params: 8,606,170
Trainable params: 8,606,170
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 1159213
	5 Labels in train : Counter({0.0: 499647, 1.0: 333227, 2.0: 166420, 5.0: 83436, 6.0: 76483})
	5 Labels in valid : Counter({0.0: 49702, 1.0: 33653, 2.0: 16656, 5.0: 8254, 6.0: 7657})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1043291 samples, validate on 115922 samples
Epoch 1/40
 - 15s - loss: 0.0605 - acc: 0.9848 - val_loss: 0.0380 - val_acc: 0.9911
Epoch 2/40
 - 15s - loss: 0.0328 - acc: 0.9918 - val_loss: 0.0390 - val_acc: 0.9912
Epoch 3/40
 - 15s - loss: 0.0308 - acc: 0.9921 - val_loss: 0.0402 - val_acc: 0.9912

==================================================================================================
	Training time : 0:01:53.001400
==================================================================================================
	Identification : 0.319
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 66
	25 : 2
	5 : 20

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 370
	25 : 1
	5 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 55011
	After : 41374

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 41374 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 41374
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 208
	POS = 30 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_503 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_504 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_503 (Embedding)       (None, 4, 208)       8605792     input_503[0][0]                  
__________________________________________________________________________________________________
embedding_504 (Embedding)       (None, 4, 30)        3570        input_504[0][0]                  
__________________________________________________________________________________________________
flatten_503 (Flatten)           (None, 832)          0           embedding_503[0][0]              
__________________________________________________________________________________________________
flatten_504 (Flatten)           (None, 120)          0           embedding_504[0][0]              
__________________________________________________________________________________________________
concatenate_252 (Concatenate)   (None, 952)          0           flatten_503[0][0]                
                                                                 flatten_504[0][0]                
__________________________________________________________________________________________________
dense_378 (Dense)               (None, 8)            7624        concatenate_252[0][0]            
==================================================================================================
Total params: 8,616,986
Trainable params: 8,616,986
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 1159213
	5 Labels in train : Counter({0.0: 499647, 1.0: 333227, 2.0: 166420, 5.0: 83436, 6.0: 76483})
	5 Labels in valid : Counter({0.0: 49894, 1.0: 33216, 2.0: 16709, 5.0: 8420, 6.0: 7683})
	Favorisation Coeff : 26

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.011
__________________________________________________________________________________________________
Train on 1043291 samples, validate on 115922 samples
Epoch 1/40
 - 15s - loss: 0.0603 - acc: 0.9848 - val_loss: 0.0391 - val_acc: 0.9908
Epoch 2/40
 - 15s - loss: 0.0326 - acc: 0.9918 - val_loss: 0.0403 - val_acc: 0.9908
Epoch 3/40
 - 15s - loss: 0.0306 - acc: 0.9922 - val_loss: 0.0415 - val_acc: 0.9907

==================================================================================================
	Training time : 0:01:52.980256
==================================================================================================
	Identification : 0.316
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 62
	25 : 2
	5 : 20

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 374
	25 : 1
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
