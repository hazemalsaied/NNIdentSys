INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 48)        705264      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 24)        5640        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 192)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 96)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 288)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 24)           6936        concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            200         dropout_1[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.1249 - acc: 0.9733 - val_loss: 5.9032e-04 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0673 - acc: 0.9859 - val_loss: 1.7233e-04 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0604 - acc: 0.9876 - val_loss: 7.7012e-05 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0568 - acc: 0.9884 - val_loss: 3.6479e-05 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0546 - acc: 0.9889 - val_loss: 2.0981e-05 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0531 - acc: 0.9892 - val_loss: 1.3530e-05 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0518 - acc: 0.9895 - val_loss: 9.2984e-06 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0508 - acc: 0.9898 - val_loss: 6.6757e-06 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0497 - acc: 0.9900 - val_loss: 4.7088e-06 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0490 - acc: 0.9900 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0486 - acc: 0.9903 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0483 - acc: 0.9903 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0476 - acc: 0.9905 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0472 - acc: 0.9905 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0469 - acc: 0.9906 - val_loss: 8.9407e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0466 - acc: 0.9907 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0461 - acc: 0.9908 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0459 - acc: 0.9908 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0455 - acc: 0.9909 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0455 - acc: 0.9910 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0452 - acc: 0.9910 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0449 - acc: 0.9911 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0449 - acc: 0.9911 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0446 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0445 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0444 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0441 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0441 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0438 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0439 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0436 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0435 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0434 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0433 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0432 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0431 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0430 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0430 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0429 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0427 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0014 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 0.0012 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 0.0013 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 0.0015 - acc: 0.9996
Epoch 5/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 0.0012 - acc: 0.9996
Epoch 8/40
 - 2s - loss: 0.0011 - acc: 0.9997
Epoch 9/40
 - 2s - loss: 0.0010 - acc: 0.9997
Epoch 10/40
 - 2s - loss: 0.0010 - acc: 0.9997
Epoch 11/40
 - 2s - loss: 8.6322e-04 - acc: 0.9998
Epoch 12/40
 - 2s - loss: 9.4058e-04 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 9.5981e-04 - acc: 0.9997
Epoch 14/40
 - 2s - loss: 9.5333e-04 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 9.8076e-04 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 9.6903e-04 - acc: 0.9996
Epoch 17/40
 - 2s - loss: 9.6828e-04 - acc: 0.9996
Epoch 18/40
 - 2s - loss: 8.5843e-04 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 7.6449e-04 - acc: 0.9997
Epoch 20/40
 - 2s - loss: 7.2760e-04 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 9.8109e-04 - acc: 0.9996
Epoch 22/40
 - 2s - loss: 8.0826e-04 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 8.6500e-04 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 7.8213e-04 - acc: 0.9997
Epoch 25/40
 - 2s - loss: 7.0311e-04 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 8.8742e-04 - acc: 0.9996
Epoch 27/40
 - 2s - loss: 8.0662e-04 - acc: 0.9997
Epoch 28/40
 - 2s - loss: 8.5964e-04 - acc: 0.9996
Epoch 29/40
 - 2s - loss: 8.5250e-04 - acc: 0.9996
Epoch 30/40
 - 2s - loss: 8.1695e-04 - acc: 0.9997
Epoch 31/40
 - 2s - loss: 7.6371e-04 - acc: 0.9997
Epoch 32/40
 - 2s - loss: 8.0858e-04 - acc: 0.9996
Epoch 33/40
 - 2s - loss: 7.1168e-04 - acc: 0.9997
Epoch 34/40
 - 2s - loss: 9.0918e-04 - acc: 0.9996
Epoch 35/40
 - 2s - loss: 7.1856e-04 - acc: 0.9997
Epoch 36/40
 - 2s - loss: 0.0010 - acc: 0.9995
Epoch 37/40
 - 2s - loss: 6.9486e-04 - acc: 0.9997
Epoch 38/40
 - 2s - loss: 6.3112e-04 - acc: 0.9997
Epoch 39/40
 - 2s - loss: 8.2985e-04 - acc: 0.9996
Epoch 40/40
 - 2s - loss: 8.3839e-04 - acc: 0.9996
# Training time = 0:11:30.932607
# F-Score(Ordinary) = 0.717, Recall: 0.761, Precision: 0.678
# F-Score(lvc) = 0.603, Recall: 0.759, Precision: 0.5
# F-Score(ireflv) = 0.779, Recall: 0.681, Precision: 0.91
# F-Score(id) = 0.733, Recall: 0.845, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 48)        705264      input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 24)        5640        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 192)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 96)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 288)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 24)           6936        concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 24)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            200         dropout_2[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.1252 - acc: 0.9735 - val_loss: 5.9312e-04 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0680 - acc: 0.9861 - val_loss: 1.8437e-04 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0613 - acc: 0.9876 - val_loss: 8.6371e-05 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0576 - acc: 0.9884 - val_loss: 4.3870e-05 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0553 - acc: 0.9889 - val_loss: 2.6822e-05 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0536 - acc: 0.9894 - val_loss: 1.7643e-05 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0524 - acc: 0.9896 - val_loss: 1.3054e-05 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0514 - acc: 0.9898 - val_loss: 9.8944e-06 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0506 - acc: 0.9900 - val_loss: 7.9871e-06 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0496 - acc: 0.9901 - val_loss: 6.0797e-06 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0491 - acc: 0.9903 - val_loss: 4.6492e-06 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0483 - acc: 0.9904 - val_loss: 3.7551e-06 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0479 - acc: 0.9905 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0475 - acc: 0.9906 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0472 - acc: 0.9907 - val_loss: 2.4438e-06 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0467 - acc: 0.9908 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0464 - acc: 0.9908 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0462 - acc: 0.9908 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0459 - acc: 0.9909 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0457 - acc: 0.9910 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0454 - acc: 0.9911 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0452 - acc: 0.9911 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0449 - acc: 0.9912 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0446 - acc: 0.9912 - val_loss: 8.9407e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0446 - acc: 0.9912 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0445 - acc: 0.9913 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0443 - acc: 0.9913 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0441 - acc: 0.9913 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0440 - acc: 0.9914 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0438 - acc: 0.9914 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0437 - acc: 0.9914 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0436 - acc: 0.9915 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0434 - acc: 0.9915 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0433 - acc: 0.9915 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0434 - acc: 0.9915 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0432 - acc: 0.9916 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0430 - acc: 0.9916 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0430 - acc: 0.9916 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0429 - acc: 0.9917 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0427 - acc: 0.9916 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0013 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 9.8980e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 9.2317e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 8.3921e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 9.0823e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 7.4277e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 5.9682e-04 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 5.3468e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 6.4504e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 5.4698e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 4.6840e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 5.4043e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 3.5294e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 4.3723e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 3.5847e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 3.8568e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 3.6818e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 4.3113e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 4.2169e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 3.6694e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 3.1945e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 3.1157e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 3.2332e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.4707e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 3.5109e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 3.3963e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 3.2339e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 2.5545e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 2.8034e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 2.5000e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 3.1704e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.8312e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.5823e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.5552e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.5270e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.1457e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.5862e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.0754e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.8444e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.4760e-04 - acc: 1.0000
# Training time = 0:11:26.828913
# F-Score(Ordinary) = 0.708, Recall: 0.728, Precision: 0.689
# F-Score(lvc) = 0.524, Recall: 0.511, Precision: 0.538
# F-Score(ireflv) = 0.788, Recall: 0.711, Precision: 0.885
# F-Score(id) = 0.769, Recall: 0.947, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 48)        705264      input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 24)        5640        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 192)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 96)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 288)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 24)           6936        concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 24)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            200         dropout_3[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.1298 - acc: 0.9728 - val_loss: 3.8649e-04 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0688 - acc: 0.9859 - val_loss: 1.1039e-04 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0620 - acc: 0.9875 - val_loss: 5.8355e-05 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0582 - acc: 0.9883 - val_loss: 4.1784e-05 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0559 - acc: 0.9888 - val_loss: 2.9326e-05 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0542 - acc: 0.9893 - val_loss: 2.3604e-05 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0526 - acc: 0.9895 - val_loss: 1.6570e-05 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0514 - acc: 0.9898 - val_loss: 1.4067e-05 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0507 - acc: 0.9900 - val_loss: 1.1504e-05 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0499 - acc: 0.9902 - val_loss: 1.1325e-05 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0491 - acc: 0.9903 - val_loss: 9.5368e-06 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0486 - acc: 0.9904 - val_loss: 8.8215e-06 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0482 - acc: 0.9905 - val_loss: 8.0467e-06 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0479 - acc: 0.9906 - val_loss: 7.8678e-06 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0474 - acc: 0.9907 - val_loss: 6.8546e-06 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0469 - acc: 0.9907 - val_loss: 6.5565e-06 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0465 - acc: 0.9909 - val_loss: 6.4373e-06 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0463 - acc: 0.9909 - val_loss: 6.2585e-06 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0460 - acc: 0.9910 - val_loss: 5.4240e-06 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0456 - acc: 0.9911 - val_loss: 5.1856e-06 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0455 - acc: 0.9911 - val_loss: 4.9472e-06 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0453 - acc: 0.9912 - val_loss: 4.5896e-06 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0451 - acc: 0.9912 - val_loss: 4.4108e-06 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0450 - acc: 0.9912 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0447 - acc: 0.9912 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0446 - acc: 0.9913 - val_loss: 3.6955e-06 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0443 - acc: 0.9913 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0442 - acc: 0.9914 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0441 - acc: 0.9914 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0440 - acc: 0.9914 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0438 - acc: 0.9914 - val_loss: 2.9206e-06 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0435 - acc: 0.9915 - val_loss: 2.9206e-06 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0436 - acc: 0.9915 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0434 - acc: 0.9915 - val_loss: 3.0398e-06 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0434 - acc: 0.9915 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0434 - acc: 0.9915 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0432 - acc: 0.9916 - val_loss: 2.5630e-06 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0431 - acc: 0.9915 - val_loss: 2.5630e-06 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0430 - acc: 0.9916 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0429 - acc: 0.9916 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 0.0011 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 8.2312e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 6.4747e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 7.0736e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 6.5967e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 6.2725e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 4.7527e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 3.9735e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 4.1584e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 5.2600e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 4.0491e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 3.8610e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 3.9777e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 3.6189e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 3.3232e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.9659e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.6898e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 3.0368e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 2.5968e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.8843e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.4535e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 3.2239e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 2.9380e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 2.1513e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.3321e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.3679e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 2.1959e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 2.2394e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.6462e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.0540e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 2.4327e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.2245e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.8447e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.5034e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 2.1184e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 1.9505e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.3428e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.6476e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.0848e-04 - acc: 1.0000
# Training time = 0:11:20.909762
# F-Score(Ordinary) = 0.71, Recall: 0.729, Precision: 0.691
# F-Score(lvc) = 0.545, Recall: 0.57, Precision: 0.523
# F-Score(ireflv) = 0.757, Recall: 0.644, Precision: 0.918
# F-Score(id) = 0.776, Recall: 0.969, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 48)        705264      input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 24)        5640        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 192)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 96)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 288)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 24)           6936        concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            200         dropout_4[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.1301 - acc: 0.9720 - val_loss: 1.7805e-04 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0678 - acc: 0.9857 - val_loss: 4.9592e-05 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0608 - acc: 0.9874 - val_loss: 2.6584e-05 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0571 - acc: 0.9883 - val_loss: 1.4901e-05 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0549 - acc: 0.9888 - val_loss: 9.7156e-06 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0532 - acc: 0.9892 - val_loss: 6.1989e-06 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0521 - acc: 0.9894 - val_loss: 4.3511e-06 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0509 - acc: 0.9898 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0500 - acc: 0.9899 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0493 - acc: 0.9902 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0487 - acc: 0.9902 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0482 - acc: 0.9904 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0476 - acc: 0.9905 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0472 - acc: 0.9906 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0468 - acc: 0.9907 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0465 - acc: 0.9907 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0461 - acc: 0.9909 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0457 - acc: 0.9909 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0455 - acc: 0.9910 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0455 - acc: 0.9910 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0449 - acc: 0.9911 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0448 - acc: 0.9912 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0445 - acc: 0.9912 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0446 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0443 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0441 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0441 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0439 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0436 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0436 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0435 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0434 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0432 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0430 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0430 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0429 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0429 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0427 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0427 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0427 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 9.6015e-04 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 0.0010 - acc: 0.9996
Epoch 3/40
 - 2s - loss: 8.6717e-04 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 7.4767e-04 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 5.7437e-04 - acc: 0.9998
Epoch 6/40
 - 2s - loss: 6.5963e-04 - acc: 0.9998
Epoch 7/40
 - 2s - loss: 6.6060e-04 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 5.0396e-04 - acc: 0.9998
Epoch 9/40
 - 2s - loss: 4.8499e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 4.5831e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 4.8651e-04 - acc: 0.9998
Epoch 12/40
 - 2s - loss: 3.2075e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 3.2628e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 3.6178e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 2.8928e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 3.0770e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 3.0456e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 3.3944e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.5958e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 2.6375e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 2.8375e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 2.4487e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 2.0779e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 1.7631e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 2.6155e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.3288e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 2.1982e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.6078e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 2.1851e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.6484e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.7525e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.8568e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.0780e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.7383e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.3290e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.6110e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.0341e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.5338e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.6063e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.4740e-04 - acc: 1.0000
# Training time = 0:11:28.895356
# F-Score(Ordinary) = 0.692, Recall: 0.665, Precision: 0.723
# F-Score(lvc) = 0.493, Recall: 0.436, Precision: 0.568
# F-Score(ireflv) = 0.747, Recall: 0.634, Precision: 0.91
# F-Score(id) = 0.765, Recall: 0.914, Precision: 0.658
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 48)        705264      input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 24)        5640        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 192)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 96)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 288)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 24)           6936        concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 24)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            200         dropout_5[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.1324 - acc: 0.9723 - val_loss: 0.0012 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0725 - acc: 0.9852 - val_loss: 4.4552e-04 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0648 - acc: 0.9870 - val_loss: 1.3060e-04 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0603 - acc: 0.9880 - val_loss: 7.0813e-05 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0575 - acc: 0.9886 - val_loss: 4.7268e-05 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0558 - acc: 0.9890 - val_loss: 3.2425e-05 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0541 - acc: 0.9894 - val_loss: 2.3783e-05 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0529 - acc: 0.9896 - val_loss: 1.5736e-05 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0518 - acc: 0.9899 - val_loss: 1.1325e-05 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0510 - acc: 0.9899 - val_loss: 8.0467e-06 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0503 - acc: 0.9902 - val_loss: 6.4373e-06 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0497 - acc: 0.9903 - val_loss: 4.4704e-06 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0491 - acc: 0.9903 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0487 - acc: 0.9905 - val_loss: 2.8610e-06 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0481 - acc: 0.9906 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0478 - acc: 0.9907 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0475 - acc: 0.9907 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0472 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0468 - acc: 0.9909 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0465 - acc: 0.9909 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0462 - acc: 0.9910 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0460 - acc: 0.9910 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0459 - acc: 0.9910 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0456 - acc: 0.9912 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0455 - acc: 0.9912 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0452 - acc: 0.9912 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0450 - acc: 0.9913 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0449 - acc: 0.9913 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0447 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0446 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0445 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0442 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0442 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0440 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0439 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0439 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0439 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0436 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0436 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0436 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0018 - acc: 0.9996
Epoch 2/40
 - 2s - loss: 0.0014 - acc: 0.9997
Epoch 3/40
 - 2s - loss: 0.0013 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 0.0013 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 0.0011 - acc: 0.9996
Epoch 6/40
 - 2s - loss: 0.0011 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 0.0010 - acc: 0.9997
Epoch 8/40
 - 2s - loss: 0.0011 - acc: 0.9996
Epoch 9/40
 - 2s - loss: 0.0010 - acc: 0.9996
Epoch 10/40
 - 2s - loss: 9.9388e-04 - acc: 0.9996
Epoch 11/40
 - 2s - loss: 8.1503e-04 - acc: 0.9997
Epoch 12/40
 - 2s - loss: 8.7167e-04 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 6.6081e-04 - acc: 0.9998
Epoch 14/40
 - 2s - loss: 9.3574e-04 - acc: 0.9996
Epoch 15/40
 - 2s - loss: 8.5576e-04 - acc: 0.9996
Epoch 16/40
 - 2s - loss: 7.8316e-04 - acc: 0.9997
Epoch 17/40
 - 2s - loss: 6.6539e-04 - acc: 0.9997
Epoch 18/40
 - 2s - loss: 4.4675e-04 - acc: 0.9998
Epoch 19/40
 - 2s - loss: 6.6394e-04 - acc: 0.9997
Epoch 20/40
 - 2s - loss: 6.5569e-04 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 6.3193e-04 - acc: 0.9997
Epoch 22/40
 - 2s - loss: 5.3151e-04 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 5.8562e-04 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 4.9346e-04 - acc: 0.9998
Epoch 25/40
 - 2s - loss: 6.4602e-04 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 6.5352e-04 - acc: 0.9997
Epoch 27/40
 - 2s - loss: 6.0891e-04 - acc: 0.9996
Epoch 28/40
 - 2s - loss: 5.6472e-04 - acc: 0.9997
Epoch 29/40
 - 2s - loss: 4.9052e-04 - acc: 0.9997
Epoch 30/40
 - 2s - loss: 6.0471e-04 - acc: 0.9997
Epoch 31/40
 - 2s - loss: 5.9109e-04 - acc: 0.9996
Epoch 32/40
 - 2s - loss: 5.5492e-04 - acc: 0.9997
Epoch 33/40
 - 2s - loss: 5.2354e-04 - acc: 0.9997
Epoch 34/40
 - 2s - loss: 3.9198e-04 - acc: 0.9998
Epoch 35/40
 - 2s - loss: 4.0833e-04 - acc: 0.9997
Epoch 36/40
 - 2s - loss: 5.6677e-04 - acc: 0.9997
Epoch 37/40
 - 2s - loss: 5.6585e-04 - acc: 0.9996
Epoch 38/40
 - 2s - loss: 4.7292e-04 - acc: 0.9998
Epoch 39/40
 - 2s - loss: 3.9059e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 4.9207e-04 - acc: 0.9999
# Training time = 0:11:27.872391
# F-Score(Ordinary) = 0.71, Recall: 0.711, Precision: 0.709
# F-Score(lvc) = 0.512, Recall: 0.466, Precision: 0.568
# F-Score(ireflv) = 0.799, Recall: 0.722, Precision: 0.893
# F-Score(id) = 0.771, Recall: 0.94, Precision: 0.653
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 48)        705264      input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 24)        5640        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 192)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 96)           0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 288)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 24)           6936        concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 24)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            200         dropout_6[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0832 - acc: 0.9822 - val_loss: 3.2425e-05 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0542 - acc: 0.9890 - val_loss: 7.5102e-06 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0501 - acc: 0.9899 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0479 - acc: 0.9904 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0466 - acc: 0.9907 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0458 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0450 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0443 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0437 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0433 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0432 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0429 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0425 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0422 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0421 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0420 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0416 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0416 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0413 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0414 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0411 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0409 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0410 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0409 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0406 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0404 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0404 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 8.0289e-04 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 6.7056e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 7.8804e-04 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 0.0010 - acc: 0.9996
Epoch 5/40
 - 2s - loss: 7.6073e-04 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 8.2875e-04 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 8.8422e-04 - acc: 0.9996
Epoch 8/40
 - 2s - loss: 7.6500e-04 - acc: 0.9997
Epoch 9/40
 - 2s - loss: 6.7666e-04 - acc: 0.9997
Epoch 10/40
 - 2s - loss: 7.1922e-04 - acc: 0.9997
Epoch 11/40
 - 2s - loss: 5.7694e-04 - acc: 0.9998
Epoch 12/40
 - 2s - loss: 6.7065e-04 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 7.1226e-04 - acc: 0.9997
Epoch 14/40
 - 2s - loss: 7.0995e-04 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 7.3178e-04 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 7.4559e-04 - acc: 0.9996
Epoch 17/40
 - 2s - loss: 7.6153e-04 - acc: 0.9996
Epoch 18/40
 - 2s - loss: 6.5520e-04 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 5.7277e-04 - acc: 0.9997
Epoch 20/40
 - 2s - loss: 5.4261e-04 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 7.9479e-04 - acc: 0.9996
Epoch 22/40
 - 2s - loss: 6.3578e-04 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 6.8571e-04 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 6.0644e-04 - acc: 0.9997
Epoch 25/40
 - 2s - loss: 5.4812e-04 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 7.2351e-04 - acc: 0.9996
Epoch 27/40
 - 2s - loss: 6.4555e-04 - acc: 0.9997
Epoch 28/40
 - 2s - loss: 6.9345e-04 - acc: 0.9996
Epoch 29/40
 - 2s - loss: 6.9016e-04 - acc: 0.9996
Epoch 30/40
 - 2s - loss: 6.6137e-04 - acc: 0.9997
Epoch 31/40
 - 2s - loss: 6.0882e-04 - acc: 0.9997
Epoch 32/40
 - 2s - loss: 6.7504e-04 - acc: 0.9996
Epoch 33/40
 - 2s - loss: 5.7568e-04 - acc: 0.9997
Epoch 34/40
 - 2s - loss: 7.6896e-04 - acc: 0.9996
Epoch 35/40
 - 2s - loss: 5.9283e-04 - acc: 0.9997
Epoch 36/40
 - 2s - loss: 8.8240e-04 - acc: 0.9995
Epoch 37/40
 - 2s - loss: 5.6525e-04 - acc: 0.9997
Epoch 38/40
 - 2s - loss: 5.1267e-04 - acc: 0.9997
Epoch 39/40
 - 2s - loss: 7.0403e-04 - acc: 0.9996
Epoch 40/40
 - 2s - loss: 7.0360e-04 - acc: 0.9996
# Training time = 0:11:22.664567
# F-Score(Ordinary) = 0.744, Recall: 0.846, Precision: 0.664
# F-Score(lvc) = 0.616, Recall: 0.823, Precision: 0.492
# F-Score(ireflv) = 0.814, Recall: 0.786, Precision: 0.844
# F-Score(id) = 0.76, Recall: 0.901, Precision: 0.658
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 48)        705264      input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 24)        5640        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 192)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 96)           0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 288)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 24)           6936        concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 24)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            200         dropout_7[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0838 - acc: 0.9824 - val_loss: 3.3796e-05 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0550 - acc: 0.9890 - val_loss: 6.7950e-06 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0509 - acc: 0.9898 - val_loss: 3.2783e-06 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0486 - acc: 0.9903 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0471 - acc: 0.9906 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0461 - acc: 0.9910 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0455 - acc: 0.9911 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0447 - acc: 0.9913 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0442 - acc: 0.9913 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0436 - acc: 0.9915 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0434 - acc: 0.9915 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0429 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0426 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0425 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0423 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0420 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0418 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0417 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0416 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0413 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0412 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0409 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0409 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0407 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 6.8532e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 4.1164e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 4.4559e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 4.0312e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 4.4359e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 3.5971e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 2.1746e-04 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.0303e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 3.1410e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.3226e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.9400e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 2.7234e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.3920e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.9433e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.3948e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.8481e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.6276e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.1536e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.7476e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.9304e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.3923e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.3876e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.3266e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 9.4590e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.8231e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.3702e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.5912e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.2932e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1904e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.0689e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.8914e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.3467e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.4008e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.2441e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 4.5764e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1334e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.3326e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 9.3817e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 6.1718e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1428e-04 - acc: 1.0000
# Training time = 0:11:23.727857
# F-Score(Ordinary) = 0.739, Recall: 0.828, Precision: 0.667
# F-Score(lvc) = 0.588, Recall: 0.73, Precision: 0.492
# F-Score(ireflv) = 0.803, Recall: 0.773, Precision: 0.836
# F-Score(id) = 0.777, Recall: 0.928, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 48)        705264      input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 24)        5640        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 192)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 96)           0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 288)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 24)           6936        concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 24)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            200         dropout_8[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0862 - acc: 0.9818 - val_loss: 4.8281e-05 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0554 - acc: 0.9889 - val_loss: 2.0147e-05 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0509 - acc: 0.9899 - val_loss: 1.5080e-05 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0485 - acc: 0.9904 - val_loss: 1.3948e-05 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0471 - acc: 0.9907 - val_loss: 1.0371e-05 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0460 - acc: 0.9909 - val_loss: 9.7752e-06 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0450 - acc: 0.9911 - val_loss: 6.3181e-06 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0443 - acc: 0.9913 - val_loss: 5.6029e-06 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0440 - acc: 0.9914 - val_loss: 4.6492e-06 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0435 - acc: 0.9915 - val_loss: 5.1260e-06 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0430 - acc: 0.9916 - val_loss: 4.4108e-06 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0428 - acc: 0.9915 - val_loss: 4.5896e-06 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0426 - acc: 0.9916 - val_loss: 4.1723e-06 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0424 - acc: 0.9917 - val_loss: 4.2915e-06 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0421 - acc: 0.9917 - val_loss: 3.9339e-06 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0418 - acc: 0.9918 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0416 - acc: 0.9918 - val_loss: 3.9339e-06 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0415 - acc: 0.9918 - val_loss: 3.8147e-06 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0414 - acc: 0.9919 - val_loss: 3.1591e-06 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0412 - acc: 0.9919 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0411 - acc: 0.9919 - val_loss: 3.1591e-06 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0410 - acc: 0.9920 - val_loss: 2.8610e-06 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0410 - acc: 0.9920 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0408 - acc: 0.9920 - val_loss: 2.6822e-06 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0408 - acc: 0.9920 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0407 - acc: 0.9920 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0405 - acc: 0.9920 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0404 - acc: 0.9920 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0404 - acc: 0.9920 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0404 - acc: 0.9921 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0402 - acc: 0.9920 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0400 - acc: 0.9922 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0398 - acc: 0.9921 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 5.8341e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 6.5303e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 4.2686e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 2.6360e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 4.3880e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 3.1119e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 4.4162e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.2183e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.1630e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.4398e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.9366e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 3.0195e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 2.3611e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 3.3671e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 2.5555e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 2.5838e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 1.4418e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.7199e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 1.7941e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 1.5115e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.0411e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.4895e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 2.7117e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 2.0937e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 1.2573e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 1.3901e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.4554e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.1763e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 9.8673e-05 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 7.4259e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.6482e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.4322e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 5.7537e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1418e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 1.8359e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 1.7042e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 5.2232e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 5.0312e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.2922e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 5.4635e-05 - acc: 1.0000
# Training time = 0:11:28.818524
# F-Score(Ordinary) = 0.736, Recall: 0.811, Precision: 0.673
# F-Score(lvc) = 0.573, Recall: 0.753, Precision: 0.462
# F-Score(ireflv) = 0.8, Recall: 0.73, Precision: 0.885
# F-Score(id) = 0.764, Recall: 0.901, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 48)        705264      input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 24)        5640        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 192)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 96)           0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 288)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 24)           6936        concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 24)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            200         dropout_9[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0853 - acc: 0.9817 - val_loss: 1.6630e-05 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0543 - acc: 0.9889 - val_loss: 3.3975e-06 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0499 - acc: 0.9899 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0477 - acc: 0.9904 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0463 - acc: 0.9907 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0453 - acc: 0.9910 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0446 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0440 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0434 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0430 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0427 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0424 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0420 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0419 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0417 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0413 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0411 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0410 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0410 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0407 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0407 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0406 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0404 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0404 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0397 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0398 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0398 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0397 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 5.0672e-04 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 5.1661e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 4.5690e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 3.9466e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 2.8597e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 3.0635e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 4.0280e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.0968e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.1713e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.8978e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.5553e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 1.0228e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.3425e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.4684e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1751e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.4906e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.4475e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.4066e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.5218e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.0749e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.2346e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1816e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 6.8316e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 5.3611e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.2413e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 7.7690e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1505e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 6.4783e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.0356e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 5.5785e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 5.3158e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1075e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.9421e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 7.1115e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 4.4410e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 6.9178e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 7.7399e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 6.3702e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 5.2005e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 5.9149e-05 - acc: 1.0000
# Training time = 0:11:36.348775
# F-Score(Ordinary) = 0.699, Recall: 0.719, Precision: 0.68
# F-Score(lvc) = 0.562, Recall: 0.641, Precision: 0.5
# F-Score(ireflv) = 0.764, Recall: 0.677, Precision: 0.877
# F-Score(id) = 0.721, Recall: 0.79, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 48)        705264      input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 24)        5640        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 192)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 96)           0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 288)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 24)           6936        concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 24)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            200         dropout_10[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0893 - acc: 0.9813 - val_loss: 2.3249e-04 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0578 - acc: 0.9885 - val_loss: 2.9982e-05 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0525 - acc: 0.9897 - val_loss: 8.9407e-06 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0499 - acc: 0.9902 - val_loss: 4.1127e-06 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0480 - acc: 0.9905 - val_loss: 2.4438e-06 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0472 - acc: 0.9908 - val_loss: 1.3709e-06 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0461 - acc: 0.9910 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0454 - acc: 0.9912 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0448 - acc: 0.9913 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0443 - acc: 0.9914 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0439 - acc: 0.9915 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0436 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0433 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0431 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0427 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0426 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0424 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0423 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0421 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0420 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0418 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0416 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0414 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0413 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0412 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0409 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0409 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0407 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0408 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0407 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0404 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0403 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0403 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0401 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0402 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0402 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 5.7934e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 4.8248e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 4.8306e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 3.5787e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 4.6994e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.3456e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 3.0550e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 3.8395e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 4.6184e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.7803e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.9152e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 3.2527e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 2.9510e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 3.2938e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 2.6595e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 2.6407e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.8282e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 1.1567e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.6933e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 3.4064e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 2.2363e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 2.0207e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 1.2364e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.9606e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 1.8746e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 1.6910e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.1395e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 9.0130e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.4181e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1311e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.9629e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.5093e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.6955e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 1.8382e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 9.3098e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.5570e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 2.4499e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 1.1286e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 9.8739e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.2128e-04 - acc: 1.0000
# Training time = 0:11:39.071832
# F-Score(Ordinary) = 0.707, Recall: 0.718, Precision: 0.696
# F-Score(lvc) = 0.497, Recall: 0.451, Precision: 0.553
# F-Score(ireflv) = 0.812, Recall: 0.776, Precision: 0.852
# F-Score(id) = 0.77, Recall: 0.927, Precision: 0.658
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 48)        705264      input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 24)        5640        input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 192)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 96)           0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 288)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 24)           6936        concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 24)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            200         dropout_11[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0661 - acc: 0.9860 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0477 - acc: 0.9903 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0449 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0433 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0425 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0420 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0415 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0410 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0407 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0404 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0403 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0402 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0399 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0397 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0394 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0393 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0390 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0386 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 6.7608e-04 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 5.5729e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 7.0958e-04 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 9.8355e-04 - acc: 0.9996
Epoch 5/40
 - 2s - loss: 7.0183e-04 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 7.7955e-04 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 8.5246e-04 - acc: 0.9996
Epoch 8/40
 - 2s - loss: 7.2618e-04 - acc: 0.9997
Epoch 9/40
 - 2s - loss: 6.3366e-04 - acc: 0.9997
Epoch 10/40
 - 2s - loss: 6.8437e-04 - acc: 0.9997
Epoch 11/40
 - 2s - loss: 5.3593e-04 - acc: 0.9998
Epoch 12/40
 - 2s - loss: 6.4177e-04 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 6.9050e-04 - acc: 0.9997
Epoch 14/40
 - 2s - loss: 6.8605e-04 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 7.0808e-04 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 7.2807e-04 - acc: 0.9996
Epoch 17/40
 - 2s - loss: 7.4823e-04 - acc: 0.9996
Epoch 18/40
 - 2s - loss: 6.3514e-04 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 5.5046e-04 - acc: 0.9997
Epoch 20/40
 - 2s - loss: 5.2023e-04 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 7.8144e-04 - acc: 0.9996
Epoch 22/40
 - 2s - loss: 6.1785e-04 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 6.6644e-04 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 5.8437e-04 - acc: 0.9997
Epoch 25/40
 - 2s - loss: 5.2808e-04 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 7.0533e-04 - acc: 0.9996
Epoch 27/40
 - 2s - loss: 6.2439e-04 - acc: 0.9997
Epoch 28/40
 - 2s - loss: 6.7103e-04 - acc: 0.9996
Epoch 29/40
 - 2s - loss: 6.6675e-04 - acc: 0.9996
Epoch 30/40
 - 2s - loss: 6.3734e-04 - acc: 0.9997
Epoch 31/40
 - 2s - loss: 5.8337e-04 - acc: 0.9997
Epoch 32/40
 - 2s - loss: 6.5316e-04 - acc: 0.9996
Epoch 33/40
 - 2s - loss: 5.5098e-04 - acc: 0.9997
Epoch 34/40
 - 2s - loss: 7.4360e-04 - acc: 0.9996
Epoch 35/40
 - 2s - loss: 5.6792e-04 - acc: 0.9997
Epoch 36/40
 - 2s - loss: 8.5449e-04 - acc: 0.9995
Epoch 37/40
 - 2s - loss: 5.3668e-04 - acc: 0.9997
Epoch 38/40
 - 2s - loss: 4.8551e-04 - acc: 0.9997
Epoch 39/40
 - 2s - loss: 6.7343e-04 - acc: 0.9996
Epoch 40/40
 - 2s - loss: 6.6970e-04 - acc: 0.9996
# Training time = 0:11:35.151102
# F-Score(Ordinary) = 0.73, Recall: 0.847, Precision: 0.642
# F-Score(lvc) = 0.615, Recall: 0.842, Precision: 0.485
# F-Score(ireflv) = 0.824, Recall: 0.845, Precision: 0.803
# F-Score(id) = 0.735, Recall: 0.85, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 48)        705264      input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 24)        5640        input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 192)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 96)           0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 288)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 24)           6936        concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 24)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            200         dropout_12[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0674 - acc: 0.9860 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0484 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0454 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0437 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0427 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0420 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0416 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0411 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0404 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0396 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0395 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0393 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0393 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0392 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0391 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0389 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0389 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0387 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0386 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0383 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0383 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0382 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0379 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 4.2636e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.1600e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.5376e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 2.4875e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 2.7537e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.3617e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 9.1608e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 9.1833e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.0939e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 1.1615e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 9.6595e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.9268e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 6.9354e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.2873e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 6.5248e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1409e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 9.7985e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.3905e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 7.7747e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.0883e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 6.4563e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 6.8930e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 5.9887e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 3.8871e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.0907e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 5.9480e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 9.3262e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 8.3865e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 5.3188e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 4.8177e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.3284e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 5.6623e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 8.0720e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 7.6278e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.7268e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 7.0799e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 7.6123e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.7620e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.0708e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.7967e-05 - acc: 1.0000
# Training time = 0:11:35.231329
# F-Score(Ordinary) = 0.742, Recall: 0.813, Precision: 0.682
# F-Score(lvc) = 0.605, Recall: 0.719, Precision: 0.523
# F-Score(ireflv) = 0.806, Recall: 0.765, Precision: 0.852
# F-Score(id) = 0.762, Recall: 0.895, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 48)        705264      input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 24)        5640        input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 192)          0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 96)           0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 288)          0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 24)           6936        concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 24)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            200         dropout_13[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0671 - acc: 0.9861 - val_loss: 2.8670e-05 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0479 - acc: 0.9905 - val_loss: 1.3471e-05 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0449 - acc: 0.9911 - val_loss: 1.2994e-05 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0432 - acc: 0.9915 - val_loss: 1.1563e-05 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0425 - acc: 0.9916 - val_loss: 8.0467e-06 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0419 - acc: 0.9918 - val_loss: 6.9142e-06 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0412 - acc: 0.9919 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0408 - acc: 0.9919 - val_loss: 3.8743e-06 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0407 - acc: 0.9920 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0403 - acc: 0.9920 - val_loss: 3.6955e-06 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0401 - acc: 0.9921 - val_loss: 3.0398e-06 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 3.9339e-06 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0399 - acc: 0.9921 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0397 - acc: 0.9922 - val_loss: 3.5167e-06 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0396 - acc: 0.9922 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0394 - acc: 0.9922 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0393 - acc: 0.9922 - val_loss: 3.8147e-06 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0392 - acc: 0.9922 - val_loss: 3.9339e-06 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0391 - acc: 0.9923 - val_loss: 2.8014e-06 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0389 - acc: 0.9923 - val_loss: 2.8610e-06 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0389 - acc: 0.9923 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0388 - acc: 0.9923 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0389 - acc: 0.9923 - val_loss: 2.8610e-06 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0387 - acc: 0.9923 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0387 - acc: 0.9923 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0387 - acc: 0.9924 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0384 - acc: 0.9924 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.8477e-06 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0380 - acc: 0.9925 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.6896e-04 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 3.2180e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 1.7958e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.2372e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.4616e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 1.1736e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.5548e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 9.7818e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 9.0399e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 4.4715e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 7.9709e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 7.8314e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 6.8933e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 8.2515e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 6.2359e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 5.8092e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 7.1489e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 4.6174e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 4.1166e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 4.0654e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 4.0688e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 4.5127e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 7.2902e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 4.9856e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 4.2111e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 4.1085e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 3.8520e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.0386e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 2.2473e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 2.8884e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 4.6839e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 5.9958e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.7232e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 5.0445e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 8.2083e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 3.7796e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1344e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.2821e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 3.1227e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.0630e-05 - acc: 1.0000
# Training time = 0:11:28.065154
# F-Score(Ordinary) = 0.723, Recall: 0.79, Precision: 0.667
# F-Score(lvc) = 0.618, Recall: 0.853, Precision: 0.485
# F-Score(ireflv) = 0.813, Recall: 0.806, Precision: 0.82
# F-Score(id) = 0.695, Recall: 0.725, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 48)        705264      input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 24)        5640        input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 192)          0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 96)           0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 288)          0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 24)           6936        concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 24)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            200         dropout_14[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0671 - acc: 0.9859 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0479 - acc: 0.9904 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0448 - acc: 0.9911 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0433 - acc: 0.9914 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0425 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0418 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0414 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0410 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0407 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0404 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0402 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0395 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0391 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0386 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0380 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 3.8429e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 3.3949e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 3.0866e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.6995e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 1.1351e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.3720e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 3.0447e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 1.0282e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 6.7174e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 6.1439e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.0416e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 5.6223e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.2139e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.3114e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 4.0361e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 8.9580e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 8.4256e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 7.3840e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 9.5864e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 3.7071e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 9.8055e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 6.1848e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 2.1871e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.0085e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 8.6494e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 3.2132e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 6.7497e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.9207e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 2.6036e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 9.1711e-06 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.0436e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.0670e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.0221e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.7677e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.3149e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 3.7894e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.8921e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 7.9362e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.5139e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 7.5730e-05 - acc: 1.0000
# Training time = 0:11:27.166637
# F-Score(Ordinary) = 0.711, Recall: 0.72, Precision: 0.702
# F-Score(lvc) = 0.598, Recall: 0.78, Precision: 0.485
# F-Score(ireflv) = 0.789, Recall: 0.729, Precision: 0.861
# F-Score(id) = 0.675, Recall: 0.648, Precision: 0.705
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 48)        705264      input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 24)        5640        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 192)          0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 96)           0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 288)          0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 24)           6936        concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 24)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            200         dropout_15[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0693 - acc: 0.9855 - val_loss: 1.4782e-05 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0487 - acc: 0.9902 - val_loss: 6.8546e-06 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0456 - acc: 0.9910 - val_loss: 5.6029e-06 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0441 - acc: 0.9913 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0429 - acc: 0.9915 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0425 - acc: 0.9917 - val_loss: 2.2054e-06 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0419 - acc: 0.9918 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0412 - acc: 0.9920 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0408 - acc: 0.9920 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0407 - acc: 0.9920 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0404 - acc: 0.9920 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0403 - acc: 0.9921 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0399 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0398 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0396 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0396 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0393 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0392 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0392 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0392 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0390 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0389 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0387 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0386 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0386 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 3.4849e-04 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 2.9102e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 2.3038e-04 - acc: 0.9998
Epoch 4/40
 - 2s - loss: 2.0720e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 2.1124e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 1.0953e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.4095e-04 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.8119e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.0623e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 9.4739e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 9.6569e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.6765e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1405e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 9.8123e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.3768e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.4182e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 9.6297e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 6.5532e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 7.0522e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.0509e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 7.6542e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 4.7044e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 3.9300e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 4.4090e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 7.3579e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 5.1841e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 7.5052e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 2.8082e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 6.2970e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.6071e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 4.7596e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 3.5688e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 6.7480e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.4019e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.2367e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 6.4828e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 7.2080e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.5032e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.5497e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.8840e-05 - acc: 1.0000
# Training time = 0:11:24.821340
# F-Score(Ordinary) = 0.737, Recall: 0.858, Precision: 0.647
# F-Score(lvc) = 0.606, Recall: 0.767, Precision: 0.5
# F-Score(ireflv) = 0.812, Recall: 0.829, Precision: 0.795
# F-Score(id) = 0.758, Recall: 0.925, Precision: 0.642
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 48)        705264      input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 24)        5640        input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 192)          0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 96)           0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 288)          0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 24)           6936        concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 24)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            200         dropout_16[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0576 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0439 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0418 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0398 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0368 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0367 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0364 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 6.4798e-04 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 5.3264e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 7.1086e-04 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 0.0010 - acc: 0.9996
Epoch 5/40
 - 2s - loss: 7.0813e-04 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 7.8880e-04 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 8.6452e-04 - acc: 0.9996
Epoch 8/40
 - 2s - loss: 7.2858e-04 - acc: 0.9997
Epoch 9/40
 - 2s - loss: 6.2842e-04 - acc: 0.9997
Epoch 10/40
 - 2s - loss: 6.7741e-04 - acc: 0.9997
Epoch 11/40
 - 2s - loss: 5.2368e-04 - acc: 0.9998
Epoch 12/40
 - 2s - loss: 6.2997e-04 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 6.7619e-04 - acc: 0.9997
Epoch 14/40
 - 2s - loss: 6.6592e-04 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 6.8258e-04 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 6.9794e-04 - acc: 0.9996
Epoch 17/40
 - 2s - loss: 7.1232e-04 - acc: 0.9996
Epoch 18/40
 - 2s - loss: 5.9769e-04 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 5.1304e-04 - acc: 0.9997
Epoch 20/40
 - 2s - loss: 4.8143e-04 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 7.2268e-04 - acc: 0.9996
Epoch 22/40
 - 2s - loss: 5.6446e-04 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 6.0437e-04 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 5.2449e-04 - acc: 0.9997
Epoch 25/40
 - 2s - loss: 4.7065e-04 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 6.2578e-04 - acc: 0.9996
Epoch 27/40
 - 2s - loss: 5.4794e-04 - acc: 0.9997
Epoch 28/40
 - 2s - loss: 5.8452e-04 - acc: 0.9996
Epoch 29/40
 - 2s - loss: 5.7545e-04 - acc: 0.9996
Epoch 30/40
 - 2s - loss: 5.4499e-04 - acc: 0.9997
Epoch 31/40
 - 2s - loss: 4.9428e-04 - acc: 0.9997
Epoch 32/40
 - 2s - loss: 5.5033e-04 - acc: 0.9996
Epoch 33/40
 - 2s - loss: 4.5931e-04 - acc: 0.9997
Epoch 34/40
 - 2s - loss: 6.1636e-04 - acc: 0.9996
Epoch 35/40
 - 2s - loss: 4.6544e-04 - acc: 0.9997
Epoch 36/40
 - 2s - loss: 6.9579e-04 - acc: 0.9995
Epoch 37/40
 - 2s - loss: 4.3064e-04 - acc: 0.9997
Epoch 38/40
 - 2s - loss: 3.8677e-04 - acc: 0.9998
Epoch 39/40
 - 2s - loss: 5.3362e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 5.2518e-04 - acc: 1.0000
# Training time = 0:11:30.722566
# F-Score(Ordinary) = 0.737, Recall: 0.827, Precision: 0.664
# F-Score(lvc) = 0.611, Recall: 0.786, Precision: 0.5
# F-Score(ireflv) = 0.819, Recall: 0.803, Precision: 0.836
# F-Score(id) = 0.751, Recall: 0.865, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 48)        705264      input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 24)        5640        input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 192)          0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 96)           0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 288)          0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 24)           6936        concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 24)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            200         dropout_17[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0579 - acc: 0.9877 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0439 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0419 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0409 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0401 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0380 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0376 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0375 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0375 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0373 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0372 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0372 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0371 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0369 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.6585e-04 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.2216e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 6.3099e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1368e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.7096e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 4.6985e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 4.8713e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.9900e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 3.3175e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 7.4043e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.9490e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 8.2238e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.4343e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.5321e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.0870e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.6775e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.6095e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.2428e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 7.2063e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 6.3825e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.0206e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 5.5475e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.5069e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 8.8768e-06 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 9.4541e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 4.8673e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.0493e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 7.8466e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 6.2956e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 5.0738e-06 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 5.9405e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 6.2618e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 6.9569e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 7.9349e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.9251e-06 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 4.8792e-06 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.8275e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.8985e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 3.0441e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 3.7141e-05 - acc: 1.0000
# Training time = 0:11:31.132920
# F-Score(Ordinary) = 0.727, Recall: 0.775, Precision: 0.685
# F-Score(lvc) = 0.559, Recall: 0.6, Precision: 0.523
# F-Score(ireflv) = 0.816, Recall: 0.797, Precision: 0.836
# F-Score(id) = 0.754, Recall: 0.855, Precision: 0.674
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 48)        705264      input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 24)        5640        input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 192)          0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 96)           0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 288)          0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 24)           6936        concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 24)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            200         dropout_18[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0577 - acc: 0.9876 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0439 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0417 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0368 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0367 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.1687e-04 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 9.6701e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.5099e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 3.3369e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 6.0275e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1624e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 7.7028e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 3.8697e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 7.7674e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 8.6271e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 7.2126e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 6.2583e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 6.1548e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.6920e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 6.6321e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.2862e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 6.0372e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.5001e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.2685e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 9.7998e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.2751e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.2426e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 5.1138e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.3687e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 5.3096e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 5.0212e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 4.9999e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 9.2391e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.3155e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.2958e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 4.8030e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 9.5495e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 4.9693e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 6.9299e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.7591e-06 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 8.6425e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 6.3392e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 5.5657e-06 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 6.1009e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 8.5084e-05 - acc: 1.0000
# Training time = 0:11:34.342163
# F-Score(Ordinary) = 0.679, Recall: 0.663, Precision: 0.696
# F-Score(lvc) = 0.509, Recall: 0.496, Precision: 0.523
# F-Score(ireflv) = 0.791, Recall: 0.75, Precision: 0.836
# F-Score(id) = 0.677, Recall: 0.675, Precision: 0.679
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 48)        705264      input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 24)        5640        input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 192)          0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 96)           0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 288)          0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 24)           6936        concatenate_19[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 24)           0           dense_37[0][0]                   
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            200         dropout_19[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0587 - acc: 0.9875 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0444 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0420 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0409 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0402 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0390 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0381 - acc: 0.9923 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0374 - acc: 0.9925 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0373 - acc: 0.9924 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0374 - acc: 0.9925 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0371 - acc: 0.9925 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0371 - acc: 0.9925 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0368 - acc: 0.9926 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.2517e-06 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1325e-06 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.4298e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.5309e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 6.4848e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 4.7516e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 4.9053e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 7.8046e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 5.5029e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.5935e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1950e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 4.2131e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 6.0610e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.8971e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 2.2447e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.2843e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 7.6189e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.7039e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 9.7937e-06 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.2304e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.7087e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.8493e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.1040e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1802e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1106e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 3.4059e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 5.8359e-06 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 4.6058e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.3991e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 8.8679e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.9530e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.4517e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 8.6995e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.8604e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.8074e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 5.5950e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.9005e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 6.1585e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 4.6252e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.0704e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 6.3120e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.4573e-05 - acc: 1.0000
# Training time = 0:11:39.668099
# F-Score(Ordinary) = 0.715, Recall: 0.745, Precision: 0.687
# F-Score(lvc) = 0.595, Recall: 0.836, Precision: 0.462
# F-Score(ireflv) = 0.811, Recall: 0.78, Precision: 0.844
# F-Score(id) = 0.655, Recall: 0.633, Precision: 0.679
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_39 (Embedding)        (None, 4, 48)        705264      input_39[0][0]                   
__________________________________________________________________________________________________
embedding_40 (Embedding)        (None, 4, 24)        5640        input_40[0][0]                   
__________________________________________________________________________________________________
flatten_39 (Flatten)            (None, 192)          0           embedding_39[0][0]               
__________________________________________________________________________________________________
flatten_40 (Flatten)            (None, 96)           0           embedding_40[0][0]               
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 288)          0           flatten_39[0][0]                 
                                                                 flatten_40[0][0]                 
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 24)           6936        concatenate_20[0][0]             
__________________________________________________________________________________________________
dropout_20 (Dropout)            (None, 24)           0           dense_39[0][0]                   
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 8)            200         dropout_20[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0588 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0445 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0424 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0413 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0404 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0402 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0396 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0389 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0380 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0379 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0379 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0375 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0375 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0372 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0371 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0371 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 4.0328e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.7530e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.1456e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 3.7501e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 1.8991e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.9386e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 1.1781e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 5.8149e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.9735e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 1.0316e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 5.2898e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 7.8416e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 9.1190e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 5.2503e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.2471e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.9080e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 2.7990e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 7.8414e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.6619e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.8723e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 5.9790e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.7359e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 4.7332e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 6.7731e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 2.0368e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 5.3623e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 4.8661e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.0296e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 6.0086e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.0804e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.0433e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 9.8413e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 9.6412e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 8.6922e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.3539e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 4.9365e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1249e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 5.3262e-06 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 5.4207e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 9.1774e-05 - acc: 1.0000
# Training time = 0:11:33.391910
# F-Score(Ordinary) = 0.728, Recall: 0.796, Precision: 0.671
# F-Score(lvc) = 0.585, Recall: 0.822, Precision: 0.455
# F-Score(ireflv) = 0.842, Recall: 0.856, Precision: 0.828
# F-Score(id) = 0.702, Recall: 0.715, Precision: 0.689
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_41 (Embedding)        (None, 4, 48)        705264      input_41[0][0]                   
__________________________________________________________________________________________________
embedding_42 (Embedding)        (None, 4, 24)        5640        input_42[0][0]                   
__________________________________________________________________________________________________
flatten_41 (Flatten)            (None, 192)          0           embedding_41[0][0]               
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 96)           0           embedding_42[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 288)          0           flatten_41[0][0]                 
                                                                 flatten_42[0][0]                 
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 24)           6936        concatenate_21[0][0]             
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 24)           0           dense_41[0][0]                   
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 8)            200         dropout_21[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0567 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0428 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0385 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0371 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0362 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0362 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0359 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0359 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.4556e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.2655e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 1.1738e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.3824e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 1.5886e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 1.3188e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 1.2835e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 5.5757e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 5.3584e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.9683e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 9.9851e-05 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 9.8582e-05 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.2072e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 5.0360e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1857e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 1.4034e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 9.2867e-05 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 1.3654e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 4.6708e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 9.0426e-05 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.5443e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 1.3083e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 1.2952e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 1.0650e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 1.4672e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 1.0400e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.0273e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.6167e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 8.0392e-05 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 1.1898e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 9.8152e-05 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 7.7967e-05 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 2.0307e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 9.6044e-05 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 9.5042e-05 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 9.4379e-05 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 1.1149e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 5.5726e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 5.5326e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.6256e-04 - acc: 0.9999
# Training time = 0:11:31.979997
# F-Score(Ordinary) = 0.724, Recall: 0.759, Precision: 0.691
# F-Score(lvc) = 0.601, Recall: 0.693, Precision: 0.53
# F-Score(ireflv) = 0.846, Recall: 0.857, Precision: 0.836
# F-Score(id) = 0.689, Recall: 0.701, Precision: 0.679
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 4, 48)        705264      input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 4, 24)        5640        input_44[0][0]                   
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 192)          0           embedding_43[0][0]               
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 96)           0           embedding_44[0][0]               
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 288)          0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 24)           6936        concatenate_22[0][0]             
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 24)           0           dense_43[0][0]                   
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 8)            200         dropout_22[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0595 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0441 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0418 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0382 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 9.7285e-04 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 0.0011 - acc: 0.9996
Epoch 3/40
 - 2s - loss: 5.2574e-04 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 5.6003e-04 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 4.2071e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 1.8142e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.7172e-04 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.3527e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.5692e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.2763e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.9917e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 2.9913e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 5.7837e-06 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 4.4304e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 3.6773e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 5.8279e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.1898e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 5.0556e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 3.6160e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 2.1675e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.4460e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.1574e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.4400e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.4328e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 5.6763e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 5.6469e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 2.1166e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 5.5977e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 3.4872e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 3.4752e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 3.4655e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.2720e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.4489e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 2.0663e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.7488e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 2.0573e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 3.4135e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 1.0632e-06 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 3.4001e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 1.3609e-04 - acc: 1.0000
# Training time = 0:11:32.511031
# F-Score(Ordinary) = 0.701, Recall: 0.702, Precision: 0.7
# F-Score(lvc) = 0.611, Recall: 0.722, Precision: 0.53
# F-Score(ireflv) = 0.772, Recall: 0.73, Precision: 0.82
# F-Score(id) = 0.686, Recall: 0.656, Precision: 0.72
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_45 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 4, 48)        705264      input_45[0][0]                   
__________________________________________________________________________________________________
embedding_46 (Embedding)        (None, 4, 24)        5640        input_46[0][0]                   
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 192)          0           embedding_45[0][0]               
__________________________________________________________________________________________________
flatten_46 (Flatten)            (None, 96)           0           embedding_46[0][0]               
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 288)          0           flatten_45[0][0]                 
                                                                 flatten_46[0][0]                 
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 24)           6936        concatenate_23[0][0]             
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 24)           0           dense_45[0][0]                   
__________________________________________________________________________________________________
dense_46 (Dense)                (None, 8)            200         dropout_23[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0577 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0434 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0412 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0362 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0362 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 7.1182e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.6387e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 3.7229e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.5216e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 3.6392e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.9137e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 5.0081e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 3.5626e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 7.3382e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.8361e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 3.5260e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 4.8946e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.4112e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 4.8538e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 1.3975e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.3887e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 2.7574e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 4.7829e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.0478e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 5.4145e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 4.0370e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 2.0178e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.3459e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 3.9998e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 3.9776e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 8.5365e-04 - acc: 0.9998
Epoch 27/40
 - 2s - loss: 3.9124e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 6.5839e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 3.2444e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 3.2312e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 4.4940e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 3.8322e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.9132e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 4.4317e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 2.5242e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 3.1413e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 1.2568e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.8775e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.2508e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.3447e-04 - acc: 0.9999
# Training time = 0:11:38.092625
# F-Score(Ordinary) = 0.653, Recall: 0.597, Precision: 0.72
# F-Score(lvc) = 0.532, Recall: 0.526, Precision: 0.538
# F-Score(ireflv) = 0.736, Recall: 0.652, Precision: 0.844
# F-Score(id) = 0.61, Recall: 0.545, Precision: 0.694
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_47 (Embedding)        (None, 4, 48)        705264      input_47[0][0]                   
__________________________________________________________________________________________________
embedding_48 (Embedding)        (None, 4, 24)        5640        input_48[0][0]                   
__________________________________________________________________________________________________
flatten_47 (Flatten)            (None, 192)          0           embedding_47[0][0]               
__________________________________________________________________________________________________
flatten_48 (Flatten)            (None, 96)           0           embedding_48[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 288)          0           flatten_47[0][0]                 
                                                                 flatten_48[0][0]                 
__________________________________________________________________________________________________
dense_47 (Dense)                (None, 24)           6936        concatenate_24[0][0]             
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 24)           0           dense_47[0][0]                   
__________________________________________________________________________________________________
dense_48 (Dense)                (None, 8)            200         dropout_24[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0579 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0434 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0411 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0364 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0361 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0360 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0361 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0359 - acc: 0.9931 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0360 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0359 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0361 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0360 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 9.7914e-04 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 9.5949e-04 - acc: 0.9996
Epoch 3/40
 - 2s - loss: 8.2061e-04 - acc: 0.9996
Epoch 4/40
 - 2s - loss: 4.9212e-04 - acc: 0.9998
Epoch 5/40
 - 2s - loss: 4.9879e-04 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 4.5568e-04 - acc: 0.9998
Epoch 7/40
 - 2s - loss: 8.2245e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 3.0321e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 2.6716e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 1.6503e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 2.2540e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 3.1680e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 2.1945e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 1.8685e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 2.4608e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 1.7265e-06 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.5293e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 1.2179e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 1.2078e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 3.5657e-04 - acc: 0.9998
Epoch 21/40
 - 2s - loss: 1.7660e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 1.1740e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 2.9868e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.3126e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 1.4356e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 1.7122e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 8.5385e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.4105e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 8.4351e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.3940e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.1105e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 5.5607e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 8.2947e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.3681e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 8.1891e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0855e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 2.1456e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 1.0677e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 1.5874e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 5.2974e-05 - acc: 1.0000
# Training time = 0:11:31.756246
# F-Score(Ordinary) = 0.667, Recall: 0.649, Precision: 0.687
# F-Score(lvc) = 0.584, Recall: 0.736, Precision: 0.485
# F-Score(ireflv) = 0.839, Recall: 0.868, Precision: 0.811
# F-Score(id) = 0.589, Recall: 0.504, Precision: 0.71
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_49 (Embedding)        (None, 4, 48)        705264      input_49[0][0]                   
__________________________________________________________________________________________________
embedding_50 (Embedding)        (None, 4, 24)        5640        input_50[0][0]                   
__________________________________________________________________________________________________
flatten_49 (Flatten)            (None, 192)          0           embedding_49[0][0]               
__________________________________________________________________________________________________
flatten_50 (Flatten)            (None, 96)           0           embedding_50[0][0]               
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 288)          0           flatten_49[0][0]                 
                                                                 flatten_50[0][0]                 
__________________________________________________________________________________________________
dense_49 (Dense)                (None, 24)           6936        concatenate_25[0][0]             
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 24)           0           dense_49[0][0]                   
__________________________________________________________________________________________________
dense_50 (Dense)                (None, 8)            200         dropout_25[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0587 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0442 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0417 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0365 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0365 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0364 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0364 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0011 - acc: 0.9995
Epoch 2/40
 - 2s - loss: 5.7469e-04 - acc: 0.9997
Epoch 3/40
 - 2s - loss: 5.5228e-04 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 8.0542e-04 - acc: 0.9996
Epoch 5/40
 - 2s - loss: 4.3088e-04 - acc: 0.9998
Epoch 6/40
 - 2s - loss: 5.9851e-04 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 4.2999e-04 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 7.5248e-04 - acc: 0.9996
Epoch 9/40
 - 2s - loss: 5.1407e-04 - acc: 0.9997
Epoch 10/40
 - 2s - loss: 4.5283e-04 - acc: 0.9997
Epoch 11/40
 - 2s - loss: 4.1785e-04 - acc: 0.9997
Epoch 12/40
 - 2s - loss: 5.5232e-04 - acc: 0.9996
Epoch 13/40
 - 2s - loss: 3.9098e-04 - acc: 0.9997
Epoch 14/40
 - 2s - loss: 4.9746e-04 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 4.0428e-04 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 5.5593e-04 - acc: 0.9996
Epoch 17/40
 - 2s - loss: 3.7531e-04 - acc: 0.9997
Epoch 18/40
 - 2s - loss: 3.3000e-04 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 2.7121e-04 - acc: 0.9998
Epoch 20/40
 - 2s - loss: 2.8137e-04 - acc: 0.9998
Epoch 21/40
 - 2s - loss: 3.3812e-04 - acc: 0.9997
Epoch 22/40
 - 2s - loss: 3.1289e-04 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 3.7887e-04 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 2.5056e-04 - acc: 0.9998
Epoch 25/40
 - 2s - loss: 4.6928e-04 - acc: 0.9996
Epoch 26/40
 - 2s - loss: 4.3549e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 3.1477e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 2.9252e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 2.5950e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.4679e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 3.1161e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 3.0122e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.0190e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.0192e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.2223e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.8410e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.4673e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.3942e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.9720e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.2681e-04 - acc: 1.0000
# Training time = 0:11:34.803692
# F-Score(Ordinary) = 0.695, Recall: 0.697, Precision: 0.694
# F-Score(lvc) = 0.573, Recall: 0.684, Precision: 0.492
# F-Score(ireflv) = 0.837, Recall: 0.831, Precision: 0.844
# F-Score(id) = 0.625, Recall: 0.58, Precision: 0.679
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_51 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_51 (Embedding)        (None, 4, 48)        705264      input_51[0][0]                   
__________________________________________________________________________________________________
embedding_52 (Embedding)        (None, 4, 24)        5640        input_52[0][0]                   
__________________________________________________________________________________________________
flatten_51 (Flatten)            (None, 192)          0           embedding_51[0][0]               
__________________________________________________________________________________________________
flatten_52 (Flatten)            (None, 96)           0           embedding_52[0][0]               
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 288)          0           flatten_51[0][0]                 
                                                                 flatten_52[0][0]                 
__________________________________________________________________________________________________
dense_51 (Dense)                (None, 24)           6936        concatenate_26[0][0]             
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 24)           0           dense_51[0][0]                   
__________________________________________________________________________________________________
dense_52 (Dense)                (None, 8)            200         dropout_26[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.2877 - acc: 0.9222 - val_loss: 3.5328e-04 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0946 - acc: 0.9787 - val_loss: 4.7446e-05 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0749 - acc: 0.9834 - val_loss: 1.2517e-05 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0676 - acc: 0.9852 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0636 - acc: 0.9863 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0610 - acc: 0.9870 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0590 - acc: 0.9876 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0574 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0561 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0551 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0544 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0539 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0531 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0525 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0521 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0516 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0511 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0508 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0504 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0503 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0500 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0497 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0497 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0493 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0492 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0489 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0488 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0486 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0485 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0482 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0482 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0479 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0476 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0475 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0013 - acc: 0.9996
Epoch 2/40
 - 2s - loss: 7.1597e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 8.9332e-04 - acc: 0.9998
Epoch 4/40
 - 2s - loss: 6.5071e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 6.0584e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 6.6227e-04 - acc: 0.9998
Epoch 7/40
 - 2s - loss: 6.9092e-04 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 3.8551e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 4.9900e-04 - acc: 0.9998
Epoch 10/40
 - 2s - loss: 4.1384e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 2.7111e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 3.3063e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 2.8968e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 2.9401e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 2.3603e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 2.8284e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 3.2041e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 2.1075e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 3.5329e-04 - acc: 0.9998
Epoch 20/40
 - 2s - loss: 2.6857e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 2.3588e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 1.9729e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 2.5648e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 2.8544e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 2.3863e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 2.9347e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 2.3255e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 3.1854e-04 - acc: 0.9998
Epoch 29/40
 - 2s - loss: 2.3785e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 2.1886e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.6233e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.2838e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.5456e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 1.3904e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.5115e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 2.0018e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 1.9829e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 2.0622e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 1.6734e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 1.2224e-04 - acc: 0.9999
# Training time = 0:13:28.173210
# F-Score(Ordinary) = 0.613, Recall: 0.826, Precision: 0.488
# F-Score(lvc) = 0.426, Recall: 0.973, Precision: 0.273
# F-Score(ireflv) = 0.753, Recall: 0.832, Precision: 0.689
# F-Score(id) = 0.614, Recall: 0.778, Precision: 0.508
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_53 (Embedding)        (None, 4, 48)        705264      input_53[0][0]                   
__________________________________________________________________________________________________
embedding_54 (Embedding)        (None, 4, 24)        5640        input_54[0][0]                   
__________________________________________________________________________________________________
flatten_53 (Flatten)            (None, 192)          0           embedding_53[0][0]               
__________________________________________________________________________________________________
flatten_54 (Flatten)            (None, 96)           0           embedding_54[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 288)          0           flatten_53[0][0]                 
                                                                 flatten_54[0][0]                 
__________________________________________________________________________________________________
dense_53 (Dense)                (None, 24)           6936        concatenate_27[0][0]             
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 24)           0           dense_53[0][0]                   
__________________________________________________________________________________________________
dense_54 (Dense)                (None, 8)            200         dropout_27[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.2997 - acc: 0.9179 - val_loss: 3.4744e-04 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0974 - acc: 0.9781 - val_loss: 6.9859e-05 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0763 - acc: 0.9834 - val_loss: 2.3365e-05 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0686 - acc: 0.9852 - val_loss: 6.3181e-06 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0643 - acc: 0.9862 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0616 - acc: 0.9870 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0596 - acc: 0.9876 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0580 - acc: 0.9880 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0566 - acc: 0.9882 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0556 - acc: 0.9885 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0548 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0538 - acc: 0.9889 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0535 - acc: 0.9890 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0528 - acc: 0.9892 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0525 - acc: 0.9893 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0519 - acc: 0.9894 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0517 - acc: 0.9894 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0510 - acc: 0.9894 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0508 - acc: 0.9895 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0506 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0503 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0496 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0493 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0494 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0491 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0488 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0486 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0484 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0482 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0482 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0480 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0477 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0475 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0474 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0474 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0472 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0019 - acc: 0.9991
Epoch 2/40
 - 2s - loss: 0.0011 - acc: 0.9996
Epoch 3/40
 - 2s - loss: 8.4921e-04 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 6.1173e-04 - acc: 0.9998
Epoch 5/40
 - 2s - loss: 7.1740e-04 - acc: 0.9998
Epoch 6/40
 - 2s - loss: 6.0446e-04 - acc: 0.9998
Epoch 7/40
 - 2s - loss: 2.9690e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 3.4538e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 4.0474e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 3.2274e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.9862e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 3.2150e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.8934e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 1.9704e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 1.6824e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.1456e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.1565e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 2.7086e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.1233e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 2.8869e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.3366e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 2.1815e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 2.0617e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 8.9287e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.6373e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 1.8340e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.3584e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.2059e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 1.9070e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 1.2144e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 2.0303e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.8258e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.5563e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 1.2797e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 3.7120e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 9.4372e-05 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 1.3951e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 7.9080e-05 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 3.7731e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.9288e-04 - acc: 0.9999
# Training time = 0:13:25.993097
# F-Score(Ordinary) = 0.491, Recall: 0.856, Precision: 0.345
# F-Score(lvc) = 0.293, Recall: 0.75, Precision: 0.182
# F-Score(ireflv) = 0.577, Recall: 0.778, Precision: 0.459
# F-Score(id) = 0.55, Recall: 0.974, Precision: 0.383
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_55 (Embedding)        (None, 4, 48)        705264      input_55[0][0]                   
__________________________________________________________________________________________________
embedding_56 (Embedding)        (None, 4, 24)        5640        input_56[0][0]                   
__________________________________________________________________________________________________
flatten_55 (Flatten)            (None, 192)          0           embedding_55[0][0]               
__________________________________________________________________________________________________
flatten_56 (Flatten)            (None, 96)           0           embedding_56[0][0]               
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 288)          0           flatten_55[0][0]                 
                                                                 flatten_56[0][0]                 
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 24)           6936        concatenate_28[0][0]             
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 24)           0           dense_55[0][0]                   
__________________________________________________________________________________________________
dense_56 (Dense)                (None, 8)            200         dropout_28[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.2958 - acc: 0.9177 - val_loss: 4.1165e-04 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0960 - acc: 0.9784 - val_loss: 1.0831e-04 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0764 - acc: 0.9836 - val_loss: 5.8891e-05 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0693 - acc: 0.9853 - val_loss: 3.8386e-05 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0655 - acc: 0.9862 - val_loss: 3.2664e-05 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0629 - acc: 0.9869 - val_loss: 2.7001e-05 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0606 - acc: 0.9875 - val_loss: 2.1696e-05 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0587 - acc: 0.9878 - val_loss: 2.1160e-05 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0578 - acc: 0.9880 - val_loss: 1.7285e-05 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0567 - acc: 0.9883 - val_loss: 1.9372e-05 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0556 - acc: 0.9886 - val_loss: 1.6570e-05 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0549 - acc: 0.9888 - val_loss: 1.7822e-05 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0544 - acc: 0.9889 - val_loss: 1.6093e-05 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0537 - acc: 0.9890 - val_loss: 1.3530e-05 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0531 - acc: 0.9892 - val_loss: 1.2219e-05 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0526 - acc: 0.9893 - val_loss: 1.1504e-05 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0520 - acc: 0.9893 - val_loss: 1.0371e-05 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0519 - acc: 0.9894 - val_loss: 1.0848e-05 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0515 - acc: 0.9895 - val_loss: 8.4043e-06 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0509 - acc: 0.9897 - val_loss: 7.8678e-06 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0510 - acc: 0.9896 - val_loss: 7.9274e-06 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0507 - acc: 0.9897 - val_loss: 7.8082e-06 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0504 - acc: 0.9898 - val_loss: 8.0467e-06 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0502 - acc: 0.9898 - val_loss: 6.4373e-06 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0500 - acc: 0.9899 - val_loss: 6.7950e-06 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0499 - acc: 0.9898 - val_loss: 5.0068e-06 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0496 - acc: 0.9899 - val_loss: 4.4704e-06 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0493 - acc: 0.9899 - val_loss: 4.6492e-06 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0493 - acc: 0.9900 - val_loss: 4.1127e-06 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0491 - acc: 0.9900 - val_loss: 4.3511e-06 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0489 - acc: 0.9900 - val_loss: 3.6955e-06 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0487 - acc: 0.9901 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0485 - acc: 0.9901 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0487 - acc: 0.9901 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0485 - acc: 0.9902 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0485 - acc: 0.9902 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0484 - acc: 0.9902 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0482 - acc: 0.9902 - val_loss: 2.3246e-06 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0482 - acc: 0.9902 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0481 - acc: 0.9903 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0018 - acc: 0.9994
Epoch 2/40
 - 2s - loss: 0.0015 - acc: 0.9995
Epoch 3/40
 - 2s - loss: 0.0011 - acc: 0.9996
Epoch 4/40
 - 2s - loss: 9.3198e-04 - acc: 0.9995
Epoch 5/40
 - 2s - loss: 9.2891e-04 - acc: 0.9995
Epoch 6/40
 - 2s - loss: 7.3180e-04 - acc: 0.9996
Epoch 7/40
 - 2s - loss: 6.0100e-04 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 4.9889e-04 - acc: 0.9998
Epoch 9/40
 - 2s - loss: 2.9961e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.8736e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 4.2125e-04 - acc: 0.9998
Epoch 12/40
 - 2s - loss: 1.7534e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.9721e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 1.9885e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 1.3806e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.2677e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.6761e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.6102e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 9.1079e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.0254e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 9.9012e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.2086e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 7.3964e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.2242e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 5.2825e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 9.6545e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 4.5511e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.0955e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 4.5021e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.5137e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 5.2123e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 9.4041e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.1219e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.9765e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 6.6139e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 5.3651e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.4253e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.3311e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 3.5054e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.6738e-05 - acc: 1.0000
# Training time = 0:13:42.845998
# F-Score(Ordinary) = 0.592, Recall: 0.892, Precision: 0.443
# F-Score(lvc) = 0.442, Recall: 0.95, Precision: 0.288
# F-Score(ireflv) = 0.81, Recall: 0.817, Precision: 0.803
# F-Score(id) = 0.486, Recall: 1.0, Precision: 0.321
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_57 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_58 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_57 (Embedding)        (None, 4, 48)        705264      input_57[0][0]                   
__________________________________________________________________________________________________
embedding_58 (Embedding)        (None, 4, 24)        5640        input_58[0][0]                   
__________________________________________________________________________________________________
flatten_57 (Flatten)            (None, 192)          0           embedding_57[0][0]               
__________________________________________________________________________________________________
flatten_58 (Flatten)            (None, 96)           0           embedding_58[0][0]               
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 288)          0           flatten_57[0][0]                 
                                                                 flatten_58[0][0]                 
__________________________________________________________________________________________________
dense_57 (Dense)                (None, 24)           6936        concatenate_29[0][0]             
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 24)           0           dense_57[0][0]                   
__________________________________________________________________________________________________
dense_58 (Dense)                (None, 8)            200         dropout_29[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.3115 - acc: 0.9145 - val_loss: 3.0033e-04 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.1019 - acc: 0.9767 - val_loss: 2.8372e-05 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0781 - acc: 0.9828 - val_loss: 8.3447e-06 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0695 - acc: 0.9851 - val_loss: 2.6822e-06 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0652 - acc: 0.9863 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0626 - acc: 0.9869 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0608 - acc: 0.9873 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0591 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0579 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0570 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0559 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0551 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0546 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0540 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0535 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0529 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0525 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0521 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0518 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0518 - acc: 0.9895 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0511 - acc: 0.9896 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0509 - acc: 0.9896 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0505 - acc: 0.9898 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0506 - acc: 0.9897 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0502 - acc: 0.9898 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0500 - acc: 0.9898 - val_loss: 8.9407e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0499 - acc: 0.9899 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0496 - acc: 0.9899 - val_loss: 1.6093e-06 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0494 - acc: 0.9899 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0492 - acc: 0.9900 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0489 - acc: 0.9900 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0488 - acc: 0.9900 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0488 - acc: 0.9901 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0487 - acc: 0.9900 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0486 - acc: 0.9902 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0483 - acc: 0.9901 - val_loss: 3.7551e-06 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0484 - acc: 0.9901 - val_loss: 3.1591e-06 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0482 - acc: 0.9902 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0482 - acc: 0.9903 - val_loss: 3.1591e-06 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0482 - acc: 0.9902 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0017 - acc: 0.9995
Epoch 2/40
 - 2s - loss: 0.0016 - acc: 0.9995
Epoch 3/40
 - 2s - loss: 9.4543e-04 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 8.6723e-04 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 6.8672e-04 - acc: 0.9998
Epoch 6/40
 - 2s - loss: 6.1581e-04 - acc: 0.9998
Epoch 7/40
 - 2s - loss: 5.2259e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 5.7323e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 3.3765e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 4.1038e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 4.0024e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 2.5634e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 2.2286e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.6618e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 1.1422e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.9977e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.8555e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 2.0268e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.1156e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 1.2328e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.4469e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 7.7234e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.7866e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 7.8236e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.5198e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 1.6909e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.8950e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 7.9327e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.0047e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.0143e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.2417e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.8438e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 9.0370e-05 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 1.1813e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 1.0554e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.2716e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 1.5689e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 1.3888e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 1.0288e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.1436e-04 - acc: 0.9999
# Training time = 0:13:50.587904
# F-Score(Ordinary) = 0.515, Recall: 0.503, Precision: 0.528
# F-Score(lvc) = 0.346, Recall: 0.933, Precision: 0.212
# F-Score(ireflv) = 0.739, Recall: 0.63, Precision: 0.893
# F-Score(id) = 0.432, Recall: 0.399, Precision: 0.472
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_59 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_60 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_59 (Embedding)        (None, 4, 48)        705264      input_59[0][0]                   
__________________________________________________________________________________________________
embedding_60 (Embedding)        (None, 4, 24)        5640        input_60[0][0]                   
__________________________________________________________________________________________________
flatten_59 (Flatten)            (None, 192)          0           embedding_59[0][0]               
__________________________________________________________________________________________________
flatten_60 (Flatten)            (None, 96)           0           embedding_60[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 288)          0           flatten_59[0][0]                 
                                                                 flatten_60[0][0]                 
__________________________________________________________________________________________________
dense_59 (Dense)                (None, 24)           6936        concatenate_30[0][0]             
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 24)           0           dense_59[0][0]                   
__________________________________________________________________________________________________
dense_60 (Dense)                (None, 8)            200         dropout_30[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.2921 - acc: 0.9191 - val_loss: 6.3553e-04 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0998 - acc: 0.9774 - val_loss: 1.8211e-04 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0785 - acc: 0.9828 - val_loss: 6.0679e-05 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0709 - acc: 0.9846 - val_loss: 2.4319e-05 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0667 - acc: 0.9858 - val_loss: 9.8944e-06 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0640 - acc: 0.9865 - val_loss: 4.3511e-06 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0617 - acc: 0.9870 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0601 - acc: 0.9873 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0587 - acc: 0.9878 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0575 - acc: 0.9881 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0566 - acc: 0.9883 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0557 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0550 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0543 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0536 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0531 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0528 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0524 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0518 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0514 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0511 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0510 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0506 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0505 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0501 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0498 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0495 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0494 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0493 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0492 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0489 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0488 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0485 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0485 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0484 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0484 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0483 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0482 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0481 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0023 - acc: 0.9993
Epoch 2/40
 - 2s - loss: 0.0027 - acc: 0.9992
Epoch 3/40
 - 2s - loss: 0.0022 - acc: 0.9993
Epoch 4/40
 - 2s - loss: 0.0017 - acc: 0.9993
Epoch 5/40
 - 2s - loss: 0.0020 - acc: 0.9994
Epoch 6/40
 - 2s - loss: 0.0015 - acc: 0.9996
Epoch 7/40
 - 2s - loss: 0.0016 - acc: 0.9996
Epoch 8/40
 - 2s - loss: 0.0016 - acc: 0.9995
Epoch 9/40
 - 2s - loss: 0.0013 - acc: 0.9996
Epoch 10/40
 - 2s - loss: 0.0010 - acc: 0.9996
Epoch 11/40
 - 2s - loss: 8.3415e-04 - acc: 0.9997
Epoch 12/40
 - 2s - loss: 9.1550e-04 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 9.1991e-04 - acc: 0.9996
Epoch 14/40
 - 2s - loss: 8.1598e-04 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 9.4723e-04 - acc: 0.9995
Epoch 16/40
 - 2s - loss: 7.2435e-04 - acc: 0.9996
Epoch 17/40
 - 2s - loss: 5.1761e-04 - acc: 0.9997
Epoch 18/40
 - 2s - loss: 6.2885e-04 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 6.7541e-04 - acc: 0.9996
Epoch 20/40
 - 2s - loss: 5.1120e-04 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 5.2824e-04 - acc: 0.9998
Epoch 22/40
 - 2s - loss: 5.2937e-04 - acc: 0.9998
Epoch 23/40
 - 2s - loss: 3.4535e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 4.1911e-04 - acc: 0.9998
Epoch 25/40
 - 2s - loss: 2.9742e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 5.1949e-04 - acc: 0.9998
Epoch 27/40
 - 2s - loss: 4.0394e-04 - acc: 0.9998
Epoch 28/40
 - 2s - loss: 6.6180e-04 - acc: 0.9998
Epoch 29/40
 - 2s - loss: 4.4887e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 4.0961e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 3.3294e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 3.4806e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 2.3643e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 3.4025e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 4.8210e-04 - acc: 0.9998
Epoch 36/40
 - 2s - loss: 2.3372e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 4.4852e-04 - acc: 0.9998
Epoch 38/40
 - 2s - loss: 1.9797e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 4.4651e-04 - acc: 0.9998
Epoch 40/40
 - 2s - loss: 4.4138e-04 - acc: 0.9998
# Training time = 0:13:47.462306
# F-Score(Ordinary) = 0.571, Recall: 0.697, Precision: 0.483
# F-Score(lvc) = 0.402, Recall: 0.919, Precision: 0.258
# F-Score(ireflv) = 0.787, Recall: 0.787, Precision: 0.787
# F-Score(id) = 0.488, Recall: 0.556, Precision: 0.435
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_61 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_62 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_61 (Embedding)        (None, 4, 48)        705264      input_61[0][0]                   
__________________________________________________________________________________________________
embedding_62 (Embedding)        (None, 4, 24)        5640        input_62[0][0]                   
__________________________________________________________________________________________________
flatten_61 (Flatten)            (None, 192)          0           embedding_61[0][0]               
__________________________________________________________________________________________________
flatten_62 (Flatten)            (None, 96)           0           embedding_62[0][0]               
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 288)          0           flatten_61[0][0]                 
                                                                 flatten_62[0][0]                 
__________________________________________________________________________________________________
dense_61 (Dense)                (None, 24)           6936        concatenate_31[0][0]             
__________________________________________________________________________________________________
dropout_31 (Dropout)            (None, 24)           0           dense_61[0][0]                   
__________________________________________________________________________________________________
dense_62 (Dense)                (None, 8)            200         dropout_31[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.1183 - acc: 0.9710 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0590 - acc: 0.9876 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0541 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0516 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0504 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0494 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0489 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0485 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0478 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0476 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0475 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0475 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0474 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0473 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0471 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0473 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0474 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0475 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0476 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0474 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0477 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0476 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0479 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0478 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0481 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0482 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0483 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0484 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0486 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0489 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0487 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0492 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0489 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 7.4381e-04 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 3.4623e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 4.2421e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.4141e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 3.5815e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.8587e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 3.2993e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.0567e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 1.5862e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.9876e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.3903e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 2.4879e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 4.0683e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.8971e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 7.6580e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.3168e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 2.2301e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 1.8527e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.3653e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 1.0809e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.1231e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 5.6932e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 7.0605e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.9464e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 2.4033e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 3.1355e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.0071e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.5512e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 2.2953e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 1.8465e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.5143e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.5053e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.2642e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 1.5615e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 8.5817e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.0853e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 1.1568e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.8118e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 1.7374e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 2.0163e-04 - acc: 0.9999
# Training time = 0:13:22.911811
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_63 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_64 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_63 (Embedding)        (None, 4, 48)        705264      input_63[0][0]                   
__________________________________________________________________________________________________
embedding_64 (Embedding)        (None, 4, 24)        5640        input_64[0][0]                   
__________________________________________________________________________________________________
flatten_63 (Flatten)            (None, 192)          0           embedding_63[0][0]               
__________________________________________________________________________________________________
flatten_64 (Flatten)            (None, 96)           0           embedding_64[0][0]               
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 288)          0           flatten_63[0][0]                 
                                                                 flatten_64[0][0]                 
__________________________________________________________________________________________________
dense_63 (Dense)                (None, 24)           6936        concatenate_32[0][0]             
__________________________________________________________________________________________________
dropout_32 (Dropout)            (None, 24)           0           dense_63[0][0]                   
__________________________________________________________________________________________________
dense_64 (Dense)                (None, 8)            200         dropout_32[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.1222 - acc: 0.9700 - val_loss: 3.3975e-06 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0598 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0549 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0524 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0508 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0497 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0490 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0483 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0478 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0475 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0474 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0470 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0470 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0469 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0469 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0468 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0468 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0464 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0466 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0467 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0466 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0466 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0465 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0464 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0470 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0468 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0467 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0470 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0470 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0473 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0475 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0476 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0476 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0477 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0476 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0475 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0018 - acc: 0.9995
Epoch 2/40
 - 2s - loss: 0.0014 - acc: 0.9996
Epoch 3/40
 - 2s - loss: 0.0013 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 9.6625e-04 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 7.7829e-04 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 8.9717e-04 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 7.2029e-04 - acc: 0.9996
Epoch 8/40
 - 2s - loss: 5.1669e-04 - acc: 0.9998
Epoch 9/40
 - 2s - loss: 3.2256e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 3.4509e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 2.4094e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 3.4003e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.2108e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.7416e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.5294e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.2432e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 3.0283e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 4.9418e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.5929e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 5.7374e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 3.3347e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 1.0476e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 2.0129e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 1.9652e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 2.8503e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 4.6629e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 3.7484e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 3.1842e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 2.8109e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 4.8563e-04 - acc: 0.9998
Epoch 31/40
 - 2s - loss: 1.7605e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 2.6982e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 4.4463e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.2811e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 8.4438e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 8.3290e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.7192e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 1.4358e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 7.5477e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.5826e-04 - acc: 0.9999
# Training time = 0:13:26.290540
# F-Score(Ordinary) = 0.175, Recall: 0.786, Precision: 0.098
# F-Score(lvc) = 0.153, Recall: 0.917, Precision: 0.083
# F-Score(ireflv) = 0.107, Recall: 0.778, Precision: 0.057
# F-Score(id) = 0.23, Recall: 0.788, Precision: 0.135
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_65 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_66 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_65 (Embedding)        (None, 4, 48)        705264      input_65[0][0]                   
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 4, 24)        5640        input_66[0][0]                   
__________________________________________________________________________________________________
flatten_65 (Flatten)            (None, 192)          0           embedding_65[0][0]               
__________________________________________________________________________________________________
flatten_66 (Flatten)            (None, 96)           0           embedding_66[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 288)          0           flatten_65[0][0]                 
                                                                 flatten_66[0][0]                 
__________________________________________________________________________________________________
dense_65 (Dense)                (None, 24)           6936        concatenate_33[0][0]             
__________________________________________________________________________________________________
dropout_33 (Dropout)            (None, 24)           0           dense_65[0][0]                   
__________________________________________________________________________________________________
dense_66 (Dense)                (None, 8)            200         dropout_33[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.1206 - acc: 0.9701 - val_loss: 1.8716e-05 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0602 - acc: 0.9875 - val_loss: 1.3590e-05 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0554 - acc: 0.9887 - val_loss: 1.2875e-05 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0525 - acc: 0.9893 - val_loss: 1.3471e-05 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0512 - acc: 0.9896 - val_loss: 1.1444e-05 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0501 - acc: 0.9898 - val_loss: 6.1393e-06 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0491 - acc: 0.9900 - val_loss: 5.6029e-06 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0485 - acc: 0.9901 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0484 - acc: 0.9902 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0481 - acc: 0.9902 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0480 - acc: 0.9903 - val_loss: 2.0862e-06 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0479 - acc: 0.9904 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0478 - acc: 0.9904 - val_loss: 2.2054e-06 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0477 - acc: 0.9904 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0477 - acc: 0.9905 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0477 - acc: 0.9904 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0476 - acc: 0.9904 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0480 - acc: 0.9904 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0477 - acc: 0.9905 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0477 - acc: 0.9906 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0480 - acc: 0.9905 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0481 - acc: 0.9905 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0479 - acc: 0.9907 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0480 - acc: 0.9906 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0481 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0483 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0484 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0485 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0484 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0483 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0485 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0485 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0484 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0486 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0487 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0487 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0489 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0486 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0488 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0488 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0010 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 6.7147e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 2.0485e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 2.1314e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 2.9817e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 1.2421e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.4866e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.0067e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 1.8399e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.1413e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 3.0713e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 6.4587e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1429e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 7.8543e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 9.9714e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 5.5770e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 3.0274e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 2.0663e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 1.2619e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 1.9718e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 5.3029e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.2807e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 6.5953e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1242e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 6.9658e-05 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 4.1764e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.8762e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 2.3797e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 3.4677e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1704e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.7992e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.5847e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.0047e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 7.3213e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.8888e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0063e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 4.4021e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.6567e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 7.0819e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 9.7699e-05 - acc: 1.0000
# Training time = 0:13:30.886059
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_67 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_68 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 4, 48)        705264      input_67[0][0]                   
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 4, 24)        5640        input_68[0][0]                   
__________________________________________________________________________________________________
flatten_67 (Flatten)            (None, 192)          0           embedding_67[0][0]               
__________________________________________________________________________________________________
flatten_68 (Flatten)            (None, 96)           0           embedding_68[0][0]               
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 288)          0           flatten_67[0][0]                 
                                                                 flatten_68[0][0]                 
__________________________________________________________________________________________________
dense_67 (Dense)                (None, 24)           6936        concatenate_34[0][0]             
__________________________________________________________________________________________________
dropout_34 (Dropout)            (None, 24)           0           dense_67[0][0]                   
__________________________________________________________________________________________________
dense_68 (Dense)                (None, 8)            200         dropout_34[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.1243 - acc: 0.9694 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0600 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0549 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0525 - acc: 0.9893 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0510 - acc: 0.9896 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1325e-06 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0493 - acc: 0.9898 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0488 - acc: 0.9901 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0483 - acc: 0.9901 - val_loss: 4.8876e-06 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0476 - acc: 0.9902 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0475 - acc: 0.9904 - val_loss: 1.0133e-06 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0474 - acc: 0.9904 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0474 - acc: 0.9904 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0473 - acc: 0.9905 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0473 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0471 - acc: 0.9905 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0472 - acc: 0.9905 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0472 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0473 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0472 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0472 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0472 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0474 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0479 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0475 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0477 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0475 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0481 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0482 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 8.9525e-04 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 4.1491e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 3.8953e-04 - acc: 0.9998
Epoch 4/40
 - 2s - loss: 3.6243e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 2.7159e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 1.6161e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.3960e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 1.4227e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.0032e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.0858e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.9753e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 9.4904e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 7.5517e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.0425e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 4.4851e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 7.3337e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 7.9721e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 8.8374e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.2501e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.2324e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1918e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 6.5850e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.2325e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 4.4895e-06 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 3.9624e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 5.8622e-06 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.4541e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.8354e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 7.6227e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.2117e-06 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 7.8357e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.3252e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1204e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.0221e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.8953e-06 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 3.8699e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 4.1251e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 5.9498e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.6038e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.5385e-06 - acc: 1.0000
# Training time = 0:13:49.940212
# F-Score(Ordinary) = 0.157, Recall: 0.231, Precision: 0.119
# F-Score(ireflv) = 0.136, Recall: 0.9, Precision: 0.074
# F-Score(id) = 0.339, Recall: 0.93, Precision: 0.207
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_69 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_70 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_69 (Embedding)        (None, 4, 48)        705264      input_69[0][0]                   
__________________________________________________________________________________________________
embedding_70 (Embedding)        (None, 4, 24)        5640        input_70[0][0]                   
__________________________________________________________________________________________________
flatten_69 (Flatten)            (None, 192)          0           embedding_69[0][0]               
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 96)           0           embedding_70[0][0]               
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 288)          0           flatten_69[0][0]                 
                                                                 flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_69 (Dense)                (None, 24)           6936        concatenate_35[0][0]             
__________________________________________________________________________________________________
dropout_35 (Dropout)            (None, 24)           0           dense_69[0][0]                   
__________________________________________________________________________________________________
dense_70 (Dense)                (None, 8)            200         dropout_35[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.1202 - acc: 0.9702 - val_loss: 3.6121e-05 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0606 - acc: 0.9873 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0552 - acc: 0.9886 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0525 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0509 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0491 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0487 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0482 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0477 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0476 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0473 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0471 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0471 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0470 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0468 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0468 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0468 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0467 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0469 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0468 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0469 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0471 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0472 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0474 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0477 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0475 - acc: 0.9907 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0474 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0474 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0478 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0480 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0478 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0481 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0015 - acc: 0.9995
Epoch 2/40
 - 2s - loss: 6.2781e-04 - acc: 0.9997
Epoch 3/40
 - 2s - loss: 5.0219e-04 - acc: 0.9998
Epoch 4/40
 - 2s - loss: 4.6619e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 3.1720e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.4013e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 2.7996e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 1.1165e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 5.2335e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.7295e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 4.4028e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 1.3526e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 3.1662e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 2.5836e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 1.8956e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 3.0760e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 1.2616e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.8272e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 4.7783e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 2.3639e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 2.9457e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 2.9306e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 1.4806e-06 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1732e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 6.4010e-04 - acc: 0.9998
Epoch 26/40
 - 2s - loss: 4.0547e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.7293e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.4654e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 3.4838e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 2.8543e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 2.8610e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 3.4358e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.7207e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.3955e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 1.7120e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1327e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 3.9717e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 2.2499e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 1.6995e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 3.3764e-04 - acc: 0.9999
# Training time = 0:13:21.816378
# F-Score(Ordinary) = 0.215, Recall: 0.687, Precision: 0.128
# F-Score(ireflv) = 0.546, Recall: 0.675, Precision: 0.459
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_71 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_72 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_71 (Embedding)        (None, 4, 48)        705264      input_71[0][0]                   
__________________________________________________________________________________________________
embedding_72 (Embedding)        (None, 4, 24)        5640        input_72[0][0]                   
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 192)          0           embedding_71[0][0]               
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 96)           0           embedding_72[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 288)          0           flatten_71[0][0]                 
                                                                 flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_71 (Dense)                (None, 24)           6936        concatenate_36[0][0]             
__________________________________________________________________________________________________
dropout_36 (Dropout)            (None, 24)           0           dense_71[0][0]                   
__________________________________________________________________________________________________
dense_72 (Dense)                (None, 8)            200         dropout_36[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0909 - acc: 0.9787 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0540 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0508 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0496 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0489 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0484 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0482 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0481 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0479 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0481 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0479 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0481 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0482 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0484 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0485 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0487 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0489 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0490 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0494 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0498 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0500 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0499 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0502 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0506 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0511 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0510 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0509 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0515 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0517 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0522 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0524 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0525 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0528 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0527 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0527 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0529 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0533 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0532 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0531 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0534 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0016 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 0.0016 - acc: 0.9997
Epoch 3/40
 - 2s - loss: 0.0023 - acc: 0.9996
Epoch 4/40
 - 2s - loss: 0.0017 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 0.0016 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 0.0020 - acc: 0.9996
Epoch 7/40
 - 2s - loss: 0.0017 - acc: 0.9997
Epoch 8/40
 - 2s - loss: 0.0019 - acc: 0.9997
Epoch 9/40
 - 2s - loss: 0.0018 - acc: 0.9997
Epoch 10/40
 - 2s - loss: 0.0017 - acc: 0.9997
Epoch 11/40
 - 2s - loss: 0.0014 - acc: 0.9997
Epoch 12/40
 - 2s - loss: 0.0016 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 0.0013 - acc: 0.9998
Epoch 14/40
 - 2s - loss: 0.0019 - acc: 0.9996
Epoch 15/40
 - 2s - loss: 0.0018 - acc: 0.9996
Epoch 16/40
 - 2s - loss: 0.0016 - acc: 0.9997
Epoch 17/40
 - 2s - loss: 0.0018 - acc: 0.9996
Epoch 18/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 0.0010 - acc: 0.9998
Epoch 20/40
 - 2s - loss: 0.0015 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 0.0013 - acc: 0.9997
Epoch 22/40
 - 2s - loss: 0.0017 - acc: 0.9996
Epoch 23/40
 - 2s - loss: 0.0015 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 25/40
 - 2s - loss: 0.0014 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 0.0014 - acc: 0.9997
Epoch 27/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 28/40
 - 2s - loss: 0.0016 - acc: 0.9996
Epoch 29/40
 - 2s - loss: 0.0016 - acc: 0.9996
Epoch 30/40
 - 2s - loss: 0.0017 - acc: 0.9996
Epoch 31/40
 - 2s - loss: 0.0015 - acc: 0.9996
Epoch 32/40
 - 2s - loss: 0.0016 - acc: 0.9996
Epoch 33/40
 - 2s - loss: 8.9698e-04 - acc: 0.9998
Epoch 34/40
 - 2s - loss: 0.0011 - acc: 0.9997
Epoch 35/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 36/40
 - 2s - loss: 0.0014 - acc: 0.9996
Epoch 37/40
 - 2s - loss: 8.9167e-04 - acc: 0.9998
Epoch 38/40
 - 2s - loss: 0.0011 - acc: 0.9997
Epoch 39/40
 - 2s - loss: 0.0014 - acc: 0.9996
Epoch 40/40
 - 2s - loss: 0.0011 - acc: 0.9997
# Training time = 0:13:29.699652
# F-Score(Ordinary) = 0.658, Recall: 0.645, Precision: 0.671
# F-Score(lvc) = 0.478, Recall: 0.426, Precision: 0.545
# F-Score(ireflv) = 0.773, Recall: 0.718, Precision: 0.836
# F-Score(id) = 0.669, Recall: 0.753, Precision: 0.601
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_73 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_74 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_73 (Embedding)        (None, 4, 48)        705264      input_73[0][0]                   
__________________________________________________________________________________________________
embedding_74 (Embedding)        (None, 4, 24)        5640        input_74[0][0]                   
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 192)          0           embedding_73[0][0]               
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 96)           0           embedding_74[0][0]               
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 288)          0           flatten_73[0][0]                 
                                                                 flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_73 (Dense)                (None, 24)           6936        concatenate_37[0][0]             
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 24)           0           dense_73[0][0]                   
__________________________________________________________________________________________________
dense_74 (Dense)                (None, 8)            200         dropout_37[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0925 - acc: 0.9784 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0543 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0512 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0495 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0486 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0483 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0476 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0474 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0473 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0472 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0472 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0472 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0473 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0471 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0474 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0477 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0477 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0482 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0479 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0480 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0484 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0483 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0482 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0484 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0488 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0489 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0487 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0490 - acc: 0.9907 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0493 - acc: 0.9906 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0495 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0495 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0495 - acc: 0.9906 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0497 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0500 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0503 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0501 - acc: 0.9904 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 8.4533e-04 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 5.8451e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 2.8463e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 2.8092e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 5.5842e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 6.4921e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 7.3928e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 6.4504e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 4.5962e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 3.6704e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 9.1941e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 5.4894e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 2.7405e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 6.3768e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 5.4517e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 4.5330e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 4.5234e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 9.0566e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 4.5119e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 9.0327e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 4.5001e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 1.7988e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 3.5907e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 4.4793e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 3.5772e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 6.2436e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 3.5604e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 3.5544e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 5.3191e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 4.4227e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 5.2942e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 7.9151e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 3.5090e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 3.5030e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 6.9862e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 3.4850e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 4.3474e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 3.4714e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 2.5999e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 7.7766e-04 - acc: 0.9999
# Training time = 0:13:28.966745
# F-Score(Ordinary) = 0.366, Recall: 0.635, Precision: 0.257
# F-Score(lvc) = 0.2, Recall: 0.571, Precision: 0.121
# F-Score(ireflv) = 0.574, Recall: 0.558, Precision: 0.59
# F-Score(id) = 0.203, Recall: 0.917, Precision: 0.114
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_75 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_76 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_75 (Embedding)        (None, 4, 48)        705264      input_75[0][0]                   
__________________________________________________________________________________________________
embedding_76 (Embedding)        (None, 4, 24)        5640        input_76[0][0]                   
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 192)          0           embedding_75[0][0]               
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 96)           0           embedding_76[0][0]               
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 288)          0           flatten_75[0][0]                 
                                                                 flatten_76[0][0]                 
__________________________________________________________________________________________________
dense_75 (Dense)                (None, 24)           6936        concatenate_38[0][0]             
__________________________________________________________________________________________________
dropout_38 (Dropout)            (None, 24)           0           dense_75[0][0]                   
__________________________________________________________________________________________________
dense_76 (Dense)                (None, 8)            200         dropout_38[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0926 - acc: 0.9781 - val_loss: 1.2040e-05 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0549 - acc: 0.9887 - val_loss: 1.5736e-05 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0517 - acc: 0.9895 - val_loss: 1.3471e-05 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0499 - acc: 0.9898 - val_loss: 9.1792e-06 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0495 - acc: 0.9900 - val_loss: 7.7486e-06 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0489 - acc: 0.9901 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0484 - acc: 0.9902 - val_loss: 3.9935e-06 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0483 - acc: 0.9902 - val_loss: 2.8014e-06 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0484 - acc: 0.9903 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0484 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0488 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0486 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0491 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0489 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0488 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0487 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0488 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0490 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0488 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0489 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0490 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0490 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0491 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0494 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0494 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0496 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0496 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0498 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0500 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0502 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0499 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0500 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0501 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0508 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0509 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0505 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0510 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0509 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0507 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0515 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0016 - acc: 0.9994
Epoch 2/40
 - 2s - loss: 7.6780e-04 - acc: 0.9997
Epoch 3/40
 - 2s - loss: 2.2252e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 3.0256e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 1.7752e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 3.1142e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 3.7669e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.2618e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1634e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.2114e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 4.6856e-06 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.4619e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 9.8190e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 6.5777e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 8.7100e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 5.6608e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 2.1837e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.0921e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 3.1541e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.0917e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 3.0852e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.1771e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.0888e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.0892e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 2.1777e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.3136e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.1738e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 2.1722e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 2.7573e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.0867e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.1696e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.3111e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.8588e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.8180e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.1678e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0842e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.6016e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.1654e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.9118e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.0830e-04 - acc: 1.0000
# Training time = 0:13:52.075048
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_77 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_78 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_77 (Embedding)        (None, 4, 48)        705264      input_77[0][0]                   
__________________________________________________________________________________________________
embedding_78 (Embedding)        (None, 4, 24)        5640        input_78[0][0]                   
__________________________________________________________________________________________________
flatten_77 (Flatten)            (None, 192)          0           embedding_77[0][0]               
__________________________________________________________________________________________________
flatten_78 (Flatten)            (None, 96)           0           embedding_78[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 288)          0           flatten_77[0][0]                 
                                                                 flatten_78[0][0]                 
__________________________________________________________________________________________________
dense_77 (Dense)                (None, 24)           6936        concatenate_39[0][0]             
__________________________________________________________________________________________________
dropout_39 (Dropout)            (None, 24)           0           dense_77[0][0]                   
__________________________________________________________________________________________________
dense_78 (Dense)                (None, 8)            200         dropout_39[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0941 - acc: 0.9778 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0549 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0512 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0498 - acc: 0.9897 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0489 - acc: 0.9900 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0484 - acc: 0.9901 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0482 - acc: 0.9901 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0481 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0479 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0478 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0475 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0477 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0476 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0478 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0480 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0482 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0481 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0484 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0483 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0488 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0488 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0484 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0491 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0493 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0496 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0497 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0496 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0493 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0496 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0495 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0498 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0501 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0501 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0504 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0510 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0508 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0508 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0509 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0517 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0518 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 5.1652e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 4.6244e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 5.3971e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 3.9493e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 5.6050e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 6.8435e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.4803e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 2.6386e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 1.5639e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 4.2116e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 9.6291e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 2.1292e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.9560e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.5519e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.0793e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 3.0987e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.0447e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.0579e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 2.8719e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.9357e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 8.4143e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 2.7336e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 9.6876e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.9003e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.7458e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.7052e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 9.2074e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 9.1119e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.8662e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.4359e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.8107e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 8.3609e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.7346e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 8.5292e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 8.1047e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.6319e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 5.6520e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 1.1891e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.5030e-04 - acc: 1.0000
# Training time = 0:13:27.882959
# F-Score(Ordinary) = 0.148, Recall: 0.9, Precision: 0.081
# F-Score(lvc) = 0.03, Recall: 1.0, Precision: 0.015
# F-Score(ireflv) = 0.094, Recall: 1.0, Precision: 0.049
# F-Score(id) = 0.241, Recall: 0.871, Precision: 0.14
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_79 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_80 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_79 (Embedding)        (None, 4, 48)        705264      input_79[0][0]                   
__________________________________________________________________________________________________
embedding_80 (Embedding)        (None, 4, 24)        5640        input_80[0][0]                   
__________________________________________________________________________________________________
flatten_79 (Flatten)            (None, 192)          0           embedding_79[0][0]               
__________________________________________________________________________________________________
flatten_80 (Flatten)            (None, 96)           0           embedding_80[0][0]               
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 288)          0           flatten_79[0][0]                 
                                                                 flatten_80[0][0]                 
__________________________________________________________________________________________________
dense_79 (Dense)                (None, 24)           6936        concatenate_40[0][0]             
__________________________________________________________________________________________________
dropout_40 (Dropout)            (None, 24)           0           dense_79[0][0]                   
__________________________________________________________________________________________________
dense_80 (Dense)                (None, 8)            200         dropout_40[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0934 - acc: 0.9780 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0552 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0515 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0499 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0489 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0484 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0477 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0476 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0475 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0477 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0477 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0479 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0477 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0479 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0482 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0482 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0481 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0484 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0480 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0485 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0487 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0487 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0487 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0490 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0490 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0490 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0491 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0492 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0495 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0494 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0499 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0502 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0504 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0502 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0508 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0511 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0510 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0513 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0011 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 4.5602e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 1.7546e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 4.3304e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 3.4552e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 6.3710e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.5866e-04 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.5827e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 4.2938e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 1.7165e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.7144e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.7131e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.7114e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 3.4155e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 2.5583e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 5.1040e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 1.6998e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 4.2393e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 4.2301e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 2.0905e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.5343e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.6882e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.6866e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 3.3671e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 1.8711e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.5218e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 3.3564e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 5.8583e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 1.6717e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 5.8349e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 8.3351e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 7.4713e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 6.6162e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 2.4760e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.7621e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 6.5629e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 4.0904e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 2.4503e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 4.0755e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 4.0662e-04 - acc: 0.9999
# Training time = 0:13:25.382270
# F-Score(Ordinary) = 0.526, Recall: 0.605, Precision: 0.465
# F-Score(lvc) = 0.482, Recall: 0.496, Precision: 0.47
# F-Score(ireflv) = 0.716, Recall: 0.586, Precision: 0.918
# F-Score(id) = 0.244, Recall: 0.964, Precision: 0.14
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_81 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_82 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_81 (Embedding)        (None, 4, 48)        705264      input_81[0][0]                   
__________________________________________________________________________________________________
embedding_82 (Embedding)        (None, 4, 24)        5640        input_82[0][0]                   
__________________________________________________________________________________________________
flatten_81 (Flatten)            (None, 192)          0           embedding_81[0][0]               
__________________________________________________________________________________________________
flatten_82 (Flatten)            (None, 96)           0           embedding_82[0][0]               
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 288)          0           flatten_81[0][0]                 
                                                                 flatten_82[0][0]                 
__________________________________________________________________________________________________
dense_81 (Dense)                (None, 24)           6936        concatenate_41[0][0]             
__________________________________________________________________________________________________
dropout_41 (Dropout)            (None, 24)           0           dense_81[0][0]                   
__________________________________________________________________________________________________
dense_82 (Dense)                (None, 8)            200         dropout_41[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0756 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0515 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0500 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0495 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0494 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0495 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0494 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0497 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0497 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0501 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0504 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0510 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0510 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0511 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0519 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0529 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0534 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0530 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0536 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0539 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0541 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0537 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0544 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0543 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0540 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0550 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0550 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0556 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0556 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0563 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0571 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0578 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0577 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0580 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0582 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0584 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0590 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0596 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0602 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0608 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0024 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 5.5294e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 8.9858e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 2.5614e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 5.1092e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 8.9095e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 5.0756e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 3.7991e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 6.3155e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 5.0391e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 3.7719e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 5.0182e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 3.7561e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 8.7359e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 8.6992e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 4.9553e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 3.7089e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 3.7022e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 4.9254e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 8.5895e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.2253e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 4.8895e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 3.6596e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 8.5112e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 2.4260e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 8.4641e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 3.6170e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 7.2126e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 7.1893e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 8.3542e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 5.9454e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 5.9267e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 3.5476e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.3387e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.8969e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 8.2240e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 4.6836e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 3.5052e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 9.3123e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 8.1094e-04 - acc: 0.9999
# Training time = 0:13:32.781826
# F-Score(Ordinary) = 0.334, Recall: 0.225, Precision: 0.649
# F-Score(lvc) = 0.434, Recall: 0.421, Precision: 0.447
# F-Score(ireflv) = 0.768, Recall: 0.716, Precision: 0.828
# F-Score(id) = 0.198, Recall: 0.118, Precision: 0.617
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_83 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_84 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_83 (Embedding)        (None, 4, 48)        705264      input_83[0][0]                   
__________________________________________________________________________________________________
embedding_84 (Embedding)        (None, 4, 24)        5640        input_84[0][0]                   
__________________________________________________________________________________________________
flatten_83 (Flatten)            (None, 192)          0           embedding_83[0][0]               
__________________________________________________________________________________________________
flatten_84 (Flatten)            (None, 96)           0           embedding_84[0][0]               
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 288)          0           flatten_83[0][0]                 
                                                                 flatten_84[0][0]                 
__________________________________________________________________________________________________
dense_83 (Dense)                (None, 24)           6936        concatenate_42[0][0]             
__________________________________________________________________________________________________
dropout_42 (Dropout)            (None, 24)           0           dense_83[0][0]                   
__________________________________________________________________________________________________
dense_84 (Dense)                (None, 8)            200         dropout_42[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0767 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0518 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0500 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0488 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0491 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0490 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0492 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0495 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0498 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0496 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0498 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0499 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0501 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0503 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0510 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0515 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0511 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0518 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0525 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0524 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0526 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0538 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0537 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0541 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0553 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0556 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0559 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0569 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0574 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0577 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0579 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0577 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0586 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0587 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0597 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0604 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0602 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0612 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0612 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0128 - acc: 0.9984
Epoch 2/40
 - 2s - loss: 0.0128 - acc: 0.9983
Epoch 3/40
 - 2s - loss: 0.0103 - acc: 0.9985
Epoch 4/40
 - 2s - loss: 0.0093 - acc: 0.9985
Epoch 5/40
 - 2s - loss: 0.0084 - acc: 0.9985
Epoch 6/40
 - 2s - loss: 0.0081 - acc: 0.9984
Epoch 7/40
 - 2s - loss: 0.0063 - acc: 0.9986
Epoch 8/40
 - 2s - loss: 0.0067 - acc: 0.9983
Epoch 9/40
 - 2s - loss: 0.0055 - acc: 0.9984
Epoch 10/40
 - 2s - loss: 0.0045 - acc: 0.9984
Epoch 11/40
 - 2s - loss: 0.0032 - acc: 0.9986
Epoch 12/40
 - 2s - loss: 0.0030 - acc: 0.9983
Epoch 13/40
 - 2s - loss: 0.0018 - acc: 0.9987
Epoch 14/40
 - 2s - loss: 0.0015 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 0.0010 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 7.5048e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 4.7818e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 3.7264e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 2.4590e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.5883e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.2739e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.0776e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 8.4133e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 6.4362e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 6.8474e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 5.6280e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 4.7744e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 4.2572e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 3.8662e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.1680e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.9522e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 3.1118e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.7389e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.3614e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.8254e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.7026e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.0139e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.7278e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.7803e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.0193e-05 - acc: 1.0000
# Training time = 0:13:30.878952
# F-Score(Ordinary) = 0.604, Recall: 0.554, Precision: 0.664
# F-Score(lvc) = 0.56, Recall: 0.65, Precision: 0.492
# F-Score(ireflv) = 0.737, Recall: 0.632, Precision: 0.885
# F-Score(id) = 0.532, Recall: 0.462, Precision: 0.627
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_85 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_86 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_85 (Embedding)        (None, 4, 48)        705264      input_85[0][0]                   
__________________________________________________________________________________________________
embedding_86 (Embedding)        (None, 4, 24)        5640        input_86[0][0]                   
__________________________________________________________________________________________________
flatten_85 (Flatten)            (None, 192)          0           embedding_85[0][0]               
__________________________________________________________________________________________________
flatten_86 (Flatten)            (None, 96)           0           embedding_86[0][0]               
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 288)          0           flatten_85[0][0]                 
                                                                 flatten_86[0][0]                 
__________________________________________________________________________________________________
dense_85 (Dense)                (None, 24)           6936        concatenate_43[0][0]             
__________________________________________________________________________________________________
dropout_43 (Dropout)            (None, 24)           0           dense_85[0][0]                   
__________________________________________________________________________________________________
dense_86 (Dense)                (None, 8)            200         dropout_43[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0771 - acc: 0.9825 - val_loss: 1.0550e-05 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0521 - acc: 0.9893 - val_loss: 3.3975e-06 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0504 - acc: 0.9897 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0494 - acc: 0.9899 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0494 - acc: 0.9899 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0491 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0493 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0497 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0494 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0497 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0500 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0494 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0500 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0498 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0502 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0504 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0508 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0509 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0512 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0510 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0518 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0520 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0519 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0521 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0518 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0528 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0524 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0530 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0538 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0534 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0536 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0540 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0541 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0542 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0543 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0545 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0552 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0553 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0560 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0552 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0022 - acc: 0.9996
Epoch 2/40
 - 2s - loss: 0.0012 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 5.2158e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 3.7051e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 3.8607e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 9.8997e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.0673e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 8.8383e-06 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.5309e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 3.0511e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.0639e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.6764e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.4975e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.4821e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.6240e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.5243e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 3.0472e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.5231e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.3655e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.5224e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.4407e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 3.0409e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.5199e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.5193e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.3294e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.3540e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 3.0348e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.0318e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.4455e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.5155e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 3.0272e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.4144e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.3038e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.3008e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 3.0241e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.5115e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.2646e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.0196e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.3137e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.5093e-04 - acc: 1.0000
# Training time = 0:13:53.714102
# F-Score(Ordinary) = 0.085, Recall: 0.87, Precision: 0.045
# F-Score(id) = 0.187, Recall: 0.952, Precision: 0.104
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_87 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_88 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_87 (Embedding)        (None, 4, 48)        705264      input_87[0][0]                   
__________________________________________________________________________________________________
embedding_88 (Embedding)        (None, 4, 24)        5640        input_88[0][0]                   
__________________________________________________________________________________________________
flatten_87 (Flatten)            (None, 192)          0           embedding_87[0][0]               
__________________________________________________________________________________________________
flatten_88 (Flatten)            (None, 96)           0           embedding_88[0][0]               
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 288)          0           flatten_87[0][0]                 
                                                                 flatten_88[0][0]                 
__________________________________________________________________________________________________
dense_87 (Dense)                (None, 24)           6936        concatenate_44[0][0]             
__________________________________________________________________________________________________
dropout_44 (Dropout)            (None, 24)           0           dense_87[0][0]                   
__________________________________________________________________________________________________
dense_88 (Dense)                (None, 8)            200         dropout_44[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0771 - acc: 0.9823 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0518 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0496 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0493 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0495 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0496 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0497 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0501 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0502 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0499 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0499 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0501 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0502 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0509 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0514 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0515 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0514 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0518 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0515 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0522 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0525 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0534 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0535 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0545 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0538 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0545 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0543 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0543 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0541 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0546 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0551 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0555 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0553 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0554 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0564 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0560 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0561 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0571 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0568 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0571 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 7.3170e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 5.5793e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 7.3470e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 0.0010 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 4.3834e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 8.7439e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 0.0010 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 5.7936e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 7.2247e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 8.6443e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 1.4333e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 0.0010 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 5.6760e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 9.8610e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 9.8242e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 6.9952e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 5.5829e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 6.9314e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 4.1324e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 8.2433e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 6.8490e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 5.4660e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 6.8154e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 8.1535e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 6.7742e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.2744e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 4.0561e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 5.3972e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 4.0404e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 6.7181e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 5.3613e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 8.0189e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 5.3074e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 5.2954e-04 - acc: 0.9999
# Training time = 0:13:30.476873
# F-Score(Ordinary) = 0.621, Recall: 0.664, Precision: 0.584
# F-Score(lvc) = 0.513, Recall: 0.761, Precision: 0.386
# F-Score(ireflv) = 0.789, Recall: 0.767, Precision: 0.811
# F-Score(id) = 0.528, Recall: 0.523, Precision: 0.534
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_89 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_90 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_89 (Embedding)        (None, 4, 48)        705264      input_89[0][0]                   
__________________________________________________________________________________________________
embedding_90 (Embedding)        (None, 4, 24)        5640        input_90[0][0]                   
__________________________________________________________________________________________________
flatten_89 (Flatten)            (None, 192)          0           embedding_89[0][0]               
__________________________________________________________________________________________________
flatten_90 (Flatten)            (None, 96)           0           embedding_90[0][0]               
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 288)          0           flatten_89[0][0]                 
                                                                 flatten_90[0][0]                 
__________________________________________________________________________________________________
dense_89 (Dense)                (None, 24)           6936        concatenate_45[0][0]             
__________________________________________________________________________________________________
dropout_45 (Dropout)            (None, 24)           0           dense_89[0][0]                   
__________________________________________________________________________________________________
dense_90 (Dense)                (None, 8)            200         dropout_45[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0778 - acc: 0.9824 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0523 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0500 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0491 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0492 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0494 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0497 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0499 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0501 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0504 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0508 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0508 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0514 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0519 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0521 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0527 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0529 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0534 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0543 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0536 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0539 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0542 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0549 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0559 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0560 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0563 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0566 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0577 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0571 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0581 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0584 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0590 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0580 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0584 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0581 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0588 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0589 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0604 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0600 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0016 - acc: 0.9996
Epoch 2/40
 - 2s - loss: 6.2409e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 4.9409e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 6.1580e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 3.6860e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 3.6795e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 3.6723e-04 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 7.3220e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 3.6517e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 4.8576e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 2.4249e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 3.6311e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 4.8303e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 7.2223e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 4.8004e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 7.1774e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 8.3391e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 5.9345e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.3695e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1843e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 7.0827e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 4.7073e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 4.6953e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 1.1731e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 5.8482e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 9.3174e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 3.4825e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 5.7884e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 6.9211e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 5.7473e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 6.8718e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 4.5667e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 9.0962e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 0.0012 - acc: 0.9998
Epoch 35/40
 - 2s - loss: 5.6202e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 6.7194e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 4.4651e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 2.2287e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 4.4472e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 4.4352e-04 - acc: 0.9999
# Training time = 0:13:31.466800
# F-Score(Ordinary) = 0.122, Recall: 0.667, Precision: 0.067
# F-Score(lvc) = 0.11, Recall: 0.571, Precision: 0.061
# F-Score(ireflv) = 0.016, Recall: 1.0, Precision: 0.008
# F-Score(id) = 0.194, Recall: 0.875, Precision: 0.109
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_91 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_92 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_91 (Embedding)        (None, 4, 48)        705264      input_91[0][0]                   
__________________________________________________________________________________________________
embedding_92 (Embedding)        (None, 4, 24)        5640        input_92[0][0]                   
__________________________________________________________________________________________________
flatten_91 (Flatten)            (None, 192)          0           embedding_91[0][0]               
__________________________________________________________________________________________________
flatten_92 (Flatten)            (None, 96)           0           embedding_92[0][0]               
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 288)          0           flatten_91[0][0]                 
                                                                 flatten_92[0][0]                 
__________________________________________________________________________________________________
dense_91 (Dense)                (None, 24)           6936        concatenate_46[0][0]             
__________________________________________________________________________________________________
dropout_46 (Dropout)            (None, 24)           0           dense_91[0][0]                   
__________________________________________________________________________________________________
dense_92 (Dense)                (None, 8)            200         dropout_46[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0671 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0525 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0530 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0538 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0541 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0547 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0550 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0544 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0550 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0553 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0561 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0560 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0564 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0561 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0565 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0571 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0573 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0577 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0577 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0580 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0580 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0580 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0598 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0594 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0604 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0609 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0615 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0624 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0623 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0635 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0629 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0638 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0639 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0647 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0650 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0658 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0660 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0660 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0664 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0668 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0037 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 0.0030 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 0.0044 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 0.0062 - acc: 0.9996
Epoch 5/40
 - 2s - loss: 0.0044 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 0.0053 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 0.0061 - acc: 0.9996
Epoch 8/40
 - 2s - loss: 0.0051 - acc: 0.9997
Epoch 9/40
 - 2s - loss: 0.0044 - acc: 0.9997
Epoch 10/40
 - 2s - loss: 0.0049 - acc: 0.9997
Epoch 11/40
 - 2s - loss: 0.0038 - acc: 0.9998
Epoch 12/40
 - 2s - loss: 0.0045 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 0.0052 - acc: 0.9997
Epoch 14/40
 - 2s - loss: 0.0050 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 0.0052 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 0.0054 - acc: 0.9996
Epoch 17/40
 - 2s - loss: 0.0055 - acc: 0.9996
Epoch 18/40
 - 2s - loss: 0.0047 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 0.0041 - acc: 0.9997
Epoch 20/40
 - 2s - loss: 0.0038 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 0.0059 - acc: 0.9996
Epoch 22/40
 - 2s - loss: 0.0049 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 0.0050 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 0.0045 - acc: 0.9997
Epoch 25/40
 - 2s - loss: 0.0028 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 1.4030e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 5.5687e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 6.8443e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 6.7978e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 4.0569e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 5.3828e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 4.0179e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.6697e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.3159e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 5.2269e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 5.1972e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 3.8787e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 8.9839e-04 - acc: 0.9999
# Training time = 0:13:48.401976
# F-Score(Ordinary) = 0.375, Recall: 0.268, Precision: 0.624
# F-Score(lvc) = 0.541, Recall: 0.747, Precision: 0.424
# F-Score(ireflv) = 0.655, Recall: 0.574, Precision: 0.762
# F-Score(id) = 0.221, Recall: 0.137, Precision: 0.57
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_93 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_94 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_93 (Embedding)        (None, 4, 48)        705264      input_93[0][0]                   
__________________________________________________________________________________________________
embedding_94 (Embedding)        (None, 4, 24)        5640        input_94[0][0]                   
__________________________________________________________________________________________________
flatten_93 (Flatten)            (None, 192)          0           embedding_93[0][0]               
__________________________________________________________________________________________________
flatten_94 (Flatten)            (None, 96)           0           embedding_94[0][0]               
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 288)          0           flatten_93[0][0]                 
                                                                 flatten_94[0][0]                 
__________________________________________________________________________________________________
dense_93 (Dense)                (None, 24)           6936        concatenate_47[0][0]             
__________________________________________________________________________________________________
dropout_47 (Dropout)            (None, 24)           0           dense_93[0][0]                   
__________________________________________________________________________________________________
dense_94 (Dense)                (None, 8)            200         dropout_47[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0688 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0525 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0516 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0522 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0522 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0525 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0524 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0521 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0531 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0530 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0533 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0534 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0547 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0553 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0557 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0556 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0572 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0571 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0568 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0581 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0576 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0581 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0588 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0590 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0602 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0621 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0613 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0627 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0624 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0621 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0639 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0625 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0640 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0648 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0656 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0660 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0672 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0684 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0678 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0671 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0030 - acc: 0.9996
Epoch 2/40
 - 2s - loss: 0.0023 - acc: 0.9997
Epoch 3/40
 - 2s - loss: 0.0030 - acc: 0.9996
Epoch 4/40
 - 2s - loss: 0.0025 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 0.0019 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 0.0022 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 0.0014 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 0.0020 - acc: 0.9997
Epoch 9/40
 - 2s - loss: 0.0015 - acc: 0.9998
Epoch 10/40
 - 2s - loss: 0.0019 - acc: 0.9997
Epoch 11/40
 - 2s - loss: 0.0024 - acc: 0.9995
Epoch 12/40
 - 2s - loss: 0.0019 - acc: 0.9996
Epoch 13/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 14/40
 - 2s - loss: 0.0011 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 0.0013 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 0.0012 - acc: 0.9997
Epoch 17/40
 - 2s - loss: 0.0012 - acc: 0.9996
Epoch 18/40
 - 2s - loss: 3.7306e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 7.0347e-04 - acc: 0.9998
Epoch 20/40
 - 2s - loss: 8.1930e-04 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 6.1123e-04 - acc: 0.9997
Epoch 22/40
 - 2s - loss: 3.5547e-04 - acc: 0.9998
Epoch 23/40
 - 2s - loss: 6.9144e-04 - acc: 0.9996
Epoch 24/40
 - 2s - loss: 4.3988e-04 - acc: 0.9997
Epoch 25/40
 - 2s - loss: 6.4177e-04 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 4.0611e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.8322e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 2.3478e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.8501e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.7288e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.0146e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.0961e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.5076e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 8.3200e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 4.5207e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 4.7585e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 4.8375e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.0091e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.3009e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.7239e-05 - acc: 1.0000
# Training time = 0:13:50.849237
# F-Score(Ordinary) = 0.323, Recall: 0.21, Precision: 0.705
# F-Score(lvc) = 0.402, Recall: 0.327, Precision: 0.523
# F-Score(ireflv) = 0.371, Recall: 0.238, Precision: 0.844
# F-Score(id) = 0.631, Recall: 0.685, Precision: 0.585
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_95 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_96 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_95 (Embedding)        (None, 4, 48)        705264      input_95[0][0]                   
__________________________________________________________________________________________________
embedding_96 (Embedding)        (None, 4, 24)        5640        input_96[0][0]                   
__________________________________________________________________________________________________
flatten_95 (Flatten)            (None, 192)          0           embedding_95[0][0]               
__________________________________________________________________________________________________
flatten_96 (Flatten)            (None, 96)           0           embedding_96[0][0]               
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 288)          0           flatten_95[0][0]                 
                                                                 flatten_96[0][0]                 
__________________________________________________________________________________________________
dense_95 (Dense)                (None, 24)           6936        concatenate_48[0][0]             
__________________________________________________________________________________________________
dropout_48 (Dropout)            (None, 24)           0           dense_95[0][0]                   
__________________________________________________________________________________________________
dense_96 (Dense)                (None, 8)            200         dropout_48[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0670 - acc: 0.9852 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0520 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0516 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0524 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0532 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0538 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0543 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0541 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0547 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0549 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0554 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0555 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0565 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0561 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0575 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0566 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0578 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0587 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0588 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0602 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0606 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0613 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0606 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0632 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0649 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0659 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0659 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0685 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0687 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0696 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0716 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0737 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0771 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0825 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0841 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0858 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0881 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0920 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0937 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0978 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0072 - acc: 0.9993
Epoch 2/40
 - 2s - loss: 0.0041 - acc: 0.9997
Epoch 3/40
 - 2s - loss: 0.0058 - acc: 0.9996
Epoch 4/40
 - 2s - loss: 0.0054 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 0.0055 - acc: 0.9997
Epoch 6/40
 - 2s - loss: 0.0055 - acc: 0.9996
Epoch 7/40
 - 2s - loss: 0.0073 - acc: 0.9995
Epoch 8/40
 - 2s - loss: 0.0059 - acc: 0.9996
Epoch 9/40
 - 2s - loss: 0.0029 - acc: 0.9998
Epoch 10/40
 - 2s - loss: 0.0058 - acc: 0.9996
Epoch 11/40
 - 2s - loss: 0.0053 - acc: 0.9997
Epoch 12/40
 - 2s - loss: 0.0049 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 0.0064 - acc: 0.9996
Epoch 14/40
 - 2s - loss: 0.0023 - acc: 0.9998
Epoch 15/40
 - 2s - loss: 0.0053 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 0.0050 - acc: 0.9997
Epoch 17/40
 - 2s - loss: 0.0046 - acc: 0.9997
Epoch 18/40
 - 2s - loss: 0.0059 - acc: 0.9996
Epoch 19/40
 - 2s - loss: 0.0040 - acc: 0.9998
Epoch 20/40
 - 2s - loss: 0.0051 - acc: 0.9997
Epoch 21/40
 - 2s - loss: 0.0040 - acc: 0.9998
Epoch 22/40
 - 2s - loss: 0.0055 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 0.0038 - acc: 0.9998
Epoch 24/40
 - 2s - loss: 0.0063 - acc: 0.9996
Epoch 25/40
 - 2s - loss: 0.0044 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 0.0047 - acc: 0.9997
Epoch 27/40
 - 2s - loss: 0.0030 - acc: 0.9998
Epoch 28/40
 - 2s - loss: 0.0036 - acc: 0.9998
Epoch 29/40
 - 2s - loss: 0.0055 - acc: 0.9996
Epoch 30/40
 - 2s - loss: 0.0037 - acc: 0.9998
Epoch 31/40
 - 2s - loss: 0.0065 - acc: 0.9996
Epoch 32/40
 - 2s - loss: 0.0045 - acc: 0.9997
Epoch 33/40
 - 2s - loss: 0.0046 - acc: 0.9997
Epoch 34/40
 - 2s - loss: 0.0062 - acc: 0.9996
Epoch 35/40
 - 2s - loss: 0.0040 - acc: 0.9998
Epoch 36/40
 - 2s - loss: 0.0032 - acc: 0.9998
Epoch 37/40
 - 2s - loss: 0.0050 - acc: 0.9997
Epoch 38/40
 - 2s - loss: 0.0050 - acc: 0.9997
Epoch 39/40
 - 2s - loss: 0.0043 - acc: 0.9997
Epoch 40/40
 - 2s - loss: 0.0044 - acc: 0.9997
# Training time = 0:13:36.146064
# F-Score(Ordinary) = 0.398, Recall: 0.291, Precision: 0.631
# F-Score(lvc) = 0.537, Recall: 0.552, Precision: 0.523
# F-Score(ireflv) = 0.26, Recall: 0.154, Precision: 0.844
# F-Score(id) = 0.594, Recall: 0.646, Precision: 0.549
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_97 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_98 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_97 (Embedding)        (None, 4, 48)        705264      input_97[0][0]                   
__________________________________________________________________________________________________
embedding_98 (Embedding)        (None, 4, 24)        5640        input_98[0][0]                   
__________________________________________________________________________________________________
flatten_97 (Flatten)            (None, 192)          0           embedding_97[0][0]               
__________________________________________________________________________________________________
flatten_98 (Flatten)            (None, 96)           0           embedding_98[0][0]               
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 288)          0           flatten_97[0][0]                 
                                                                 flatten_98[0][0]                 
__________________________________________________________________________________________________
dense_97 (Dense)                (None, 24)           6936        concatenate_49[0][0]             
__________________________________________________________________________________________________
dropout_49 (Dropout)            (None, 24)           0           dense_97[0][0]                   
__________________________________________________________________________________________________
dense_98 (Dense)                (None, 8)            200         dropout_49[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0669 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0518 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0513 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0516 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0526 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0532 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0531 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0545 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0543 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0556 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0558 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0568 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0570 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0584 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0589 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0591 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0595 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0604 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0602 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0616 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0622 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0622 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0627 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0630 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0637 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0639 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0658 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0658 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0654 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0668 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0673 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0693 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0689 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0689 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0706 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0717 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0746 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0755 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0778 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0809 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0334 - acc: 0.9978
Epoch 2/40
 - 2s - loss: 0.0231 - acc: 0.9985
Epoch 3/40
 - 2s - loss: 0.0281 - acc: 0.9982
Epoch 4/40
 - 2s - loss: 0.0291 - acc: 0.9981
Epoch 5/40
 - 2s - loss: 0.0203 - acc: 0.9987
Epoch 6/40
 - 2s - loss: 0.0246 - acc: 0.9984
Epoch 7/40
 - 2s - loss: 0.0226 - acc: 0.9986
Epoch 8/40
 - 2s - loss: 0.0224 - acc: 0.9986
Epoch 9/40
 - 2s - loss: 0.0237 - acc: 0.9985
Epoch 10/40
 - 2s - loss: 0.0270 - acc: 0.9983
Epoch 11/40
 - 2s - loss: 0.0249 - acc: 0.9984
Epoch 12/40
 - 2s - loss: 0.0263 - acc: 0.9983
Epoch 13/40
 - 2s - loss: 0.0267 - acc: 0.9983
Epoch 14/40
 - 2s - loss: 0.0242 - acc: 0.9985
Epoch 15/40
 - 2s - loss: 0.0266 - acc: 0.9983
Epoch 16/40
 - 2s - loss: 0.0256 - acc: 0.9984
Epoch 17/40
 - 2s - loss: 0.0230 - acc: 0.9985
Epoch 18/40
 - 2s - loss: 0.0255 - acc: 0.9984
Epoch 19/40
 - 2s - loss: 0.0250 - acc: 0.9984
Epoch 20/40
 - 2s - loss: 0.0259 - acc: 0.9984
Epoch 21/40
 - 2s - loss: 0.0244 - acc: 0.9984
Epoch 22/40
 - 2s - loss: 0.0240 - acc: 0.9984
Epoch 23/40
 - 2s - loss: 0.0062 - acc: 0.9994
Epoch 24/40
 - 2s - loss: 2.8571e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 3.7829e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 4.6866e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 4.6403e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 4.5942e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 4.5478e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 4.5016e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.7885e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 3.5536e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 8.8468e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 4.3909e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 9.4960e-04 - acc: 0.9998
Epoch 36/40
 - 2s - loss: 2.5519e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 8.4777e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 6.7074e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 4.9535e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 3.2660e-04 - acc: 0.9999
# Training time = 0:13:54.623260
# F-Score(Ordinary) = 0.13, Recall: 0.073, Precision: 0.595
# F-Score(lvc) = 0.474, Recall: 0.633, Precision: 0.379
# F-Score(ireflv) = 0.441, Recall: 0.307, Precision: 0.779
# F-Score(id) = 0.061, Recall: 0.032, Precision: 0.544
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_99 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_100 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_99 (Embedding)        (None, 4, 48)        705264      input_99[0][0]                   
__________________________________________________________________________________________________
embedding_100 (Embedding)       (None, 4, 24)        5640        input_100[0][0]                  
__________________________________________________________________________________________________
flatten_99 (Flatten)            (None, 192)          0           embedding_99[0][0]               
__________________________________________________________________________________________________
flatten_100 (Flatten)           (None, 96)           0           embedding_100[0][0]              
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 288)          0           flatten_99[0][0]                 
                                                                 flatten_100[0][0]                
__________________________________________________________________________________________________
dense_99 (Dense)                (None, 24)           6936        concatenate_50[0][0]             
__________________________________________________________________________________________________
dropout_50 (Dropout)            (None, 24)           0           dense_99[0][0]                   
__________________________________________________________________________________________________
dense_100 (Dense)               (None, 8)            200         dropout_50[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 18s - loss: 0.0674 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0521 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0525 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0529 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0539 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0546 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0551 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0560 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0568 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0572 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0578 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0583 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0591 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0588 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0589 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0592 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0591 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0599 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0599 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0614 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0611 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0617 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0624 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0629 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0634 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0641 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0651 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0647 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0664 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0667 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0670 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0696 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0693 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0715 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0723 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0731 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0736 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0751 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0751 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0794 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0020 - acc: 0.9998
Epoch 2/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 7.7758e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 9.2692e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 6.1427e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 6.1129e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 6.0831e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 7.5618e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 5.9640e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 5.9343e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 7.3758e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 4.4037e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 7.3015e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 8.7003e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 0.0013 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 4.2755e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.8415e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 4.2477e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 7.0417e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 4.2033e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.7933e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 5.5632e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 1.3871e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 5.5261e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 5.4372e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 5.4076e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 6.7176e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 3.9645e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 6.5696e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 6.5233e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 2.5971e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 3.8812e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 8.9900e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 7.6339e-04 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 5.0526e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 6.2740e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 8.7056e-04 - acc: 0.9999
# Training time = 0:13:23.520042
# F-Score(Ordinary) = 0.226, Recall: 0.139, Precision: 0.613
# F-Score(lvc) = 0.088, Recall: 0.049, Precision: 0.462
# F-Score(ireflv) = 0.416, Recall: 0.286, Precision: 0.762
# F-Score(id) = 0.352, Recall: 0.264, Precision: 0.528
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_101 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_102 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_101 (Embedding)       (None, 4, 48)        705264      input_101[0][0]                  
__________________________________________________________________________________________________
embedding_102 (Embedding)       (None, 4, 24)        5640        input_102[0][0]                  
__________________________________________________________________________________________________
flatten_101 (Flatten)           (None, 192)          0           embedding_101[0][0]              
__________________________________________________________________________________________________
flatten_102 (Flatten)           (None, 96)           0           embedding_102[0][0]              
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 288)          0           flatten_101[0][0]                
                                                                 flatten_102[0][0]                
__________________________________________________________________________________________________
dense_101 (Dense)               (None, 24)           6936        concatenate_51[0][0]             
__________________________________________________________________________________________________
dropout_51 (Dropout)            (None, 24)           0           dense_101[0][0]                  
__________________________________________________________________________________________________
dense_102 (Dense)               (None, 8)            200         dropout_51[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.1377 - acc: 0.9665 - val_loss: 5.7221e-06 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0567 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0506 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0477 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0460 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0450 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0441 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0434 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0428 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0424 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0422 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0420 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0408 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0407 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 4.0687e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.9724e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 3.0268e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 3.4755e-04 - acc: 0.9998
Epoch 5/40
 - 2s - loss: 2.4960e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 4.1549e-04 - acc: 0.9998
Epoch 7/40
 - 2s - loss: 3.7062e-04 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 2.0716e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 2.2925e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.2931e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.1348e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 1.8736e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.4182e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 5.7367e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.3793e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 1.2694e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.0799e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 1.7285e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 1.0619e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 9.2177e-05 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 5.7928e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.3981e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 1.3321e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 9.6880e-05 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 8.0217e-05 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 1.1251e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 5.6741e-05 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.3027e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 5.5881e-05 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 1.2435e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 6.6197e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.6476e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 5.2624e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 8.3816e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 7.1531e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.9913e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 7.7370e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 7.1435e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 3.4748e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 3.3700e-05 - acc: 1.0000
# Training time = 0:12:34.715443
# F-Score(Ordinary) = 0.46, Recall: 0.902, Precision: 0.309
# F-Score(lvc) = 0.3, Recall: 0.857, Precision: 0.182
# F-Score(ireflv) = 0.771, Recall: 0.92, Precision: 0.664
# F-Score(id) = 0.289, Recall: 0.943, Precision: 0.171
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_103 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_104 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_103 (Embedding)       (None, 4, 48)        705264      input_103[0][0]                  
__________________________________________________________________________________________________
embedding_104 (Embedding)       (None, 4, 24)        5640        input_104[0][0]                  
__________________________________________________________________________________________________
flatten_103 (Flatten)           (None, 192)          0           embedding_103[0][0]              
__________________________________________________________________________________________________
flatten_104 (Flatten)           (None, 96)           0           embedding_104[0][0]              
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 288)          0           flatten_103[0][0]                
                                                                 flatten_104[0][0]                
__________________________________________________________________________________________________
dense_103 (Dense)               (None, 24)           6936        concatenate_52[0][0]             
__________________________________________________________________________________________________
dropout_52 (Dropout)            (None, 24)           0           dense_103[0][0]                  
__________________________________________________________________________________________________
dense_104 (Dense)               (None, 8)            200         dropout_52[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.1408 - acc: 0.9668 - val_loss: 1.4901e-05 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0570 - acc: 0.9880 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0511 - acc: 0.9893 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0482 - acc: 0.9899 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0463 - acc: 0.9903 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0450 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0442 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0433 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0428 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0424 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0417 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0415 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0412 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0408 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0406 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0404 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0400 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.4852e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.6487e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.0748e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.6149e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1110e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 4.5449e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.4216e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.9474e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 5.8054e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 4.0480e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.5809e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 2.2811e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.5479e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.9713e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 6.9991e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 3.9873e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 3.5281e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 6.2840e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 6.3426e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 5.4761e-06 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.9250e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.5654e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 5.4116e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.7209e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 6.5257e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 5.2724e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.2752e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.4251e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 2.7105e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 5.1749e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.7531e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.6551e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.0529e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.6944e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.5357e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 8.8159e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.7630e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 7.0861e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 7.7438e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 3.7384e-07 - acc: 1.0000
# Training time = 0:12:36.154193
# F-Score(Ordinary) = 0.035, Recall: 1.0, Precision: 0.018
# F-Score(lvc) = 0.059, Recall: 1.0, Precision: 0.03
# F-Score(ireflv) = 0.016, Recall: 1.0, Precision: 0.008
# F-Score(id) = 0.031, Recall: 1.0, Precision: 0.016
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_105 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_106 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_105 (Embedding)       (None, 4, 48)        705264      input_105[0][0]                  
__________________________________________________________________________________________________
embedding_106 (Embedding)       (None, 4, 24)        5640        input_106[0][0]                  
__________________________________________________________________________________________________
flatten_105 (Flatten)           (None, 192)          0           embedding_105[0][0]              
__________________________________________________________________________________________________
flatten_106 (Flatten)           (None, 96)           0           embedding_106[0][0]              
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 288)          0           flatten_105[0][0]                
                                                                 flatten_106[0][0]                
__________________________________________________________________________________________________
dense_105 (Dense)               (None, 24)           6936        concatenate_53[0][0]             
__________________________________________________________________________________________________
dropout_53 (Dropout)            (None, 24)           0           dense_105[0][0]                  
__________________________________________________________________________________________________
dense_106 (Dense)               (None, 8)            200         dropout_53[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.1420 - acc: 0.9661 - val_loss: 3.4869e-05 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0580 - acc: 0.9878 - val_loss: 9.8944e-06 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0517 - acc: 0.9892 - val_loss: 7.1526e-06 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0483 - acc: 0.9899 - val_loss: 5.2452e-06 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0464 - acc: 0.9903 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0451 - acc: 0.9905 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0441 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0433 - acc: 0.9909 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0429 - acc: 0.9910 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0424 - acc: 0.9912 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0420 - acc: 0.9912 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0418 - acc: 0.9913 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0414 - acc: 0.9914 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0413 - acc: 0.9914 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0410 - acc: 0.9915 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0409 - acc: 0.9915 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0407 - acc: 0.9916 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0406 - acc: 0.9916 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0404 - acc: 0.9916 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0403 - acc: 0.9917 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0402 - acc: 0.9916 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0401 - acc: 0.9918 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.6443e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 8.5241e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.0985e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 4.8051e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 5.9950e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 7.0807e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 6.9298e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.3124e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.5083e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.4877e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.6934e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 4.6678e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.3581e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 4.4536e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 5.7557e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 4.1556e-06 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.4587e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 7.3549e-06 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 3.8381e-06 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 3.7794e-06 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 4.2265e-06 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 3.8934e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 3.8131e-06 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 4.8190e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 5.8274e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 4.5630e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.4392e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.7385e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 9.9937e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 2.1389e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.0291e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.8007e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.2042e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.4657e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.6180e-06 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0925e-06 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.0676e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 6.2245e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 3.9554e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.7653e-07 - acc: 1.0000
# Training time = 0:12:36.836970
# F-Score(Ordinary) = 0.167, Recall: 0.953, Precision: 0.092
# F-Score(lvc) = 0.24, Recall: 1.0, Precision: 0.136
# F-Score(ireflv) = 0.164, Recall: 0.917, Precision: 0.09
# F-Score(id) = 0.117, Recall: 0.923, Precision: 0.062
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_107 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_108 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_107 (Embedding)       (None, 4, 48)        705264      input_107[0][0]                  
__________________________________________________________________________________________________
embedding_108 (Embedding)       (None, 4, 24)        5640        input_108[0][0]                  
__________________________________________________________________________________________________
flatten_107 (Flatten)           (None, 192)          0           embedding_107[0][0]              
__________________________________________________________________________________________________
flatten_108 (Flatten)           (None, 96)           0           embedding_108[0][0]              
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 288)          0           flatten_107[0][0]                
                                                                 flatten_108[0][0]                
__________________________________________________________________________________________________
dense_107 (Dense)               (None, 24)           6936        concatenate_54[0][0]             
__________________________________________________________________________________________________
dropout_54 (Dropout)            (None, 24)           0           dense_107[0][0]                  
__________________________________________________________________________________________________
dense_108 (Dense)               (None, 8)            200         dropout_54[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.1461 - acc: 0.9649 - val_loss: 1.4067e-05 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0568 - acc: 0.9878 - val_loss: 4.5300e-06 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0509 - acc: 0.9891 - val_loss: 3.6359e-06 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0478 - acc: 0.9899 - val_loss: 3.9339e-06 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0460 - acc: 0.9903 - val_loss: 4.8876e-06 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0448 - acc: 0.9906 - val_loss: 6.0797e-06 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0439 - acc: 0.9907 - val_loss: 4.7684e-06 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0432 - acc: 0.9910 - val_loss: 4.4704e-06 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0426 - acc: 0.9911 - val_loss: 5.2452e-06 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0422 - acc: 0.9912 - val_loss: 5.0068e-06 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0417 - acc: 0.9913 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0415 - acc: 0.9914 - val_loss: 4.8280e-06 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0413 - acc: 0.9914 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0409 - acc: 0.9915 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.7285e-06 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0406 - acc: 0.9916 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.3709e-06 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.0133e-06 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0390 - acc: 0.9920 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.8692e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.7200e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 2.2526e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 9.0498e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 9.9982e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 9.0474e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.2984e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 7.4793e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.6977e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.2969e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.4358e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 4.0577e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 7.5281e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.2519e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 5.6849e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 4.0709e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 4.3665e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 3.4324e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 3.4698e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 6.1223e-06 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 6.8789e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 4.7311e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.0254e-06 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.3949e-06 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 6.5036e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.1871e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.9143e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 9.6561e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.0998e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 5.2766e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.2827e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 7.3062e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.3649e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.2164e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.2363e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0506e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.1431e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 9.2947e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.0461e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 7.8546e-05 - acc: 1.0000
# Training time = 0:12:34.814546
# F-Score(Ordinary) = 0.073, Recall: 1.0, Precision: 0.038
# F-Score(lvc) = 0.141, Recall: 1.0, Precision: 0.076
# F-Score(ireflv) = 0.094, Recall: 1.0, Precision: 0.049
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_109 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_110 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_109 (Embedding)       (None, 4, 48)        705264      input_109[0][0]                  
__________________________________________________________________________________________________
embedding_110 (Embedding)       (None, 4, 24)        5640        input_110[0][0]                  
__________________________________________________________________________________________________
flatten_109 (Flatten)           (None, 192)          0           embedding_109[0][0]              
__________________________________________________________________________________________________
flatten_110 (Flatten)           (None, 96)           0           embedding_110[0][0]              
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 288)          0           flatten_109[0][0]                
                                                                 flatten_110[0][0]                
__________________________________________________________________________________________________
dense_109 (Dense)               (None, 24)           6936        concatenate_55[0][0]             
__________________________________________________________________________________________________
dropout_55 (Dropout)            (None, 24)           0           dense_109[0][0]                  
__________________________________________________________________________________________________
dense_110 (Dense)               (None, 8)            200         dropout_55[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.1423 - acc: 0.9657 - val_loss: 9.6207e-05 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0591 - acc: 0.9875 - val_loss: 4.5896e-06 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0524 - acc: 0.9890 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0491 - acc: 0.9897 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0469 - acc: 0.9902 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0457 - acc: 0.9905 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0445 - acc: 0.9908 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0438 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0432 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0427 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0423 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0420 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0417 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0415 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0413 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0411 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0410 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0408 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 3.4092e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 4.4384e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 2.7497e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.1737e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 3.2263e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 1.0962e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.7726e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.8888e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 1.7268e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 1.1637e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 8.1667e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 6.9431e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.0390e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 8.5624e-05 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 7.5883e-05 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 1.0502e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 2.1606e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.8274e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 9.1911e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 6.8401e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.2565e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.3219e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.0717e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.3030e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 4.6214e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1961e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 5.3420e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 7.2850e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 9.7361e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.2977e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 3.0313e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 6.7041e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 8.5908e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.0904e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 6.9933e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.9402e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.7597e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.4839e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1441e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 2.8588e-05 - acc: 1.0000
# Training time = 0:12:33.561256
# F-Score(Ordinary) = 0.056, Recall: 0.929, Precision: 0.029
# F-Score(lvc) = 0.015, Recall: 1.0, Precision: 0.008
# F-Score(id) = 0.117, Recall: 1.0, Precision: 0.062
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_111 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_112 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_111 (Embedding)       (None, 4, 48)        705264      input_111[0][0]                  
__________________________________________________________________________________________________
embedding_112 (Embedding)       (None, 4, 24)        5640        input_112[0][0]                  
__________________________________________________________________________________________________
flatten_111 (Flatten)           (None, 192)          0           embedding_111[0][0]              
__________________________________________________________________________________________________
flatten_112 (Flatten)           (None, 96)           0           embedding_112[0][0]              
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 288)          0           flatten_111[0][0]                
                                                                 flatten_112[0][0]                
__________________________________________________________________________________________________
dense_111 (Dense)               (None, 24)           6936        concatenate_56[0][0]             
__________________________________________________________________________________________________
dropout_56 (Dropout)            (None, 24)           0           dense_111[0][0]                  
__________________________________________________________________________________________________
dense_112 (Dense)               (None, 8)            200         dropout_56[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.0990 - acc: 0.9766 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0502 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0462 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0443 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0432 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0426 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.5590e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.3186e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 2.2391e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 2.5529e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 1.4439e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 3.2103e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 2.8180e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 1.7450e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 1.7322e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.4511e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.0233e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.6947e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 6.7531e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 6.7295e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 3.3631e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.6692e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.6438e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 2.6118e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 1.2944e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 9.6580e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 6.4176e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.2275e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 9.4782e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.2564e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 6.2581e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.2445e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.2365e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.2286e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 3.0681e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 2.4284e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.2030e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 2.0856e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 8.8707e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.7598e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 2.0308e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 2.5756e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 8.5059e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 5.6504e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.9596e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 1.6611e-04 - acc: 0.9999
# Training time = 0:12:42.396793
# F-Score(Ordinary) = 0.66, Recall: 0.759, Precision: 0.584
# F-Score(lvc) = 0.478, Recall: 0.649, Precision: 0.379
# F-Score(ireflv) = 0.805, Recall: 0.853, Precision: 0.762
# F-Score(id) = 0.646, Recall: 0.72, Precision: 0.585
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_113 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_114 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_113 (Embedding)       (None, 4, 48)        705264      input_113[0][0]                  
__________________________________________________________________________________________________
embedding_114 (Embedding)       (None, 4, 24)        5640        input_114[0][0]                  
__________________________________________________________________________________________________
flatten_113 (Flatten)           (None, 192)          0           embedding_113[0][0]              
__________________________________________________________________________________________________
flatten_114 (Flatten)           (None, 96)           0           embedding_114[0][0]              
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 288)          0           flatten_113[0][0]                
                                                                 flatten_114[0][0]                
__________________________________________________________________________________________________
dense_113 (Dense)               (None, 24)           6936        concatenate_57[0][0]             
__________________________________________________________________________________________________
dropout_57 (Dropout)            (None, 24)           0           dense_113[0][0]                  
__________________________________________________________________________________________________
dense_114 (Dense)               (None, 8)            200         dropout_57[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.1013 - acc: 0.9768 - val_loss: 1.4901e-06 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0509 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0466 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0445 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0433 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0424 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0419 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0408 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0389 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0389 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 3.0649e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.0541e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 8.7408e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 4.9451e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.3996e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 3.7252e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 3.2551e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 4.6307e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.0829e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 8.0322e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.2266e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.8252e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 2.1331e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 7.0497e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 2.3209e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.1548e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1788e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 6.8880e-06 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 7.3929e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.9210e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.3298e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 6.1342e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 4.2849e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 6.7000e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.0552e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 3.1026e-06 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.3367e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 4.0673e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.3764e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.2832e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 8.6736e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 3.7803e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 9.1513e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.2328e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.3189e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 4.4833e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.2036e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.4798e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 7.1165e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.8052e-06 - acc: 1.0000
# Training time = 0:12:38.337250
# F-Score(Ordinary) = 0.035, Recall: 1.0, Precision: 0.018
# F-Score(ireflv) = 0.123, Recall: 1.0, Precision: 0.066
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_115 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_116 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_115 (Embedding)       (None, 4, 48)        705264      input_115[0][0]                  
__________________________________________________________________________________________________
embedding_116 (Embedding)       (None, 4, 24)        5640        input_116[0][0]                  
__________________________________________________________________________________________________
flatten_115 (Flatten)           (None, 192)          0           embedding_115[0][0]              
__________________________________________________________________________________________________
flatten_116 (Flatten)           (None, 96)           0           embedding_116[0][0]              
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 288)          0           flatten_115[0][0]                
                                                                 flatten_116[0][0]                
__________________________________________________________________________________________________
dense_115 (Dense)               (None, 24)           6936        concatenate_58[0][0]             
__________________________________________________________________________________________________
dropout_58 (Dropout)            (None, 24)           0           dense_115[0][0]                  
__________________________________________________________________________________________________
dense_116 (Dense)               (None, 8)            200         dropout_58[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.1021 - acc: 0.9762 - val_loss: 1.8358e-05 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0512 - acc: 0.9893 - val_loss: 1.6093e-05 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0467 - acc: 0.9902 - val_loss: 1.8954e-05 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0444 - acc: 0.9906 - val_loss: 8.7023e-06 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0433 - acc: 0.9909 - val_loss: 3.9339e-06 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0426 - acc: 0.9911 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0419 - acc: 0.9913 - val_loss: 2.6822e-06 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0412 - acc: 0.9914 - val_loss: 1.7285e-06 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0408 - acc: 0.9915 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0407 - acc: 0.9916 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0406 - acc: 0.9916 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0403 - acc: 0.9916 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.0133e-06 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0400 - acc: 0.9918 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.0133e-06 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0390 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.9684e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.3682e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 6.1770e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 8.1637e-05 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 1.3909e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.0221e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 3.4925e-06 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.2478e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.0692e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.8659e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.1222e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 5.6780e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 2.7512e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.0844e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.5524e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.8950e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.0934e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 3.0643e-06 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 2.3004e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 4.2634e-06 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 5.9382e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.7323e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 2.2364e-06 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 4.9110e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1420e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.9890e-06 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 9.5691e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 2.4132e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 2.9647e-06 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 6.7762e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 4.5260e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.5879e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.3609e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.7598e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.6194e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.2271e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 8.9642e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 4.7705e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1963e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 3.9004e-07 - acc: 1.0000
# Training time = 0:12:32.275409
# F-Score(Ordinary) = 0.073, Recall: 1.0, Precision: 0.038
# F-Score(lvc) = 0.015, Recall: 1.0, Precision: 0.008
# F-Score(ireflv) = 0.123, Recall: 1.0, Precision: 0.066
# F-Score(id) = 0.08, Recall: 1.0, Precision: 0.041
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_117 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_118 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_117 (Embedding)       (None, 4, 48)        705264      input_117[0][0]                  
__________________________________________________________________________________________________
embedding_118 (Embedding)       (None, 4, 24)        5640        input_118[0][0]                  
__________________________________________________________________________________________________
flatten_117 (Flatten)           (None, 192)          0           embedding_117[0][0]              
__________________________________________________________________________________________________
flatten_118 (Flatten)           (None, 96)           0           embedding_118[0][0]              
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 288)          0           flatten_117[0][0]                
                                                                 flatten_118[0][0]                
__________________________________________________________________________________________________
dense_117 (Dense)               (None, 24)           6936        concatenate_59[0][0]             
__________________________________________________________________________________________________
dropout_59 (Dropout)            (None, 24)           0           dense_117[0][0]                  
__________________________________________________________________________________________________
dense_118 (Dense)               (None, 8)            200         dropout_59[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.1032 - acc: 0.9759 - val_loss: 4.3511e-06 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0504 - acc: 0.9893 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0461 - acc: 0.9902 - val_loss: 2.2054e-06 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0442 - acc: 0.9906 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0431 - acc: 0.9910 - val_loss: 5.1260e-06 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0423 - acc: 0.9910 - val_loss: 4.6492e-06 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0418 - acc: 0.9912 - val_loss: 2.8610e-06 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0406 - acc: 0.9916 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0405 - acc: 0.9916 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0403 - acc: 0.9917 - val_loss: 4.2915e-06 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.8477e-06 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.3709e-06 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.0133e-06 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0392 - acc: 0.9920 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0391 - acc: 0.9921 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0390 - acc: 0.9921 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0390 - acc: 0.9921 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0386 - acc: 0.9922 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.9491e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 7.2503e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.2603e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 5.7847e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 8.3830e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 8.2936e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.9354e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.0440e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.8876e-06 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.0825e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 4.7409e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 3.6439e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.8968e-06 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 8.6235e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 4.8975e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1217e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 7.8761e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 3.4299e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 5.5694e-06 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 4.4635e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.4041e-06 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 2.1438e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.2718e-06 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 4.6693e-06 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 4.6844e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.7536e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 3.7712e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.7608e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 4.1364e-06 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 4.2809e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 4.6932e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.4132e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.1949e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.2095e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 3.6374e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.2821e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.6289e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.3118e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.8057e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.7616e-07 - acc: 1.0000
# Training time = 0:12:30.577069
# F-Score(Ordinary) = 0.171, Recall: 0.933, Precision: 0.094
# F-Score(lvc) = 0.128, Recall: 1.0, Precision: 0.068
# F-Score(ireflv) = 0.227, Recall: 0.842, Precision: 0.131
# F-Score(id) = 0.162, Recall: 1.0, Precision: 0.088
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_119 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_120 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_119 (Embedding)       (None, 4, 48)        705264      input_119[0][0]                  
__________________________________________________________________________________________________
embedding_120 (Embedding)       (None, 4, 24)        5640        input_120[0][0]                  
__________________________________________________________________________________________________
flatten_119 (Flatten)           (None, 192)          0           embedding_119[0][0]              
__________________________________________________________________________________________________
flatten_120 (Flatten)           (None, 96)           0           embedding_120[0][0]              
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 288)          0           flatten_119[0][0]                
                                                                 flatten_120[0][0]                
__________________________________________________________________________________________________
dense_119 (Dense)               (None, 24)           6936        concatenate_60[0][0]             
__________________________________________________________________________________________________
dropout_60 (Dropout)            (None, 24)           0           dense_119[0][0]                  
__________________________________________________________________________________________________
dense_120 (Dense)               (None, 8)            200         dropout_60[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.1030 - acc: 0.9760 - val_loss: 1.6809e-05 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0520 - acc: 0.9891 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0472 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0449 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0435 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0428 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0421 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0417 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0413 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.7779e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.8669e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 1.0236e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 3.9549e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.9791e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 5.0150e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 7.1553e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 3.5556e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 6.4786e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 8.0767e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.5116e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.2366e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 2.2789e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 4.4625e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1141e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 6.1298e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 8.2808e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.3357e-06 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 7.5626e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 7.0612e-06 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 3.1462e-06 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 3.7434e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.7170e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.2994e-06 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 3.7102e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 4.8450e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.6673e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 6.9720e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 4.6759e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.7214e-06 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.3526e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 4.8990e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.2431e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.6134e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.4296e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.2259e-06 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 3.8040e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.8491e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 5.7478e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1467e-06 - acc: 1.0000
# Training time = 0:12:30.966331
# F-Score(Ordinary) = 0.093, Recall: 0.917, Precision: 0.049
# F-Score(lvc) = 0.059, Recall: 1.0, Precision: 0.03
# F-Score(ireflv) = 0.048, Recall: 1.0, Precision: 0.025
# F-Score(id) = 0.144, Recall: 1.0, Precision: 0.078
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_121 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_122 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_121 (Embedding)       (None, 4, 48)        705264      input_121[0][0]                  
__________________________________________________________________________________________________
embedding_122 (Embedding)       (None, 4, 24)        5640        input_122[0][0]                  
__________________________________________________________________________________________________
flatten_121 (Flatten)           (None, 192)          0           embedding_121[0][0]              
__________________________________________________________________________________________________
flatten_122 (Flatten)           (None, 96)           0           embedding_122[0][0]              
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 288)          0           flatten_121[0][0]                
                                                                 flatten_122[0][0]                
__________________________________________________________________________________________________
dense_121 (Dense)               (None, 24)           6936        concatenate_61[0][0]             
__________________________________________________________________________________________________
dropout_61 (Dropout)            (None, 24)           0           dense_121[0][0]                  
__________________________________________________________________________________________________
dense_122 (Dense)               (None, 8)            200         dropout_61[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.0728 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0458 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0434 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0422 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0414 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0412 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0408 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.5646e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.4621e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 2.4104e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 2.7492e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 1.5425e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 3.3936e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 2.9280e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 1.7886e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 1.7565e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.4051e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.0124e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.6617e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 6.5724e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 6.5140e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 3.2441e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.5985e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.4923e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 2.4135e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 1.1777e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 8.7076e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 5.7483e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.9709e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 8.2728e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.0860e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 5.3641e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.0574e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.0386e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.0198e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 2.5293e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.9734e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 9.5969e-05 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.6352e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 6.8440e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.3375e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 1.5103e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 1.8629e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 6.0245e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.9673e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.3528e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 1.1193e-04 - acc: 1.0000
# Training time = 0:12:37.494220
# F-Score(Ordinary) = 0.662, Recall: 0.645, Precision: 0.68
# F-Score(lvc) = 0.534, Recall: 0.663, Precision: 0.447
# F-Score(ireflv) = 0.831, Recall: 0.881, Precision: 0.787
# F-Score(id) = 0.554, Recall: 0.473, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_123 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_124 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_123 (Embedding)       (None, 4, 48)        705264      input_123[0][0]                  
__________________________________________________________________________________________________
embedding_124 (Embedding)       (None, 4, 24)        5640        input_124[0][0]                  
__________________________________________________________________________________________________
flatten_123 (Flatten)           (None, 192)          0           embedding_123[0][0]              
__________________________________________________________________________________________________
flatten_124 (Flatten)           (None, 96)           0           embedding_124[0][0]              
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 288)          0           flatten_123[0][0]                
                                                                 flatten_124[0][0]                
__________________________________________________________________________________________________
dense_123 (Dense)               (None, 24)           6936        concatenate_62[0][0]             
__________________________________________________________________________________________________
dropout_62 (Dropout)            (None, 24)           0           dense_123[0][0]                  
__________________________________________________________________________________________________
dense_124 (Dense)               (None, 8)            200         dropout_62[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0740 - acc: 0.9833 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0460 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0434 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0423 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0417 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0412 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0407 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0404 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.5210e-04 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 7.3851e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.2718e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 4.3261e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 7.9510e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 7.4082e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.0819e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.0459e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 6.9882e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.7812e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 7.3890e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.4050e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 2.3862e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.0350e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.7160e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 3.4596e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 2.9297e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 3.3157e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.2126e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.2122e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1953e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1958e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1923e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 2.8482e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1938e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 3.1755e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 7.2691e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 5.7578e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.6902e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.8870e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.6133e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.7519e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:34.298320
# F-Score(Ordinary) = 0.185, Recall: 0.939, Precision: 0.103
# F-Score(lvc) = 0.086, Recall: 0.857, Precision: 0.045
# F-Score(ireflv) = 0.476, Recall: 0.929, Precision: 0.32
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_125 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_126 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_125 (Embedding)       (None, 4, 48)        705264      input_125[0][0]                  
__________________________________________________________________________________________________
embedding_126 (Embedding)       (None, 4, 24)        5640        input_126[0][0]                  
__________________________________________________________________________________________________
flatten_125 (Flatten)           (None, 192)          0           embedding_125[0][0]              
__________________________________________________________________________________________________
flatten_126 (Flatten)           (None, 96)           0           embedding_126[0][0]              
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 288)          0           flatten_125[0][0]                
                                                                 flatten_126[0][0]                
__________________________________________________________________________________________________
dense_125 (Dense)               (None, 24)           6936        concatenate_63[0][0]             
__________________________________________________________________________________________________
dropout_63 (Dropout)            (None, 24)           0           dense_125[0][0]                  
__________________________________________________________________________________________________
dense_126 (Dense)               (None, 8)            200         dropout_63[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0748 - acc: 0.9831 - val_loss: 4.5896e-06 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0463 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0435 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0422 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0415 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1492e-04 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 3.2963e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 2.3992e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 8.6903e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 8.4675e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 7.9892e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 5.4315e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.7782e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 3.0752e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.9657e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.8266e-06 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 5.4403e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 2.9630e-06 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.1466e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 4.1623e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 8.1511e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.2210e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.3353e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1930e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1964e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.5972e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.2383e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.6621e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 4.7908e-06 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 3.1969e-06 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.0275e-06 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.8456e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.2572e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.2177e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.2049e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 7.6421e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 4.3106e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.5882e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.2976e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1925e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.2508e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.2027e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:35.578945
# F-Score(Ordinary) = 0.248, Recall: 0.833, Precision: 0.145
# F-Score(lvc) = 0.291, Recall: 0.885, Precision: 0.174
# F-Score(ireflv) = 0.418, Recall: 0.917, Precision: 0.27
# F-Score(id) = 0.089, Recall: 1.0, Precision: 0.047
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_127 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_128 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_127 (Embedding)       (None, 4, 48)        705264      input_127[0][0]                  
__________________________________________________________________________________________________
embedding_128 (Embedding)       (None, 4, 24)        5640        input_128[0][0]                  
__________________________________________________________________________________________________
flatten_127 (Flatten)           (None, 192)          0           embedding_127[0][0]              
__________________________________________________________________________________________________
flatten_128 (Flatten)           (None, 96)           0           embedding_128[0][0]              
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 288)          0           flatten_127[0][0]                
                                                                 flatten_128[0][0]                
__________________________________________________________________________________________________
dense_127 (Dense)               (None, 24)           6936        concatenate_64[0][0]             
__________________________________________________________________________________________________
dropout_64 (Dropout)            (None, 24)           0           dense_127[0][0]                  
__________________________________________________________________________________________________
dense_128 (Dense)               (None, 8)            200         dropout_64[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.0748 - acc: 0.9831 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0460 - acc: 0.9902 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0432 - acc: 0.9908 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0415 - acc: 0.9914 - val_loss: 5.8413e-06 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0410 - acc: 0.9915 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0404 - acc: 0.9916 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 5.9009e-06 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.8477e-06 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.8011e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.2323e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 7.0488e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 8.8088e-06 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 8.9872e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 8.1866e-06 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 6.4759e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.1684e-06 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 5.3701e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.0011e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.7073e-06 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.2792e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 4.2704e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 7.5434e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.2141e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.3522e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.2910e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.0295e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.2384e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1946e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.4101e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.2020e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.6029e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1945e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 7.7183e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 7.3209e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1925e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.2046e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1942e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.2304e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1929e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.2033e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1924e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.5178e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1943e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1943e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.0348e-06 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.6481e-06 - acc: 1.0000
# Training time = 0:12:37.203976
# F-Score(Ordinary) = 0.156, Recall: 0.95, Precision: 0.085
# F-Score(lvc) = 0.044, Recall: 1.0, Precision: 0.023
# F-Score(ireflv) = 0.379, Recall: 0.935, Precision: 0.238
# F-Score(id) = 0.06, Recall: 1.0, Precision: 0.031
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_129 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_130 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_129 (Embedding)       (None, 4, 48)        705264      input_129[0][0]                  
__________________________________________________________________________________________________
embedding_130 (Embedding)       (None, 4, 24)        5640        input_130[0][0]                  
__________________________________________________________________________________________________
flatten_129 (Flatten)           (None, 192)          0           embedding_129[0][0]              
__________________________________________________________________________________________________
flatten_130 (Flatten)           (None, 96)           0           embedding_130[0][0]              
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 288)          0           flatten_129[0][0]                
                                                                 flatten_130[0][0]                
__________________________________________________________________________________________________
dense_129 (Dense)               (None, 24)           6936        concatenate_65[0][0]             
__________________________________________________________________________________________________
dropout_65 (Dropout)            (None, 24)           0           dense_129[0][0]                  
__________________________________________________________________________________________________
dense_130 (Dense)               (None, 8)            200         dropout_65[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0755 - acc: 0.9830 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0467 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0439 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0426 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0389 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0388 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0389 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.9461e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 7.4346e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.7400e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.0007e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 3.2819e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 9.2496e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 3.4581e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.4922e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 4.4773e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.8234e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.3510e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1135e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.0880e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 3.6137e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 5.2140e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.3513e-06 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.4173e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 5.1603e-06 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 5.6672e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.2370e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 5.6548e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.2718e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.2462e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 9.9195e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.2214e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.6685e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.0958e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1923e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1939e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1926e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 5.6029e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.5082e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.3241e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.2449e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.2137e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1925e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.2007e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 5.5877e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:30.791923
# F-Score(Ordinary) = 0.073, Recall: 1.0, Precision: 0.038
# F-Score(ireflv) = 0.109, Recall: 1.0, Precision: 0.057
# F-Score(id) = 0.099, Recall: 1.0, Precision: 0.052
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_131 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_132 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_131 (Embedding)       (None, 4, 48)        705264      input_131[0][0]                  
__________________________________________________________________________________________________
embedding_132 (Embedding)       (None, 4, 24)        5640        input_132[0][0]                  
__________________________________________________________________________________________________
flatten_131 (Flatten)           (None, 192)          0           embedding_131[0][0]              
__________________________________________________________________________________________________
flatten_132 (Flatten)           (None, 96)           0           embedding_132[0][0]              
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 288)          0           flatten_131[0][0]                
                                                                 flatten_132[0][0]                
__________________________________________________________________________________________________
dense_131 (Dense)               (None, 24)           6936        concatenate_66[0][0]             
__________________________________________________________________________________________________
dropout_66 (Dropout)            (None, 24)           0           dense_131[0][0]                  
__________________________________________________________________________________________________
dense_132 (Dense)               (None, 8)            200         dropout_66[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0637 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0451 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0431 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0419 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0412 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.6204e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.2790e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.5154e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.2415e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.4481e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.4041e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 4.6762e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.8345e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 3.3131e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 4.7836e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.0340e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 3.0369e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.4816e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 3.3616e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 3.6806e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 1.7769e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.5851e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 1.6709e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.4269e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 1.9506e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 2.9857e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 2.4773e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 1.0247e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.3296e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 9.7111e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.1771e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.1902e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.1512e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 2.1875e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 1.2905e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.2325e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.4019e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.7444e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 1.4134e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 1.1289e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 5.3740e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 8.5641e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 6.4924e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 7.6879e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 9.9910e-05 - acc: 1.0000
# Training time = 0:12:33.218548
# F-Score(Ordinary) = 0.639, Recall: 0.648, Precision: 0.631
# F-Score(lvc) = 0.538, Recall: 0.815, Precision: 0.402
# F-Score(ireflv) = 0.822, Recall: 0.851, Precision: 0.795
# F-Score(id) = 0.552, Recall: 0.484, Precision: 0.642
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_133 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_134 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_133 (Embedding)       (None, 4, 48)        705264      input_133[0][0]                  
__________________________________________________________________________________________________
embedding_134 (Embedding)       (None, 4, 24)        5640        input_134[0][0]                  
__________________________________________________________________________________________________
flatten_133 (Flatten)           (None, 192)          0           embedding_133[0][0]              
__________________________________________________________________________________________________
flatten_134 (Flatten)           (None, 96)           0           embedding_134[0][0]              
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 288)          0           flatten_133[0][0]                
                                                                 flatten_134[0][0]                
__________________________________________________________________________________________________
dense_133 (Dense)               (None, 24)           6936        concatenate_67[0][0]             
__________________________________________________________________________________________________
dropout_67 (Dropout)            (None, 24)           0           dense_133[0][0]                  
__________________________________________________________________________________________________
dense_134 (Dense)               (None, 8)            200         dropout_67[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0644 - acc: 0.9856 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0449 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0430 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0421 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0414 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0410 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0407 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0404 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0394 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.4993e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.1179e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.1857e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.0877e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.1540e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 2.1410e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.3644e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.2503e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.1293e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1971e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.0611e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 3.1642e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.0970e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.0445e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.0417e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.0389e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.0362e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.0334e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.0306e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.0519e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.0223e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.0196e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0168e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.0140e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:30.816414
# F-Score(Ordinary) = 0.679, Recall: 0.751, Precision: 0.62
# F-Score(lvc) = 0.48, Recall: 0.581, Precision: 0.409
# F-Score(ireflv) = 0.771, Recall: 0.721, Precision: 0.828
# F-Score(id) = 0.717, Recall: 0.868, Precision: 0.611
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_135 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_136 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_135 (Embedding)       (None, 4, 48)        705264      input_135[0][0]                  
__________________________________________________________________________________________________
embedding_136 (Embedding)       (None, 4, 24)        5640        input_136[0][0]                  
__________________________________________________________________________________________________
flatten_135 (Flatten)           (None, 192)          0           embedding_135[0][0]              
__________________________________________________________________________________________________
flatten_136 (Flatten)           (None, 96)           0           embedding_136[0][0]              
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 288)          0           flatten_135[0][0]                
                                                                 flatten_136[0][0]                
__________________________________________________________________________________________________
dense_135 (Dense)               (None, 24)           6936        concatenate_68[0][0]             
__________________________________________________________________________________________________
dropout_68 (Dropout)            (None, 24)           0           dense_135[0][0]                  
__________________________________________________________________________________________________
dense_136 (Dense)               (None, 8)            200         dropout_68[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0643 - acc: 0.9855 - val_loss: 1.8477e-06 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0447 - acc: 0.9904 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0430 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0420 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0394 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0395 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.2392e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.1158e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 2.7153e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.6924e-06 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 4.2827e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 2.8775e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.5353e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.4788e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 6.5572e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.2324e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1841e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.2659e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1787e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1760e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.2274e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.2033e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1732e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.2131e-06 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1923e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1923e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1972e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1925e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:34.047629
# F-Score(Ordinary) = 0.537, Recall: 0.866, Precision: 0.389
# F-Score(lvc) = 0.457, Recall: 0.93, Precision: 0.303
# F-Score(ireflv) = 0.745, Recall: 0.927, Precision: 0.623
# F-Score(id) = 0.424, Recall: 0.75, Precision: 0.295
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_137 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_138 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_137 (Embedding)       (None, 4, 48)        705264      input_137[0][0]                  
__________________________________________________________________________________________________
embedding_138 (Embedding)       (None, 4, 24)        5640        input_138[0][0]                  
__________________________________________________________________________________________________
flatten_137 (Flatten)           (None, 192)          0           embedding_137[0][0]              
__________________________________________________________________________________________________
flatten_138 (Flatten)           (None, 96)           0           embedding_138[0][0]              
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 288)          0           flatten_137[0][0]                
                                                                 flatten_138[0][0]                
__________________________________________________________________________________________________
dense_137 (Dense)               (None, 24)           6936        concatenate_69[0][0]             
__________________________________________________________________________________________________
dropout_69 (Dropout)            (None, 24)           0           dense_137[0][0]                  
__________________________________________________________________________________________________
dense_138 (Dense)               (None, 8)            200         dropout_69[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.0645 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0451 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0430 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0420 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0415 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.8443e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 6.5802e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.9997e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 3.9266e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.5636e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 7.7472e-06 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 4.6624e-06 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 7.1048e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.0776e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.9957e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.6416e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.3232e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.5795e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.2663e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.2544e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1980e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1971e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1939e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1924e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1926e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 8.6839e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 8.6562e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 8.6286e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:37.348526
# F-Score(Ordinary) = 0.145, Recall: 0.946, Precision: 0.078
# F-Score(ireflv) = 0.313, Recall: 0.92, Precision: 0.189
# F-Score(id) = 0.117, Recall: 1.0, Precision: 0.062
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_139 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_140 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_139 (Embedding)       (None, 4, 48)        705264      input_139[0][0]                  
__________________________________________________________________________________________________
embedding_140 (Embedding)       (None, 4, 24)        5640        input_140[0][0]                  
__________________________________________________________________________________________________
flatten_139 (Flatten)           (None, 192)          0           embedding_139[0][0]              
__________________________________________________________________________________________________
flatten_140 (Flatten)           (None, 96)           0           embedding_140[0][0]              
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 288)          0           flatten_139[0][0]                
                                                                 flatten_140[0][0]                
__________________________________________________________________________________________________
dense_139 (Dense)               (None, 24)           6936        concatenate_70[0][0]             
__________________________________________________________________________________________________
dropout_70 (Dropout)            (None, 24)           0           dense_139[0][0]                  
__________________________________________________________________________________________________
dense_140 (Dense)               (None, 8)            200         dropout_70[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adamax, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0651 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0452 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0432 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0420 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0413 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.3129e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.0301e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.8498e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 9.4238e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 9.3434e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.8865e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 9.2998e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 3.6877e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 9.1591e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 9.1313e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 9.1036e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1923e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 9.0759e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.7038e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 8.9653e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 8.9376e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 2.6623e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 8.8270e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 8.7993e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.7504e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.7393e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 8.6611e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 8.6335e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 8.6058e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.7117e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 8.5229e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 8.4952e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:32.359309
# F-Score(Ordinary) = 0.689, Recall: 0.737, Precision: 0.647
# F-Score(lvc) = 0.607, Recall: 0.884, Precision: 0.462
# F-Score(ireflv) = 0.845, Recall: 0.863, Precision: 0.828
# F-Score(id) = 0.627, Recall: 0.607, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_141 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_142 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_141 (Embedding)       (None, 4, 48)        705264      input_141[0][0]                  
__________________________________________________________________________________________________
embedding_142 (Embedding)       (None, 4, 24)        5640        input_142[0][0]                  
__________________________________________________________________________________________________
flatten_141 (Flatten)           (None, 192)          0           embedding_141[0][0]              
__________________________________________________________________________________________________
flatten_142 (Flatten)           (None, 96)           0           embedding_142[0][0]              
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 288)          0           flatten_141[0][0]                
                                                                 flatten_142[0][0]                
__________________________________________________________________________________________________
dense_141 (Dense)               (None, 24)           6936        concatenate_71[0][0]             
__________________________________________________________________________________________________
dropout_71 (Dropout)            (None, 24)           0           dense_141[0][0]                  
__________________________________________________________________________________________________
dense_142 (Dense)               (None, 8)            200         dropout_71[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0784 - acc: 0.9826 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0448 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0423 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0382 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0376 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.9262e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.2624e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.2715e-06 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 2.2115e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 7.3005e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 2.1533e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.4128e-04 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.5732e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.3916e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 6.8660e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.2512e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.2524e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 6.8085e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 6.7251e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 6.6510e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1956e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 6.5752e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1928e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 6.4938e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 6.4154e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1923e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.2587e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.2322e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 6.0728e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 6.0041e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.7528e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.6975e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 5.5562e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 5.4969e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.0751e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.2873e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.2198e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:12.835628
# F-Score(Ordinary) = 0.613, Recall: 0.544, Precision: 0.7
# F-Score(lvc) = 0.585, Recall: 0.775, Precision: 0.47
# F-Score(ireflv) = 0.802, Recall: 0.792, Precision: 0.811
# F-Score(id) = 0.494, Recall: 0.376, Precision: 0.72
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_143 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_144 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_143 (Embedding)       (None, 4, 48)        705264      input_143[0][0]                  
__________________________________________________________________________________________________
embedding_144 (Embedding)       (None, 4, 24)        5640        input_144[0][0]                  
__________________________________________________________________________________________________
flatten_143 (Flatten)           (None, 192)          0           embedding_143[0][0]              
__________________________________________________________________________________________________
flatten_144 (Flatten)           (None, 96)           0           embedding_144[0][0]              
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 288)          0           flatten_143[0][0]                
                                                                 flatten_144[0][0]                
__________________________________________________________________________________________________
dense_143 (Dense)               (None, 24)           6936        concatenate_72[0][0]             
__________________________________________________________________________________________________
dropout_72 (Dropout)            (None, 24)           0           dense_143[0][0]                  
__________________________________________________________________________________________________
dense_144 (Dense)               (None, 8)            200         dropout_72[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0797 - acc: 0.9827 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0451 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0426 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0390 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0380 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.9187e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 8.4341e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.7919e-06 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.0893e-06 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 7.9154e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 7.8452e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 7.7725e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.5354e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.5107e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.4173e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 7.4697e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 7.3925e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 7.3168e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.2125e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 7.2503e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 7.1676e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.4110e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.3858e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.3643e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.3400e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.3168e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.2927e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 6.3727e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 6.3032e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.2405e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.2126e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 5.9608e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 5.9017e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 5.8283e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.7633e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 5.6781e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 5.5958e-05 - acc: 1.0000
# Training time = 0:12:15.309537
# F-Score(Ordinary) = 0.647, Recall: 0.592, Precision: 0.714
# F-Score(lvc) = 0.391, Recall: 0.307, Precision: 0.538
# F-Score(ireflv) = 0.81, Recall: 0.73, Precision: 0.91
# F-Score(id) = 0.745, Recall: 0.833, Precision: 0.674
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_145 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_146 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_145 (Embedding)       (None, 4, 48)        705264      input_145[0][0]                  
__________________________________________________________________________________________________
embedding_146 (Embedding)       (None, 4, 24)        5640        input_146[0][0]                  
__________________________________________________________________________________________________
flatten_145 (Flatten)           (None, 192)          0           embedding_145[0][0]              
__________________________________________________________________________________________________
flatten_146 (Flatten)           (None, 96)           0           embedding_146[0][0]              
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 288)          0           flatten_145[0][0]                
                                                                 flatten_146[0][0]                
__________________________________________________________________________________________________
dense_145 (Dense)               (None, 24)           6936        concatenate_73[0][0]             
__________________________________________________________________________________________________
dropout_73 (Dropout)            (None, 24)           0           dense_145[0][0]                  
__________________________________________________________________________________________________
dense_146 (Dense)               (None, 8)            200         dropout_73[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0803 - acc: 0.9824 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0451 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0424 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0380 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0377 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0387 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0387 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0375 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 5.4875e-05 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 3.1750e-06 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.0125e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.0012e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 3.8551e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 9.8813e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 9.8102e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.8903e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.4093e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.4042e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.3177e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.2428e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.2364e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.2064e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.2056e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 9.7312e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 9.6442e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1941e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1929e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1923e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1924e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.9032e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 9.4078e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.8552e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.6302e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 8.9565e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 8.8995e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 8.8365e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 8.7520e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.7259e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 8.5421e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:11.360136
# F-Score(Ordinary) = 0.694, Recall: 0.769, Precision: 0.633
# F-Score(lvc) = 0.621, Recall: 0.887, Precision: 0.477
# F-Score(ireflv) = 0.809, Recall: 0.841, Precision: 0.779
# F-Score(id) = 0.647, Recall: 0.663, Precision: 0.632
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_147 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_148 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_147 (Embedding)       (None, 4, 48)        705264      input_147[0][0]                  
__________________________________________________________________________________________________
embedding_148 (Embedding)       (None, 4, 24)        5640        input_148[0][0]                  
__________________________________________________________________________________________________
flatten_147 (Flatten)           (None, 192)          0           embedding_147[0][0]              
__________________________________________________________________________________________________
flatten_148 (Flatten)           (None, 96)           0           embedding_148[0][0]              
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 288)          0           flatten_147[0][0]                
                                                                 flatten_148[0][0]                
__________________________________________________________________________________________________
dense_147 (Dense)               (None, 24)           6936        concatenate_74[0][0]             
__________________________________________________________________________________________________
dropout_74 (Dropout)            (None, 24)           0           dense_147[0][0]                  
__________________________________________________________________________________________________
dense_148 (Dense)               (None, 8)            200         dropout_74[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0802 - acc: 0.9825 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0447 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0423 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0411 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0388 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0381 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 9.3057e-05 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 3.9789e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.8948e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 5.5731e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 5.5898e-06 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 9.8720e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 4.4252e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.4545e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 4.1442e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.6370e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 9.7830e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.4650e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.2507e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.2155e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.2126e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.2011e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.2550e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 9.6935e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1979e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1939e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1932e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1931e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1929e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1926e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 9.6064e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1923e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 9.5194e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 9.4324e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 9.3540e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:12.515872
# F-Score(Ordinary) = 0.665, Recall: 0.639, Precision: 0.694
# F-Score(lvc) = 0.628, Recall: 0.867, Precision: 0.492
# F-Score(ireflv) = 0.807, Recall: 0.828, Precision: 0.787
# F-Score(id) = 0.554, Recall: 0.459, Precision: 0.699
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_149 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_150 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_149 (Embedding)       (None, 4, 48)        705264      input_149[0][0]                  
__________________________________________________________________________________________________
embedding_150 (Embedding)       (None, 4, 24)        5640        input_150[0][0]                  
__________________________________________________________________________________________________
flatten_149 (Flatten)           (None, 192)          0           embedding_149[0][0]              
__________________________________________________________________________________________________
flatten_150 (Flatten)           (None, 96)           0           embedding_150[0][0]              
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 288)          0           flatten_149[0][0]                
                                                                 flatten_150[0][0]                
__________________________________________________________________________________________________
dense_149 (Dense)               (None, 24)           6936        concatenate_75[0][0]             
__________________________________________________________________________________________________
dropout_75 (Dropout)            (None, 24)           0           dense_149[0][0]                  
__________________________________________________________________________________________________
dense_150 (Dense)               (None, 8)            200         dropout_75[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0808 - acc: 0.9823 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0453 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0426 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0382 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0377 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0377 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0374 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 5.7108e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 4.8688e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 3.4163e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 4.0193e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 2.6244e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.5723e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 2.5084e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 3.0420e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 4.2576e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.2640e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 2.2025e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 2.6613e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.5507e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.5000e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 2.8695e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 4.0707e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 1.3016e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 8.4917e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.2380e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.9724e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.1346e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 7.3533e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.4146e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 3.4355e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.3218e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 2.4513e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.1499e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.0967e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 1.2933e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 1.9059e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 6.6896e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.0538e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 9.7732e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 3.7095e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.3225e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1283e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 8.6829e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 5.3228e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 6.1597e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 7.7414e-05 - acc: 1.0000
# Training time = 0:12:32.388232
# F-Score(Ordinary) = 0.725, Recall: 0.83, Precision: 0.644
# F-Score(lvc) = 0.641, Recall: 0.87, Precision: 0.508
# F-Score(ireflv) = 0.809, Recall: 0.841, Precision: 0.779
# F-Score(id) = 0.714, Recall: 0.796, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_151 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_152 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_151 (Embedding)       (None, 4, 48)        705264      input_151[0][0]                  
__________________________________________________________________________________________________
embedding_152 (Embedding)       (None, 4, 24)        5640        input_152[0][0]                  
__________________________________________________________________________________________________
flatten_151 (Flatten)           (None, 192)          0           embedding_151[0][0]              
__________________________________________________________________________________________________
flatten_152 (Flatten)           (None, 96)           0           embedding_152[0][0]              
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 288)          0           flatten_151[0][0]                
                                                                 flatten_152[0][0]                
__________________________________________________________________________________________________
dense_151 (Dense)               (None, 24)           6936        concatenate_76[0][0]             
__________________________________________________________________________________________________
dropout_76 (Dropout)            (None, 24)           0           dense_151[0][0]                  
__________________________________________________________________________________________________
dense_152 (Dense)               (None, 8)            200         dropout_76[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0673 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0448 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0425 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0411 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0428 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0413 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0407 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0397 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0406 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0401 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0422 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0436 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0414 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0417 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0417 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0403 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0404 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0398 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0399 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0410 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0405 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 8.4628e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 8.1084e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 7.9488e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 9.0371e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 5.0431e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 9.3523e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 5.6503e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 5.4700e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 7.3511e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 3.0398e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 4.8993e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.9075e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.8706e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 9.2192e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 4.4253e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 6.6273e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 6.1876e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.9382e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 2.1263e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.3733e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 4.5179e-04 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 1.8200e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.3069e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 1.0993e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.0852e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.9411e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.8002e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 4.3002e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.1297e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.3936e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 2.1702e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 8.3202e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.4814e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 1.4661e-04 - acc: 0.9999
Epoch 36/40
 - 2s - loss: 1.5313e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 4.3463e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.6816e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 7.8327e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 5.2347e-05 - acc: 1.0000
# Training time = 0:12:05.987796
# F-Score(Ordinary) = 0.591, Recall: 0.511, Precision: 0.7
# F-Score(lvc) = 0.545, Recall: 0.6, Precision: 0.5
# F-Score(ireflv) = 0.754, Recall: 0.675, Precision: 0.852
# F-Score(id) = 0.502, Recall: 0.39, Precision: 0.705
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_153 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_154 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_153 (Embedding)       (None, 4, 48)        705264      input_153[0][0]                  
__________________________________________________________________________________________________
embedding_154 (Embedding)       (None, 4, 24)        5640        input_154[0][0]                  
__________________________________________________________________________________________________
flatten_153 (Flatten)           (None, 192)          0           embedding_153[0][0]              
__________________________________________________________________________________________________
flatten_154 (Flatten)           (None, 96)           0           embedding_154[0][0]              
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 288)          0           flatten_153[0][0]                
                                                                 flatten_154[0][0]                
__________________________________________________________________________________________________
dense_153 (Dense)               (None, 24)           6936        concatenate_77[0][0]             
__________________________________________________________________________________________________
dropout_77 (Dropout)            (None, 24)           0           dense_153[0][0]                  
__________________________________________________________________________________________________
dense_154 (Dense)               (None, 8)            200         dropout_77[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0679 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0446 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0425 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0415 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0410 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0407 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0410 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0405 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.0115e-04 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1053e-06 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.7388e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.3577e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.6864e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 2.6485e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.3492e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.2982e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.6029e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.2382e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.2820e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 3.7563e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1950e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.4469e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1946e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.2042e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1888e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1743e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1588e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1414e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1244e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.2005e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.0800e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.0639e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0492e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.0339e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:25.544197
# F-Score(Ordinary) = 0.609, Recall: 0.561, Precision: 0.664
# F-Score(lvc) = 0.584, Recall: 0.736, Precision: 0.485
# F-Score(ireflv) = 0.743, Recall: 0.645, Precision: 0.877
# F-Score(id) = 0.512, Recall: 0.435, Precision: 0.622
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_155 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_156 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_155 (Embedding)       (None, 4, 48)        705264      input_155[0][0]                  
__________________________________________________________________________________________________
embedding_156 (Embedding)       (None, 4, 24)        5640        input_156[0][0]                  
__________________________________________________________________________________________________
flatten_155 (Flatten)           (None, 192)          0           embedding_155[0][0]              
__________________________________________________________________________________________________
flatten_156 (Flatten)           (None, 96)           0           embedding_156[0][0]              
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 288)          0           flatten_155[0][0]                
                                                                 flatten_156[0][0]                
__________________________________________________________________________________________________
dense_155 (Dense)               (None, 24)           6936        concatenate_78[0][0]             
__________________________________________________________________________________________________
dropout_78 (Dropout)            (None, 24)           0           dense_155[0][0]                  
__________________________________________________________________________________________________
dense_156 (Dense)               (None, 8)            200         dropout_78[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0684 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0446 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0424 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0415 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0394 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0394 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0410 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0407 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0408 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0424 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0414 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.1135e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 4.7659e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 8.9533e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 3.3258e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 3.1810e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 4.2098e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.8772e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.4490e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.3712e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.3869e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.4806e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.2268e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.2144e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.2019e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1955e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.2134e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1969e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1942e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1928e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1924e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1929e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.6444e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.6270e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:13.492848
# F-Score(Ordinary) = 0.683, Recall: 0.727, Precision: 0.644
# F-Score(lvc) = 0.614, Recall: 0.886, Precision: 0.47
# F-Score(ireflv) = 0.818, Recall: 0.808, Precision: 0.828
# F-Score(id) = 0.614, Recall: 0.602, Precision: 0.627
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_157 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_158 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_157 (Embedding)       (None, 4, 48)        705264      input_157[0][0]                  
__________________________________________________________________________________________________
embedding_158 (Embedding)       (None, 4, 24)        5640        input_158[0][0]                  
__________________________________________________________________________________________________
flatten_157 (Flatten)           (None, 192)          0           embedding_157[0][0]              
__________________________________________________________________________________________________
flatten_158 (Flatten)           (None, 96)           0           embedding_158[0][0]              
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 288)          0           flatten_157[0][0]                
                                                                 flatten_158[0][0]                
__________________________________________________________________________________________________
dense_157 (Dense)               (None, 24)           6936        concatenate_79[0][0]             
__________________________________________________________________________________________________
dropout_79 (Dropout)            (None, 24)           0           dense_157[0][0]                  
__________________________________________________________________________________________________
dense_158 (Dense)               (None, 8)            200         dropout_79[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0681 - acc: 0.9848 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0448 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0428 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0419 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0414 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0404 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0407 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0403 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0404 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0402 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0413 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0415 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0408 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0410 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.7155e-05 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 5.2133e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.8987e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.8003e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.6962e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.9324e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.6216e-04 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.8166e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.3917e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.2452e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.2694e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.2844e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.2344e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.6086e-04 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1939e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1928e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.2010e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.5912e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1954e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1930e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.5740e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.5567e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1924e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.5412e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.5238e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.5065e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.4913e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.4760e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.4610e-04 - acc: 1.0000
# Training time = 0:12:16.444629
# F-Score(Ordinary) = 0.569, Recall: 0.483, Precision: 0.694
# F-Score(lvc) = 0.59, Recall: 0.753, Precision: 0.485
# F-Score(ireflv) = 0.759, Recall: 0.669, Precision: 0.877
# F-Score(id) = 0.434, Recall: 0.322, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_159 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_160 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_159 (Embedding)       (None, 4, 48)        705264      input_159[0][0]                  
__________________________________________________________________________________________________
embedding_160 (Embedding)       (None, 4, 24)        5640        input_160[0][0]                  
__________________________________________________________________________________________________
flatten_159 (Flatten)           (None, 192)          0           embedding_159[0][0]              
__________________________________________________________________________________________________
flatten_160 (Flatten)           (None, 96)           0           embedding_160[0][0]              
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 288)          0           flatten_159[0][0]                
                                                                 flatten_160[0][0]                
__________________________________________________________________________________________________
dense_159 (Dense)               (None, 24)           6936        concatenate_80[0][0]             
__________________________________________________________________________________________________
dropout_80 (Dropout)            (None, 24)           0           dense_159[0][0]                  
__________________________________________________________________________________________________
dense_160 (Dense)               (None, 8)            200         dropout_80[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0686 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0450 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0428 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0419 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0412 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0411 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0410 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0409 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0394 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0412 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0409 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0403 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0408 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0415 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0416 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0406 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0424 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0408 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0403 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0416 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0413 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0424 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0414 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0410 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 9.4424e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 8.0079e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 5.5928e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 6.5439e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 4.2473e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 4.1405e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 4.0117e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 4.8264e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 6.4385e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 3.5247e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 3.3992e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 4.0621e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 2.3427e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 3.7321e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 4.2107e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 5.8355e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 1.8300e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1811e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.6982e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 2.6425e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.4834e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 9.4466e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.7739e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 4.2154e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.5812e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 2.7669e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.2283e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 1.1215e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 1.2491e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 1.6879e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 5.4885e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 8.0900e-05 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 6.8313e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.4190e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 3.2776e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 6.1138e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 4.0643e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.2253e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.3310e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.5539e-05 - acc: 1.0000
# Training time = 0:12:15.276613
# F-Score(Ordinary) = 0.578, Recall: 0.502, Precision: 0.68
# F-Score(lvc) = 0.577, Recall: 0.747, Precision: 0.47
# F-Score(ireflv) = 0.762, Recall: 0.665, Precision: 0.893
# F-Score(id) = 0.439, Recall: 0.338, Precision: 0.627
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_161 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_162 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_161 (Embedding)       (None, 4, 48)        705264      input_161[0][0]                  
__________________________________________________________________________________________________
embedding_162 (Embedding)       (None, 4, 24)        5640        input_162[0][0]                  
__________________________________________________________________________________________________
flatten_161 (Flatten)           (None, 192)          0           embedding_161[0][0]              
__________________________________________________________________________________________________
flatten_162 (Flatten)           (None, 96)           0           embedding_162[0][0]              
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 288)          0           flatten_161[0][0]                
                                                                 flatten_162[0][0]                
__________________________________________________________________________________________________
dense_161 (Dense)               (None, 24)           6936        concatenate_81[0][0]             
__________________________________________________________________________________________________
dropout_81 (Dropout)            (None, 24)           0           dense_161[0][0]                  
__________________________________________________________________________________________________
dense_162 (Dense)               (None, 8)            200         dropout_81[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.0632 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0484 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0485 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0478 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0478 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0478 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0472 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0483 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0481 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0492 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0493 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0512 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0504 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0499 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0492 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0498 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0515 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0503 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0506 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0497 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0504 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0507 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0498 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0510 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0508 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0504 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0515 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0526 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0526 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0556 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0561 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0555 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0560 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0551 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0552 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0568 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0570 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.1477 - acc: 0.9825 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0629 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0631 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0050 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 6.1880e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.6091e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.2817e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 6.9063e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 3.8033e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.1237e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.2020e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 6.8897e-06 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 3.9165e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.2438e-06 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.3275e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 8.0463e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 4.9760e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 3.2607e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.2739e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.6890e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.4066e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.2109e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:41.593785
# F-Score(Ordinary) = 0.409, Recall: 0.298, Precision: 0.651
# F-Score(lvc) = 0.408, Recall: 0.365, Precision: 0.462
# F-Score(ireflv) = 0.667, Recall: 0.554, Precision: 0.836
# F-Score(id) = 0.301, Recall: 0.197, Precision: 0.637
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_163 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_164 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_163 (Embedding)       (None, 4, 48)        705264      input_163[0][0]                  
__________________________________________________________________________________________________
embedding_164 (Embedding)       (None, 4, 24)        5640        input_164[0][0]                  
__________________________________________________________________________________________________
flatten_163 (Flatten)           (None, 192)          0           embedding_163[0][0]              
__________________________________________________________________________________________________
flatten_164 (Flatten)           (None, 96)           0           embedding_164[0][0]              
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 288)          0           flatten_163[0][0]                
                                                                 flatten_164[0][0]                
__________________________________________________________________________________________________
dense_163 (Dense)               (None, 24)           6936        concatenate_82[0][0]             
__________________________________________________________________________________________________
dropout_82 (Dropout)            (None, 24)           0           dense_163[0][0]                  
__________________________________________________________________________________________________
dense_164 (Dense)               (None, 8)            200         dropout_82[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0634 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0487 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0473 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0475 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0487 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0476 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0474 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0471 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0474 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0473 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0483 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0486 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0482 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0475 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0479 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0475 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0475 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0499 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0482 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0503 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0506 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0502 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0503 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0501 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0503 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0498 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0518 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0513 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0519 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0541 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0522 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0524 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0525 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0527 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0529 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0538 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0537 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0528 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0533 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0117 - acc: 0.9981
Epoch 2/40
 - 2s - loss: 0.0048 - acc: 0.9986
Epoch 3/40
 - 2s - loss: 0.0022 - acc: 0.9993
Epoch 4/40
 - 2s - loss: 5.2513e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.3190e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 9.8810e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 4.2706e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.4551e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.5706e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 7.6097e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 4.8242e-06 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 2.9064e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.2444e-06 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1137e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 6.3112e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 4.2915e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 3.1680e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.3572e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.7677e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.5616e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.4294e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.3447e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.2951e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.2592e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.2480e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.2298e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.2240e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.2214e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.2144e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.2109e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.2089e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.2069e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.2058e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.2038e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.2024e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.2010e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.2016e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1992e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1987e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1982e-07 - acc: 1.0000
# Training time = 0:12:14.509247
# F-Score(Ordinary) = 0.444, Recall: 0.462, Precision: 0.427
# F-Score(lvc) = 0.437, Recall: 0.408, Precision: 0.47
# F-Score(ireflv) = 0.633, Recall: 0.498, Precision: 0.869
# F-Score(id) = 0.147, Recall: 0.447, Precision: 0.088
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_165 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_166 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_165 (Embedding)       (None, 4, 48)        705264      input_165[0][0]                  
__________________________________________________________________________________________________
embedding_166 (Embedding)       (None, 4, 24)        5640        input_166[0][0]                  
__________________________________________________________________________________________________
flatten_165 (Flatten)           (None, 192)          0           embedding_165[0][0]              
__________________________________________________________________________________________________
flatten_166 (Flatten)           (None, 96)           0           embedding_166[0][0]              
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 288)          0           flatten_165[0][0]                
                                                                 flatten_166[0][0]                
__________________________________________________________________________________________________
dense_165 (Dense)               (None, 24)           6936        concatenate_83[0][0]             
__________________________________________________________________________________________________
dropout_83 (Dropout)            (None, 24)           0           dense_165[0][0]                  
__________________________________________________________________________________________________
dense_166 (Dense)               (None, 8)            200         dropout_83[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0639 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0489 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0475 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0466 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0464 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0479 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0475 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0472 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0469 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0462 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0472 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0477 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0483 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0485 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0490 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0491 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0516 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0512 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0498 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0503 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0495 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0505 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0499 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0517 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0520 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0516 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0524 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0529 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0522 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0537 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0532 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0520 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0528 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0538 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0550 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0547 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0541 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0552 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0559 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0610 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 5.5920e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1927e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1927e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1933e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1927e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1926e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1922e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
# Training time = 0:12:34.175429
# F-Score(Ordinary) = 0.508, Recall: 0.502, Precision: 0.515
# F-Score(lvc) = 0.476, Recall: 0.83, Precision: 0.333
# F-Score(ireflv) = 0.771, Recall: 0.851, Precision: 0.705
# F-Score(id) = 0.362, Recall: 0.296, Precision: 0.466
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_167 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_168 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_167 (Embedding)       (None, 4, 48)        705264      input_167[0][0]                  
__________________________________________________________________________________________________
embedding_168 (Embedding)       (None, 4, 24)        5640        input_168[0][0]                  
__________________________________________________________________________________________________
flatten_167 (Flatten)           (None, 192)          0           embedding_167[0][0]              
__________________________________________________________________________________________________
flatten_168 (Flatten)           (None, 96)           0           embedding_168[0][0]              
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 288)          0           flatten_167[0][0]                
                                                                 flatten_168[0][0]                
__________________________________________________________________________________________________
dense_167 (Dense)               (None, 24)           6936        concatenate_84[0][0]             
__________________________________________________________________________________________________
dropout_84 (Dropout)            (None, 24)           0           dense_167[0][0]                  
__________________________________________________________________________________________________
dense_168 (Dense)               (None, 8)            200         dropout_84[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.0633 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0492 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0487 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0480 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0475 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0472 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0470 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0470 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0482 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0474 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0475 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0500 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0479 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0497 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0495 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0480 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0499 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0510 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0514 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0506 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0503 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0498 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0501 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0494 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0522 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0525 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0496 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0503 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0502 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0511 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0511 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0520 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0517 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0527 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0512 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0523 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0526 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0556 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0535 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 0.0013 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 6.6380e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 0.0015 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 0.0013 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 0.0013 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 0.0020 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 6.6380e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 0.0018 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 0.0015 - acc: 0.9999
Epoch 23/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 24/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 6.6380e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 0.0020 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 28/40
 - 2s - loss: 2.2135e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 4.4257e-04 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 0.0022 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 6.6380e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 8.8502e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 6.6380e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 0.0013 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 0.0018 - acc: 0.9999
Epoch 38/40
 - 2s - loss: 0.0011 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 0.0013 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 0.0011 - acc: 0.9999
# Training time = 0:12:37.449167
# F-Score(Ordinary) = 0.313, Recall: 0.208, Precision: 0.633
# F-Score(lvc) = 0.23, Recall: 0.146, Precision: 0.545
# F-Score(ireflv) = 0.566, Recall: 0.439, Precision: 0.795
# F-Score(id) = 0.252, Recall: 0.164, Precision: 0.549
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_169 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_170 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_169 (Embedding)       (None, 4, 48)        705264      input_169[0][0]                  
__________________________________________________________________________________________________
embedding_170 (Embedding)       (None, 4, 24)        5640        input_170[0][0]                  
__________________________________________________________________________________________________
flatten_169 (Flatten)           (None, 192)          0           embedding_169[0][0]              
__________________________________________________________________________________________________
flatten_170 (Flatten)           (None, 96)           0           embedding_170[0][0]              
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 288)          0           flatten_169[0][0]                
                                                                 flatten_170[0][0]                
__________________________________________________________________________________________________
dense_169 (Dense)               (None, 24)           6936        concatenate_85[0][0]             
__________________________________________________________________________________________________
dropout_85 (Dropout)            (None, 24)           0           dense_169[0][0]                  
__________________________________________________________________________________________________
dense_170 (Dense)               (None, 8)            200         dropout_85[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.0645 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0490 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0480 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0471 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0478 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0476 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0472 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0475 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0477 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0482 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0480 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0478 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0484 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0497 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.0488 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.0481 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.0513 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 17s - loss: 0.0494 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 17s - loss: 0.0506 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.0498 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 17s - loss: 0.0511 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 17s - loss: 0.0501 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.0496 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 17s - loss: 0.0495 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 17s - loss: 0.0510 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 17s - loss: 0.0499 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 17s - loss: 0.0503 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 17s - loss: 0.0517 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 17s - loss: 0.0509 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 17s - loss: 0.0503 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 17s - loss: 0.0535 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 17s - loss: 0.0508 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 17s - loss: 0.0515 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 17s - loss: 0.0518 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 17s - loss: 0.0525 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 17s - loss: 0.0510 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 17s - loss: 0.0561 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 17s - loss: 0.0569 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 17s - loss: 0.0549 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 17s - loss: 0.0522 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0051 - acc: 0.9997
Epoch 2/40
 - 2s - loss: 0.0069 - acc: 0.9996
Epoch 3/40
 - 2s - loss: 0.0049 - acc: 0.9997
Epoch 4/40
 - 2s - loss: 0.0049 - acc: 0.9997
Epoch 5/40
 - 2s - loss: 0.0060 - acc: 0.9996
Epoch 6/40
 - 2s - loss: 0.0042 - acc: 0.9997
Epoch 7/40
 - 2s - loss: 0.0044 - acc: 0.9997
Epoch 8/40
 - 2s - loss: 0.0053 - acc: 0.9997
Epoch 9/40
 - 2s - loss: 0.0071 - acc: 0.9996
Epoch 10/40
 - 2s - loss: 0.0038 - acc: 0.9998
Epoch 11/40
 - 2s - loss: 0.0055 - acc: 0.9997
Epoch 12/40
 - 2s - loss: 0.0042 - acc: 0.9997
Epoch 13/40
 - 2s - loss: 0.0040 - acc: 0.9998
Epoch 14/40
 - 2s - loss: 0.0055 - acc: 0.9997
Epoch 15/40
 - 2s - loss: 0.0044 - acc: 0.9997
Epoch 16/40
 - 2s - loss: 0.0064 - acc: 0.9996
Epoch 17/40
 - 2s - loss: 0.0055 - acc: 0.9997
Epoch 18/40
 - 2s - loss: 0.0042 - acc: 0.9997
Epoch 19/40
 - 2s - loss: 0.0042 - acc: 0.9997
Epoch 20/40
 - 2s - loss: 0.0060 - acc: 0.9996
Epoch 21/40
 - 2s - loss: 0.0042 - acc: 0.9997
Epoch 22/40
 - 2s - loss: 0.0042 - acc: 0.9997
Epoch 23/40
 - 2s - loss: 0.0051 - acc: 0.9997
Epoch 24/40
 - 2s - loss: 0.0066 - acc: 0.9996
Epoch 25/40
 - 2s - loss: 0.0051 - acc: 0.9997
Epoch 26/40
 - 2s - loss: 0.0066 - acc: 0.9996
Epoch 27/40
 - 2s - loss: 0.0049 - acc: 0.9997
Epoch 28/40
 - 2s - loss: 0.0040 - acc: 0.9998
Epoch 29/40
 - 2s - loss: 0.0031 - acc: 0.9998
Epoch 30/40
 - 2s - loss: 0.0066 - acc: 0.9996
Epoch 31/40
 - 2s - loss: 0.0051 - acc: 0.9997
Epoch 32/40
 - 2s - loss: 0.0040 - acc: 0.9998
Epoch 33/40
 - 2s - loss: 0.0066 - acc: 0.9996
Epoch 34/40
 - 2s - loss: 0.0051 - acc: 0.9997
Epoch 35/40
 - 2s - loss: 0.0049 - acc: 0.9997
Epoch 36/40
 - 2s - loss: 0.0062 - acc: 0.9996
Epoch 37/40
 - 2s - loss: 0.0053 - acc: 0.9997
Epoch 38/40
 - 2s - loss: 0.0033 - acc: 0.9998
Epoch 39/40
 - 2s - loss: 0.0051 - acc: 0.9997
Epoch 40/40
 - 2s - loss: 0.0046 - acc: 0.9997
# Training time = 0:12:49.846670
# F-Score(Ordinary) = 0.508, Recall: 0.392, Precision: 0.718
# F-Score(lvc) = 0.547, Recall: 0.627, Precision: 0.485
# F-Score(ireflv) = 0.679, Recall: 0.548, Precision: 0.893
# F-Score(id) = 0.363, Recall: 0.25, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_171 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_172 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_171 (Embedding)       (None, 4, 48)        705264      input_171[0][0]                  
__________________________________________________________________________________________________
embedding_172 (Embedding)       (None, 4, 24)        5640        input_172[0][0]                  
__________________________________________________________________________________________________
flatten_171 (Flatten)           (None, 192)          0           embedding_171[0][0]              
__________________________________________________________________________________________________
flatten_172 (Flatten)           (None, 96)           0           embedding_172[0][0]              
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 288)          0           flatten_171[0][0]                
                                                                 flatten_172[0][0]                
__________________________________________________________________________________________________
dense_171 (Dense)               (None, 24)           6936        concatenate_86[0][0]             
__________________________________________________________________________________________________
dropout_86 (Dropout)            (None, 24)           0           dense_171[0][0]                  
__________________________________________________________________________________________________
dense_172 (Dense)               (None, 8)            200         dropout_86[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 17s - loss: 0.0724 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 17s - loss: 0.0629 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 17s - loss: 0.0689 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 17s - loss: 0.0703 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 17s - loss: 0.0748 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 17s - loss: 0.0757 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 17s - loss: 0.0793 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 17s - loss: 0.0874 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 17s - loss: 0.0934 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 17s - loss: 0.0970 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 17s - loss: 0.0963 - acc: 0.9826 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 17s - loss: 0.0992 - acc: 0.9823 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 17s - loss: 0.0968 - acc: 0.9818 - val_loss: 2.2292e-05 - val_acc: 1.0000
Epoch 14/40
 - 17s - loss: 0.0990 - acc: 0.9814 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 17s - loss: 0.1048 - acc: 0.9813 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 17s - loss: 0.1089 - acc: 0.9818 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 17s - loss: 0.1482 - acc: 0.9801 - val_loss: 0.1688 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1699 - acc: 0.9797 - val_loss: 0.0944 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1552 - acc: 0.9775 - val_loss: 0.2332 - val_acc: 1.0000
Epoch 20/40
 - 17s - loss: 0.1554 - acc: 0.9761 - val_loss: 0.2233 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1591 - acc: 0.9766 - val_loss: 0.2394 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1770 - acc: 0.9747 - val_loss: 0.2525 - val_acc: 1.0000
Epoch 23/40
 - 17s - loss: 0.1634 - acc: 0.9754 - val_loss: 0.2764 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1994 - acc: 0.9730 - val_loss: 0.2776 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1643 - acc: 0.9746 - val_loss: 0.2068 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1921 - acc: 0.9730 - val_loss: 0.2205 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1823 - acc: 0.9756 - val_loss: 0.2150 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1788 - acc: 0.9730 - val_loss: 0.2449 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.2521 - acc: 0.9693 - val_loss: 0.3026 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1974 - acc: 0.9715 - val_loss: 0.2840 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1898 - acc: 0.9720 - val_loss: 0.3876 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.2076 - acc: 0.9651 - val_loss: 0.3999 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.2205 - acc: 0.9685 - val_loss: 0.2746 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.2254 - acc: 0.9681 - val_loss: 0.4268 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.2345 - acc: 0.9664 - val_loss: 0.4101 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.2776 - acc: 0.9638 - val_loss: 0.4203 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.2430 - acc: 0.9544 - val_loss: 0.2876 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.2586 - acc: 0.9536 - val_loss: 0.7122 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.2760 - acc: 0.9320 - val_loss: 0.7833 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.3141 - acc: 0.9284 - val_loss: 0.9183 - val_acc: 0.0000e+00
Epoch 1/40
 - 2s - loss: 0.0197 - acc: 0.9974
Epoch 2/40
 - 2s - loss: 0.0014 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 5.9379e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 2.9403e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.5551e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 8.4884e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 4.7096e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.6359e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.4824e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 8.3582e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 4.7189e-06 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 2.6693e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.5023e-06 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 8.4354e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 4.8334e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.5246e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:32.628864
# F-Score(Ordinary) = 0.344, Recall: 0.24, Precision: 0.606
# F-Score(lvc) = 0.332, Recall: 0.317, Precision: 0.348
# F-Score(ireflv) = 0.662, Recall: 0.552, Precision: 0.828
# F-Score(id) = 0.196, Recall: 0.132, Precision: 0.378
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_173 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_174 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_173 (Embedding)       (None, 4, 48)        705264      input_173[0][0]                  
__________________________________________________________________________________________________
embedding_174 (Embedding)       (None, 4, 24)        5640        input_174[0][0]                  
__________________________________________________________________________________________________
flatten_173 (Flatten)           (None, 192)          0           embedding_173[0][0]              
__________________________________________________________________________________________________
flatten_174 (Flatten)           (None, 96)           0           embedding_174[0][0]              
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 288)          0           flatten_173[0][0]                
                                                                 flatten_174[0][0]                
__________________________________________________________________________________________________
dense_173 (Dense)               (None, 24)           6936        concatenate_87[0][0]             
__________________________________________________________________________________________________
dropout_87 (Dropout)            (None, 24)           0           dense_173[0][0]                  
__________________________________________________________________________________________________
dense_174 (Dense)               (None, 8)            200         dropout_87[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0723 - acc: 0.9837 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0637 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0646 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0672 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0713 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0731 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0771 - acc: 0.9844 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0815 - acc: 0.9844 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0851 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0912 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0909 - acc: 0.9822 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0948 - acc: 0.9809 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.1086 - acc: 0.9811 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.2184 - acc: 0.9741 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.1400 - acc: 0.9755 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1148 - acc: 0.9756 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1113 - acc: 0.9766 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1157 - acc: 0.9763 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1272 - acc: 0.9754 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1278 - acc: 0.9749 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1220 - acc: 0.9748 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1271 - acc: 0.9754 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1243 - acc: 0.9741 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1305 - acc: 0.9739 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1333 - acc: 0.9743 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1374 - acc: 0.9747 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1391 - acc: 0.9730 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1430 - acc: 0.9734 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1596 - acc: 0.9723 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1467 - acc: 0.9725 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1617 - acc: 0.9706 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1574 - acc: 0.9711 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1776 - acc: 0.9712 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1725 - acc: 0.9704 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1810 - acc: 0.9723 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1930 - acc: 0.9682 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1929 - acc: 0.9683 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.2239 - acc: 0.9700 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.2280 - acc: 0.9726 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.2414 - acc: 0.9711 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0170 - acc: 0.9947
Epoch 2/40
 - 2s - loss: 8.2152e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 3.4378e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.6890e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 8.9300e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 4.8943e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.7256e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.5259e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 8.6653e-06 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 4.9182e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.8100e-06 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.6392e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 9.7418e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 5.8777e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 3.7455e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.5487e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.8406e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.4724e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.2869e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:08.426106
# F-Score(Ordinary) = 0.412, Recall: 0.295, Precision: 0.687
# F-Score(lvc) = 0.31, Recall: 0.201, Precision: 0.674
# F-Score(id) = 0.357, Recall: 0.267, Precision: 0.539
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_175 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_176 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_175 (Embedding)       (None, 4, 48)        705264      input_175[0][0]                  
__________________________________________________________________________________________________
embedding_176 (Embedding)       (None, 4, 24)        5640        input_176[0][0]                  
__________________________________________________________________________________________________
flatten_175 (Flatten)           (None, 192)          0           embedding_175[0][0]              
__________________________________________________________________________________________________
flatten_176 (Flatten)           (None, 96)           0           embedding_176[0][0]              
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 288)          0           flatten_175[0][0]                
                                                                 flatten_176[0][0]                
__________________________________________________________________________________________________
dense_175 (Dense)               (None, 24)           6936        concatenate_88[0][0]             
__________________________________________________________________________________________________
dropout_88 (Dropout)            (None, 24)           0           dense_175[0][0]                  
__________________________________________________________________________________________________
dense_176 (Dense)               (None, 8)            200         dropout_88[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0710 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0642 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0650 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0671 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0687 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0718 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0755 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0770 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0824 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0890 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0948 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0914 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0933 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0946 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0987 - acc: 0.9842 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1071 - acc: 0.9790 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1079 - acc: 0.9785 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1123 - acc: 0.9786 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1121 - acc: 0.9786 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1098 - acc: 0.9787 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1122 - acc: 0.9789 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1112 - acc: 0.9787 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1153 - acc: 0.9785 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1311 - acc: 0.9777 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1316 - acc: 0.9777 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1218 - acc: 0.9781 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1215 - acc: 0.9781 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1263 - acc: 0.9775 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1236 - acc: 0.9778 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1305 - acc: 0.9743 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1381 - acc: 0.9726 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1393 - acc: 0.9712 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1400 - acc: 0.9721 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1478 - acc: 0.9728 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1442 - acc: 0.9724 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1460 - acc: 0.9724 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1494 - acc: 0.9722 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1695 - acc: 0.9719 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1691 - acc: 0.9717 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1858 - acc: 0.9714 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.2398 - acc: 0.9851
Epoch 2/40
 - 2s - loss: 0.2661 - acc: 0.9835
Epoch 3/40
 - 2s - loss: 0.2491 - acc: 0.9845
Epoch 4/40
 - 2s - loss: 0.2668 - acc: 0.9834
Epoch 5/40
 - 2s - loss: 0.2462 - acc: 0.9847
Epoch 6/40
 - 2s - loss: 0.2462 - acc: 0.9847
Epoch 7/40
 - 2s - loss: 0.2582 - acc: 0.9840
Epoch 8/40
 - 2s - loss: 0.2513 - acc: 0.9844
Epoch 9/40
 - 2s - loss: 0.2458 - acc: 0.9848
Epoch 10/40
 - 2s - loss: 0.2438 - acc: 0.9849
Epoch 11/40
 - 2s - loss: 0.2509 - acc: 0.9844
Epoch 12/40
 - 2s - loss: 0.2520 - acc: 0.9844
Epoch 13/40
 - 2s - loss: 0.2568 - acc: 0.9841
Epoch 14/40
 - 2s - loss: 0.2498 - acc: 0.9845
Epoch 15/40
 - 2s - loss: 0.2531 - acc: 0.9843
Epoch 16/40
 - 2s - loss: 0.2504 - acc: 0.9845
Epoch 17/40
 - 2s - loss: 0.2544 - acc: 0.9842
Epoch 18/40
 - 2s - loss: 0.2549 - acc: 0.9842
Epoch 19/40
 - 2s - loss: 0.2699 - acc: 0.9833
Epoch 20/40
 - 2s - loss: 0.2560 - acc: 0.9841
Epoch 21/40
 - 2s - loss: 0.2518 - acc: 0.9844
Epoch 22/40
 - 2s - loss: 0.2391 - acc: 0.9852
Epoch 23/40
 - 2s - loss: 0.2460 - acc: 0.9847
Epoch 24/40
 - 2s - loss: 0.2659 - acc: 0.9835
Epoch 25/40
 - 2s - loss: 0.2683 - acc: 0.9834
Epoch 26/40
 - 2s - loss: 0.2575 - acc: 0.9840
Epoch 27/40
 - 2s - loss: 0.2345 - acc: 0.9855
Epoch 28/40
 - 2s - loss: 0.2597 - acc: 0.9839
Epoch 29/40
 - 2s - loss: 0.2506 - acc: 0.9844
Epoch 30/40
 - 2s - loss: 0.2551 - acc: 0.9842
Epoch 31/40
 - 2s - loss: 0.2695 - acc: 0.9833
Epoch 32/40
 - 2s - loss: 0.2626 - acc: 0.9837
Epoch 33/40
 - 2s - loss: 0.2566 - acc: 0.9841
Epoch 34/40
 - 2s - loss: 0.2579 - acc: 0.9840
Epoch 35/40
 - 2s - loss: 0.2564 - acc: 0.9841
Epoch 36/40
 - 2s - loss: 0.2460 - acc: 0.9847
Epoch 37/40
 - 2s - loss: 0.2433 - acc: 0.9849
Epoch 38/40
 - 2s - loss: 0.2553 - acc: 0.9842
Epoch 39/40
 - 2s - loss: 0.2296 - acc: 0.9858
Epoch 40/40
 - 2s - loss: 0.2469 - acc: 0.9847
# Training time = 0:12:14.533880
# F-Score(Ordinary) = 0.534, Recall: 0.478, Precision: 0.604
# F-Score(lvc) = 0.469, Recall: 0.953, Precision: 0.311
# F-Score(ireflv) = 0.731, Recall: 0.664, Precision: 0.811
# F-Score(id) = 0.431, Recall: 0.327, Precision: 0.632
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_177 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_178 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_177 (Embedding)       (None, 4, 48)        705264      input_177[0][0]                  
__________________________________________________________________________________________________
embedding_178 (Embedding)       (None, 4, 24)        5640        input_178[0][0]                  
__________________________________________________________________________________________________
flatten_177 (Flatten)           (None, 192)          0           embedding_177[0][0]              
__________________________________________________________________________________________________
flatten_178 (Flatten)           (None, 96)           0           embedding_178[0][0]              
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 288)          0           flatten_177[0][0]                
                                                                 flatten_178[0][0]                
__________________________________________________________________________________________________
dense_177 (Dense)               (None, 24)           6936        concatenate_89[0][0]             
__________________________________________________________________________________________________
dropout_89 (Dropout)            (None, 24)           0           dense_177[0][0]                  
__________________________________________________________________________________________________
dense_178 (Dense)               (None, 8)            200         dropout_89[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0713 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0637 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0660 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0688 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0732 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0788 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0862 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0937 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0906 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0901 - acc: 0.9825 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0968 - acc: 0.9822 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0953 - acc: 0.9816 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.1109 - acc: 0.9806 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.1028 - acc: 0.9816 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.1073 - acc: 0.9814 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1062 - acc: 0.9816 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1209 - acc: 0.9805 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1261 - acc: 0.9790 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1239 - acc: 0.9794 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1226 - acc: 0.9803 - val_loss: 1.5617e-05 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1421 - acc: 0.9789 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1352 - acc: 0.9805 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1334 - acc: 0.9779 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1270 - acc: 0.9749 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1823 - acc: 0.9713 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1384 - acc: 0.9732 - val_loss: 5.1260e-06 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1474 - acc: 0.9727 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1556 - acc: 0.9724 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.3441 - acc: 0.9601 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 30/40
 - 16s - loss: 0.7683 - acc: 0.9341 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 31/40
 - 16s - loss: 0.8390 - acc: 0.9310 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 32/40
 - 16s - loss: 0.7757 - acc: 0.9350 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 33/40
 - 16s - loss: 0.7915 - acc: 0.9333 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 34/40
 - 16s - loss: 0.8051 - acc: 0.9347 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 35/40
 - 16s - loss: 0.8189 - acc: 0.9320 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 36/40
 - 16s - loss: 0.8170 - acc: 0.9353 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 37/40
 - 16s - loss: 0.8187 - acc: 0.9370 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 38/40
 - 16s - loss: 0.8131 - acc: 0.9372 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 39/40
 - 16s - loss: 0.8195 - acc: 0.9348 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 40/40
 - 16s - loss: 0.8293 - acc: 0.9341 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 1/40
 - 2s - loss: 12.8953 - acc: 0.1956
Epoch 2/40
 - 2s - loss: 12.8751 - acc: 0.2013
Epoch 3/40
 - 2s - loss: 12.8700 - acc: 0.2015
Epoch 4/40
 - 2s - loss: 12.9016 - acc: 0.1996
Epoch 5/40
 - 2s - loss: 12.9361 - acc: 0.1974
Epoch 6/40
 - 2s - loss: 12.8871 - acc: 0.2005
Epoch 7/40
 - 2s - loss: 12.9134 - acc: 0.1988
Epoch 8/40
 - 2s - loss: 12.8610 - acc: 0.2021
Epoch 9/40
 - 2s - loss: 12.9143 - acc: 0.1988
Epoch 10/40
 - 2s - loss: 12.8970 - acc: 0.1998
Epoch 11/40
 - 2s - loss: 12.9218 - acc: 0.1983
Epoch 12/40
 - 2s - loss: 12.8933 - acc: 0.2001
Epoch 13/40
 - 2s - loss: 12.9083 - acc: 0.1991
Epoch 14/40
 - 2s - loss: 12.9271 - acc: 0.1980
Epoch 15/40
 - 2s - loss: 12.8765 - acc: 0.2011
Epoch 16/40
 - 2s - loss: 12.8608 - acc: 0.2021
Epoch 17/40
 - 2s - loss: 12.9010 - acc: 0.1996
Epoch 18/40
 - 2s - loss: 12.9130 - acc: 0.1989
Epoch 19/40
 - 2s - loss: 12.8880 - acc: 0.2004
Epoch 20/40
 - 2s - loss: 12.8902 - acc: 0.2003
Epoch 21/40
 - 2s - loss: 12.8647 - acc: 0.2018
Epoch 22/40
 - 2s - loss: 12.9127 - acc: 0.1989
Epoch 23/40
 - 2s - loss: 12.8331 - acc: 0.2038
Epoch 24/40
 - 2s - loss: 12.9212 - acc: 0.1983
Epoch 25/40
 - 2s - loss: 12.9136 - acc: 0.1988
Epoch 26/40
 - 2s - loss: 12.8913 - acc: 0.2002
Epoch 27/40
 - 2s - loss: 12.9535 - acc: 0.1963
Epoch 28/40
 - 2s - loss: 12.8632 - acc: 0.2019
Epoch 29/40
 - 2s - loss: 12.9205 - acc: 0.1984
Epoch 30/40
 - 2s - loss: 12.9274 - acc: 0.1980
Epoch 31/40
 - 2s - loss: 12.9001 - acc: 0.1996
Epoch 32/40
 - 2s - loss: 12.8751 - acc: 0.2012
Epoch 33/40
 - 2s - loss: 12.9189 - acc: 0.1985
Epoch 34/40
 - 2s - loss: 12.8900 - acc: 0.2003
Epoch 35/40
 - 2s - loss: 12.8769 - acc: 0.2011
Epoch 36/40
 - 2s - loss: 12.8780 - acc: 0.2010
Epoch 37/40
 - 2s - loss: 12.8926 - acc: 0.2001
Epoch 38/40
 - 2s - loss: 12.9262 - acc: 0.1980
Epoch 39/40
 - 2s - loss: 12.8546 - acc: 0.2025
Epoch 40/40
 - 2s - loss: 12.8878 - acc: 0.2004
# Training time = 0:12:19.534626
# F-Score(Ordinary) = 0.42, Recall: 0.305, Precision: 0.678
# F-Score(lvc) = 0.387, Recall: 0.311, Precision: 0.515
# F-Score(id) = 0.401, Recall: 0.29, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_179 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_180 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_179 (Embedding)       (None, 4, 48)        705264      input_179[0][0]                  
__________________________________________________________________________________________________
embedding_180 (Embedding)       (None, 4, 24)        5640        input_180[0][0]                  
__________________________________________________________________________________________________
flatten_179 (Flatten)           (None, 192)          0           embedding_179[0][0]              
__________________________________________________________________________________________________
flatten_180 (Flatten)           (None, 96)           0           embedding_180[0][0]              
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 288)          0           flatten_179[0][0]                
                                                                 flatten_180[0][0]                
__________________________________________________________________________________________________
dense_179 (Dense)               (None, 24)           6936        concatenate_90[0][0]             
__________________________________________________________________________________________________
dropout_90 (Dropout)            (None, 24)           0           dense_179[0][0]                  
__________________________________________________________________________________________________
dense_180 (Dense)               (None, 8)            200         dropout_90[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = nadam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0713 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0649 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0691 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0693 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0698 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0721 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0768 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0854 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0860 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0885 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0931 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0965 - acc: 0.9821 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0937 - acc: 0.9830 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0987 - acc: 0.9823 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0976 - acc: 0.9816 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1050 - acc: 0.9815 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1216 - acc: 0.9801 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1076 - acc: 0.9818 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1122 - acc: 0.9810 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1103 - acc: 0.9813 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1121 - acc: 0.9803 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1201 - acc: 0.9789 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1142 - acc: 0.9805 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1239 - acc: 0.9800 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1210 - acc: 0.9781 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1339 - acc: 0.9804 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1370 - acc: 0.9778 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1329 - acc: 0.9789 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1448 - acc: 0.9795 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1460 - acc: 0.9776 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1535 - acc: 0.9780 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1444 - acc: 0.9798 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1440 - acc: 0.9765 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1431 - acc: 0.9755 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1662 - acc: 0.9701 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1566 - acc: 0.9708 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1610 - acc: 0.9714 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1565 - acc: 0.9708 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1635 - acc: 0.9703 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1713 - acc: 0.9693 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 0.0073 - acc: 0.9993
Epoch 2/40
 - 2s - loss: 5.5024e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.3053e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1449e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 5.9980e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 3.3706e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.8654e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.0590e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 5.9866e-06 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 3.4215e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.9819e-06 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1632e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 6.9799e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 4.3790e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 2.9031e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.0768e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.5822e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.3267e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:30.367699
# F-Score(Ordinary) = 0.321, Recall: 0.432, Precision: 0.255
# F-Score(ireflv) = 0.641, Recall: 0.5, Precision: 0.893
# F-Score(id) = 0.01, Recall: 0.5, Precision: 0.005
********************
