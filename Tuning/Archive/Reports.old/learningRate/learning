INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 48)        705264      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 24)        5640        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 192)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 96)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 288)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 24)           6936        concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            200         dropout_1[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.2332 - acc: 0.9336 - val_loss: 0.1282 - val_acc: 0.9585
Epoch 2/40
 - 4s - loss: 0.1221 - acc: 0.9587 - val_loss: 0.1016 - val_acc: 0.9624
Epoch 3/40
 - 4s - loss: 0.1020 - acc: 0.9624 - val_loss: 0.0911 - val_acc: 0.9650
Epoch 4/40
 - 4s - loss: 0.0919 - acc: 0.9644 - val_loss: 0.0858 - val_acc: 0.9660
Epoch 5/40
 - 4s - loss: 0.0849 - acc: 0.9660 - val_loss: 0.0832 - val_acc: 0.9665
Epoch 6/40
 - 4s - loss: 0.0799 - acc: 0.9676 - val_loss: 0.0799 - val_acc: 0.9669
Epoch 7/40
 - 4s - loss: 0.0768 - acc: 0.9677 - val_loss: 0.0784 - val_acc: 0.9674
Epoch 8/40
 - 4s - loss: 0.0743 - acc: 0.9683 - val_loss: 0.0771 - val_acc: 0.9672
Epoch 9/40
 - 4s - loss: 0.0720 - acc: 0.9690 - val_loss: 0.0769 - val_acc: 0.9677
Epoch 10/40
 - 4s - loss: 0.0699 - acc: 0.9696 - val_loss: 0.0766 - val_acc: 0.9672
Epoch 11/40
 - 4s - loss: 0.0685 - acc: 0.9696 - val_loss: 0.0752 - val_acc: 0.9672
Epoch 12/40
 - 4s - loss: 0.0659 - acc: 0.9707 - val_loss: 0.0738 - val_acc: 0.9679
Epoch 13/40
 - 4s - loss: 0.0651 - acc: 0.9709 - val_loss: 0.0737 - val_acc: 0.9682
Epoch 14/40
 - 4s - loss: 0.0634 - acc: 0.9710 - val_loss: 0.0744 - val_acc: 0.9685
Epoch 15/40
 - 4s - loss: 0.0625 - acc: 0.9713 - val_loss: 0.0743 - val_acc: 0.9681
Epoch 16/40
 - 4s - loss: 0.0608 - acc: 0.9717 - val_loss: 0.0726 - val_acc: 0.9683
Epoch 17/40
 - 4s - loss: 0.0601 - acc: 0.9722 - val_loss: 0.0732 - val_acc: 0.9683
Epoch 18/40
 - 4s - loss: 0.0585 - acc: 0.9724 - val_loss: 0.0726 - val_acc: 0.9686
Epoch 19/40
 - 4s - loss: 0.0578 - acc: 0.9725 - val_loss: 0.0732 - val_acc: 0.9681
Epoch 20/40
 - 4s - loss: 0.0571 - acc: 0.9729 - val_loss: 0.0744 - val_acc: 0.9677
Epoch 21/40
 - 4s - loss: 0.0560 - acc: 0.9729 - val_loss: 0.0742 - val_acc: 0.9674
Epoch 22/40
 - 4s - loss: 0.0549 - acc: 0.9738 - val_loss: 0.0715 - val_acc: 0.9683
Epoch 23/40
 - 4s - loss: 0.0542 - acc: 0.9741 - val_loss: 0.0730 - val_acc: 0.9686
Epoch 24/40
 - 4s - loss: 0.0536 - acc: 0.9738 - val_loss: 0.0717 - val_acc: 0.9692
Epoch 25/40
 - 4s - loss: 0.0527 - acc: 0.9743 - val_loss: 0.0741 - val_acc: 0.9682
Epoch 26/40
 - 4s - loss: 0.0520 - acc: 0.9744 - val_loss: 0.0735 - val_acc: 0.9686
Epoch 27/40
 - 4s - loss: 0.0513 - acc: 0.9748 - val_loss: 0.0748 - val_acc: 0.9682
Epoch 28/40
 - 4s - loss: 0.0507 - acc: 0.9746 - val_loss: 0.0720 - val_acc: 0.9691
Epoch 29/40
 - 4s - loss: 0.0504 - acc: 0.9750 - val_loss: 0.0748 - val_acc: 0.9679
Epoch 30/40
 - 4s - loss: 0.0494 - acc: 0.9748 - val_loss: 0.0741 - val_acc: 0.9689
Epoch 31/40
 - 4s - loss: 0.0489 - acc: 0.9752 - val_loss: 0.0742 - val_acc: 0.9689
Epoch 32/40
 - 4s - loss: 0.0482 - acc: 0.9756 - val_loss: 0.0732 - val_acc: 0.9688
Epoch 33/40
 - 4s - loss: 0.0482 - acc: 0.9752 - val_loss: 0.0747 - val_acc: 0.9694
Epoch 34/40
 - 4s - loss: 0.0474 - acc: 0.9752 - val_loss: 0.0754 - val_acc: 0.9685
Epoch 35/40
 - 4s - loss: 0.0470 - acc: 0.9755 - val_loss: 0.0740 - val_acc: 0.9686
Epoch 36/40
 - 4s - loss: 0.0466 - acc: 0.9758 - val_loss: 0.0733 - val_acc: 0.9687
Epoch 37/40
 - 4s - loss: 0.0463 - acc: 0.9757 - val_loss: 0.0760 - val_acc: 0.9691
Epoch 38/40
 - 4s - loss: 0.0459 - acc: 0.9757 - val_loss: 0.0743 - val_acc: 0.9688
Epoch 39/40
 - 4s - loss: 0.0454 - acc: 0.9759 - val_loss: 0.0768 - val_acc: 0.9687
Epoch 40/40
 - 4s - loss: 0.0451 - acc: 0.9760 - val_loss: 0.0751 - val_acc: 0.9692
Epoch 1/40
 - 0s - loss: 0.0708 - acc: 0.9672
Epoch 2/40
 - 0s - loss: 0.0620 - acc: 0.9700
Epoch 3/40
 - 0s - loss: 0.0597 - acc: 0.9713
Epoch 4/40
 - 0s - loss: 0.0549 - acc: 0.9723
Epoch 5/40
 - 0s - loss: 0.0534 - acc: 0.9738
Epoch 6/40
 - 0s - loss: 0.0520 - acc: 0.9736
Epoch 7/40
 - 0s - loss: 0.0520 - acc: 0.9742
Epoch 8/40
 - 0s - loss: 0.0492 - acc: 0.9738
Epoch 9/40
 - 0s - loss: 0.0490 - acc: 0.9744
Epoch 10/40
 - 0s - loss: 0.0477 - acc: 0.9753
Epoch 11/40
 - 0s - loss: 0.0473 - acc: 0.9753
Epoch 12/40
 - 0s - loss: 0.0452 - acc: 0.9760
Epoch 13/40
 - 0s - loss: 0.0456 - acc: 0.9762
Epoch 14/40
 - 0s - loss: 0.0449 - acc: 0.9748
Epoch 15/40
 - 0s - loss: 0.0444 - acc: 0.9754
Epoch 16/40
 - 0s - loss: 0.0426 - acc: 0.9773
Epoch 17/40
 - 0s - loss: 0.0427 - acc: 0.9769
Epoch 18/40
 - 0s - loss: 0.0424 - acc: 0.9760
Epoch 19/40
 - 0s - loss: 0.0423 - acc: 0.9767
Epoch 20/40
 - 0s - loss: 0.0414 - acc: 0.9760
Epoch 21/40
 - 0s - loss: 0.0413 - acc: 0.9768
Epoch 22/40
 - 0s - loss: 0.0408 - acc: 0.9766
Epoch 23/40
 - 0s - loss: 0.0400 - acc: 0.9770
Epoch 24/40
 - 0s - loss: 0.0398 - acc: 0.9762
Epoch 25/40
 - 0s - loss: 0.0394 - acc: 0.9770
Epoch 26/40
 - 0s - loss: 0.0400 - acc: 0.9771
Epoch 27/40
 - 0s - loss: 0.0391 - acc: 0.9777
Epoch 28/40
 - 0s - loss: 0.0388 - acc: 0.9772
Epoch 29/40
 - 0s - loss: 0.0388 - acc: 0.9777
Epoch 30/40
 - 0s - loss: 0.0383 - acc: 0.9779
Epoch 31/40
 - 0s - loss: 0.0366 - acc: 0.9792
Epoch 32/40
 - 0s - loss: 0.0379 - acc: 0.9772
Epoch 33/40
 - 0s - loss: 0.0376 - acc: 0.9771
Epoch 34/40
 - 0s - loss: 0.0378 - acc: 0.9768
Epoch 35/40
 - 0s - loss: 0.0374 - acc: 0.9773
Epoch 36/40
 - 0s - loss: 0.0370 - acc: 0.9780
Epoch 37/40
 - 0s - loss: 0.0368 - acc: 0.9787
Epoch 38/40
 - 0s - loss: 0.0365 - acc: 0.9780
Epoch 39/40
 - 0s - loss: 0.0365 - acc: 0.9777
Epoch 40/40
 - 0s - loss: 0.0372 - acc: 0.9769
# Training time = 0:03:31.134907
# F-Score(Ordinary) = 0.168, Recall: 0.465, Precision: 0.103
# F-Score(lvc) = 0.14, Recall: 0.909, Precision: 0.076
# F-Score(ireflv) = 0.078, Recall: 0.833, Precision: 0.041
# F-Score(id) = 0.225, Recall: 0.378, Precision: 0.161
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 48)        705264      input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 24)        5640        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 192)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 96)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 288)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 24)           6936        concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 24)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            200         dropout_2[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.2333 - acc: 0.9330 - val_loss: 0.1245 - val_acc: 0.9596
Epoch 2/40
 - 4s - loss: 0.1208 - acc: 0.9592 - val_loss: 0.1028 - val_acc: 0.9630
Epoch 3/40
 - 4s - loss: 0.1040 - acc: 0.9633 - val_loss: 0.0951 - val_acc: 0.9650
Epoch 4/40
 - 4s - loss: 0.0959 - acc: 0.9651 - val_loss: 0.0901 - val_acc: 0.9668
Epoch 5/40
 - 4s - loss: 0.0884 - acc: 0.9667 - val_loss: 0.0855 - val_acc: 0.9674
Epoch 6/40
 - 4s - loss: 0.0827 - acc: 0.9676 - val_loss: 0.0815 - val_acc: 0.9671
Epoch 7/40
 - 4s - loss: 0.0781 - acc: 0.9684 - val_loss: 0.0793 - val_acc: 0.9682
Epoch 8/40
 - 4s - loss: 0.0751 - acc: 0.9688 - val_loss: 0.0774 - val_acc: 0.9681
Epoch 9/40
 - 4s - loss: 0.0728 - acc: 0.9694 - val_loss: 0.0760 - val_acc: 0.9681
Epoch 10/40
 - 4s - loss: 0.0713 - acc: 0.9696 - val_loss: 0.0751 - val_acc: 0.9690
Epoch 11/40
 - 4s - loss: 0.0692 - acc: 0.9701 - val_loss: 0.0740 - val_acc: 0.9691
Epoch 12/40
 - 4s - loss: 0.0679 - acc: 0.9703 - val_loss: 0.0765 - val_acc: 0.9675
Epoch 13/40
 - 4s - loss: 0.0662 - acc: 0.9707 - val_loss: 0.0748 - val_acc: 0.9683
Epoch 14/40
 - 4s - loss: 0.0647 - acc: 0.9709 - val_loss: 0.0751 - val_acc: 0.9685
Epoch 15/40
 - 4s - loss: 0.0637 - acc: 0.9713 - val_loss: 0.0728 - val_acc: 0.9687
Epoch 16/40
 - 4s - loss: 0.0625 - acc: 0.9715 - val_loss: 0.0721 - val_acc: 0.9681
Epoch 17/40
 - 4s - loss: 0.0612 - acc: 0.9717 - val_loss: 0.0722 - val_acc: 0.9689
Epoch 18/40
 - 4s - loss: 0.0604 - acc: 0.9724 - val_loss: 0.0719 - val_acc: 0.9690
Epoch 19/40
 - 4s - loss: 0.0592 - acc: 0.9726 - val_loss: 0.0718 - val_acc: 0.9688
Epoch 20/40
 - 4s - loss: 0.0582 - acc: 0.9729 - val_loss: 0.0721 - val_acc: 0.9687
Epoch 21/40
 - 4s - loss: 0.0571 - acc: 0.9730 - val_loss: 0.0715 - val_acc: 0.9695
Epoch 22/40
 - 4s - loss: 0.0563 - acc: 0.9734 - val_loss: 0.0709 - val_acc: 0.9690
Epoch 23/40
 - 4s - loss: 0.0555 - acc: 0.9735 - val_loss: 0.0704 - val_acc: 0.9693
Epoch 24/40
 - 4s - loss: 0.0549 - acc: 0.9740 - val_loss: 0.0721 - val_acc: 0.9688
Epoch 25/40
 - 4s - loss: 0.0540 - acc: 0.9740 - val_loss: 0.0728 - val_acc: 0.9688
Epoch 26/40
 - 4s - loss: 0.0528 - acc: 0.9742 - val_loss: 0.0716 - val_acc: 0.9700
Epoch 27/40
 - 4s - loss: 0.0524 - acc: 0.9746 - val_loss: 0.0733 - val_acc: 0.9692
Epoch 28/40
 - 4s - loss: 0.0520 - acc: 0.9744 - val_loss: 0.0724 - val_acc: 0.9696
Epoch 29/40
 - 4s - loss: 0.0509 - acc: 0.9748 - val_loss: 0.0744 - val_acc: 0.9689
Epoch 30/40
 - 4s - loss: 0.0504 - acc: 0.9750 - val_loss: 0.0722 - val_acc: 0.9693
Epoch 31/40
 - 4s - loss: 0.0498 - acc: 0.9751 - val_loss: 0.0726 - val_acc: 0.9694
Epoch 32/40
 - 4s - loss: 0.0492 - acc: 0.9758 - val_loss: 0.0738 - val_acc: 0.9687
Epoch 33/40
 - 4s - loss: 0.0485 - acc: 0.9756 - val_loss: 0.0752 - val_acc: 0.9691
Epoch 34/40
 - 4s - loss: 0.0481 - acc: 0.9754 - val_loss: 0.0723 - val_acc: 0.9697
Epoch 35/40
 - 4s - loss: 0.0475 - acc: 0.9756 - val_loss: 0.0719 - val_acc: 0.9696
Epoch 36/40
 - 4s - loss: 0.0473 - acc: 0.9758 - val_loss: 0.0726 - val_acc: 0.9689
Epoch 37/40
 - 4s - loss: 0.0467 - acc: 0.9759 - val_loss: 0.0740 - val_acc: 0.9694
Epoch 38/40
 - 4s - loss: 0.0463 - acc: 0.9761 - val_loss: 0.0752 - val_acc: 0.9692
Epoch 39/40
 - 4s - loss: 0.0457 - acc: 0.9759 - val_loss: 0.0750 - val_acc: 0.9697
Epoch 40/40
 - 4s - loss: 0.0454 - acc: 0.9762 - val_loss: 0.0754 - val_acc: 0.9690
Epoch 1/40
 - 0s - loss: 0.0695 - acc: 0.9693
Epoch 2/40
 - 0s - loss: 0.0617 - acc: 0.9707
Epoch 3/40
 - 0s - loss: 0.0596 - acc: 0.9707
Epoch 4/40
 - 0s - loss: 0.0554 - acc: 0.9727
Epoch 5/40
 - 0s - loss: 0.0545 - acc: 0.9727
Epoch 6/40
 - 0s - loss: 0.0526 - acc: 0.9737
Epoch 7/40
 - 0s - loss: 0.0508 - acc: 0.9746
Epoch 8/40
 - 0s - loss: 0.0497 - acc: 0.9745
Epoch 9/40
 - 0s - loss: 0.0480 - acc: 0.9747
Epoch 10/40
 - 0s - loss: 0.0476 - acc: 0.9749
Epoch 11/40
 - 0s - loss: 0.0467 - acc: 0.9753
Epoch 12/40
 - 0s - loss: 0.0459 - acc: 0.9758
Epoch 13/40
 - 0s - loss: 0.0449 - acc: 0.9756
Epoch 14/40
 - 0s - loss: 0.0446 - acc: 0.9758
Epoch 15/40
 - 0s - loss: 0.0437 - acc: 0.9773
Epoch 16/40
 - 0s - loss: 0.0424 - acc: 0.9775
Epoch 17/40
 - 0s - loss: 0.0427 - acc: 0.9774
Epoch 18/40
 - 0s - loss: 0.0416 - acc: 0.9761
Epoch 19/40
 - 0s - loss: 0.0420 - acc: 0.9771
Epoch 20/40
 - 0s - loss: 0.0418 - acc: 0.9767
Epoch 21/40
 - 0s - loss: 0.0411 - acc: 0.9771
Epoch 22/40
 - 0s - loss: 0.0410 - acc: 0.9763
Epoch 23/40
 - 0s - loss: 0.0404 - acc: 0.9777
Epoch 24/40
 - 0s - loss: 0.0401 - acc: 0.9773
Epoch 25/40
 - 0s - loss: 0.0402 - acc: 0.9766
Epoch 26/40
 - 0s - loss: 0.0390 - acc: 0.9777
Epoch 27/40
 - 0s - loss: 0.0382 - acc: 0.9778
Epoch 28/40
 - 0s - loss: 0.0384 - acc: 0.9786
Epoch 29/40
 - 0s - loss: 0.0382 - acc: 0.9785
Epoch 30/40
 - 0s - loss: 0.0389 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0381 - acc: 0.9777
Epoch 32/40
 - 0s - loss: 0.0374 - acc: 0.9785
Epoch 33/40
 - 0s - loss: 0.0381 - acc: 0.9770
Epoch 34/40
 - 0s - loss: 0.0381 - acc: 0.9776
Epoch 35/40
 - 0s - loss: 0.0372 - acc: 0.9783
Epoch 36/40
 - 0s - loss: 0.0376 - acc: 0.9786
Epoch 37/40
 - 0s - loss: 0.0376 - acc: 0.9778
Epoch 38/40
 - 0s - loss: 0.0374 - acc: 0.9775
Epoch 39/40
 - 0s - loss: 0.0375 - acc: 0.9778
Epoch 40/40
 - 0s - loss: 0.0372 - acc: 0.9787
# Training time = 0:03:21.186238
# F-Score(Ordinary) = 0.226, Recall: 0.369, Precision: 0.163
# F-Score(lvc) = 0.227, Recall: 0.234, Precision: 0.22
# F-Score(ireflv) = 0.333, Recall: 0.484, Precision: 0.254
# F-Score(id) = 0.089, Recall: 0.9, Precision: 0.047
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 48)        705264      input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 24)        5640        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 192)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 96)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 288)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 24)           6936        concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 24)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            200         dropout_3[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.2252 - acc: 0.9371 - val_loss: 0.1267 - val_acc: 0.9595
Epoch 2/40
 - 4s - loss: 0.1176 - acc: 0.9598 - val_loss: 0.1014 - val_acc: 0.9634
Epoch 3/40
 - 4s - loss: 0.1002 - acc: 0.9634 - val_loss: 0.0921 - val_acc: 0.9646
Epoch 4/40
 - 4s - loss: 0.0887 - acc: 0.9656 - val_loss: 0.0864 - val_acc: 0.9654
Epoch 5/40
 - 4s - loss: 0.0818 - acc: 0.9667 - val_loss: 0.0815 - val_acc: 0.9665
Epoch 6/40
 - 4s - loss: 0.0774 - acc: 0.9674 - val_loss: 0.0786 - val_acc: 0.9674
Epoch 7/40
 - 4s - loss: 0.0739 - acc: 0.9683 - val_loss: 0.0765 - val_acc: 0.9671
Epoch 8/40
 - 4s - loss: 0.0716 - acc: 0.9689 - val_loss: 0.0786 - val_acc: 0.9671
Epoch 9/40
 - 4s - loss: 0.0693 - acc: 0.9693 - val_loss: 0.0761 - val_acc: 0.9667
Epoch 10/40
 - 4s - loss: 0.0676 - acc: 0.9698 - val_loss: 0.0742 - val_acc: 0.9676
Epoch 11/40
 - 4s - loss: 0.0662 - acc: 0.9699 - val_loss: 0.0748 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0643 - acc: 0.9705 - val_loss: 0.0743 - val_acc: 0.9683
Epoch 13/40
 - 4s - loss: 0.0630 - acc: 0.9713 - val_loss: 0.0729 - val_acc: 0.9679
Epoch 14/40
 - 4s - loss: 0.0620 - acc: 0.9711 - val_loss: 0.0760 - val_acc: 0.9674
Epoch 15/40
 - 4s - loss: 0.0608 - acc: 0.9713 - val_loss: 0.0719 - val_acc: 0.9681
Epoch 16/40
 - 4s - loss: 0.0596 - acc: 0.9718 - val_loss: 0.0717 - val_acc: 0.9681
Epoch 17/40
 - 4s - loss: 0.0585 - acc: 0.9723 - val_loss: 0.0721 - val_acc: 0.9680
Epoch 18/40
 - 4s - loss: 0.0577 - acc: 0.9723 - val_loss: 0.0709 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0567 - acc: 0.9727 - val_loss: 0.0740 - val_acc: 0.9677
Epoch 20/40
 - 4s - loss: 0.0552 - acc: 0.9730 - val_loss: 0.0729 - val_acc: 0.9680
Epoch 21/40
 - 4s - loss: 0.0545 - acc: 0.9733 - val_loss: 0.0709 - val_acc: 0.9686
Epoch 22/40
 - 4s - loss: 0.0534 - acc: 0.9734 - val_loss: 0.0722 - val_acc: 0.9683
Epoch 23/40
 - 4s - loss: 0.0528 - acc: 0.9741 - val_loss: 0.0717 - val_acc: 0.9687
Epoch 24/40
 - 4s - loss: 0.0520 - acc: 0.9743 - val_loss: 0.0727 - val_acc: 0.9686
Epoch 25/40
 - 4s - loss: 0.0513 - acc: 0.9742 - val_loss: 0.0726 - val_acc: 0.9687
Epoch 26/40
 - 4s - loss: 0.0507 - acc: 0.9743 - val_loss: 0.0707 - val_acc: 0.9683
Epoch 27/40
 - 4s - loss: 0.0497 - acc: 0.9748 - val_loss: 0.0724 - val_acc: 0.9689
Epoch 28/40
 - 4s - loss: 0.0493 - acc: 0.9746 - val_loss: 0.0704 - val_acc: 0.9694
Epoch 29/40
 - 4s - loss: 0.0487 - acc: 0.9753 - val_loss: 0.0711 - val_acc: 0.9688
Epoch 30/40
 - 4s - loss: 0.0486 - acc: 0.9751 - val_loss: 0.0709 - val_acc: 0.9689
Epoch 31/40
 - 4s - loss: 0.0475 - acc: 0.9753 - val_loss: 0.0720 - val_acc: 0.9684
Epoch 32/40
 - 4s - loss: 0.0471 - acc: 0.9755 - val_loss: 0.0737 - val_acc: 0.9688
Epoch 33/40
 - 4s - loss: 0.0464 - acc: 0.9760 - val_loss: 0.0759 - val_acc: 0.9688
Epoch 34/40
 - 4s - loss: 0.0462 - acc: 0.9756 - val_loss: 0.0724 - val_acc: 0.9685
Epoch 35/40
 - 4s - loss: 0.0457 - acc: 0.9760 - val_loss: 0.0738 - val_acc: 0.9685
Epoch 36/40
 - 4s - loss: 0.0454 - acc: 0.9758 - val_loss: 0.0732 - val_acc: 0.9688
Epoch 37/40
 - 4s - loss: 0.0449 - acc: 0.9759 - val_loss: 0.0728 - val_acc: 0.9691
Epoch 38/40
 - 4s - loss: 0.0445 - acc: 0.9760 - val_loss: 0.0756 - val_acc: 0.9690
Epoch 39/40
 - 4s - loss: 0.0442 - acc: 0.9760 - val_loss: 0.0737 - val_acc: 0.9682
Epoch 40/40
 - 4s - loss: 0.0439 - acc: 0.9761 - val_loss: 0.0749 - val_acc: 0.9684
Epoch 1/40
 - 0s - loss: 0.0696 - acc: 0.9673
Epoch 2/40
 - 0s - loss: 0.0612 - acc: 0.9693
Epoch 3/40
 - 0s - loss: 0.0569 - acc: 0.9715
Epoch 4/40
 - 0s - loss: 0.0552 - acc: 0.9713
Epoch 5/40
 - 0s - loss: 0.0524 - acc: 0.9733
Epoch 6/40
 - 0s - loss: 0.0516 - acc: 0.9743
Epoch 7/40
 - 0s - loss: 0.0497 - acc: 0.9740
Epoch 8/40
 - 0s - loss: 0.0477 - acc: 0.9753
Epoch 9/40
 - 0s - loss: 0.0477 - acc: 0.9753
Epoch 10/40
 - 0s - loss: 0.0456 - acc: 0.9761
Epoch 11/40
 - 0s - loss: 0.0462 - acc: 0.9749
Epoch 12/40
 - 0s - loss: 0.0448 - acc: 0.9755
Epoch 13/40
 - 0s - loss: 0.0437 - acc: 0.9765
Epoch 14/40
 - 0s - loss: 0.0425 - acc: 0.9760
Epoch 15/40
 - 0s - loss: 0.0433 - acc: 0.9753
Epoch 16/40
 - 0s - loss: 0.0417 - acc: 0.9761
Epoch 17/40
 - 0s - loss: 0.0410 - acc: 0.9776
Epoch 18/40
 - 0s - loss: 0.0405 - acc: 0.9772
Epoch 19/40
 - 0s - loss: 0.0413 - acc: 0.9756
Epoch 20/40
 - 0s - loss: 0.0404 - acc: 0.9768
Epoch 21/40
 - 0s - loss: 0.0394 - acc: 0.9785
Epoch 22/40
 - 0s - loss: 0.0393 - acc: 0.9769
Epoch 23/40
 - 0s - loss: 0.0389 - acc: 0.9771
Epoch 24/40
 - 0s - loss: 0.0383 - acc: 0.9771
Epoch 25/40
 - 0s - loss: 0.0378 - acc: 0.9778
Epoch 26/40
 - 0s - loss: 0.0378 - acc: 0.9776
Epoch 27/40
 - 0s - loss: 0.0369 - acc: 0.9784
Epoch 28/40
 - 0s - loss: 0.0381 - acc: 0.9777
Epoch 29/40
 - 0s - loss: 0.0377 - acc: 0.9761
Epoch 30/40
 - 0s - loss: 0.0367 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0368 - acc: 0.9780
Epoch 32/40
 - 0s - loss: 0.0367 - acc: 0.9788
Epoch 33/40
 - 0s - loss: 0.0361 - acc: 0.9793
Epoch 34/40
 - 0s - loss: 0.0357 - acc: 0.9793
Epoch 35/40
 - 0s - loss: 0.0364 - acc: 0.9786
Epoch 36/40
 - 0s - loss: 0.0357 - acc: 0.9789
Epoch 37/40
 - 0s - loss: 0.0362 - acc: 0.9783
Epoch 38/40
 - 0s - loss: 0.0355 - acc: 0.9789
Epoch 39/40
 - 0s - loss: 0.0359 - acc: 0.9776
Epoch 40/40
 - 0s - loss: 0.0355 - acc: 0.9781
# Training time = 0:03:21.732804
# F-Score(Ordinary) = 0.225, Recall: 0.332, Precision: 0.17
# F-Score(lvc) = 0.266, Recall: 0.221, Precision: 0.333
# F-Score(ireflv) = 0.03, Recall: 0.2, Precision: 0.016
# F-Score(id) = 0.178, Recall: 0.95, Precision: 0.098
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 48)        705264      input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 24)        5640        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 192)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 96)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 288)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 24)           6936        concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            200         dropout_4[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.2466 - acc: 0.9321 - val_loss: 0.1353 - val_acc: 0.9598
Epoch 2/40
 - 4s - loss: 0.1278 - acc: 0.9585 - val_loss: 0.1046 - val_acc: 0.9631
Epoch 3/40
 - 4s - loss: 0.1052 - acc: 0.9626 - val_loss: 0.0922 - val_acc: 0.9653
Epoch 4/40
 - 4s - loss: 0.0922 - acc: 0.9647 - val_loss: 0.0859 - val_acc: 0.9666
Epoch 5/40
 - 4s - loss: 0.0848 - acc: 0.9660 - val_loss: 0.0816 - val_acc: 0.9667
Epoch 6/40
 - 4s - loss: 0.0801 - acc: 0.9673 - val_loss: 0.0796 - val_acc: 0.9672
Epoch 7/40
 - 4s - loss: 0.0769 - acc: 0.9684 - val_loss: 0.0782 - val_acc: 0.9677
Epoch 8/40
 - 4s - loss: 0.0741 - acc: 0.9684 - val_loss: 0.0785 - val_acc: 0.9676
Epoch 9/40
 - 4s - loss: 0.0717 - acc: 0.9692 - val_loss: 0.0750 - val_acc: 0.9679
Epoch 10/40
 - 4s - loss: 0.0699 - acc: 0.9696 - val_loss: 0.0745 - val_acc: 0.9679
Epoch 11/40
 - 4s - loss: 0.0680 - acc: 0.9699 - val_loss: 0.0736 - val_acc: 0.9684
Epoch 12/40
 - 4s - loss: 0.0668 - acc: 0.9705 - val_loss: 0.0739 - val_acc: 0.9678
Epoch 13/40
 - 4s - loss: 0.0650 - acc: 0.9709 - val_loss: 0.0745 - val_acc: 0.9682
Epoch 14/40
 - 4s - loss: 0.0640 - acc: 0.9715 - val_loss: 0.0730 - val_acc: 0.9684
Epoch 15/40
 - 4s - loss: 0.0628 - acc: 0.9714 - val_loss: 0.0728 - val_acc: 0.9681
Epoch 16/40
 - 4s - loss: 0.0616 - acc: 0.9718 - val_loss: 0.0719 - val_acc: 0.9682
Epoch 17/40
 - 4s - loss: 0.0605 - acc: 0.9723 - val_loss: 0.0730 - val_acc: 0.9679
Epoch 18/40
 - 4s - loss: 0.0593 - acc: 0.9728 - val_loss: 0.0721 - val_acc: 0.9685
Epoch 19/40
 - 4s - loss: 0.0586 - acc: 0.9729 - val_loss: 0.0756 - val_acc: 0.9673
Epoch 20/40
 - 4s - loss: 0.0573 - acc: 0.9730 - val_loss: 0.0735 - val_acc: 0.9681
Epoch 21/40
 - 4s - loss: 0.0565 - acc: 0.9732 - val_loss: 0.0735 - val_acc: 0.9683
Epoch 22/40
 - 4s - loss: 0.0559 - acc: 0.9733 - val_loss: 0.0721 - val_acc: 0.9691
Epoch 23/40
 - 4s - loss: 0.0551 - acc: 0.9737 - val_loss: 0.0719 - val_acc: 0.9686
Epoch 24/40
 - 4s - loss: 0.0546 - acc: 0.9739 - val_loss: 0.0707 - val_acc: 0.9692
Epoch 25/40
 - 4s - loss: 0.0532 - acc: 0.9743 - val_loss: 0.0727 - val_acc: 0.9692
Epoch 26/40
 - 4s - loss: 0.0531 - acc: 0.9745 - val_loss: 0.0721 - val_acc: 0.9691
Epoch 27/40
 - 4s - loss: 0.0520 - acc: 0.9752 - val_loss: 0.0717 - val_acc: 0.9689
Epoch 28/40
 - 4s - loss: 0.0516 - acc: 0.9748 - val_loss: 0.0727 - val_acc: 0.9690
Epoch 29/40
 - 4s - loss: 0.0509 - acc: 0.9749 - val_loss: 0.0731 - val_acc: 0.9691
Epoch 30/40
 - 4s - loss: 0.0505 - acc: 0.9746 - val_loss: 0.0715 - val_acc: 0.9693
Epoch 31/40
 - 4s - loss: 0.0498 - acc: 0.9755 - val_loss: 0.0722 - val_acc: 0.9684
Epoch 32/40
 - 4s - loss: 0.0494 - acc: 0.9754 - val_loss: 0.0727 - val_acc: 0.9692
Epoch 33/40
 - 4s - loss: 0.0486 - acc: 0.9756 - val_loss: 0.0747 - val_acc: 0.9694
Epoch 34/40
 - 4s - loss: 0.0484 - acc: 0.9756 - val_loss: 0.0727 - val_acc: 0.9689
Epoch 35/40
 - 4s - loss: 0.0481 - acc: 0.9755 - val_loss: 0.0736 - val_acc: 0.9689
Epoch 36/40
 - 4s - loss: 0.0477 - acc: 0.9756 - val_loss: 0.0748 - val_acc: 0.9693
Epoch 37/40
 - 4s - loss: 0.0470 - acc: 0.9761 - val_loss: 0.0749 - val_acc: 0.9685
Epoch 38/40
 - 4s - loss: 0.0467 - acc: 0.9762 - val_loss: 0.0750 - val_acc: 0.9689
Epoch 39/40
 - 4s - loss: 0.0463 - acc: 0.9761 - val_loss: 0.0760 - val_acc: 0.9680
Epoch 40/40
 - 4s - loss: 0.0460 - acc: 0.9759 - val_loss: 0.0728 - val_acc: 0.9687
Epoch 1/40
 - 0s - loss: 0.0710 - acc: 0.9678
Epoch 2/40
 - 0s - loss: 0.0635 - acc: 0.9693
Epoch 3/40
 - 0s - loss: 0.0589 - acc: 0.9715
Epoch 4/40
 - 0s - loss: 0.0565 - acc: 0.9724
Epoch 5/40
 - 0s - loss: 0.0534 - acc: 0.9737
Epoch 6/40
 - 0s - loss: 0.0522 - acc: 0.9736
Epoch 7/40
 - 0s - loss: 0.0509 - acc: 0.9750
Epoch 8/40
 - 0s - loss: 0.0504 - acc: 0.9735
Epoch 9/40
 - 0s - loss: 0.0487 - acc: 0.9746
Epoch 10/40
 - 0s - loss: 0.0469 - acc: 0.9762
Epoch 11/40
 - 0s - loss: 0.0464 - acc: 0.9764
Epoch 12/40
 - 0s - loss: 0.0467 - acc: 0.9751
Epoch 13/40
 - 0s - loss: 0.0458 - acc: 0.9755
Epoch 14/40
 - 0s - loss: 0.0450 - acc: 0.9753
Epoch 15/40
 - 0s - loss: 0.0434 - acc: 0.9778
Epoch 16/40
 - 0s - loss: 0.0436 - acc: 0.9765
Epoch 17/40
 - 0s - loss: 0.0432 - acc: 0.9766
Epoch 18/40
 - 0s - loss: 0.0428 - acc: 0.9769
Epoch 19/40
 - 0s - loss: 0.0420 - acc: 0.9781
Epoch 20/40
 - 0s - loss: 0.0422 - acc: 0.9773
Epoch 21/40
 - 0s - loss: 0.0415 - acc: 0.9771
Epoch 22/40
 - 0s - loss: 0.0408 - acc: 0.9789
Epoch 23/40
 - 0s - loss: 0.0415 - acc: 0.9771
Epoch 24/40
 - 0s - loss: 0.0408 - acc: 0.9770
Epoch 25/40
 - 0s - loss: 0.0397 - acc: 0.9794
Epoch 26/40
 - 0s - loss: 0.0401 - acc: 0.9776
Epoch 27/40
 - 0s - loss: 0.0403 - acc: 0.9777
Epoch 28/40
 - 0s - loss: 0.0396 - acc: 0.9782
Epoch 29/40
 - 0s - loss: 0.0391 - acc: 0.9780
Epoch 30/40
 - 0s - loss: 0.0392 - acc: 0.9776
Epoch 31/40
 - 0s - loss: 0.0393 - acc: 0.9780
Epoch 32/40
 - 0s - loss: 0.0390 - acc: 0.9791
Epoch 33/40
 - 0s - loss: 0.0384 - acc: 0.9790
Epoch 34/40
 - 0s - loss: 0.0381 - acc: 0.9793
Epoch 35/40
 - 0s - loss: 0.0393 - acc: 0.9777
Epoch 36/40
 - 0s - loss: 0.0380 - acc: 0.9785
Epoch 37/40
 - 0s - loss: 0.0384 - acc: 0.9776
Epoch 38/40
 - 0s - loss: 0.0372 - acc: 0.9789
Epoch 39/40
 - 0s - loss: 0.0378 - acc: 0.9787
Epoch 40/40
 - 0s - loss: 0.0382 - acc: 0.9783
# Training time = 0:03:20.817891
# F-Score(Ordinary) = 0.237, Recall: 0.333, Precision: 0.183
# F-Score(lvc) = 0.322, Recall: 0.262, Precision: 0.417
# F-Score(ireflv) = 0.082, Recall: 0.25, Precision: 0.049
# F-Score(id) = 0.117, Recall: 1.0, Precision: 0.062
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 48)        705264      input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 24)        5640        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 192)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 96)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 288)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 24)           6936        concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 24)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            200         dropout_5[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.2310 - acc: 0.9330 - val_loss: 0.1297 - val_acc: 0.9601
Epoch 2/40
 - 4s - loss: 0.1246 - acc: 0.9581 - val_loss: 0.1050 - val_acc: 0.9626
Epoch 3/40
 - 4s - loss: 0.1055 - acc: 0.9623 - val_loss: 0.0947 - val_acc: 0.9653
Epoch 4/40
 - 4s - loss: 0.0950 - acc: 0.9647 - val_loss: 0.0877 - val_acc: 0.9663
Epoch 5/40
 - 4s - loss: 0.0870 - acc: 0.9662 - val_loss: 0.0823 - val_acc: 0.9670
Epoch 6/40
 - 4s - loss: 0.0813 - acc: 0.9673 - val_loss: 0.0793 - val_acc: 0.9671
Epoch 7/40
 - 4s - loss: 0.0782 - acc: 0.9675 - val_loss: 0.0791 - val_acc: 0.9675
Epoch 8/40
 - 4s - loss: 0.0756 - acc: 0.9685 - val_loss: 0.0787 - val_acc: 0.9673
Epoch 9/40
 - 4s - loss: 0.0736 - acc: 0.9686 - val_loss: 0.0771 - val_acc: 0.9675
Epoch 10/40
 - 4s - loss: 0.0712 - acc: 0.9697 - val_loss: 0.0756 - val_acc: 0.9679
Epoch 11/40
 - 4s - loss: 0.0702 - acc: 0.9697 - val_loss: 0.0758 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0686 - acc: 0.9700 - val_loss: 0.0731 - val_acc: 0.9684
Epoch 13/40
 - 4s - loss: 0.0666 - acc: 0.9711 - val_loss: 0.0734 - val_acc: 0.9682
Epoch 14/40
 - 4s - loss: 0.0655 - acc: 0.9707 - val_loss: 0.0727 - val_acc: 0.9680
Epoch 15/40
 - 4s - loss: 0.0637 - acc: 0.9714 - val_loss: 0.0724 - val_acc: 0.9684
Epoch 16/40
 - 4s - loss: 0.0629 - acc: 0.9717 - val_loss: 0.0727 - val_acc: 0.9686
Epoch 17/40
 - 4s - loss: 0.0615 - acc: 0.9721 - val_loss: 0.0717 - val_acc: 0.9686
Epoch 18/40
 - 4s - loss: 0.0607 - acc: 0.9723 - val_loss: 0.0720 - val_acc: 0.9689
Epoch 19/40
 - 4s - loss: 0.0595 - acc: 0.9726 - val_loss: 0.0716 - val_acc: 0.9685
Epoch 20/40
 - 4s - loss: 0.0591 - acc: 0.9727 - val_loss: 0.0716 - val_acc: 0.9694
Epoch 21/40
 - 4s - loss: 0.0580 - acc: 0.9727 - val_loss: 0.0715 - val_acc: 0.9690
Epoch 22/40
 - 4s - loss: 0.0570 - acc: 0.9731 - val_loss: 0.0706 - val_acc: 0.9687
Epoch 23/40
 - 4s - loss: 0.0560 - acc: 0.9735 - val_loss: 0.0717 - val_acc: 0.9685
Epoch 24/40
 - 4s - loss: 0.0548 - acc: 0.9740 - val_loss: 0.0717 - val_acc: 0.9689
Epoch 25/40
 - 4s - loss: 0.0541 - acc: 0.9741 - val_loss: 0.0716 - val_acc: 0.9687
Epoch 26/40
 - 4s - loss: 0.0538 - acc: 0.9741 - val_loss: 0.0728 - val_acc: 0.9694
Epoch 27/40
 - 4s - loss: 0.0532 - acc: 0.9743 - val_loss: 0.0713 - val_acc: 0.9694
Epoch 28/40
 - 4s - loss: 0.0522 - acc: 0.9749 - val_loss: 0.0722 - val_acc: 0.9695
Epoch 29/40
 - 4s - loss: 0.0519 - acc: 0.9748 - val_loss: 0.0714 - val_acc: 0.9694
Epoch 30/40
 - 4s - loss: 0.0511 - acc: 0.9751 - val_loss: 0.0732 - val_acc: 0.9689
Epoch 31/40
 - 4s - loss: 0.0509 - acc: 0.9751 - val_loss: 0.0708 - val_acc: 0.9693
Epoch 32/40
 - 4s - loss: 0.0499 - acc: 0.9752 - val_loss: 0.0721 - val_acc: 0.9694
Epoch 33/40
 - 4s - loss: 0.0494 - acc: 0.9754 - val_loss: 0.0718 - val_acc: 0.9694
Epoch 34/40
 - 4s - loss: 0.0492 - acc: 0.9754 - val_loss: 0.0742 - val_acc: 0.9691
Epoch 35/40
 - 4s - loss: 0.0485 - acc: 0.9755 - val_loss: 0.0744 - val_acc: 0.9680
Epoch 36/40
 - 4s - loss: 0.0482 - acc: 0.9757 - val_loss: 0.0723 - val_acc: 0.9695
Epoch 37/40
 - 4s - loss: 0.0478 - acc: 0.9756 - val_loss: 0.0736 - val_acc: 0.9692
Epoch 38/40
 - 4s - loss: 0.0474 - acc: 0.9757 - val_loss: 0.0725 - val_acc: 0.9695
Epoch 39/40
 - 4s - loss: 0.0469 - acc: 0.9758 - val_loss: 0.0722 - val_acc: 0.9691
Epoch 40/40
 - 4s - loss: 0.0466 - acc: 0.9756 - val_loss: 0.0745 - val_acc: 0.9690
Epoch 1/40
 - 0s - loss: 0.0719 - acc: 0.9679
Epoch 2/40
 - 0s - loss: 0.0630 - acc: 0.9714
Epoch 3/40
 - 0s - loss: 0.0594 - acc: 0.9715
Epoch 4/40
 - 0s - loss: 0.0568 - acc: 0.9720
Epoch 5/40
 - 0s - loss: 0.0545 - acc: 0.9726
Epoch 6/40
 - 0s - loss: 0.0531 - acc: 0.9737
Epoch 7/40
 - 0s - loss: 0.0508 - acc: 0.9748
Epoch 8/40
 - 0s - loss: 0.0499 - acc: 0.9743
Epoch 9/40
 - 0s - loss: 0.0497 - acc: 0.9741
Epoch 10/40
 - 0s - loss: 0.0480 - acc: 0.9757
Epoch 11/40
 - 0s - loss: 0.0480 - acc: 0.9757
Epoch 12/40
 - 0s - loss: 0.0466 - acc: 0.9757
Epoch 13/40
 - 0s - loss: 0.0463 - acc: 0.9749
Epoch 14/40
 - 0s - loss: 0.0452 - acc: 0.9747
Epoch 15/40
 - 0s - loss: 0.0441 - acc: 0.9773
Epoch 16/40
 - 0s - loss: 0.0437 - acc: 0.9760
Epoch 17/40
 - 0s - loss: 0.0445 - acc: 0.9752
Epoch 18/40
 - 0s - loss: 0.0431 - acc: 0.9765
Epoch 19/40
 - 0s - loss: 0.0429 - acc: 0.9776
Epoch 20/40
 - 0s - loss: 0.0423 - acc: 0.9767
Epoch 21/40
 - 0s - loss: 0.0426 - acc: 0.9759
Epoch 22/40
 - 0s - loss: 0.0410 - acc: 0.9774
Epoch 23/40
 - 0s - loss: 0.0408 - acc: 0.9769
Epoch 24/40
 - 0s - loss: 0.0410 - acc: 0.9772
Epoch 25/40
 - 0s - loss: 0.0408 - acc: 0.9771
Epoch 26/40
 - 0s - loss: 0.0400 - acc: 0.9778
Epoch 27/40
 - 0s - loss: 0.0408 - acc: 0.9766
Epoch 28/40
 - 0s - loss: 0.0402 - acc: 0.9778
Epoch 29/40
 - 0s - loss: 0.0398 - acc: 0.9779
Epoch 30/40
 - 0s - loss: 0.0403 - acc: 0.9773
Epoch 31/40
 - 0s - loss: 0.0396 - acc: 0.9768
Epoch 32/40
 - 0s - loss: 0.0391 - acc: 0.9784
Epoch 33/40
 - 0s - loss: 0.0381 - acc: 0.9791
Epoch 34/40
 - 0s - loss: 0.0378 - acc: 0.9794
Epoch 35/40
 - 0s - loss: 0.0383 - acc: 0.9785
Epoch 36/40
 - 0s - loss: 0.0380 - acc: 0.9782
Epoch 37/40
 - 0s - loss: 0.0377 - acc: 0.9781
Epoch 38/40
 - 0s - loss: 0.0378 - acc: 0.9781
Epoch 39/40
 - 0s - loss: 0.0375 - acc: 0.9779
Epoch 40/40
 - 0s - loss: 0.0378 - acc: 0.9782
# Training time = 0:03:21.509622
# F-Score(Ordinary) = 0.235, Recall: 0.422, Precision: 0.163
# F-Score(lvc) = 0.247, Recall: 0.864, Precision: 0.144
# F-Score(ireflv) = 0.252, Recall: 0.857, Precision: 0.148
# F-Score(id) = 0.211, Recall: 0.262, Precision: 0.176
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 48)        705264      input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 24)        5640        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 192)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 96)           0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 288)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 24)           6936        concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 24)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            200         dropout_6[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1829 - acc: 0.9447 - val_loss: 0.1051 - val_acc: 0.9617
Epoch 2/40
 - 4s - loss: 0.1002 - acc: 0.9626 - val_loss: 0.0876 - val_acc: 0.9643
Epoch 3/40
 - 4s - loss: 0.0846 - acc: 0.9659 - val_loss: 0.0809 - val_acc: 0.9672
Epoch 4/40
 - 4s - loss: 0.0777 - acc: 0.9676 - val_loss: 0.0786 - val_acc: 0.9672
Epoch 5/40
 - 4s - loss: 0.0732 - acc: 0.9689 - val_loss: 0.0764 - val_acc: 0.9674
Epoch 6/40
 - 4s - loss: 0.0697 - acc: 0.9697 - val_loss: 0.0748 - val_acc: 0.9679
Epoch 7/40
 - 4s - loss: 0.0671 - acc: 0.9699 - val_loss: 0.0731 - val_acc: 0.9684
Epoch 8/40
 - 4s - loss: 0.0649 - acc: 0.9704 - val_loss: 0.0724 - val_acc: 0.9678
Epoch 9/40
 - 4s - loss: 0.0622 - acc: 0.9713 - val_loss: 0.0739 - val_acc: 0.9679
Epoch 10/40
 - 4s - loss: 0.0601 - acc: 0.9716 - val_loss: 0.0730 - val_acc: 0.9678
Epoch 11/40
 - 4s - loss: 0.0586 - acc: 0.9720 - val_loss: 0.0718 - val_acc: 0.9684
Epoch 12/40
 - 4s - loss: 0.0562 - acc: 0.9728 - val_loss: 0.0703 - val_acc: 0.9688
Epoch 13/40
 - 4s - loss: 0.0554 - acc: 0.9731 - val_loss: 0.0715 - val_acc: 0.9686
Epoch 14/40
 - 4s - loss: 0.0539 - acc: 0.9734 - val_loss: 0.0739 - val_acc: 0.9692
Epoch 15/40
 - 4s - loss: 0.0529 - acc: 0.9739 - val_loss: 0.0757 - val_acc: 0.9683
Epoch 16/40
 - 4s - loss: 0.0512 - acc: 0.9742 - val_loss: 0.0731 - val_acc: 0.9683
Epoch 17/40
 - 4s - loss: 0.0505 - acc: 0.9743 - val_loss: 0.0758 - val_acc: 0.9683
Epoch 18/40
 - 4s - loss: 0.0491 - acc: 0.9749 - val_loss: 0.0741 - val_acc: 0.9687
Epoch 19/40
 - 4s - loss: 0.0484 - acc: 0.9751 - val_loss: 0.0750 - val_acc: 0.9680
Epoch 20/40
 - 4s - loss: 0.0480 - acc: 0.9751 - val_loss: 0.0758 - val_acc: 0.9686
Epoch 21/40
 - 4s - loss: 0.0471 - acc: 0.9750 - val_loss: 0.0783 - val_acc: 0.9668
Epoch 22/40
 - 4s - loss: 0.0459 - acc: 0.9757 - val_loss: 0.0752 - val_acc: 0.9687
Epoch 23/40
 - 4s - loss: 0.0456 - acc: 0.9757 - val_loss: 0.0737 - val_acc: 0.9688
Epoch 24/40
 - 4s - loss: 0.0448 - acc: 0.9758 - val_loss: 0.0749 - val_acc: 0.9693
Epoch 25/40
 - 4s - loss: 0.0443 - acc: 0.9762 - val_loss: 0.0775 - val_acc: 0.9689
Epoch 26/40
 - 4s - loss: 0.0437 - acc: 0.9761 - val_loss: 0.0793 - val_acc: 0.9687
Epoch 27/40
 - 4s - loss: 0.0433 - acc: 0.9764 - val_loss: 0.0790 - val_acc: 0.9687
Epoch 28/40
 - 4s - loss: 0.0427 - acc: 0.9763 - val_loss: 0.0776 - val_acc: 0.9692
Epoch 29/40
 - 4s - loss: 0.0424 - acc: 0.9761 - val_loss: 0.0784 - val_acc: 0.9687
Epoch 30/40
 - 4s - loss: 0.0418 - acc: 0.9764 - val_loss: 0.0801 - val_acc: 0.9689
Epoch 31/40
 - 4s - loss: 0.0414 - acc: 0.9766 - val_loss: 0.0787 - val_acc: 0.9695
Epoch 32/40
 - 4s - loss: 0.0408 - acc: 0.9768 - val_loss: 0.0788 - val_acc: 0.9688
Epoch 33/40
 - 4s - loss: 0.0408 - acc: 0.9767 - val_loss: 0.0805 - val_acc: 0.9694
Epoch 34/40
 - 4s - loss: 0.0405 - acc: 0.9765 - val_loss: 0.0829 - val_acc: 0.9685
Epoch 35/40
 - 4s - loss: 0.0402 - acc: 0.9767 - val_loss: 0.0811 - val_acc: 0.9691
Epoch 36/40
 - 4s - loss: 0.0398 - acc: 0.9769 - val_loss: 0.0813 - val_acc: 0.9686
Epoch 37/40
 - 4s - loss: 0.0396 - acc: 0.9768 - val_loss: 0.0827 - val_acc: 0.9691
Epoch 38/40
 - 4s - loss: 0.0392 - acc: 0.9768 - val_loss: 0.0800 - val_acc: 0.9689
Epoch 39/40
 - 4s - loss: 0.0389 - acc: 0.9769 - val_loss: 0.0827 - val_acc: 0.9689
Epoch 40/40
 - 4s - loss: 0.0389 - acc: 0.9770 - val_loss: 0.0833 - val_acc: 0.9695
Epoch 1/40
 - 0s - loss: 0.0701 - acc: 0.9678
Epoch 2/40
 - 0s - loss: 0.0579 - acc: 0.9699
Epoch 3/40
 - 0s - loss: 0.0545 - acc: 0.9712
Epoch 4/40
 - 0s - loss: 0.0501 - acc: 0.9736
Epoch 5/40
 - 0s - loss: 0.0484 - acc: 0.9737
Epoch 6/40
 - 0s - loss: 0.0459 - acc: 0.9744
Epoch 7/40
 - 0s - loss: 0.0459 - acc: 0.9745
Epoch 8/40
 - 0s - loss: 0.0424 - acc: 0.9746
Epoch 9/40
 - 0s - loss: 0.0418 - acc: 0.9755
Epoch 10/40
 - 0s - loss: 0.0410 - acc: 0.9755
Epoch 11/40
 - 0s - loss: 0.0399 - acc: 0.9759
Epoch 12/40
 - 0s - loss: 0.0388 - acc: 0.9772
Epoch 13/40
 - 0s - loss: 0.0389 - acc: 0.9765
Epoch 14/40
 - 0s - loss: 0.0376 - acc: 0.9761
Epoch 15/40
 - 0s - loss: 0.0371 - acc: 0.9762
Epoch 16/40
 - 0s - loss: 0.0363 - acc: 0.9781
Epoch 17/40
 - 0s - loss: 0.0360 - acc: 0.9769
Epoch 18/40
 - 0s - loss: 0.0354 - acc: 0.9773
Epoch 19/40
 - 0s - loss: 0.0353 - acc: 0.9766
Epoch 20/40
 - 0s - loss: 0.0351 - acc: 0.9774
Epoch 21/40
 - 0s - loss: 0.0345 - acc: 0.9777
Epoch 22/40
 - 0s - loss: 0.0344 - acc: 0.9780
Epoch 23/40
 - 0s - loss: 0.0344 - acc: 0.9779
Epoch 24/40
 - 0s - loss: 0.0340 - acc: 0.9782
Epoch 25/40
 - 0s - loss: 0.0336 - acc: 0.9784
Epoch 26/40
 - 0s - loss: 0.0346 - acc: 0.9785
Epoch 27/40
 - 0s - loss: 0.0334 - acc: 0.9778
Epoch 28/40
 - 0s - loss: 0.0335 - acc: 0.9788
Epoch 29/40
 - 0s - loss: 0.0337 - acc: 0.9793
Epoch 30/40
 - 0s - loss: 0.0333 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0324 - acc: 0.9794
Epoch 32/40
 - 0s - loss: 0.0330 - acc: 0.9785
Epoch 33/40
 - 0s - loss: 0.0330 - acc: 0.9783
Epoch 34/40
 - 0s - loss: 0.0328 - acc: 0.9780
Epoch 35/40
 - 0s - loss: 0.0327 - acc: 0.9783
Epoch 36/40
 - 0s - loss: 0.0327 - acc: 0.9782
Epoch 37/40
 - 0s - loss: 0.0325 - acc: 0.9796
Epoch 38/40
 - 0s - loss: 0.0324 - acc: 0.9782
Epoch 39/40
 - 0s - loss: 0.0325 - acc: 0.9784
Epoch 40/40
 - 0s - loss: 0.0326 - acc: 0.9778
# Training time = 0:03:23.442172
# F-Score(Ordinary) = 0.09, Recall: 0.348, Precision: 0.051
# F-Score(id) = 0.18, Recall: 0.371, Precision: 0.119
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 48)        705264      input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 24)        5640        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 192)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 96)           0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 288)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 24)           6936        concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 24)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            200         dropout_7[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1822 - acc: 0.9447 - val_loss: 0.1032 - val_acc: 0.9628
Epoch 2/40
 - 4s - loss: 0.1019 - acc: 0.9633 - val_loss: 0.0911 - val_acc: 0.9667
Epoch 3/40
 - 4s - loss: 0.0884 - acc: 0.9666 - val_loss: 0.0828 - val_acc: 0.9671
Epoch 4/40
 - 4s - loss: 0.0800 - acc: 0.9679 - val_loss: 0.0788 - val_acc: 0.9676
Epoch 5/40
 - 4s - loss: 0.0740 - acc: 0.9688 - val_loss: 0.0766 - val_acc: 0.9683
Epoch 6/40
 - 4s - loss: 0.0702 - acc: 0.9695 - val_loss: 0.0756 - val_acc: 0.9675
Epoch 7/40
 - 4s - loss: 0.0670 - acc: 0.9705 - val_loss: 0.0748 - val_acc: 0.9684
Epoch 8/40
 - 4s - loss: 0.0648 - acc: 0.9710 - val_loss: 0.0740 - val_acc: 0.9685
Epoch 9/40
 - 4s - loss: 0.0632 - acc: 0.9713 - val_loss: 0.0717 - val_acc: 0.9684
Epoch 10/40
 - 4s - loss: 0.0619 - acc: 0.9717 - val_loss: 0.0726 - val_acc: 0.9691
Epoch 11/40
 - 4s - loss: 0.0600 - acc: 0.9722 - val_loss: 0.0736 - val_acc: 0.9689
Epoch 12/40
 - 4s - loss: 0.0588 - acc: 0.9725 - val_loss: 0.0749 - val_acc: 0.9677
Epoch 13/40
 - 4s - loss: 0.0572 - acc: 0.9729 - val_loss: 0.0747 - val_acc: 0.9684
Epoch 14/40
 - 4s - loss: 0.0555 - acc: 0.9733 - val_loss: 0.0767 - val_acc: 0.9691
Epoch 15/40
 - 4s - loss: 0.0544 - acc: 0.9740 - val_loss: 0.0732 - val_acc: 0.9687
Epoch 16/40
 - 4s - loss: 0.0533 - acc: 0.9741 - val_loss: 0.0724 - val_acc: 0.9684
Epoch 17/40
 - 4s - loss: 0.0522 - acc: 0.9741 - val_loss: 0.0726 - val_acc: 0.9688
Epoch 18/40
 - 4s - loss: 0.0512 - acc: 0.9745 - val_loss: 0.0744 - val_acc: 0.9689
Epoch 19/40
 - 4s - loss: 0.0502 - acc: 0.9752 - val_loss: 0.0736 - val_acc: 0.9691
Epoch 20/40
 - 4s - loss: 0.0494 - acc: 0.9754 - val_loss: 0.0749 - val_acc: 0.9690
Epoch 21/40
 - 4s - loss: 0.0486 - acc: 0.9751 - val_loss: 0.0737 - val_acc: 0.9694
Epoch 22/40
 - 4s - loss: 0.0476 - acc: 0.9756 - val_loss: 0.0741 - val_acc: 0.9693
Epoch 23/40
 - 4s - loss: 0.0473 - acc: 0.9757 - val_loss: 0.0748 - val_acc: 0.9688
Epoch 24/40
 - 4s - loss: 0.0464 - acc: 0.9762 - val_loss: 0.0772 - val_acc: 0.9682
Epoch 25/40
 - 4s - loss: 0.0458 - acc: 0.9758 - val_loss: 0.0798 - val_acc: 0.9683
Epoch 26/40
 - 4s - loss: 0.0450 - acc: 0.9760 - val_loss: 0.0765 - val_acc: 0.9692
Epoch 27/40
 - 4s - loss: 0.0448 - acc: 0.9763 - val_loss: 0.0791 - val_acc: 0.9693
Epoch 28/40
 - 4s - loss: 0.0442 - acc: 0.9763 - val_loss: 0.0803 - val_acc: 0.9685
Epoch 29/40
 - 4s - loss: 0.0436 - acc: 0.9762 - val_loss: 0.0800 - val_acc: 0.9691
Epoch 30/40
 - 4s - loss: 0.0433 - acc: 0.9769 - val_loss: 0.0774 - val_acc: 0.9689
Epoch 31/40
 - 4s - loss: 0.0429 - acc: 0.9763 - val_loss: 0.0798 - val_acc: 0.9689
Epoch 32/40
 - 4s - loss: 0.0424 - acc: 0.9770 - val_loss: 0.0810 - val_acc: 0.9688
Epoch 33/40
 - 4s - loss: 0.0420 - acc: 0.9769 - val_loss: 0.0836 - val_acc: 0.9686
Epoch 34/40
 - 4s - loss: 0.0418 - acc: 0.9767 - val_loss: 0.0787 - val_acc: 0.9698
Epoch 35/40
 - 4s - loss: 0.0414 - acc: 0.9767 - val_loss: 0.0801 - val_acc: 0.9695
Epoch 36/40
 - 4s - loss: 0.0409 - acc: 0.9771 - val_loss: 0.0813 - val_acc: 0.9684
Epoch 37/40
 - 4s - loss: 0.0407 - acc: 0.9771 - val_loss: 0.0823 - val_acc: 0.9695
Epoch 38/40
 - 4s - loss: 0.0407 - acc: 0.9768 - val_loss: 0.0854 - val_acc: 0.9692
Epoch 39/40
 - 4s - loss: 0.0400 - acc: 0.9773 - val_loss: 0.0825 - val_acc: 0.9694
Epoch 40/40
 - 4s - loss: 0.0399 - acc: 0.9772 - val_loss: 0.0829 - val_acc: 0.9686
Epoch 1/40
 - 0s - loss: 0.0720 - acc: 0.9674
Epoch 2/40
 - 0s - loss: 0.0608 - acc: 0.9704
Epoch 3/40
 - 0s - loss: 0.0563 - acc: 0.9710
Epoch 4/40
 - 0s - loss: 0.0514 - acc: 0.9715
Epoch 5/40
 - 0s - loss: 0.0496 - acc: 0.9750
Epoch 6/40
 - 0s - loss: 0.0472 - acc: 0.9751
Epoch 7/40
 - 0s - loss: 0.0449 - acc: 0.9746
Epoch 8/40
 - 0s - loss: 0.0434 - acc: 0.9754
Epoch 9/40
 - 0s - loss: 0.0424 - acc: 0.9750
Epoch 10/40
 - 0s - loss: 0.0416 - acc: 0.9764
Epoch 11/40
 - 0s - loss: 0.0402 - acc: 0.9759
Epoch 12/40
 - 0s - loss: 0.0396 - acc: 0.9770
Epoch 13/40
 - 0s - loss: 0.0381 - acc: 0.9775
Epoch 14/40
 - 0s - loss: 0.0383 - acc: 0.9773
Epoch 15/40
 - 0s - loss: 0.0376 - acc: 0.9774
Epoch 16/40
 - 0s - loss: 0.0369 - acc: 0.9783
Epoch 17/40
 - 0s - loss: 0.0370 - acc: 0.9780
Epoch 18/40
 - 0s - loss: 0.0359 - acc: 0.9779
Epoch 19/40
 - 0s - loss: 0.0359 - acc: 0.9789
Epoch 20/40
 - 0s - loss: 0.0358 - acc: 0.9791
Epoch 21/40
 - 0s - loss: 0.0353 - acc: 0.9788
Epoch 22/40
 - 0s - loss: 0.0359 - acc: 0.9770
Epoch 23/40
 - 0s - loss: 0.0353 - acc: 0.9779
Epoch 24/40
 - 0s - loss: 0.0352 - acc: 0.9788
Epoch 25/40
 - 0s - loss: 0.0355 - acc: 0.9772
Epoch 26/40
 - 0s - loss: 0.0341 - acc: 0.9790
Epoch 27/40
 - 0s - loss: 0.0344 - acc: 0.9789
Epoch 28/40
 - 0s - loss: 0.0339 - acc: 0.9789
Epoch 29/40
 - 0s - loss: 0.0342 - acc: 0.9797
Epoch 30/40
 - 0s - loss: 0.0342 - acc: 0.9793
Epoch 31/40
 - 0s - loss: 0.0340 - acc: 0.9783
Epoch 32/40
 - 0s - loss: 0.0336 - acc: 0.9802
Epoch 33/40
 - 0s - loss: 0.0343 - acc: 0.9789
Epoch 34/40
 - 0s - loss: 0.0339 - acc: 0.9790
Epoch 35/40
 - 0s - loss: 0.0331 - acc: 0.9792
Epoch 36/40
 - 0s - loss: 0.0340 - acc: 0.9779
Epoch 37/40
 - 0s - loss: 0.0334 - acc: 0.9797
Epoch 38/40
 - 0s - loss: 0.0331 - acc: 0.9785
Epoch 39/40
 - 0s - loss: 0.0335 - acc: 0.9778
Epoch 40/40
 - 0s - loss: 0.0332 - acc: 0.9794
# Training time = 0:03:22.163519
# F-Score(Ordinary) = 0.145, Recall: 0.597, Precision: 0.083
# F-Score(lvc) = 0.152, Recall: 0.846, Precision: 0.083
# F-Score(ireflv) = 0.143, Recall: 0.344, Precision: 0.09
# F-Score(id) = 0.143, Recall: 0.882, Precision: 0.078
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 48)        705264      input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 24)        5640        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 192)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 96)           0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 288)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 24)           6936        concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 24)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            200         dropout_8[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1774 - acc: 0.9465 - val_loss: 0.1027 - val_acc: 0.9634
Epoch 2/40
 - 4s - loss: 0.0965 - acc: 0.9633 - val_loss: 0.0856 - val_acc: 0.9662
Epoch 3/40
 - 4s - loss: 0.0822 - acc: 0.9661 - val_loss: 0.0813 - val_acc: 0.9661
Epoch 4/40
 - 4s - loss: 0.0747 - acc: 0.9681 - val_loss: 0.0794 - val_acc: 0.9670
Epoch 5/40
 - 4s - loss: 0.0709 - acc: 0.9690 - val_loss: 0.0764 - val_acc: 0.9673
Epoch 6/40
 - 4s - loss: 0.0680 - acc: 0.9694 - val_loss: 0.0744 - val_acc: 0.9679
Epoch 7/40
 - 4s - loss: 0.0649 - acc: 0.9703 - val_loss: 0.0723 - val_acc: 0.9679
Epoch 8/40
 - 4s - loss: 0.0629 - acc: 0.9706 - val_loss: 0.0755 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0605 - acc: 0.9711 - val_loss: 0.0725 - val_acc: 0.9676
Epoch 10/40
 - 4s - loss: 0.0585 - acc: 0.9723 - val_loss: 0.0705 - val_acc: 0.9686
Epoch 11/40
 - 4s - loss: 0.0569 - acc: 0.9725 - val_loss: 0.0725 - val_acc: 0.9681
Epoch 12/40
 - 4s - loss: 0.0548 - acc: 0.9731 - val_loss: 0.0729 - val_acc: 0.9687
Epoch 13/40
 - 4s - loss: 0.0535 - acc: 0.9735 - val_loss: 0.0701 - val_acc: 0.9690
Epoch 14/40
 - 4s - loss: 0.0523 - acc: 0.9736 - val_loss: 0.0760 - val_acc: 0.9676
Epoch 15/40
 - 4s - loss: 0.0511 - acc: 0.9737 - val_loss: 0.0711 - val_acc: 0.9689
Epoch 16/40
 - 4s - loss: 0.0500 - acc: 0.9740 - val_loss: 0.0718 - val_acc: 0.9687
Epoch 17/40
 - 4s - loss: 0.0489 - acc: 0.9746 - val_loss: 0.0732 - val_acc: 0.9690
Epoch 18/40
 - 4s - loss: 0.0482 - acc: 0.9750 - val_loss: 0.0706 - val_acc: 0.9683
Epoch 19/40
 - 4s - loss: 0.0473 - acc: 0.9752 - val_loss: 0.0772 - val_acc: 0.9681
Epoch 20/40
 - 4s - loss: 0.0461 - acc: 0.9754 - val_loss: 0.0738 - val_acc: 0.9686
Epoch 21/40
 - 4s - loss: 0.0455 - acc: 0.9757 - val_loss: 0.0736 - val_acc: 0.9692
Epoch 22/40
 - 4s - loss: 0.0446 - acc: 0.9761 - val_loss: 0.0757 - val_acc: 0.9688
Epoch 23/40
 - 4s - loss: 0.0443 - acc: 0.9760 - val_loss: 0.0747 - val_acc: 0.9686
Epoch 24/40
 - 4s - loss: 0.0436 - acc: 0.9761 - val_loss: 0.0777 - val_acc: 0.9682
Epoch 25/40
 - 4s - loss: 0.0431 - acc: 0.9763 - val_loss: 0.0791 - val_acc: 0.9685
Epoch 26/40
 - 4s - loss: 0.0428 - acc: 0.9758 - val_loss: 0.0741 - val_acc: 0.9688
Epoch 27/40
 - 4s - loss: 0.0420 - acc: 0.9760 - val_loss: 0.0771 - val_acc: 0.9688
Epoch 28/40
 - 4s - loss: 0.0418 - acc: 0.9766 - val_loss: 0.0751 - val_acc: 0.9690
Epoch 29/40
 - 4s - loss: 0.0412 - acc: 0.9767 - val_loss: 0.0764 - val_acc: 0.9693
Epoch 30/40
 - 4s - loss: 0.0414 - acc: 0.9766 - val_loss: 0.0774 - val_acc: 0.9689
Epoch 31/40
 - 4s - loss: 0.0406 - acc: 0.9766 - val_loss: 0.0784 - val_acc: 0.9686
Epoch 32/40
 - 4s - loss: 0.0403 - acc: 0.9768 - val_loss: 0.0815 - val_acc: 0.9692
Epoch 33/40
 - 4s - loss: 0.0402 - acc: 0.9769 - val_loss: 0.0821 - val_acc: 0.9692
Epoch 34/40
 - 4s - loss: 0.0398 - acc: 0.9770 - val_loss: 0.0775 - val_acc: 0.9693
Epoch 35/40
 - 4s - loss: 0.0397 - acc: 0.9771 - val_loss: 0.0797 - val_acc: 0.9688
Epoch 36/40
 - 4s - loss: 0.0393 - acc: 0.9770 - val_loss: 0.0802 - val_acc: 0.9695
Epoch 37/40
 - 4s - loss: 0.0387 - acc: 0.9776 - val_loss: 0.0802 - val_acc: 0.9699
Epoch 38/40
 - 4s - loss: 0.0387 - acc: 0.9771 - val_loss: 0.0831 - val_acc: 0.9693
Epoch 39/40
 - 4s - loss: 0.0384 - acc: 0.9771 - val_loss: 0.0822 - val_acc: 0.9688
Epoch 40/40
 - 4s - loss: 0.0382 - acc: 0.9771 - val_loss: 0.0847 - val_acc: 0.9689
Epoch 1/40
 - 0s - loss: 0.0698 - acc: 0.9680
Epoch 2/40
 - 0s - loss: 0.0576 - acc: 0.9699
Epoch 3/40
 - 0s - loss: 0.0526 - acc: 0.9728
Epoch 4/40
 - 0s - loss: 0.0504 - acc: 0.9741
Epoch 5/40
 - 0s - loss: 0.0471 - acc: 0.9738
Epoch 6/40
 - 0s - loss: 0.0459 - acc: 0.9742
Epoch 7/40
 - 0s - loss: 0.0435 - acc: 0.9755
Epoch 8/40
 - 0s - loss: 0.0415 - acc: 0.9760
Epoch 9/40
 - 0s - loss: 0.0410 - acc: 0.9770
Epoch 10/40
 - 0s - loss: 0.0387 - acc: 0.9767
Epoch 11/40
 - 0s - loss: 0.0391 - acc: 0.9758
Epoch 12/40
 - 0s - loss: 0.0381 - acc: 0.9769
Epoch 13/40
 - 0s - loss: 0.0374 - acc: 0.9774
Epoch 14/40
 - 0s - loss: 0.0365 - acc: 0.9764
Epoch 15/40
 - 0s - loss: 0.0362 - acc: 0.9776
Epoch 16/40
 - 0s - loss: 0.0353 - acc: 0.9782
Epoch 17/40
 - 0s - loss: 0.0355 - acc: 0.9779
Epoch 18/40
 - 0s - loss: 0.0342 - acc: 0.9793
Epoch 19/40
 - 0s - loss: 0.0350 - acc: 0.9775
Epoch 20/40
 - 0s - loss: 0.0344 - acc: 0.9776
Epoch 21/40
 - 0s - loss: 0.0342 - acc: 0.9785
Epoch 22/40
 - 0s - loss: 0.0339 - acc: 0.9785
Epoch 23/40
 - 0s - loss: 0.0334 - acc: 0.9778
Epoch 24/40
 - 0s - loss: 0.0333 - acc: 0.9786
Epoch 25/40
 - 0s - loss: 0.0333 - acc: 0.9781
Epoch 26/40
 - 0s - loss: 0.0331 - acc: 0.9784
Epoch 27/40
 - 0s - loss: 0.0326 - acc: 0.9778
Epoch 28/40
 - 0s - loss: 0.0328 - acc: 0.9795
Epoch 29/40
 - 0s - loss: 0.0333 - acc: 0.9774
Epoch 30/40
 - 0s - loss: 0.0328 - acc: 0.9789
Epoch 31/40
 - 0s - loss: 0.0327 - acc: 0.9782
Epoch 32/40
 - 0s - loss: 0.0325 - acc: 0.9790
Epoch 33/40
 - 0s - loss: 0.0319 - acc: 0.9807
Epoch 34/40
 - 0s - loss: 0.0318 - acc: 0.9780
Epoch 35/40
 - 0s - loss: 0.0320 - acc: 0.9790
Epoch 36/40
 - 0s - loss: 0.0321 - acc: 0.9799
Epoch 37/40
 - 0s - loss: 0.0323 - acc: 0.9787
Epoch 38/40
 - 0s - loss: 0.0319 - acc: 0.9796
Epoch 39/40
 - 0s - loss: 0.0321 - acc: 0.9786
Epoch 40/40
 - 0s - loss: 0.0317 - acc: 0.9785
# Training time = 0:03:22.145950
# F-Score(Ordinary) = 0.255, Recall: 0.386, Precision: 0.19
# F-Score(lvc) = 0.4, Recall: 0.327, Precision: 0.515
# F-Score(ireflv) = 0.047, Recall: 0.429, Precision: 0.025
# F-Score(id) = 0.051, Recall: 1.0, Precision: 0.026
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 48)        705264      input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 24)        5640        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 192)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 96)           0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 288)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 24)           6936        concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 24)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            200         dropout_9[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1928 - acc: 0.9435 - val_loss: 0.1055 - val_acc: 0.9630
Epoch 2/40
 - 4s - loss: 0.1011 - acc: 0.9629 - val_loss: 0.0876 - val_acc: 0.9654
Epoch 3/40
 - 4s - loss: 0.0849 - acc: 0.9659 - val_loss: 0.0803 - val_acc: 0.9669
Epoch 4/40
 - 4s - loss: 0.0776 - acc: 0.9672 - val_loss: 0.0783 - val_acc: 0.9678
Epoch 5/40
 - 4s - loss: 0.0728 - acc: 0.9686 - val_loss: 0.0759 - val_acc: 0.9675
Epoch 6/40
 - 4s - loss: 0.0696 - acc: 0.9695 - val_loss: 0.0738 - val_acc: 0.9679
Epoch 7/40
 - 4s - loss: 0.0671 - acc: 0.9704 - val_loss: 0.0746 - val_acc: 0.9683
Epoch 8/40
 - 4s - loss: 0.0645 - acc: 0.9708 - val_loss: 0.0761 - val_acc: 0.9676
Epoch 9/40
 - 4s - loss: 0.0624 - acc: 0.9714 - val_loss: 0.0723 - val_acc: 0.9684
Epoch 10/40
 - 4s - loss: 0.0605 - acc: 0.9721 - val_loss: 0.0712 - val_acc: 0.9682
Epoch 11/40
 - 4s - loss: 0.0588 - acc: 0.9728 - val_loss: 0.0714 - val_acc: 0.9686
Epoch 12/40
 - 4s - loss: 0.0575 - acc: 0.9730 - val_loss: 0.0707 - val_acc: 0.9689
Epoch 13/40
 - 4s - loss: 0.0554 - acc: 0.9736 - val_loss: 0.0731 - val_acc: 0.9686
Epoch 14/40
 - 4s - loss: 0.0545 - acc: 0.9741 - val_loss: 0.0724 - val_acc: 0.9688
Epoch 15/40
 - 4s - loss: 0.0533 - acc: 0.9741 - val_loss: 0.0724 - val_acc: 0.9690
Epoch 16/40
 - 4s - loss: 0.0520 - acc: 0.9746 - val_loss: 0.0718 - val_acc: 0.9686
Epoch 17/40
 - 4s - loss: 0.0509 - acc: 0.9750 - val_loss: 0.0728 - val_acc: 0.9691
Epoch 18/40
 - 4s - loss: 0.0498 - acc: 0.9755 - val_loss: 0.0742 - val_acc: 0.9685
Epoch 19/40
 - 4s - loss: 0.0491 - acc: 0.9751 - val_loss: 0.0772 - val_acc: 0.9683
Epoch 20/40
 - 4s - loss: 0.0484 - acc: 0.9756 - val_loss: 0.0753 - val_acc: 0.9687
Epoch 21/40
 - 4s - loss: 0.0475 - acc: 0.9760 - val_loss: 0.0775 - val_acc: 0.9687
Epoch 22/40
 - 4s - loss: 0.0469 - acc: 0.9757 - val_loss: 0.0746 - val_acc: 0.9696
Epoch 23/40
 - 4s - loss: 0.0463 - acc: 0.9759 - val_loss: 0.0740 - val_acc: 0.9692
Epoch 24/40
 - 4s - loss: 0.0459 - acc: 0.9758 - val_loss: 0.0740 - val_acc: 0.9694
Epoch 25/40
 - 4s - loss: 0.0449 - acc: 0.9762 - val_loss: 0.0762 - val_acc: 0.9692
Epoch 26/40
 - 4s - loss: 0.0449 - acc: 0.9763 - val_loss: 0.0757 - val_acc: 0.9692
Epoch 27/40
 - 4s - loss: 0.0441 - acc: 0.9768 - val_loss: 0.0766 - val_acc: 0.9694
Epoch 28/40
 - 4s - loss: 0.0437 - acc: 0.9762 - val_loss: 0.0773 - val_acc: 0.9690
Epoch 29/40
 - 4s - loss: 0.0434 - acc: 0.9767 - val_loss: 0.0775 - val_acc: 0.9697
Epoch 30/40
 - 4s - loss: 0.0430 - acc: 0.9762 - val_loss: 0.0761 - val_acc: 0.9693
Epoch 31/40
 - 4s - loss: 0.0425 - acc: 0.9768 - val_loss: 0.0772 - val_acc: 0.9687
Epoch 32/40
 - 4s - loss: 0.0423 - acc: 0.9765 - val_loss: 0.0793 - val_acc: 0.9688
Epoch 33/40
 - 4s - loss: 0.0418 - acc: 0.9767 - val_loss: 0.0824 - val_acc: 0.9691
Epoch 34/40
 - 4s - loss: 0.0416 - acc: 0.9766 - val_loss: 0.0803 - val_acc: 0.9690
Epoch 35/40
 - 4s - loss: 0.0415 - acc: 0.9767 - val_loss: 0.0807 - val_acc: 0.9691
Epoch 36/40
 - 4s - loss: 0.0413 - acc: 0.9767 - val_loss: 0.0826 - val_acc: 0.9692
Epoch 37/40
 - 4s - loss: 0.0407 - acc: 0.9771 - val_loss: 0.0828 - val_acc: 0.9684
Epoch 38/40
 - 4s - loss: 0.0406 - acc: 0.9770 - val_loss: 0.0834 - val_acc: 0.9691
Epoch 39/40
 - 4s - loss: 0.0403 - acc: 0.9768 - val_loss: 0.0836 - val_acc: 0.9687
Epoch 40/40
 - 4s - loss: 0.0399 - acc: 0.9771 - val_loss: 0.0830 - val_acc: 0.9695
Epoch 1/40
 - 0s - loss: 0.0703 - acc: 0.9685
Epoch 2/40
 - 0s - loss: 0.0591 - acc: 0.9706
Epoch 3/40
 - 0s - loss: 0.0542 - acc: 0.9724
Epoch 4/40
 - 0s - loss: 0.0515 - acc: 0.9737
Epoch 5/40
 - 0s - loss: 0.0479 - acc: 0.9742
Epoch 6/40
 - 0s - loss: 0.0469 - acc: 0.9745
Epoch 7/40
 - 0s - loss: 0.0454 - acc: 0.9761
Epoch 8/40
 - 0s - loss: 0.0440 - acc: 0.9745
Epoch 9/40
 - 0s - loss: 0.0414 - acc: 0.9763
Epoch 10/40
 - 0s - loss: 0.0409 - acc: 0.9763
Epoch 11/40
 - 0s - loss: 0.0399 - acc: 0.9767
Epoch 12/40
 - 0s - loss: 0.0399 - acc: 0.9763
Epoch 13/40
 - 0s - loss: 0.0388 - acc: 0.9780
Epoch 14/40
 - 0s - loss: 0.0383 - acc: 0.9773
Epoch 15/40
 - 0s - loss: 0.0368 - acc: 0.9786
Epoch 16/40
 - 0s - loss: 0.0374 - acc: 0.9770
Epoch 17/40
 - 0s - loss: 0.0369 - acc: 0.9784
Epoch 18/40
 - 0s - loss: 0.0368 - acc: 0.9782
Epoch 19/40
 - 0s - loss: 0.0360 - acc: 0.9789
Epoch 20/40
 - 0s - loss: 0.0362 - acc: 0.9789
Epoch 21/40
 - 0s - loss: 0.0356 - acc: 0.9785
Epoch 22/40
 - 0s - loss: 0.0351 - acc: 0.9794
Epoch 23/40
 - 0s - loss: 0.0357 - acc: 0.9778
Epoch 24/40
 - 0s - loss: 0.0353 - acc: 0.9786
Epoch 25/40
 - 0s - loss: 0.0347 - acc: 0.9801
Epoch 26/40
 - 0s - loss: 0.0345 - acc: 0.9782
Epoch 27/40
 - 0s - loss: 0.0349 - acc: 0.9790
Epoch 28/40
 - 0s - loss: 0.0344 - acc: 0.9784
Epoch 29/40
 - 0s - loss: 0.0340 - acc: 0.9791
Epoch 30/40
 - 0s - loss: 0.0340 - acc: 0.9792
Epoch 31/40
 - 0s - loss: 0.0346 - acc: 0.9793
Epoch 32/40
 - 0s - loss: 0.0342 - acc: 0.9783
Epoch 33/40
 - 0s - loss: 0.0338 - acc: 0.9786
Epoch 34/40
 - 0s - loss: 0.0334 - acc: 0.9796
Epoch 35/40
 - 0s - loss: 0.0346 - acc: 0.9778
Epoch 36/40
 - 0s - loss: 0.0334 - acc: 0.9782
Epoch 37/40
 - 0s - loss: 0.0338 - acc: 0.9775
Epoch 38/40
 - 0s - loss: 0.0331 - acc: 0.9782
Epoch 39/40
 - 0s - loss: 0.0333 - acc: 0.9789
Epoch 40/40
 - 0s - loss: 0.0333 - acc: 0.9785
# Training time = 0:03:21.852529
# F-Score(Ordinary) = 0.217, Recall: 0.412, Precision: 0.148
# F-Score(lvc) = 0.404, Recall: 0.38, Precision: 0.432
# F-Score(ireflv) = 0.061, Recall: 0.444, Precision: 0.033
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 48)        705264      input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 24)        5640        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 192)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 96)           0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 288)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 24)           6936        concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 24)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            200         dropout_10[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1831 - acc: 0.9439 - val_loss: 0.1075 - val_acc: 0.9619
Epoch 2/40
 - 4s - loss: 0.1028 - acc: 0.9626 - val_loss: 0.0909 - val_acc: 0.9661
Epoch 3/40
 - 4s - loss: 0.0865 - acc: 0.9656 - val_loss: 0.0806 - val_acc: 0.9673
Epoch 4/40
 - 4s - loss: 0.0793 - acc: 0.9673 - val_loss: 0.0780 - val_acc: 0.9677
Epoch 5/40
 - 4s - loss: 0.0747 - acc: 0.9686 - val_loss: 0.0756 - val_acc: 0.9677
Epoch 6/40
 - 4s - loss: 0.0708 - acc: 0.9692 - val_loss: 0.0745 - val_acc: 0.9682
Epoch 7/40
 - 4s - loss: 0.0684 - acc: 0.9696 - val_loss: 0.0744 - val_acc: 0.9681
Epoch 8/40
 - 4s - loss: 0.0658 - acc: 0.9707 - val_loss: 0.0744 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0638 - acc: 0.9709 - val_loss: 0.0738 - val_acc: 0.9676
Epoch 10/40
 - 4s - loss: 0.0613 - acc: 0.9720 - val_loss: 0.0722 - val_acc: 0.9687
Epoch 11/40
 - 4s - loss: 0.0601 - acc: 0.9720 - val_loss: 0.0736 - val_acc: 0.9684
Epoch 12/40
 - 4s - loss: 0.0582 - acc: 0.9726 - val_loss: 0.0698 - val_acc: 0.9689
Epoch 13/40
 - 4s - loss: 0.0563 - acc: 0.9734 - val_loss: 0.0712 - val_acc: 0.9688
Epoch 14/40
 - 4s - loss: 0.0552 - acc: 0.9735 - val_loss: 0.0721 - val_acc: 0.9694
Epoch 15/40
 - 4s - loss: 0.0533 - acc: 0.9740 - val_loss: 0.0698 - val_acc: 0.9687
Epoch 16/40
 - 4s - loss: 0.0524 - acc: 0.9744 - val_loss: 0.0731 - val_acc: 0.9687
Epoch 17/40
 - 4s - loss: 0.0513 - acc: 0.9746 - val_loss: 0.0715 - val_acc: 0.9690
Epoch 18/40
 - 4s - loss: 0.0505 - acc: 0.9746 - val_loss: 0.0730 - val_acc: 0.9691
Epoch 19/40
 - 4s - loss: 0.0494 - acc: 0.9752 - val_loss: 0.0744 - val_acc: 0.9680
Epoch 20/40
 - 4s - loss: 0.0492 - acc: 0.9751 - val_loss: 0.0718 - val_acc: 0.9693
Epoch 21/40
 - 4s - loss: 0.0481 - acc: 0.9752 - val_loss: 0.0729 - val_acc: 0.9687
Epoch 22/40
 - 4s - loss: 0.0472 - acc: 0.9756 - val_loss: 0.0721 - val_acc: 0.9687
Epoch 23/40
 - 4s - loss: 0.0466 - acc: 0.9758 - val_loss: 0.0726 - val_acc: 0.9691
Epoch 24/40
 - 4s - loss: 0.0453 - acc: 0.9763 - val_loss: 0.0743 - val_acc: 0.9693
Epoch 25/40
 - 4s - loss: 0.0450 - acc: 0.9761 - val_loss: 0.0736 - val_acc: 0.9688
Epoch 26/40
 - 4s - loss: 0.0447 - acc: 0.9760 - val_loss: 0.0789 - val_acc: 0.9692
Epoch 27/40
 - 4s - loss: 0.0443 - acc: 0.9762 - val_loss: 0.0758 - val_acc: 0.9694
Epoch 28/40
 - 4s - loss: 0.0435 - acc: 0.9767 - val_loss: 0.0764 - val_acc: 0.9692
Epoch 29/40
 - 4s - loss: 0.0434 - acc: 0.9762 - val_loss: 0.0762 - val_acc: 0.9685
Epoch 30/40
 - 4s - loss: 0.0428 - acc: 0.9764 - val_loss: 0.0774 - val_acc: 0.9695
Epoch 31/40
 - 4s - loss: 0.0427 - acc: 0.9763 - val_loss: 0.0774 - val_acc: 0.9694
Epoch 32/40
 - 4s - loss: 0.0419 - acc: 0.9766 - val_loss: 0.0759 - val_acc: 0.9692
Epoch 33/40
 - 4s - loss: 0.0417 - acc: 0.9765 - val_loss: 0.0769 - val_acc: 0.9690
Epoch 34/40
 - 4s - loss: 0.0416 - acc: 0.9766 - val_loss: 0.0830 - val_acc: 0.9687
Epoch 35/40
 - 4s - loss: 0.0410 - acc: 0.9768 - val_loss: 0.0809 - val_acc: 0.9685
Epoch 36/40
 - 4s - loss: 0.0410 - acc: 0.9768 - val_loss: 0.0773 - val_acc: 0.9697
Epoch 37/40
 - 4s - loss: 0.0407 - acc: 0.9767 - val_loss: 0.0811 - val_acc: 0.9693
Epoch 38/40
 - 4s - loss: 0.0403 - acc: 0.9769 - val_loss: 0.0787 - val_acc: 0.9694
Epoch 39/40
 - 4s - loss: 0.0400 - acc: 0.9767 - val_loss: 0.0814 - val_acc: 0.9687
Epoch 40/40
 - 4s - loss: 0.0398 - acc: 0.9768 - val_loss: 0.0831 - val_acc: 0.9693
Epoch 1/40
 - 0s - loss: 0.0711 - acc: 0.9672
Epoch 2/40
 - 0s - loss: 0.0595 - acc: 0.9703
Epoch 3/40
 - 0s - loss: 0.0544 - acc: 0.9726
Epoch 4/40
 - 0s - loss: 0.0510 - acc: 0.9737
Epoch 5/40
 - 0s - loss: 0.0477 - acc: 0.9749
Epoch 6/40
 - 0s - loss: 0.0463 - acc: 0.9743
Epoch 7/40
 - 0s - loss: 0.0440 - acc: 0.9761
Epoch 8/40
 - 0s - loss: 0.0431 - acc: 0.9753
Epoch 9/40
 - 0s - loss: 0.0416 - acc: 0.9758
Epoch 10/40
 - 0s - loss: 0.0405 - acc: 0.9765
Epoch 11/40
 - 0s - loss: 0.0400 - acc: 0.9770
Epoch 12/40
 - 0s - loss: 0.0385 - acc: 0.9770
Epoch 13/40
 - 0s - loss: 0.0381 - acc: 0.9781
Epoch 14/40
 - 0s - loss: 0.0370 - acc: 0.9778
Epoch 15/40
 - 0s - loss: 0.0369 - acc: 0.9784
Epoch 16/40
 - 0s - loss: 0.0362 - acc: 0.9783
Epoch 17/40
 - 0s - loss: 0.0364 - acc: 0.9774
Epoch 18/40
 - 0s - loss: 0.0359 - acc: 0.9767
Epoch 19/40
 - 0s - loss: 0.0350 - acc: 0.9785
Epoch 20/40
 - 0s - loss: 0.0357 - acc: 0.9776
Epoch 21/40
 - 0s - loss: 0.0358 - acc: 0.9774
Epoch 22/40
 - 0s - loss: 0.0345 - acc: 0.9788
Epoch 23/40
 - 0s - loss: 0.0348 - acc: 0.9779
Epoch 24/40
 - 0s - loss: 0.0349 - acc: 0.9784
Epoch 25/40
 - 0s - loss: 0.0343 - acc: 0.9789
Epoch 26/40
 - 0s - loss: 0.0342 - acc: 0.9788
Epoch 27/40
 - 0s - loss: 0.0347 - acc: 0.9783
Epoch 28/40
 - 0s - loss: 0.0344 - acc: 0.9784
Epoch 29/40
 - 0s - loss: 0.0338 - acc: 0.9788
Epoch 30/40
 - 0s - loss: 0.0341 - acc: 0.9794
Epoch 31/40
 - 0s - loss: 0.0337 - acc: 0.9774
Epoch 32/40
 - 0s - loss: 0.0336 - acc: 0.9782
Epoch 33/40
 - 0s - loss: 0.0332 - acc: 0.9783
Epoch 34/40
 - 0s - loss: 0.0328 - acc: 0.9795
Epoch 35/40
 - 0s - loss: 0.0331 - acc: 0.9779
Epoch 36/40
 - 0s - loss: 0.0332 - acc: 0.9800
Epoch 37/40
 - 0s - loss: 0.0328 - acc: 0.9789
Epoch 38/40
 - 0s - loss: 0.0327 - acc: 0.9785
Epoch 39/40
 - 0s - loss: 0.0332 - acc: 0.9784
Epoch 40/40
 - 0s - loss: 0.0330 - acc: 0.9783
# Training time = 0:03:20.521904
# F-Score(Ordinary) = 0.504, Recall: 0.698, Precision: 0.394
# F-Score(lvc) = 0.37, Recall: 1.0, Precision: 0.227
# F-Score(ireflv) = 0.701, Recall: 0.92, Precision: 0.566
# F-Score(id) = 0.447, Recall: 0.517, Precision: 0.394
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 48)        705264      input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 24)        5640        input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 192)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 96)           0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 288)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 24)           6936        concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 24)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            200         dropout_11[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1480 - acc: 0.9516 - val_loss: 0.0935 - val_acc: 0.9647
Epoch 2/40
 - 4s - loss: 0.0854 - acc: 0.9655 - val_loss: 0.0822 - val_acc: 0.9664
Epoch 3/40
 - 4s - loss: 0.0744 - acc: 0.9681 - val_loss: 0.0771 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0687 - acc: 0.9694 - val_loss: 0.0742 - val_acc: 0.9675
Epoch 5/40
 - 4s - loss: 0.0645 - acc: 0.9707 - val_loss: 0.0722 - val_acc: 0.9677
Epoch 6/40
 - 4s - loss: 0.0610 - acc: 0.9711 - val_loss: 0.0714 - val_acc: 0.9685
Epoch 7/40
 - 4s - loss: 0.0584 - acc: 0.9720 - val_loss: 0.0706 - val_acc: 0.9685
Epoch 8/40
 - 4s - loss: 0.0563 - acc: 0.9719 - val_loss: 0.0729 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0537 - acc: 0.9734 - val_loss: 0.0760 - val_acc: 0.9688
Epoch 10/40
 - 4s - loss: 0.0518 - acc: 0.9738 - val_loss: 0.0733 - val_acc: 0.9690
Epoch 11/40
 - 4s - loss: 0.0501 - acc: 0.9739 - val_loss: 0.0740 - val_acc: 0.9685
Epoch 12/40
 - 4s - loss: 0.0484 - acc: 0.9749 - val_loss: 0.0715 - val_acc: 0.9691
Epoch 13/40
 - 4s - loss: 0.0475 - acc: 0.9746 - val_loss: 0.0737 - val_acc: 0.9693
Epoch 14/40
 - 4s - loss: 0.0459 - acc: 0.9757 - val_loss: 0.0790 - val_acc: 0.9686
Epoch 15/40
 - 4s - loss: 0.0454 - acc: 0.9754 - val_loss: 0.0785 - val_acc: 0.9687
Epoch 16/40
 - 4s - loss: 0.0442 - acc: 0.9754 - val_loss: 0.0766 - val_acc: 0.9688
Epoch 17/40
 - 4s - loss: 0.0434 - acc: 0.9758 - val_loss: 0.0829 - val_acc: 0.9683
Epoch 18/40
 - 4s - loss: 0.0426 - acc: 0.9759 - val_loss: 0.0811 - val_acc: 0.9690
Epoch 19/40
 - 4s - loss: 0.0419 - acc: 0.9762 - val_loss: 0.0828 - val_acc: 0.9682
Epoch 20/40
 - 4s - loss: 0.0419 - acc: 0.9760 - val_loss: 0.0833 - val_acc: 0.9689
Epoch 21/40
 - 4s - loss: 0.0412 - acc: 0.9763 - val_loss: 0.0840 - val_acc: 0.9671
Epoch 22/40
 - 4s - loss: 0.0401 - acc: 0.9770 - val_loss: 0.0816 - val_acc: 0.9684
Epoch 23/40
 - 4s - loss: 0.0403 - acc: 0.9765 - val_loss: 0.0815 - val_acc: 0.9695
Epoch 24/40
 - 4s - loss: 0.0395 - acc: 0.9766 - val_loss: 0.0818 - val_acc: 0.9691
Epoch 25/40
 - 4s - loss: 0.0391 - acc: 0.9769 - val_loss: 0.0892 - val_acc: 0.9690
Epoch 26/40
 - 4s - loss: 0.0386 - acc: 0.9772 - val_loss: 0.0875 - val_acc: 0.9688
Epoch 27/40
 - 4s - loss: 0.0388 - acc: 0.9771 - val_loss: 0.0873 - val_acc: 0.9685
Epoch 28/40
 - 4s - loss: 0.0382 - acc: 0.9772 - val_loss: 0.0868 - val_acc: 0.9695
Epoch 29/40
 - 4s - loss: 0.0380 - acc: 0.9773 - val_loss: 0.0892 - val_acc: 0.9691
Epoch 30/40
 - 4s - loss: 0.0374 - acc: 0.9771 - val_loss: 0.0904 - val_acc: 0.9694
Epoch 31/40
 - 4s - loss: 0.0373 - acc: 0.9772 - val_loss: 0.0878 - val_acc: 0.9699
Epoch 32/40
 - 4s - loss: 0.0369 - acc: 0.9774 - val_loss: 0.0910 - val_acc: 0.9691
Epoch 33/40
 - 4s - loss: 0.0370 - acc: 0.9775 - val_loss: 0.0945 - val_acc: 0.9691
Epoch 34/40
 - 4s - loss: 0.0368 - acc: 0.9772 - val_loss: 0.0936 - val_acc: 0.9694
Epoch 35/40
 - 4s - loss: 0.0366 - acc: 0.9776 - val_loss: 0.0926 - val_acc: 0.9698
Epoch 36/40
 - 4s - loss: 0.0365 - acc: 0.9774 - val_loss: 0.0908 - val_acc: 0.9681
Epoch 37/40
 - 4s - loss: 0.0362 - acc: 0.9775 - val_loss: 0.0984 - val_acc: 0.9687
Epoch 38/40
 - 4s - loss: 0.0362 - acc: 0.9774 - val_loss: 0.0889 - val_acc: 0.9688
Epoch 39/40
 - 4s - loss: 0.0359 - acc: 0.9775 - val_loss: 0.0925 - val_acc: 0.9697
Epoch 40/40
 - 4s - loss: 0.0359 - acc: 0.9780 - val_loss: 0.0957 - val_acc: 0.9700
Epoch 1/40
 - 0s - loss: 0.0734 - acc: 0.9680
Epoch 2/40
 - 0s - loss: 0.0573 - acc: 0.9713
Epoch 3/40
 - 0s - loss: 0.0532 - acc: 0.9716
Epoch 4/40
 - 0s - loss: 0.0469 - acc: 0.9732
Epoch 5/40
 - 0s - loss: 0.0448 - acc: 0.9744
Epoch 6/40
 - 0s - loss: 0.0417 - acc: 0.9755
Epoch 7/40
 - 0s - loss: 0.0411 - acc: 0.9752
Epoch 8/40
 - 0s - loss: 0.0396 - acc: 0.9746
Epoch 9/40
 - 0s - loss: 0.0385 - acc: 0.9764
Epoch 10/40
 - 0s - loss: 0.0377 - acc: 0.9764
Epoch 11/40
 - 0s - loss: 0.0356 - acc: 0.9779
Epoch 12/40
 - 0s - loss: 0.0360 - acc: 0.9777
Epoch 13/40
 - 0s - loss: 0.0354 - acc: 0.9773
Epoch 14/40
 - 0s - loss: 0.0340 - acc: 0.9781
Epoch 15/40
 - 0s - loss: 0.0346 - acc: 0.9773
Epoch 16/40
 - 0s - loss: 0.0336 - acc: 0.9785
Epoch 17/40
 - 0s - loss: 0.0334 - acc: 0.9769
Epoch 18/40
 - 0s - loss: 0.0330 - acc: 0.9771
Epoch 19/40
 - 0s - loss: 0.0327 - acc: 0.9781
Epoch 20/40
 - 0s - loss: 0.0328 - acc: 0.9779
Epoch 21/40
 - 0s - loss: 0.0323 - acc: 0.9781
Epoch 22/40
 - 0s - loss: 0.0324 - acc: 0.9796
Epoch 23/40
 - 0s - loss: 0.0328 - acc: 0.9769
Epoch 24/40
 - 0s - loss: 0.0321 - acc: 0.9791
Epoch 25/40
 - 0s - loss: 0.0315 - acc: 0.9788
Epoch 26/40
 - 0s - loss: 0.0327 - acc: 0.9779
Epoch 27/40
 - 0s - loss: 0.0322 - acc: 0.9783
Epoch 28/40
 - 0s - loss: 0.0318 - acc: 0.9784
Epoch 29/40
 - 0s - loss: 0.0319 - acc: 0.9783
Epoch 30/40
 - 0s - loss: 0.0313 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0304 - acc: 0.9805
Epoch 32/40
 - 0s - loss: 0.0314 - acc: 0.9800
Epoch 33/40
 - 0s - loss: 0.0313 - acc: 0.9780
Epoch 34/40
 - 0s - loss: 0.0316 - acc: 0.9797
Epoch 35/40
 - 0s - loss: 0.0313 - acc: 0.9792
Epoch 36/40
 - 0s - loss: 0.0308 - acc: 0.9790
Epoch 37/40
 - 0s - loss: 0.0310 - acc: 0.9811
Epoch 38/40
 - 0s - loss: 0.0307 - acc: 0.9790
Epoch 39/40
 - 0s - loss: 0.0306 - acc: 0.9778
Epoch 40/40
 - 0s - loss: 0.0310 - acc: 0.9773
# Training time = 0:03:21.302306
# F-Score(Ordinary) = 0.1, Recall: 0.356, Precision: 0.058
# F-Score(ireflv) = 0.193, Recall: 0.609, Precision: 0.115
# F-Score(id) = 0.099, Recall: 0.245, Precision: 0.062
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 48)        705264      input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 24)        5640        input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 192)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 96)           0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 288)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 24)           6936        concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 24)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            200         dropout_12[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1482 - acc: 0.9520 - val_loss: 0.0901 - val_acc: 0.9662
Epoch 2/40
 - 4s - loss: 0.0867 - acc: 0.9659 - val_loss: 0.0832 - val_acc: 0.9665
Epoch 3/40
 - 4s - loss: 0.0756 - acc: 0.9683 - val_loss: 0.0752 - val_acc: 0.9675
Epoch 4/40
 - 4s - loss: 0.0700 - acc: 0.9695 - val_loss: 0.0752 - val_acc: 0.9681
Epoch 5/40
 - 4s - loss: 0.0659 - acc: 0.9703 - val_loss: 0.0741 - val_acc: 0.9685
Epoch 6/40
 - 4s - loss: 0.0630 - acc: 0.9708 - val_loss: 0.0765 - val_acc: 0.9673
Epoch 7/40
 - 4s - loss: 0.0599 - acc: 0.9719 - val_loss: 0.0743 - val_acc: 0.9679
Epoch 8/40
 - 4s - loss: 0.0577 - acc: 0.9724 - val_loss: 0.0742 - val_acc: 0.9686
Epoch 9/40
 - 4s - loss: 0.0560 - acc: 0.9731 - val_loss: 0.0713 - val_acc: 0.9688
Epoch 10/40
 - 4s - loss: 0.0540 - acc: 0.9738 - val_loss: 0.0739 - val_acc: 0.9696
Epoch 11/40
 - 4s - loss: 0.0521 - acc: 0.9741 - val_loss: 0.0762 - val_acc: 0.9695
Epoch 12/40
 - 4s - loss: 0.0510 - acc: 0.9746 - val_loss: 0.0804 - val_acc: 0.9667
Epoch 13/40
 - 4s - loss: 0.0496 - acc: 0.9748 - val_loss: 0.0777 - val_acc: 0.9686
Epoch 14/40
 - 4s - loss: 0.0480 - acc: 0.9753 - val_loss: 0.0803 - val_acc: 0.9698
Epoch 15/40
 - 4s - loss: 0.0470 - acc: 0.9755 - val_loss: 0.0766 - val_acc: 0.9700
Epoch 16/40
 - 4s - loss: 0.0461 - acc: 0.9758 - val_loss: 0.0773 - val_acc: 0.9684
Epoch 17/40
 - 4s - loss: 0.0452 - acc: 0.9756 - val_loss: 0.0761 - val_acc: 0.9695
Epoch 18/40
 - 4s - loss: 0.0447 - acc: 0.9763 - val_loss: 0.0787 - val_acc: 0.9695
Epoch 19/40
 - 4s - loss: 0.0442 - acc: 0.9762 - val_loss: 0.0781 - val_acc: 0.9690
Epoch 20/40
 - 4s - loss: 0.0434 - acc: 0.9763 - val_loss: 0.0813 - val_acc: 0.9694
Epoch 21/40
 - 4s - loss: 0.0429 - acc: 0.9762 - val_loss: 0.0803 - val_acc: 0.9694
Epoch 22/40
 - 4s - loss: 0.0421 - acc: 0.9766 - val_loss: 0.0812 - val_acc: 0.9696
Epoch 23/40
 - 4s - loss: 0.0421 - acc: 0.9763 - val_loss: 0.0816 - val_acc: 0.9690
Epoch 24/40
 - 4s - loss: 0.0415 - acc: 0.9767 - val_loss: 0.0854 - val_acc: 0.9685
Epoch 25/40
 - 4s - loss: 0.0411 - acc: 0.9764 - val_loss: 0.0914 - val_acc: 0.9683
Epoch 26/40
 - 4s - loss: 0.0405 - acc: 0.9769 - val_loss: 0.0841 - val_acc: 0.9696
Epoch 27/40
 - 4s - loss: 0.0405 - acc: 0.9765 - val_loss: 0.0830 - val_acc: 0.9696
Epoch 28/40
 - 4s - loss: 0.0398 - acc: 0.9770 - val_loss: 0.0899 - val_acc: 0.9687
Epoch 29/40
 - 4s - loss: 0.0397 - acc: 0.9770 - val_loss: 0.0951 - val_acc: 0.9690
Epoch 30/40
 - 4s - loss: 0.0395 - acc: 0.9771 - val_loss: 0.0870 - val_acc: 0.9689
Epoch 31/40
 - 4s - loss: 0.0392 - acc: 0.9771 - val_loss: 0.0895 - val_acc: 0.9688
Epoch 32/40
 - 4s - loss: 0.0388 - acc: 0.9773 - val_loss: 0.0935 - val_acc: 0.9689
Epoch 33/40
 - 4s - loss: 0.0387 - acc: 0.9773 - val_loss: 0.0932 - val_acc: 0.9693
Epoch 34/40
 - 4s - loss: 0.0385 - acc: 0.9771 - val_loss: 0.0926 - val_acc: 0.9701
Epoch 35/40
 - 4s - loss: 0.0382 - acc: 0.9773 - val_loss: 0.0901 - val_acc: 0.9701
Epoch 36/40
 - 4s - loss: 0.0376 - acc: 0.9775 - val_loss: 0.0897 - val_acc: 0.9692
Epoch 37/40
 - 4s - loss: 0.0375 - acc: 0.9776 - val_loss: 0.0924 - val_acc: 0.9690
Epoch 38/40
 - 4s - loss: 0.0377 - acc: 0.9771 - val_loss: 0.0959 - val_acc: 0.9691
Epoch 39/40
 - 4s - loss: 0.0371 - acc: 0.9773 - val_loss: 0.0938 - val_acc: 0.9695
Epoch 40/40
 - 4s - loss: 0.0369 - acc: 0.9776 - val_loss: 0.0968 - val_acc: 0.9688
Epoch 1/40
 - 0s - loss: 0.0763 - acc: 0.9665
Epoch 2/40
 - 0s - loss: 0.0599 - acc: 0.9704
Epoch 3/40
 - 0s - loss: 0.0538 - acc: 0.9718
Epoch 4/40
 - 0s - loss: 0.0474 - acc: 0.9740
Epoch 5/40
 - 0s - loss: 0.0459 - acc: 0.9730
Epoch 6/40
 - 0s - loss: 0.0435 - acc: 0.9750
Epoch 7/40
 - 0s - loss: 0.0410 - acc: 0.9755
Epoch 8/40
 - 0s - loss: 0.0393 - acc: 0.9766
Epoch 9/40
 - 0s - loss: 0.0382 - acc: 0.9759
Epoch 10/40
 - 0s - loss: 0.0380 - acc: 0.9767
Epoch 11/40
 - 0s - loss: 0.0357 - acc: 0.9766
Epoch 12/40
 - 0s - loss: 0.0356 - acc: 0.9778
Epoch 13/40
 - 0s - loss: 0.0349 - acc: 0.9784
Epoch 14/40
 - 0s - loss: 0.0357 - acc: 0.9777
Epoch 15/40
 - 0s - loss: 0.0348 - acc: 0.9778
Epoch 16/40
 - 0s - loss: 0.0343 - acc: 0.9784
Epoch 17/40
 - 0s - loss: 0.0337 - acc: 0.9790
Epoch 18/40
 - 0s - loss: 0.0334 - acc: 0.9776
Epoch 19/40
 - 0s - loss: 0.0338 - acc: 0.9785
Epoch 20/40
 - 0s - loss: 0.0339 - acc: 0.9786
Epoch 21/40
 - 0s - loss: 0.0331 - acc: 0.9790
Epoch 22/40
 - 0s - loss: 0.0336 - acc: 0.9779
Epoch 23/40
 - 0s - loss: 0.0331 - acc: 0.9778
Epoch 24/40
 - 0s - loss: 0.0328 - acc: 0.9788
Epoch 25/40
 - 0s - loss: 0.0333 - acc: 0.9777
Epoch 26/40
 - 0s - loss: 0.0326 - acc: 0.9786
Epoch 27/40
 - 0s - loss: 0.0325 - acc: 0.9792
Epoch 28/40
 - 0s - loss: 0.0323 - acc: 0.9791
Epoch 29/40
 - 0s - loss: 0.0322 - acc: 0.9797
Epoch 30/40
 - 0s - loss: 0.0323 - acc: 0.9787
Epoch 31/40
 - 0s - loss: 0.0320 - acc: 0.9793
Epoch 32/40
 - 0s - loss: 0.0321 - acc: 0.9789
Epoch 33/40
 - 0s - loss: 0.0327 - acc: 0.9798
Epoch 34/40
 - 0s - loss: 0.0320 - acc: 0.9790
Epoch 35/40
 - 0s - loss: 0.0320 - acc: 0.9782
Epoch 36/40
 - 0s - loss: 0.0326 - acc: 0.9779
Epoch 37/40
 - 0s - loss: 0.0315 - acc: 0.9791
Epoch 38/40
 - 0s - loss: 0.0315 - acc: 0.9781
Epoch 39/40
 - 0s - loss: 0.0318 - acc: 0.9777
Epoch 40/40
 - 0s - loss: 0.0313 - acc: 0.9786
# Training time = 0:03:21.906866
# F-Score(Ordinary) = 0.085, Recall: 0.8, Precision: 0.045
# F-Score(lvc) = 0.044, Recall: 0.75, Precision: 0.023
# F-Score(ireflv) = 0.134, Recall: 0.75, Precision: 0.074
# F-Score(id) = 0.079, Recall: 0.889, Precision: 0.041
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 48)        705264      input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 24)        5640        input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 192)          0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 96)           0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 288)          0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 24)           6936        concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 24)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            200         dropout_13[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1446 - acc: 0.9530 - val_loss: 0.0890 - val_acc: 0.9648
Epoch 2/40
 - 4s - loss: 0.0825 - acc: 0.9654 - val_loss: 0.0785 - val_acc: 0.9669
Epoch 3/40
 - 4s - loss: 0.0731 - acc: 0.9679 - val_loss: 0.0741 - val_acc: 0.9677
Epoch 4/40
 - 4s - loss: 0.0673 - acc: 0.9697 - val_loss: 0.0760 - val_acc: 0.9675
Epoch 5/40
 - 4s - loss: 0.0635 - acc: 0.9705 - val_loss: 0.0726 - val_acc: 0.9685
Epoch 6/40
 - 4s - loss: 0.0602 - acc: 0.9711 - val_loss: 0.0709 - val_acc: 0.9685
Epoch 7/40
 - 4s - loss: 0.0570 - acc: 0.9725 - val_loss: 0.0692 - val_acc: 0.9687
Epoch 8/40
 - 4s - loss: 0.0548 - acc: 0.9727 - val_loss: 0.0750 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0526 - acc: 0.9729 - val_loss: 0.0724 - val_acc: 0.9687
Epoch 10/40
 - 4s - loss: 0.0507 - acc: 0.9740 - val_loss: 0.0694 - val_acc: 0.9695
Epoch 11/40
 - 4s - loss: 0.0489 - acc: 0.9746 - val_loss: 0.0750 - val_acc: 0.9689
Epoch 12/40
 - 4s - loss: 0.0476 - acc: 0.9746 - val_loss: 0.0759 - val_acc: 0.9693
Epoch 13/40
 - 4s - loss: 0.0466 - acc: 0.9748 - val_loss: 0.0730 - val_acc: 0.9692
Epoch 14/40
 - 4s - loss: 0.0455 - acc: 0.9753 - val_loss: 0.0809 - val_acc: 0.9684
Epoch 15/40
 - 4s - loss: 0.0449 - acc: 0.9750 - val_loss: 0.0744 - val_acc: 0.9694
Epoch 16/40
 - 4s - loss: 0.0440 - acc: 0.9757 - val_loss: 0.0770 - val_acc: 0.9686
Epoch 17/40
 - 4s - loss: 0.0432 - acc: 0.9757 - val_loss: 0.0798 - val_acc: 0.9685
Epoch 18/40
 - 4s - loss: 0.0428 - acc: 0.9761 - val_loss: 0.0738 - val_acc: 0.9683
Epoch 19/40
 - 4s - loss: 0.0423 - acc: 0.9760 - val_loss: 0.0818 - val_acc: 0.9691
Epoch 20/40
 - 4s - loss: 0.0413 - acc: 0.9763 - val_loss: 0.0795 - val_acc: 0.9684
Epoch 21/40
 - 4s - loss: 0.0411 - acc: 0.9763 - val_loss: 0.0801 - val_acc: 0.9692
Epoch 22/40
 - 4s - loss: 0.0403 - acc: 0.9766 - val_loss: 0.0828 - val_acc: 0.9692
Epoch 23/40
 - 4s - loss: 0.0404 - acc: 0.9764 - val_loss: 0.0823 - val_acc: 0.9684
Epoch 24/40
 - 4s - loss: 0.0401 - acc: 0.9763 - val_loss: 0.0829 - val_acc: 0.9696
Epoch 25/40
 - 4s - loss: 0.0396 - acc: 0.9766 - val_loss: 0.0888 - val_acc: 0.9690
Epoch 26/40
 - 4s - loss: 0.0394 - acc: 0.9762 - val_loss: 0.0801 - val_acc: 0.9694
Epoch 27/40
 - 4s - loss: 0.0388 - acc: 0.9770 - val_loss: 0.0862 - val_acc: 0.9691
Epoch 28/40
 - 4s - loss: 0.0385 - acc: 0.9769 - val_loss: 0.0807 - val_acc: 0.9687
Epoch 29/40
 - 4s - loss: 0.0381 - acc: 0.9770 - val_loss: 0.0859 - val_acc: 0.9684
Epoch 30/40
 - 4s - loss: 0.0382 - acc: 0.9770 - val_loss: 0.0868 - val_acc: 0.9696
Epoch 31/40
 - 4s - loss: 0.0376 - acc: 0.9773 - val_loss: 0.0898 - val_acc: 0.9691
Epoch 32/40
 - 4s - loss: 0.0375 - acc: 0.9776 - val_loss: 0.0932 - val_acc: 0.9697
Epoch 33/40
 - 4s - loss: 0.0372 - acc: 0.9772 - val_loss: 0.0924 - val_acc: 0.9695
Epoch 34/40
 - 4s - loss: 0.0367 - acc: 0.9773 - val_loss: 0.0878 - val_acc: 0.9686
Epoch 35/40
 - 4s - loss: 0.0368 - acc: 0.9774 - val_loss: 0.0904 - val_acc: 0.9689
Epoch 36/40
 - 4s - loss: 0.0365 - acc: 0.9774 - val_loss: 0.0905 - val_acc: 0.9699
Epoch 37/40
 - 4s - loss: 0.0359 - acc: 0.9780 - val_loss: 0.0903 - val_acc: 0.9701
Epoch 38/40
 - 4s - loss: 0.0357 - acc: 0.9777 - val_loss: 0.0931 - val_acc: 0.9698
Epoch 39/40
 - 4s - loss: 0.0360 - acc: 0.9772 - val_loss: 0.0953 - val_acc: 0.9686
Epoch 40/40
 - 4s - loss: 0.0356 - acc: 0.9777 - val_loss: 0.0940 - val_acc: 0.9695
Epoch 1/40
 - 0s - loss: 0.0732 - acc: 0.9678
Epoch 2/40
 - 0s - loss: 0.0571 - acc: 0.9705
Epoch 3/40
 - 0s - loss: 0.0510 - acc: 0.9739
Epoch 4/40
 - 0s - loss: 0.0483 - acc: 0.9725
Epoch 5/40
 - 0s - loss: 0.0439 - acc: 0.9746
Epoch 6/40
 - 0s - loss: 0.0425 - acc: 0.9745
Epoch 7/40
 - 0s - loss: 0.0408 - acc: 0.9758
Epoch 8/40
 - 0s - loss: 0.0388 - acc: 0.9754
Epoch 9/40
 - 0s - loss: 0.0371 - acc: 0.9765
Epoch 10/40
 - 0s - loss: 0.0359 - acc: 0.9773
Epoch 11/40
 - 0s - loss: 0.0350 - acc: 0.9772
Epoch 12/40
 - 0s - loss: 0.0347 - acc: 0.9778
Epoch 13/40
 - 0s - loss: 0.0349 - acc: 0.9778
Epoch 14/40
 - 0s - loss: 0.0332 - acc: 0.9783
Epoch 15/40
 - 0s - loss: 0.0336 - acc: 0.9778
Epoch 16/40
 - 0s - loss: 0.0330 - acc: 0.9788
Epoch 17/40
 - 0s - loss: 0.0333 - acc: 0.9790
Epoch 18/40
 - 0s - loss: 0.0325 - acc: 0.9802
Epoch 19/40
 - 0s - loss: 0.0328 - acc: 0.9786
Epoch 20/40
 - 0s - loss: 0.0323 - acc: 0.9777
Epoch 21/40
 - 0s - loss: 0.0320 - acc: 0.9780
Epoch 22/40
 - 0s - loss: 0.0317 - acc: 0.9792
Epoch 23/40
 - 0s - loss: 0.0322 - acc: 0.9783
Epoch 24/40
 - 0s - loss: 0.0318 - acc: 0.9785
Epoch 25/40
 - 0s - loss: 0.0317 - acc: 0.9783
Epoch 26/40
 - 0s - loss: 0.0313 - acc: 0.9783
Epoch 27/40
 - 0s - loss: 0.0311 - acc: 0.9782
Epoch 28/40
 - 0s - loss: 0.0319 - acc: 0.9789
Epoch 29/40
 - 0s - loss: 0.0317 - acc: 0.9773
Epoch 30/40
 - 0s - loss: 0.0315 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0315 - acc: 0.9790
Epoch 32/40
 - 0s - loss: 0.0314 - acc: 0.9781
Epoch 33/40
 - 0s - loss: 0.0310 - acc: 0.9797
Epoch 34/40
 - 0s - loss: 0.0306 - acc: 0.9790
Epoch 35/40
 - 0s - loss: 0.0308 - acc: 0.9797
Epoch 36/40
 - 0s - loss: 0.0303 - acc: 0.9789
Epoch 37/40
 - 0s - loss: 0.0305 - acc: 0.9794
Epoch 38/40
 - 0s - loss: 0.0309 - acc: 0.9802
Epoch 39/40
 - 0s - loss: 0.0305 - acc: 0.9791
Epoch 40/40
 - 0s - loss: 0.0304 - acc: 0.9790
# Training time = 0:03:22.273319
# F-Score(Ordinary) = 0.157, Recall: 0.547, Precision: 0.092
# F-Score(lvc) = 0.366, Recall: 0.529, Precision: 0.28
# F-Score(id) = 0.021, Recall: 1.0, Precision: 0.01
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 48)        705264      input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 24)        5640        input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 192)          0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 96)           0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 288)          0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 24)           6936        concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 24)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            200         dropout_14[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1516 - acc: 0.9513 - val_loss: 0.0871 - val_acc: 0.9651
Epoch 2/40
 - 4s - loss: 0.0841 - acc: 0.9656 - val_loss: 0.0849 - val_acc: 0.9638
Epoch 3/40
 - 4s - loss: 0.0744 - acc: 0.9679 - val_loss: 0.0751 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0686 - acc: 0.9691 - val_loss: 0.0740 - val_acc: 0.9684
Epoch 5/40
 - 4s - loss: 0.0643 - acc: 0.9705 - val_loss: 0.0738 - val_acc: 0.9675
Epoch 6/40
 - 4s - loss: 0.0610 - acc: 0.9717 - val_loss: 0.0697 - val_acc: 0.9690
Epoch 7/40
 - 4s - loss: 0.0585 - acc: 0.9724 - val_loss: 0.0725 - val_acc: 0.9686
Epoch 8/40
 - 4s - loss: 0.0553 - acc: 0.9733 - val_loss: 0.0754 - val_acc: 0.9682
Epoch 9/40
 - 4s - loss: 0.0534 - acc: 0.9737 - val_loss: 0.0726 - val_acc: 0.9687
Epoch 10/40
 - 4s - loss: 0.0516 - acc: 0.9745 - val_loss: 0.0717 - val_acc: 0.9689
Epoch 11/40
 - 4s - loss: 0.0501 - acc: 0.9741 - val_loss: 0.0728 - val_acc: 0.9691
Epoch 12/40
 - 4s - loss: 0.0488 - acc: 0.9750 - val_loss: 0.0736 - val_acc: 0.9688
Epoch 13/40
 - 4s - loss: 0.0471 - acc: 0.9756 - val_loss: 0.0741 - val_acc: 0.9697
Epoch 14/40
 - 4s - loss: 0.0465 - acc: 0.9752 - val_loss: 0.0737 - val_acc: 0.9688
Epoch 15/40
 - 4s - loss: 0.0455 - acc: 0.9755 - val_loss: 0.0767 - val_acc: 0.9687
Epoch 16/40
 - 4s - loss: 0.0442 - acc: 0.9760 - val_loss: 0.0759 - val_acc: 0.9689
Epoch 17/40
 - 4s - loss: 0.0433 - acc: 0.9763 - val_loss: 0.0745 - val_acc: 0.9693
Epoch 18/40
 - 4s - loss: 0.0426 - acc: 0.9763 - val_loss: 0.0789 - val_acc: 0.9685
Epoch 19/40
 - 4s - loss: 0.0420 - acc: 0.9767 - val_loss: 0.0817 - val_acc: 0.9682
Epoch 20/40
 - 4s - loss: 0.0417 - acc: 0.9765 - val_loss: 0.0825 - val_acc: 0.9693
Epoch 21/40
 - 4s - loss: 0.0411 - acc: 0.9770 - val_loss: 0.0837 - val_acc: 0.9687
Epoch 22/40
 - 4s - loss: 0.0405 - acc: 0.9767 - val_loss: 0.0799 - val_acc: 0.9699
Epoch 23/40
 - 4s - loss: 0.0403 - acc: 0.9768 - val_loss: 0.0816 - val_acc: 0.9696
Epoch 24/40
 - 4s - loss: 0.0398 - acc: 0.9769 - val_loss: 0.0808 - val_acc: 0.9695
Epoch 25/40
 - 4s - loss: 0.0394 - acc: 0.9766 - val_loss: 0.0847 - val_acc: 0.9694
Epoch 26/40
 - 4s - loss: 0.0392 - acc: 0.9772 - val_loss: 0.0841 - val_acc: 0.9698
Epoch 27/40
 - 4s - loss: 0.0388 - acc: 0.9769 - val_loss: 0.0865 - val_acc: 0.9693
Epoch 28/40
 - 4s - loss: 0.0384 - acc: 0.9769 - val_loss: 0.0861 - val_acc: 0.9694
Epoch 29/40
 - 4s - loss: 0.0383 - acc: 0.9772 - val_loss: 0.0894 - val_acc: 0.9690
Epoch 30/40
 - 4s - loss: 0.0376 - acc: 0.9772 - val_loss: 0.0860 - val_acc: 0.9692
Epoch 31/40
 - 4s - loss: 0.0374 - acc: 0.9776 - val_loss: 0.0855 - val_acc: 0.9688
Epoch 32/40
 - 4s - loss: 0.0376 - acc: 0.9774 - val_loss: 0.0899 - val_acc: 0.9692
Epoch 33/40
 - 4s - loss: 0.0371 - acc: 0.9773 - val_loss: 0.0953 - val_acc: 0.9697
Epoch 34/40
 - 4s - loss: 0.0370 - acc: 0.9776 - val_loss: 0.0921 - val_acc: 0.9693
Epoch 35/40
 - 4s - loss: 0.0365 - acc: 0.9775 - val_loss: 0.0930 - val_acc: 0.9685
Epoch 36/40
 - 4s - loss: 0.0366 - acc: 0.9775 - val_loss: 0.0948 - val_acc: 0.9687
Epoch 37/40
 - 4s - loss: 0.0361 - acc: 0.9776 - val_loss: 0.0937 - val_acc: 0.9691
Epoch 38/40
 - 4s - loss: 0.0361 - acc: 0.9775 - val_loss: 0.0926 - val_acc: 0.9689
Epoch 39/40
 - 4s - loss: 0.0357 - acc: 0.9777 - val_loss: 0.0974 - val_acc: 0.9684
Epoch 40/40
 - 4s - loss: 0.0356 - acc: 0.9780 - val_loss: 0.0978 - val_acc: 0.9695
Epoch 1/40
 - 0s - loss: 0.0738 - acc: 0.9678
Epoch 2/40
 - 0s - loss: 0.0568 - acc: 0.9723
Epoch 3/40
 - 0s - loss: 0.0500 - acc: 0.9729
Epoch 4/40
 - 0s - loss: 0.0473 - acc: 0.9743
Epoch 5/40
 - 0s - loss: 0.0437 - acc: 0.9744
Epoch 6/40
 - 0s - loss: 0.0422 - acc: 0.9753
Epoch 7/40
 - 0s - loss: 0.0408 - acc: 0.9765
Epoch 8/40
 - 0s - loss: 0.0399 - acc: 0.9761
Epoch 9/40
 - 0s - loss: 0.0374 - acc: 0.9772
Epoch 10/40
 - 0s - loss: 0.0375 - acc: 0.9774
Epoch 11/40
 - 0s - loss: 0.0369 - acc: 0.9776
Epoch 12/40
 - 0s - loss: 0.0363 - acc: 0.9768
Epoch 13/40
 - 0s - loss: 0.0354 - acc: 0.9786
Epoch 14/40
 - 0s - loss: 0.0350 - acc: 0.9776
Epoch 15/40
 - 0s - loss: 0.0345 - acc: 0.9772
Epoch 16/40
 - 0s - loss: 0.0345 - acc: 0.9777
Epoch 17/40
 - 0s - loss: 0.0345 - acc: 0.9773
Epoch 18/40
 - 0s - loss: 0.0341 - acc: 0.9785
Epoch 19/40
 - 0s - loss: 0.0338 - acc: 0.9786
Epoch 20/40
 - 0s - loss: 0.0340 - acc: 0.9794
Epoch 21/40
 - 0s - loss: 0.0337 - acc: 0.9793
Epoch 22/40
 - 0s - loss: 0.0335 - acc: 0.9795
Epoch 23/40
 - 0s - loss: 0.0341 - acc: 0.9777
Epoch 24/40
 - 0s - loss: 0.0337 - acc: 0.9782
Epoch 25/40
 - 0s - loss: 0.0330 - acc: 0.9783
Epoch 26/40
 - 0s - loss: 0.0329 - acc: 0.9791
Epoch 27/40
 - 0s - loss: 0.0328 - acc: 0.9778
Epoch 28/40
 - 0s - loss: 0.0320 - acc: 0.9784
Epoch 29/40
 - 0s - loss: 0.0314 - acc: 0.9803
Epoch 30/40
 - 0s - loss: 0.0312 - acc: 0.9795
Epoch 31/40
 - 0s - loss: 0.0318 - acc: 0.9802
Epoch 32/40
 - 0s - loss: 0.0309 - acc: 0.9794
Epoch 33/40
 - 0s - loss: 0.0315 - acc: 0.9785
Epoch 34/40
 - 0s - loss: 0.0308 - acc: 0.9802
Epoch 35/40
 - 0s - loss: 0.0315 - acc: 0.9790
Epoch 36/40
 - 0s - loss: 0.0307 - acc: 0.9787
Epoch 37/40
 - 0s - loss: 0.0310 - acc: 0.9785
Epoch 38/40
 - 0s - loss: 0.0304 - acc: 0.9787
Epoch 39/40
 - 0s - loss: 0.0313 - acc: 0.9800
Epoch 40/40
 - 0s - loss: 0.0307 - acc: 0.9791
# Training time = 0:03:20.064303
# F-Score(Ordinary) = 0.249, Recall: 0.448, Precision: 0.172
# F-Score(lvc) = 0.476, Recall: 0.437, Precision: 0.523
# F-Score(ireflv) = 0.089, Recall: 0.462, Precision: 0.049
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 48)        705264      input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 24)        5640        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 192)          0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 96)           0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 288)          0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 24)           6936        concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 24)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            200         dropout_15[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1497 - acc: 0.9517 - val_loss: 0.0948 - val_acc: 0.9633
Epoch 2/40
 - 4s - loss: 0.0869 - acc: 0.9649 - val_loss: 0.0814 - val_acc: 0.9673
Epoch 3/40
 - 4s - loss: 0.0757 - acc: 0.9679 - val_loss: 0.0752 - val_acc: 0.9680
Epoch 4/40
 - 4s - loss: 0.0699 - acc: 0.9694 - val_loss: 0.0740 - val_acc: 0.9681
Epoch 5/40
 - 4s - loss: 0.0656 - acc: 0.9706 - val_loss: 0.0723 - val_acc: 0.9671
Epoch 6/40
 - 4s - loss: 0.0617 - acc: 0.9714 - val_loss: 0.0713 - val_acc: 0.9681
Epoch 7/40
 - 4s - loss: 0.0590 - acc: 0.9718 - val_loss: 0.0752 - val_acc: 0.9682
Epoch 8/40
 - 4s - loss: 0.0564 - acc: 0.9726 - val_loss: 0.0731 - val_acc: 0.9687
Epoch 9/40
 - 4s - loss: 0.0541 - acc: 0.9729 - val_loss: 0.0734 - val_acc: 0.9679
Epoch 10/40
 - 4s - loss: 0.0517 - acc: 0.9739 - val_loss: 0.0741 - val_acc: 0.9685
Epoch 11/40
 - 4s - loss: 0.0504 - acc: 0.9745 - val_loss: 0.0764 - val_acc: 0.9684
Epoch 12/40
 - 4s - loss: 0.0488 - acc: 0.9746 - val_loss: 0.0717 - val_acc: 0.9695
Epoch 13/40
 - 4s - loss: 0.0473 - acc: 0.9750 - val_loss: 0.0734 - val_acc: 0.9694
Epoch 14/40
 - 4s - loss: 0.0465 - acc: 0.9755 - val_loss: 0.0783 - val_acc: 0.9695
Epoch 15/40
 - 4s - loss: 0.0452 - acc: 0.9755 - val_loss: 0.0721 - val_acc: 0.9689
Epoch 16/40
 - 4s - loss: 0.0446 - acc: 0.9753 - val_loss: 0.0796 - val_acc: 0.9688
Epoch 17/40
 - 4s - loss: 0.0438 - acc: 0.9759 - val_loss: 0.0763 - val_acc: 0.9690
Epoch 18/40
 - 4s - loss: 0.0430 - acc: 0.9760 - val_loss: 0.0789 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0424 - acc: 0.9763 - val_loss: 0.0780 - val_acc: 0.9679
Epoch 20/40
 - 4s - loss: 0.0420 - acc: 0.9760 - val_loss: 0.0794 - val_acc: 0.9689
Epoch 21/40
 - 4s - loss: 0.0416 - acc: 0.9760 - val_loss: 0.0830 - val_acc: 0.9684
Epoch 22/40
 - 4s - loss: 0.0408 - acc: 0.9764 - val_loss: 0.0821 - val_acc: 0.9684
Epoch 23/40
 - 4s - loss: 0.0405 - acc: 0.9764 - val_loss: 0.0818 - val_acc: 0.9694
Epoch 24/40
 - 4s - loss: 0.0399 - acc: 0.9771 - val_loss: 0.0815 - val_acc: 0.9701
Epoch 25/40
 - 4s - loss: 0.0396 - acc: 0.9770 - val_loss: 0.0825 - val_acc: 0.9686
Epoch 26/40
 - 4s - loss: 0.0395 - acc: 0.9769 - val_loss: 0.0918 - val_acc: 0.9690
Epoch 27/40
 - 4s - loss: 0.0389 - acc: 0.9774 - val_loss: 0.0872 - val_acc: 0.9693
Epoch 28/40
 - 4s - loss: 0.0385 - acc: 0.9772 - val_loss: 0.0853 - val_acc: 0.9687
Epoch 29/40
 - 4s - loss: 0.0386 - acc: 0.9772 - val_loss: 0.0881 - val_acc: 0.9687
Epoch 30/40
 - 4s - loss: 0.0381 - acc: 0.9773 - val_loss: 0.0905 - val_acc: 0.9687
Epoch 31/40
 - 4s - loss: 0.0379 - acc: 0.9774 - val_loss: 0.0925 - val_acc: 0.9686
Epoch 32/40
 - 4s - loss: 0.0377 - acc: 0.9774 - val_loss: 0.0863 - val_acc: 0.9692
Epoch 33/40
 - 4s - loss: 0.0376 - acc: 0.9770 - val_loss: 0.0921 - val_acc: 0.9687
Epoch 34/40
 - 4s - loss: 0.0374 - acc: 0.9772 - val_loss: 0.0941 - val_acc: 0.9687
Epoch 35/40
 - 4s - loss: 0.0369 - acc: 0.9772 - val_loss: 0.0917 - val_acc: 0.9692
Epoch 36/40
 - 4s - loss: 0.0371 - acc: 0.9773 - val_loss: 0.0888 - val_acc: 0.9693
Epoch 37/40
 - 4s - loss: 0.0370 - acc: 0.9770 - val_loss: 0.1007 - val_acc: 0.9689
Epoch 38/40
 - 4s - loss: 0.0365 - acc: 0.9778 - val_loss: 0.0917 - val_acc: 0.9691
Epoch 39/40
 - 4s - loss: 0.0364 - acc: 0.9775 - val_loss: 0.0940 - val_acc: 0.9689
Epoch 40/40
 - 4s - loss: 0.0365 - acc: 0.9775 - val_loss: 0.0983 - val_acc: 0.9688
Epoch 1/40
 - 0s - loss: 0.0758 - acc: 0.9663
Epoch 2/40
 - 0s - loss: 0.0589 - acc: 0.9697
Epoch 3/40
 - 0s - loss: 0.0512 - acc: 0.9720
Epoch 4/40
 - 0s - loss: 0.0482 - acc: 0.9717
Epoch 5/40
 - 0s - loss: 0.0438 - acc: 0.9734
Epoch 6/40
 - 0s - loss: 0.0417 - acc: 0.9750
Epoch 7/40
 - 0s - loss: 0.0401 - acc: 0.9771
Epoch 8/40
 - 0s - loss: 0.0389 - acc: 0.9753
Epoch 9/40
 - 0s - loss: 0.0383 - acc: 0.9754
Epoch 10/40
 - 0s - loss: 0.0380 - acc: 0.9758
Epoch 11/40
 - 0s - loss: 0.0364 - acc: 0.9764
Epoch 12/40
 - 0s - loss: 0.0355 - acc: 0.9770
Epoch 13/40
 - 0s - loss: 0.0353 - acc: 0.9779
Epoch 14/40
 - 0s - loss: 0.0351 - acc: 0.9774
Epoch 15/40
 - 0s - loss: 0.0344 - acc: 0.9773
Epoch 16/40
 - 0s - loss: 0.0342 - acc: 0.9783
Epoch 17/40
 - 0s - loss: 0.0339 - acc: 0.9784
Epoch 18/40
 - 0s - loss: 0.0337 - acc: 0.9770
Epoch 19/40
 - 0s - loss: 0.0331 - acc: 0.9778
Epoch 20/40
 - 0s - loss: 0.0336 - acc: 0.9784
Epoch 21/40
 - 0s - loss: 0.0331 - acc: 0.9784
Epoch 22/40
 - 0s - loss: 0.0326 - acc: 0.9784
Epoch 23/40
 - 0s - loss: 0.0321 - acc: 0.9780
Epoch 24/40
 - 0s - loss: 0.0329 - acc: 0.9778
Epoch 25/40
 - 0s - loss: 0.0321 - acc: 0.9788
Epoch 26/40
 - 0s - loss: 0.0322 - acc: 0.9795
Epoch 27/40
 - 0s - loss: 0.0325 - acc: 0.9779
Epoch 28/40
 - 0s - loss: 0.0324 - acc: 0.9777
Epoch 29/40
 - 0s - loss: 0.0318 - acc: 0.9790
Epoch 30/40
 - 0s - loss: 0.0318 - acc: 0.9798
Epoch 31/40
 - 0s - loss: 0.0319 - acc: 0.9778
Epoch 32/40
 - 0s - loss: 0.0315 - acc: 0.9792
Epoch 33/40
 - 0s - loss: 0.0311 - acc: 0.9786
Epoch 34/40
 - 0s - loss: 0.0315 - acc: 0.9793
Epoch 35/40
 - 0s - loss: 0.0309 - acc: 0.9787
Epoch 36/40
 - 0s - loss: 0.0308 - acc: 0.9798
Epoch 37/40
 - 0s - loss: 0.0312 - acc: 0.9783
Epoch 38/40
 - 0s - loss: 0.0310 - acc: 0.9794
Epoch 39/40
 - 0s - loss: 0.0314 - acc: 0.9786
Epoch 40/40
 - 0s - loss: 0.0308 - acc: 0.9789
# Training time = 0:03:21.553459
# F-Score(Ordinary) = 0.602, Recall: 0.581, Precision: 0.624
# F-Score(lvc) = 0.57, Recall: 0.838, Precision: 0.432
# F-Score(ireflv) = 0.817, Recall: 0.87, Precision: 0.77
# F-Score(id) = 0.459, Recall: 0.375, Precision: 0.591
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 48)        705264      input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 24)        5640        input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 192)          0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 96)           0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 288)          0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 24)           6936        concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 24)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            200         dropout_16[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1195 - acc: 0.9573 - val_loss: 0.0846 - val_acc: 0.9662
Epoch 2/40
 - 4s - loss: 0.0760 - acc: 0.9672 - val_loss: 0.0763 - val_acc: 0.9678
Epoch 3/40
 - 4s - loss: 0.0673 - acc: 0.9692 - val_loss: 0.0750 - val_acc: 0.9681
Epoch 4/40
 - 4s - loss: 0.0620 - acc: 0.9706 - val_loss: 0.0741 - val_acc: 0.9675
Epoch 5/40
 - 4s - loss: 0.0584 - acc: 0.9713 - val_loss: 0.0758 - val_acc: 0.9682
Epoch 6/40
 - 4s - loss: 0.0548 - acc: 0.9728 - val_loss: 0.0739 - val_acc: 0.9683
Epoch 7/40
 - 4s - loss: 0.0524 - acc: 0.9735 - val_loss: 0.0762 - val_acc: 0.9688
Epoch 8/40
 - 4s - loss: 0.0511 - acc: 0.9735 - val_loss: 0.0778 - val_acc: 0.9685
Epoch 9/40
 - 4s - loss: 0.0488 - acc: 0.9748 - val_loss: 0.0771 - val_acc: 0.9690
Epoch 10/40
 - 4s - loss: 0.0477 - acc: 0.9744 - val_loss: 0.0820 - val_acc: 0.9683
Epoch 11/40
 - 4s - loss: 0.0467 - acc: 0.9748 - val_loss: 0.0801 - val_acc: 0.9671
Epoch 12/40
 - 4s - loss: 0.0452 - acc: 0.9750 - val_loss: 0.0758 - val_acc: 0.9694
Epoch 13/40
 - 4s - loss: 0.0451 - acc: 0.9749 - val_loss: 0.0810 - val_acc: 0.9681
Epoch 14/40
 - 4s - loss: 0.0441 - acc: 0.9756 - val_loss: 0.0917 - val_acc: 0.9687
Epoch 15/40
 - 4s - loss: 0.0434 - acc: 0.9758 - val_loss: 0.0895 - val_acc: 0.9681
Epoch 16/40
 - 4s - loss: 0.0428 - acc: 0.9755 - val_loss: 0.0857 - val_acc: 0.9680
Epoch 17/40
 - 4s - loss: 0.0422 - acc: 0.9756 - val_loss: 0.0988 - val_acc: 0.9681
Epoch 18/40
 - 4s - loss: 0.0419 - acc: 0.9759 - val_loss: 0.0992 - val_acc: 0.9685
Epoch 19/40
 - 4s - loss: 0.0413 - acc: 0.9756 - val_loss: 0.0968 - val_acc: 0.9686
Epoch 20/40
 - 4s - loss: 0.0411 - acc: 0.9761 - val_loss: 0.0968 - val_acc: 0.9684
Epoch 21/40
 - 4s - loss: 0.0407 - acc: 0.9758 - val_loss: 0.0986 - val_acc: 0.9671
Epoch 22/40
 - 4s - loss: 0.0393 - acc: 0.9768 - val_loss: 0.1047 - val_acc: 0.9679
Epoch 23/40
 - 4s - loss: 0.0397 - acc: 0.9766 - val_loss: 0.0969 - val_acc: 0.9689
Epoch 24/40
 - 4s - loss: 0.0393 - acc: 0.9765 - val_loss: 0.0971 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0385 - acc: 0.9766 - val_loss: 0.1011 - val_acc: 0.9680
Epoch 26/40
 - 4s - loss: 0.0388 - acc: 0.9769 - val_loss: 0.1019 - val_acc: 0.9679
Epoch 27/40
 - 4s - loss: 0.0383 - acc: 0.9772 - val_loss: 0.1037 - val_acc: 0.9678
Epoch 28/40
 - 4s - loss: 0.0379 - acc: 0.9769 - val_loss: 0.1007 - val_acc: 0.9679
Epoch 29/40
 - 4s - loss: 0.0382 - acc: 0.9771 - val_loss: 0.1084 - val_acc: 0.9677
Epoch 30/40
 - 4s - loss: 0.0378 - acc: 0.9771 - val_loss: 0.1070 - val_acc: 0.9691
Epoch 31/40
 - 4s - loss: 0.0377 - acc: 0.9770 - val_loss: 0.1027 - val_acc: 0.9686
Epoch 32/40
 - 4s - loss: 0.0375 - acc: 0.9770 - val_loss: 0.1077 - val_acc: 0.9682
Epoch 33/40
 - 4s - loss: 0.0375 - acc: 0.9772 - val_loss: 0.1081 - val_acc: 0.9679
Epoch 34/40
 - 4s - loss: 0.0373 - acc: 0.9772 - val_loss: 0.1037 - val_acc: 0.9686
Epoch 35/40
 - 4s - loss: 0.0376 - acc: 0.9776 - val_loss: 0.1103 - val_acc: 0.9689
Epoch 36/40
 - 4s - loss: 0.0374 - acc: 0.9772 - val_loss: 0.1055 - val_acc: 0.9687
Epoch 37/40
 - 4s - loss: 0.0369 - acc: 0.9773 - val_loss: 0.1092 - val_acc: 0.9696
Epoch 38/40
 - 4s - loss: 0.0372 - acc: 0.9773 - val_loss: 0.1071 - val_acc: 0.9687
Epoch 39/40
 - 4s - loss: 0.0367 - acc: 0.9771 - val_loss: 0.1142 - val_acc: 0.9684
Epoch 40/40
 - 4s - loss: 0.0365 - acc: 0.9775 - val_loss: 0.1114 - val_acc: 0.9687
Epoch 1/40
 - 0s - loss: 0.0838 - acc: 0.9663
Epoch 2/40
 - 0s - loss: 0.0626 - acc: 0.9698
Epoch 3/40
 - 0s - loss: 0.0550 - acc: 0.9716
Epoch 4/40
 - 0s - loss: 0.0495 - acc: 0.9731
Epoch 5/40
 - 0s - loss: 0.0492 - acc: 0.9729
Epoch 6/40
 - 0s - loss: 0.0456 - acc: 0.9759
Epoch 7/40
 - 0s - loss: 0.0456 - acc: 0.9746
Epoch 8/40
 - 0s - loss: 0.0436 - acc: 0.9754
Epoch 9/40
 - 0s - loss: 0.0422 - acc: 0.9763
Epoch 10/40
 - 0s - loss: 0.0406 - acc: 0.9764
Epoch 11/40
 - 0s - loss: 0.0398 - acc: 0.9768
Epoch 12/40
 - 0s - loss: 0.0381 - acc: 0.9768
Epoch 13/40
 - 0s - loss: 0.0385 - acc: 0.9781
Epoch 14/40
 - 0s - loss: 0.0379 - acc: 0.9772
Epoch 15/40
 - 0s - loss: 0.0384 - acc: 0.9776
Epoch 16/40
 - 0s - loss: 0.0375 - acc: 0.9778
Epoch 17/40
 - 0s - loss: 0.0370 - acc: 0.9775
Epoch 18/40
 - 0s - loss: 0.0370 - acc: 0.9777
Epoch 19/40
 - 0s - loss: 0.0364 - acc: 0.9774
Epoch 20/40
 - 0s - loss: 0.0366 - acc: 0.9783
Epoch 21/40
 - 0s - loss: 0.0368 - acc: 0.9784
Epoch 22/40
 - 0s - loss: 0.0360 - acc: 0.9780
Epoch 23/40
 - 0s - loss: 0.0372 - acc: 0.9778
Epoch 24/40
 - 0s - loss: 0.0368 - acc: 0.9770
Epoch 25/40
 - 0s - loss: 0.0353 - acc: 0.9777
Epoch 26/40
 - 0s - loss: 0.0365 - acc: 0.9788
Epoch 27/40
 - 0s - loss: 0.0356 - acc: 0.9788
Epoch 28/40
 - 0s - loss: 0.0357 - acc: 0.9777
Epoch 29/40
 - 0s - loss: 0.0356 - acc: 0.9785
Epoch 30/40
 - 0s - loss: 0.0352 - acc: 0.9778
Epoch 31/40
 - 0s - loss: 0.0342 - acc: 0.9796
Epoch 32/40
 - 0s - loss: 0.0352 - acc: 0.9790
Epoch 33/40
 - 0s - loss: 0.0353 - acc: 0.9788
Epoch 34/40
 - 0s - loss: 0.0356 - acc: 0.9796
Epoch 35/40
 - 0s - loss: 0.0359 - acc: 0.9777
Epoch 36/40
 - 0s - loss: 0.0353 - acc: 0.9780
Epoch 37/40
 - 0s - loss: 0.0348 - acc: 0.9802
Epoch 38/40
 - 0s - loss: 0.0348 - acc: 0.9787
Epoch 39/40
 - 0s - loss: 0.0349 - acc: 0.9774
Epoch 40/40
 - 0s - loss: 0.0354 - acc: 0.9780
# Training time = 0:03:22.923709
# F-Score(Ordinary) = 0.13, Recall: 0.453, Precision: 0.076
# F-Score(ireflv) = 0.231, Recall: 0.35, Precision: 0.172
# F-Score(id) = 0.126, Recall: 0.929, Precision: 0.067
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 48)        705264      input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 24)        5640        input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 192)          0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 96)           0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 288)          0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 24)           6936        concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 24)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            200         dropout_17[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1192 - acc: 0.9575 - val_loss: 0.0777 - val_acc: 0.9678
Epoch 2/40
 - 4s - loss: 0.0763 - acc: 0.9673 - val_loss: 0.0802 - val_acc: 0.9660
Epoch 3/40
 - 4s - loss: 0.0681 - acc: 0.9693 - val_loss: 0.0725 - val_acc: 0.9672
Epoch 4/40
 - 4s - loss: 0.0630 - acc: 0.9705 - val_loss: 0.0755 - val_acc: 0.9685
Epoch 5/40
 - 4s - loss: 0.0592 - acc: 0.9719 - val_loss: 0.0737 - val_acc: 0.9687
Epoch 6/40
 - 4s - loss: 0.0557 - acc: 0.9723 - val_loss: 0.0827 - val_acc: 0.9670
Epoch 7/40
 - 4s - loss: 0.0528 - acc: 0.9734 - val_loss: 0.0758 - val_acc: 0.9681
Epoch 8/40
 - 4s - loss: 0.0504 - acc: 0.9739 - val_loss: 0.0797 - val_acc: 0.9684
Epoch 9/40
 - 4s - loss: 0.0494 - acc: 0.9742 - val_loss: 0.0768 - val_acc: 0.9685
Epoch 10/40
 - 4s - loss: 0.0482 - acc: 0.9747 - val_loss: 0.0823 - val_acc: 0.9689
Epoch 11/40
 - 4s - loss: 0.0469 - acc: 0.9748 - val_loss: 0.0873 - val_acc: 0.9685
Epoch 12/40
 - 4s - loss: 0.0461 - acc: 0.9751 - val_loss: 0.0834 - val_acc: 0.9678
Epoch 13/40
 - 4s - loss: 0.0449 - acc: 0.9754 - val_loss: 0.0900 - val_acc: 0.9690
Epoch 14/40
 - 4s - loss: 0.0441 - acc: 0.9754 - val_loss: 0.0908 - val_acc: 0.9698
Epoch 15/40
 - 4s - loss: 0.0436 - acc: 0.9756 - val_loss: 0.0880 - val_acc: 0.9695
Epoch 16/40
 - 4s - loss: 0.0432 - acc: 0.9757 - val_loss: 0.0888 - val_acc: 0.9673
Epoch 17/40
 - 4s - loss: 0.0418 - acc: 0.9756 - val_loss: 0.0903 - val_acc: 0.9689
Epoch 18/40
 - 4s - loss: 0.0414 - acc: 0.9762 - val_loss: 0.0985 - val_acc: 0.9691
Epoch 19/40
 - 4s - loss: 0.0413 - acc: 0.9765 - val_loss: 0.0934 - val_acc: 0.9689
Epoch 20/40
 - 4s - loss: 0.0409 - acc: 0.9760 - val_loss: 0.0889 - val_acc: 0.9686
Epoch 21/40
 - 4s - loss: 0.0406 - acc: 0.9763 - val_loss: 0.0897 - val_acc: 0.9690
Epoch 22/40
 - 4s - loss: 0.0391 - acc: 0.9770 - val_loss: 0.0910 - val_acc: 0.9683
Epoch 23/40
 - 4s - loss: 0.0395 - acc: 0.9763 - val_loss: 0.0929 - val_acc: 0.9680
Epoch 24/40
 - 4s - loss: 0.0389 - acc: 0.9770 - val_loss: 0.0990 - val_acc: 0.9678
Epoch 25/40
 - 4s - loss: 0.0386 - acc: 0.9768 - val_loss: 0.1026 - val_acc: 0.9685
Epoch 26/40
 - 4s - loss: 0.0386 - acc: 0.9768 - val_loss: 0.0909 - val_acc: 0.9697
Epoch 27/40
 - 4s - loss: 0.0384 - acc: 0.9769 - val_loss: 0.0994 - val_acc: 0.9690
Epoch 28/40
 - 4s - loss: 0.0379 - acc: 0.9774 - val_loss: 0.1024 - val_acc: 0.9688
Epoch 29/40
 - 4s - loss: 0.0376 - acc: 0.9774 - val_loss: 0.1062 - val_acc: 0.9687
Epoch 30/40
 - 4s - loss: 0.0374 - acc: 0.9773 - val_loss: 0.1039 - val_acc: 0.9680
Epoch 31/40
 - 4s - loss: 0.0374 - acc: 0.9775 - val_loss: 0.1025 - val_acc: 0.9681
Epoch 32/40
 - 4s - loss: 0.0372 - acc: 0.9775 - val_loss: 0.1027 - val_acc: 0.9683
Epoch 33/40
 - 4s - loss: 0.0379 - acc: 0.9768 - val_loss: 0.1003 - val_acc: 0.9685
Epoch 34/40
 - 4s - loss: 0.0367 - acc: 0.9773 - val_loss: 0.1093 - val_acc: 0.9694
Epoch 35/40
 - 4s - loss: 0.0369 - acc: 0.9769 - val_loss: 0.1056 - val_acc: 0.9696
Epoch 36/40
 - 4s - loss: 0.0365 - acc: 0.9777 - val_loss: 0.1110 - val_acc: 0.9687
Epoch 37/40
 - 4s - loss: 0.0365 - acc: 0.9776 - val_loss: 0.1092 - val_acc: 0.9691
Epoch 38/40
 - 4s - loss: 0.0363 - acc: 0.9774 - val_loss: 0.1025 - val_acc: 0.9693
Epoch 39/40
 - 4s - loss: 0.0360 - acc: 0.9778 - val_loss: 0.1124 - val_acc: 0.9689
Epoch 40/40
 - 4s - loss: 0.0358 - acc: 0.9779 - val_loss: 0.1110 - val_acc: 0.9692
Epoch 1/40
 - 0s - loss: 0.0845 - acc: 0.9656
Epoch 2/40
 - 0s - loss: 0.0598 - acc: 0.9692
Epoch 3/40
 - 0s - loss: 0.0527 - acc: 0.9717
Epoch 4/40
 - 0s - loss: 0.0485 - acc: 0.9723
Epoch 5/40
 - 0s - loss: 0.0464 - acc: 0.9745
Epoch 6/40
 - 0s - loss: 0.0445 - acc: 0.9753
Epoch 7/40
 - 0s - loss: 0.0426 - acc: 0.9759
Epoch 8/40
 - 0s - loss: 0.0406 - acc: 0.9761
Epoch 9/40
 - 0s - loss: 0.0384 - acc: 0.9770
Epoch 10/40
 - 0s - loss: 0.0380 - acc: 0.9764
Epoch 11/40
 - 0s - loss: 0.0377 - acc: 0.9763
Epoch 12/40
 - 0s - loss: 0.0359 - acc: 0.9792
Epoch 13/40
 - 0s - loss: 0.0356 - acc: 0.9789
Epoch 14/40
 - 0s - loss: 0.0358 - acc: 0.9775
Epoch 15/40
 - 0s - loss: 0.0351 - acc: 0.9775
Epoch 16/40
 - 0s - loss: 0.0353 - acc: 0.9779
Epoch 17/40
 - 0s - loss: 0.0346 - acc: 0.9782
Epoch 18/40
 - 0s - loss: 0.0346 - acc: 0.9786
Epoch 19/40
 - 0s - loss: 0.0351 - acc: 0.9785
Epoch 20/40
 - 0s - loss: 0.0345 - acc: 0.9781
Epoch 21/40
 - 0s - loss: 0.0338 - acc: 0.9783
Epoch 22/40
 - 0s - loss: 0.0341 - acc: 0.9789
Epoch 23/40
 - 0s - loss: 0.0334 - acc: 0.9778
Epoch 24/40
 - 0s - loss: 0.0339 - acc: 0.9786
Epoch 25/40
 - 0s - loss: 0.0333 - acc: 0.9794
Epoch 26/40
 - 0s - loss: 0.0341 - acc: 0.9784
Epoch 27/40
 - 0s - loss: 0.0324 - acc: 0.9788
Epoch 28/40
 - 0s - loss: 0.0329 - acc: 0.9789
Epoch 29/40
 - 0s - loss: 0.0332 - acc: 0.9791
Epoch 30/40
 - 0s - loss: 0.0323 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0328 - acc: 0.9794
Epoch 32/40
 - 0s - loss: 0.0327 - acc: 0.9795
Epoch 33/40
 - 0s - loss: 0.0334 - acc: 0.9799
Epoch 34/40
 - 0s - loss: 0.0324 - acc: 0.9800
Epoch 35/40
 - 0s - loss: 0.0328 - acc: 0.9786
Epoch 36/40
 - 0s - loss: 0.0333 - acc: 0.9787
Epoch 37/40
 - 0s - loss: 0.0324 - acc: 0.9796
Epoch 38/40
 - 0s - loss: 0.0315 - acc: 0.9790
Epoch 39/40
 - 0s - loss: 0.0319 - acc: 0.9792
Epoch 40/40
 - 0s - loss: 0.0333 - acc: 0.9786
# Training time = 0:03:21.686522
# F-Score(Ordinary) = 0.138, Recall: 0.369, Precision: 0.085
# F-Score(lvc) = 0.28, Recall: 0.719, Precision: 0.174
# F-Score(ireflv) = 0.108, Recall: 0.159, Precision: 0.082
# F-Score(id) = 0.04, Recall: 0.5, Precision: 0.021
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 48)        705264      input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 24)        5640        input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 192)          0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 96)           0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 288)          0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 24)           6936        concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 24)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            200         dropout_18[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1179 - acc: 0.9577 - val_loss: 0.0793 - val_acc: 0.9668
Epoch 2/40
 - 4s - loss: 0.0742 - acc: 0.9673 - val_loss: 0.0732 - val_acc: 0.9672
Epoch 3/40
 - 4s - loss: 0.0658 - acc: 0.9696 - val_loss: 0.0722 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0607 - acc: 0.9711 - val_loss: 0.0753 - val_acc: 0.9680
Epoch 5/40
 - 4s - loss: 0.0561 - acc: 0.9721 - val_loss: 0.0739 - val_acc: 0.9689
Epoch 6/40
 - 4s - loss: 0.0533 - acc: 0.9724 - val_loss: 0.0740 - val_acc: 0.9694
Epoch 7/40
 - 4s - loss: 0.0505 - acc: 0.9738 - val_loss: 0.0722 - val_acc: 0.9692
Epoch 8/40
 - 4s - loss: 0.0486 - acc: 0.9740 - val_loss: 0.0817 - val_acc: 0.9685
Epoch 9/40
 - 4s - loss: 0.0469 - acc: 0.9744 - val_loss: 0.0791 - val_acc: 0.9683
Epoch 10/40
 - 4s - loss: 0.0457 - acc: 0.9747 - val_loss: 0.0741 - val_acc: 0.9697
Epoch 11/40
 - 4s - loss: 0.0443 - acc: 0.9749 - val_loss: 0.0823 - val_acc: 0.9690
Epoch 12/40
 - 4s - loss: 0.0436 - acc: 0.9750 - val_loss: 0.0899 - val_acc: 0.9682
Epoch 13/40
 - 4s - loss: 0.0425 - acc: 0.9758 - val_loss: 0.0813 - val_acc: 0.9691
Epoch 14/40
 - 4s - loss: 0.0417 - acc: 0.9760 - val_loss: 0.0943 - val_acc: 0.9675
Epoch 15/40
 - 4s - loss: 0.0413 - acc: 0.9757 - val_loss: 0.0883 - val_acc: 0.9688
Epoch 16/40
 - 4s - loss: 0.0409 - acc: 0.9763 - val_loss: 0.0850 - val_acc: 0.9689
Epoch 17/40
 - 4s - loss: 0.0401 - acc: 0.9766 - val_loss: 0.0904 - val_acc: 0.9687
Epoch 18/40
 - 4s - loss: 0.0399 - acc: 0.9762 - val_loss: 0.0902 - val_acc: 0.9691
Epoch 19/40
 - 4s - loss: 0.0397 - acc: 0.9765 - val_loss: 0.0987 - val_acc: 0.9686
Epoch 20/40
 - 4s - loss: 0.0393 - acc: 0.9762 - val_loss: 0.0937 - val_acc: 0.9687
Epoch 21/40
 - 4s - loss: 0.0387 - acc: 0.9764 - val_loss: 0.0924 - val_acc: 0.9697
Epoch 22/40
 - 4s - loss: 0.0382 - acc: 0.9766 - val_loss: 0.1000 - val_acc: 0.9688
Epoch 23/40
 - 4s - loss: 0.0381 - acc: 0.9771 - val_loss: 0.0935 - val_acc: 0.9690
Epoch 24/40
 - 4s - loss: 0.0376 - acc: 0.9772 - val_loss: 0.1054 - val_acc: 0.9692
Epoch 25/40
 - 4s - loss: 0.0375 - acc: 0.9768 - val_loss: 0.0997 - val_acc: 0.9691
Epoch 26/40
 - 4s - loss: 0.0373 - acc: 0.9767 - val_loss: 0.0910 - val_acc: 0.9684
Epoch 27/40
 - 4s - loss: 0.0370 - acc: 0.9772 - val_loss: 0.1052 - val_acc: 0.9687
Epoch 28/40
 - 4s - loss: 0.0368 - acc: 0.9776 - val_loss: 0.0947 - val_acc: 0.9685
Epoch 29/40
 - 4s - loss: 0.0365 - acc: 0.9775 - val_loss: 0.0992 - val_acc: 0.9687
Epoch 30/40
 - 4s - loss: 0.0366 - acc: 0.9772 - val_loss: 0.1049 - val_acc: 0.9690
Epoch 31/40
 - 4s - loss: 0.0366 - acc: 0.9771 - val_loss: 0.1067 - val_acc: 0.9684
Epoch 32/40
 - 4s - loss: 0.0362 - acc: 0.9779 - val_loss: 0.1043 - val_acc: 0.9693
Epoch 33/40
 - 4s - loss: 0.0361 - acc: 0.9773 - val_loss: 0.1048 - val_acc: 0.9695
Epoch 34/40
 - 4s - loss: 0.0358 - acc: 0.9773 - val_loss: 0.0980 - val_acc: 0.9685
Epoch 35/40
 - 4s - loss: 0.0356 - acc: 0.9776 - val_loss: 0.1016 - val_acc: 0.9687
Epoch 36/40
 - 4s - loss: 0.0355 - acc: 0.9779 - val_loss: 0.1037 - val_acc: 0.9688
Epoch 37/40
 - 4s - loss: 0.0353 - acc: 0.9778 - val_loss: 0.1006 - val_acc: 0.9697
Epoch 38/40
 - 4s - loss: 0.0350 - acc: 0.9777 - val_loss: 0.1122 - val_acc: 0.9690
Epoch 39/40
 - 4s - loss: 0.0353 - acc: 0.9775 - val_loss: 0.1047 - val_acc: 0.9688
Epoch 40/40
 - 4s - loss: 0.0354 - acc: 0.9774 - val_loss: 0.1064 - val_acc: 0.9688
Epoch 1/40
 - 0s - loss: 0.0827 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.0600 - acc: 0.9691
Epoch 3/40
 - 0s - loss: 0.0502 - acc: 0.9726
Epoch 4/40
 - 0s - loss: 0.0469 - acc: 0.9728
Epoch 5/40
 - 0s - loss: 0.0431 - acc: 0.9740
Epoch 6/40
 - 0s - loss: 0.0411 - acc: 0.9739
Epoch 7/40
 - 0s - loss: 0.0393 - acc: 0.9747
Epoch 8/40
 - 0s - loss: 0.0381 - acc: 0.9755
Epoch 9/40
 - 0s - loss: 0.0373 - acc: 0.9756
Epoch 10/40
 - 0s - loss: 0.0366 - acc: 0.9775
Epoch 11/40
 - 0s - loss: 0.0359 - acc: 0.9760
Epoch 12/40
 - 0s - loss: 0.0353 - acc: 0.9769
Epoch 13/40
 - 0s - loss: 0.0342 - acc: 0.9777
Epoch 14/40
 - 0s - loss: 0.0339 - acc: 0.9781
Epoch 15/40
 - 0s - loss: 0.0341 - acc: 0.9783
Epoch 16/40
 - 0s - loss: 0.0333 - acc: 0.9779
Epoch 17/40
 - 0s - loss: 0.0338 - acc: 0.9788
Epoch 18/40
 - 0s - loss: 0.0327 - acc: 0.9793
Epoch 19/40
 - 0s - loss: 0.0330 - acc: 0.9779
Epoch 20/40
 - 0s - loss: 0.0328 - acc: 0.9772
Epoch 21/40
 - 0s - loss: 0.0323 - acc: 0.9773
Epoch 22/40
 - 0s - loss: 0.0320 - acc: 0.9793
Epoch 23/40
 - 0s - loss: 0.0322 - acc: 0.9787
Epoch 24/40
 - 0s - loss: 0.0322 - acc: 0.9783
Epoch 25/40
 - 0s - loss: 0.0319 - acc: 0.9792
Epoch 26/40
 - 0s - loss: 0.0317 - acc: 0.9785
Epoch 27/40
 - 0s - loss: 0.0314 - acc: 0.9783
Epoch 28/40
 - 0s - loss: 0.0318 - acc: 0.9789
Epoch 29/40
 - 0s - loss: 0.0319 - acc: 0.9774
Epoch 30/40
 - 0s - loss: 0.0317 - acc: 0.9791
Epoch 31/40
 - 0s - loss: 0.0315 - acc: 0.9788
Epoch 32/40
 - 0s - loss: 0.0315 - acc: 0.9775
Epoch 33/40
 - 0s - loss: 0.0307 - acc: 0.9795
Epoch 34/40
 - 0s - loss: 0.0310 - acc: 0.9787
Epoch 35/40
 - 0s - loss: 0.0316 - acc: 0.9784
Epoch 36/40
 - 0s - loss: 0.0313 - acc: 0.9787
Epoch 37/40
 - 0s - loss: 0.0313 - acc: 0.9797
Epoch 38/40
 - 0s - loss: 0.0317 - acc: 0.9794
Epoch 39/40
 - 0s - loss: 0.0307 - acc: 0.9794
Epoch 40/40
 - 0s - loss: 0.0318 - acc: 0.9788
# Training time = 0:03:21.792873
# F-Score(Ordinary) = 0.157, Recall: 0.796, Precision: 0.087
# F-Score(lvc) = 0.071, Recall: 0.556, Precision: 0.038
# F-Score(id) = 0.294, Recall: 0.895, Precision: 0.176
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 48)        705264      input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 24)        5640        input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 192)          0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 96)           0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 288)          0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 24)           6936        concatenate_19[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 24)           0           dense_37[0][0]                   
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            200         dropout_19[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1239 - acc: 0.9567 - val_loss: 0.0827 - val_acc: 0.9651
Epoch 2/40
 - 4s - loss: 0.0775 - acc: 0.9668 - val_loss: 0.0912 - val_acc: 0.9592
Epoch 3/40
 - 4s - loss: 0.0689 - acc: 0.9691 - val_loss: 0.0732 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0634 - acc: 0.9707 - val_loss: 0.0746 - val_acc: 0.9683
Epoch 5/40
 - 4s - loss: 0.0597 - acc: 0.9719 - val_loss: 0.0781 - val_acc: 0.9672
Epoch 6/40
 - 4s - loss: 0.0563 - acc: 0.9726 - val_loss: 0.0739 - val_acc: 0.9687
Epoch 7/40
 - 4s - loss: 0.0538 - acc: 0.9733 - val_loss: 0.0766 - val_acc: 0.9680
Epoch 8/40
 - 4s - loss: 0.0509 - acc: 0.9741 - val_loss: 0.0794 - val_acc: 0.9685
Epoch 9/40
 - 4s - loss: 0.0494 - acc: 0.9741 - val_loss: 0.0797 - val_acc: 0.9685
Epoch 10/40
 - 4s - loss: 0.0474 - acc: 0.9750 - val_loss: 0.0766 - val_acc: 0.9690
Epoch 11/40
 - 4s - loss: 0.0465 - acc: 0.9749 - val_loss: 0.0827 - val_acc: 0.9674
Epoch 12/40
 - 4s - loss: 0.0454 - acc: 0.9755 - val_loss: 0.0821 - val_acc: 0.9682
Epoch 13/40
 - 4s - loss: 0.0443 - acc: 0.9752 - val_loss: 0.0875 - val_acc: 0.9687
Epoch 14/40
 - 4s - loss: 0.0437 - acc: 0.9755 - val_loss: 0.0825 - val_acc: 0.9681
Epoch 15/40
 - 4s - loss: 0.0430 - acc: 0.9759 - val_loss: 0.0924 - val_acc: 0.9679
Epoch 16/40
 - 4s - loss: 0.0419 - acc: 0.9763 - val_loss: 0.0866 - val_acc: 0.9691
Epoch 17/40
 - 4s - loss: 0.0417 - acc: 0.9764 - val_loss: 0.0844 - val_acc: 0.9690
Epoch 18/40
 - 4s - loss: 0.0408 - acc: 0.9762 - val_loss: 0.0923 - val_acc: 0.9693
Epoch 19/40
 - 4s - loss: 0.0410 - acc: 0.9764 - val_loss: 0.1019 - val_acc: 0.9665
Epoch 20/40
 - 4s - loss: 0.0407 - acc: 0.9767 - val_loss: 0.0920 - val_acc: 0.9691
Epoch 21/40
 - 4s - loss: 0.0400 - acc: 0.9769 - val_loss: 0.1043 - val_acc: 0.9685
Epoch 22/40
 - 4s - loss: 0.0398 - acc: 0.9768 - val_loss: 0.0974 - val_acc: 0.9687
Epoch 23/40
 - 4s - loss: 0.0396 - acc: 0.9766 - val_loss: 0.1017 - val_acc: 0.9689
Epoch 24/40
 - 4s - loss: 0.0394 - acc: 0.9767 - val_loss: 0.0971 - val_acc: 0.9691
Epoch 25/40
 - 4s - loss: 0.0389 - acc: 0.9768 - val_loss: 0.0959 - val_acc: 0.9697
Epoch 26/40
 - 4s - loss: 0.0390 - acc: 0.9771 - val_loss: 0.0987 - val_acc: 0.9686
Epoch 27/40
 - 4s - loss: 0.0385 - acc: 0.9770 - val_loss: 0.1022 - val_acc: 0.9685
Epoch 28/40
 - 4s - loss: 0.0385 - acc: 0.9768 - val_loss: 0.0997 - val_acc: 0.9696
Epoch 29/40
 - 4s - loss: 0.0384 - acc: 0.9768 - val_loss: 0.0973 - val_acc: 0.9682
Epoch 30/40
 - 4s - loss: 0.0376 - acc: 0.9774 - val_loss: 0.0991 - val_acc: 0.9693
Epoch 31/40
 - 4s - loss: 0.0377 - acc: 0.9774 - val_loss: 0.1030 - val_acc: 0.9686
Epoch 32/40
 - 4s - loss: 0.0383 - acc: 0.9771 - val_loss: 0.1000 - val_acc: 0.9690
Epoch 33/40
 - 4s - loss: 0.0375 - acc: 0.9770 - val_loss: 0.1093 - val_acc: 0.9693
Epoch 34/40
 - 4s - loss: 0.0374 - acc: 0.9774 - val_loss: 0.1124 - val_acc: 0.9685
Epoch 35/40
 - 4s - loss: 0.0375 - acc: 0.9777 - val_loss: 0.1068 - val_acc: 0.9687
Epoch 36/40
 - 4s - loss: 0.0373 - acc: 0.9770 - val_loss: 0.1048 - val_acc: 0.9687
Epoch 37/40
 - 4s - loss: 0.0368 - acc: 0.9775 - val_loss: 0.1130 - val_acc: 0.9691
Epoch 38/40
 - 4s - loss: 0.0371 - acc: 0.9772 - val_loss: 0.1127 - val_acc: 0.9692
Epoch 39/40
 - 4s - loss: 0.0369 - acc: 0.9777 - val_loss: 0.1085 - val_acc: 0.9695
Epoch 40/40
 - 4s - loss: 0.0365 - acc: 0.9771 - val_loss: 0.1092 - val_acc: 0.9691
Epoch 1/40
 - 0s - loss: 0.0843 - acc: 0.9657
Epoch 2/40
 - 0s - loss: 0.0600 - acc: 0.9694
Epoch 3/40
 - 0s - loss: 0.0533 - acc: 0.9728
Epoch 4/40
 - 0s - loss: 0.0497 - acc: 0.9719
Epoch 5/40
 - 0s - loss: 0.0456 - acc: 0.9733
Epoch 6/40
 - 0s - loss: 0.0442 - acc: 0.9735
Epoch 7/40
 - 0s - loss: 0.0422 - acc: 0.9746
Epoch 8/40
 - 0s - loss: 0.0406 - acc: 0.9766
Epoch 9/40
 - 0s - loss: 0.0387 - acc: 0.9770
Epoch 10/40
 - 0s - loss: 0.0384 - acc: 0.9765
Epoch 11/40
 - 0s - loss: 0.0373 - acc: 0.9778
Epoch 12/40
 - 0s - loss: 0.0370 - acc: 0.9775
Epoch 13/40
 - 0s - loss: 0.0376 - acc: 0.9772
Epoch 14/40
 - 0s - loss: 0.0368 - acc: 0.9777
Epoch 15/40
 - 0s - loss: 0.0371 - acc: 0.9780
Epoch 16/40
 - 0s - loss: 0.0363 - acc: 0.9787
Epoch 17/40
 - 0s - loss: 0.0360 - acc: 0.9771
Epoch 18/40
 - 0s - loss: 0.0352 - acc: 0.9788
Epoch 19/40
 - 0s - loss: 0.0354 - acc: 0.9792
Epoch 20/40
 - 0s - loss: 0.0346 - acc: 0.9790
Epoch 21/40
 - 0s - loss: 0.0343 - acc: 0.9778
Epoch 22/40
 - 0s - loss: 0.0353 - acc: 0.9781
Epoch 23/40
 - 0s - loss: 0.0351 - acc: 0.9773
Epoch 24/40
 - 0s - loss: 0.0349 - acc: 0.9797
Epoch 25/40
 - 0s - loss: 0.0348 - acc: 0.9776
Epoch 26/40
 - 0s - loss: 0.0343 - acc: 0.9785
Epoch 27/40
 - 0s - loss: 0.0349 - acc: 0.9791
Epoch 28/40
 - 0s - loss: 0.0346 - acc: 0.9778
Epoch 29/40
 - 0s - loss: 0.0342 - acc: 0.9797
Epoch 30/40
 - 0s - loss: 0.0339 - acc: 0.9789
Epoch 31/40
 - 0s - loss: 0.0349 - acc: 0.9786
Epoch 32/40
 - 0s - loss: 0.0336 - acc: 0.9797
Epoch 33/40
 - 0s - loss: 0.0341 - acc: 0.9791
Epoch 34/40
 - 0s - loss: 0.0341 - acc: 0.9790
Epoch 35/40
 - 0s - loss: 0.0342 - acc: 0.9786
Epoch 36/40
 - 0s - loss: 0.0335 - acc: 0.9804
Epoch 37/40
 - 0s - loss: 0.0338 - acc: 0.9785
Epoch 38/40
 - 0s - loss: 0.0329 - acc: 0.9786
Epoch 39/40
 - 0s - loss: 0.0341 - acc: 0.9786
Epoch 40/40
 - 0s - loss: 0.0337 - acc: 0.9784
# Training time = 0:03:22.310137
# F-Score(Ordinary) = 0.228, Recall: 0.759, Precision: 0.134
# F-Score(lvc) = 0.446, Recall: 0.907, Precision: 0.295
# F-Score(id) = 0.183, Recall: 0.583, Precision: 0.109
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_39 (Embedding)        (None, 4, 48)        705264      input_39[0][0]                   
__________________________________________________________________________________________________
embedding_40 (Embedding)        (None, 4, 24)        5640        input_40[0][0]                   
__________________________________________________________________________________________________
flatten_39 (Flatten)            (None, 192)          0           embedding_39[0][0]               
__________________________________________________________________________________________________
flatten_40 (Flatten)            (None, 96)           0           embedding_40[0][0]               
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 288)          0           flatten_39[0][0]                 
                                                                 flatten_40[0][0]                 
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 24)           6936        concatenate_20[0][0]             
__________________________________________________________________________________________________
dropout_20 (Dropout)            (None, 24)           0           dense_39[0][0]                   
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 8)            200         dropout_20[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1231 - acc: 0.9567 - val_loss: 0.0867 - val_acc: 0.9644
Epoch 2/40
 - 4s - loss: 0.0783 - acc: 0.9667 - val_loss: 0.0796 - val_acc: 0.9669
Epoch 3/40
 - 4s - loss: 0.0688 - acc: 0.9688 - val_loss: 0.0756 - val_acc: 0.9683
Epoch 4/40
 - 4s - loss: 0.0633 - acc: 0.9704 - val_loss: 0.0720 - val_acc: 0.9681
Epoch 5/40
 - 4s - loss: 0.0595 - acc: 0.9714 - val_loss: 0.0752 - val_acc: 0.9672
Epoch 6/40
 - 4s - loss: 0.0561 - acc: 0.9723 - val_loss: 0.0739 - val_acc: 0.9688
Epoch 7/40
 - 4s - loss: 0.0535 - acc: 0.9728 - val_loss: 0.0802 - val_acc: 0.9684
Epoch 8/40
 - 4s - loss: 0.0516 - acc: 0.9734 - val_loss: 0.0798 - val_acc: 0.9685
Epoch 9/40
 - 4s - loss: 0.0494 - acc: 0.9737 - val_loss: 0.0813 - val_acc: 0.9681
Epoch 10/40
 - 4s - loss: 0.0479 - acc: 0.9743 - val_loss: 0.0841 - val_acc: 0.9680
Epoch 11/40
 - 4s - loss: 0.0471 - acc: 0.9753 - val_loss: 0.0888 - val_acc: 0.9681
Epoch 12/40
 - 4s - loss: 0.0460 - acc: 0.9750 - val_loss: 0.0801 - val_acc: 0.9690
Epoch 13/40
 - 4s - loss: 0.0451 - acc: 0.9754 - val_loss: 0.0826 - val_acc: 0.9687
Epoch 14/40
 - 4s - loss: 0.0444 - acc: 0.9757 - val_loss: 0.0875 - val_acc: 0.9690
Epoch 15/40
 - 4s - loss: 0.0440 - acc: 0.9756 - val_loss: 0.0799 - val_acc: 0.9687
Epoch 16/40
 - 4s - loss: 0.0432 - acc: 0.9758 - val_loss: 0.0927 - val_acc: 0.9684
Epoch 17/40
 - 4s - loss: 0.0422 - acc: 0.9761 - val_loss: 0.0864 - val_acc: 0.9692
Epoch 18/40
 - 4s - loss: 0.0423 - acc: 0.9761 - val_loss: 0.0915 - val_acc: 0.9685
Epoch 19/40
 - 4s - loss: 0.0420 - acc: 0.9763 - val_loss: 0.0949 - val_acc: 0.9676
Epoch 20/40
 - 4s - loss: 0.0415 - acc: 0.9760 - val_loss: 0.0857 - val_acc: 0.9692
Epoch 21/40
 - 4s - loss: 0.0407 - acc: 0.9766 - val_loss: 0.0897 - val_acc: 0.9684
Epoch 22/40
 - 4s - loss: 0.0403 - acc: 0.9766 - val_loss: 0.0948 - val_acc: 0.9681
Epoch 23/40
 - 4s - loss: 0.0401 - acc: 0.9766 - val_loss: 0.0978 - val_acc: 0.9693
Epoch 24/40
 - 4s - loss: 0.0394 - acc: 0.9769 - val_loss: 0.0972 - val_acc: 0.9700
Epoch 25/40
 - 4s - loss: 0.0393 - acc: 0.9770 - val_loss: 0.0911 - val_acc: 0.9686
Epoch 26/40
 - 4s - loss: 0.0390 - acc: 0.9772 - val_loss: 0.1009 - val_acc: 0.9685
Epoch 27/40
 - 4s - loss: 0.0388 - acc: 0.9772 - val_loss: 0.0954 - val_acc: 0.9690
Epoch 28/40
 - 4s - loss: 0.0382 - acc: 0.9773 - val_loss: 0.1018 - val_acc: 0.9693
Epoch 29/40
 - 4s - loss: 0.0384 - acc: 0.9773 - val_loss: 0.1014 - val_acc: 0.9685
Epoch 30/40
 - 4s - loss: 0.0390 - acc: 0.9769 - val_loss: 0.1017 - val_acc: 0.9685
Epoch 31/40
 - 4s - loss: 0.0382 - acc: 0.9775 - val_loss: 0.1038 - val_acc: 0.9689
Epoch 32/40
 - 4s - loss: 0.0380 - acc: 0.9775 - val_loss: 0.0999 - val_acc: 0.9689
Epoch 33/40
 - 4s - loss: 0.0380 - acc: 0.9771 - val_loss: 0.1072 - val_acc: 0.9687
Epoch 34/40
 - 4s - loss: 0.0375 - acc: 0.9777 - val_loss: 0.1099 - val_acc: 0.9678
Epoch 35/40
 - 4s - loss: 0.0373 - acc: 0.9774 - val_loss: 0.1124 - val_acc: 0.9688
Epoch 36/40
 - 4s - loss: 0.0378 - acc: 0.9776 - val_loss: 0.1034 - val_acc: 0.9690
Epoch 37/40
 - 4s - loss: 0.0372 - acc: 0.9774 - val_loss: 0.1087 - val_acc: 0.9687
Epoch 38/40
 - 4s - loss: 0.0372 - acc: 0.9775 - val_loss: 0.0970 - val_acc: 0.9686
Epoch 39/40
 - 4s - loss: 0.0372 - acc: 0.9775 - val_loss: 0.1143 - val_acc: 0.9686
Epoch 40/40
 - 4s - loss: 0.0374 - acc: 0.9772 - val_loss: 0.1070 - val_acc: 0.9692
Epoch 1/40
 - 0s - loss: 0.0838 - acc: 0.9655
Epoch 2/40
 - 0s - loss: 0.0629 - acc: 0.9687
Epoch 3/40
 - 0s - loss: 0.0547 - acc: 0.9712
Epoch 4/40
 - 0s - loss: 0.0502 - acc: 0.9747
Epoch 5/40
 - 0s - loss: 0.0469 - acc: 0.9758
Epoch 6/40
 - 0s - loss: 0.0445 - acc: 0.9744
Epoch 7/40
 - 0s - loss: 0.0431 - acc: 0.9760
Epoch 8/40
 - 0s - loss: 0.0410 - acc: 0.9752
Epoch 9/40
 - 0s - loss: 0.0407 - acc: 0.9757
Epoch 10/40
 - 0s - loss: 0.0391 - acc: 0.9763
Epoch 11/40
 - 0s - loss: 0.0392 - acc: 0.9774
Epoch 12/40
 - 0s - loss: 0.0379 - acc: 0.9762
Epoch 13/40
 - 0s - loss: 0.0391 - acc: 0.9772
Epoch 14/40
 - 0s - loss: 0.0379 - acc: 0.9777
Epoch 15/40
 - 0s - loss: 0.0388 - acc: 0.9772
Epoch 16/40
 - 0s - loss: 0.0379 - acc: 0.9769
Epoch 17/40
 - 0s - loss: 0.0373 - acc: 0.9778
Epoch 18/40
 - 0s - loss: 0.0363 - acc: 0.9782
Epoch 19/40
 - 0s - loss: 0.0367 - acc: 0.9763
Epoch 20/40
 - 0s - loss: 0.0378 - acc: 0.9774
Epoch 21/40
 - 0s - loss: 0.0361 - acc: 0.9775
Epoch 22/40
 - 0s - loss: 0.0370 - acc: 0.9780
Epoch 23/40
 - 0s - loss: 0.0361 - acc: 0.9781
Epoch 24/40
 - 0s - loss: 0.0356 - acc: 0.9785
Epoch 25/40
 - 0s - loss: 0.0366 - acc: 0.9772
Epoch 26/40
 - 0s - loss: 0.0355 - acc: 0.9788
Epoch 27/40
 - 0s - loss: 0.0350 - acc: 0.9777
Epoch 28/40
 - 0s - loss: 0.0352 - acc: 0.9778
Epoch 29/40
 - 0s - loss: 0.0351 - acc: 0.9795
Epoch 30/40
 - 0s - loss: 0.0355 - acc: 0.9785
Epoch 31/40
 - 0s - loss: 0.0356 - acc: 0.9790
Epoch 32/40
 - 0s - loss: 0.0347 - acc: 0.9789
Epoch 33/40
 - 0s - loss: 0.0348 - acc: 0.9789
Epoch 34/40
 - 0s - loss: 0.0347 - acc: 0.9781
Epoch 35/40
 - 0s - loss: 0.0347 - acc: 0.9789
Epoch 36/40
 - 0s - loss: 0.0347 - acc: 0.9788
Epoch 37/40
 - 0s - loss: 0.0345 - acc: 0.9791
Epoch 38/40
 - 0s - loss: 0.0349 - acc: 0.9789
Epoch 39/40
 - 0s - loss: 0.0353 - acc: 0.9784
Epoch 40/40
 - 0s - loss: 0.0355 - acc: 0.9780
# Training time = 0:03:21.950046
# F-Score(Ordinary) = 0.574, Recall: 0.684, Precision: 0.494
# F-Score(lvc) = 0.579, Recall: 0.756, Precision: 0.47
# F-Score(ireflv) = 0.646, Recall: 0.886, Precision: 0.508
# F-Score(id) = 0.527, Recall: 0.561, Precision: 0.497
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_41 (Embedding)        (None, 4, 48)        705264      input_41[0][0]                   
__________________________________________________________________________________________________
embedding_42 (Embedding)        (None, 4, 24)        5640        input_42[0][0]                   
__________________________________________________________________________________________________
flatten_41 (Flatten)            (None, 192)          0           embedding_41[0][0]               
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 96)           0           embedding_42[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 288)          0           flatten_41[0][0]                 
                                                                 flatten_42[0][0]                 
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 24)           6936        concatenate_21[0][0]             
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 24)           0           dense_41[0][0]                   
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 8)            200         dropout_21[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1097 - acc: 0.9596 - val_loss: 0.0827 - val_acc: 0.9662
Epoch 2/40
 - 4s - loss: 0.0745 - acc: 0.9677 - val_loss: 0.0762 - val_acc: 0.9681
Epoch 3/40
 - 4s - loss: 0.0658 - acc: 0.9696 - val_loss: 0.0763 - val_acc: 0.9678
Epoch 4/40
 - 4s - loss: 0.0607 - acc: 0.9710 - val_loss: 0.0745 - val_acc: 0.9679
Epoch 5/40
 - 4s - loss: 0.0570 - acc: 0.9718 - val_loss: 0.0776 - val_acc: 0.9678
Epoch 6/40
 - 4s - loss: 0.0542 - acc: 0.9725 - val_loss: 0.0819 - val_acc: 0.9682
Epoch 7/40
 - 4s - loss: 0.0517 - acc: 0.9732 - val_loss: 0.0828 - val_acc: 0.9682
Epoch 8/40
 - 4s - loss: 0.0496 - acc: 0.9732 - val_loss: 0.0861 - val_acc: 0.9688
Epoch 9/40
 - 4s - loss: 0.0480 - acc: 0.9744 - val_loss: 0.0908 - val_acc: 0.9680
Epoch 10/40
 - 4s - loss: 0.0474 - acc: 0.9740 - val_loss: 0.0923 - val_acc: 0.9675
Epoch 11/40
 - 4s - loss: 0.0458 - acc: 0.9750 - val_loss: 0.0902 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0445 - acc: 0.9752 - val_loss: 0.0830 - val_acc: 0.9691
Epoch 13/40
 - 4s - loss: 0.0445 - acc: 0.9753 - val_loss: 0.0911 - val_acc: 0.9678
Epoch 14/40
 - 4s - loss: 0.0435 - acc: 0.9753 - val_loss: 0.0924 - val_acc: 0.9687
Epoch 15/40
 - 4s - loss: 0.0432 - acc: 0.9756 - val_loss: 0.0924 - val_acc: 0.9682
Epoch 16/40
 - 4s - loss: 0.0430 - acc: 0.9755 - val_loss: 0.0929 - val_acc: 0.9683
Epoch 17/40
 - 4s - loss: 0.0423 - acc: 0.9760 - val_loss: 0.0985 - val_acc: 0.9671
Epoch 18/40
 - 4s - loss: 0.0418 - acc: 0.9759 - val_loss: 0.0993 - val_acc: 0.9695
Epoch 19/40
 - 4s - loss: 0.0413 - acc: 0.9759 - val_loss: 0.0978 - val_acc: 0.9673
Epoch 20/40
 - 4s - loss: 0.0419 - acc: 0.9759 - val_loss: 0.1071 - val_acc: 0.9689
Epoch 21/40
 - 4s - loss: 0.0408 - acc: 0.9763 - val_loss: 0.0998 - val_acc: 0.9684
Epoch 22/40
 - 4s - loss: 0.0407 - acc: 0.9762 - val_loss: 0.1016 - val_acc: 0.9683
Epoch 23/40
 - 4s - loss: 0.0401 - acc: 0.9766 - val_loss: 0.1001 - val_acc: 0.9685
Epoch 24/40
 - 4s - loss: 0.0404 - acc: 0.9760 - val_loss: 0.1077 - val_acc: 0.9694
Epoch 25/40
 - 4s - loss: 0.0401 - acc: 0.9766 - val_loss: 0.1037 - val_acc: 0.9690
Epoch 26/40
 - 4s - loss: 0.0392 - acc: 0.9768 - val_loss: 0.1097 - val_acc: 0.9686
Epoch 27/40
 - 4s - loss: 0.0394 - acc: 0.9767 - val_loss: 0.1057 - val_acc: 0.9687
Epoch 28/40
 - 4s - loss: 0.0391 - acc: 0.9767 - val_loss: 0.1048 - val_acc: 0.9687
Epoch 29/40
 - 4s - loss: 0.0388 - acc: 0.9767 - val_loss: 0.1092 - val_acc: 0.9679
Epoch 30/40
 - 4s - loss: 0.0387 - acc: 0.9766 - val_loss: 0.1200 - val_acc: 0.9696
Epoch 31/40
 - 4s - loss: 0.0388 - acc: 0.9770 - val_loss: 0.1079 - val_acc: 0.9688
Epoch 32/40
 - 4s - loss: 0.0386 - acc: 0.9770 - val_loss: 0.1057 - val_acc: 0.9695
Epoch 33/40
 - 4s - loss: 0.0384 - acc: 0.9772 - val_loss: 0.1051 - val_acc: 0.9697
Epoch 34/40
 - 4s - loss: 0.0383 - acc: 0.9770 - val_loss: 0.1092 - val_acc: 0.9688
Epoch 35/40
 - 4s - loss: 0.0381 - acc: 0.9771 - val_loss: 0.1079 - val_acc: 0.9690
Epoch 36/40
 - 4s - loss: 0.0384 - acc: 0.9771 - val_loss: 0.1077 - val_acc: 0.9691
Epoch 37/40
 - 4s - loss: 0.0377 - acc: 0.9774 - val_loss: 0.1115 - val_acc: 0.9690
Epoch 38/40
 - 4s - loss: 0.0377 - acc: 0.9773 - val_loss: 0.1094 - val_acc: 0.9694
Epoch 39/40
 - 4s - loss: 0.0379 - acc: 0.9771 - val_loss: 0.1133 - val_acc: 0.9699
Epoch 40/40
 - 4s - loss: 0.0375 - acc: 0.9775 - val_loss: 0.1169 - val_acc: 0.9684
Epoch 1/40
 - 0s - loss: 0.0920 - acc: 0.9651
Epoch 2/40
 - 0s - loss: 0.0682 - acc: 0.9693
Epoch 3/40
 - 0s - loss: 0.0569 - acc: 0.9714
Epoch 4/40
 - 0s - loss: 0.0502 - acc: 0.9726
Epoch 5/40
 - 0s - loss: 0.0483 - acc: 0.9737
Epoch 6/40
 - 0s - loss: 0.0455 - acc: 0.9753
Epoch 7/40
 - 0s - loss: 0.0447 - acc: 0.9747
Epoch 8/40
 - 0s - loss: 0.0431 - acc: 0.9754
Epoch 9/40
 - 0s - loss: 0.0417 - acc: 0.9761
Epoch 10/40
 - 0s - loss: 0.0405 - acc: 0.9766
Epoch 11/40
 - 0s - loss: 0.0404 - acc: 0.9768
Epoch 12/40
 - 0s - loss: 0.0389 - acc: 0.9769
Epoch 13/40
 - 0s - loss: 0.0386 - acc: 0.9785
Epoch 14/40
 - 0s - loss: 0.0376 - acc: 0.9785
Epoch 15/40
 - 0s - loss: 0.0386 - acc: 0.9785
Epoch 16/40
 - 0s - loss: 0.0390 - acc: 0.9773
Epoch 17/40
 - 0s - loss: 0.0374 - acc: 0.9774
Epoch 18/40
 - 0s - loss: 0.0378 - acc: 0.9773
Epoch 19/40
 - 0s - loss: 0.0379 - acc: 0.9767
Epoch 20/40
 - 0s - loss: 0.0382 - acc: 0.9778
Epoch 21/40
 - 0s - loss: 0.0369 - acc: 0.9780
Epoch 22/40
 - 0s - loss: 0.0378 - acc: 0.9775
Epoch 23/40
 - 0s - loss: 0.0375 - acc: 0.9786
Epoch 24/40
 - 0s - loss: 0.0375 - acc: 0.9775
Epoch 25/40
 - 0s - loss: 0.0367 - acc: 0.9769
Epoch 26/40
 - 0s - loss: 0.0370 - acc: 0.9782
Epoch 27/40
 - 0s - loss: 0.0362 - acc: 0.9782
Epoch 28/40
 - 0s - loss: 0.0366 - acc: 0.9781
Epoch 29/40
 - 0s - loss: 0.0365 - acc: 0.9778
Epoch 30/40
 - 0s - loss: 0.0364 - acc: 0.9784
Epoch 31/40
 - 0s - loss: 0.0356 - acc: 0.9792
Epoch 32/40
 - 0s - loss: 0.0358 - acc: 0.9790
Epoch 33/40
 - 0s - loss: 0.0363 - acc: 0.9780
Epoch 34/40
 - 0s - loss: 0.0349 - acc: 0.9797
Epoch 35/40
 - 0s - loss: 0.0352 - acc: 0.9787
Epoch 36/40
 - 0s - loss: 0.0350 - acc: 0.9776
Epoch 37/40
 - 0s - loss: 0.0344 - acc: 0.9797
Epoch 38/40
 - 0s - loss: 0.0348 - acc: 0.9789
Epoch 39/40
 - 0s - loss: 0.0355 - acc: 0.9768
Epoch 40/40
 - 0s - loss: 0.0347 - acc: 0.9776
# Training time = 0:03:21.767318
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 4, 48)        705264      input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 4, 24)        5640        input_44[0][0]                   
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 192)          0           embedding_43[0][0]               
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 96)           0           embedding_44[0][0]               
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 288)          0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 24)           6936        concatenate_22[0][0]             
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 24)           0           dense_43[0][0]                   
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 8)            200         dropout_22[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1104 - acc: 0.9589 - val_loss: 0.0755 - val_acc: 0.9676
Epoch 2/40
 - 4s - loss: 0.0750 - acc: 0.9673 - val_loss: 0.0838 - val_acc: 0.9659
Epoch 3/40
 - 4s - loss: 0.0672 - acc: 0.9692 - val_loss: 0.0752 - val_acc: 0.9675
Epoch 4/40
 - 4s - loss: 0.0613 - acc: 0.9707 - val_loss: 0.0792 - val_acc: 0.9678
Epoch 5/40
 - 4s - loss: 0.0570 - acc: 0.9720 - val_loss: 0.0769 - val_acc: 0.9692
Epoch 6/40
 - 4s - loss: 0.0533 - acc: 0.9731 - val_loss: 0.0817 - val_acc: 0.9677
Epoch 7/40
 - 4s - loss: 0.0508 - acc: 0.9737 - val_loss: 0.0792 - val_acc: 0.9687
Epoch 8/40
 - 4s - loss: 0.0492 - acc: 0.9737 - val_loss: 0.0884 - val_acc: 0.9692
Epoch 9/40
 - 4s - loss: 0.0482 - acc: 0.9744 - val_loss: 0.0829 - val_acc: 0.9689
Epoch 10/40
 - 4s - loss: 0.0470 - acc: 0.9746 - val_loss: 0.0879 - val_acc: 0.9690
Epoch 11/40
 - 4s - loss: 0.0466 - acc: 0.9748 - val_loss: 0.0946 - val_acc: 0.9685
Epoch 12/40
 - 4s - loss: 0.0459 - acc: 0.9748 - val_loss: 0.0890 - val_acc: 0.9682
Epoch 13/40
 - 4s - loss: 0.0442 - acc: 0.9753 - val_loss: 0.0893 - val_acc: 0.9684
Epoch 14/40
 - 4s - loss: 0.0435 - acc: 0.9754 - val_loss: 0.1022 - val_acc: 0.9687
Epoch 15/40
 - 4s - loss: 0.0439 - acc: 0.9752 - val_loss: 0.0882 - val_acc: 0.9694
Epoch 16/40
 - 4s - loss: 0.0427 - acc: 0.9759 - val_loss: 0.0870 - val_acc: 0.9682
Epoch 17/40
 - 4s - loss: 0.0419 - acc: 0.9759 - val_loss: 0.0930 - val_acc: 0.9696
Epoch 18/40
 - 4s - loss: 0.0421 - acc: 0.9765 - val_loss: 0.1054 - val_acc: 0.9688
Epoch 19/40
 - 4s - loss: 0.0421 - acc: 0.9759 - val_loss: 0.0969 - val_acc: 0.9686
Epoch 20/40
 - 4s - loss: 0.0415 - acc: 0.9759 - val_loss: 0.0972 - val_acc: 0.9694
Epoch 21/40
 - 4s - loss: 0.0413 - acc: 0.9763 - val_loss: 0.1013 - val_acc: 0.9687
Epoch 22/40
 - 4s - loss: 0.0406 - acc: 0.9768 - val_loss: 0.0962 - val_acc: 0.9688
Epoch 23/40
 - 4s - loss: 0.0413 - acc: 0.9760 - val_loss: 0.1019 - val_acc: 0.9684
Epoch 24/40
 - 4s - loss: 0.0408 - acc: 0.9764 - val_loss: 0.1083 - val_acc: 0.9672
Epoch 25/40
 - 4s - loss: 0.0399 - acc: 0.9766 - val_loss: 0.1077 - val_acc: 0.9683
Epoch 26/40
 - 4s - loss: 0.0399 - acc: 0.9761 - val_loss: 0.1015 - val_acc: 0.9687
Epoch 27/40
 - 4s - loss: 0.0399 - acc: 0.9764 - val_loss: 0.1118 - val_acc: 0.9696
Epoch 28/40
 - 4s - loss: 0.0399 - acc: 0.9765 - val_loss: 0.1004 - val_acc: 0.9687
Epoch 29/40
 - 4s - loss: 0.0387 - acc: 0.9771 - val_loss: 0.1052 - val_acc: 0.9686
Epoch 30/40
 - 4s - loss: 0.0392 - acc: 0.9768 - val_loss: 0.1056 - val_acc: 0.9682
Epoch 31/40
 - 4s - loss: 0.0384 - acc: 0.9775 - val_loss: 0.1100 - val_acc: 0.9680
Epoch 32/40
 - 4s - loss: 0.0388 - acc: 0.9767 - val_loss: 0.1023 - val_acc: 0.9690
Epoch 33/40
 - 4s - loss: 0.0391 - acc: 0.9769 - val_loss: 0.1073 - val_acc: 0.9692
Epoch 34/40
 - 4s - loss: 0.0385 - acc: 0.9770 - val_loss: 0.1086 - val_acc: 0.9692
Epoch 35/40
 - 4s - loss: 0.0387 - acc: 0.9768 - val_loss: 0.1087 - val_acc: 0.9687
Epoch 36/40
 - 4s - loss: 0.0385 - acc: 0.9772 - val_loss: 0.1137 - val_acc: 0.9687
Epoch 37/40
 - 4s - loss: 0.0381 - acc: 0.9772 - val_loss: 0.1237 - val_acc: 0.9684
Epoch 38/40
 - 4s - loss: 0.0384 - acc: 0.9770 - val_loss: 0.1120 - val_acc: 0.9684
Epoch 39/40
 - 4s - loss: 0.0380 - acc: 0.9775 - val_loss: 0.1198 - val_acc: 0.9687
Epoch 40/40
 - 4s - loss: 0.0380 - acc: 0.9770 - val_loss: 0.1115 - val_acc: 0.9681
Epoch 1/40
 - 0s - loss: 0.0919 - acc: 0.9653
Epoch 2/40
 - 0s - loss: 0.0678 - acc: 0.9695
Epoch 3/40
 - 0s - loss: 0.0605 - acc: 0.9712
Epoch 4/40
 - 0s - loss: 0.0524 - acc: 0.9727
Epoch 5/40
 - 0s - loss: 0.0494 - acc: 0.9730
Epoch 6/40
 - 0s - loss: 0.0475 - acc: 0.9746
Epoch 7/40
 - 0s - loss: 0.0451 - acc: 0.9761
Epoch 8/40
 - 0s - loss: 0.0447 - acc: 0.9751
Epoch 9/40
 - 0s - loss: 0.0421 - acc: 0.9758
Epoch 10/40
 - 0s - loss: 0.0435 - acc: 0.9755
Epoch 11/40
 - 0s - loss: 0.0415 - acc: 0.9761
Epoch 12/40
 - 0s - loss: 0.0416 - acc: 0.9773
Epoch 13/40
 - 0s - loss: 0.0403 - acc: 0.9785
Epoch 14/40
 - 0s - loss: 0.0388 - acc: 0.9786
Epoch 15/40
 - 0s - loss: 0.0392 - acc: 0.9767
Epoch 16/40
 - 0s - loss: 0.0397 - acc: 0.9775
Epoch 17/40
 - 0s - loss: 0.0378 - acc: 0.9778
Epoch 18/40
 - 0s - loss: 0.0381 - acc: 0.9774
Epoch 19/40
 - 0s - loss: 0.0387 - acc: 0.9781
Epoch 20/40
 - 0s - loss: 0.0387 - acc: 0.9786
Epoch 21/40
 - 0s - loss: 0.0380 - acc: 0.9773
Epoch 22/40
 - 0s - loss: 0.0382 - acc: 0.9779
Epoch 23/40
 - 0s - loss: 0.0381 - acc: 0.9771
Epoch 24/40
 - 0s - loss: 0.0379 - acc: 0.9781
Epoch 25/40
 - 0s - loss: 0.0380 - acc: 0.9791
Epoch 26/40
 - 0s - loss: 0.0372 - acc: 0.9786
Epoch 27/40
 - 0s - loss: 0.0361 - acc: 0.9785
Epoch 28/40
 - 0s - loss: 0.0368 - acc: 0.9785
Epoch 29/40
 - 0s - loss: 0.0368 - acc: 0.9776
Epoch 30/40
 - 0s - loss: 0.0367 - acc: 0.9785
Epoch 31/40
 - 0s - loss: 0.0361 - acc: 0.9790
Epoch 32/40
 - 0s - loss: 0.0361 - acc: 0.9791
Epoch 33/40
 - 0s - loss: 0.0361 - acc: 0.9796
Epoch 34/40
 - 0s - loss: 0.0360 - acc: 0.9789
Epoch 35/40
 - 0s - loss: 0.0356 - acc: 0.9780
Epoch 36/40
 - 0s - loss: 0.0371 - acc: 0.9778
Epoch 37/40
 - 0s - loss: 0.0356 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0349 - acc: 0.9784
Epoch 39/40
 - 0s - loss: 0.0360 - acc: 0.9789
Epoch 40/40
 - 0s - loss: 0.0364 - acc: 0.9779
# Training time = 0:03:21.581085
# F-Score(Ordinary) = 0.203, Recall: 0.197, Precision: 0.21
# F-Score(lvc) = 0.293, Recall: 0.92, Precision: 0.174
# F-Score(ireflv) = 0.172, Recall: 0.288, Precision: 0.123
# F-Score(id) = 0.121, Recall: 0.09, Precision: 0.187
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_45 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 4, 48)        705264      input_45[0][0]                   
__________________________________________________________________________________________________
embedding_46 (Embedding)        (None, 4, 24)        5640        input_46[0][0]                   
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 192)          0           embedding_45[0][0]               
__________________________________________________________________________________________________
flatten_46 (Flatten)            (None, 96)           0           embedding_46[0][0]               
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 288)          0           flatten_45[0][0]                 
                                                                 flatten_46[0][0]                 
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 24)           6936        concatenate_23[0][0]             
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 24)           0           dense_45[0][0]                   
__________________________________________________________________________________________________
dense_46 (Dense)                (None, 8)            200         dropout_23[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1088 - acc: 0.9590 - val_loss: 0.0790 - val_acc: 0.9670
Epoch 2/40
 - 4s - loss: 0.0726 - acc: 0.9676 - val_loss: 0.0732 - val_acc: 0.9680
Epoch 3/40
 - 4s - loss: 0.0643 - acc: 0.9695 - val_loss: 0.0737 - val_acc: 0.9664
Epoch 4/40
 - 4s - loss: 0.0595 - acc: 0.9709 - val_loss: 0.0795 - val_acc: 0.9673
Epoch 5/40
 - 4s - loss: 0.0547 - acc: 0.9721 - val_loss: 0.0751 - val_acc: 0.9682
Epoch 6/40
 - 4s - loss: 0.0527 - acc: 0.9724 - val_loss: 0.0800 - val_acc: 0.9680
Epoch 7/40
 - 4s - loss: 0.0506 - acc: 0.9736 - val_loss: 0.0776 - val_acc: 0.9692
Epoch 8/40
 - 4s - loss: 0.0489 - acc: 0.9741 - val_loss: 0.0942 - val_acc: 0.9682
Epoch 9/40
 - 4s - loss: 0.0474 - acc: 0.9741 - val_loss: 0.0816 - val_acc: 0.9684
Epoch 10/40
 - 4s - loss: 0.0463 - acc: 0.9746 - val_loss: 0.0801 - val_acc: 0.9694
Epoch 11/40
 - 4s - loss: 0.0452 - acc: 0.9752 - val_loss: 0.0854 - val_acc: 0.9691
Epoch 12/40
 - 4s - loss: 0.0445 - acc: 0.9748 - val_loss: 0.0904 - val_acc: 0.9681
Epoch 13/40
 - 4s - loss: 0.0433 - acc: 0.9755 - val_loss: 0.0907 - val_acc: 0.9685
Epoch 14/40
 - 4s - loss: 0.0430 - acc: 0.9755 - val_loss: 0.0937 - val_acc: 0.9681
Epoch 15/40
 - 4s - loss: 0.0424 - acc: 0.9752 - val_loss: 0.0904 - val_acc: 0.9683
Epoch 16/40
 - 4s - loss: 0.0425 - acc: 0.9755 - val_loss: 0.0945 - val_acc: 0.9689
Epoch 17/40
 - 4s - loss: 0.0417 - acc: 0.9756 - val_loss: 0.0936 - val_acc: 0.9686
Epoch 18/40
 - 4s - loss: 0.0417 - acc: 0.9757 - val_loss: 0.0916 - val_acc: 0.9694
Epoch 19/40
 - 4s - loss: 0.0412 - acc: 0.9757 - val_loss: 0.0967 - val_acc: 0.9688
Epoch 20/40
 - 4s - loss: 0.0406 - acc: 0.9757 - val_loss: 0.0994 - val_acc: 0.9690
Epoch 21/40
 - 4s - loss: 0.0400 - acc: 0.9763 - val_loss: 0.1052 - val_acc: 0.9693
Epoch 22/40
 - 4s - loss: 0.0401 - acc: 0.9762 - val_loss: 0.1216 - val_acc: 0.9695
Epoch 23/40
 - 4s - loss: 0.0396 - acc: 0.9769 - val_loss: 0.0984 - val_acc: 0.9680
Epoch 24/40
 - 4s - loss: 0.0395 - acc: 0.9767 - val_loss: 0.1004 - val_acc: 0.9684
Epoch 25/40
 - 4s - loss: 0.0400 - acc: 0.9764 - val_loss: 0.1131 - val_acc: 0.9679
Epoch 26/40
 - 4s - loss: 0.0395 - acc: 0.9762 - val_loss: 0.0997 - val_acc: 0.9683
Epoch 27/40
 - 4s - loss: 0.0386 - acc: 0.9770 - val_loss: 0.1028 - val_acc: 0.9682
Epoch 28/40
 - 4s - loss: 0.0390 - acc: 0.9769 - val_loss: 0.1058 - val_acc: 0.9680
Epoch 29/40
 - 4s - loss: 0.0383 - acc: 0.9771 - val_loss: 0.1001 - val_acc: 0.9692
Epoch 30/40
 - 4s - loss: 0.0377 - acc: 0.9770 - val_loss: 0.1066 - val_acc: 0.9687
Epoch 31/40
 - 4s - loss: 0.0383 - acc: 0.9765 - val_loss: 0.1020 - val_acc: 0.9684
Epoch 32/40
 - 4s - loss: 0.0384 - acc: 0.9773 - val_loss: 0.1011 - val_acc: 0.9696
Epoch 33/40
 - 4s - loss: 0.0386 - acc: 0.9771 - val_loss: 0.1055 - val_acc: 0.9691
Epoch 34/40
 - 4s - loss: 0.0381 - acc: 0.9769 - val_loss: 0.0951 - val_acc: 0.9684
Epoch 35/40
 - 4s - loss: 0.0377 - acc: 0.9770 - val_loss: 0.1027 - val_acc: 0.9694
Epoch 36/40
 - 4s - loss: 0.0375 - acc: 0.9774 - val_loss: 0.1069 - val_acc: 0.9688
Epoch 37/40
 - 4s - loss: 0.0380 - acc: 0.9772 - val_loss: 0.1073 - val_acc: 0.9686
Epoch 38/40
 - 4s - loss: 0.0375 - acc: 0.9769 - val_loss: 0.1134 - val_acc: 0.9687
Epoch 39/40
 - 4s - loss: 0.0378 - acc: 0.9772 - val_loss: 0.1081 - val_acc: 0.9675
Epoch 40/40
 - 4s - loss: 0.0383 - acc: 0.9766 - val_loss: 0.1036 - val_acc: 0.9695
Epoch 1/40
 - 0s - loss: 0.0880 - acc: 0.9659
Epoch 2/40
 - 0s - loss: 0.0675 - acc: 0.9680
Epoch 3/40
 - 0s - loss: 0.0582 - acc: 0.9726
Epoch 4/40
 - 0s - loss: 0.0558 - acc: 0.9718
Epoch 5/40
 - 0s - loss: 0.0489 - acc: 0.9739
Epoch 6/40
 - 0s - loss: 0.0472 - acc: 0.9749
Epoch 7/40
 - 0s - loss: 0.0466 - acc: 0.9757
Epoch 8/40
 - 0s - loss: 0.0433 - acc: 0.9760
Epoch 9/40
 - 0s - loss: 0.0444 - acc: 0.9754
Epoch 10/40
 - 0s - loss: 0.0423 - acc: 0.9770
Epoch 11/40
 - 0s - loss: 0.0412 - acc: 0.9756
Epoch 12/40
 - 0s - loss: 0.0409 - acc: 0.9760
Epoch 13/40
 - 0s - loss: 0.0396 - acc: 0.9772
Epoch 14/40
 - 0s - loss: 0.0388 - acc: 0.9775
Epoch 15/40
 - 0s - loss: 0.0399 - acc: 0.9778
Epoch 16/40
 - 0s - loss: 0.0397 - acc: 0.9770
Epoch 17/40
 - 0s - loss: 0.0389 - acc: 0.9781
Epoch 18/40
 - 0s - loss: 0.0372 - acc: 0.9786
Epoch 19/40
 - 0s - loss: 0.0378 - acc: 0.9785
Epoch 20/40
 - 0s - loss: 0.0380 - acc: 0.9768
Epoch 21/40
 - 0s - loss: 0.0381 - acc: 0.9772
Epoch 22/40
 - 0s - loss: 0.0369 - acc: 0.9795
Epoch 23/40
 - 0s - loss: 0.0375 - acc: 0.9780
Epoch 24/40
 - 0s - loss: 0.0368 - acc: 0.9773
Epoch 25/40
 - 0s - loss: 0.0372 - acc: 0.9775
Epoch 26/40
 - 0s - loss: 0.0369 - acc: 0.9790
Epoch 27/40
 - 0s - loss: 0.0370 - acc: 0.9779
Epoch 28/40
 - 0s - loss: 0.0371 - acc: 0.9779
Epoch 29/40
 - 0s - loss: 0.0370 - acc: 0.9774
Epoch 30/40
 - 0s - loss: 0.0367 - acc: 0.9793
Epoch 31/40
 - 0s - loss: 0.0375 - acc: 0.9777
Epoch 32/40
 - 0s - loss: 0.0366 - acc: 0.9774
Epoch 33/40
 - 0s - loss: 0.0356 - acc: 0.9793
Epoch 34/40
 - 0s - loss: 0.0360 - acc: 0.9791
Epoch 35/40
 - 0s - loss: 0.0355 - acc: 0.9788
Epoch 36/40
 - 0s - loss: 0.0352 - acc: 0.9787
Epoch 37/40
 - 0s - loss: 0.0351 - acc: 0.9795
Epoch 38/40
 - 0s - loss: 0.0360 - acc: 0.9793
Epoch 39/40
 - 0s - loss: 0.0374 - acc: 0.9788
Epoch 40/40
 - 0s - loss: 0.0369 - acc: 0.9771
# Training time = 0:03:23.077037
# F-Score(Ordinary) = 0.349, Recall: 0.372, Precision: 0.329
# F-Score(lvc) = 0.124, Recall: 0.094, Precision: 0.182
# F-Score(ireflv) = 0.117, Recall: 0.281, Precision: 0.074
# F-Score(id) = 0.678, Recall: 0.944, Precision: 0.528
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_47 (Embedding)        (None, 4, 48)        705264      input_47[0][0]                   
__________________________________________________________________________________________________
embedding_48 (Embedding)        (None, 4, 24)        5640        input_48[0][0]                   
__________________________________________________________________________________________________
flatten_47 (Flatten)            (None, 192)          0           embedding_47[0][0]               
__________________________________________________________________________________________________
flatten_48 (Flatten)            (None, 96)           0           embedding_48[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 288)          0           flatten_47[0][0]                 
                                                                 flatten_48[0][0]                 
__________________________________________________________________________________________________
dense_47 (Dense)                (None, 24)           6936        concatenate_24[0][0]             
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 24)           0           dense_47[0][0]                   
__________________________________________________________________________________________________
dense_48 (Dense)                (None, 8)            200         dropout_24[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1145 - acc: 0.9587 - val_loss: 0.0844 - val_acc: 0.9619
Epoch 2/40
 - 4s - loss: 0.0761 - acc: 0.9670 - val_loss: 0.0872 - val_acc: 0.9609
Epoch 3/40
 - 4s - loss: 0.0683 - acc: 0.9692 - val_loss: 0.0751 - val_acc: 0.9683
Epoch 4/40
 - 4s - loss: 0.0631 - acc: 0.9705 - val_loss: 0.0776 - val_acc: 0.9685
Epoch 5/40
 - 4s - loss: 0.0582 - acc: 0.9723 - val_loss: 0.0849 - val_acc: 0.9656
Epoch 6/40
 - 4s - loss: 0.0555 - acc: 0.9727 - val_loss: 0.0790 - val_acc: 0.9685
Epoch 7/40
 - 4s - loss: 0.0526 - acc: 0.9738 - val_loss: 0.0827 - val_acc: 0.9687
Epoch 8/40
 - 4s - loss: 0.0503 - acc: 0.9739 - val_loss: 0.0820 - val_acc: 0.9684
Epoch 9/40
 - 4s - loss: 0.0483 - acc: 0.9744 - val_loss: 0.0834 - val_acc: 0.9683
Epoch 10/40
 - 4s - loss: 0.0473 - acc: 0.9749 - val_loss: 0.0824 - val_acc: 0.9680
Epoch 11/40
 - 4s - loss: 0.0466 - acc: 0.9746 - val_loss: 0.0878 - val_acc: 0.9664
Epoch 12/40
 - 4s - loss: 0.0458 - acc: 0.9754 - val_loss: 0.0918 - val_acc: 0.9695
Epoch 13/40
 - 4s - loss: 0.0449 - acc: 0.9754 - val_loss: 0.0886 - val_acc: 0.9690
Epoch 14/40
 - 4s - loss: 0.0443 - acc: 0.9753 - val_loss: 0.0901 - val_acc: 0.9693
Epoch 15/40
 - 4s - loss: 0.0440 - acc: 0.9751 - val_loss: 0.0941 - val_acc: 0.9693
Epoch 16/40
 - 4s - loss: 0.0430 - acc: 0.9757 - val_loss: 0.0935 - val_acc: 0.9702
Epoch 17/40
 - 4s - loss: 0.0430 - acc: 0.9758 - val_loss: 0.0836 - val_acc: 0.9694
Epoch 18/40
 - 4s - loss: 0.0413 - acc: 0.9763 - val_loss: 0.0910 - val_acc: 0.9693
Epoch 19/40
 - 4s - loss: 0.0421 - acc: 0.9761 - val_loss: 0.1016 - val_acc: 0.9679
Epoch 20/40
 - 4s - loss: 0.0421 - acc: 0.9760 - val_loss: 0.0989 - val_acc: 0.9686
Epoch 21/40
 - 4s - loss: 0.0413 - acc: 0.9767 - val_loss: 0.1030 - val_acc: 0.9678
Epoch 22/40
 - 4s - loss: 0.0415 - acc: 0.9760 - val_loss: 0.1059 - val_acc: 0.9688
Epoch 23/40
 - 4s - loss: 0.0409 - acc: 0.9760 - val_loss: 0.0959 - val_acc: 0.9698
Epoch 24/40
 - 4s - loss: 0.0404 - acc: 0.9764 - val_loss: 0.1002 - val_acc: 0.9688
Epoch 25/40
 - 4s - loss: 0.0400 - acc: 0.9767 - val_loss: 0.1009 - val_acc: 0.9700
Epoch 26/40
 - 4s - loss: 0.0403 - acc: 0.9767 - val_loss: 0.1042 - val_acc: 0.9684
Epoch 27/40
 - 4s - loss: 0.0396 - acc: 0.9765 - val_loss: 0.1087 - val_acc: 0.9695
Epoch 28/40
 - 4s - loss: 0.0397 - acc: 0.9769 - val_loss: 0.1046 - val_acc: 0.9701
Epoch 29/40
 - 4s - loss: 0.0394 - acc: 0.9766 - val_loss: 0.1111 - val_acc: 0.9687
Epoch 30/40
 - 4s - loss: 0.0396 - acc: 0.9770 - val_loss: 0.1011 - val_acc: 0.9692
Epoch 31/40
 - 4s - loss: 0.0389 - acc: 0.9769 - val_loss: 0.1059 - val_acc: 0.9702
Epoch 32/40
 - 4s - loss: 0.0389 - acc: 0.9768 - val_loss: 0.1111 - val_acc: 0.9693
Epoch 33/40
 - 4s - loss: 0.0392 - acc: 0.9768 - val_loss: 0.1082 - val_acc: 0.9695
Epoch 34/40
 - 4s - loss: 0.0387 - acc: 0.9768 - val_loss: 0.1112 - val_acc: 0.9696
Epoch 35/40
 - 4s - loss: 0.0384 - acc: 0.9772 - val_loss: 0.1116 - val_acc: 0.9692
Epoch 36/40
 - 4s - loss: 0.0385 - acc: 0.9770 - val_loss: 0.1130 - val_acc: 0.9678
Epoch 37/40
 - 4s - loss: 0.0395 - acc: 0.9772 - val_loss: 0.1144 - val_acc: 0.9694
Epoch 38/40
 - 4s - loss: 0.0387 - acc: 0.9771 - val_loss: 0.1066 - val_acc: 0.9695
Epoch 39/40
 - 4s - loss: 0.0380 - acc: 0.9777 - val_loss: 0.1089 - val_acc: 0.9695
Epoch 40/40
 - 4s - loss: 0.0378 - acc: 0.9771 - val_loss: 0.1186 - val_acc: 0.9685
Epoch 1/40
 - 0s - loss: 0.0870 - acc: 0.9661
Epoch 2/40
 - 0s - loss: 0.0671 - acc: 0.9695
Epoch 3/40
 - 0s - loss: 0.0582 - acc: 0.9708
Epoch 4/40
 - 0s - loss: 0.0549 - acc: 0.9725
Epoch 5/40
 - 0s - loss: 0.0500 - acc: 0.9730
Epoch 6/40
 - 0s - loss: 0.0471 - acc: 0.9752
Epoch 7/40
 - 0s - loss: 0.0463 - acc: 0.9745
Epoch 8/40
 - 0s - loss: 0.0442 - acc: 0.9761
Epoch 9/40
 - 0s - loss: 0.0437 - acc: 0.9761
Epoch 10/40
 - 0s - loss: 0.0425 - acc: 0.9757
Epoch 11/40
 - 0s - loss: 0.0408 - acc: 0.9773
Epoch 12/40
 - 0s - loss: 0.0411 - acc: 0.9759
Epoch 13/40
 - 0s - loss: 0.0413 - acc: 0.9760
Epoch 14/40
 - 0s - loss: 0.0394 - acc: 0.9765
Epoch 15/40
 - 0s - loss: 0.0395 - acc: 0.9776
Epoch 16/40
 - 0s - loss: 0.0401 - acc: 0.9782
Epoch 17/40
 - 0s - loss: 0.0384 - acc: 0.9770
Epoch 18/40
 - 0s - loss: 0.0385 - acc: 0.9780
Epoch 19/40
 - 0s - loss: 0.0374 - acc: 0.9787
Epoch 20/40
 - 0s - loss: 0.0380 - acc: 0.9786
Epoch 21/40
 - 0s - loss: 0.0378 - acc: 0.9772
Epoch 22/40
 - 0s - loss: 0.0374 - acc: 0.9777
Epoch 23/40
 - 0s - loss: 0.0375 - acc: 0.9768
Epoch 24/40
 - 0s - loss: 0.0375 - acc: 0.9779
Epoch 25/40
 - 0s - loss: 0.0371 - acc: 0.9785
Epoch 26/40
 - 0s - loss: 0.0360 - acc: 0.9781
Epoch 27/40
 - 0s - loss: 0.0363 - acc: 0.9783
Epoch 28/40
 - 0s - loss: 0.0369 - acc: 0.9783
Epoch 29/40
 - 0s - loss: 0.0366 - acc: 0.9781
Epoch 30/40
 - 0s - loss: 0.0359 - acc: 0.9784
Epoch 31/40
 - 0s - loss: 0.0375 - acc: 0.9791
Epoch 32/40
 - 0s - loss: 0.0357 - acc: 0.9801
Epoch 33/40
 - 0s - loss: 0.0360 - acc: 0.9787
Epoch 34/40
 - 0s - loss: 0.0353 - acc: 0.9798
Epoch 35/40
 - 0s - loss: 0.0365 - acc: 0.9784
Epoch 36/40
 - 0s - loss: 0.0360 - acc: 0.9790
Epoch 37/40
 - 0s - loss: 0.0347 - acc: 0.9788
Epoch 38/40
 - 0s - loss: 0.0353 - acc: 0.9778
Epoch 39/40
 - 0s - loss: 0.0355 - acc: 0.9778
Epoch 40/40
 - 0s - loss: 0.0350 - acc: 0.9782
# Training time = 0:03:21.900094
# F-Score(Ordinary) = 0.139, Recall: 0.183, Precision: 0.112
# F-Score(lvc) = 0.215, Recall: 0.941, Precision: 0.121
# F-Score(id) = 0.062, Recall: 0.055, Precision: 0.073
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_49 (Embedding)        (None, 4, 48)        705264      input_49[0][0]                   
__________________________________________________________________________________________________
embedding_50 (Embedding)        (None, 4, 24)        5640        input_50[0][0]                   
__________________________________________________________________________________________________
flatten_49 (Flatten)            (None, 192)          0           embedding_49[0][0]               
__________________________________________________________________________________________________
flatten_50 (Flatten)            (None, 96)           0           embedding_50[0][0]               
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 288)          0           flatten_49[0][0]                 
                                                                 flatten_50[0][0]                 
__________________________________________________________________________________________________
dense_49 (Dense)                (None, 24)           6936        concatenate_25[0][0]             
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 24)           0           dense_49[0][0]                   
__________________________________________________________________________________________________
dense_50 (Dense)                (None, 8)            200         dropout_25[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = sgd, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1151 - acc: 0.9581 - val_loss: 0.0881 - val_acc: 0.9637
Epoch 2/40
 - 4s - loss: 0.0749 - acc: 0.9676 - val_loss: 0.0749 - val_acc: 0.9678
Epoch 3/40
 - 4s - loss: 0.0659 - acc: 0.9694 - val_loss: 0.0744 - val_acc: 0.9677
Epoch 4/40
 - 4s - loss: 0.0607 - acc: 0.9705 - val_loss: 0.0737 - val_acc: 0.9679
Epoch 5/40
 - 4s - loss: 0.0564 - acc: 0.9722 - val_loss: 0.0764 - val_acc: 0.9669
Epoch 6/40
 - 4s - loss: 0.0535 - acc: 0.9727 - val_loss: 0.0735 - val_acc: 0.9687
Epoch 7/40
 - 4s - loss: 0.0510 - acc: 0.9734 - val_loss: 0.0805 - val_acc: 0.9685
Epoch 8/40
 - 4s - loss: 0.0497 - acc: 0.9735 - val_loss: 0.0790 - val_acc: 0.9686
Epoch 9/40
 - 4s - loss: 0.0479 - acc: 0.9743 - val_loss: 0.0802 - val_acc: 0.9693
Epoch 10/40
 - 4s - loss: 0.0468 - acc: 0.9746 - val_loss: 0.0875 - val_acc: 0.9682
Epoch 11/40
 - 4s - loss: 0.0460 - acc: 0.9755 - val_loss: 0.0887 - val_acc: 0.9695
Epoch 12/40
 - 4s - loss: 0.0453 - acc: 0.9748 - val_loss: 0.0841 - val_acc: 0.9693
Epoch 13/40
 - 4s - loss: 0.0442 - acc: 0.9756 - val_loss: 0.0896 - val_acc: 0.9685
Epoch 14/40
 - 4s - loss: 0.0439 - acc: 0.9758 - val_loss: 0.0920 - val_acc: 0.9696
Epoch 15/40
 - 4s - loss: 0.0437 - acc: 0.9753 - val_loss: 0.0855 - val_acc: 0.9696
Epoch 16/40
 - 4s - loss: 0.0435 - acc: 0.9756 - val_loss: 0.0969 - val_acc: 0.9693
Epoch 17/40
 - 4s - loss: 0.0423 - acc: 0.9760 - val_loss: 0.0921 - val_acc: 0.9692
Epoch 18/40
 - 4s - loss: 0.0422 - acc: 0.9758 - val_loss: 0.0960 - val_acc: 0.9670
Epoch 19/40
 - 4s - loss: 0.0417 - acc: 0.9763 - val_loss: 0.0933 - val_acc: 0.9683
Epoch 20/40
 - 4s - loss: 0.0412 - acc: 0.9764 - val_loss: 0.0938 - val_acc: 0.9692
Epoch 21/40
 - 4s - loss: 0.0409 - acc: 0.9763 - val_loss: 0.1069 - val_acc: 0.9678
Epoch 22/40
 - 4s - loss: 0.0403 - acc: 0.9764 - val_loss: 0.1001 - val_acc: 0.9673
Epoch 23/40
 - 4s - loss: 0.0406 - acc: 0.9765 - val_loss: 0.0996 - val_acc: 0.9687
Epoch 24/40
 - 4s - loss: 0.0400 - acc: 0.9760 - val_loss: 0.0972 - val_acc: 0.9683
Epoch 25/40
 - 4s - loss: 0.0396 - acc: 0.9771 - val_loss: 0.1055 - val_acc: 0.9668
Epoch 26/40
 - 4s - loss: 0.0401 - acc: 0.9763 - val_loss: 0.1041 - val_acc: 0.9693
Epoch 27/40
 - 4s - loss: 0.0400 - acc: 0.9765 - val_loss: 0.1035 - val_acc: 0.9687
Epoch 28/40
 - 4s - loss: 0.0389 - acc: 0.9770 - val_loss: 0.1090 - val_acc: 0.9695
Epoch 29/40
 - 4s - loss: 0.0394 - acc: 0.9767 - val_loss: 0.1106 - val_acc: 0.9680
Epoch 30/40
 - 4s - loss: 0.0399 - acc: 0.9767 - val_loss: 0.1103 - val_acc: 0.9682
Epoch 31/40
 - 4s - loss: 0.0393 - acc: 0.9768 - val_loss: 0.1053 - val_acc: 0.9690
Epoch 32/40
 - 4s - loss: 0.0389 - acc: 0.9770 - val_loss: 0.1109 - val_acc: 0.9682
Epoch 33/40
 - 4s - loss: 0.0389 - acc: 0.9767 - val_loss: 0.1099 - val_acc: 0.9683
Epoch 34/40
 - 4s - loss: 0.0384 - acc: 0.9771 - val_loss: 0.1097 - val_acc: 0.9683
Epoch 35/40
 - 4s - loss: 0.0381 - acc: 0.9772 - val_loss: 0.1130 - val_acc: 0.9682
Epoch 36/40
 - 4s - loss: 0.0383 - acc: 0.9768 - val_loss: 0.1145 - val_acc: 0.9677
Epoch 37/40
 - 4s - loss: 0.0388 - acc: 0.9768 - val_loss: 0.1116 - val_acc: 0.9689
Epoch 38/40
 - 4s - loss: 0.0382 - acc: 0.9773 - val_loss: 0.1079 - val_acc: 0.9691
Epoch 39/40
 - 4s - loss: 0.0380 - acc: 0.9772 - val_loss: 0.1224 - val_acc: 0.9677
Epoch 40/40
 - 4s - loss: 0.0385 - acc: 0.9771 - val_loss: 0.1143 - val_acc: 0.9687
Epoch 1/40
 - 0s - loss: 0.0892 - acc: 0.9644
Epoch 2/40
 - 0s - loss: 0.0657 - acc: 0.9691
Epoch 3/40
 - 0s - loss: 0.0561 - acc: 0.9713
Epoch 4/40
 - 0s - loss: 0.0534 - acc: 0.9728
Epoch 5/40
 - 0s - loss: 0.0493 - acc: 0.9733
Epoch 6/40
 - 0s - loss: 0.0476 - acc: 0.9735
Epoch 7/40
 - 0s - loss: 0.0444 - acc: 0.9758
Epoch 8/40
 - 0s - loss: 0.0435 - acc: 0.9760
Epoch 9/40
 - 0s - loss: 0.0432 - acc: 0.9755
Epoch 10/40
 - 0s - loss: 0.0422 - acc: 0.9762
Epoch 11/40
 - 0s - loss: 0.0410 - acc: 0.9766
Epoch 12/40
 - 0s - loss: 0.0407 - acc: 0.9756
Epoch 13/40
 - 0s - loss: 0.0391 - acc: 0.9777
Epoch 14/40
 - 0s - loss: 0.0390 - acc: 0.9767
Epoch 15/40
 - 0s - loss: 0.0390 - acc: 0.9780
Epoch 16/40
 - 0s - loss: 0.0384 - acc: 0.9773
Epoch 17/40
 - 0s - loss: 0.0390 - acc: 0.9778
Epoch 18/40
 - 0s - loss: 0.0392 - acc: 0.9765
Epoch 19/40
 - 0s - loss: 0.0382 - acc: 0.9774
Epoch 20/40
 - 0s - loss: 0.0381 - acc: 0.9784
Epoch 21/40
 - 0s - loss: 0.0369 - acc: 0.9774
Epoch 22/40
 - 0s - loss: 0.0375 - acc: 0.9778
Epoch 23/40
 - 0s - loss: 0.0364 - acc: 0.9782
Epoch 24/40
 - 0s - loss: 0.0372 - acc: 0.9781
Epoch 25/40
 - 0s - loss: 0.0376 - acc: 0.9773
Epoch 26/40
 - 0s - loss: 0.0369 - acc: 0.9787
Epoch 27/40
 - 0s - loss: 0.0372 - acc: 0.9770
Epoch 28/40
 - 0s - loss: 0.0356 - acc: 0.9786
Epoch 29/40
 - 0s - loss: 0.0360 - acc: 0.9792
Epoch 30/40
 - 0s - loss: 0.0361 - acc: 0.9797
Epoch 31/40
 - 0s - loss: 0.0363 - acc: 0.9776
Epoch 32/40
 - 0s - loss: 0.0356 - acc: 0.9793
Epoch 33/40
 - 0s - loss: 0.0353 - acc: 0.9785
Epoch 34/40
 - 0s - loss: 0.0354 - acc: 0.9785
Epoch 35/40
 - 0s - loss: 0.0359 - acc: 0.9778
Epoch 36/40
 - 0s - loss: 0.0353 - acc: 0.9797
Epoch 37/40
 - 0s - loss: 0.0351 - acc: 0.9785
Epoch 38/40
 - 0s - loss: 0.0368 - acc: 0.9789
Epoch 39/40
 - 0s - loss: 0.0354 - acc: 0.9780
Epoch 40/40
 - 0s - loss: 0.0374 - acc: 0.9779
# Training time = 0:03:21.119789
# F-Score(Ordinary) = 0.535, Recall: 0.455, Precision: 0.651
# F-Score(lvc) = 0.304, Recall: 0.205, Precision: 0.583
# F-Score(ireflv) = 0.77, Recall: 0.901, Precision: 0.672
# F-Score(id) = 0.67, Recall: 0.707, Precision: 0.637
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_51 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_51 (Embedding)        (None, 4, 48)        705264      input_51[0][0]                   
__________________________________________________________________________________________________
embedding_52 (Embedding)        (None, 4, 24)        5640        input_52[0][0]                   
__________________________________________________________________________________________________
flatten_51 (Flatten)            (None, 192)          0           embedding_51[0][0]               
__________________________________________________________________________________________________
flatten_52 (Flatten)            (None, 96)           0           embedding_52[0][0]               
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 288)          0           flatten_51[0][0]                 
                                                                 flatten_52[0][0]                 
__________________________________________________________________________________________________
dense_51 (Dense)                (None, 24)           6936        concatenate_26[0][0]             
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 24)           0           dense_51[0][0]                   
__________________________________________________________________________________________________
dense_52 (Dense)                (None, 8)            200         dropout_26[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1723 - acc: 0.9501 - val_loss: 0.0883 - val_acc: 0.9643
Epoch 2/40
 - 4s - loss: 0.0809 - acc: 0.9662 - val_loss: 0.0795 - val_acc: 0.9651
Epoch 3/40
 - 4s - loss: 0.0723 - acc: 0.9680 - val_loss: 0.0783 - val_acc: 0.9663
Epoch 4/40
 - 4s - loss: 0.0684 - acc: 0.9686 - val_loss: 0.0751 - val_acc: 0.9663
Epoch 5/40
 - 4s - loss: 0.0659 - acc: 0.9693 - val_loss: 0.0746 - val_acc: 0.9665
Epoch 6/40
 - 4s - loss: 0.0642 - acc: 0.9698 - val_loss: 0.0732 - val_acc: 0.9675
Epoch 7/40
 - 4s - loss: 0.0627 - acc: 0.9704 - val_loss: 0.0736 - val_acc: 0.9677
Epoch 8/40
 - 4s - loss: 0.0620 - acc: 0.9705 - val_loss: 0.0744 - val_acc: 0.9678
Epoch 9/40
 - 4s - loss: 0.0609 - acc: 0.9712 - val_loss: 0.0755 - val_acc: 0.9680
Epoch 10/40
 - 4s - loss: 0.0609 - acc: 0.9716 - val_loss: 0.0751 - val_acc: 0.9677
Epoch 11/40
 - 4s - loss: 0.0593 - acc: 0.9718 - val_loss: 0.0736 - val_acc: 0.9687
Epoch 12/40
 - 4s - loss: 0.0593 - acc: 0.9726 - val_loss: 0.0761 - val_acc: 0.9683
Epoch 13/40
 - 4s - loss: 0.0595 - acc: 0.9722 - val_loss: 0.0757 - val_acc: 0.9685
Epoch 14/40
 - 4s - loss: 0.0591 - acc: 0.9724 - val_loss: 0.0797 - val_acc: 0.9684
Epoch 15/40
 - 4s - loss: 0.0589 - acc: 0.9729 - val_loss: 0.0812 - val_acc: 0.9679
Epoch 16/40
 - 4s - loss: 0.0590 - acc: 0.9728 - val_loss: 0.0787 - val_acc: 0.9686
Epoch 17/40
 - 4s - loss: 0.0590 - acc: 0.9728 - val_loss: 0.0850 - val_acc: 0.9681
Epoch 18/40
 - 4s - loss: 0.0586 - acc: 0.9733 - val_loss: 0.0815 - val_acc: 0.9680
Epoch 19/40
 - 4s - loss: 0.0580 - acc: 0.9731 - val_loss: 0.0825 - val_acc: 0.9688
Epoch 20/40
 - 4s - loss: 0.0581 - acc: 0.9730 - val_loss: 0.0855 - val_acc: 0.9678
Epoch 21/40
 - 4s - loss: 0.0583 - acc: 0.9733 - val_loss: 0.0873 - val_acc: 0.9682
Epoch 22/40
 - 4s - loss: 0.0587 - acc: 0.9736 - val_loss: 0.0837 - val_acc: 0.9684
Epoch 23/40
 - 4s - loss: 0.0589 - acc: 0.9734 - val_loss: 0.0895 - val_acc: 0.9680
Epoch 24/40
 - 4s - loss: 0.0585 - acc: 0.9735 - val_loss: 0.0893 - val_acc: 0.9686
Epoch 25/40
 - 4s - loss: 0.0574 - acc: 0.9739 - val_loss: 0.0878 - val_acc: 0.9683
Epoch 26/40
 - 4s - loss: 0.0579 - acc: 0.9739 - val_loss: 0.0910 - val_acc: 0.9675
Epoch 27/40
 - 4s - loss: 0.0575 - acc: 0.9740 - val_loss: 0.0913 - val_acc: 0.9676
Epoch 28/40
 - 4s - loss: 0.0576 - acc: 0.9739 - val_loss: 0.0947 - val_acc: 0.9680
Epoch 29/40
 - 4s - loss: 0.0581 - acc: 0.9740 - val_loss: 0.0952 - val_acc: 0.9670
Epoch 30/40
 - 4s - loss: 0.0579 - acc: 0.9741 - val_loss: 0.0946 - val_acc: 0.9675
Epoch 31/40
 - 4s - loss: 0.0582 - acc: 0.9744 - val_loss: 0.0939 - val_acc: 0.9675
Epoch 32/40
 - 4s - loss: 0.0576 - acc: 0.9743 - val_loss: 0.0899 - val_acc: 0.9675
Epoch 33/40
 - 4s - loss: 0.0575 - acc: 0.9745 - val_loss: 0.0896 - val_acc: 0.9673
Epoch 34/40
 - 4s - loss: 0.0574 - acc: 0.9742 - val_loss: 0.1051 - val_acc: 0.9679
Epoch 35/40
 - 4s - loss: 0.0589 - acc: 0.9744 - val_loss: 0.0959 - val_acc: 0.9676
Epoch 36/40
 - 4s - loss: 0.0582 - acc: 0.9747 - val_loss: 0.0979 - val_acc: 0.9675
Epoch 37/40
 - 4s - loss: 0.0584 - acc: 0.9744 - val_loss: 0.1044 - val_acc: 0.9686
Epoch 38/40
 - 4s - loss: 0.0582 - acc: 0.9745 - val_loss: 0.1026 - val_acc: 0.9682
Epoch 39/40
 - 4s - loss: 0.0579 - acc: 0.9747 - val_loss: 0.1011 - val_acc: 0.9683
Epoch 40/40
 - 4s - loss: 0.0574 - acc: 0.9747 - val_loss: 0.1102 - val_acc: 0.9686
Epoch 1/40
 - 0s - loss: 0.0997 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.0902 - acc: 0.9669
Epoch 3/40
 - 0s - loss: 0.0850 - acc: 0.9680
Epoch 4/40
 - 0s - loss: 0.0849 - acc: 0.9688
Epoch 5/40
 - 0s - loss: 0.0806 - acc: 0.9694
Epoch 6/40
 - 0s - loss: 0.0774 - acc: 0.9694
Epoch 7/40
 - 0s - loss: 0.0792 - acc: 0.9686
Epoch 8/40
 - 0s - loss: 0.0726 - acc: 0.9692
Epoch 9/40
 - 0s - loss: 0.0724 - acc: 0.9706
Epoch 10/40
 - 0s - loss: 0.0713 - acc: 0.9708
Epoch 11/40
 - 0s - loss: 0.0673 - acc: 0.9715
Epoch 12/40
 - 0s - loss: 0.0657 - acc: 0.9723
Epoch 13/40
 - 0s - loss: 0.0659 - acc: 0.9722
Epoch 14/40
 - 0s - loss: 0.0655 - acc: 0.9720
Epoch 15/40
 - 0s - loss: 0.0636 - acc: 0.9718
Epoch 16/40
 - 0s - loss: 0.0617 - acc: 0.9726
Epoch 17/40
 - 0s - loss: 0.0634 - acc: 0.9720
Epoch 18/40
 - 0s - loss: 0.0619 - acc: 0.9721
Epoch 19/40
 - 0s - loss: 0.0615 - acc: 0.9715
Epoch 20/40
 - 0s - loss: 0.0592 - acc: 0.9747
Epoch 21/40
 - 0s - loss: 0.0595 - acc: 0.9740
Epoch 22/40
 - 0s - loss: 0.0596 - acc: 0.9752
Epoch 23/40
 - 0s - loss: 0.0579 - acc: 0.9733
Epoch 24/40
 - 0s - loss: 0.0556 - acc: 0.9737
Epoch 25/40
 - 0s - loss: 0.0560 - acc: 0.9759
Epoch 26/40
 - 0s - loss: 0.0568 - acc: 0.9747
Epoch 27/40
 - 0s - loss: 0.0570 - acc: 0.9748
Epoch 28/40
 - 0s - loss: 0.0557 - acc: 0.9751
Epoch 29/40
 - 0s - loss: 0.0546 - acc: 0.9744
Epoch 30/40
 - 0s - loss: 0.0533 - acc: 0.9752
Epoch 31/40
 - 0s - loss: 0.0551 - acc: 0.9765
Epoch 32/40
 - 0s - loss: 0.0546 - acc: 0.9746
Epoch 33/40
 - 0s - loss: 0.0539 - acc: 0.9758
Epoch 34/40
 - 0s - loss: 0.0539 - acc: 0.9761
Epoch 35/40
 - 0s - loss: 0.0527 - acc: 0.9749
Epoch 36/40
 - 0s - loss: 0.0536 - acc: 0.9752
Epoch 37/40
 - 0s - loss: 0.0547 - acc: 0.9770
Epoch 38/40
 - 0s - loss: 0.0497 - acc: 0.9769
Epoch 39/40
 - 0s - loss: 0.0518 - acc: 0.9758
Epoch 40/40
 - 0s - loss: 0.0501 - acc: 0.9752
# Training time = 0:03:36.212400
# F-Score(Ordinary) = 0.136, Recall: 0.846, Precision: 0.074
# F-Score(lvc) = 0.113, Recall: 0.889, Precision: 0.061
# F-Score(ireflv) = 0.26, Recall: 0.792, Precision: 0.156
# F-Score(id) = 0.06, Recall: 1.0, Precision: 0.031
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_53 (Embedding)        (None, 4, 48)        705264      input_53[0][0]                   
__________________________________________________________________________________________________
embedding_54 (Embedding)        (None, 4, 24)        5640        input_54[0][0]                   
__________________________________________________________________________________________________
flatten_53 (Flatten)            (None, 192)          0           embedding_53[0][0]               
__________________________________________________________________________________________________
flatten_54 (Flatten)            (None, 96)           0           embedding_54[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 288)          0           flatten_53[0][0]                 
                                                                 flatten_54[0][0]                 
__________________________________________________________________________________________________
dense_53 (Dense)                (None, 24)           6936        concatenate_27[0][0]             
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 24)           0           dense_53[0][0]                   
__________________________________________________________________________________________________
dense_54 (Dense)                (None, 8)            200         dropout_27[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1778 - acc: 0.9511 - val_loss: 0.0883 - val_acc: 0.9654
Epoch 2/40
 - 4s - loss: 0.0810 - acc: 0.9662 - val_loss: 0.0782 - val_acc: 0.9663
Epoch 3/40
 - 4s - loss: 0.0718 - acc: 0.9681 - val_loss: 0.0745 - val_acc: 0.9664
Epoch 4/40
 - 4s - loss: 0.0676 - acc: 0.9690 - val_loss: 0.0734 - val_acc: 0.9670
Epoch 5/40
 - 4s - loss: 0.0655 - acc: 0.9699 - val_loss: 0.0732 - val_acc: 0.9668
Epoch 6/40
 - 4s - loss: 0.0641 - acc: 0.9702 - val_loss: 0.0721 - val_acc: 0.9671
Epoch 7/40
 - 4s - loss: 0.0628 - acc: 0.9709 - val_loss: 0.0714 - val_acc: 0.9668
Epoch 8/40
 - 4s - loss: 0.0614 - acc: 0.9711 - val_loss: 0.0736 - val_acc: 0.9683
Epoch 9/40
 - 4s - loss: 0.0610 - acc: 0.9716 - val_loss: 0.0745 - val_acc: 0.9682
Epoch 10/40
 - 4s - loss: 0.0609 - acc: 0.9716 - val_loss: 0.0761 - val_acc: 0.9681
Epoch 11/40
 - 4s - loss: 0.0602 - acc: 0.9717 - val_loss: 0.0787 - val_acc: 0.9674
Epoch 12/40
 - 4s - loss: 0.0601 - acc: 0.9722 - val_loss: 0.0761 - val_acc: 0.9676
Epoch 13/40
 - 4s - loss: 0.0600 - acc: 0.9726 - val_loss: 0.0776 - val_acc: 0.9681
Epoch 14/40
 - 4s - loss: 0.0597 - acc: 0.9727 - val_loss: 0.0872 - val_acc: 0.9685
Epoch 15/40
 - 4s - loss: 0.0599 - acc: 0.9727 - val_loss: 0.0779 - val_acc: 0.9681
Epoch 16/40
 - 4s - loss: 0.0591 - acc: 0.9730 - val_loss: 0.0780 - val_acc: 0.9676
Epoch 17/40
 - 4s - loss: 0.0589 - acc: 0.9726 - val_loss: 0.0801 - val_acc: 0.9679
Epoch 18/40
 - 4s - loss: 0.0591 - acc: 0.9733 - val_loss: 0.0819 - val_acc: 0.9683
Epoch 19/40
 - 4s - loss: 0.0591 - acc: 0.9735 - val_loss: 0.0837 - val_acc: 0.9677
Epoch 20/40
 - 4s - loss: 0.0582 - acc: 0.9733 - val_loss: 0.0849 - val_acc: 0.9681
Epoch 21/40
 - 4s - loss: 0.0588 - acc: 0.9737 - val_loss: 0.0875 - val_acc: 0.9687
Epoch 22/40
 - 4s - loss: 0.0587 - acc: 0.9736 - val_loss: 0.0851 - val_acc: 0.9666
Epoch 23/40
 - 4s - loss: 0.0585 - acc: 0.9735 - val_loss: 0.0895 - val_acc: 0.9679
Epoch 24/40
 - 4s - loss: 0.0586 - acc: 0.9735 - val_loss: 0.0960 - val_acc: 0.9668
Epoch 25/40
 - 4s - loss: 0.0585 - acc: 0.9733 - val_loss: 0.0942 - val_acc: 0.9673
Epoch 26/40
 - 4s - loss: 0.0584 - acc: 0.9737 - val_loss: 0.0952 - val_acc: 0.9679
Epoch 27/40
 - 4s - loss: 0.0589 - acc: 0.9736 - val_loss: 0.0980 - val_acc: 0.9680
Epoch 28/40
 - 4s - loss: 0.0592 - acc: 0.9737 - val_loss: 0.0892 - val_acc: 0.9683
Epoch 29/40
 - 4s - loss: 0.0595 - acc: 0.9739 - val_loss: 0.1093 - val_acc: 0.9671
Epoch 30/40
 - 4s - loss: 0.0588 - acc: 0.9742 - val_loss: 0.0912 - val_acc: 0.9674
Epoch 31/40
 - 4s - loss: 0.0584 - acc: 0.9742 - val_loss: 0.0978 - val_acc: 0.9671
Epoch 32/40
 - 4s - loss: 0.0585 - acc: 0.9745 - val_loss: 0.0941 - val_acc: 0.9685
Epoch 33/40
 - 4s - loss: 0.0587 - acc: 0.9739 - val_loss: 0.1051 - val_acc: 0.9680
Epoch 34/40
 - 4s - loss: 0.0585 - acc: 0.9744 - val_loss: 0.0955 - val_acc: 0.9670
Epoch 35/40
 - 4s - loss: 0.0589 - acc: 0.9740 - val_loss: 0.0930 - val_acc: 0.9662
Epoch 36/40
 - 4s - loss: 0.0581 - acc: 0.9743 - val_loss: 0.1015 - val_acc: 0.9654
Epoch 37/40
 - 4s - loss: 0.0579 - acc: 0.9747 - val_loss: 0.0938 - val_acc: 0.9685
Epoch 38/40
 - 4s - loss: 0.0594 - acc: 0.9746 - val_loss: 0.1067 - val_acc: 0.9679
Epoch 39/40
 - 4s - loss: 0.0583 - acc: 0.9747 - val_loss: 0.1107 - val_acc: 0.9681
Epoch 40/40
 - 4s - loss: 0.0587 - acc: 0.9748 - val_loss: 0.1109 - val_acc: 0.9680
Epoch 1/40
 - 0s - loss: 0.0982 - acc: 0.9655
Epoch 2/40
 - 0s - loss: 0.0893 - acc: 0.9648
Epoch 3/40
 - 0s - loss: 0.0860 - acc: 0.9696
Epoch 4/40
 - 0s - loss: 0.0824 - acc: 0.9678
Epoch 5/40
 - 0s - loss: 0.0793 - acc: 0.9691
Epoch 6/40
 - 0s - loss: 0.0749 - acc: 0.9697
Epoch 7/40
 - 0s - loss: 0.0746 - acc: 0.9696
Epoch 8/40
 - 0s - loss: 0.0701 - acc: 0.9710
Epoch 9/40
 - 0s - loss: 0.0701 - acc: 0.9706
Epoch 10/40
 - 0s - loss: 0.0672 - acc: 0.9697
Epoch 11/40
 - 0s - loss: 0.0674 - acc: 0.9717
Epoch 12/40
 - 0s - loss: 0.0662 - acc: 0.9723
Epoch 13/40
 - 0s - loss: 0.0663 - acc: 0.9728
Epoch 14/40
 - 0s - loss: 0.0634 - acc: 0.9713
Epoch 15/40
 - 0s - loss: 0.0626 - acc: 0.9728
Epoch 16/40
 - 0s - loss: 0.0631 - acc: 0.9738
Epoch 17/40
 - 0s - loss: 0.0608 - acc: 0.9732
Epoch 18/40
 - 0s - loss: 0.0590 - acc: 0.9740
Epoch 19/40
 - 0s - loss: 0.0605 - acc: 0.9733
Epoch 20/40
 - 0s - loss: 0.0577 - acc: 0.9737
Epoch 21/40
 - 0s - loss: 0.0587 - acc: 0.9747
Epoch 22/40
 - 0s - loss: 0.0577 - acc: 0.9739
Epoch 23/40
 - 0s - loss: 0.0580 - acc: 0.9736
Epoch 24/40
 - 0s - loss: 0.0563 - acc: 0.9741
Epoch 25/40
 - 0s - loss: 0.0576 - acc: 0.9748
Epoch 26/40
 - 0s - loss: 0.0553 - acc: 0.9752
Epoch 27/40
 - 0s - loss: 0.0561 - acc: 0.9765
Epoch 28/40
 - 0s - loss: 0.0544 - acc: 0.9764
Epoch 29/40
 - 0s - loss: 0.0543 - acc: 0.9752
Epoch 30/40
 - 0s - loss: 0.0514 - acc: 0.9754
Epoch 31/40
 - 0s - loss: 0.0507 - acc: 0.9756
Epoch 32/40
 - 0s - loss: 0.0543 - acc: 0.9768
Epoch 33/40
 - 0s - loss: 0.0577 - acc: 0.9760
Epoch 34/40
 - 0s - loss: 0.0516 - acc: 0.9758
Epoch 35/40
 - 0s - loss: 0.0537 - acc: 0.9760
Epoch 36/40
 - 0s - loss: 0.0528 - acc: 0.9759
Epoch 37/40
 - 0s - loss: 0.0519 - acc: 0.9755
Epoch 38/40
 - 0s - loss: 0.0497 - acc: 0.9774
Epoch 39/40
 - 0s - loss: 0.0526 - acc: 0.9759
Epoch 40/40
 - 0s - loss: 0.0515 - acc: 0.9761
# Training time = 0:03:33.946104
# F-Score(Ordinary) = 0.186, Recall: 0.81, Precision: 0.105
# F-Score(ireflv) = 0.252, Recall: 0.655, Precision: 0.156
# F-Score(id) = 0.252, Recall: 0.966, Precision: 0.145
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_55 (Embedding)        (None, 4, 48)        705264      input_55[0][0]                   
__________________________________________________________________________________________________
embedding_56 (Embedding)        (None, 4, 24)        5640        input_56[0][0]                   
__________________________________________________________________________________________________
flatten_55 (Flatten)            (None, 192)          0           embedding_55[0][0]               
__________________________________________________________________________________________________
flatten_56 (Flatten)            (None, 96)           0           embedding_56[0][0]               
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 288)          0           flatten_55[0][0]                 
                                                                 flatten_56[0][0]                 
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 24)           6936        concatenate_28[0][0]             
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 24)           0           dense_55[0][0]                   
__________________________________________________________________________________________________
dense_56 (Dense)                (None, 8)            200         dropout_28[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1725 - acc: 0.9525 - val_loss: 0.0884 - val_acc: 0.9649
Epoch 2/40
 - 4s - loss: 0.0803 - acc: 0.9668 - val_loss: 0.0769 - val_acc: 0.9670
Epoch 3/40
 - 4s - loss: 0.0725 - acc: 0.9682 - val_loss: 0.0748 - val_acc: 0.9671
Epoch 4/40
 - 4s - loss: 0.0687 - acc: 0.9686 - val_loss: 0.0748 - val_acc: 0.9669
Epoch 5/40
 - 4s - loss: 0.0665 - acc: 0.9695 - val_loss: 0.0752 - val_acc: 0.9665
Epoch 6/40
 - 4s - loss: 0.0646 - acc: 0.9702 - val_loss: 0.0725 - val_acc: 0.9679
Epoch 7/40
 - 4s - loss: 0.0628 - acc: 0.9707 - val_loss: 0.0733 - val_acc: 0.9677
Epoch 8/40
 - 4s - loss: 0.0620 - acc: 0.9711 - val_loss: 0.0741 - val_acc: 0.9688
Epoch 9/40
 - 4s - loss: 0.0604 - acc: 0.9716 - val_loss: 0.0740 - val_acc: 0.9672
Epoch 10/40
 - 4s - loss: 0.0603 - acc: 0.9720 - val_loss: 0.0718 - val_acc: 0.9682
Epoch 11/40
 - 4s - loss: 0.0598 - acc: 0.9719 - val_loss: 0.0727 - val_acc: 0.9683
Epoch 12/40
 - 4s - loss: 0.0592 - acc: 0.9725 - val_loss: 0.0780 - val_acc: 0.9688
Epoch 13/40
 - 4s - loss: 0.0595 - acc: 0.9723 - val_loss: 0.0769 - val_acc: 0.9689
Epoch 14/40
 - 4s - loss: 0.0590 - acc: 0.9725 - val_loss: 0.0774 - val_acc: 0.9689
Epoch 15/40
 - 4s - loss: 0.0593 - acc: 0.9726 - val_loss: 0.0759 - val_acc: 0.9694
Epoch 16/40
 - 4s - loss: 0.0588 - acc: 0.9726 - val_loss: 0.0749 - val_acc: 0.9687
Epoch 17/40
 - 4s - loss: 0.0586 - acc: 0.9727 - val_loss: 0.0806 - val_acc: 0.9689
Epoch 18/40
 - 4s - loss: 0.0586 - acc: 0.9732 - val_loss: 0.0793 - val_acc: 0.9689
Epoch 19/40
 - 4s - loss: 0.0590 - acc: 0.9733 - val_loss: 0.0801 - val_acc: 0.9689
Epoch 20/40
 - 4s - loss: 0.0577 - acc: 0.9732 - val_loss: 0.0821 - val_acc: 0.9692
Epoch 21/40
 - 4s - loss: 0.0588 - acc: 0.9736 - val_loss: 0.0832 - val_acc: 0.9680
Epoch 22/40
 - 4s - loss: 0.0588 - acc: 0.9736 - val_loss: 0.0856 - val_acc: 0.9691
Epoch 23/40
 - 4s - loss: 0.0590 - acc: 0.9737 - val_loss: 0.0833 - val_acc: 0.9692
Epoch 24/40
 - 4s - loss: 0.0590 - acc: 0.9737 - val_loss: 0.0818 - val_acc: 0.9693
Epoch 25/40
 - 4s - loss: 0.0590 - acc: 0.9738 - val_loss: 0.0942 - val_acc: 0.9692
Epoch 26/40
 - 4s - loss: 0.0583 - acc: 0.9735 - val_loss: 0.0858 - val_acc: 0.9680
Epoch 27/40
 - 4s - loss: 0.0592 - acc: 0.9741 - val_loss: 0.0921 - val_acc: 0.9692
Epoch 28/40
 - 4s - loss: 0.0591 - acc: 0.9736 - val_loss: 0.0847 - val_acc: 0.9683
Epoch 29/40
 - 4s - loss: 0.0590 - acc: 0.9740 - val_loss: 0.0918 - val_acc: 0.9689
Epoch 30/40
 - 4s - loss: 0.0596 - acc: 0.9738 - val_loss: 0.0921 - val_acc: 0.9686
Epoch 31/40
 - 4s - loss: 0.0594 - acc: 0.9737 - val_loss: 0.0890 - val_acc: 0.9681
Epoch 32/40
 - 4s - loss: 0.0590 - acc: 0.9743 - val_loss: 0.0925 - val_acc: 0.9698
Epoch 33/40
 - 4s - loss: 0.0592 - acc: 0.9742 - val_loss: 0.0916 - val_acc: 0.9691
Epoch 34/40
 - 4s - loss: 0.0595 - acc: 0.9745 - val_loss: 0.0941 - val_acc: 0.9676
Epoch 35/40
 - 4s - loss: 0.0587 - acc: 0.9743 - val_loss: 0.0980 - val_acc: 0.9678
Epoch 36/40
 - 4s - loss: 0.0594 - acc: 0.9743 - val_loss: 0.0996 - val_acc: 0.9689
Epoch 37/40
 - 4s - loss: 0.0591 - acc: 0.9743 - val_loss: 0.0931 - val_acc: 0.9681
Epoch 38/40
 - 4s - loss: 0.0589 - acc: 0.9746 - val_loss: 0.1006 - val_acc: 0.9687
Epoch 39/40
 - 4s - loss: 0.0589 - acc: 0.9746 - val_loss: 0.1020 - val_acc: 0.9675
Epoch 40/40
 - 4s - loss: 0.0593 - acc: 0.9747 - val_loss: 0.1002 - val_acc: 0.9683
Epoch 1/40
 - 0s - loss: 0.0997 - acc: 0.9667
Epoch 2/40
 - 0s - loss: 0.0911 - acc: 0.9675
Epoch 3/40
 - 0s - loss: 0.0853 - acc: 0.9687
Epoch 4/40
 - 0s - loss: 0.0819 - acc: 0.9682
Epoch 5/40
 - 0s - loss: 0.0786 - acc: 0.9692
Epoch 6/40
 - 0s - loss: 0.0746 - acc: 0.9693
Epoch 7/40
 - 0s - loss: 0.0731 - acc: 0.9696
Epoch 8/40
 - 0s - loss: 0.0714 - acc: 0.9703
Epoch 9/40
 - 0s - loss: 0.0698 - acc: 0.9712
Epoch 10/40
 - 0s - loss: 0.0676 - acc: 0.9727
Epoch 11/40
 - 0s - loss: 0.0666 - acc: 0.9720
Epoch 12/40
 - 0s - loss: 0.0647 - acc: 0.9716
Epoch 13/40
 - 0s - loss: 0.0632 - acc: 0.9723
Epoch 14/40
 - 0s - loss: 0.0613 - acc: 0.9742
Epoch 15/40
 - 0s - loss: 0.0592 - acc: 0.9732
Epoch 16/40
 - 0s - loss: 0.0604 - acc: 0.9729
Epoch 17/40
 - 0s - loss: 0.0593 - acc: 0.9736
Epoch 18/40
 - 0s - loss: 0.0571 - acc: 0.9741
Epoch 19/40
 - 0s - loss: 0.0582 - acc: 0.9732
Epoch 20/40
 - 0s - loss: 0.0563 - acc: 0.9737
Epoch 21/40
 - 0s - loss: 0.0571 - acc: 0.9753
Epoch 22/40
 - 0s - loss: 0.0561 - acc: 0.9751
Epoch 23/40
 - 0s - loss: 0.0550 - acc: 0.9751
Epoch 24/40
 - 0s - loss: 0.0544 - acc: 0.9751
Epoch 25/40
 - 0s - loss: 0.0528 - acc: 0.9754
Epoch 26/40
 - 0s - loss: 0.0536 - acc: 0.9756
Epoch 27/40
 - 0s - loss: 0.0522 - acc: 0.9753
Epoch 28/40
 - 0s - loss: 0.0552 - acc: 0.9742
Epoch 29/40
 - 0s - loss: 0.0532 - acc: 0.9756
Epoch 30/40
 - 0s - loss: 0.0508 - acc: 0.9762
Epoch 31/40
 - 0s - loss: 0.0532 - acc: 0.9754
Epoch 32/40
 - 0s - loss: 0.0544 - acc: 0.9768
Epoch 33/40
 - 0s - loss: 0.0529 - acc: 0.9756
Epoch 34/40
 - 0s - loss: 0.0502 - acc: 0.9758
Epoch 35/40
 - 0s - loss: 0.0524 - acc: 0.9770
Epoch 36/40
 - 0s - loss: 0.0536 - acc: 0.9754
Epoch 37/40
 - 0s - loss: 0.0527 - acc: 0.9761
Epoch 38/40
 - 0s - loss: 0.0522 - acc: 0.9765
Epoch 39/40
 - 0s - loss: 0.0523 - acc: 0.9753
Epoch 40/40
 - 0s - loss: 0.0488 - acc: 0.9761
# Training time = 0:03:34.854932
# F-Score(Ordinary) = 0.202, Recall: 0.895, Precision: 0.114
# F-Score(lvc) = 0.044, Recall: 1.0, Precision: 0.023
# F-Score(ireflv) = 0.176, Recall: 0.857, Precision: 0.098
# F-Score(id) = 0.3, Recall: 0.875, Precision: 0.181
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_57 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_58 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_57 (Embedding)        (None, 4, 48)        705264      input_57[0][0]                   
__________________________________________________________________________________________________
embedding_58 (Embedding)        (None, 4, 24)        5640        input_58[0][0]                   
__________________________________________________________________________________________________
flatten_57 (Flatten)            (None, 192)          0           embedding_57[0][0]               
__________________________________________________________________________________________________
flatten_58 (Flatten)            (None, 96)           0           embedding_58[0][0]               
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 288)          0           flatten_57[0][0]                 
                                                                 flatten_58[0][0]                 
__________________________________________________________________________________________________
dense_57 (Dense)                (None, 24)           6936        concatenate_29[0][0]             
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 24)           0           dense_57[0][0]                   
__________________________________________________________________________________________________
dense_58 (Dense)                (None, 8)            200         dropout_29[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1868 - acc: 0.9457 - val_loss: 0.0852 - val_acc: 0.9654
Epoch 2/40
 - 4s - loss: 0.0802 - acc: 0.9662 - val_loss: 0.0779 - val_acc: 0.9663
Epoch 3/40
 - 4s - loss: 0.0725 - acc: 0.9680 - val_loss: 0.0754 - val_acc: 0.9666
Epoch 4/40
 - 4s - loss: 0.0688 - acc: 0.9689 - val_loss: 0.0757 - val_acc: 0.9673
Epoch 5/40
 - 4s - loss: 0.0659 - acc: 0.9698 - val_loss: 0.0751 - val_acc: 0.9671
Epoch 6/40
 - 4s - loss: 0.0644 - acc: 0.9703 - val_loss: 0.0753 - val_acc: 0.9676
Epoch 7/40
 - 4s - loss: 0.0633 - acc: 0.9706 - val_loss: 0.0734 - val_acc: 0.9682
Epoch 8/40
 - 4s - loss: 0.0614 - acc: 0.9708 - val_loss: 0.0763 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0609 - acc: 0.9714 - val_loss: 0.0776 - val_acc: 0.9680
Epoch 10/40
 - 4s - loss: 0.0609 - acc: 0.9717 - val_loss: 0.0759 - val_acc: 0.9678
Epoch 11/40
 - 4s - loss: 0.0602 - acc: 0.9720 - val_loss: 0.0812 - val_acc: 0.9682
Epoch 12/40
 - 4s - loss: 0.0606 - acc: 0.9717 - val_loss: 0.0763 - val_acc: 0.9679
Epoch 13/40
 - 4s - loss: 0.0600 - acc: 0.9720 - val_loss: 0.0791 - val_acc: 0.9671
Epoch 14/40
 - 4s - loss: 0.0601 - acc: 0.9726 - val_loss: 0.0816 - val_acc: 0.9674
Epoch 15/40
 - 4s - loss: 0.0602 - acc: 0.9724 - val_loss: 0.0861 - val_acc: 0.9686
Epoch 16/40
 - 4s - loss: 0.0601 - acc: 0.9726 - val_loss: 0.0839 - val_acc: 0.9684
Epoch 17/40
 - 4s - loss: 0.0602 - acc: 0.9731 - val_loss: 0.0849 - val_acc: 0.9692
Epoch 18/40
 - 4s - loss: 0.0601 - acc: 0.9731 - val_loss: 0.0884 - val_acc: 0.9684
Epoch 19/40
 - 4s - loss: 0.0604 - acc: 0.9730 - val_loss: 0.0919 - val_acc: 0.9671
Epoch 20/40
 - 4s - loss: 0.0606 - acc: 0.9732 - val_loss: 0.0924 - val_acc: 0.9672
Epoch 21/40
 - 4s - loss: 0.0611 - acc: 0.9732 - val_loss: 0.0926 - val_acc: 0.9676
Epoch 22/40
 - 4s - loss: 0.0608 - acc: 0.9736 - val_loss: 0.0960 - val_acc: 0.9680
Epoch 23/40
 - 4s - loss: 0.0619 - acc: 0.9736 - val_loss: 0.0929 - val_acc: 0.9677
Epoch 24/40
 - 4s - loss: 0.0613 - acc: 0.9735 - val_loss: 0.0949 - val_acc: 0.9681
Epoch 25/40
 - 4s - loss: 0.0607 - acc: 0.9734 - val_loss: 0.0944 - val_acc: 0.9685
Epoch 26/40
 - 4s - loss: 0.0609 - acc: 0.9739 - val_loss: 0.0916 - val_acc: 0.9674
Epoch 27/40
 - 4s - loss: 0.0615 - acc: 0.9740 - val_loss: 0.0962 - val_acc: 0.9678
Epoch 28/40
 - 4s - loss: 0.0610 - acc: 0.9738 - val_loss: 0.1152 - val_acc: 0.9684
Epoch 29/40
 - 4s - loss: 0.0625 - acc: 0.9737 - val_loss: 0.0998 - val_acc: 0.9679
Epoch 30/40
 - 4s - loss: 0.0620 - acc: 0.9736 - val_loss: 0.1027 - val_acc: 0.9677
Epoch 31/40
 - 4s - loss: 0.0616 - acc: 0.9739 - val_loss: 0.1071 - val_acc: 0.9672
Epoch 32/40
 - 4s - loss: 0.0622 - acc: 0.9739 - val_loss: 0.1002 - val_acc: 0.9672
Epoch 33/40
 - 4s - loss: 0.0618 - acc: 0.9737 - val_loss: 0.1015 - val_acc: 0.9677
Epoch 34/40
 - 4s - loss: 0.0625 - acc: 0.9740 - val_loss: 0.1020 - val_acc: 0.9673
Epoch 35/40
 - 4s - loss: 0.0620 - acc: 0.9742 - val_loss: 0.1030 - val_acc: 0.9671
Epoch 36/40
 - 4s - loss: 0.0612 - acc: 0.9740 - val_loss: 0.1086 - val_acc: 0.9662
Epoch 37/40
 - 4s - loss: 0.0617 - acc: 0.9747 - val_loss: 0.1270 - val_acc: 0.9674
Epoch 38/40
 - 4s - loss: 0.0615 - acc: 0.9744 - val_loss: 0.1073 - val_acc: 0.9670
Epoch 39/40
 - 4s - loss: 0.0609 - acc: 0.9742 - val_loss: 0.1120 - val_acc: 0.9675
Epoch 40/40
 - 4s - loss: 0.0612 - acc: 0.9746 - val_loss: 0.1120 - val_acc: 0.9678
Epoch 1/40
 - 0s - loss: 0.1066 - acc: 0.9650
Epoch 2/40
 - 0s - loss: 0.0961 - acc: 0.9663
Epoch 3/40
 - 0s - loss: 0.0896 - acc: 0.9671
Epoch 4/40
 - 0s - loss: 0.0865 - acc: 0.9674
Epoch 5/40
 - 0s - loss: 0.0811 - acc: 0.9685
Epoch 6/40
 - 0s - loss: 0.0808 - acc: 0.9694
Epoch 7/40
 - 0s - loss: 0.0797 - acc: 0.9686
Epoch 8/40
 - 0s - loss: 0.0765 - acc: 0.9703
Epoch 9/40
 - 0s - loss: 0.0774 - acc: 0.9694
Epoch 10/40
 - 0s - loss: 0.0728 - acc: 0.9705
Epoch 11/40
 - 0s - loss: 0.0720 - acc: 0.9712
Epoch 12/40
 - 0s - loss: 0.0704 - acc: 0.9704
Epoch 13/40
 - 0s - loss: 0.0708 - acc: 0.9715
Epoch 14/40
 - 0s - loss: 0.0674 - acc: 0.9720
Epoch 15/40
 - 0s - loss: 0.0646 - acc: 0.9731
Epoch 16/40
 - 0s - loss: 0.0678 - acc: 0.9721
Epoch 17/40
 - 0s - loss: 0.0659 - acc: 0.9720
Epoch 18/40
 - 0s - loss: 0.0635 - acc: 0.9727
Epoch 19/40
 - 0s - loss: 0.0638 - acc: 0.9734
Epoch 20/40
 - 0s - loss: 0.0617 - acc: 0.9738
Epoch 21/40
 - 0s - loss: 0.0611 - acc: 0.9742
Epoch 22/40
 - 0s - loss: 0.0618 - acc: 0.9726
Epoch 23/40
 - 0s - loss: 0.0607 - acc: 0.9730
Epoch 24/40
 - 0s - loss: 0.0611 - acc: 0.9742
Epoch 25/40
 - 0s - loss: 0.0618 - acc: 0.9731
Epoch 26/40
 - 0s - loss: 0.0599 - acc: 0.9747
Epoch 27/40
 - 0s - loss: 0.0579 - acc: 0.9753
Epoch 28/40
 - 0s - loss: 0.0597 - acc: 0.9745
Epoch 29/40
 - 0s - loss: 0.0546 - acc: 0.9745
Epoch 30/40
 - 0s - loss: 0.0558 - acc: 0.9748
Epoch 31/40
 - 0s - loss: 0.0564 - acc: 0.9754
Epoch 32/40
 - 0s - loss: 0.0552 - acc: 0.9751
Epoch 33/40
 - 0s - loss: 0.0560 - acc: 0.9755
Epoch 34/40
 - 0s - loss: 0.0582 - acc: 0.9759
Epoch 35/40
 - 0s - loss: 0.0570 - acc: 0.9745
Epoch 36/40
 - 0s - loss: 0.0546 - acc: 0.9740
Epoch 37/40
 - 0s - loss: 0.0532 - acc: 0.9759
Epoch 38/40
 - 0s - loss: 0.0530 - acc: 0.9754
Epoch 39/40
 - 0s - loss: 0.0547 - acc: 0.9750
Epoch 40/40
 - 0s - loss: 0.0546 - acc: 0.9765
# Training time = 0:03:36.162053
# F-Score(Ordinary) = 0.056, Recall: 0.867, Precision: 0.029
# F-Score(lvc) = 0.126, Recall: 0.818, Precision: 0.068
# F-Score(ireflv) = 0.032, Recall: 1.0, Precision: 0.016
# F-Score(id) = 0.021, Recall: 1.0, Precision: 0.01
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_59 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_60 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_59 (Embedding)        (None, 4, 48)        705264      input_59[0][0]                   
__________________________________________________________________________________________________
embedding_60 (Embedding)        (None, 4, 24)        5640        input_60[0][0]                   
__________________________________________________________________________________________________
flatten_59 (Flatten)            (None, 192)          0           embedding_59[0][0]               
__________________________________________________________________________________________________
flatten_60 (Flatten)            (None, 96)           0           embedding_60[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 288)          0           flatten_59[0][0]                 
                                                                 flatten_60[0][0]                 
__________________________________________________________________________________________________
dense_59 (Dense)                (None, 24)           6936        concatenate_30[0][0]             
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 24)           0           dense_59[0][0]                   
__________________________________________________________________________________________________
dense_60 (Dense)                (None, 8)            200         dropout_30[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1811 - acc: 0.9454 - val_loss: 0.0923 - val_acc: 0.9658
Epoch 2/40
 - 4s - loss: 0.0837 - acc: 0.9665 - val_loss: 0.0787 - val_acc: 0.9663
Epoch 3/40
 - 4s - loss: 0.0731 - acc: 0.9678 - val_loss: 0.0739 - val_acc: 0.9669
Epoch 4/40
 - 4s - loss: 0.0691 - acc: 0.9688 - val_loss: 0.0742 - val_acc: 0.9660
Epoch 5/40
 - 4s - loss: 0.0666 - acc: 0.9695 - val_loss: 0.0734 - val_acc: 0.9658
Epoch 6/40
 - 4s - loss: 0.0643 - acc: 0.9700 - val_loss: 0.0731 - val_acc: 0.9672
Epoch 7/40
 - 4s - loss: 0.0630 - acc: 0.9702 - val_loss: 0.0746 - val_acc: 0.9672
Epoch 8/40
 - 4s - loss: 0.0617 - acc: 0.9709 - val_loss: 0.0744 - val_acc: 0.9680
Epoch 9/40
 - 4s - loss: 0.0608 - acc: 0.9715 - val_loss: 0.0730 - val_acc: 0.9676
Epoch 10/40
 - 4s - loss: 0.0603 - acc: 0.9720 - val_loss: 0.0751 - val_acc: 0.9682
Epoch 11/40
 - 4s - loss: 0.0600 - acc: 0.9722 - val_loss: 0.0741 - val_acc: 0.9677
Epoch 12/40
 - 4s - loss: 0.0598 - acc: 0.9720 - val_loss: 0.0733 - val_acc: 0.9686
Epoch 13/40
 - 4s - loss: 0.0595 - acc: 0.9725 - val_loss: 0.0741 - val_acc: 0.9690
Epoch 14/40
 - 4s - loss: 0.0595 - acc: 0.9726 - val_loss: 0.0782 - val_acc: 0.9688
Epoch 15/40
 - 4s - loss: 0.0591 - acc: 0.9728 - val_loss: 0.0779 - val_acc: 0.9686
Epoch 16/40
 - 4s - loss: 0.0589 - acc: 0.9726 - val_loss: 0.0788 - val_acc: 0.9691
Epoch 17/40
 - 4s - loss: 0.0594 - acc: 0.9725 - val_loss: 0.0796 - val_acc: 0.9683
Epoch 18/40
 - 4s - loss: 0.0592 - acc: 0.9729 - val_loss: 0.0833 - val_acc: 0.9677
Epoch 19/40
 - 4s - loss: 0.0594 - acc: 0.9734 - val_loss: 0.0822 - val_acc: 0.9669
Epoch 20/40
 - 4s - loss: 0.0595 - acc: 0.9731 - val_loss: 0.0817 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0595 - acc: 0.9734 - val_loss: 0.0875 - val_acc: 0.9682
Epoch 22/40
 - 4s - loss: 0.0594 - acc: 0.9734 - val_loss: 0.0844 - val_acc: 0.9679
Epoch 23/40
 - 4s - loss: 0.0597 - acc: 0.9734 - val_loss: 0.0860 - val_acc: 0.9677
Epoch 24/40
 - 4s - loss: 0.0590 - acc: 0.9735 - val_loss: 0.0884 - val_acc: 0.9685
Epoch 25/40
 - 4s - loss: 0.0594 - acc: 0.9738 - val_loss: 0.0892 - val_acc: 0.9680
Epoch 26/40
 - 4s - loss: 0.0591 - acc: 0.9739 - val_loss: 0.0921 - val_acc: 0.9687
Epoch 27/40
 - 4s - loss: 0.0597 - acc: 0.9736 - val_loss: 0.0891 - val_acc: 0.9691
Epoch 28/40
 - 4s - loss: 0.0590 - acc: 0.9740 - val_loss: 0.0963 - val_acc: 0.9685
Epoch 29/40
 - 4s - loss: 0.0589 - acc: 0.9740 - val_loss: 0.0909 - val_acc: 0.9675
Epoch 30/40
 - 4s - loss: 0.0589 - acc: 0.9741 - val_loss: 0.0918 - val_acc: 0.9679
Epoch 31/40
 - 4s - loss: 0.0592 - acc: 0.9743 - val_loss: 0.0891 - val_acc: 0.9678
Epoch 32/40
 - 4s - loss: 0.0595 - acc: 0.9738 - val_loss: 0.0915 - val_acc: 0.9684
Epoch 33/40
 - 4s - loss: 0.0595 - acc: 0.9741 - val_loss: 0.0934 - val_acc: 0.9683
Epoch 34/40
 - 4s - loss: 0.0593 - acc: 0.9740 - val_loss: 0.0953 - val_acc: 0.9679
Epoch 35/40
 - 4s - loss: 0.0587 - acc: 0.9743 - val_loss: 0.1023 - val_acc: 0.9680
Epoch 36/40
 - 4s - loss: 0.0595 - acc: 0.9743 - val_loss: 0.0962 - val_acc: 0.9677
Epoch 37/40
 - 4s - loss: 0.0591 - acc: 0.9743 - val_loss: 0.0967 - val_acc: 0.9679
Epoch 38/40
 - 4s - loss: 0.0595 - acc: 0.9744 - val_loss: 0.0949 - val_acc: 0.9679
Epoch 39/40
 - 4s - loss: 0.0596 - acc: 0.9744 - val_loss: 0.1024 - val_acc: 0.9670
Epoch 40/40
 - 4s - loss: 0.0596 - acc: 0.9744 - val_loss: 0.1005 - val_acc: 0.9669
Epoch 1/40
 - 0s - loss: 0.0973 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.0899 - acc: 0.9663
Epoch 3/40
 - 0s - loss: 0.0862 - acc: 0.9669
Epoch 4/40
 - 0s - loss: 0.0828 - acc: 0.9687
Epoch 5/40
 - 0s - loss: 0.0776 - acc: 0.9699
Epoch 6/40
 - 0s - loss: 0.0763 - acc: 0.9703
Epoch 7/40
 - 0s - loss: 0.0741 - acc: 0.9712
Epoch 8/40
 - 0s - loss: 0.0735 - acc: 0.9708
Epoch 9/40
 - 0s - loss: 0.0714 - acc: 0.9705
Epoch 10/40
 - 0s - loss: 0.0691 - acc: 0.9712
Epoch 11/40
 - 0s - loss: 0.0693 - acc: 0.9711
Epoch 12/40
 - 0s - loss: 0.0672 - acc: 0.9704
Epoch 13/40
 - 0s - loss: 0.0660 - acc: 0.9719
Epoch 14/40
 - 0s - loss: 0.0654 - acc: 0.9723
Epoch 15/40
 - 0s - loss: 0.0634 - acc: 0.9717
Epoch 16/40
 - 0s - loss: 0.0634 - acc: 0.9735
Epoch 17/40
 - 0s - loss: 0.0624 - acc: 0.9719
Epoch 18/40
 - 0s - loss: 0.0603 - acc: 0.9737
Epoch 19/40
 - 0s - loss: 0.0582 - acc: 0.9738
Epoch 20/40
 - 0s - loss: 0.0595 - acc: 0.9748
Epoch 21/40
 - 0s - loss: 0.0587 - acc: 0.9736
Epoch 22/40
 - 0s - loss: 0.0569 - acc: 0.9745
Epoch 23/40
 - 0s - loss: 0.0579 - acc: 0.9747
Epoch 24/40
 - 0s - loss: 0.0570 - acc: 0.9742
Epoch 25/40
 - 0s - loss: 0.0565 - acc: 0.9735
Epoch 26/40
 - 0s - loss: 0.0561 - acc: 0.9759
Epoch 27/40
 - 0s - loss: 0.0558 - acc: 0.9744
Epoch 28/40
 - 0s - loss: 0.0553 - acc: 0.9747
Epoch 29/40
 - 0s - loss: 0.0559 - acc: 0.9744
Epoch 30/40
 - 0s - loss: 0.0562 - acc: 0.9755
Epoch 31/40
 - 0s - loss: 0.0533 - acc: 0.9749
Epoch 32/40
 - 0s - loss: 0.0510 - acc: 0.9745
Epoch 33/40
 - 0s - loss: 0.0525 - acc: 0.9760
Epoch 34/40
 - 0s - loss: 0.0526 - acc: 0.9762
Epoch 35/40
 - 0s - loss: 0.0509 - acc: 0.9752
Epoch 36/40
 - 0s - loss: 0.0503 - acc: 0.9756
Epoch 37/40
 - 0s - loss: 0.0522 - acc: 0.9762
Epoch 38/40
 - 0s - loss: 0.0506 - acc: 0.9761
Epoch 39/40
 - 0s - loss: 0.0506 - acc: 0.9759
Epoch 40/40
 - 0s - loss: 0.0512 - acc: 0.9761
# Training time = 0:03:34.730138
# F-Score(Ordinary) = 0.539, Recall: 0.825, Precision: 0.4
# F-Score(lvc) = 0.279, Recall: 0.6, Precision: 0.182
# F-Score(ireflv) = 0.602, Recall: 0.738, Precision: 0.508
# F-Score(id) = 0.636, Recall: 0.978, Precision: 0.472
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_61 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_62 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_61 (Embedding)        (None, 4, 48)        705264      input_61[0][0]                   
__________________________________________________________________________________________________
embedding_62 (Embedding)        (None, 4, 24)        5640        input_62[0][0]                   
__________________________________________________________________________________________________
flatten_61 (Flatten)            (None, 192)          0           embedding_61[0][0]               
__________________________________________________________________________________________________
flatten_62 (Flatten)            (None, 96)           0           embedding_62[0][0]               
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 288)          0           flatten_61[0][0]                 
                                                                 flatten_62[0][0]                 
__________________________________________________________________________________________________
dense_61 (Dense)                (None, 24)           6936        concatenate_31[0][0]             
__________________________________________________________________________________________________
dropout_31 (Dropout)            (None, 24)           0           dense_61[0][0]                   
__________________________________________________________________________________________________
dense_62 (Dense)                (None, 8)            200         dropout_31[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1270 - acc: 0.9579 - val_loss: 0.0818 - val_acc: 0.9649
Epoch 2/40
 - 4s - loss: 0.0737 - acc: 0.9676 - val_loss: 0.0752 - val_acc: 0.9663
Epoch 3/40
 - 4s - loss: 0.0676 - acc: 0.9690 - val_loss: 0.0813 - val_acc: 0.9662
Epoch 4/40
 - 4s - loss: 0.0656 - acc: 0.9698 - val_loss: 0.0754 - val_acc: 0.9673
Epoch 5/40
 - 4s - loss: 0.0642 - acc: 0.9700 - val_loss: 0.0774 - val_acc: 0.9672
Epoch 6/40
 - 4s - loss: 0.0639 - acc: 0.9708 - val_loss: 0.0800 - val_acc: 0.9687
Epoch 7/40
 - 4s - loss: 0.0638 - acc: 0.9702 - val_loss: 0.0826 - val_acc: 0.9668
Epoch 8/40
 - 4s - loss: 0.0630 - acc: 0.9710 - val_loss: 0.0810 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0627 - acc: 0.9717 - val_loss: 0.0882 - val_acc: 0.9679
Epoch 10/40
 - 4s - loss: 0.0635 - acc: 0.9716 - val_loss: 0.0894 - val_acc: 0.9678
Epoch 11/40
 - 4s - loss: 0.0629 - acc: 0.9721 - val_loss: 0.0935 - val_acc: 0.9687
Epoch 12/40
 - 4s - loss: 0.0630 - acc: 0.9723 - val_loss: 0.0977 - val_acc: 0.9680
Epoch 13/40
 - 4s - loss: 0.0622 - acc: 0.9724 - val_loss: 0.0883 - val_acc: 0.9678
Epoch 14/40
 - 4s - loss: 0.0622 - acc: 0.9730 - val_loss: 0.0950 - val_acc: 0.9687
Epoch 15/40
 - 4s - loss: 0.0626 - acc: 0.9724 - val_loss: 0.0947 - val_acc: 0.9671
Epoch 16/40
 - 4s - loss: 0.0628 - acc: 0.9728 - val_loss: 0.0909 - val_acc: 0.9679
Epoch 17/40
 - 4s - loss: 0.0626 - acc: 0.9730 - val_loss: 0.0964 - val_acc: 0.9678
Epoch 18/40
 - 4s - loss: 0.0627 - acc: 0.9733 - val_loss: 0.0968 - val_acc: 0.9685
Epoch 19/40
 - 4s - loss: 0.0630 - acc: 0.9733 - val_loss: 0.1043 - val_acc: 0.9681
Epoch 20/40
 - 4s - loss: 0.0635 - acc: 0.9731 - val_loss: 0.1021 - val_acc: 0.9680
Epoch 21/40
 - 4s - loss: 0.0624 - acc: 0.9737 - val_loss: 0.1076 - val_acc: 0.9668
Epoch 22/40
 - 4s - loss: 0.0629 - acc: 0.9735 - val_loss: 0.1075 - val_acc: 0.9673
Epoch 23/40
 - 4s - loss: 0.0629 - acc: 0.9740 - val_loss: 0.1102 - val_acc: 0.9683
Epoch 24/40
 - 4s - loss: 0.0624 - acc: 0.9741 - val_loss: 0.1153 - val_acc: 0.9675
Epoch 25/40
 - 4s - loss: 0.0629 - acc: 0.9739 - val_loss: 0.1057 - val_acc: 0.9680
Epoch 26/40
 - 4s - loss: 0.0622 - acc: 0.9742 - val_loss: 0.1044 - val_acc: 0.9685
Epoch 27/40
 - 4s - loss: 0.0618 - acc: 0.9743 - val_loss: 0.1108 - val_acc: 0.9667
Epoch 28/40
 - 4s - loss: 0.0622 - acc: 0.9745 - val_loss: 0.1054 - val_acc: 0.9679
Epoch 29/40
 - 4s - loss: 0.0612 - acc: 0.9744 - val_loss: 0.1111 - val_acc: 0.9674
Epoch 30/40
 - 4s - loss: 0.0613 - acc: 0.9745 - val_loss: 0.1161 - val_acc: 0.9677
Epoch 31/40
 - 4s - loss: 0.0615 - acc: 0.9749 - val_loss: 0.1108 - val_acc: 0.9675
Epoch 32/40
 - 4s - loss: 0.0614 - acc: 0.9745 - val_loss: 0.1091 - val_acc: 0.9670
Epoch 33/40
 - 4s - loss: 0.0620 - acc: 0.9748 - val_loss: 0.1135 - val_acc: 0.9684
Epoch 34/40
 - 4s - loss: 0.0614 - acc: 0.9746 - val_loss: 0.1258 - val_acc: 0.9673
Epoch 35/40
 - 4s - loss: 0.0623 - acc: 0.9754 - val_loss: 0.1271 - val_acc: 0.9671
Epoch 36/40
 - 4s - loss: 0.0630 - acc: 0.9747 - val_loss: 0.1244 - val_acc: 0.9662
Epoch 37/40
 - 4s - loss: 0.0616 - acc: 0.9750 - val_loss: 0.1294 - val_acc: 0.9671
Epoch 38/40
 - 4s - loss: 0.0611 - acc: 0.9750 - val_loss: 0.1299 - val_acc: 0.9673
Epoch 39/40
 - 4s - loss: 0.0611 - acc: 0.9749 - val_loss: 0.1302 - val_acc: 0.9678
Epoch 40/40
 - 4s - loss: 0.0605 - acc: 0.9754 - val_loss: 0.1341 - val_acc: 0.9683
Epoch 1/40
 - 0s - loss: 0.1259 - acc: 0.9641
Epoch 2/40
 - 0s - loss: 0.1043 - acc: 0.9668
Epoch 3/40
 - 0s - loss: 0.1049 - acc: 0.9669
Epoch 4/40
 - 0s - loss: 0.0982 - acc: 0.9671
Epoch 5/40
 - 0s - loss: 0.0961 - acc: 0.9681
Epoch 6/40
 - 0s - loss: 0.0915 - acc: 0.9684
Epoch 7/40
 - 0s - loss: 0.0877 - acc: 0.9696
Epoch 8/40
 - 0s - loss: 0.0886 - acc: 0.9696
Epoch 9/40
 - 0s - loss: 0.0858 - acc: 0.9702
Epoch 10/40
 - 0s - loss: 0.0808 - acc: 0.9707
Epoch 11/40
 - 0s - loss: 0.0779 - acc: 0.9720
Epoch 12/40
 - 0s - loss: 0.0749 - acc: 0.9719
Epoch 13/40
 - 0s - loss: 0.0721 - acc: 0.9731
Epoch 14/40
 - 0s - loss: 0.0741 - acc: 0.9722
Epoch 15/40
 - 0s - loss: 0.0725 - acc: 0.9732
Epoch 16/40
 - 0s - loss: 0.0719 - acc: 0.9732
Epoch 17/40
 - 0s - loss: 0.0717 - acc: 0.9738
Epoch 18/40
 - 0s - loss: 0.0678 - acc: 0.9739
Epoch 19/40
 - 0s - loss: 0.0663 - acc: 0.9730
Epoch 20/40
 - 0s - loss: 0.0650 - acc: 0.9743
Epoch 21/40
 - 0s - loss: 0.0666 - acc: 0.9740
Epoch 22/40
 - 0s - loss: 0.0634 - acc: 0.9756
Epoch 23/40
 - 0s - loss: 0.0625 - acc: 0.9733
Epoch 24/40
 - 0s - loss: 0.0600 - acc: 0.9749
Epoch 25/40
 - 0s - loss: 0.0596 - acc: 0.9748
Epoch 26/40
 - 0s - loss: 0.0630 - acc: 0.9743
Epoch 27/40
 - 0s - loss: 0.0629 - acc: 0.9753
Epoch 28/40
 - 0s - loss: 0.0617 - acc: 0.9753
Epoch 29/40
 - 0s - loss: 0.0647 - acc: 0.9752
Epoch 30/40
 - 0s - loss: 0.0620 - acc: 0.9767
Epoch 31/40
 - 0s - loss: 0.0606 - acc: 0.9757
Epoch 32/40
 - 0s - loss: 0.0616 - acc: 0.9764
Epoch 33/40
 - 0s - loss: 0.0602 - acc: 0.9766
Epoch 34/40
 - 0s - loss: 0.0605 - acc: 0.9760
Epoch 35/40
 - 0s - loss: 0.0598 - acc: 0.9758
Epoch 36/40
 - 0s - loss: 0.0576 - acc: 0.9762
Epoch 37/40
 - 0s - loss: 0.0617 - acc: 0.9775
Epoch 38/40
 - 0s - loss: 0.0571 - acc: 0.9762
Epoch 39/40
 - 0s - loss: 0.0588 - acc: 0.9760
Epoch 40/40
 - 0s - loss: 0.0569 - acc: 0.9752
# Training time = 0:03:36.831729
# F-Score(Ordinary) = 0.386, Recall: 0.797, Precision: 0.255
# F-Score(ireflv) = 0.341, Recall: 0.667, Precision: 0.23
# F-Score(id) = 0.585, Recall: 0.851, Precision: 0.446
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_63 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_64 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_63 (Embedding)        (None, 4, 48)        705264      input_63[0][0]                   
__________________________________________________________________________________________________
embedding_64 (Embedding)        (None, 4, 24)        5640        input_64[0][0]                   
__________________________________________________________________________________________________
flatten_63 (Flatten)            (None, 192)          0           embedding_63[0][0]               
__________________________________________________________________________________________________
flatten_64 (Flatten)            (None, 96)           0           embedding_64[0][0]               
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 288)          0           flatten_63[0][0]                 
                                                                 flatten_64[0][0]                 
__________________________________________________________________________________________________
dense_63 (Dense)                (None, 24)           6936        concatenate_32[0][0]             
__________________________________________________________________________________________________
dropout_32 (Dropout)            (None, 24)           0           dense_63[0][0]                   
__________________________________________________________________________________________________
dense_64 (Dense)                (None, 8)            200         dropout_32[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1293 - acc: 0.9582 - val_loss: 0.0777 - val_acc: 0.9669
Epoch 2/40
 - 4s - loss: 0.0730 - acc: 0.9672 - val_loss: 0.0778 - val_acc: 0.9666
Epoch 3/40
 - 4s - loss: 0.0681 - acc: 0.9686 - val_loss: 0.0751 - val_acc: 0.9673
Epoch 4/40
 - 4s - loss: 0.0660 - acc: 0.9699 - val_loss: 0.0741 - val_acc: 0.9674
Epoch 5/40
 - 4s - loss: 0.0650 - acc: 0.9704 - val_loss: 0.0761 - val_acc: 0.9677
Epoch 6/40
 - 4s - loss: 0.0644 - acc: 0.9713 - val_loss: 0.0791 - val_acc: 0.9679
Epoch 7/40
 - 4s - loss: 0.0634 - acc: 0.9715 - val_loss: 0.0772 - val_acc: 0.9671
Epoch 8/40
 - 4s - loss: 0.0630 - acc: 0.9716 - val_loss: 0.0805 - val_acc: 0.9673
Epoch 9/40
 - 4s - loss: 0.0623 - acc: 0.9721 - val_loss: 0.0797 - val_acc: 0.9679
Epoch 10/40
 - 4s - loss: 0.0627 - acc: 0.9726 - val_loss: 0.0898 - val_acc: 0.9683
Epoch 11/40
 - 4s - loss: 0.0625 - acc: 0.9724 - val_loss: 0.0908 - val_acc: 0.9673
Epoch 12/40
 - 4s - loss: 0.0621 - acc: 0.9729 - val_loss: 0.0883 - val_acc: 0.9670
Epoch 13/40
 - 4s - loss: 0.0616 - acc: 0.9728 - val_loss: 0.0898 - val_acc: 0.9665
Epoch 14/40
 - 4s - loss: 0.0615 - acc: 0.9733 - val_loss: 0.1029 - val_acc: 0.9682
Epoch 15/40
 - 4s - loss: 0.0623 - acc: 0.9731 - val_loss: 0.0915 - val_acc: 0.9684
Epoch 16/40
 - 4s - loss: 0.0623 - acc: 0.9730 - val_loss: 0.0918 - val_acc: 0.9664
Epoch 17/40
 - 4s - loss: 0.0617 - acc: 0.9734 - val_loss: 0.0941 - val_acc: 0.9687
Epoch 18/40
 - 4s - loss: 0.0624 - acc: 0.9734 - val_loss: 0.0939 - val_acc: 0.9676
Epoch 19/40
 - 4s - loss: 0.0630 - acc: 0.9738 - val_loss: 0.0996 - val_acc: 0.9674
Epoch 20/40
 - 4s - loss: 0.0628 - acc: 0.9736 - val_loss: 0.1037 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0636 - acc: 0.9740 - val_loss: 0.0980 - val_acc: 0.9680
Epoch 22/40
 - 4s - loss: 0.0631 - acc: 0.9737 - val_loss: 0.0981 - val_acc: 0.9668
Epoch 23/40
 - 4s - loss: 0.0641 - acc: 0.9741 - val_loss: 0.0999 - val_acc: 0.9674
Epoch 24/40
 - 4s - loss: 0.0630 - acc: 0.9741 - val_loss: 0.1091 - val_acc: 0.9668
Epoch 25/40
 - 4s - loss: 0.0632 - acc: 0.9741 - val_loss: 0.1091 - val_acc: 0.9674
Epoch 26/40
 - 4s - loss: 0.0628 - acc: 0.9737 - val_loss: 0.1159 - val_acc: 0.9689
Epoch 27/40
 - 4s - loss: 0.0631 - acc: 0.9742 - val_loss: 0.1116 - val_acc: 0.9685
Epoch 28/40
 - 4s - loss: 0.0633 - acc: 0.9742 - val_loss: 0.1069 - val_acc: 0.9685
Epoch 29/40
 - 4s - loss: 0.0640 - acc: 0.9746 - val_loss: 0.1101 - val_acc: 0.9680
Epoch 30/40
 - 4s - loss: 0.0631 - acc: 0.9748 - val_loss: 0.1117 - val_acc: 0.9669
Epoch 31/40
 - 4s - loss: 0.0628 - acc: 0.9749 - val_loss: 0.1099 - val_acc: 0.9685
Epoch 32/40
 - 4s - loss: 0.0635 - acc: 0.9745 - val_loss: 0.1022 - val_acc: 0.9685
Epoch 33/40
 - 4s - loss: 0.0637 - acc: 0.9746 - val_loss: 0.1142 - val_acc: 0.9691
Epoch 34/40
 - 4s - loss: 0.0639 - acc: 0.9747 - val_loss: 0.1159 - val_acc: 0.9673
Epoch 35/40
 - 4s - loss: 0.0639 - acc: 0.9744 - val_loss: 0.1135 - val_acc: 0.9661
Epoch 36/40
 - 4s - loss: 0.0638 - acc: 0.9746 - val_loss: 0.1157 - val_acc: 0.9685
Epoch 37/40
 - 4s - loss: 0.0631 - acc: 0.9751 - val_loss: 0.1130 - val_acc: 0.9682
Epoch 38/40
 - 4s - loss: 0.0642 - acc: 0.9748 - val_loss: 0.1207 - val_acc: 0.9690
Epoch 39/40
 - 4s - loss: 0.0635 - acc: 0.9752 - val_loss: 0.1521 - val_acc: 0.9686
Epoch 40/40
 - 4s - loss: 0.0641 - acc: 0.9753 - val_loss: 0.1319 - val_acc: 0.9681
Epoch 1/40
 - 0s - loss: 0.1180 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.1035 - acc: 0.9660
Epoch 3/40
 - 0s - loss: 0.0994 - acc: 0.9671
Epoch 4/40
 - 0s - loss: 0.0975 - acc: 0.9676
Epoch 5/40
 - 0s - loss: 0.0922 - acc: 0.9679
Epoch 6/40
 - 0s - loss: 0.0850 - acc: 0.9708
Epoch 7/40
 - 0s - loss: 0.0873 - acc: 0.9695
Epoch 8/40
 - 0s - loss: 0.0825 - acc: 0.9708
Epoch 9/40
 - 0s - loss: 0.0834 - acc: 0.9711
Epoch 10/40
 - 0s - loss: 0.0774 - acc: 0.9705
Epoch 11/40
 - 0s - loss: 0.0729 - acc: 0.9724
Epoch 12/40
 - 0s - loss: 0.0762 - acc: 0.9717
Epoch 13/40
 - 0s - loss: 0.0704 - acc: 0.9728
Epoch 14/40
 - 0s - loss: 0.0746 - acc: 0.9720
Epoch 15/40
 - 0s - loss: 0.0715 - acc: 0.9728
Epoch 16/40
 - 0s - loss: 0.0719 - acc: 0.9737
Epoch 17/40
 - 0s - loss: 0.0711 - acc: 0.9743
Epoch 18/40
 - 0s - loss: 0.0696 - acc: 0.9734
Epoch 19/40
 - 0s - loss: 0.0663 - acc: 0.9748
Epoch 20/40
 - 0s - loss: 0.0688 - acc: 0.9732
Epoch 21/40
 - 0s - loss: 0.0684 - acc: 0.9739
Epoch 22/40
 - 0s - loss: 0.0677 - acc: 0.9748
Epoch 23/40
 - 0s - loss: 0.0676 - acc: 0.9742
Epoch 24/40
 - 0s - loss: 0.0649 - acc: 0.9757
Epoch 25/40
 - 0s - loss: 0.0654 - acc: 0.9757
Epoch 26/40
 - 0s - loss: 0.0622 - acc: 0.9746
Epoch 27/40
 - 0s - loss: 0.0621 - acc: 0.9752
Epoch 28/40
 - 0s - loss: 0.0618 - acc: 0.9761
Epoch 29/40
 - 0s - loss: 0.0610 - acc: 0.9757
Epoch 30/40
 - 0s - loss: 0.0617 - acc: 0.9757
Epoch 31/40
 - 0s - loss: 0.0598 - acc: 0.9762
Epoch 32/40
 - 0s - loss: 0.0600 - acc: 0.9760
Epoch 33/40
 - 0s - loss: 0.0625 - acc: 0.9758
Epoch 34/40
 - 0s - loss: 0.0580 - acc: 0.9763
Epoch 35/40
 - 0s - loss: 0.0617 - acc: 0.9763
Epoch 36/40
 - 0s - loss: 0.0586 - acc: 0.9753
Epoch 37/40
 - 0s - loss: 0.0601 - acc: 0.9749
Epoch 38/40
 - 0s - loss: 0.0591 - acc: 0.9761
Epoch 39/40
 - 0s - loss: 0.0571 - acc: 0.9761
Epoch 40/40
 - 0s - loss: 0.0597 - acc: 0.9757
# Training time = 0:03:42.536448
# F-Score(Ordinary) = 0.379, Recall: 0.878, Precision: 0.242
# F-Score(lvc) = 0.068, Recall: 0.357, Precision: 0.038
# F-Score(ireflv) = 0.374, Recall: 0.879, Precision: 0.238
# F-Score(id) = 0.55, Recall: 0.974, Precision: 0.383
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_65 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_66 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_65 (Embedding)        (None, 4, 48)        705264      input_65[0][0]                   
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 4, 24)        5640        input_66[0][0]                   
__________________________________________________________________________________________________
flatten_65 (Flatten)            (None, 192)          0           embedding_65[0][0]               
__________________________________________________________________________________________________
flatten_66 (Flatten)            (None, 96)           0           embedding_66[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 288)          0           flatten_65[0][0]                 
                                                                 flatten_66[0][0]                 
__________________________________________________________________________________________________
dense_65 (Dense)                (None, 24)           6936        concatenate_33[0][0]             
__________________________________________________________________________________________________
dropout_33 (Dropout)            (None, 24)           0           dense_65[0][0]                   
__________________________________________________________________________________________________
dense_66 (Dense)                (None, 8)            200         dropout_33[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1268 - acc: 0.9590 - val_loss: 0.0793 - val_acc: 0.9660
Epoch 2/40
 - 4s - loss: 0.0734 - acc: 0.9672 - val_loss: 0.0740 - val_acc: 0.9674
Epoch 3/40
 - 4s - loss: 0.0684 - acc: 0.9688 - val_loss: 0.0735 - val_acc: 0.9676
Epoch 4/40
 - 4s - loss: 0.0659 - acc: 0.9698 - val_loss: 0.0750 - val_acc: 0.9679
Epoch 5/40
 - 4s - loss: 0.0638 - acc: 0.9703 - val_loss: 0.0770 - val_acc: 0.9678
Epoch 6/40
 - 4s - loss: 0.0629 - acc: 0.9711 - val_loss: 0.0757 - val_acc: 0.9679
Epoch 7/40
 - 4s - loss: 0.0618 - acc: 0.9720 - val_loss: 0.0743 - val_acc: 0.9681
Epoch 8/40
 - 4s - loss: 0.0617 - acc: 0.9721 - val_loss: 0.0809 - val_acc: 0.9689
Epoch 9/40
 - 4s - loss: 0.0613 - acc: 0.9721 - val_loss: 0.0843 - val_acc: 0.9658
Epoch 10/40
 - 4s - loss: 0.0625 - acc: 0.9723 - val_loss: 0.0848 - val_acc: 0.9681
Epoch 11/40
 - 4s - loss: 0.0627 - acc: 0.9723 - val_loss: 0.0824 - val_acc: 0.9685
Epoch 12/40
 - 4s - loss: 0.0619 - acc: 0.9728 - val_loss: 0.0910 - val_acc: 0.9681
Epoch 13/40
 - 4s - loss: 0.0630 - acc: 0.9724 - val_loss: 0.0877 - val_acc: 0.9687
Epoch 14/40
 - 4s - loss: 0.0622 - acc: 0.9730 - val_loss: 0.0916 - val_acc: 0.9680
Epoch 15/40
 - 4s - loss: 0.0633 - acc: 0.9723 - val_loss: 0.0883 - val_acc: 0.9681
Epoch 16/40
 - 4s - loss: 0.0628 - acc: 0.9728 - val_loss: 0.0869 - val_acc: 0.9682
Epoch 17/40
 - 4s - loss: 0.0637 - acc: 0.9735 - val_loss: 0.0940 - val_acc: 0.9684
Epoch 18/40
 - 4s - loss: 0.0634 - acc: 0.9732 - val_loss: 0.1022 - val_acc: 0.9684
Epoch 19/40
 - 4s - loss: 0.0640 - acc: 0.9732 - val_loss: 0.0976 - val_acc: 0.9678
Epoch 20/40
 - 4s - loss: 0.0634 - acc: 0.9736 - val_loss: 0.1100 - val_acc: 0.9688
Epoch 21/40
 - 4s - loss: 0.0632 - acc: 0.9735 - val_loss: 0.0974 - val_acc: 0.9676
Epoch 22/40
 - 4s - loss: 0.0633 - acc: 0.9735 - val_loss: 0.1035 - val_acc: 0.9682
Epoch 23/40
 - 4s - loss: 0.0640 - acc: 0.9741 - val_loss: 0.0948 - val_acc: 0.9687
Epoch 24/40
 - 4s - loss: 0.0643 - acc: 0.9741 - val_loss: 0.1249 - val_acc: 0.9689
Epoch 25/40
 - 4s - loss: 0.0638 - acc: 0.9741 - val_loss: 0.1155 - val_acc: 0.9687
Epoch 26/40
 - 4s - loss: 0.0641 - acc: 0.9737 - val_loss: 0.1021 - val_acc: 0.9674
Epoch 27/40
 - 4s - loss: 0.0634 - acc: 0.9741 - val_loss: 0.1144 - val_acc: 0.9674
Epoch 28/40
 - 4s - loss: 0.0644 - acc: 0.9742 - val_loss: 0.1043 - val_acc: 0.9670
Epoch 29/40
 - 4s - loss: 0.0642 - acc: 0.9744 - val_loss: 0.1152 - val_acc: 0.9672
Epoch 30/40
 - 4s - loss: 0.0643 - acc: 0.9747 - val_loss: 0.1241 - val_acc: 0.9682
Epoch 31/40
 - 4s - loss: 0.0646 - acc: 0.9743 - val_loss: 0.1161 - val_acc: 0.9672
Epoch 32/40
 - 4s - loss: 0.0649 - acc: 0.9749 - val_loss: 0.1196 - val_acc: 0.9690
Epoch 33/40
 - 4s - loss: 0.0646 - acc: 0.9745 - val_loss: 0.1363 - val_acc: 0.9682
Epoch 34/40
 - 4s - loss: 0.0644 - acc: 0.9752 - val_loss: 0.1236 - val_acc: 0.9667
Epoch 35/40
 - 4s - loss: 0.0642 - acc: 0.9748 - val_loss: 0.1355 - val_acc: 0.9665
Epoch 36/40
 - 4s - loss: 0.0645 - acc: 0.9748 - val_loss: 0.1266 - val_acc: 0.9681
Epoch 37/40
 - 4s - loss: 0.0644 - acc: 0.9750 - val_loss: 0.1182 - val_acc: 0.9670
Epoch 38/40
 - 4s - loss: 0.0648 - acc: 0.9752 - val_loss: 0.1302 - val_acc: 0.9672
Epoch 39/40
 - 4s - loss: 0.0640 - acc: 0.9753 - val_loss: 0.1275 - val_acc: 0.9658
Epoch 40/40
 - 4s - loss: 0.0664 - acc: 0.9748 - val_loss: 0.1258 - val_acc: 0.9672
Epoch 1/40
 - 0s - loss: 0.1255 - acc: 0.9642
Epoch 2/40
 - 0s - loss: 0.1142 - acc: 0.9666
Epoch 3/40
 - 0s - loss: 0.1048 - acc: 0.9696
Epoch 4/40
 - 0s - loss: 0.1016 - acc: 0.9691
Epoch 5/40
 - 0s - loss: 0.0961 - acc: 0.9690
Epoch 6/40
 - 0s - loss: 0.0965 - acc: 0.9700
Epoch 7/40
 - 0s - loss: 0.0912 - acc: 0.9709
Epoch 8/40
 - 0s - loss: 0.0876 - acc: 0.9711
Epoch 9/40
 - 0s - loss: 0.0851 - acc: 0.9719
Epoch 10/40
 - 0s - loss: 0.0841 - acc: 0.9715
Epoch 11/40
 - 0s - loss: 0.0831 - acc: 0.9720
Epoch 12/40
 - 0s - loss: 0.0807 - acc: 0.9724
Epoch 13/40
 - 0s - loss: 0.0787 - acc: 0.9727
Epoch 14/40
 - 0s - loss: 0.0786 - acc: 0.9730
Epoch 15/40
 - 0s - loss: 0.0764 - acc: 0.9726
Epoch 16/40
 - 0s - loss: 0.0753 - acc: 0.9742
Epoch 17/40
 - 0s - loss: 0.0764 - acc: 0.9736
Epoch 18/40
 - 0s - loss: 0.0731 - acc: 0.9737
Epoch 19/40
 - 0s - loss: 0.0737 - acc: 0.9747
Epoch 20/40
 - 0s - loss: 0.0704 - acc: 0.9735
Epoch 21/40
 - 0s - loss: 0.0700 - acc: 0.9735
Epoch 22/40
 - 0s - loss: 0.0738 - acc: 0.9746
Epoch 23/40
 - 0s - loss: 0.0715 - acc: 0.9736
Epoch 24/40
 - 0s - loss: 0.0664 - acc: 0.9739
Epoch 25/40
 - 0s - loss: 0.0709 - acc: 0.9750
Epoch 26/40
 - 0s - loss: 0.0708 - acc: 0.9751
Epoch 27/40
 - 0s - loss: 0.0681 - acc: 0.9745
Epoch 28/40
 - 0s - loss: 0.0669 - acc: 0.9755
Epoch 29/40
 - 0s - loss: 0.0656 - acc: 0.9736
Epoch 30/40
 - 0s - loss: 0.0651 - acc: 0.9759
Epoch 31/40
 - 0s - loss: 0.0654 - acc: 0.9761
Epoch 32/40
 - 0s - loss: 0.0667 - acc: 0.9757
Epoch 33/40
 - 0s - loss: 0.0672 - acc: 0.9754
Epoch 34/40
 - 0s - loss: 0.0648 - acc: 0.9757
Epoch 35/40
 - 0s - loss: 0.0653 - acc: 0.9757
Epoch 36/40
 - 0s - loss: 0.0664 - acc: 0.9762
Epoch 37/40
 - 0s - loss: 0.0652 - acc: 0.9760
Epoch 38/40
 - 0s - loss: 0.0647 - acc: 0.9755
Epoch 39/40
 - 0s - loss: 0.0674 - acc: 0.9747
Epoch 40/40
 - 0s - loss: 0.0613 - acc: 0.9763
# Training time = 0:03:35.282520
# F-Score(Ordinary) = 0.175, Recall: 0.786, Precision: 0.098
# F-Score(lvc) = 0.071, Recall: 0.625, Precision: 0.038
# F-Score(ireflv) = 0.016, Recall: 1.0, Precision: 0.008
# F-Score(id) = 0.317, Recall: 0.809, Precision: 0.197
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_67 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_68 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 4, 48)        705264      input_67[0][0]                   
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 4, 24)        5640        input_68[0][0]                   
__________________________________________________________________________________________________
flatten_67 (Flatten)            (None, 192)          0           embedding_67[0][0]               
__________________________________________________________________________________________________
flatten_68 (Flatten)            (None, 96)           0           embedding_68[0][0]               
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 288)          0           flatten_67[0][0]                 
                                                                 flatten_68[0][0]                 
__________________________________________________________________________________________________
dense_67 (Dense)                (None, 24)           6936        concatenate_34[0][0]             
__________________________________________________________________________________________________
dropout_34 (Dropout)            (None, 24)           0           dense_67[0][0]                   
__________________________________________________________________________________________________
dense_68 (Dense)                (None, 8)            200         dropout_34[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1338 - acc: 0.9556 - val_loss: 0.0777 - val_acc: 0.9658
Epoch 2/40
 - 4s - loss: 0.0732 - acc: 0.9676 - val_loss: 0.0756 - val_acc: 0.9663
Epoch 3/40
 - 4s - loss: 0.0680 - acc: 0.9689 - val_loss: 0.0743 - val_acc: 0.9677
Epoch 4/40
 - 4s - loss: 0.0658 - acc: 0.9695 - val_loss: 0.0758 - val_acc: 0.9673
Epoch 5/40
 - 4s - loss: 0.0642 - acc: 0.9707 - val_loss: 0.0788 - val_acc: 0.9677
Epoch 6/40
 - 4s - loss: 0.0635 - acc: 0.9710 - val_loss: 0.0802 - val_acc: 0.9677
Epoch 7/40
 - 4s - loss: 0.0636 - acc: 0.9712 - val_loss: 0.0812 - val_acc: 0.9683
Epoch 8/40
 - 4s - loss: 0.0625 - acc: 0.9714 - val_loss: 0.0907 - val_acc: 0.9676
Epoch 9/40
 - 4s - loss: 0.0618 - acc: 0.9724 - val_loss: 0.0876 - val_acc: 0.9682
Epoch 10/40
 - 4s - loss: 0.0627 - acc: 0.9724 - val_loss: 0.0854 - val_acc: 0.9676
Epoch 11/40
 - 4s - loss: 0.0625 - acc: 0.9727 - val_loss: 0.0981 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0634 - acc: 0.9723 - val_loss: 0.0858 - val_acc: 0.9680
Epoch 13/40
 - 4s - loss: 0.0630 - acc: 0.9730 - val_loss: 0.0978 - val_acc: 0.9679
Epoch 14/40
 - 4s - loss: 0.0632 - acc: 0.9731 - val_loss: 0.0939 - val_acc: 0.9671
Epoch 15/40
 - 4s - loss: 0.0635 - acc: 0.9731 - val_loss: 0.0996 - val_acc: 0.9685
Epoch 16/40
 - 4s - loss: 0.0636 - acc: 0.9730 - val_loss: 0.0975 - val_acc: 0.9684
Epoch 17/40
 - 4s - loss: 0.0640 - acc: 0.9733 - val_loss: 0.1007 - val_acc: 0.9676
Epoch 18/40
 - 4s - loss: 0.0635 - acc: 0.9738 - val_loss: 0.0988 - val_acc: 0.9683
Epoch 19/40
 - 4s - loss: 0.0635 - acc: 0.9737 - val_loss: 0.1128 - val_acc: 0.9682
Epoch 20/40
 - 4s - loss: 0.0636 - acc: 0.9738 - val_loss: 0.1031 - val_acc: 0.9678
Epoch 21/40
 - 4s - loss: 0.0648 - acc: 0.9742 - val_loss: 0.1237 - val_acc: 0.9671
Epoch 22/40
 - 4s - loss: 0.0642 - acc: 0.9740 - val_loss: 0.1031 - val_acc: 0.9685
Epoch 23/40
 - 4s - loss: 0.0649 - acc: 0.9735 - val_loss: 0.1178 - val_acc: 0.9687
Epoch 24/40
 - 4s - loss: 0.0637 - acc: 0.9744 - val_loss: 0.1109 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0637 - acc: 0.9741 - val_loss: 0.1128 - val_acc: 0.9674
Epoch 26/40
 - 4s - loss: 0.0645 - acc: 0.9743 - val_loss: 0.1239 - val_acc: 0.9672
Epoch 27/40
 - 4s - loss: 0.0639 - acc: 0.9744 - val_loss: 0.1315 - val_acc: 0.9683
Epoch 28/40
 - 4s - loss: 0.0646 - acc: 0.9743 - val_loss: 0.1202 - val_acc: 0.9678
Epoch 29/40
 - 4s - loss: 0.0669 - acc: 0.9745 - val_loss: 0.1112 - val_acc: 0.9671
Epoch 30/40
 - 4s - loss: 0.0654 - acc: 0.9742 - val_loss: 0.1152 - val_acc: 0.9680
Epoch 31/40
 - 4s - loss: 0.0645 - acc: 0.9749 - val_loss: 0.1296 - val_acc: 0.9665
Epoch 32/40
 - 4s - loss: 0.0632 - acc: 0.9744 - val_loss: 0.1227 - val_acc: 0.9683
Epoch 33/40
 - 4s - loss: 0.0640 - acc: 0.9747 - val_loss: 0.1215 - val_acc: 0.9689
Epoch 34/40
 - 4s - loss: 0.0656 - acc: 0.9746 - val_loss: 0.1248 - val_acc: 0.9674
Epoch 35/40
 - 4s - loss: 0.0655 - acc: 0.9750 - val_loss: 0.1312 - val_acc: 0.9673
Epoch 36/40
 - 4s - loss: 0.0645 - acc: 0.9750 - val_loss: 0.1357 - val_acc: 0.9674
Epoch 37/40
 - 4s - loss: 0.0647 - acc: 0.9749 - val_loss: 0.1393 - val_acc: 0.9672
Epoch 38/40
 - 4s - loss: 0.0644 - acc: 0.9750 - val_loss: 0.1310 - val_acc: 0.9682
Epoch 39/40
 - 4s - loss: 0.0646 - acc: 0.9750 - val_loss: 0.1379 - val_acc: 0.9669
Epoch 40/40
 - 4s - loss: 0.0654 - acc: 0.9754 - val_loss: 0.1416 - val_acc: 0.9684
Epoch 1/40
 - 0s - loss: 0.1305 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.1170 - acc: 0.9671
Epoch 3/40
 - 0s - loss: 0.1061 - acc: 0.9679
Epoch 4/40
 - 0s - loss: 0.1048 - acc: 0.9686
Epoch 5/40
 - 0s - loss: 0.0951 - acc: 0.9688
Epoch 6/40
 - 0s - loss: 0.0941 - acc: 0.9709
Epoch 7/40
 - 0s - loss: 0.0946 - acc: 0.9709
Epoch 8/40
 - 0s - loss: 0.0855 - acc: 0.9709
Epoch 9/40
 - 0s - loss: 0.0887 - acc: 0.9707
Epoch 10/40
 - 0s - loss: 0.0836 - acc: 0.9714
Epoch 11/40
 - 0s - loss: 0.0850 - acc: 0.9711
Epoch 12/40
 - 0s - loss: 0.0852 - acc: 0.9713
Epoch 13/40
 - 0s - loss: 0.0818 - acc: 0.9736
Epoch 14/40
 - 0s - loss: 0.0765 - acc: 0.9729
Epoch 15/40
 - 0s - loss: 0.0773 - acc: 0.9735
Epoch 16/40
 - 0s - loss: 0.0777 - acc: 0.9724
Epoch 17/40
 - 0s - loss: 0.0728 - acc: 0.9739
Epoch 18/40
 - 0s - loss: 0.0735 - acc: 0.9742
Epoch 19/40
 - 0s - loss: 0.0727 - acc: 0.9747
Epoch 20/40
 - 0s - loss: 0.0748 - acc: 0.9746
Epoch 21/40
 - 0s - loss: 0.0690 - acc: 0.9745
Epoch 22/40
 - 0s - loss: 0.0699 - acc: 0.9755
Epoch 23/40
 - 0s - loss: 0.0698 - acc: 0.9754
Epoch 24/40
 - 0s - loss: 0.0691 - acc: 0.9752
Epoch 25/40
 - 0s - loss: 0.0686 - acc: 0.9745
Epoch 26/40
 - 0s - loss: 0.0706 - acc: 0.9755
Epoch 27/40
 - 0s - loss: 0.0661 - acc: 0.9749
Epoch 28/40
 - 0s - loss: 0.0708 - acc: 0.9761
Epoch 29/40
 - 0s - loss: 0.0634 - acc: 0.9755
Epoch 30/40
 - 0s - loss: 0.0659 - acc: 0.9761
Epoch 31/40
 - 0s - loss: 0.0705 - acc: 0.9747
Epoch 32/40
 - 0s - loss: 0.0628 - acc: 0.9764
Epoch 33/40
 - 0s - loss: 0.0665 - acc: 0.9766
Epoch 34/40
 - 0s - loss: 0.0673 - acc: 0.9767
Epoch 35/40
 - 0s - loss: 0.0603 - acc: 0.9757
Epoch 36/40
 - 0s - loss: 0.0637 - acc: 0.9762
Epoch 37/40
 - 0s - loss: 0.0646 - acc: 0.9759
Epoch 38/40
 - 0s - loss: 0.0632 - acc: 0.9767
Epoch 39/40
 - 0s - loss: 0.0610 - acc: 0.9763
Epoch 40/40
 - 0s - loss: 0.0620 - acc: 0.9764
# Training time = 0:03:37.002201
# F-Score(Ordinary) = 0.156, Recall: 0.284, Precision: 0.107
# F-Score(lvc) = 0.216, Recall: 0.213, Precision: 0.22
# F-Score(ireflv) = 0.142, Recall: 0.333, Precision: 0.09
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_69 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_70 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_69 (Embedding)        (None, 4, 48)        705264      input_69[0][0]                   
__________________________________________________________________________________________________
embedding_70 (Embedding)        (None, 4, 24)        5640        input_70[0][0]                   
__________________________________________________________________________________________________
flatten_69 (Flatten)            (None, 192)          0           embedding_69[0][0]               
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 96)           0           embedding_70[0][0]               
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 288)          0           flatten_69[0][0]                 
                                                                 flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_69 (Dense)                (None, 24)           6936        concatenate_35[0][0]             
__________________________________________________________________________________________________
dropout_35 (Dropout)            (None, 24)           0           dense_69[0][0]                   
__________________________________________________________________________________________________
dense_70 (Dense)                (None, 8)            200         dropout_35[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1320 - acc: 0.9553 - val_loss: 0.0786 - val_acc: 0.9664
Epoch 2/40
 - 4s - loss: 0.0740 - acc: 0.9672 - val_loss: 0.0740 - val_acc: 0.9668
Epoch 3/40
 - 4s - loss: 0.0683 - acc: 0.9685 - val_loss: 0.0727 - val_acc: 0.9680
Epoch 4/40
 - 4s - loss: 0.0658 - acc: 0.9699 - val_loss: 0.0746 - val_acc: 0.9666
Epoch 5/40
 - 4s - loss: 0.0641 - acc: 0.9706 - val_loss: 0.0752 - val_acc: 0.9664
Epoch 6/40
 - 4s - loss: 0.0628 - acc: 0.9710 - val_loss: 0.0780 - val_acc: 0.9679
Epoch 7/40
 - 4s - loss: 0.0626 - acc: 0.9712 - val_loss: 0.0783 - val_acc: 0.9682
Epoch 8/40
 - 4s - loss: 0.0622 - acc: 0.9720 - val_loss: 0.0851 - val_acc: 0.9682
Epoch 9/40
 - 4s - loss: 0.0617 - acc: 0.9725 - val_loss: 0.0808 - val_acc: 0.9676
Epoch 10/40
 - 4s - loss: 0.0618 - acc: 0.9726 - val_loss: 0.0825 - val_acc: 0.9681
Epoch 11/40
 - 4s - loss: 0.0616 - acc: 0.9724 - val_loss: 0.0841 - val_acc: 0.9680
Epoch 12/40
 - 4s - loss: 0.0623 - acc: 0.9722 - val_loss: 0.0827 - val_acc: 0.9686
Epoch 13/40
 - 4s - loss: 0.0616 - acc: 0.9731 - val_loss: 0.0863 - val_acc: 0.9684
Epoch 14/40
 - 4s - loss: 0.0618 - acc: 0.9730 - val_loss: 0.0912 - val_acc: 0.9685
Epoch 15/40
 - 4s - loss: 0.0611 - acc: 0.9734 - val_loss: 0.0872 - val_acc: 0.9682
Epoch 16/40
 - 4s - loss: 0.0609 - acc: 0.9731 - val_loss: 0.0909 - val_acc: 0.9683
Epoch 17/40
 - 4s - loss: 0.0612 - acc: 0.9735 - val_loss: 0.0918 - val_acc: 0.9682
Epoch 18/40
 - 4s - loss: 0.0617 - acc: 0.9733 - val_loss: 0.1001 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0607 - acc: 0.9737 - val_loss: 0.0938 - val_acc: 0.9666
Epoch 20/40
 - 4s - loss: 0.0616 - acc: 0.9735 - val_loss: 0.0966 - val_acc: 0.9673
Epoch 21/40
 - 4s - loss: 0.0609 - acc: 0.9735 - val_loss: 0.1055 - val_acc: 0.9673
Epoch 22/40
 - 4s - loss: 0.0605 - acc: 0.9742 - val_loss: 0.1028 - val_acc: 0.9677
Epoch 23/40
 - 4s - loss: 0.0608 - acc: 0.9740 - val_loss: 0.1034 - val_acc: 0.9675
Epoch 24/40
 - 4s - loss: 0.0601 - acc: 0.9743 - val_loss: 0.1172 - val_acc: 0.9681
Epoch 25/40
 - 4s - loss: 0.0606 - acc: 0.9746 - val_loss: 0.1056 - val_acc: 0.9674
Epoch 26/40
 - 4s - loss: 0.0613 - acc: 0.9746 - val_loss: 0.1321 - val_acc: 0.9679
Epoch 27/40
 - 4s - loss: 0.0614 - acc: 0.9741 - val_loss: 0.1134 - val_acc: 0.9679
Epoch 28/40
 - 4s - loss: 0.0608 - acc: 0.9751 - val_loss: 0.1144 - val_acc: 0.9674
Epoch 29/40
 - 4s - loss: 0.0602 - acc: 0.9747 - val_loss: 0.1174 - val_acc: 0.9677
Epoch 30/40
 - 4s - loss: 0.0592 - acc: 0.9746 - val_loss: 0.1083 - val_acc: 0.9659
Epoch 31/40
 - 4s - loss: 0.0599 - acc: 0.9747 - val_loss: 0.1230 - val_acc: 0.9667
Epoch 32/40
 - 4s - loss: 0.0606 - acc: 0.9752 - val_loss: 0.1129 - val_acc: 0.9674
Epoch 33/40
 - 4s - loss: 0.0592 - acc: 0.9743 - val_loss: 0.1158 - val_acc: 0.9663
Epoch 34/40
 - 4s - loss: 0.0603 - acc: 0.9749 - val_loss: 0.1203 - val_acc: 0.9661
Epoch 35/40
 - 4s - loss: 0.0601 - acc: 0.9748 - val_loss: 0.1351 - val_acc: 0.9669
Epoch 36/40
 - 4s - loss: 0.0616 - acc: 0.9747 - val_loss: 0.1198 - val_acc: 0.9665
Epoch 37/40
 - 4s - loss: 0.0622 - acc: 0.9751 - val_loss: 0.1200 - val_acc: 0.9663
Epoch 38/40
 - 4s - loss: 0.0615 - acc: 0.9752 - val_loss: 0.1263 - val_acc: 0.9671
Epoch 39/40
 - 4s - loss: 0.0625 - acc: 0.9751 - val_loss: 0.1265 - val_acc: 0.9663
Epoch 40/40
 - 4s - loss: 0.0610 - acc: 0.9749 - val_loss: 0.1402 - val_acc: 0.9662
Epoch 1/40
 - 0s - loss: 0.1236 - acc: 0.9638
Epoch 2/40
 - 0s - loss: 0.1103 - acc: 0.9659
Epoch 3/40
 - 0s - loss: 0.1038 - acc: 0.9682
Epoch 4/40
 - 0s - loss: 0.0996 - acc: 0.9691
Epoch 5/40
 - 0s - loss: 0.0972 - acc: 0.9693
Epoch 6/40
 - 0s - loss: 0.0916 - acc: 0.9688
Epoch 7/40
 - 0s - loss: 0.0889 - acc: 0.9698
Epoch 8/40
 - 0s - loss: 0.0867 - acc: 0.9706
Epoch 9/40
 - 0s - loss: 0.0869 - acc: 0.9717
Epoch 10/40
 - 0s - loss: 0.0815 - acc: 0.9723
Epoch 11/40
 - 0s - loss: 0.0805 - acc: 0.9722
Epoch 12/40
 - 0s - loss: 0.0784 - acc: 0.9728
Epoch 13/40
 - 0s - loss: 0.0780 - acc: 0.9724
Epoch 14/40
 - 0s - loss: 0.0780 - acc: 0.9728
Epoch 15/40
 - 0s - loss: 0.0753 - acc: 0.9739
Epoch 16/40
 - 0s - loss: 0.0765 - acc: 0.9731
Epoch 17/40
 - 0s - loss: 0.0752 - acc: 0.9735
Epoch 18/40
 - 0s - loss: 0.0716 - acc: 0.9747
Epoch 19/40
 - 0s - loss: 0.0714 - acc: 0.9745
Epoch 20/40
 - 0s - loss: 0.0713 - acc: 0.9756
Epoch 21/40
 - 0s - loss: 0.0727 - acc: 0.9743
Epoch 22/40
 - 0s - loss: 0.0703 - acc: 0.9753
Epoch 23/40
 - 0s - loss: 0.0688 - acc: 0.9735
Epoch 24/40
 - 0s - loss: 0.0677 - acc: 0.9737
Epoch 25/40
 - 0s - loss: 0.0647 - acc: 0.9749
Epoch 26/40
 - 0s - loss: 0.0672 - acc: 0.9755
Epoch 27/40
 - 0s - loss: 0.0679 - acc: 0.9750
Epoch 28/40
 - 0s - loss: 0.0641 - acc: 0.9750
Epoch 29/40
 - 0s - loss: 0.0625 - acc: 0.9754
Epoch 30/40
 - 0s - loss: 0.0649 - acc: 0.9758
Epoch 31/40
 - 0s - loss: 0.0627 - acc: 0.9753
Epoch 32/40
 - 0s - loss: 0.0616 - acc: 0.9757
Epoch 33/40
 - 0s - loss: 0.0591 - acc: 0.9766
Epoch 34/40
 - 0s - loss: 0.0583 - acc: 0.9761
Epoch 35/40
 - 0s - loss: 0.0606 - acc: 0.9767
Epoch 36/40
 - 0s - loss: 0.0623 - acc: 0.9770
Epoch 37/40
 - 0s - loss: 0.0606 - acc: 0.9771
Epoch 38/40
 - 0s - loss: 0.0615 - acc: 0.9767
Epoch 39/40
 - 0s - loss: 0.0592 - acc: 0.9758
Epoch 40/40
 - 0s - loss: 0.0614 - acc: 0.9773
# Training time = 0:03:44.284691
# F-Score(Ordinary) = 0.53, Recall: 0.818, Precision: 0.391
# F-Score(lvc) = 0.454, Recall: 0.627, Precision: 0.356
# F-Score(ireflv) = 0.329, Recall: 0.722, Precision: 0.213
# F-Score(id) = 0.682, Recall: 0.981, Precision: 0.523
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_71 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_72 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_71 (Embedding)        (None, 4, 48)        705264      input_71[0][0]                   
__________________________________________________________________________________________________
embedding_72 (Embedding)        (None, 4, 24)        5640        input_72[0][0]                   
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 192)          0           embedding_71[0][0]               
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 96)           0           embedding_72[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 288)          0           flatten_71[0][0]                 
                                                                 flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_71 (Dense)                (None, 24)           6936        concatenate_36[0][0]             
__________________________________________________________________________________________________
dropout_36 (Dropout)            (None, 24)           0           dense_71[0][0]                   
__________________________________________________________________________________________________
dense_72 (Dense)                (None, 8)            200         dropout_36[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1047 - acc: 0.9614 - val_loss: 0.0780 - val_acc: 0.9664
Epoch 2/40
 - 4s - loss: 0.0726 - acc: 0.9678 - val_loss: 0.0755 - val_acc: 0.9661
Epoch 3/40
 - 4s - loss: 0.0696 - acc: 0.9690 - val_loss: 0.0913 - val_acc: 0.9665
Epoch 4/40
 - 4s - loss: 0.0704 - acc: 0.9698 - val_loss: 0.0872 - val_acc: 0.9675
Epoch 5/40
 - 4s - loss: 0.0717 - acc: 0.9701 - val_loss: 0.1012 - val_acc: 0.9665
Epoch 6/40
 - 4s - loss: 0.0719 - acc: 0.9700 - val_loss: 0.0882 - val_acc: 0.9666
Epoch 7/40
 - 4s - loss: 0.0739 - acc: 0.9707 - val_loss: 0.0994 - val_acc: 0.9634
Epoch 8/40
 - 4s - loss: 0.0733 - acc: 0.9712 - val_loss: 0.0948 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0723 - acc: 0.9719 - val_loss: 0.1111 - val_acc: 0.9686
Epoch 10/40
 - 4s - loss: 0.0735 - acc: 0.9716 - val_loss: 0.1174 - val_acc: 0.9678
Epoch 11/40
 - 4s - loss: 0.0730 - acc: 0.9724 - val_loss: 0.1299 - val_acc: 0.9671
Epoch 12/40
 - 4s - loss: 0.0720 - acc: 0.9728 - val_loss: 0.1111 - val_acc: 0.9671
Epoch 13/40
 - 4s - loss: 0.0712 - acc: 0.9726 - val_loss: 0.1137 - val_acc: 0.9659
Epoch 14/40
 - 4s - loss: 0.0723 - acc: 0.9729 - val_loss: 0.1182 - val_acc: 0.9672
Epoch 15/40
 - 4s - loss: 0.0720 - acc: 0.9734 - val_loss: 0.1292 - val_acc: 0.9674
Epoch 16/40
 - 4s - loss: 0.0715 - acc: 0.9735 - val_loss: 0.1230 - val_acc: 0.9663
Epoch 17/40
 - 4s - loss: 0.0726 - acc: 0.9734 - val_loss: 0.1276 - val_acc: 0.9679
Epoch 18/40
 - 4s - loss: 0.0724 - acc: 0.9740 - val_loss: 0.1274 - val_acc: 0.9662
Epoch 19/40
 - 4s - loss: 0.0716 - acc: 0.9739 - val_loss: 0.1295 - val_acc: 0.9666
Epoch 20/40
 - 4s - loss: 0.0733 - acc: 0.9740 - val_loss: 0.1273 - val_acc: 0.9670
Epoch 21/40
 - 4s - loss: 0.0738 - acc: 0.9745 - val_loss: 0.1563 - val_acc: 0.9677
Epoch 22/40
 - 4s - loss: 0.0732 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.9666
Epoch 23/40
 - 4s - loss: 0.0730 - acc: 0.9742 - val_loss: 0.1428 - val_acc: 0.9672
Epoch 24/40
 - 4s - loss: 0.0757 - acc: 0.9742 - val_loss: 0.1504 - val_acc: 0.9678
Epoch 25/40
 - 4s - loss: 0.0754 - acc: 0.9749 - val_loss: 0.1519 - val_acc: 0.9674
Epoch 26/40
 - 4s - loss: 0.0755 - acc: 0.9744 - val_loss: 0.1456 - val_acc: 0.9671
Epoch 27/40
 - 4s - loss: 0.0754 - acc: 0.9746 - val_loss: 0.1590 - val_acc: 0.9672
Epoch 28/40
 - 4s - loss: 0.0756 - acc: 0.9748 - val_loss: 0.1494 - val_acc: 0.9678
Epoch 29/40
 - 4s - loss: 0.0782 - acc: 0.9750 - val_loss: 0.1596 - val_acc: 0.9669
Epoch 30/40
 - 4s - loss: 0.0764 - acc: 0.9748 - val_loss: 0.1737 - val_acc: 0.9668
Epoch 31/40
 - 4s - loss: 0.0759 - acc: 0.9752 - val_loss: 0.1692 - val_acc: 0.9676
Epoch 32/40
 - 4s - loss: 0.0775 - acc: 0.9752 - val_loss: 0.1557 - val_acc: 0.9672
Epoch 33/40
 - 4s - loss: 0.0750 - acc: 0.9753 - val_loss: 0.1626 - val_acc: 0.9661
Epoch 34/40
 - 4s - loss: 0.0805 - acc: 0.9750 - val_loss: 0.1722 - val_acc: 0.9676
Epoch 35/40
 - 4s - loss: 0.0787 - acc: 0.9753 - val_loss: 0.1682 - val_acc: 0.9671
Epoch 36/40
 - 4s - loss: 0.0778 - acc: 0.9753 - val_loss: 0.1594 - val_acc: 0.9677
Epoch 37/40
 - 4s - loss: 0.0768 - acc: 0.9753 - val_loss: 0.1673 - val_acc: 0.9672
Epoch 38/40
 - 4s - loss: 0.0790 - acc: 0.9752 - val_loss: 0.1600 - val_acc: 0.9676
Epoch 39/40
 - 4s - loss: 0.0771 - acc: 0.9753 - val_loss: 0.1629 - val_acc: 0.9679
Epoch 40/40
 - 4s - loss: 0.0769 - acc: 0.9760 - val_loss: 0.1677 - val_acc: 0.9672
Epoch 1/40
 - 0s - loss: 0.1750 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.1399 - acc: 0.9664
Epoch 3/40
 - 0s - loss: 0.1317 - acc: 0.9665
Epoch 4/40
 - 0s - loss: 0.1284 - acc: 0.9671
Epoch 5/40
 - 0s - loss: 0.1189 - acc: 0.9697
Epoch 6/40
 - 0s - loss: 0.1173 - acc: 0.9698
Epoch 7/40
 - 0s - loss: 0.1170 - acc: 0.9699
Epoch 8/40
 - 0s - loss: 0.1083 - acc: 0.9707
Epoch 9/40
 - 0s - loss: 0.1075 - acc: 0.9704
Epoch 10/40
 - 0s - loss: 0.1090 - acc: 0.9724
Epoch 11/40
 - 0s - loss: 0.1038 - acc: 0.9731
Epoch 12/40
 - 0s - loss: 0.1018 - acc: 0.9728
Epoch 13/40
 - 0s - loss: 0.1001 - acc: 0.9741
Epoch 14/40
 - 0s - loss: 0.1039 - acc: 0.9724
Epoch 15/40
 - 0s - loss: 0.0952 - acc: 0.9735
Epoch 16/40
 - 0s - loss: 0.0999 - acc: 0.9738
Epoch 17/40
 - 0s - loss: 0.0930 - acc: 0.9725
Epoch 18/40
 - 0s - loss: 0.0929 - acc: 0.9745
Epoch 19/40
 - 0s - loss: 0.0872 - acc: 0.9742
Epoch 20/40
 - 0s - loss: 0.0910 - acc: 0.9747
Epoch 21/40
 - 0s - loss: 0.0875 - acc: 0.9749
Epoch 22/40
 - 0s - loss: 0.0867 - acc: 0.9759
Epoch 23/40
 - 0s - loss: 0.0840 - acc: 0.9751
Epoch 24/40
 - 0s - loss: 0.0802 - acc: 0.9740
Epoch 25/40
 - 0s - loss: 0.0862 - acc: 0.9749
Epoch 26/40
 - 0s - loss: 0.0841 - acc: 0.9752
Epoch 27/40
 - 0s - loss: 0.0831 - acc: 0.9760
Epoch 28/40
 - 0s - loss: 0.0864 - acc: 0.9752
Epoch 29/40
 - 0s - loss: 0.0842 - acc: 0.9762
Epoch 30/40
 - 0s - loss: 0.0847 - acc: 0.9757
Epoch 31/40
 - 0s - loss: 0.0793 - acc: 0.9764
Epoch 32/40
 - 0s - loss: 0.0838 - acc: 0.9761
Epoch 33/40
 - 0s - loss: 0.0790 - acc: 0.9766
Epoch 34/40
 - 0s - loss: 0.0780 - acc: 0.9770
Epoch 35/40
 - 0s - loss: 0.0850 - acc: 0.9766
Epoch 36/40
 - 0s - loss: 0.0822 - acc: 0.9761
Epoch 37/40
 - 0s - loss: 0.0801 - acc: 0.9770
Epoch 38/40
 - 0s - loss: 0.0792 - acc: 0.9758
Epoch 39/40
 - 0s - loss: 0.0812 - acc: 0.9763
Epoch 40/40
 - 0s - loss: 0.0772 - acc: 0.9759
# Training time = 0:03:45.980406
# F-Score(Ordinary) = 0.125, Recall: 0.909, Precision: 0.067
# F-Score(ireflv) = 0.288, Recall: 0.875, Precision: 0.172
# F-Score(id) = 0.089, Recall: 1.0, Precision: 0.047
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_73 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_74 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_73 (Embedding)        (None, 4, 48)        705264      input_73[0][0]                   
__________________________________________________________________________________________________
embedding_74 (Embedding)        (None, 4, 24)        5640        input_74[0][0]                   
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 192)          0           embedding_73[0][0]               
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 96)           0           embedding_74[0][0]               
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 288)          0           flatten_73[0][0]                 
                                                                 flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_73 (Dense)                (None, 24)           6936        concatenate_37[0][0]             
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 24)           0           dense_73[0][0]                   
__________________________________________________________________________________________________
dense_74 (Dense)                (None, 8)            200         dropout_37[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1051 - acc: 0.9613 - val_loss: 0.0763 - val_acc: 0.9673
Epoch 2/40
 - 4s - loss: 0.0726 - acc: 0.9680 - val_loss: 0.0789 - val_acc: 0.9670
Epoch 3/40
 - 4s - loss: 0.0702 - acc: 0.9691 - val_loss: 0.0829 - val_acc: 0.9671
Epoch 4/40
 - 4s - loss: 0.0688 - acc: 0.9701 - val_loss: 0.0802 - val_acc: 0.9679
Epoch 5/40
 - 4s - loss: 0.0687 - acc: 0.9709 - val_loss: 0.0899 - val_acc: 0.9676
Epoch 6/40
 - 4s - loss: 0.0689 - acc: 0.9716 - val_loss: 0.0896 - val_acc: 0.9671
Epoch 7/40
 - 4s - loss: 0.0685 - acc: 0.9718 - val_loss: 0.0895 - val_acc: 0.9678
Epoch 8/40
 - 4s - loss: 0.0677 - acc: 0.9721 - val_loss: 0.0983 - val_acc: 0.9661
Epoch 9/40
 - 4s - loss: 0.0686 - acc: 0.9726 - val_loss: 0.0958 - val_acc: 0.9675
Epoch 10/40
 - 4s - loss: 0.0708 - acc: 0.9729 - val_loss: 0.1175 - val_acc: 0.9672
Epoch 11/40
 - 4s - loss: 0.0702 - acc: 0.9725 - val_loss: 0.1216 - val_acc: 0.9656
Epoch 12/40
 - 4s - loss: 0.0705 - acc: 0.9729 - val_loss: 0.1030 - val_acc: 0.9674
Epoch 13/40
 - 4s - loss: 0.0688 - acc: 0.9729 - val_loss: 0.1215 - val_acc: 0.9635
Epoch 14/40
 - 4s - loss: 0.0703 - acc: 0.9734 - val_loss: 0.1292 - val_acc: 0.9680
Epoch 15/40
 - 4s - loss: 0.0703 - acc: 0.9736 - val_loss: 0.1220 - val_acc: 0.9669
Epoch 16/40
 - 4s - loss: 0.0726 - acc: 0.9735 - val_loss: 0.1174 - val_acc: 0.9679
Epoch 17/40
 - 4s - loss: 0.0711 - acc: 0.9735 - val_loss: 0.1165 - val_acc: 0.9672
Epoch 18/40
 - 4s - loss: 0.0716 - acc: 0.9740 - val_loss: 0.1264 - val_acc: 0.9686
Epoch 19/40
 - 4s - loss: 0.0729 - acc: 0.9743 - val_loss: 0.1250 - val_acc: 0.9682
Epoch 20/40
 - 4s - loss: 0.0744 - acc: 0.9741 - val_loss: 0.1450 - val_acc: 0.9676
Epoch 21/40
 - 4s - loss: 0.0718 - acc: 0.9745 - val_loss: 0.1305 - val_acc: 0.9675
Epoch 22/40
 - 4s - loss: 0.0745 - acc: 0.9745 - val_loss: 0.1435 - val_acc: 0.9673
Epoch 23/40
 - 4s - loss: 0.0739 - acc: 0.9743 - val_loss: 0.1318 - val_acc: 0.9666
Epoch 24/40
 - 4s - loss: 0.0738 - acc: 0.9744 - val_loss: 0.1390 - val_acc: 0.9681
Epoch 25/40
 - 4s - loss: 0.0759 - acc: 0.9746 - val_loss: 0.1439 - val_acc: 0.9672
Epoch 26/40
 - 4s - loss: 0.0727 - acc: 0.9744 - val_loss: 0.1469 - val_acc: 0.9679
Epoch 27/40
 - 4s - loss: 0.0739 - acc: 0.9746 - val_loss: 0.1404 - val_acc: 0.9680
Epoch 28/40
 - 4s - loss: 0.0737 - acc: 0.9748 - val_loss: 0.1453 - val_acc: 0.9663
Epoch 29/40
 - 4s - loss: 0.0750 - acc: 0.9752 - val_loss: 0.1531 - val_acc: 0.9676
Epoch 30/40
 - 4s - loss: 0.0720 - acc: 0.9751 - val_loss: 0.1383 - val_acc: 0.9679
Epoch 31/40
 - 4s - loss: 0.0738 - acc: 0.9755 - val_loss: 0.1748 - val_acc: 0.9674
Epoch 32/40
 - 4s - loss: 0.0765 - acc: 0.9749 - val_loss: 0.1589 - val_acc: 0.9678
Epoch 33/40
 - 4s - loss: 0.0749 - acc: 0.9750 - val_loss: 0.1464 - val_acc: 0.9671
Epoch 34/40
 - 4s - loss: 0.0768 - acc: 0.9748 - val_loss: 0.1646 - val_acc: 0.9669
Epoch 35/40
 - 4s - loss: 0.0770 - acc: 0.9750 - val_loss: 0.1706 - val_acc: 0.9665
Epoch 36/40
 - 4s - loss: 0.0793 - acc: 0.9753 - val_loss: 0.1520 - val_acc: 0.9657
Epoch 37/40
 - 4s - loss: 0.0777 - acc: 0.9752 - val_loss: 0.1443 - val_acc: 0.9673
Epoch 38/40
 - 4s - loss: 0.0798 - acc: 0.9751 - val_loss: 0.1598 - val_acc: 0.9677
Epoch 39/40
 - 4s - loss: 0.0768 - acc: 0.9753 - val_loss: 0.1930 - val_acc: 0.9672
Epoch 40/40
 - 4s - loss: 0.0774 - acc: 0.9749 - val_loss: 0.1497 - val_acc: 0.9671
Epoch 1/40
 - 0s - loss: 0.1528 - acc: 0.9664
Epoch 2/40
 - 0s - loss: 0.1347 - acc: 0.9663
Epoch 3/40
 - 0s - loss: 0.1311 - acc: 0.9688
Epoch 4/40
 - 0s - loss: 0.1144 - acc: 0.9689
Epoch 5/40
 - 0s - loss: 0.1215 - acc: 0.9704
Epoch 6/40
 - 0s - loss: 0.1111 - acc: 0.9703
Epoch 7/40
 - 0s - loss: 0.1139 - acc: 0.9699
Epoch 8/40
 - 0s - loss: 0.1107 - acc: 0.9711
Epoch 9/40
 - 0s - loss: 0.1135 - acc: 0.9716
Epoch 10/40
 - 0s - loss: 0.1061 - acc: 0.9726
Epoch 11/40
 - 0s - loss: 0.1049 - acc: 0.9723
Epoch 12/40
 - 0s - loss: 0.1053 - acc: 0.9724
Epoch 13/40
 - 0s - loss: 0.1071 - acc: 0.9723
Epoch 14/40
 - 0s - loss: 0.0977 - acc: 0.9732
Epoch 15/40
 - 0s - loss: 0.0962 - acc: 0.9731
Epoch 16/40
 - 0s - loss: 0.0990 - acc: 0.9738
Epoch 17/40
 - 0s - loss: 0.0990 - acc: 0.9733
Epoch 18/40
 - 0s - loss: 0.0984 - acc: 0.9740
Epoch 19/40
 - 0s - loss: 0.1029 - acc: 0.9740
Epoch 20/40
 - 0s - loss: 0.0994 - acc: 0.9737
Epoch 21/40
 - 0s - loss: 0.1000 - acc: 0.9740
Epoch 22/40
 - 0s - loss: 0.0947 - acc: 0.9740
Epoch 23/40
 - 0s - loss: 0.0934 - acc: 0.9747
Epoch 24/40
 - 0s - loss: 0.0994 - acc: 0.9749
Epoch 25/40
 - 0s - loss: 0.0984 - acc: 0.9750
Epoch 26/40
 - 0s - loss: 0.0878 - acc: 0.9757
Epoch 27/40
 - 0s - loss: 0.0899 - acc: 0.9751
Epoch 28/40
 - 0s - loss: 0.0866 - acc: 0.9757
Epoch 29/40
 - 0s - loss: 0.0902 - acc: 0.9751
Epoch 30/40
 - 0s - loss: 0.0812 - acc: 0.9761
Epoch 31/40
 - 0s - loss: 0.0890 - acc: 0.9762
Epoch 32/40
 - 0s - loss: 0.0887 - acc: 0.9757
Epoch 33/40
 - 0s - loss: 0.0846 - acc: 0.9767
Epoch 34/40
 - 0s - loss: 0.0819 - acc: 0.9763
Epoch 35/40
 - 0s - loss: 0.0894 - acc: 0.9761
Epoch 36/40
 - 0s - loss: 0.0830 - acc: 0.9769
Epoch 37/40
 - 0s - loss: 0.0904 - acc: 0.9758
Epoch 38/40
 - 0s - loss: 0.0795 - acc: 0.9768
Epoch 39/40
 - 0s - loss: 0.0854 - acc: 0.9765
Epoch 40/40
 - 0s - loss: 0.0794 - acc: 0.9762
# Training time = 0:03:32.856361
# F-Score(Ordinary) = 0.403, Recall: 0.767, Precision: 0.273
# F-Score(ireflv) = 0.402, Recall: 0.597, Precision: 0.303
# F-Score(id) = 0.579, Recall: 0.866, Precision: 0.435
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_75 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_76 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_75 (Embedding)        (None, 4, 48)        705264      input_75[0][0]                   
__________________________________________________________________________________________________
embedding_76 (Embedding)        (None, 4, 24)        5640        input_76[0][0]                   
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 192)          0           embedding_75[0][0]               
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 96)           0           embedding_76[0][0]               
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 288)          0           flatten_75[0][0]                 
                                                                 flatten_76[0][0]                 
__________________________________________________________________________________________________
dense_75 (Dense)                (None, 24)           6936        concatenate_38[0][0]             
__________________________________________________________________________________________________
dropout_38 (Dropout)            (None, 24)           0           dense_75[0][0]                   
__________________________________________________________________________________________________
dense_76 (Dense)                (None, 8)            200         dropout_38[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1036 - acc: 0.9620 - val_loss: 0.0786 - val_acc: 0.9659
Epoch 2/40
 - 4s - loss: 0.0724 - acc: 0.9676 - val_loss: 0.0758 - val_acc: 0.9658
Epoch 3/40
 - 4s - loss: 0.0695 - acc: 0.9693 - val_loss: 0.0775 - val_acc: 0.9676
Epoch 4/40
 - 4s - loss: 0.0691 - acc: 0.9702 - val_loss: 0.0846 - val_acc: 0.9671
Epoch 5/40
 - 4s - loss: 0.0689 - acc: 0.9706 - val_loss: 0.0903 - val_acc: 0.9676
Epoch 6/40
 - 4s - loss: 0.0696 - acc: 0.9709 - val_loss: 0.0894 - val_acc: 0.9677
Epoch 7/40
 - 4s - loss: 0.0690 - acc: 0.9716 - val_loss: 0.0960 - val_acc: 0.9674
Epoch 8/40
 - 4s - loss: 0.0711 - acc: 0.9717 - val_loss: 0.1129 - val_acc: 0.9685
Epoch 9/40
 - 4s - loss: 0.0720 - acc: 0.9715 - val_loss: 0.1061 - val_acc: 0.9623
Epoch 10/40
 - 4s - loss: 0.0711 - acc: 0.9720 - val_loss: 0.1177 - val_acc: 0.9675
Epoch 11/40
 - 4s - loss: 0.0720 - acc: 0.9719 - val_loss: 0.1048 - val_acc: 0.9688
Epoch 12/40
 - 4s - loss: 0.0713 - acc: 0.9723 - val_loss: 0.1091 - val_acc: 0.9687
Epoch 13/40
 - 4s - loss: 0.0721 - acc: 0.9725 - val_loss: 0.1046 - val_acc: 0.9669
Epoch 14/40
 - 4s - loss: 0.0721 - acc: 0.9730 - val_loss: 0.1127 - val_acc: 0.9679
Epoch 15/40
 - 4s - loss: 0.0712 - acc: 0.9726 - val_loss: 0.1169 - val_acc: 0.9675
Epoch 16/40
 - 4s - loss: 0.0743 - acc: 0.9734 - val_loss: 0.1049 - val_acc: 0.9676
Epoch 17/40
 - 4s - loss: 0.0744 - acc: 0.9732 - val_loss: 0.1341 - val_acc: 0.9678
Epoch 18/40
 - 4s - loss: 0.0765 - acc: 0.9738 - val_loss: 0.1288 - val_acc: 0.9671
Epoch 19/40
 - 4s - loss: 0.0752 - acc: 0.9737 - val_loss: 0.1473 - val_acc: 0.9681
Epoch 20/40
 - 4s - loss: 0.0771 - acc: 0.9739 - val_loss: 0.1356 - val_acc: 0.9662
Epoch 21/40
 - 4s - loss: 0.0750 - acc: 0.9741 - val_loss: 0.1450 - val_acc: 0.9656
Epoch 22/40
 - 4s - loss: 0.0758 - acc: 0.9739 - val_loss: 0.1342 - val_acc: 0.9675
Epoch 23/40
 - 4s - loss: 0.0772 - acc: 0.9742 - val_loss: 0.1498 - val_acc: 0.9679
Epoch 24/40
 - 4s - loss: 0.0790 - acc: 0.9741 - val_loss: 0.1423 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0772 - acc: 0.9746 - val_loss: 0.1501 - val_acc: 0.9686
Epoch 26/40
 - 4s - loss: 0.0790 - acc: 0.9742 - val_loss: 0.1627 - val_acc: 0.9669
Epoch 27/40
 - 4s - loss: 0.0770 - acc: 0.9749 - val_loss: 0.1273 - val_acc: 0.9681
Epoch 28/40
 - 4s - loss: 0.0789 - acc: 0.9747 - val_loss: 0.1455 - val_acc: 0.9685
Epoch 29/40
 - 4s - loss: 0.0832 - acc: 0.9749 - val_loss: 0.1403 - val_acc: 0.9672
Epoch 30/40
 - 4s - loss: 0.0787 - acc: 0.9751 - val_loss: 0.1579 - val_acc: 0.9673
Epoch 31/40
 - 4s - loss: 0.0807 - acc: 0.9749 - val_loss: 0.1544 - val_acc: 0.9685
Epoch 32/40
 - 4s - loss: 0.0781 - acc: 0.9753 - val_loss: 0.1496 - val_acc: 0.9679
Epoch 33/40
 - 4s - loss: 0.0798 - acc: 0.9752 - val_loss: 0.1585 - val_acc: 0.9677
Epoch 34/40
 - 4s - loss: 0.0790 - acc: 0.9750 - val_loss: 0.1635 - val_acc: 0.9673
Epoch 35/40
 - 4s - loss: 0.0784 - acc: 0.9754 - val_loss: 0.1592 - val_acc: 0.9677
Epoch 36/40
 - 4s - loss: 0.0777 - acc: 0.9753 - val_loss: 0.1649 - val_acc: 0.9684
Epoch 37/40
 - 4s - loss: 0.0772 - acc: 0.9754 - val_loss: 0.1632 - val_acc: 0.9681
Epoch 38/40
 - 4s - loss: 0.0775 - acc: 0.9751 - val_loss: 0.1718 - val_acc: 0.9686
Epoch 39/40
 - 4s - loss: 0.0764 - acc: 0.9756 - val_loss: 0.1558 - val_acc: 0.9682
Epoch 40/40
 - 4s - loss: 0.0800 - acc: 0.9754 - val_loss: 0.1570 - val_acc: 0.9681
Epoch 1/40
 - 0s - loss: 0.1533 - acc: 0.9656
Epoch 2/40
 - 0s - loss: 0.1417 - acc: 0.9668
Epoch 3/40
 - 0s - loss: 0.1324 - acc: 0.9685
Epoch 4/40
 - 0s - loss: 0.1353 - acc: 0.9683
Epoch 5/40
 - 0s - loss: 0.1275 - acc: 0.9679
Epoch 6/40
 - 0s - loss: 0.1184 - acc: 0.9699
Epoch 7/40
 - 0s - loss: 0.1099 - acc: 0.9707
Epoch 8/40
 - 0s - loss: 0.1069 - acc: 0.9706
Epoch 9/40
 - 0s - loss: 0.1121 - acc: 0.9717
Epoch 10/40
 - 0s - loss: 0.1129 - acc: 0.9720
Epoch 11/40
 - 0s - loss: 0.1103 - acc: 0.9721
Epoch 12/40
 - 0s - loss: 0.1006 - acc: 0.9723
Epoch 13/40
 - 0s - loss: 0.1030 - acc: 0.9730
Epoch 14/40
 - 0s - loss: 0.0984 - acc: 0.9742
Epoch 15/40
 - 0s - loss: 0.1053 - acc: 0.9733
Epoch 16/40
 - 0s - loss: 0.1040 - acc: 0.9737
Epoch 17/40
 - 0s - loss: 0.0969 - acc: 0.9732
Epoch 18/40
 - 0s - loss: 0.0904 - acc: 0.9742
Epoch 19/40
 - 0s - loss: 0.0883 - acc: 0.9742
Epoch 20/40
 - 0s - loss: 0.0901 - acc: 0.9746
Epoch 21/40
 - 0s - loss: 0.0948 - acc: 0.9750
Epoch 22/40
 - 0s - loss: 0.0928 - acc: 0.9753
Epoch 23/40
 - 0s - loss: 0.0898 - acc: 0.9750
Epoch 24/40
 - 0s - loss: 0.0911 - acc: 0.9749
Epoch 25/40
 - 0s - loss: 0.0892 - acc: 0.9752
Epoch 26/40
 - 0s - loss: 0.0899 - acc: 0.9756
Epoch 27/40
 - 0s - loss: 0.0814 - acc: 0.9753
Epoch 28/40
 - 0s - loss: 0.0874 - acc: 0.9770
Epoch 29/40
 - 0s - loss: 0.0856 - acc: 0.9745
Epoch 30/40
 - 0s - loss: 0.0808 - acc: 0.9759
Epoch 31/40
 - 0s - loss: 0.0807 - acc: 0.9761
Epoch 32/40
 - 0s - loss: 0.0939 - acc: 0.9756
Epoch 33/40
 - 0s - loss: 0.0773 - acc: 0.9767
Epoch 34/40
 - 0s - loss: 0.0815 - acc: 0.9760
Epoch 35/40
 - 0s - loss: 0.0879 - acc: 0.9767
Epoch 36/40
 - 0s - loss: 0.0786 - acc: 0.9760
Epoch 37/40
 - 0s - loss: 0.0778 - acc: 0.9768
Epoch 38/40
 - 0s - loss: 0.0744 - acc: 0.9766
Epoch 39/40
 - 0s - loss: 0.0974 - acc: 0.9761
Epoch 40/40
 - 0s - loss: 0.0764 - acc: 0.9759
# Training time = 0:03:36.066772
# F-Score(Ordinary) = 0.044, Recall: 0.909, Precision: 0.022
# F-Score(lvc) = 0.127, Recall: 0.9, Precision: 0.068
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_77 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_78 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_77 (Embedding)        (None, 4, 48)        705264      input_77[0][0]                   
__________________________________________________________________________________________________
embedding_78 (Embedding)        (None, 4, 24)        5640        input_78[0][0]                   
__________________________________________________________________________________________________
flatten_77 (Flatten)            (None, 192)          0           embedding_77[0][0]               
__________________________________________________________________________________________________
flatten_78 (Flatten)            (None, 96)           0           embedding_78[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 288)          0           flatten_77[0][0]                 
                                                                 flatten_78[0][0]                 
__________________________________________________________________________________________________
dense_77 (Dense)                (None, 24)           6936        concatenate_39[0][0]             
__________________________________________________________________________________________________
dropout_39 (Dropout)            (None, 24)           0           dense_77[0][0]                   
__________________________________________________________________________________________________
dense_78 (Dense)                (None, 8)            200         dropout_39[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1071 - acc: 0.9603 - val_loss: 0.0744 - val_acc: 0.9656
Epoch 2/40
 - 4s - loss: 0.0720 - acc: 0.9677 - val_loss: 0.0783 - val_acc: 0.9653
Epoch 3/40
 - 4s - loss: 0.0697 - acc: 0.9692 - val_loss: 0.0820 - val_acc: 0.9672
Epoch 4/40
 - 4s - loss: 0.0697 - acc: 0.9693 - val_loss: 0.0863 - val_acc: 0.9671
Epoch 5/40
 - 4s - loss: 0.0699 - acc: 0.9709 - val_loss: 0.0878 - val_acc: 0.9671
Epoch 6/40
 - 4s - loss: 0.0709 - acc: 0.9708 - val_loss: 0.0892 - val_acc: 0.9678
Epoch 7/40
 - 4s - loss: 0.0714 - acc: 0.9718 - val_loss: 0.1003 - val_acc: 0.9674
Epoch 8/40
 - 4s - loss: 0.0703 - acc: 0.9716 - val_loss: 0.1103 - val_acc: 0.9654
Epoch 9/40
 - 4s - loss: 0.0714 - acc: 0.9723 - val_loss: 0.0979 - val_acc: 0.9674
Epoch 10/40
 - 4s - loss: 0.0718 - acc: 0.9727 - val_loss: 0.1271 - val_acc: 0.9681
Epoch 11/40
 - 4s - loss: 0.0735 - acc: 0.9721 - val_loss: 0.1138 - val_acc: 0.9668
Epoch 12/40
 - 4s - loss: 0.0725 - acc: 0.9728 - val_loss: 0.1027 - val_acc: 0.9675
Epoch 13/40
 - 4s - loss: 0.0740 - acc: 0.9729 - val_loss: 0.1188 - val_acc: 0.9671
Epoch 14/40
 - 4s - loss: 0.0751 - acc: 0.9733 - val_loss: 0.1183 - val_acc: 0.9672
Epoch 15/40
 - 4s - loss: 0.0731 - acc: 0.9732 - val_loss: 0.1503 - val_acc: 0.9667
Epoch 16/40
 - 4s - loss: 0.0748 - acc: 0.9735 - val_loss: 0.1273 - val_acc: 0.9684
Epoch 17/40
 - 4s - loss: 0.0752 - acc: 0.9735 - val_loss: 0.1348 - val_acc: 0.9683
Epoch 18/40
 - 4s - loss: 0.0743 - acc: 0.9741 - val_loss: 0.1503 - val_acc: 0.9674
Epoch 19/40
 - 4s - loss: 0.0758 - acc: 0.9740 - val_loss: 0.1513 - val_acc: 0.9648
Epoch 20/40
 - 4s - loss: 0.0740 - acc: 0.9741 - val_loss: 0.1339 - val_acc: 0.9660
Epoch 21/40
 - 4s - loss: 0.0760 - acc: 0.9746 - val_loss: 0.1432 - val_acc: 0.9678
Epoch 22/40
 - 4s - loss: 0.0761 - acc: 0.9740 - val_loss: 0.1509 - val_acc: 0.9656
Epoch 23/40
 - 4s - loss: 0.0761 - acc: 0.9739 - val_loss: 0.1497 - val_acc: 0.9678
Epoch 24/40
 - 4s - loss: 0.0781 - acc: 0.9746 - val_loss: 0.1361 - val_acc: 0.9670
Epoch 25/40
 - 4s - loss: 0.0764 - acc: 0.9745 - val_loss: 0.1585 - val_acc: 0.9671
Epoch 26/40
 - 4s - loss: 0.0821 - acc: 0.9748 - val_loss: 0.1639 - val_acc: 0.9669
Epoch 27/40
 - 4s - loss: 0.0799 - acc: 0.9748 - val_loss: 0.1586 - val_acc: 0.9669
Epoch 28/40
 - 4s - loss: 0.0788 - acc: 0.9744 - val_loss: 0.1702 - val_acc: 0.9672
Epoch 29/40
 - 4s - loss: 0.0808 - acc: 0.9744 - val_loss: 0.1625 - val_acc: 0.9667
Epoch 30/40
 - 4s - loss: 0.0820 - acc: 0.9744 - val_loss: 0.1503 - val_acc: 0.9667
Epoch 31/40
 - 4s - loss: 0.0836 - acc: 0.9749 - val_loss: 0.1797 - val_acc: 0.9667
Epoch 32/40
 - 4s - loss: 0.0802 - acc: 0.9749 - val_loss: 0.1559 - val_acc: 0.9674
Epoch 33/40
 - 4s - loss: 0.0813 - acc: 0.9750 - val_loss: 0.1747 - val_acc: 0.9673
Epoch 34/40
 - 4s - loss: 0.0843 - acc: 0.9748 - val_loss: 0.1594 - val_acc: 0.9669
Epoch 35/40
 - 4s - loss: 0.0845 - acc: 0.9749 - val_loss: 0.1590 - val_acc: 0.9665
Epoch 36/40
 - 4s - loss: 0.0823 - acc: 0.9752 - val_loss: 0.1987 - val_acc: 0.9669
Epoch 37/40
 - 4s - loss: 0.0829 - acc: 0.9753 - val_loss: 0.1572 - val_acc: 0.9673
Epoch 38/40
 - 4s - loss: 0.0805 - acc: 0.9751 - val_loss: 0.1749 - val_acc: 0.9678
Epoch 39/40
 - 4s - loss: 0.0835 - acc: 0.9750 - val_loss: 0.1683 - val_acc: 0.9675
Epoch 40/40
 - 4s - loss: 0.0841 - acc: 0.9754 - val_loss: 0.1687 - val_acc: 0.9672
Epoch 1/40
 - 0s - loss: 0.1646 - acc: 0.9649
Epoch 2/40
 - 0s - loss: 0.1637 - acc: 0.9670
Epoch 3/40
 - 0s - loss: 0.1554 - acc: 0.9672
Epoch 4/40
 - 0s - loss: 0.1424 - acc: 0.9692
Epoch 5/40
 - 0s - loss: 0.1321 - acc: 0.9688
Epoch 6/40
 - 0s - loss: 0.1222 - acc: 0.9686
Epoch 7/40
 - 0s - loss: 0.1144 - acc: 0.9697
Epoch 8/40
 - 0s - loss: 0.1076 - acc: 0.9697
Epoch 9/40
 - 0s - loss: 0.1127 - acc: 0.9723
Epoch 10/40
 - 0s - loss: 0.1163 - acc: 0.9723
Epoch 11/40
 - 0s - loss: 0.1048 - acc: 0.9719
Epoch 12/40
 - 0s - loss: 0.1056 - acc: 0.9731
Epoch 13/40
 - 0s - loss: 0.1062 - acc: 0.9737
Epoch 14/40
 - 0s - loss: 0.1047 - acc: 0.9733
Epoch 15/40
 - 0s - loss: 0.1116 - acc: 0.9734
Epoch 16/40
 - 0s - loss: 0.0929 - acc: 0.9752
Epoch 17/40
 - 0s - loss: 0.0897 - acc: 0.9739
Epoch 18/40
 - 0s - loss: 0.0928 - acc: 0.9750
Epoch 19/40
 - 0s - loss: 0.0921 - acc: 0.9740
Epoch 20/40
 - 0s - loss: 0.0909 - acc: 0.9740
Epoch 21/40
 - 0s - loss: 0.0916 - acc: 0.9751
Epoch 22/40
 - 0s - loss: 0.0946 - acc: 0.9748
Epoch 23/40
 - 0s - loss: 0.0958 - acc: 0.9747
Epoch 24/40
 - 0s - loss: 0.0936 - acc: 0.9755
Epoch 25/40
 - 0s - loss: 0.0956 - acc: 0.9753
Epoch 26/40
 - 0s - loss: 0.0869 - acc: 0.9750
Epoch 27/40
 - 0s - loss: 0.0870 - acc: 0.9756
Epoch 28/40
 - 0s - loss: 0.0913 - acc: 0.9756
Epoch 29/40
 - 0s - loss: 0.0853 - acc: 0.9758
Epoch 30/40
 - 0s - loss: 0.0803 - acc: 0.9752
Epoch 31/40
 - 0s - loss: 0.0877 - acc: 0.9765
Epoch 32/40
 - 0s - loss: 0.0792 - acc: 0.9760
Epoch 33/40
 - 0s - loss: 0.0775 - acc: 0.9761
Epoch 34/40
 - 0s - loss: 0.0921 - acc: 0.9759
Epoch 35/40
 - 0s - loss: 0.0803 - acc: 0.9757
Epoch 36/40
 - 0s - loss: 0.0852 - acc: 0.9756
Epoch 37/40
 - 0s - loss: 0.0820 - acc: 0.9755
Epoch 38/40
 - 0s - loss: 0.0853 - acc: 0.9762
Epoch 39/40
 - 0s - loss: 0.0844 - acc: 0.9756
Epoch 40/40
 - 0s - loss: 0.0892 - acc: 0.9759
# Training time = 0:03:34.413256
# F-Score(Ordinary) = 0.117, Recall: 0.933, Precision: 0.063
# F-Score(lvc) = 0.164, Recall: 0.857, Precision: 0.091
# F-Score(id) = 0.153, Recall: 1.0, Precision: 0.083
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_79 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_80 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_79 (Embedding)        (None, 4, 48)        705264      input_79[0][0]                   
__________________________________________________________________________________________________
embedding_80 (Embedding)        (None, 4, 24)        5640        input_80[0][0]                   
__________________________________________________________________________________________________
flatten_79 (Flatten)            (None, 192)          0           embedding_79[0][0]               
__________________________________________________________________________________________________
flatten_80 (Flatten)            (None, 96)           0           embedding_80[0][0]               
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 288)          0           flatten_79[0][0]                 
                                                                 flatten_80[0][0]                 
__________________________________________________________________________________________________
dense_79 (Dense)                (None, 24)           6936        concatenate_40[0][0]             
__________________________________________________________________________________________________
dropout_40 (Dropout)            (None, 24)           0           dense_79[0][0]                   
__________________________________________________________________________________________________
dense_80 (Dense)                (None, 8)            200         dropout_40[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1072 - acc: 0.9598 - val_loss: 0.0772 - val_acc: 0.9663
Epoch 2/40
 - 4s - loss: 0.0719 - acc: 0.9679 - val_loss: 0.0762 - val_acc: 0.9668
Epoch 3/40
 - 4s - loss: 0.0686 - acc: 0.9693 - val_loss: 0.0739 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0675 - acc: 0.9700 - val_loss: 0.0800 - val_acc: 0.9671
Epoch 5/40
 - 4s - loss: 0.0668 - acc: 0.9712 - val_loss: 0.0915 - val_acc: 0.9674
Epoch 6/40
 - 4s - loss: 0.0667 - acc: 0.9710 - val_loss: 0.0865 - val_acc: 0.9677
Epoch 7/40
 - 4s - loss: 0.0674 - acc: 0.9717 - val_loss: 0.0954 - val_acc: 0.9671
Epoch 8/40
 - 4s - loss: 0.0664 - acc: 0.9722 - val_loss: 0.0991 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0654 - acc: 0.9727 - val_loss: 0.0882 - val_acc: 0.9677
Epoch 10/40
 - 4s - loss: 0.0663 - acc: 0.9727 - val_loss: 0.1125 - val_acc: 0.9674
Epoch 11/40
 - 4s - loss: 0.0671 - acc: 0.9732 - val_loss: 0.1072 - val_acc: 0.9676
Epoch 12/40
 - 4s - loss: 0.0676 - acc: 0.9733 - val_loss: 0.1006 - val_acc: 0.9678
Epoch 13/40
 - 4s - loss: 0.0674 - acc: 0.9741 - val_loss: 0.1232 - val_acc: 0.9672
Epoch 14/40
 - 4s - loss: 0.0683 - acc: 0.9740 - val_loss: 0.1202 - val_acc: 0.9681
Epoch 15/40
 - 4s - loss: 0.0689 - acc: 0.9737 - val_loss: 0.1145 - val_acc: 0.9672
Epoch 16/40
 - 4s - loss: 0.0678 - acc: 0.9737 - val_loss: 0.1148 - val_acc: 0.9652
Epoch 17/40
 - 4s - loss: 0.0680 - acc: 0.9739 - val_loss: 0.1185 - val_acc: 0.9668
Epoch 18/40
 - 4s - loss: 0.0684 - acc: 0.9740 - val_loss: 0.1341 - val_acc: 0.9674
Epoch 19/40
 - 4s - loss: 0.0688 - acc: 0.9745 - val_loss: 0.1299 - val_acc: 0.9671
Epoch 20/40
 - 4s - loss: 0.0699 - acc: 0.9742 - val_loss: 0.1192 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0693 - acc: 0.9746 - val_loss: 0.1440 - val_acc: 0.9663
Epoch 22/40
 - 4s - loss: 0.0690 - acc: 0.9747 - val_loss: 0.1328 - val_acc: 0.9677
Epoch 23/40
 - 4s - loss: 0.0684 - acc: 0.9745 - val_loss: 0.1308 - val_acc: 0.9673
Epoch 24/40
 - 4s - loss: 0.0699 - acc: 0.9751 - val_loss: 0.1357 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0699 - acc: 0.9755 - val_loss: 0.1383 - val_acc: 0.9687
Epoch 26/40
 - 4s - loss: 0.0701 - acc: 0.9750 - val_loss: 0.1408 - val_acc: 0.9683
Epoch 27/40
 - 4s - loss: 0.0694 - acc: 0.9754 - val_loss: 0.1329 - val_acc: 0.9680
Epoch 28/40
 - 4s - loss: 0.0699 - acc: 0.9751 - val_loss: 0.1341 - val_acc: 0.9686
Epoch 29/40
 - 4s - loss: 0.0720 - acc: 0.9756 - val_loss: 0.1571 - val_acc: 0.9679
Epoch 30/40
 - 4s - loss: 0.0724 - acc: 0.9750 - val_loss: 0.1470 - val_acc: 0.9667
Epoch 31/40
 - 4s - loss: 0.0734 - acc: 0.9753 - val_loss: 0.1461 - val_acc: 0.9675
Epoch 32/40
 - 4s - loss: 0.0741 - acc: 0.9754 - val_loss: 0.1559 - val_acc: 0.9676
Epoch 33/40
 - 4s - loss: 0.0720 - acc: 0.9748 - val_loss: 0.1677 - val_acc: 0.9678
Epoch 34/40
 - 4s - loss: 0.0763 - acc: 0.9755 - val_loss: 0.1623 - val_acc: 0.9674
Epoch 35/40
 - 4s - loss: 0.0739 - acc: 0.9752 - val_loss: 0.1623 - val_acc: 0.9678
Epoch 36/40
 - 4s - loss: 0.0753 - acc: 0.9749 - val_loss: 0.1616 - val_acc: 0.9657
Epoch 37/40
 - 4s - loss: 0.0752 - acc: 0.9755 - val_loss: 0.1456 - val_acc: 0.9680
Epoch 38/40
 - 4s - loss: 0.0719 - acc: 0.9758 - val_loss: 0.1570 - val_acc: 0.9671
Epoch 39/40
 - 4s - loss: 0.0721 - acc: 0.9760 - val_loss: 0.1615 - val_acc: 0.9682
Epoch 40/40
 - 4s - loss: 0.0752 - acc: 0.9760 - val_loss: 0.1690 - val_acc: 0.9675
Epoch 1/40
 - 0s - loss: 0.1610 - acc: 0.9663
Epoch 2/40
 - 0s - loss: 0.1429 - acc: 0.9682
Epoch 3/40
 - 0s - loss: 0.1314 - acc: 0.9689
Epoch 4/40
 - 0s - loss: 0.1279 - acc: 0.9695
Epoch 5/40
 - 0s - loss: 0.1326 - acc: 0.9701
Epoch 6/40
 - 0s - loss: 0.1184 - acc: 0.9700
Epoch 7/40
 - 0s - loss: 0.1163 - acc: 0.9719
Epoch 8/40
 - 0s - loss: 0.1148 - acc: 0.9718
Epoch 9/40
 - 0s - loss: 0.1089 - acc: 0.9721
Epoch 10/40
 - 0s - loss: 0.1062 - acc: 0.9720
Epoch 11/40
 - 0s - loss: 0.1043 - acc: 0.9724
Epoch 12/40
 - 0s - loss: 0.1062 - acc: 0.9739
Epoch 13/40
 - 0s - loss: 0.0972 - acc: 0.9738
Epoch 14/40
 - 0s - loss: 0.0961 - acc: 0.9738
Epoch 15/40
 - 0s - loss: 0.1021 - acc: 0.9740
Epoch 16/40
 - 0s - loss: 0.0986 - acc: 0.9744
Epoch 17/40
 - 0s - loss: 0.0940 - acc: 0.9740
Epoch 18/40
 - 0s - loss: 0.0999 - acc: 0.9752
Epoch 19/40
 - 0s - loss: 0.0906 - acc: 0.9754
Epoch 20/40
 - 0s - loss: 0.0893 - acc: 0.9761
Epoch 21/40
 - 0s - loss: 0.0928 - acc: 0.9748
Epoch 22/40
 - 0s - loss: 0.0942 - acc: 0.9757
Epoch 23/40
 - 0s - loss: 0.0907 - acc: 0.9748
Epoch 24/40
 - 0s - loss: 0.0869 - acc: 0.9754
Epoch 25/40
 - 0s - loss: 0.0889 - acc: 0.9752
Epoch 26/40
 - 0s - loss: 0.0898 - acc: 0.9761
Epoch 27/40
 - 0s - loss: 0.0973 - acc: 0.9755
Epoch 28/40
 - 0s - loss: 0.0863 - acc: 0.9759
Epoch 29/40
 - 0s - loss: 0.0948 - acc: 0.9763
Epoch 30/40
 - 0s - loss: 0.0892 - acc: 0.9754
Epoch 31/40
 - 0s - loss: 0.0853 - acc: 0.9768
Epoch 32/40
 - 0s - loss: 0.0836 - acc: 0.9770
Epoch 33/40
 - 0s - loss: 0.0843 - acc: 0.9754
Epoch 34/40
 - 0s - loss: 0.0803 - acc: 0.9760
Epoch 35/40
 - 0s - loss: 0.0837 - acc: 0.9762
Epoch 36/40
 - 0s - loss: 0.0865 - acc: 0.9758
Epoch 37/40
 - 0s - loss: 0.0896 - acc: 0.9763
Epoch 38/40
 - 0s - loss: 0.0826 - acc: 0.9771
Epoch 39/40
 - 0s - loss: 0.0889 - acc: 0.9763
Epoch 40/40
 - 0s - loss: 0.0898 - acc: 0.9766
# Training time = 0:03:34.553574
# F-Score(Ordinary) = 0.35, Recall: 0.619, Precision: 0.244
# F-Score(lvc) = 0.316, Recall: 0.484, Precision: 0.235
# F-Score(ireflv) = 0.641, Recall: 0.679, Precision: 0.607
# F-Score(id) = 0.031, Recall: 1.0, Precision: 0.016
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_81 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_82 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_81 (Embedding)        (None, 4, 48)        705264      input_81[0][0]                   
__________________________________________________________________________________________________
embedding_82 (Embedding)        (None, 4, 24)        5640        input_82[0][0]                   
__________________________________________________________________________________________________
flatten_81 (Flatten)            (None, 192)          0           embedding_81[0][0]               
__________________________________________________________________________________________________
flatten_82 (Flatten)            (None, 96)           0           embedding_82[0][0]               
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 288)          0           flatten_81[0][0]                 
                                                                 flatten_82[0][0]                 
__________________________________________________________________________________________________
dense_81 (Dense)                (None, 24)           6936        concatenate_41[0][0]             
__________________________________________________________________________________________________
dropout_41 (Dropout)            (None, 24)           0           dense_81[0][0]                   
__________________________________________________________________________________________________
dense_82 (Dense)                (None, 8)            200         dropout_41[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0978 - acc: 0.9624 - val_loss: 0.0864 - val_acc: 0.9653
Epoch 2/40
 - 4s - loss: 0.0868 - acc: 0.9668 - val_loss: 0.0879 - val_acc: 0.9657
Epoch 3/40
 - 4s - loss: 0.0901 - acc: 0.9679 - val_loss: 0.1045 - val_acc: 0.9638
Epoch 4/40
 - 4s - loss: 0.0917 - acc: 0.9691 - val_loss: 0.1231 - val_acc: 0.9665
Epoch 5/40
 - 4s - loss: 0.0942 - acc: 0.9694 - val_loss: 0.1384 - val_acc: 0.9656
Epoch 6/40
 - 4s - loss: 0.0933 - acc: 0.9701 - val_loss: 0.1200 - val_acc: 0.9665
Epoch 7/40
 - 4s - loss: 0.0957 - acc: 0.9707 - val_loss: 0.1427 - val_acc: 0.9645
Epoch 8/40
 - 4s - loss: 0.0972 - acc: 0.9715 - val_loss: 0.1412 - val_acc: 0.9663
Epoch 9/40
 - 4s - loss: 0.0960 - acc: 0.9718 - val_loss: 0.1531 - val_acc: 0.9663
Epoch 10/40
 - 4s - loss: 0.0983 - acc: 0.9722 - val_loss: 0.1477 - val_acc: 0.9667
Epoch 11/40
 - 4s - loss: 0.0981 - acc: 0.9726 - val_loss: 0.1677 - val_acc: 0.9648
Epoch 12/40
 - 4s - loss: 0.1009 - acc: 0.9727 - val_loss: 0.1538 - val_acc: 0.9669
Epoch 13/40
 - 4s - loss: 0.1035 - acc: 0.9726 - val_loss: 0.1910 - val_acc: 0.9655
Epoch 14/40
 - 4s - loss: 0.1009 - acc: 0.9729 - val_loss: 0.1533 - val_acc: 0.9663
Epoch 15/40
 - 4s - loss: 0.0979 - acc: 0.9731 - val_loss: 0.1490 - val_acc: 0.9678
Epoch 16/40
 - 4s - loss: 0.0997 - acc: 0.9734 - val_loss: 0.1529 - val_acc: 0.9668
Epoch 17/40
 - 4s - loss: 0.1011 - acc: 0.9734 - val_loss: 0.1699 - val_acc: 0.9671
Epoch 18/40
 - 4s - loss: 0.1045 - acc: 0.9739 - val_loss: 0.1893 - val_acc: 0.9675
Epoch 19/40
 - 4s - loss: 0.1066 - acc: 0.9737 - val_loss: 0.1750 - val_acc: 0.9666
Epoch 20/40
 - 4s - loss: 0.1089 - acc: 0.9740 - val_loss: 0.1970 - val_acc: 0.9671
Epoch 21/40
 - 4s - loss: 0.1112 - acc: 0.9740 - val_loss: 0.1960 - val_acc: 0.9669
Epoch 22/40
 - 4s - loss: 0.1102 - acc: 0.9741 - val_loss: 0.1693 - val_acc: 0.9668
Epoch 23/40
 - 4s - loss: 0.1122 - acc: 0.9745 - val_loss: 0.2023 - val_acc: 0.9679
Epoch 24/40
 - 4s - loss: 0.1162 - acc: 0.9743 - val_loss: 0.1847 - val_acc: 0.9668
Epoch 25/40
 - 4s - loss: 0.1189 - acc: 0.9748 - val_loss: 0.1953 - val_acc: 0.9668
Epoch 26/40
 - 4s - loss: 0.1127 - acc: 0.9748 - val_loss: 0.1989 - val_acc: 0.9658
Epoch 27/40
 - 4s - loss: 0.1175 - acc: 0.9746 - val_loss: 0.2064 - val_acc: 0.9671
Epoch 28/40
 - 4s - loss: 0.1164 - acc: 0.9747 - val_loss: 0.2076 - val_acc: 0.9679
Epoch 29/40
 - 4s - loss: 0.1169 - acc: 0.9746 - val_loss: 0.2087 - val_acc: 0.9666
Epoch 30/40
 - 4s - loss: 0.1185 - acc: 0.9748 - val_loss: 0.2301 - val_acc: 0.9670
Epoch 31/40
 - 4s - loss: 0.1173 - acc: 0.9749 - val_loss: 0.2012 - val_acc: 0.9670
Epoch 32/40
 - 4s - loss: 0.1165 - acc: 0.9754 - val_loss: 0.2153 - val_acc: 0.9676
Epoch 33/40
 - 4s - loss: 0.1218 - acc: 0.9748 - val_loss: 0.2191 - val_acc: 0.9677
Epoch 34/40
 - 4s - loss: 0.1250 - acc: 0.9745 - val_loss: 0.2223 - val_acc: 0.9675
Epoch 35/40
 - 4s - loss: 0.1261 - acc: 0.9749 - val_loss: 0.2645 - val_acc: 0.9667
Epoch 36/40
 - 4s - loss: 0.1365 - acc: 0.9752 - val_loss: 0.2380 - val_acc: 0.9679
Epoch 37/40
 - 4s - loss: 0.1463 - acc: 0.9750 - val_loss: 0.2367 - val_acc: 0.9673
Epoch 38/40
 - 4s - loss: 0.1414 - acc: 0.9749 - val_loss: 0.2564 - val_acc: 0.9671
Epoch 39/40
 - 4s - loss: 0.1471 - acc: 0.9749 - val_loss: 0.2518 - val_acc: 0.9675
Epoch 40/40
 - 4s - loss: 0.1432 - acc: 0.9751 - val_loss: 0.2394 - val_acc: 0.9674
Epoch 1/40
 - 0s - loss: 0.2311 - acc: 0.9645
Epoch 2/40
 - 0s - loss: 0.2397 - acc: 0.9651
Epoch 3/40
 - 0s - loss: 0.2421 - acc: 0.9663
Epoch 4/40
 - 0s - loss: 0.2241 - acc: 0.9671
Epoch 5/40
 - 0s - loss: 0.2082 - acc: 0.9685
Epoch 6/40
 - 0s - loss: 0.2067 - acc: 0.9683
Epoch 7/40
 - 0s - loss: 0.2112 - acc: 0.9689
Epoch 8/40
 - 0s - loss: 0.1972 - acc: 0.9687
Epoch 9/40
 - 0s - loss: 0.2006 - acc: 0.9695
Epoch 10/40
 - 0s - loss: 0.2026 - acc: 0.9694
Epoch 11/40
 - 0s - loss: 0.2030 - acc: 0.9698
Epoch 12/40
 - 0s - loss: 0.1855 - acc: 0.9712
Epoch 13/40
 - 0s - loss: 0.1946 - acc: 0.9712
Epoch 14/40
 - 0s - loss: 0.2003 - acc: 0.9709
Epoch 15/40
 - 0s - loss: 0.1849 - acc: 0.9707
Epoch 16/40
 - 0s - loss: 0.1833 - acc: 0.9717
Epoch 17/40
 - 0s - loss: 0.1970 - acc: 0.9714
Epoch 18/40
 - 0s - loss: 0.1874 - acc: 0.9713
Epoch 19/40
 - 0s - loss: 0.1823 - acc: 0.9719
Epoch 20/40
 - 0s - loss: 0.1885 - acc: 0.9728
Epoch 21/40
 - 0s - loss: 0.1880 - acc: 0.9721
Epoch 22/40
 - 0s - loss: 0.1829 - acc: 0.9741
Epoch 23/40
 - 0s - loss: 0.1830 - acc: 0.9723
Epoch 24/40
 - 0s - loss: 0.1871 - acc: 0.9715
Epoch 25/40
 - 0s - loss: 0.1753 - acc: 0.9722
Epoch 26/40
 - 0s - loss: 0.1860 - acc: 0.9725
Epoch 27/40
 - 0s - loss: 0.1776 - acc: 0.9728
Epoch 28/40
 - 0s - loss: 0.1790 - acc: 0.9726
Epoch 29/40
 - 0s - loss: 0.1803 - acc: 0.9734
Epoch 30/40
 - 0s - loss: 0.1807 - acc: 0.9733
Epoch 31/40
 - 0s - loss: 0.1707 - acc: 0.9735
Epoch 32/40
 - 0s - loss: 0.1701 - acc: 0.9741
Epoch 33/40
 - 0s - loss: 0.1653 - acc: 0.9735
Epoch 34/40
 - 0s - loss: 0.1786 - acc: 0.9737
Epoch 35/40
 - 0s - loss: 0.1838 - acc: 0.9730
Epoch 36/40
 - 0s - loss: 0.1660 - acc: 0.9738
Epoch 37/40
 - 0s - loss: 0.1595 - acc: 0.9742
Epoch 38/40
 - 0s - loss: 0.1716 - acc: 0.9739
Epoch 39/40
 - 0s - loss: 0.1610 - acc: 0.9744
Epoch 40/40
 - 0s - loss: 0.1584 - acc: 0.9745
# Training time = 0:03:34.649225
# F-Score(Ordinary) = 0.337, Recall: 0.731, Precision: 0.219
# F-Score(ireflv) = 0.75, Recall: 0.716, Precision: 0.787
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_83 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_84 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_83 (Embedding)        (None, 4, 48)        705264      input_83[0][0]                   
__________________________________________________________________________________________________
embedding_84 (Embedding)        (None, 4, 24)        5640        input_84[0][0]                   
__________________________________________________________________________________________________
flatten_83 (Flatten)            (None, 192)          0           embedding_83[0][0]               
__________________________________________________________________________________________________
flatten_84 (Flatten)            (None, 96)           0           embedding_84[0][0]               
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 288)          0           flatten_83[0][0]                 
                                                                 flatten_84[0][0]                 
__________________________________________________________________________________________________
dense_83 (Dense)                (None, 24)           6936        concatenate_42[0][0]             
__________________________________________________________________________________________________
dropout_42 (Dropout)            (None, 24)           0           dense_83[0][0]                   
__________________________________________________________________________________________________
dense_84 (Dense)                (None, 8)            200         dropout_42[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0983 - acc: 0.9617 - val_loss: 0.0911 - val_acc: 0.9661
Epoch 2/40
 - 4s - loss: 0.0832 - acc: 0.9673 - val_loss: 0.0954 - val_acc: 0.9651
Epoch 3/40
 - 4s - loss: 0.0856 - acc: 0.9678 - val_loss: 0.0987 - val_acc: 0.9672
Epoch 4/40
 - 4s - loss: 0.0862 - acc: 0.9696 - val_loss: 0.1194 - val_acc: 0.9665
Epoch 5/40
 - 4s - loss: 0.0861 - acc: 0.9699 - val_loss: 0.1190 - val_acc: 0.9661
Epoch 6/40
 - 4s - loss: 0.0903 - acc: 0.9710 - val_loss: 0.1176 - val_acc: 0.9646
Epoch 7/40
 - 4s - loss: 0.0892 - acc: 0.9713 - val_loss: 0.1446 - val_acc: 0.9612
Epoch 8/40
 - 4s - loss: 0.0933 - acc: 0.9718 - val_loss: 0.1300 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0964 - acc: 0.9718 - val_loss: 0.1412 - val_acc: 0.9663
Epoch 10/40
 - 4s - loss: 0.0982 - acc: 0.9726 - val_loss: 0.1623 - val_acc: 0.9671
Epoch 11/40
 - 4s - loss: 0.0990 - acc: 0.9728 - val_loss: 0.1746 - val_acc: 0.9660
Epoch 12/40
 - 4s - loss: 0.0979 - acc: 0.9729 - val_loss: 0.1735 - val_acc: 0.9667
Epoch 13/40
 - 4s - loss: 0.1043 - acc: 0.9730 - val_loss: 0.1810 - val_acc: 0.9666
Epoch 14/40
 - 4s - loss: 0.1067 - acc: 0.9734 - val_loss: 0.2085 - val_acc: 0.9658
Epoch 15/40
 - 4s - loss: 0.1048 - acc: 0.9731 - val_loss: 0.2055 - val_acc: 0.9660
Epoch 16/40
 - 4s - loss: 0.1052 - acc: 0.9737 - val_loss: 0.1879 - val_acc: 0.9659
Epoch 17/40
 - 4s - loss: 0.1015 - acc: 0.9735 - val_loss: 0.1853 - val_acc: 0.9665
Epoch 18/40
 - 4s - loss: 0.1044 - acc: 0.9741 - val_loss: 0.1741 - val_acc: 0.9679
Epoch 19/40
 - 4s - loss: 0.1057 - acc: 0.9745 - val_loss: 0.1947 - val_acc: 0.9658
Epoch 20/40
 - 4s - loss: 0.1053 - acc: 0.9741 - val_loss: 0.1853 - val_acc: 0.9660
Epoch 21/40
 - 4s - loss: 0.1073 - acc: 0.9741 - val_loss: 0.1867 - val_acc: 0.9673
Epoch 22/40
 - 4s - loss: 0.1048 - acc: 0.9744 - val_loss: 0.2100 - val_acc: 0.9657
Epoch 23/40
 - 4s - loss: 0.1058 - acc: 0.9747 - val_loss: 0.1933 - val_acc: 0.9671
Epoch 24/40
 - 4s - loss: 0.1155 - acc: 0.9744 - val_loss: 0.2240 - val_acc: 0.9663
Epoch 25/40
 - 4s - loss: 0.1114 - acc: 0.9747 - val_loss: 0.1999 - val_acc: 0.9673
Epoch 26/40
 - 4s - loss: 0.1138 - acc: 0.9745 - val_loss: 0.2018 - val_acc: 0.9671
Epoch 27/40
 - 4s - loss: 0.1190 - acc: 0.9746 - val_loss: 0.1906 - val_acc: 0.9682
Epoch 28/40
 - 4s - loss: 0.1147 - acc: 0.9749 - val_loss: 0.2137 - val_acc: 0.9682
Epoch 29/40
 - 4s - loss: 0.1279 - acc: 0.9749 - val_loss: 0.2219 - val_acc: 0.9673
Epoch 30/40
 - 4s - loss: 0.1222 - acc: 0.9749 - val_loss: 0.2224 - val_acc: 0.9665
Epoch 31/40
 - 4s - loss: 0.1287 - acc: 0.9749 - val_loss: 0.2063 - val_acc: 0.9669
Epoch 32/40
 - 4s - loss: 0.1295 - acc: 0.9749 - val_loss: 0.2252 - val_acc: 0.9663
Epoch 33/40
 - 4s - loss: 0.1276 - acc: 0.9752 - val_loss: 0.2399 - val_acc: 0.9666
Epoch 34/40
 - 4s - loss: 0.1316 - acc: 0.9747 - val_loss: 0.2144 - val_acc: 0.9666
Epoch 35/40
 - 4s - loss: 0.1323 - acc: 0.9748 - val_loss: 0.2305 - val_acc: 0.9667
Epoch 36/40
 - 4s - loss: 0.1364 - acc: 0.9751 - val_loss: 0.2514 - val_acc: 0.9664
Epoch 37/40
 - 4s - loss: 0.1413 - acc: 0.9751 - val_loss: 0.2544 - val_acc: 0.9669
Epoch 38/40
 - 4s - loss: 0.1430 - acc: 0.9748 - val_loss: 0.2532 - val_acc: 0.9673
Epoch 39/40
 - 4s - loss: 0.1448 - acc: 0.9753 - val_loss: 0.2680 - val_acc: 0.9675
Epoch 40/40
 - 4s - loss: 0.1520 - acc: 0.9752 - val_loss: 0.2436 - val_acc: 0.9670
Epoch 1/40
 - 0s - loss: 0.2694 - acc: 0.9663
Epoch 2/40
 - 0s - loss: 0.2364 - acc: 0.9672
Epoch 3/40
 - 0s - loss: 0.2388 - acc: 0.9660
Epoch 4/40
 - 0s - loss: 0.2369 - acc: 0.9676
Epoch 5/40
 - 0s - loss: 0.2349 - acc: 0.9687
Epoch 6/40
 - 0s - loss: 0.2214 - acc: 0.9687
Epoch 7/40
 - 0s - loss: 0.2158 - acc: 0.9691
Epoch 8/40
 - 0s - loss: 0.2075 - acc: 0.9684
Epoch 9/40
 - 0s - loss: 0.2103 - acc: 0.9690
Epoch 10/40
 - 0s - loss: 0.2004 - acc: 0.9699
Epoch 11/40
 - 0s - loss: 0.2028 - acc: 0.9713
Epoch 12/40
 - 0s - loss: 0.2030 - acc: 0.9712
Epoch 13/40
 - 0s - loss: 0.2071 - acc: 0.9715
Epoch 14/40
 - 0s - loss: 0.2070 - acc: 0.9713
Epoch 15/40
 - 0s - loss: 0.1984 - acc: 0.9711
Epoch 16/40
 - 0s - loss: 0.2022 - acc: 0.9716
Epoch 17/40
 - 0s - loss: 0.2017 - acc: 0.9722
Epoch 18/40
 - 0s - loss: 0.2214 - acc: 0.9716
Epoch 19/40
 - 0s - loss: 0.2050 - acc: 0.9725
Epoch 20/40
 - 0s - loss: 0.1988 - acc: 0.9724
Epoch 21/40
 - 0s - loss: 0.1865 - acc: 0.9724
Epoch 22/40
 - 0s - loss: 0.1830 - acc: 0.9729
Epoch 23/40
 - 0s - loss: 0.2044 - acc: 0.9722
Epoch 24/40
 - 0s - loss: 0.1997 - acc: 0.9725
Epoch 25/40
 - 0s - loss: 0.2029 - acc: 0.9724
Epoch 26/40
 - 0s - loss: 0.1871 - acc: 0.9730
Epoch 27/40
 - 0s - loss: 0.2015 - acc: 0.9734
Epoch 28/40
 - 0s - loss: 0.1959 - acc: 0.9736
Epoch 29/40
 - 0s - loss: 0.1921 - acc: 0.9738
Epoch 30/40
 - 0s - loss: 0.2009 - acc: 0.9736
Epoch 31/40
 - 0s - loss: 0.2136 - acc: 0.9740
Epoch 32/40
 - 0s - loss: 0.1907 - acc: 0.9744
Epoch 33/40
 - 0s - loss: 0.1996 - acc: 0.9734
Epoch 34/40
 - 0s - loss: 0.1889 - acc: 0.9745
Epoch 35/40
 - 0s - loss: 0.1929 - acc: 0.9731
Epoch 36/40
 - 0s - loss: 0.2015 - acc: 0.9740
Epoch 37/40
 - 0s - loss: 0.1807 - acc: 0.9745
Epoch 38/40
 - 0s - loss: 0.1911 - acc: 0.9737
Epoch 39/40
 - 0s - loss: 0.1832 - acc: 0.9740
Epoch 40/40
 - 0s - loss: 0.1927 - acc: 0.9745
# Training time = 0:03:37.615603
# F-Score(Ordinary) = 0.341, Recall: 0.609, Precision: 0.237
# F-Score(ireflv) = 0.725, Recall: 0.63, Precision: 0.852
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_85 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_86 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_85 (Embedding)        (None, 4, 48)        705264      input_85[0][0]                   
__________________________________________________________________________________________________
embedding_86 (Embedding)        (None, 4, 24)        5640        input_86[0][0]                   
__________________________________________________________________________________________________
flatten_85 (Flatten)            (None, 192)          0           embedding_85[0][0]               
__________________________________________________________________________________________________
flatten_86 (Flatten)            (None, 96)           0           embedding_86[0][0]               
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 288)          0           flatten_85[0][0]                 
                                                                 flatten_86[0][0]                 
__________________________________________________________________________________________________
dense_85 (Dense)                (None, 24)           6936        concatenate_43[0][0]             
__________________________________________________________________________________________________
dropout_43 (Dropout)            (None, 24)           0           dense_85[0][0]                   
__________________________________________________________________________________________________
dense_86 (Dense)                (None, 8)            200         dropout_43[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0981 - acc: 0.9623 - val_loss: 0.0837 - val_acc: 0.9658
Epoch 2/40
 - 4s - loss: 0.0845 - acc: 0.9668 - val_loss: 0.0865 - val_acc: 0.9660
Epoch 3/40
 - 4s - loss: 0.0856 - acc: 0.9685 - val_loss: 0.1038 - val_acc: 0.9652
Epoch 4/40
 - 4s - loss: 0.0887 - acc: 0.9692 - val_loss: 0.1152 - val_acc: 0.9679
Epoch 5/40
 - 4s - loss: 0.0907 - acc: 0.9699 - val_loss: 0.1204 - val_acc: 0.9667
Epoch 6/40
 - 4s - loss: 0.0946 - acc: 0.9706 - val_loss: 0.1212 - val_acc: 0.9667
Epoch 7/40
 - 4s - loss: 0.0934 - acc: 0.9713 - val_loss: 0.1315 - val_acc: 0.9674
Epoch 8/40
 - 4s - loss: 0.0951 - acc: 0.9720 - val_loss: 0.1437 - val_acc: 0.9683
Epoch 9/40
 - 4s - loss: 0.0934 - acc: 0.9722 - val_loss: 0.1527 - val_acc: 0.9654
Epoch 10/40
 - 4s - loss: 0.0967 - acc: 0.9726 - val_loss: 0.1507 - val_acc: 0.9668
Epoch 11/40
 - 4s - loss: 0.0990 - acc: 0.9731 - val_loss: 0.1751 - val_acc: 0.9660
Epoch 12/40
 - 4s - loss: 0.0992 - acc: 0.9727 - val_loss: 0.1429 - val_acc: 0.9668
Epoch 13/40
 - 4s - loss: 0.1021 - acc: 0.9730 - val_loss: 0.1830 - val_acc: 0.9660
Epoch 14/40
 - 4s - loss: 0.1001 - acc: 0.9733 - val_loss: 0.1594 - val_acc: 0.9667
Epoch 15/40
 - 4s - loss: 0.1067 - acc: 0.9731 - val_loss: 0.1498 - val_acc: 0.9671
Epoch 16/40
 - 4s - loss: 0.1028 - acc: 0.9736 - val_loss: 0.1705 - val_acc: 0.9653
Epoch 17/40
 - 4s - loss: 0.1062 - acc: 0.9734 - val_loss: 0.1772 - val_acc: 0.9664
Epoch 18/40
 - 4s - loss: 0.1085 - acc: 0.9735 - val_loss: 0.1864 - val_acc: 0.9668
Epoch 19/40
 - 4s - loss: 0.1109 - acc: 0.9737 - val_loss: 0.1720 - val_acc: 0.9667
Epoch 20/40
 - 4s - loss: 0.1099 - acc: 0.9740 - val_loss: 0.1726 - val_acc: 0.9670
Epoch 21/40
 - 4s - loss: 0.1113 - acc: 0.9741 - val_loss: 0.1705 - val_acc: 0.9668
Epoch 22/40
 - 4s - loss: 0.1108 - acc: 0.9741 - val_loss: 0.2049 - val_acc: 0.9669
Epoch 23/40
 - 4s - loss: 0.1140 - acc: 0.9744 - val_loss: 0.1814 - val_acc: 0.9674
Epoch 24/40
 - 4s - loss: 0.1108 - acc: 0.9745 - val_loss: 0.2257 - val_acc: 0.9673
Epoch 25/40
 - 4s - loss: 0.1208 - acc: 0.9745 - val_loss: 0.1956 - val_acc: 0.9678
Epoch 26/40
 - 4s - loss: 0.1201 - acc: 0.9742 - val_loss: 0.1957 - val_acc: 0.9672
Epoch 27/40
 - 4s - loss: 0.1265 - acc: 0.9746 - val_loss: 0.2337 - val_acc: 0.9668
Epoch 28/40
 - 4s - loss: 0.1230 - acc: 0.9747 - val_loss: 0.2138 - val_acc: 0.9672
Epoch 29/40
 - 4s - loss: 0.1275 - acc: 0.9749 - val_loss: 0.2210 - val_acc: 0.9671
Epoch 30/40
 - 4s - loss: 0.1269 - acc: 0.9749 - val_loss: 0.2258 - val_acc: 0.9671
Epoch 31/40
 - 4s - loss: 0.1341 - acc: 0.9745 - val_loss: 0.2324 - val_acc: 0.9675
Epoch 32/40
 - 4s - loss: 0.1324 - acc: 0.9751 - val_loss: 0.2402 - val_acc: 0.9672
Epoch 33/40
 - 4s - loss: 0.1380 - acc: 0.9748 - val_loss: 0.2401 - val_acc: 0.9674
Epoch 34/40
 - 4s - loss: 0.1325 - acc: 0.9748 - val_loss: 0.2408 - val_acc: 0.9671
Epoch 35/40
 - 4s - loss: 0.1364 - acc: 0.9749 - val_loss: 0.2660 - val_acc: 0.9674
Epoch 36/40
 - 4s - loss: 0.1378 - acc: 0.9749 - val_loss: 0.2354 - val_acc: 0.9673
Epoch 37/40
 - 4s - loss: 0.1369 - acc: 0.9754 - val_loss: 0.2259 - val_acc: 0.9674
Epoch 38/40
 - 4s - loss: 0.1376 - acc: 0.9749 - val_loss: 0.2352 - val_acc: 0.9668
Epoch 39/40
 - 4s - loss: 0.1479 - acc: 0.9747 - val_loss: 0.2451 - val_acc: 0.9669
Epoch 40/40
 - 4s - loss: 0.1445 - acc: 0.9748 - val_loss: 0.2512 - val_acc: 0.9677
Epoch 1/40
 - 0s - loss: 0.2554 - acc: 0.9661
Epoch 2/40
 - 0s - loss: 0.2421 - acc: 0.9656
Epoch 3/40
 - 0s - loss: 0.2521 - acc: 0.9674
Epoch 4/40
 - 0s - loss: 0.2510 - acc: 0.9655
Epoch 5/40
 - 0s - loss: 0.2389 - acc: 0.9679
Epoch 6/40
 - 0s - loss: 0.2206 - acc: 0.9671
Epoch 7/40
 - 0s - loss: 0.2280 - acc: 0.9679
Epoch 8/40
 - 0s - loss: 0.2132 - acc: 0.9668
Epoch 9/40
 - 0s - loss: 0.2061 - acc: 0.9678
Epoch 10/40
 - 0s - loss: 0.1982 - acc: 0.9689
Epoch 11/40
 - 0s - loss: 0.1938 - acc: 0.9690
Epoch 12/40
 - 0s - loss: 0.2079 - acc: 0.9678
Epoch 13/40
 - 0s - loss: 0.2081 - acc: 0.9694
Epoch 14/40
 - 0s - loss: 0.1873 - acc: 0.9703
Epoch 15/40
 - 0s - loss: 0.2045 - acc: 0.9703
Epoch 16/40
 - 0s - loss: 0.2018 - acc: 0.9704
Epoch 17/40
 - 0s - loss: 0.2027 - acc: 0.9704
Epoch 18/40
 - 0s - loss: 0.2050 - acc: 0.9701
Epoch 19/40
 - 0s - loss: 0.1893 - acc: 0.9713
Epoch 20/40
 - 0s - loss: 0.1832 - acc: 0.9706
Epoch 21/40
 - 0s - loss: 0.1974 - acc: 0.9709
Epoch 22/40
 - 0s - loss: 0.1852 - acc: 0.9712
Epoch 23/40
 - 0s - loss: 0.1794 - acc: 0.9706
Epoch 24/40
 - 0s - loss: 0.1802 - acc: 0.9721
Epoch 25/40
 - 0s - loss: 0.1858 - acc: 0.9719
Epoch 26/40
 - 0s - loss: 0.1845 - acc: 0.9717
Epoch 27/40
 - 0s - loss: 0.1836 - acc: 0.9714
Epoch 28/40
 - 0s - loss: 0.1709 - acc: 0.9727
Epoch 29/40
 - 0s - loss: 0.1836 - acc: 0.9718
Epoch 30/40
 - 0s - loss: 0.1774 - acc: 0.9718
Epoch 31/40
 - 0s - loss: 0.1739 - acc: 0.9718
Epoch 32/40
 - 0s - loss: 0.1688 - acc: 0.9717
Epoch 33/40
 - 0s - loss: 0.1747 - acc: 0.9731
Epoch 34/40
 - 0s - loss: 0.1774 - acc: 0.9719
Epoch 35/40
 - 0s - loss: 0.1785 - acc: 0.9720
Epoch 36/40
 - 0s - loss: 0.1651 - acc: 0.9725
Epoch 37/40
 - 0s - loss: 0.1741 - acc: 0.9726
Epoch 38/40
 - 0s - loss: 0.1833 - acc: 0.9725
Epoch 39/40
 - 0s - loss: 0.1670 - acc: 0.9732
Epoch 40/40
 - 0s - loss: 0.1728 - acc: 0.9729
# Training time = 0:03:36.727351
# F-Score(Ordinary) = 0.179, Recall: 0.676, Precision: 0.103
# F-Score(lvc) = 0.085, Recall: 0.6, Precision: 0.045
# F-Score(ireflv) = 0.444, Recall: 0.69, Precision: 0.328
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_87 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_88 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_87 (Embedding)        (None, 4, 48)        705264      input_87[0][0]                   
__________________________________________________________________________________________________
embedding_88 (Embedding)        (None, 4, 24)        5640        input_88[0][0]                   
__________________________________________________________________________________________________
flatten_87 (Flatten)            (None, 192)          0           embedding_87[0][0]               
__________________________________________________________________________________________________
flatten_88 (Flatten)            (None, 96)           0           embedding_88[0][0]               
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 288)          0           flatten_87[0][0]                 
                                                                 flatten_88[0][0]                 
__________________________________________________________________________________________________
dense_87 (Dense)                (None, 24)           6936        concatenate_44[0][0]             
__________________________________________________________________________________________________
dropout_44 (Dropout)            (None, 24)           0           dense_87[0][0]                   
__________________________________________________________________________________________________
dense_88 (Dense)                (None, 8)            200         dropout_44[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0985 - acc: 0.9615 - val_loss: 0.0837 - val_acc: 0.9656
Epoch 2/40
 - 4s - loss: 0.0849 - acc: 0.9670 - val_loss: 0.0875 - val_acc: 0.9648
Epoch 3/40
 - 4s - loss: 0.0863 - acc: 0.9684 - val_loss: 0.1002 - val_acc: 0.9659
Epoch 4/40
 - 4s - loss: 0.0887 - acc: 0.9687 - val_loss: 0.1553 - val_acc: 0.9658
Epoch 5/40
 - 4s - loss: 0.0897 - acc: 0.9702 - val_loss: 0.1239 - val_acc: 0.9668
Epoch 6/40
 - 4s - loss: 0.0928 - acc: 0.9703 - val_loss: 0.1293 - val_acc: 0.9649
Epoch 7/40
 - 4s - loss: 0.0971 - acc: 0.9708 - val_loss: 0.1549 - val_acc: 0.9678
Epoch 8/40
 - 4s - loss: 0.0958 - acc: 0.9715 - val_loss: 0.1443 - val_acc: 0.9656
Epoch 9/40
 - 4s - loss: 0.0952 - acc: 0.9719 - val_loss: 0.1556 - val_acc: 0.9673
Epoch 10/40
 - 4s - loss: 0.1034 - acc: 0.9716 - val_loss: 0.1441 - val_acc: 0.9669
Epoch 11/40
 - 4s - loss: 0.1085 - acc: 0.9719 - val_loss: 0.1660 - val_acc: 0.9680
Epoch 12/40
 - 4s - loss: 0.1099 - acc: 0.9724 - val_loss: 0.1641 - val_acc: 0.9674
Epoch 13/40
 - 4s - loss: 0.1111 - acc: 0.9726 - val_loss: 0.1767 - val_acc: 0.9655
Epoch 14/40
 - 4s - loss: 0.1163 - acc: 0.9725 - val_loss: 0.1649 - val_acc: 0.9659
Epoch 15/40
 - 4s - loss: 0.1099 - acc: 0.9733 - val_loss: 0.1585 - val_acc: 0.9669
Epoch 16/40
 - 4s - loss: 0.1074 - acc: 0.9735 - val_loss: 0.1924 - val_acc: 0.9648
Epoch 17/40
 - 4s - loss: 0.1099 - acc: 0.9734 - val_loss: 0.1698 - val_acc: 0.9668
Epoch 18/40
 - 4s - loss: 0.1176 - acc: 0.9738 - val_loss: 0.1618 - val_acc: 0.9677
Epoch 19/40
 - 4s - loss: 0.1130 - acc: 0.9736 - val_loss: 0.2137 - val_acc: 0.9662
Epoch 20/40
 - 4s - loss: 0.1236 - acc: 0.9739 - val_loss: 0.2186 - val_acc: 0.9673
Epoch 21/40
 - 4s - loss: 0.1204 - acc: 0.9742 - val_loss: 0.1940 - val_acc: 0.9676
Epoch 22/40
 - 4s - loss: 0.1251 - acc: 0.9738 - val_loss: 0.1955 - val_acc: 0.9667
Epoch 23/40
 - 4s - loss: 0.1291 - acc: 0.9740 - val_loss: 0.2061 - val_acc: 0.9677
Epoch 24/40
 - 4s - loss: 0.1311 - acc: 0.9739 - val_loss: 0.2119 - val_acc: 0.9676
Epoch 25/40
 - 4s - loss: 0.1322 - acc: 0.9741 - val_loss: 0.1801 - val_acc: 0.9686
Epoch 26/40
 - 4s - loss: 0.1327 - acc: 0.9747 - val_loss: 0.2178 - val_acc: 0.9681
Epoch 27/40
 - 4s - loss: 0.1388 - acc: 0.9745 - val_loss: 0.2274 - val_acc: 0.9679
Epoch 28/40
 - 4s - loss: 0.1442 - acc: 0.9745 - val_loss: 0.2200 - val_acc: 0.9682
Epoch 29/40
 - 4s - loss: 0.1748 - acc: 0.9743 - val_loss: 0.2567 - val_acc: 0.9678
Epoch 30/40
 - 4s - loss: 0.1588 - acc: 0.9748 - val_loss: 0.2501 - val_acc: 0.9677
Epoch 31/40
 - 4s - loss: 0.1626 - acc: 0.9748 - val_loss: 0.2548 - val_acc: 0.9673
Epoch 32/40
 - 4s - loss: 0.1677 - acc: 0.9747 - val_loss: 0.2724 - val_acc: 0.9676
Epoch 33/40
 - 4s - loss: 0.1629 - acc: 0.9748 - val_loss: 0.2574 - val_acc: 0.9681
Epoch 34/40
 - 4s - loss: 0.1635 - acc: 0.9747 - val_loss: 0.2819 - val_acc: 0.9669
Epoch 35/40
 - 4s - loss: 0.1684 - acc: 0.9752 - val_loss: 0.2927 - val_acc: 0.9677
Epoch 36/40
 - 4s - loss: 0.1686 - acc: 0.9749 - val_loss: 0.2665 - val_acc: 0.9684
Epoch 37/40
 - 4s - loss: 0.1727 - acc: 0.9747 - val_loss: 0.2560 - val_acc: 0.9683
Epoch 38/40
 - 4s - loss: 0.1770 - acc: 0.9749 - val_loss: 0.2818 - val_acc: 0.9678
Epoch 39/40
 - 4s - loss: 0.1749 - acc: 0.9747 - val_loss: 0.2494 - val_acc: 0.9671
Epoch 40/40
 - 4s - loss: 0.1583 - acc: 0.9753 - val_loss: 0.2626 - val_acc: 0.9676
Epoch 1/40
 - 0s - loss: 0.2569 - acc: 0.9675
Epoch 2/40
 - 0s - loss: 0.2521 - acc: 0.9654
Epoch 3/40
 - 0s - loss: 0.2526 - acc: 0.9677
Epoch 4/40
 - 0s - loss: 0.2486 - acc: 0.9660
Epoch 5/40
 - 0s - loss: 0.2327 - acc: 0.9667
Epoch 6/40
 - 0s - loss: 0.2329 - acc: 0.9686
Epoch 7/40
 - 0s - loss: 0.2300 - acc: 0.9689
Epoch 8/40
 - 0s - loss: 0.2183 - acc: 0.9684
Epoch 9/40
 - 0s - loss: 0.2412 - acc: 0.9679
Epoch 10/40
 - 0s - loss: 0.2181 - acc: 0.9683
Epoch 11/40
 - 0s - loss: 0.2044 - acc: 0.9707
Epoch 12/40
 - 0s - loss: 0.2200 - acc: 0.9703
Epoch 13/40
 - 0s - loss: 0.2158 - acc: 0.9697
Epoch 14/40
 - 0s - loss: 0.2010 - acc: 0.9693
Epoch 15/40
 - 0s - loss: 0.1939 - acc: 0.9707
Epoch 16/40
 - 0s - loss: 0.2054 - acc: 0.9704
Epoch 17/40
 - 0s - loss: 0.2152 - acc: 0.9697
Epoch 18/40
 - 0s - loss: 0.2061 - acc: 0.9720
Epoch 19/40
 - 0s - loss: 0.1909 - acc: 0.9704
Epoch 20/40
 - 0s - loss: 0.2036 - acc: 0.9703
Epoch 21/40
 - 0s - loss: 0.2014 - acc: 0.9714
Epoch 22/40
 - 0s - loss: 0.2024 - acc: 0.9704
Epoch 23/40
 - 0s - loss: 0.2032 - acc: 0.9710
Epoch 24/40
 - 0s - loss: 0.1946 - acc: 0.9708
Epoch 25/40
 - 0s - loss: 0.2040 - acc: 0.9712
Epoch 26/40
 - 0s - loss: 0.2079 - acc: 0.9711
Epoch 27/40
 - 0s - loss: 0.1922 - acc: 0.9719
Epoch 28/40
 - 0s - loss: 0.2057 - acc: 0.9724
Epoch 29/40
 - 0s - loss: 0.1932 - acc: 0.9733
Epoch 30/40
 - 0s - loss: 0.1924 - acc: 0.9720
Epoch 31/40
 - 0s - loss: 0.1911 - acc: 0.9722
Epoch 32/40
 - 0s - loss: 0.1858 - acc: 0.9725
Epoch 33/40
 - 0s - loss: 0.1947 - acc: 0.9726
Epoch 34/40
 - 0s - loss: 0.1853 - acc: 0.9725
Epoch 35/40
 - 0s - loss: 0.1954 - acc: 0.9714
Epoch 36/40
 - 0s - loss: 0.1790 - acc: 0.9730
Epoch 37/40
 - 0s - loss: 0.1891 - acc: 0.9732
Epoch 38/40
 - 0s - loss: 0.1950 - acc: 0.9727
Epoch 39/40
 - 0s - loss: 0.1964 - acc: 0.9724
Epoch 40/40
 - 0s - loss: 0.1816 - acc: 0.9734
# Training time = 0:03:35.412271
# F-Score(Ordinary) = 0.213, Recall: 0.915, Precision: 0.121
# F-Score(lvc) = 0.565, Recall: 0.915, Precision: 0.409
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_89 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_90 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_89 (Embedding)        (None, 4, 48)        705264      input_89[0][0]                   
__________________________________________________________________________________________________
embedding_90 (Embedding)        (None, 4, 24)        5640        input_90[0][0]                   
__________________________________________________________________________________________________
flatten_89 (Flatten)            (None, 192)          0           embedding_89[0][0]               
__________________________________________________________________________________________________
flatten_90 (Flatten)            (None, 96)           0           embedding_90[0][0]               
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 288)          0           flatten_89[0][0]                 
                                                                 flatten_90[0][0]                 
__________________________________________________________________________________________________
dense_89 (Dense)                (None, 24)           6936        concatenate_45[0][0]             
__________________________________________________________________________________________________
dropout_45 (Dropout)            (None, 24)           0           dense_89[0][0]                   
__________________________________________________________________________________________________
dense_90 (Dense)                (None, 8)            200         dropout_45[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0987 - acc: 0.9617 - val_loss: 0.0864 - val_acc: 0.9654
Epoch 2/40
 - 5s - loss: 0.0825 - acc: 0.9669 - val_loss: 0.0882 - val_acc: 0.9668
Epoch 3/40
 - 5s - loss: 0.0822 - acc: 0.9686 - val_loss: 0.0896 - val_acc: 0.9679
Epoch 4/40
 - 5s - loss: 0.0851 - acc: 0.9693 - val_loss: 0.1007 - val_acc: 0.9653
Epoch 5/40
 - 5s - loss: 0.0865 - acc: 0.9704 - val_loss: 0.1286 - val_acc: 0.9656
Epoch 6/40
 - 4s - loss: 0.0874 - acc: 0.9704 - val_loss: 0.1298 - val_acc: 0.9671
Epoch 7/40
 - 4s - loss: 0.0873 - acc: 0.9710 - val_loss: 0.1220 - val_acc: 0.9668
Epoch 8/40
 - 4s - loss: 0.0885 - acc: 0.9718 - val_loss: 0.1348 - val_acc: 0.9679
Epoch 9/40
 - 4s - loss: 0.0901 - acc: 0.9719 - val_loss: 0.1353 - val_acc: 0.9670
Epoch 10/40
 - 4s - loss: 0.0901 - acc: 0.9728 - val_loss: 0.1596 - val_acc: 0.9680
Epoch 11/40
 - 4s - loss: 0.0952 - acc: 0.9730 - val_loss: 0.1837 - val_acc: 0.9663
Epoch 12/40
 - 5s - loss: 0.0980 - acc: 0.9725 - val_loss: 0.1469 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0981 - acc: 0.9733 - val_loss: 0.1607 - val_acc: 0.9669
Epoch 14/40
 - 4s - loss: 0.0977 - acc: 0.9735 - val_loss: 0.1536 - val_acc: 0.9675
Epoch 15/40
 - 5s - loss: 0.1006 - acc: 0.9732 - val_loss: 0.1691 - val_acc: 0.9676
Epoch 16/40
 - 4s - loss: 0.0979 - acc: 0.9738 - val_loss: 0.1666 - val_acc: 0.9669
Epoch 17/40
 - 4s - loss: 0.1002 - acc: 0.9738 - val_loss: 0.1803 - val_acc: 0.9669
Epoch 18/40
 - 5s - loss: 0.0982 - acc: 0.9742 - val_loss: 0.1685 - val_acc: 0.9665
Epoch 19/40
 - 4s - loss: 0.0996 - acc: 0.9742 - val_loss: 0.1774 - val_acc: 0.9670
Epoch 20/40
 - 4s - loss: 0.1008 - acc: 0.9741 - val_loss: 0.1672 - val_acc: 0.9670
Epoch 21/40
 - 4s - loss: 0.1007 - acc: 0.9746 - val_loss: 0.1913 - val_acc: 0.9663
Epoch 22/40
 - 4s - loss: 0.0975 - acc: 0.9748 - val_loss: 0.1788 - val_acc: 0.9663
Epoch 23/40
 - 4s - loss: 0.0957 - acc: 0.9751 - val_loss: 0.1827 - val_acc: 0.9673
Epoch 24/40
 - 4s - loss: 0.0984 - acc: 0.9746 - val_loss: 0.1777 - val_acc: 0.9683
Epoch 25/40
 - 4s - loss: 0.0938 - acc: 0.9747 - val_loss: 0.1796 - val_acc: 0.9677
Epoch 26/40
 - 4s - loss: 0.0976 - acc: 0.9752 - val_loss: 0.2013 - val_acc: 0.9674
Epoch 27/40
 - 4s - loss: 0.0988 - acc: 0.9750 - val_loss: 0.2042 - val_acc: 0.9680
Epoch 28/40
 - 4s - loss: 0.0998 - acc: 0.9752 - val_loss: 0.1824 - val_acc: 0.9686
Epoch 29/40
 - 5s - loss: 0.0982 - acc: 0.9756 - val_loss: 0.1904 - val_acc: 0.9681
Epoch 30/40
 - 4s - loss: 0.1020 - acc: 0.9755 - val_loss: 0.1911 - val_acc: 0.9674
Epoch 31/40
 - 4s - loss: 0.1013 - acc: 0.9754 - val_loss: 0.2029 - val_acc: 0.9672
Epoch 32/40
 - 4s - loss: 0.1052 - acc: 0.9759 - val_loss: 0.1988 - val_acc: 0.9674
Epoch 33/40
 - 4s - loss: 0.1076 - acc: 0.9757 - val_loss: 0.1972 - val_acc: 0.9670
Epoch 34/40
 - 4s - loss: 0.1059 - acc: 0.9759 - val_loss: 0.2080 - val_acc: 0.9677
Epoch 35/40
 - 4s - loss: 0.1087 - acc: 0.9759 - val_loss: 0.2053 - val_acc: 0.9677
Epoch 36/40
 - 4s - loss: 0.1080 - acc: 0.9761 - val_loss: 0.2024 - val_acc: 0.9668
Epoch 37/40
 - 4s - loss: 0.1081 - acc: 0.9757 - val_loss: 0.2101 - val_acc: 0.9676
Epoch 38/40
 - 4s - loss: 0.1072 - acc: 0.9760 - val_loss: 0.2036 - val_acc: 0.9668
Epoch 39/40
 - 4s - loss: 0.1106 - acc: 0.9765 - val_loss: 0.2116 - val_acc: 0.9669
Epoch 40/40
 - 4s - loss: 0.1146 - acc: 0.9761 - val_loss: 0.2375 - val_acc: 0.9664
Epoch 1/40
 - 0s - loss: 0.2178 - acc: 0.9667
Epoch 2/40
 - 0s - loss: 0.1930 - acc: 0.9665
Epoch 3/40
 - 0s - loss: 0.1784 - acc: 0.9676
Epoch 4/40
 - 0s - loss: 0.1718 - acc: 0.9695
Epoch 5/40
 - 0s - loss: 0.1753 - acc: 0.9696
Epoch 6/40
 - 0s - loss: 0.1669 - acc: 0.9704
Epoch 7/40
 - 0s - loss: 0.1637 - acc: 0.9707
Epoch 8/40
 - 0s - loss: 0.1659 - acc: 0.9719
Epoch 9/40
 - 0s - loss: 0.1575 - acc: 0.9706
Epoch 10/40
 - 0s - loss: 0.1506 - acc: 0.9717
Epoch 11/40
 - 0s - loss: 0.1511 - acc: 0.9724
Epoch 12/40
 - 0s - loss: 0.1480 - acc: 0.9732
Epoch 13/40
 - 0s - loss: 0.1480 - acc: 0.9726
Epoch 14/40
 - 0s - loss: 0.1574 - acc: 0.9734
Epoch 15/40
 - 0s - loss: 0.1565 - acc: 0.9727
Epoch 16/40
 - 0s - loss: 0.1514 - acc: 0.9736
Epoch 17/40
 - 0s - loss: 0.1413 - acc: 0.9745
Epoch 18/40
 - 0s - loss: 0.1424 - acc: 0.9745
Epoch 19/40
 - 0s - loss: 0.1457 - acc: 0.9747
Epoch 20/40
 - 0s - loss: 0.1435 - acc: 0.9748
Epoch 21/40
 - 0s - loss: 0.1365 - acc: 0.9743
Epoch 22/40
 - 0s - loss: 0.1423 - acc: 0.9745
Epoch 23/40
 - 0s - loss: 0.1512 - acc: 0.9745
Epoch 24/40
 - 0s - loss: 0.1479 - acc: 0.9753
Epoch 25/40
 - 0s - loss: 0.1373 - acc: 0.9752
Epoch 26/40
 - 0s - loss: 0.1316 - acc: 0.9758
Epoch 27/40
 - 0s - loss: 0.1284 - acc: 0.9758
Epoch 28/40
 - 0s - loss: 0.1311 - acc: 0.9754
Epoch 29/40
 - 0s - loss: 0.1400 - acc: 0.9762
Epoch 30/40
 - 0s - loss: 0.1326 - acc: 0.9751
Epoch 31/40
 - 0s - loss: 0.1309 - acc: 0.9757
Epoch 32/40
 - 0s - loss: 0.1315 - acc: 0.9753
Epoch 33/40
 - 0s - loss: 0.1397 - acc: 0.9746
Epoch 34/40
 - 0s - loss: 0.1350 - acc: 0.9749
Epoch 35/40
 - 0s - loss: 0.1289 - acc: 0.9758
Epoch 36/40
 - 0s - loss: 0.1292 - acc: 0.9755
Epoch 37/40
 - 0s - loss: 0.1324 - acc: 0.9751
Epoch 38/40
 - 0s - loss: 0.1452 - acc: 0.9760
Epoch 39/40
 - 0s - loss: 0.1332 - acc: 0.9749
Epoch 40/40
 - 0s - loss: 0.1308 - acc: 0.9752
# Training time = 0:03:39.212902
# F-Score(Ordinary) = 0.039, Recall: 0.692, Precision: 0.02
# F-Score(lvc) = 0.124, Recall: 0.692, Precision: 0.068
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_91 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_92 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_91 (Embedding)        (None, 4, 48)        705264      input_91[0][0]                   
__________________________________________________________________________________________________
embedding_92 (Embedding)        (None, 4, 24)        5640        input_92[0][0]                   
__________________________________________________________________________________________________
flatten_91 (Flatten)            (None, 192)          0           embedding_91[0][0]               
__________________________________________________________________________________________________
flatten_92 (Flatten)            (None, 96)           0           embedding_92[0][0]               
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 288)          0           flatten_91[0][0]                 
                                                                 flatten_92[0][0]                 
__________________________________________________________________________________________________
dense_91 (Dense)                (None, 24)           6936        concatenate_46[0][0]             
__________________________________________________________________________________________________
dropout_46 (Dropout)            (None, 24)           0           dense_91[0][0]                   
__________________________________________________________________________________________________
dense_92 (Dense)                (None, 8)            200         dropout_46[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1076 - acc: 0.9609 - val_loss: 0.1117 - val_acc: 0.9655
Epoch 2/40
 - 4s - loss: 0.1064 - acc: 0.9655 - val_loss: 0.1771 - val_acc: 0.9658
Epoch 3/40
 - 4s - loss: 0.1153 - acc: 0.9675 - val_loss: 0.1507 - val_acc: 0.9658
Epoch 4/40
 - 4s - loss: 0.1235 - acc: 0.9693 - val_loss: 0.1606 - val_acc: 0.9654
Epoch 5/40
 - 4s - loss: 0.1275 - acc: 0.9698 - val_loss: 0.1500 - val_acc: 0.9674
Epoch 6/40
 - 4s - loss: 0.1254 - acc: 0.9706 - val_loss: 0.1453 - val_acc: 0.9667
Epoch 7/40
 - 4s - loss: 0.1261 - acc: 0.9714 - val_loss: 0.1811 - val_acc: 0.9648
Epoch 8/40
 - 4s - loss: 0.1371 - acc: 0.9714 - val_loss: 0.1901 - val_acc: 0.9676
Epoch 9/40
 - 4s - loss: 0.1428 - acc: 0.9716 - val_loss: 0.1884 - val_acc: 0.9674
Epoch 10/40
 - 4s - loss: 0.1496 - acc: 0.9720 - val_loss: 0.1814 - val_acc: 0.9648
Epoch 11/40
 - 4s - loss: 0.1528 - acc: 0.9725 - val_loss: 0.2088 - val_acc: 0.9663
Epoch 12/40
 - 4s - loss: 0.1586 - acc: 0.9723 - val_loss: 0.2371 - val_acc: 0.9659
Epoch 13/40
 - 4s - loss: 0.1719 - acc: 0.9728 - val_loss: 0.2219 - val_acc: 0.9666
Epoch 14/40
 - 4s - loss: 0.1757 - acc: 0.9730 - val_loss: 0.2993 - val_acc: 0.9669
Epoch 15/40
 - 4s - loss: 0.2162 - acc: 0.9729 - val_loss: 0.2725 - val_acc: 0.9667
Epoch 16/40
 - 4s - loss: 0.2172 - acc: 0.9730 - val_loss: 0.2595 - val_acc: 0.9670
Epoch 17/40
 - 4s - loss: 0.2231 - acc: 0.9733 - val_loss: 0.3351 - val_acc: 0.9668
Epoch 18/40
 - 4s - loss: 0.2533 - acc: 0.9730 - val_loss: 0.3752 - val_acc: 0.9670
Epoch 19/40
 - 4s - loss: 0.2406 - acc: 0.9734 - val_loss: 0.3339 - val_acc: 0.9664
Epoch 20/40
 - 4s - loss: 0.2484 - acc: 0.9735 - val_loss: 0.3541 - val_acc: 0.9653
Epoch 21/40
 - 4s - loss: 0.2424 - acc: 0.9736 - val_loss: 0.3534 - val_acc: 0.9666
Epoch 22/40
 - 4s - loss: 0.2835 - acc: 0.9728 - val_loss: 0.3462 - val_acc: 0.9665
Epoch 23/40
 - 4s - loss: 0.2622 - acc: 0.9735 - val_loss: 0.3025 - val_acc: 0.9668
Epoch 24/40
 - 4s - loss: 0.2472 - acc: 0.9737 - val_loss: 0.3501 - val_acc: 0.9676
Epoch 25/40
 - 4s - loss: 0.2538 - acc: 0.9737 - val_loss: 0.3330 - val_acc: 0.9676
Epoch 26/40
 - 4s - loss: 0.2475 - acc: 0.9736 - val_loss: 0.3160 - val_acc: 0.9665
Epoch 27/40
 - 4s - loss: 0.2420 - acc: 0.9733 - val_loss: 0.3189 - val_acc: 0.9657
Epoch 28/40
 - 4s - loss: 0.2385 - acc: 0.9734 - val_loss: 0.3130 - val_acc: 0.9666
Epoch 29/40
 - 4s - loss: 0.2654 - acc: 0.9726 - val_loss: 0.3447 - val_acc: 0.9664
Epoch 30/40
 - 4s - loss: 0.2715 - acc: 0.9729 - val_loss: 0.4203 - val_acc: 0.9661
Epoch 31/40
 - 4s - loss: 0.3100 - acc: 0.9724 - val_loss: 0.3619 - val_acc: 0.9648
Epoch 32/40
 - 4s - loss: 0.3047 - acc: 0.9732 - val_loss: 0.4317 - val_acc: 0.9664
Epoch 33/40
 - 4s - loss: 0.3308 - acc: 0.9730 - val_loss: 0.4297 - val_acc: 0.9665
Epoch 34/40
 - 4s - loss: 0.3202 - acc: 0.9731 - val_loss: 0.4157 - val_acc: 0.9656
Epoch 35/40
 - 4s - loss: 0.3342 - acc: 0.9727 - val_loss: 0.4047 - val_acc: 0.9664
Epoch 36/40
 - 4s - loss: 0.3240 - acc: 0.9729 - val_loss: 0.4481 - val_acc: 0.9657
Epoch 37/40
 - 4s - loss: 0.3267 - acc: 0.9732 - val_loss: 0.4169 - val_acc: 0.9665
Epoch 38/40
 - 4s - loss: 0.3333 - acc: 0.9731 - val_loss: 0.4237 - val_acc: 0.9663
Epoch 39/40
 - 4s - loss: 0.3379 - acc: 0.9734 - val_loss: 0.4404 - val_acc: 0.9670
Epoch 40/40
 - 4s - loss: 0.3329 - acc: 0.9736 - val_loss: 0.4357 - val_acc: 0.9660
Epoch 1/40
 - 0s - loss: 0.4266 - acc: 0.9637
Epoch 2/40
 - 0s - loss: 0.3846 - acc: 0.9642
Epoch 3/40
 - 0s - loss: 0.4097 - acc: 0.9641
Epoch 4/40
 - 0s - loss: 0.4004 - acc: 0.9649
Epoch 5/40
 - 0s - loss: 0.4079 - acc: 0.9652
Epoch 6/40
 - 0s - loss: 0.4355 - acc: 0.9657
Epoch 7/40
 - 0s - loss: 0.4210 - acc: 0.9655
Epoch 8/40
 - 0s - loss: 0.4542 - acc: 0.9661
Epoch 9/40
 - 0s - loss: 0.4801 - acc: 0.9654
Epoch 10/40
 - 0s - loss: 0.4419 - acc: 0.9659
Epoch 11/40
 - 0s - loss: 0.4337 - acc: 0.9658
Epoch 12/40
 - 0s - loss: 0.4144 - acc: 0.9660
Epoch 13/40
 - 0s - loss: 0.4415 - acc: 0.9660
Epoch 14/40
 - 0s - loss: 0.4090 - acc: 0.9666
Epoch 15/40
 - 0s - loss: 0.3914 - acc: 0.9670
Epoch 16/40
 - 0s - loss: 0.4153 - acc: 0.9664
Epoch 17/40
 - 0s - loss: 0.4177 - acc: 0.9669
Epoch 18/40
 - 0s - loss: 0.4393 - acc: 0.9672
Epoch 19/40
 - 0s - loss: 0.4175 - acc: 0.9668
Epoch 20/40
 - 0s - loss: 0.4184 - acc: 0.9669
Epoch 21/40
 - 0s - loss: 0.3982 - acc: 0.9686
Epoch 22/40
 - 0s - loss: 0.4257 - acc: 0.9681
Epoch 23/40
 - 0s - loss: 0.4409 - acc: 0.9686
Epoch 24/40
 - 0s - loss: 0.4393 - acc: 0.9672
Epoch 25/40
 - 0s - loss: 0.4227 - acc: 0.9680
Epoch 26/40
 - 0s - loss: 0.4328 - acc: 0.9687
Epoch 27/40
 - 0s - loss: 0.4576 - acc: 0.9676
Epoch 28/40
 - 0s - loss: 0.4435 - acc: 0.9684
Epoch 29/40
 - 0s - loss: 0.4467 - acc: 0.9679
Epoch 30/40
 - 0s - loss: 0.4436 - acc: 0.9679
Epoch 31/40
 - 0s - loss: 0.4146 - acc: 0.9672
Epoch 32/40
 - 0s - loss: 0.4122 - acc: 0.9685
Epoch 33/40
 - 0s - loss: 0.4338 - acc: 0.9686
Epoch 34/40
 - 0s - loss: 0.4454 - acc: 0.9677
Epoch 35/40
 - 0s - loss: 0.4248 - acc: 0.9683
Epoch 36/40
 - 0s - loss: 0.4178 - acc: 0.9676
Epoch 37/40
 - 0s - loss: 0.4320 - acc: 0.9683
Epoch 38/40
 - 0s - loss: 0.4129 - acc: 0.9689
Epoch 39/40
 - 0s - loss: 0.4034 - acc: 0.9695
Epoch 40/40
 - 0s - loss: 0.4336 - acc: 0.9683
# Training time = 0:03:35.118286
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_93 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_94 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_93 (Embedding)        (None, 4, 48)        705264      input_93[0][0]                   
__________________________________________________________________________________________________
embedding_94 (Embedding)        (None, 4, 24)        5640        input_94[0][0]                   
__________________________________________________________________________________________________
flatten_93 (Flatten)            (None, 192)          0           embedding_93[0][0]               
__________________________________________________________________________________________________
flatten_94 (Flatten)            (None, 96)           0           embedding_94[0][0]               
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 288)          0           flatten_93[0][0]                 
                                                                 flatten_94[0][0]                 
__________________________________________________________________________________________________
dense_93 (Dense)                (None, 24)           6936        concatenate_47[0][0]             
__________________________________________________________________________________________________
dropout_47 (Dropout)            (None, 24)           0           dense_93[0][0]                   
__________________________________________________________________________________________________
dense_94 (Dense)                (None, 8)            200         dropout_47[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1081 - acc: 0.9607 - val_loss: 0.1023 - val_acc: 0.9659
Epoch 2/40
 - 4s - loss: 0.1076 - acc: 0.9663 - val_loss: 0.1277 - val_acc: 0.9666
Epoch 3/40
 - 4s - loss: 0.1164 - acc: 0.9681 - val_loss: 0.1558 - val_acc: 0.9660
Epoch 4/40
 - 4s - loss: 0.1289 - acc: 0.9697 - val_loss: 0.1652 - val_acc: 0.9663
Epoch 5/40
 - 4s - loss: 0.1260 - acc: 0.9698 - val_loss: 0.1589 - val_acc: 0.9660
Epoch 6/40
 - 4s - loss: 0.1402 - acc: 0.9710 - val_loss: 0.2055 - val_acc: 0.9640
Epoch 7/40
 - 4s - loss: 0.1309 - acc: 0.9709 - val_loss: 0.1637 - val_acc: 0.9667
Epoch 8/40
 - 4s - loss: 0.1665 - acc: 0.9715 - val_loss: 0.2371 - val_acc: 0.9653
Epoch 9/40
 - 4s - loss: 0.1875 - acc: 0.9715 - val_loss: 0.2166 - val_acc: 0.9664
Epoch 10/40
 - 4s - loss: 0.1925 - acc: 0.9714 - val_loss: 0.2321 - val_acc: 0.9665
Epoch 11/40
 - 4s - loss: 0.2011 - acc: 0.9717 - val_loss: 0.2793 - val_acc: 0.9656
Epoch 12/40
 - 4s - loss: 0.2139 - acc: 0.9718 - val_loss: 0.2547 - val_acc: 0.9650
Epoch 13/40
 - 4s - loss: 0.2158 - acc: 0.9722 - val_loss: 0.2652 - val_acc: 0.9663
Epoch 14/40
 - 4s - loss: 0.2173 - acc: 0.9723 - val_loss: 0.3059 - val_acc: 0.9667
Epoch 15/40
 - 4s - loss: 0.2251 - acc: 0.9720 - val_loss: 0.2876 - val_acc: 0.9654
Epoch 16/40
 - 4s - loss: 0.2144 - acc: 0.9726 - val_loss: 0.2751 - val_acc: 0.9670
Epoch 17/40
 - 4s - loss: 0.2179 - acc: 0.9720 - val_loss: 0.2761 - val_acc: 0.9652
Epoch 18/40
 - 4s - loss: 0.2064 - acc: 0.9727 - val_loss: 0.2617 - val_acc: 0.9661
Epoch 19/40
 - 4s - loss: 0.2090 - acc: 0.9722 - val_loss: 0.3001 - val_acc: 0.9674
Epoch 20/40
 - 4s - loss: 0.2097 - acc: 0.9724 - val_loss: 0.2751 - val_acc: 0.9671
Epoch 21/40
 - 4s - loss: 0.2095 - acc: 0.9723 - val_loss: 0.2443 - val_acc: 0.9660
Epoch 22/40
 - 4s - loss: 0.2049 - acc: 0.9722 - val_loss: 0.3090 - val_acc: 0.9661
Epoch 23/40
 - 4s - loss: 0.2201 - acc: 0.9723 - val_loss: 0.3011 - val_acc: 0.9664
Epoch 24/40
 - 4s - loss: 0.2207 - acc: 0.9723 - val_loss: 0.2862 - val_acc: 0.9665
Epoch 25/40
 - 4s - loss: 0.2168 - acc: 0.9722 - val_loss: 0.3135 - val_acc: 0.9663
Epoch 26/40
 - 4s - loss: 0.2167 - acc: 0.9726 - val_loss: 0.2783 - val_acc: 0.9674
Epoch 27/40
 - 4s - loss: 0.2116 - acc: 0.9727 - val_loss: 0.2876 - val_acc: 0.9661
Epoch 28/40
 - 4s - loss: 0.2090 - acc: 0.9732 - val_loss: 0.2773 - val_acc: 0.9671
Epoch 29/40
 - 4s - loss: 0.2195 - acc: 0.9723 - val_loss: 0.2929 - val_acc: 0.9665
Epoch 30/40
 - 4s - loss: 0.2102 - acc: 0.9725 - val_loss: 0.2973 - val_acc: 0.9675
Epoch 31/40
 - 4s - loss: 0.2126 - acc: 0.9726 - val_loss: 0.2891 - val_acc: 0.9663
Epoch 32/40
 - 4s - loss: 0.2072 - acc: 0.9732 - val_loss: 0.2921 - val_acc: 0.9670
Epoch 33/40
 - 4s - loss: 0.2106 - acc: 0.9728 - val_loss: 0.2962 - val_acc: 0.9667
Epoch 34/40
 - 4s - loss: 0.2109 - acc: 0.9733 - val_loss: 0.2813 - val_acc: 0.9673
Epoch 35/40
 - 4s - loss: 0.2172 - acc: 0.9730 - val_loss: 0.2907 - val_acc: 0.9657
Epoch 36/40
 - 4s - loss: 0.2188 - acc: 0.9731 - val_loss: 0.2903 - val_acc: 0.9668
Epoch 37/40
 - 4s - loss: 0.2215 - acc: 0.9731 - val_loss: 0.2873 - val_acc: 0.9668
Epoch 38/40
 - 4s - loss: 0.2077 - acc: 0.9729 - val_loss: 0.2954 - val_acc: 0.9673
Epoch 39/40
 - 4s - loss: 0.2055 - acc: 0.9731 - val_loss: 0.2978 - val_acc: 0.9664
Epoch 40/40
 - 4s - loss: 0.2126 - acc: 0.9732 - val_loss: 0.2890 - val_acc: 0.9661
Epoch 1/40
 - 0s - loss: 0.3035 - acc: 0.9653
Epoch 2/40
 - 0s - loss: 0.2998 - acc: 0.9662
Epoch 3/40
 - 0s - loss: 0.3124 - acc: 0.9665
Epoch 4/40
 - 0s - loss: 0.3180 - acc: 0.9660
Epoch 5/40
 - 0s - loss: 0.3191 - acc: 0.9668
Epoch 6/40
 - 0s - loss: 0.3604 - acc: 0.9660
Epoch 7/40
 - 0s - loss: 0.3171 - acc: 0.9670
Epoch 8/40
 - 0s - loss: 0.2763 - acc: 0.9659
Epoch 9/40
 - 0s - loss: 0.2766 - acc: 0.9673
Epoch 10/40
 - 0s - loss: 0.2896 - acc: 0.9679
Epoch 11/40
 - 0s - loss: 0.2706 - acc: 0.9679
Epoch 12/40
 - 0s - loss: 0.2840 - acc: 0.9680
Epoch 13/40
 - 0s - loss: 0.3064 - acc: 0.9671
Epoch 14/40
 - 0s - loss: 0.3027 - acc: 0.9676
Epoch 15/40
 - 0s - loss: 0.2745 - acc: 0.9678
Epoch 16/40
 - 0s - loss: 0.2769 - acc: 0.9676
Epoch 17/40
 - 0s - loss: 0.2739 - acc: 0.9683
Epoch 18/40
 - 0s - loss: 0.2719 - acc: 0.9691
Epoch 19/40
 - 0s - loss: 0.2810 - acc: 0.9683
Epoch 20/40
 - 0s - loss: 0.2737 - acc: 0.9683
Epoch 21/40
 - 0s - loss: 0.2762 - acc: 0.9685
Epoch 22/40
 - 0s - loss: 0.2829 - acc: 0.9692
Epoch 23/40
 - 0s - loss: 0.2812 - acc: 0.9689
Epoch 24/40
 - 0s - loss: 0.2772 - acc: 0.9694
Epoch 25/40
 - 0s - loss: 0.2576 - acc: 0.9704
Epoch 26/40
 - 0s - loss: 0.2762 - acc: 0.9691
Epoch 27/40
 - 0s - loss: 0.2695 - acc: 0.9687
Epoch 28/40
 - 0s - loss: 0.2789 - acc: 0.9695
Epoch 29/40
 - 0s - loss: 0.2774 - acc: 0.9703
Epoch 30/40
 - 0s - loss: 0.2906 - acc: 0.9686
Epoch 31/40
 - 0s - loss: 0.2661 - acc: 0.9690
Epoch 32/40
 - 0s - loss: 0.2642 - acc: 0.9701
Epoch 33/40
 - 0s - loss: 0.2676 - acc: 0.9698
Epoch 34/40
 - 0s - loss: 0.2728 - acc: 0.9698
Epoch 35/40
 - 0s - loss: 0.2668 - acc: 0.9693
Epoch 36/40
 - 0s - loss: 0.2653 - acc: 0.9698
Epoch 37/40
 - 0s - loss: 0.2609 - acc: 0.9703
Epoch 38/40
 - 0s - loss: 0.2815 - acc: 0.9693
Epoch 39/40
 - 0s - loss: 0.2618 - acc: 0.9702
Epoch 40/40
 - 0s - loss: 0.2673 - acc: 0.9696
# Training time = 0:03:34.855142
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_95 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_96 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_95 (Embedding)        (None, 4, 48)        705264      input_95[0][0]                   
__________________________________________________________________________________________________
embedding_96 (Embedding)        (None, 4, 24)        5640        input_96[0][0]                   
__________________________________________________________________________________________________
flatten_95 (Flatten)            (None, 192)          0           embedding_95[0][0]               
__________________________________________________________________________________________________
flatten_96 (Flatten)            (None, 96)           0           embedding_96[0][0]               
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 288)          0           flatten_95[0][0]                 
                                                                 flatten_96[0][0]                 
__________________________________________________________________________________________________
dense_95 (Dense)                (None, 24)           6936        concatenate_48[0][0]             
__________________________________________________________________________________________________
dropout_48 (Dropout)            (None, 24)           0           dense_95[0][0]                   
__________________________________________________________________________________________________
dense_96 (Dense)                (None, 8)            200         dropout_48[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1063 - acc: 0.9610 - val_loss: 0.0916 - val_acc: 0.9649
Epoch 2/40
 - 4s - loss: 0.1068 - acc: 0.9656 - val_loss: 0.1106 - val_acc: 0.9644
Epoch 3/40
 - 4s - loss: 0.1175 - acc: 0.9678 - val_loss: 0.1392 - val_acc: 0.9664
Epoch 4/40
 - 4s - loss: 0.1337 - acc: 0.9682 - val_loss: 0.1776 - val_acc: 0.9653
Epoch 5/40
 - 4s - loss: 0.1375 - acc: 0.9691 - val_loss: 0.1856 - val_acc: 0.9661
Epoch 6/40
 - 4s - loss: 0.1485 - acc: 0.9696 - val_loss: 0.1924 - val_acc: 0.9653
Epoch 7/40
 - 4s - loss: 0.1558 - acc: 0.9705 - val_loss: 0.2365 - val_acc: 0.9654
Epoch 8/40
 - 4s - loss: 0.1579 - acc: 0.9707 - val_loss: 0.2099 - val_acc: 0.9668
Epoch 9/40
 - 4s - loss: 0.1610 - acc: 0.9710 - val_loss: 0.1985 - val_acc: 0.9662
Epoch 10/40
 - 4s - loss: 0.1847 - acc: 0.9714 - val_loss: 0.2374 - val_acc: 0.9653
Epoch 11/40
 - 4s - loss: 0.1758 - acc: 0.9716 - val_loss: 0.2234 - val_acc: 0.9664
Epoch 12/40
 - 4s - loss: 0.1740 - acc: 0.9716 - val_loss: 0.2478 - val_acc: 0.9662
Epoch 13/40
 - 4s - loss: 0.1822 - acc: 0.9715 - val_loss: 0.2310 - val_acc: 0.9668
Epoch 14/40
 - 4s - loss: 0.1778 - acc: 0.9723 - val_loss: 0.2500 - val_acc: 0.9666
Epoch 15/40
 - 4s - loss: 0.1838 - acc: 0.9727 - val_loss: 0.2503 - val_acc: 0.9654
Epoch 16/40
 - 4s - loss: 0.1990 - acc: 0.9721 - val_loss: 0.2655 - val_acc: 0.9664
Epoch 17/40
 - 4s - loss: 0.1931 - acc: 0.9724 - val_loss: 0.2644 - val_acc: 0.9664
Epoch 18/40
 - 4s - loss: 0.1875 - acc: 0.9729 - val_loss: 0.2638 - val_acc: 0.9662
Epoch 19/40
 - 4s - loss: 0.2026 - acc: 0.9724 - val_loss: 0.3203 - val_acc: 0.9662
Epoch 20/40
 - 4s - loss: 0.2036 - acc: 0.9730 - val_loss: 0.3280 - val_acc: 0.9666
Epoch 21/40
 - 4s - loss: 0.2095 - acc: 0.9724 - val_loss: 0.2768 - val_acc: 0.9670
Epoch 22/40
 - 4s - loss: 0.2088 - acc: 0.9730 - val_loss: 0.2768 - val_acc: 0.9663
Epoch 23/40
 - 4s - loss: 0.2166 - acc: 0.9728 - val_loss: 0.3160 - val_acc: 0.9660
Epoch 24/40
 - 4s - loss: 0.2116 - acc: 0.9735 - val_loss: 0.3083 - val_acc: 0.9664
Epoch 25/40
 - 4s - loss: 0.2149 - acc: 0.9732 - val_loss: 0.2845 - val_acc: 0.9663
Epoch 26/40
 - 4s - loss: 0.2201 - acc: 0.9730 - val_loss: 0.3252 - val_acc: 0.9635
Epoch 27/40
 - 4s - loss: 0.2140 - acc: 0.9729 - val_loss: 0.3042 - val_acc: 0.9663
Epoch 28/40
 - 4s - loss: 0.2335 - acc: 0.9730 - val_loss: 0.3508 - val_acc: 0.9658
Epoch 29/40
 - 4s - loss: 0.2363 - acc: 0.9728 - val_loss: 0.3953 - val_acc: 0.9668
Epoch 30/40
 - 4s - loss: 0.2199 - acc: 0.9728 - val_loss: 0.2756 - val_acc: 0.9670
Epoch 31/40
 - 4s - loss: 0.2263 - acc: 0.9730 - val_loss: 0.2893 - val_acc: 0.9673
Epoch 32/40
 - 4s - loss: 0.2094 - acc: 0.9734 - val_loss: 0.3044 - val_acc: 0.9667
Epoch 33/40
 - 4s - loss: 0.2179 - acc: 0.9735 - val_loss: 0.2867 - val_acc: 0.9667
Epoch 34/40
 - 4s - loss: 0.2091 - acc: 0.9736 - val_loss: 0.3043 - val_acc: 0.9667
Epoch 35/40
 - 4s - loss: 0.2232 - acc: 0.9737 - val_loss: 0.3166 - val_acc: 0.9665
Epoch 36/40
 - 4s - loss: 0.2122 - acc: 0.9735 - val_loss: 0.2917 - val_acc: 0.9644
Epoch 37/40
 - 4s - loss: 0.2549 - acc: 0.9732 - val_loss: 0.3536 - val_acc: 0.9665
Epoch 38/40
 - 4s - loss: 0.2202 - acc: 0.9736 - val_loss: 0.3276 - val_acc: 0.9665
Epoch 39/40
 - 4s - loss: 0.2350 - acc: 0.9735 - val_loss: 0.3114 - val_acc: 0.9662
Epoch 40/40
 - 4s - loss: 0.2598 - acc: 0.9733 - val_loss: 0.3104 - val_acc: 0.9666
Epoch 1/40
 - 0s - loss: 0.3319 - acc: 0.9653
Epoch 2/40
 - 0s - loss: 0.3429 - acc: 0.9647
Epoch 3/40
 - 0s - loss: 0.3509 - acc: 0.9648
Epoch 4/40
 - 0s - loss: 0.3317 - acc: 0.9652
Epoch 5/40
 - 0s - loss: 0.3290 - acc: 0.9653
Epoch 6/40
 - 0s - loss: 0.3119 - acc: 0.9654
Epoch 7/40
 - 0s - loss: 0.3183 - acc: 0.9651
Epoch 8/40
 - 0s - loss: 0.3061 - acc: 0.9671
Epoch 9/40
 - 0s - loss: 0.3294 - acc: 0.9673
Epoch 10/40
 - 0s - loss: 0.3059 - acc: 0.9663
Epoch 11/40
 - 0s - loss: 0.2750 - acc: 0.9682
Epoch 12/40
 - 0s - loss: 0.2747 - acc: 0.9678
Epoch 13/40
 - 0s - loss: 0.2885 - acc: 0.9679
Epoch 14/40
 - 0s - loss: 0.2910 - acc: 0.9665
Epoch 15/40
 - 0s - loss: 0.2823 - acc: 0.9686
Epoch 16/40
 - 0s - loss: 0.2774 - acc: 0.9681
Epoch 17/40
 - 0s - loss: 0.2766 - acc: 0.9682
Epoch 18/40
 - 0s - loss: 0.2852 - acc: 0.9675
Epoch 19/40
 - 0s - loss: 0.3120 - acc: 0.9675
Epoch 20/40
 - 0s - loss: 0.2931 - acc: 0.9673
Epoch 21/40
 - 0s - loss: 0.2547 - acc: 0.9677
Epoch 22/40
 - 0s - loss: 0.2754 - acc: 0.9686
Epoch 23/40
 - 0s - loss: 0.2928 - acc: 0.9684
Epoch 24/40
 - 0s - loss: 0.2773 - acc: 0.9690
Epoch 25/40
 - 0s - loss: 0.2651 - acc: 0.9684
Epoch 26/40
 - 0s - loss: 0.2682 - acc: 0.9684
Epoch 27/40
 - 0s - loss: 0.2878 - acc: 0.9691
Epoch 28/40
 - 0s - loss: 0.2724 - acc: 0.9697
Epoch 29/40
 - 0s - loss: 0.2688 - acc: 0.9693
Epoch 30/40
 - 0s - loss: 0.2832 - acc: 0.9698
Epoch 31/40
 - 0s - loss: 0.2758 - acc: 0.9694
Epoch 32/40
 - 0s - loss: 0.2741 - acc: 0.9698
Epoch 33/40
 - 0s - loss: 0.2621 - acc: 0.9703
Epoch 34/40
 - 0s - loss: 0.2874 - acc: 0.9693
Epoch 35/40
 - 0s - loss: 0.3083 - acc: 0.9691
Epoch 36/40
 - 0s - loss: 0.2782 - acc: 0.9701
Epoch 37/40
 - 0s - loss: 0.2734 - acc: 0.9703
Epoch 38/40
 - 0s - loss: 0.2793 - acc: 0.9704
Epoch 39/40
 - 0s - loss: 0.2468 - acc: 0.9702
Epoch 40/40
 - 0s - loss: 0.2556 - acc: 0.9698
# Training time = 0:03:38.973980
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_97 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_98 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_97 (Embedding)        (None, 4, 48)        705264      input_97[0][0]                   
__________________________________________________________________________________________________
embedding_98 (Embedding)        (None, 4, 24)        5640        input_98[0][0]                   
__________________________________________________________________________________________________
flatten_97 (Flatten)            (None, 192)          0           embedding_97[0][0]               
__________________________________________________________________________________________________
flatten_98 (Flatten)            (None, 96)           0           embedding_98[0][0]               
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 288)          0           flatten_97[0][0]                 
                                                                 flatten_98[0][0]                 
__________________________________________________________________________________________________
dense_97 (Dense)                (None, 24)           6936        concatenate_49[0][0]             
__________________________________________________________________________________________________
dropout_49 (Dropout)            (None, 24)           0           dense_97[0][0]                   
__________________________________________________________________________________________________
dense_98 (Dense)                (None, 8)            200         dropout_49[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1092 - acc: 0.9610 - val_loss: 0.1043 - val_acc: 0.9627
Epoch 2/40
 - 4s - loss: 0.1088 - acc: 0.9661 - val_loss: 0.1293 - val_acc: 0.9644
Epoch 3/40
 - 4s - loss: 0.1174 - acc: 0.9675 - val_loss: 0.1474 - val_acc: 0.9634
Epoch 4/40
 - 4s - loss: 0.1290 - acc: 0.9689 - val_loss: 0.1470 - val_acc: 0.9675
Epoch 5/40
 - 4s - loss: 0.1303 - acc: 0.9697 - val_loss: 0.2005 - val_acc: 0.9668
Epoch 6/40
 - 4s - loss: 0.1350 - acc: 0.9701 - val_loss: 0.1531 - val_acc: 0.9667
Epoch 7/40
 - 4s - loss: 0.1403 - acc: 0.9705 - val_loss: 0.1810 - val_acc: 0.9674
Epoch 8/40
 - 4s - loss: 0.1455 - acc: 0.9707 - val_loss: 0.1871 - val_acc: 0.9662
Epoch 9/40
 - 4s - loss: 0.1587 - acc: 0.9716 - val_loss: 0.2124 - val_acc: 0.9661
Epoch 10/40
 - 4s - loss: 0.1636 - acc: 0.9720 - val_loss: 0.2341 - val_acc: 0.9663
Epoch 11/40
 - 4s - loss: 0.1826 - acc: 0.9711 - val_loss: 0.2667 - val_acc: 0.9667
Epoch 12/40
 - 4s - loss: 0.1760 - acc: 0.9721 - val_loss: 0.2562 - val_acc: 0.9668
Epoch 13/40
 - 4s - loss: 0.1881 - acc: 0.9720 - val_loss: 0.2598 - val_acc: 0.9669
Epoch 14/40
 - 4s - loss: 0.1849 - acc: 0.9721 - val_loss: 0.3204 - val_acc: 0.9668
Epoch 15/40
 - 4s - loss: 0.1875 - acc: 0.9722 - val_loss: 0.2631 - val_acc: 0.9671
Epoch 16/40
 - 4s - loss: 0.1773 - acc: 0.9723 - val_loss: 0.2614 - val_acc: 0.9671
Epoch 17/40
 - 4s - loss: 0.1894 - acc: 0.9726 - val_loss: 0.2550 - val_acc: 0.9658
Epoch 18/40
 - 4s - loss: 0.1834 - acc: 0.9730 - val_loss: 0.2666 - val_acc: 0.9657
Epoch 19/40
 - 4s - loss: 0.1853 - acc: 0.9726 - val_loss: 0.2762 - val_acc: 0.9679
Epoch 20/40
 - 4s - loss: 0.1881 - acc: 0.9729 - val_loss: 0.2831 - val_acc: 0.9673
Epoch 21/40
 - 4s - loss: 0.1978 - acc: 0.9726 - val_loss: 0.2665 - val_acc: 0.9659
Epoch 22/40
 - 4s - loss: 0.2061 - acc: 0.9731 - val_loss: 0.2973 - val_acc: 0.9662
Epoch 23/40
 - 4s - loss: 0.2121 - acc: 0.9728 - val_loss: 0.3509 - val_acc: 0.9663
Epoch 24/40
 - 4s - loss: 0.2161 - acc: 0.9729 - val_loss: 0.3156 - val_acc: 0.9672
Epoch 25/40
 - 4s - loss: 0.2253 - acc: 0.9732 - val_loss: 0.3166 - val_acc: 0.9660
Epoch 26/40
 - 4s - loss: 0.2291 - acc: 0.9730 - val_loss: 0.3238 - val_acc: 0.9669
Epoch 27/40
 - 4s - loss: 0.2492 - acc: 0.9730 - val_loss: 0.3173 - val_acc: 0.9671
Epoch 28/40
 - 4s - loss: 0.2391 - acc: 0.9732 - val_loss: 0.3208 - val_acc: 0.9667
Epoch 29/40
 - 4s - loss: 0.2570 - acc: 0.9730 - val_loss: 0.3297 - val_acc: 0.9668
Epoch 30/40
 - 4s - loss: 0.3273 - acc: 0.9730 - val_loss: 0.5103 - val_acc: 0.9648
Epoch 31/40
 - 4s - loss: 0.2973 - acc: 0.9733 - val_loss: 0.3526 - val_acc: 0.9659
Epoch 32/40
 - 4s - loss: 0.2687 - acc: 0.9732 - val_loss: 0.3632 - val_acc: 0.9661
Epoch 33/40
 - 4s - loss: 0.3022 - acc: 0.9729 - val_loss: 0.3884 - val_acc: 0.9657
Epoch 34/40
 - 4s - loss: 0.2981 - acc: 0.9730 - val_loss: 0.4296 - val_acc: 0.9661
Epoch 35/40
 - 4s - loss: 0.3391 - acc: 0.9729 - val_loss: 0.5268 - val_acc: 0.9653
Epoch 36/40
 - 4s - loss: 0.3944 - acc: 0.9721 - val_loss: 0.5303 - val_acc: 0.9659
Epoch 37/40
 - 4s - loss: 0.3626 - acc: 0.9729 - val_loss: 0.4578 - val_acc: 0.9652
Epoch 38/40
 - 4s - loss: 0.3601 - acc: 0.9727 - val_loss: 0.4787 - val_acc: 0.9663
Epoch 39/40
 - 4s - loss: 0.3748 - acc: 0.9727 - val_loss: 0.4967 - val_acc: 0.9657
Epoch 40/40
 - 4s - loss: 0.3899 - acc: 0.9724 - val_loss: 0.4996 - val_acc: 0.9668
Epoch 1/40
 - 0s - loss: 0.5194 - acc: 0.9649
Epoch 2/40
 - 0s - loss: 0.5047 - acc: 0.9659
Epoch 3/40
 - 0s - loss: 0.5098 - acc: 0.9654
Epoch 4/40
 - 0s - loss: 0.5248 - acc: 0.9650
Epoch 5/40
 - 0s - loss: 0.5292 - acc: 0.9656
Epoch 6/40
 - 0s - loss: 0.5173 - acc: 0.9661
Epoch 7/40
 - 0s - loss: 0.5264 - acc: 0.9657
Epoch 8/40
 - 0s - loss: 0.5057 - acc: 0.9663
Epoch 9/40
 - 0s - loss: 0.5152 - acc: 0.9662
Epoch 10/40
 - 0s - loss: 0.5089 - acc: 0.9667
Epoch 11/40
 - 0s - loss: 0.5089 - acc: 0.9670
Epoch 12/40
 - 0s - loss: 0.5042 - acc: 0.9666
Epoch 13/40
 - 0s - loss: 0.5160 - acc: 0.9658
Epoch 14/40
 - 0s - loss: 0.5134 - acc: 0.9666
Epoch 15/40
 - 0s - loss: 0.5171 - acc: 0.9670
Epoch 16/40
 - 0s - loss: 0.5091 - acc: 0.9668
Epoch 17/40
 - 0s - loss: 0.5009 - acc: 0.9669
Epoch 18/40
 - 0s - loss: 0.4978 - acc: 0.9673
Epoch 19/40
 - 0s - loss: 0.4875 - acc: 0.9677
Epoch 20/40
 - 0s - loss: 0.4809 - acc: 0.9682
Epoch 21/40
 - 0s - loss: 0.5007 - acc: 0.9669
Epoch 22/40
 - 0s - loss: 0.4961 - acc: 0.9665
Epoch 23/40
 - 0s - loss: 0.4806 - acc: 0.9679
Epoch 24/40
 - 0s - loss: 0.5000 - acc: 0.9656
Epoch 25/40
 - 0s - loss: 0.4919 - acc: 0.9670
Epoch 26/40
 - 0s - loss: 0.4992 - acc: 0.9672
Epoch 27/40
 - 0s - loss: 0.4917 - acc: 0.9676
Epoch 28/40
 - 0s - loss: 0.4870 - acc: 0.9680
Epoch 29/40
 - 0s - loss: 0.4915 - acc: 0.9683
Epoch 30/40
 - 0s - loss: 0.4752 - acc: 0.9686
Epoch 31/40
 - 0s - loss: 0.4956 - acc: 0.9679
Epoch 32/40
 - 0s - loss: 0.5036 - acc: 0.9677
Epoch 33/40
 - 0s - loss: 0.4900 - acc: 0.9687
Epoch 34/40
 - 0s - loss: 0.4844 - acc: 0.9691
Epoch 35/40
 - 0s - loss: 0.4946 - acc: 0.9684
Epoch 36/40
 - 0s - loss: 0.4962 - acc: 0.9682
Epoch 37/40
 - 0s - loss: 0.4882 - acc: 0.9680
Epoch 38/40
 - 0s - loss: 0.4768 - acc: 0.9688
Epoch 39/40
 - 0s - loss: 0.4699 - acc: 0.9691
Epoch 40/40
 - 0s - loss: 0.4671 - acc: 0.9688
# Training time = 0:03:37.274932
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_99 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_100 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_99 (Embedding)        (None, 4, 48)        705264      input_99[0][0]                   
__________________________________________________________________________________________________
embedding_100 (Embedding)       (None, 4, 24)        5640        input_100[0][0]                  
__________________________________________________________________________________________________
flatten_99 (Flatten)            (None, 192)          0           embedding_99[0][0]               
__________________________________________________________________________________________________
flatten_100 (Flatten)           (None, 96)           0           embedding_100[0][0]              
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 288)          0           flatten_99[0][0]                 
                                                                 flatten_100[0][0]                
__________________________________________________________________________________________________
dense_99 (Dense)                (None, 24)           6936        concatenate_50[0][0]             
__________________________________________________________________________________________________
dropout_50 (Dropout)            (None, 24)           0           dense_99[0][0]                   
__________________________________________________________________________________________________
dense_100 (Dense)               (None, 8)            200         dropout_50[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1092 - acc: 0.9605 - val_loss: 0.1061 - val_acc: 0.9651
Epoch 2/40
 - 4s - loss: 0.1044 - acc: 0.9658 - val_loss: 0.1103 - val_acc: 0.9656
Epoch 3/40
 - 4s - loss: 0.1154 - acc: 0.9675 - val_loss: 0.1339 - val_acc: 0.9667
Epoch 4/40
 - 4s - loss: 0.1268 - acc: 0.9688 - val_loss: 0.1986 - val_acc: 0.9665
Epoch 5/40
 - 4s - loss: 0.1275 - acc: 0.9699 - val_loss: 0.1716 - val_acc: 0.9669
Epoch 6/40
 - 4s - loss: 0.1349 - acc: 0.9709 - val_loss: 0.1679 - val_acc: 0.9684
Epoch 7/40
 - 4s - loss: 0.1378 - acc: 0.9713 - val_loss: 0.1566 - val_acc: 0.9676
Epoch 8/40
 - 4s - loss: 0.1436 - acc: 0.9714 - val_loss: 0.1854 - val_acc: 0.9663
Epoch 9/40
 - 4s - loss: 0.1544 - acc: 0.9723 - val_loss: 0.2313 - val_acc: 0.9670
Epoch 10/40
 - 4s - loss: 0.1626 - acc: 0.9725 - val_loss: 0.2594 - val_acc: 0.9671
Epoch 11/40
 - 4s - loss: 0.1731 - acc: 0.9726 - val_loss: 0.3105 - val_acc: 0.9668
Epoch 12/40
 - 4s - loss: 0.1712 - acc: 0.9724 - val_loss: 0.2497 - val_acc: 0.9668
Epoch 13/40
 - 4s - loss: 0.1784 - acc: 0.9731 - val_loss: 0.2520 - val_acc: 0.9675
Epoch 14/40
 - 4s - loss: 0.1877 - acc: 0.9726 - val_loss: 0.2520 - val_acc: 0.9677
Epoch 15/40
 - 4s - loss: 0.1962 - acc: 0.9726 - val_loss: 0.2562 - val_acc: 0.9671
Epoch 16/40
 - 4s - loss: 0.1872 - acc: 0.9730 - val_loss: 0.2858 - val_acc: 0.9664
Epoch 17/40
 - 4s - loss: 0.1982 - acc: 0.9731 - val_loss: 0.2815 - val_acc: 0.9672
Epoch 18/40
 - 4s - loss: 0.1995 - acc: 0.9732 - val_loss: 0.2847 - val_acc: 0.9657
Epoch 19/40
 - 4s - loss: 0.2101 - acc: 0.9733 - val_loss: 0.2864 - val_acc: 0.9659
Epoch 20/40
 - 4s - loss: 0.2011 - acc: 0.9732 - val_loss: 0.3010 - val_acc: 0.9664
Epoch 21/40
 - 4s - loss: 0.2173 - acc: 0.9735 - val_loss: 0.3066 - val_acc: 0.9662
Epoch 22/40
 - 4s - loss: 0.2050 - acc: 0.9734 - val_loss: 0.2852 - val_acc: 0.9676
Epoch 23/40
 - 4s - loss: 0.2084 - acc: 0.9736 - val_loss: 0.3219 - val_acc: 0.9662
Epoch 24/40
 - 4s - loss: 0.2192 - acc: 0.9734 - val_loss: 0.3058 - val_acc: 0.9670
Epoch 25/40
 - 4s - loss: 0.2284 - acc: 0.9735 - val_loss: 0.3404 - val_acc: 0.9664
Epoch 26/40
 - 4s - loss: 0.2140 - acc: 0.9736 - val_loss: 0.3319 - val_acc: 0.9678
Epoch 27/40
 - 4s - loss: 0.2091 - acc: 0.9737 - val_loss: 0.3112 - val_acc: 0.9675
Epoch 28/40
 - 4s - loss: 0.1973 - acc: 0.9738 - val_loss: 0.2999 - val_acc: 0.9664
Epoch 29/40
 - 4s - loss: 0.2118 - acc: 0.9742 - val_loss: 0.3455 - val_acc: 0.9651
Epoch 30/40
 - 4s - loss: 0.2088 - acc: 0.9736 - val_loss: 0.3239 - val_acc: 0.9660
Epoch 31/40
 - 4s - loss: 0.2081 - acc: 0.9741 - val_loss: 0.3156 - val_acc: 0.9672
Epoch 32/40
 - 4s - loss: 0.2165 - acc: 0.9741 - val_loss: 0.3087 - val_acc: 0.9665
Epoch 33/40
 - 4s - loss: 0.2025 - acc: 0.9741 - val_loss: 0.3033 - val_acc: 0.9665
Epoch 34/40
 - 4s - loss: 0.2187 - acc: 0.9740 - val_loss: 0.3494 - val_acc: 0.9661
Epoch 35/40
 - 4s - loss: 0.2181 - acc: 0.9743 - val_loss: 0.3594 - val_acc: 0.9658
Epoch 36/40
 - 4s - loss: 0.2139 - acc: 0.9741 - val_loss: 0.3770 - val_acc: 0.9659
Epoch 37/40
 - 4s - loss: 0.2093 - acc: 0.9739 - val_loss: 0.3082 - val_acc: 0.9658
Epoch 38/40
 - 4s - loss: 0.2253 - acc: 0.9739 - val_loss: 0.3617 - val_acc: 0.9646
Epoch 39/40
 - 4s - loss: 0.2410 - acc: 0.9738 - val_loss: 0.3992 - val_acc: 0.9655
Epoch 40/40
 - 4s - loss: 0.2290 - acc: 0.9737 - val_loss: 0.3322 - val_acc: 0.9653
Epoch 1/40
 - 0s - loss: 0.3525 - acc: 0.9645
Epoch 2/40
 - 0s - loss: 0.3265 - acc: 0.9661
Epoch 3/40
 - 0s - loss: 0.3014 - acc: 0.9663
Epoch 4/40
 - 0s - loss: 0.2995 - acc: 0.9655
Epoch 5/40
 - 0s - loss: 0.3292 - acc: 0.9647
Epoch 6/40
 - 0s - loss: 0.3139 - acc: 0.9653
Epoch 7/40
 - 0s - loss: 0.3419 - acc: 0.9663
Epoch 8/40
 - 0s - loss: 0.3043 - acc: 0.9667
Epoch 9/40
 - 0s - loss: 0.2963 - acc: 0.9668
Epoch 10/40
 - 0s - loss: 0.2917 - acc: 0.9652
Epoch 11/40
 - 0s - loss: 0.2952 - acc: 0.9679
Epoch 12/40
 - 0s - loss: 0.2943 - acc: 0.9681
Epoch 13/40
 - 0s - loss: 0.3220 - acc: 0.9682
Epoch 14/40
 - 0s - loss: 0.3767 - acc: 0.9685
Epoch 15/40
 - 0s - loss: 0.4048 - acc: 0.9688
Epoch 16/40
 - 0s - loss: 0.3527 - acc: 0.9686
Epoch 17/40
 - 0s - loss: 0.3085 - acc: 0.9687
Epoch 18/40
 - 0s - loss: 0.3185 - acc: 0.9683
Epoch 19/40
 - 0s - loss: 0.3203 - acc: 0.9688
Epoch 20/40
 - 0s - loss: 0.3371 - acc: 0.9684
Epoch 21/40
 - 0s - loss: 0.3313 - acc: 0.9687
Epoch 22/40
 - 0s - loss: 0.3872 - acc: 0.9686
Epoch 23/40
 - 0s - loss: 0.3857 - acc: 0.9682
Epoch 24/40
 - 0s - loss: 0.3645 - acc: 0.9692
Epoch 25/40
 - 0s - loss: 0.3932 - acc: 0.9687
Epoch 26/40
 - 0s - loss: 0.4373 - acc: 0.9683
Epoch 27/40
 - 0s - loss: 0.4951 - acc: 0.9673
Epoch 28/40
 - 0s - loss: 0.4643 - acc: 0.9685
Epoch 29/40
 - 0s - loss: 0.4136 - acc: 0.9689
Epoch 30/40
 - 0s - loss: 0.4068 - acc: 0.9685
Epoch 31/40
 - 0s - loss: 0.3946 - acc: 0.9699
Epoch 32/40
 - 0s - loss: 0.4350 - acc: 0.9692
Epoch 33/40
 - 0s - loss: 0.4213 - acc: 0.9695
Epoch 34/40
 - 0s - loss: 0.4398 - acc: 0.9691
Epoch 35/40
 - 0s - loss: 0.4114 - acc: 0.9690
Epoch 36/40
 - 0s - loss: 0.3814 - acc: 0.9696
Epoch 37/40
 - 0s - loss: 0.4307 - acc: 0.9690
Epoch 38/40
 - 0s - loss: 0.3948 - acc: 0.9695
Epoch 39/40
 - 0s - loss: 0.3776 - acc: 0.9701
Epoch 40/40
 - 0s - loss: 0.3968 - acc: 0.9701
# Training time = 0:03:34.151895
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_101 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_102 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_101 (Embedding)       (None, 4, 48)        705264      input_101[0][0]                  
__________________________________________________________________________________________________
embedding_102 (Embedding)       (None, 4, 24)        5640        input_102[0][0]                  
__________________________________________________________________________________________________
flatten_101 (Flatten)           (None, 192)          0           embedding_101[0][0]              
__________________________________________________________________________________________________
flatten_102 (Flatten)           (None, 96)           0           embedding_102[0][0]              
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 288)          0           flatten_101[0][0]                
                                                                 flatten_102[0][0]                
__________________________________________________________________________________________________
dense_101 (Dense)               (None, 24)           6936        concatenate_51[0][0]             
__________________________________________________________________________________________________
dropout_51 (Dropout)            (None, 24)           0           dense_101[0][0]                  
__________________________________________________________________________________________________
dense_102 (Dense)               (None, 8)            200         dropout_51[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1734 - acc: 0.9501 - val_loss: 0.0813 - val_acc: 0.9666
Epoch 2/40
 - 5s - loss: 0.0678 - acc: 0.9688 - val_loss: 0.0718 - val_acc: 0.9677
Epoch 3/40
 - 5s - loss: 0.0550 - acc: 0.9724 - val_loss: 0.0721 - val_acc: 0.9675
Epoch 4/40
 - 5s - loss: 0.0492 - acc: 0.9739 - val_loss: 0.0749 - val_acc: 0.9674
Epoch 5/40
 - 5s - loss: 0.0456 - acc: 0.9746 - val_loss: 0.0776 - val_acc: 0.9680
Epoch 6/40
 - 5s - loss: 0.0435 - acc: 0.9750 - val_loss: 0.0793 - val_acc: 0.9681
Epoch 7/40
 - 5s - loss: 0.0422 - acc: 0.9758 - val_loss: 0.0823 - val_acc: 0.9675
Epoch 8/40
 - 5s - loss: 0.0408 - acc: 0.9757 - val_loss: 0.0846 - val_acc: 0.9679
Epoch 9/40
 - 5s - loss: 0.0397 - acc: 0.9766 - val_loss: 0.0855 - val_acc: 0.9672
Epoch 10/40
 - 5s - loss: 0.0391 - acc: 0.9770 - val_loss: 0.0868 - val_acc: 0.9672
Epoch 11/40
 - 5s - loss: 0.0379 - acc: 0.9766 - val_loss: 0.0920 - val_acc: 0.9666
Epoch 12/40
 - 5s - loss: 0.0376 - acc: 0.9770 - val_loss: 0.0883 - val_acc: 0.9685
Epoch 13/40
 - 5s - loss: 0.0370 - acc: 0.9767 - val_loss: 0.0938 - val_acc: 0.9671
Epoch 14/40
 - 5s - loss: 0.0364 - acc: 0.9772 - val_loss: 0.0941 - val_acc: 0.9678
Epoch 15/40
 - 5s - loss: 0.0359 - acc: 0.9774 - val_loss: 0.0966 - val_acc: 0.9672
Epoch 16/40
 - 5s - loss: 0.0356 - acc: 0.9772 - val_loss: 0.0951 - val_acc: 0.9672
Epoch 17/40
 - 5s - loss: 0.0353 - acc: 0.9776 - val_loss: 0.0989 - val_acc: 0.9677
Epoch 18/40
 - 5s - loss: 0.0349 - acc: 0.9778 - val_loss: 0.1002 - val_acc: 0.9675
Epoch 19/40
 - 5s - loss: 0.0346 - acc: 0.9778 - val_loss: 0.0999 - val_acc: 0.9674
Epoch 20/40
 - 5s - loss: 0.0345 - acc: 0.9776 - val_loss: 0.1030 - val_acc: 0.9674
Epoch 21/40
 - 5s - loss: 0.0341 - acc: 0.9780 - val_loss: 0.0991 - val_acc: 0.9675
Epoch 22/40
 - 5s - loss: 0.0339 - acc: 0.9784 - val_loss: 0.1019 - val_acc: 0.9664
Epoch 23/40
 - 5s - loss: 0.0335 - acc: 0.9779 - val_loss: 0.1015 - val_acc: 0.9674
Epoch 24/40
 - 5s - loss: 0.0335 - acc: 0.9779 - val_loss: 0.1023 - val_acc: 0.9661
Epoch 25/40
 - 5s - loss: 0.0333 - acc: 0.9779 - val_loss: 0.1061 - val_acc: 0.9684
Epoch 26/40
 - 5s - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1064 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0331 - acc: 0.9786 - val_loss: 0.1080 - val_acc: 0.9669
Epoch 28/40
 - 5s - loss: 0.0327 - acc: 0.9783 - val_loss: 0.1086 - val_acc: 0.9671
Epoch 29/40
 - 5s - loss: 0.0328 - acc: 0.9785 - val_loss: 0.1103 - val_acc: 0.9665
Epoch 30/40
 - 5s - loss: 0.0324 - acc: 0.9782 - val_loss: 0.1114 - val_acc: 0.9676
Epoch 31/40
 - 5s - loss: 0.0323 - acc: 0.9785 - val_loss: 0.1084 - val_acc: 0.9651
Epoch 32/40
 - 5s - loss: 0.0324 - acc: 0.9784 - val_loss: 0.1143 - val_acc: 0.9658
Epoch 33/40
 - 5s - loss: 0.0321 - acc: 0.9787 - val_loss: 0.1144 - val_acc: 0.9677
Epoch 34/40
 - 5s - loss: 0.0322 - acc: 0.9785 - val_loss: 0.1104 - val_acc: 0.9672
Epoch 35/40
 - 5s - loss: 0.0321 - acc: 0.9790 - val_loss: 0.1114 - val_acc: 0.9662
Epoch 36/40
 - 5s - loss: 0.0317 - acc: 0.9792 - val_loss: 0.1150 - val_acc: 0.9650
Epoch 37/40
 - 5s - loss: 0.0319 - acc: 0.9785 - val_loss: 0.1160 - val_acc: 0.9658
Epoch 38/40
 - 5s - loss: 0.0319 - acc: 0.9786 - val_loss: 0.1150 - val_acc: 0.9649
Epoch 39/40
 - 5s - loss: 0.0317 - acc: 0.9783 - val_loss: 0.1153 - val_acc: 0.9671
Epoch 40/40
 - 5s - loss: 0.0318 - acc: 0.9789 - val_loss: 0.1148 - val_acc: 0.9683
Epoch 1/40
 - 1s - loss: 0.0967 - acc: 0.9661
Epoch 2/40
 - 1s - loss: 0.0693 - acc: 0.9693
Epoch 3/40
 - 1s - loss: 0.0623 - acc: 0.9701
Epoch 4/40
 - 1s - loss: 0.0559 - acc: 0.9712
Epoch 5/40
 - 1s - loss: 0.0538 - acc: 0.9730
Epoch 6/40
 - 1s - loss: 0.0506 - acc: 0.9735
Epoch 7/40
 - 1s - loss: 0.0493 - acc: 0.9738
Epoch 8/40
 - 1s - loss: 0.0469 - acc: 0.9736
Epoch 9/40
 - 1s - loss: 0.0455 - acc: 0.9746
Epoch 10/40
 - 1s - loss: 0.0452 - acc: 0.9746
Epoch 11/40
 - 1s - loss: 0.0431 - acc: 0.9760
Epoch 12/40
 - 1s - loss: 0.0422 - acc: 0.9760
Epoch 13/40
 - 1s - loss: 0.0421 - acc: 0.9766
Epoch 14/40
 - 1s - loss: 0.0408 - acc: 0.9764
Epoch 15/40
 - 1s - loss: 0.0400 - acc: 0.9751
Epoch 16/40
 - 1s - loss: 0.0386 - acc: 0.9778
Epoch 17/40
 - 1s - loss: 0.0389 - acc: 0.9775
Epoch 18/40
 - 1s - loss: 0.0382 - acc: 0.9766
Epoch 19/40
 - 1s - loss: 0.0369 - acc: 0.9777
Epoch 20/40
 - 1s - loss: 0.0374 - acc: 0.9779
Epoch 21/40
 - 1s - loss: 0.0373 - acc: 0.9772
Epoch 22/40
 - 1s - loss: 0.0360 - acc: 0.9776
Epoch 23/40
 - 1s - loss: 0.0355 - acc: 0.9774
Epoch 24/40
 - 1s - loss: 0.0361 - acc: 0.9778
Epoch 25/40
 - 1s - loss: 0.0351 - acc: 0.9773
Epoch 26/40
 - 1s - loss: 0.0358 - acc: 0.9775
Epoch 27/40
 - 1s - loss: 0.0353 - acc: 0.9778
Epoch 28/40
 - 1s - loss: 0.0349 - acc: 0.9770
Epoch 29/40
 - 1s - loss: 0.0351 - acc: 0.9770
Epoch 30/40
 - 1s - loss: 0.0342 - acc: 0.9782
Epoch 31/40
 - 1s - loss: 0.0337 - acc: 0.9791
Epoch 32/40
 - 1s - loss: 0.0333 - acc: 0.9790
Epoch 33/40
 - 1s - loss: 0.0335 - acc: 0.9783
Epoch 34/40
 - 1s - loss: 0.0327 - acc: 0.9799
Epoch 35/40
 - 1s - loss: 0.0328 - acc: 0.9786
Epoch 36/40
 - 1s - loss: 0.0329 - acc: 0.9779
Epoch 37/40
 - 1s - loss: 0.0330 - acc: 0.9788
Epoch 38/40
 - 1s - loss: 0.0330 - acc: 0.9785
Epoch 39/40
 - 1s - loss: 0.0322 - acc: 0.9779
Epoch 40/40
 - 1s - loss: 0.0326 - acc: 0.9776
# Training time = 0:03:50.676091
# F-Score(Ordinary) = 0.097, Recall: 0.793, Precision: 0.051
# F-Score(ireflv) = 0.305, Recall: 0.793, Precision: 0.189
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_103 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_104 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_103 (Embedding)       (None, 4, 48)        705264      input_103[0][0]                  
__________________________________________________________________________________________________
embedding_104 (Embedding)       (None, 4, 24)        5640        input_104[0][0]                  
__________________________________________________________________________________________________
flatten_103 (Flatten)           (None, 192)          0           embedding_103[0][0]              
__________________________________________________________________________________________________
flatten_104 (Flatten)           (None, 96)           0           embedding_104[0][0]              
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 288)          0           flatten_103[0][0]                
                                                                 flatten_104[0][0]                
__________________________________________________________________________________________________
dense_103 (Dense)               (None, 24)           6936        concatenate_52[0][0]             
__________________________________________________________________________________________________
dropout_52 (Dropout)            (None, 24)           0           dense_103[0][0]                  
__________________________________________________________________________________________________
dense_104 (Dense)               (None, 8)            200         dropout_52[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1798 - acc: 0.9508 - val_loss: 0.0777 - val_acc: 0.9682
Epoch 2/40
 - 5s - loss: 0.0669 - acc: 0.9696 - val_loss: 0.0691 - val_acc: 0.9684
Epoch 3/40
 - 5s - loss: 0.0546 - acc: 0.9720 - val_loss: 0.0709 - val_acc: 0.9673
Epoch 4/40
 - 5s - loss: 0.0490 - acc: 0.9742 - val_loss: 0.0735 - val_acc: 0.9680
Epoch 5/40
 - 5s - loss: 0.0460 - acc: 0.9748 - val_loss: 0.0771 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0435 - acc: 0.9753 - val_loss: 0.0790 - val_acc: 0.9675
Epoch 7/40
 - 5s - loss: 0.0422 - acc: 0.9759 - val_loss: 0.0799 - val_acc: 0.9674
Epoch 8/40
 - 5s - loss: 0.0409 - acc: 0.9763 - val_loss: 0.0831 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0401 - acc: 0.9763 - val_loss: 0.0842 - val_acc: 0.9666
Epoch 10/40
 - 5s - loss: 0.0393 - acc: 0.9763 - val_loss: 0.0880 - val_acc: 0.9675
Epoch 11/40
 - 5s - loss: 0.0382 - acc: 0.9766 - val_loss: 0.0932 - val_acc: 0.9677
Epoch 12/40
 - 5s - loss: 0.0378 - acc: 0.9768 - val_loss: 0.0918 - val_acc: 0.9676
Epoch 13/40
 - 5s - loss: 0.0374 - acc: 0.9773 - val_loss: 0.0924 - val_acc: 0.9671
Epoch 14/40
 - 5s - loss: 0.0367 - acc: 0.9770 - val_loss: 0.0971 - val_acc: 0.9675
Epoch 15/40
 - 5s - loss: 0.0363 - acc: 0.9774 - val_loss: 0.0915 - val_acc: 0.9674
Epoch 16/40
 - 5s - loss: 0.0357 - acc: 0.9774 - val_loss: 0.0961 - val_acc: 0.9671
Epoch 17/40
 - 5s - loss: 0.0353 - acc: 0.9774 - val_loss: 0.0960 - val_acc: 0.9668
Epoch 18/40
 - 5s - loss: 0.0353 - acc: 0.9779 - val_loss: 0.0991 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0346 - acc: 0.9778 - val_loss: 0.0966 - val_acc: 0.9666
Epoch 20/40
 - 5s - loss: 0.0345 - acc: 0.9778 - val_loss: 0.0980 - val_acc: 0.9671
Epoch 21/40
 - 5s - loss: 0.0341 - acc: 0.9780 - val_loss: 0.0998 - val_acc: 0.9651
Epoch 22/40
 - 5s - loss: 0.0340 - acc: 0.9779 - val_loss: 0.1021 - val_acc: 0.9658
Epoch 23/40
 - 5s - loss: 0.0340 - acc: 0.9782 - val_loss: 0.1014 - val_acc: 0.9663
Epoch 24/40
 - 5s - loss: 0.0336 - acc: 0.9782 - val_loss: 0.1061 - val_acc: 0.9650
Epoch 25/40
 - 5s - loss: 0.0335 - acc: 0.9783 - val_loss: 0.1070 - val_acc: 0.9666
Epoch 26/40
 - 5s - loss: 0.0331 - acc: 0.9782 - val_loss: 0.1064 - val_acc: 0.9670
Epoch 27/40
 - 5s - loss: 0.0331 - acc: 0.9785 - val_loss: 0.1045 - val_acc: 0.9660
Epoch 28/40
 - 5s - loss: 0.0330 - acc: 0.9783 - val_loss: 0.1066 - val_acc: 0.9655
Epoch 29/40
 - 5s - loss: 0.0329 - acc: 0.9783 - val_loss: 0.1065 - val_acc: 0.9664
Epoch 30/40
 - 5s - loss: 0.0325 - acc: 0.9789 - val_loss: 0.1084 - val_acc: 0.9668
Epoch 31/40
 - 5s - loss: 0.0325 - acc: 0.9784 - val_loss: 0.1079 - val_acc: 0.9665
Epoch 32/40
 - 5s - loss: 0.0325 - acc: 0.9787 - val_loss: 0.1092 - val_acc: 0.9658
Epoch 33/40
 - 5s - loss: 0.0324 - acc: 0.9786 - val_loss: 0.1090 - val_acc: 0.9656
Epoch 34/40
 - 5s - loss: 0.0323 - acc: 0.9785 - val_loss: 0.1116 - val_acc: 0.9651
Epoch 35/40
 - 5s - loss: 0.0323 - acc: 0.9782 - val_loss: 0.1101 - val_acc: 0.9638
Epoch 36/40
 - 5s - loss: 0.0318 - acc: 0.9790 - val_loss: 0.1117 - val_acc: 0.9646
Epoch 37/40
 - 5s - loss: 0.0320 - acc: 0.9789 - val_loss: 0.1121 - val_acc: 0.9636
Epoch 38/40
 - 5s - loss: 0.0318 - acc: 0.9787 - val_loss: 0.1141 - val_acc: 0.9652
Epoch 39/40
 - 5s - loss: 0.0319 - acc: 0.9785 - val_loss: 0.1165 - val_acc: 0.9640
Epoch 40/40
 - 5s - loss: 0.0320 - acc: 0.9787 - val_loss: 0.1171 - val_acc: 0.9652
Epoch 1/40
 - 1s - loss: 0.0968 - acc: 0.9659
Epoch 2/40
 - 1s - loss: 0.0702 - acc: 0.9666
Epoch 3/40
 - 1s - loss: 0.0614 - acc: 0.9711
Epoch 4/40
 - 1s - loss: 0.0553 - acc: 0.9725
Epoch 5/40
 - 1s - loss: 0.0535 - acc: 0.9725
Epoch 6/40
 - 1s - loss: 0.0502 - acc: 0.9735
Epoch 7/40
 - 1s - loss: 0.0488 - acc: 0.9733
Epoch 8/40
 - 1s - loss: 0.0475 - acc: 0.9754
Epoch 9/40
 - 1s - loss: 0.0450 - acc: 0.9746
Epoch 10/40
 - 1s - loss: 0.0445 - acc: 0.9754
Epoch 11/40
 - 1s - loss: 0.0428 - acc: 0.9774
Epoch 12/40
 - 1s - loss: 0.0417 - acc: 0.9765
Epoch 13/40
 - 1s - loss: 0.0402 - acc: 0.9771
Epoch 14/40
 - 1s - loss: 0.0403 - acc: 0.9771
Epoch 15/40
 - 1s - loss: 0.0396 - acc: 0.9769
Epoch 16/40
 - 1s - loss: 0.0382 - acc: 0.9788
Epoch 17/40
 - 1s - loss: 0.0381 - acc: 0.9778
Epoch 18/40
 - 1s - loss: 0.0379 - acc: 0.9777
Epoch 19/40
 - 1s - loss: 0.0371 - acc: 0.9777
Epoch 20/40
 - 1s - loss: 0.0372 - acc: 0.9778
Epoch 21/40
 - 1s - loss: 0.0367 - acc: 0.9782
Epoch 22/40
 - 1s - loss: 0.0361 - acc: 0.9783
Epoch 23/40
 - 1s - loss: 0.0354 - acc: 0.9788
Epoch 24/40
 - 1s - loss: 0.0366 - acc: 0.9786
Epoch 25/40
 - 1s - loss: 0.0354 - acc: 0.9788
Epoch 26/40
 - 1s - loss: 0.0352 - acc: 0.9791
Epoch 27/40
 - 1s - loss: 0.0348 - acc: 0.9790
Epoch 28/40
 - 1s - loss: 0.0347 - acc: 0.9789
Epoch 29/40
 - 1s - loss: 0.0344 - acc: 0.9789
Epoch 30/40
 - 1s - loss: 0.0343 - acc: 0.9791
Epoch 31/40
 - 1s - loss: 0.0348 - acc: 0.9780
Epoch 32/40
 - 1s - loss: 0.0343 - acc: 0.9794
Epoch 33/40
 - 1s - loss: 0.0353 - acc: 0.9790
Epoch 34/40
 - 1s - loss: 0.0345 - acc: 0.9788
Epoch 35/40
 - 1s - loss: 0.0339 - acc: 0.9792
Epoch 36/40
 - 1s - loss: 0.0347 - acc: 0.9785
Epoch 37/40
 - 1s - loss: 0.0350 - acc: 0.9786
Epoch 38/40
 - 1s - loss: 0.0330 - acc: 0.9795
Epoch 39/40
 - 1s - loss: 0.0340 - acc: 0.9783
Epoch 40/40
 - 1s - loss: 0.0340 - acc: 0.9784
# Training time = 0:03:48.873187
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_105 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_106 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_105 (Embedding)       (None, 4, 48)        705264      input_105[0][0]                  
__________________________________________________________________________________________________
embedding_106 (Embedding)       (None, 4, 24)        5640        input_106[0][0]                  
__________________________________________________________________________________________________
flatten_105 (Flatten)           (None, 192)          0           embedding_105[0][0]              
__________________________________________________________________________________________________
flatten_106 (Flatten)           (None, 96)           0           embedding_106[0][0]              
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 288)          0           flatten_105[0][0]                
                                                                 flatten_106[0][0]                
__________________________________________________________________________________________________
dense_105 (Dense)               (None, 24)           6936        concatenate_53[0][0]             
__________________________________________________________________________________________________
dropout_53 (Dropout)            (None, 24)           0           dense_105[0][0]                  
__________________________________________________________________________________________________
dense_106 (Dense)               (None, 8)            200         dropout_53[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1735 - acc: 0.9515 - val_loss: 0.0794 - val_acc: 0.9663
Epoch 2/40
 - 5s - loss: 0.0670 - acc: 0.9695 - val_loss: 0.0703 - val_acc: 0.9680
Epoch 3/40
 - 5s - loss: 0.0552 - acc: 0.9730 - val_loss: 0.0725 - val_acc: 0.9677
Epoch 4/40
 - 5s - loss: 0.0493 - acc: 0.9736 - val_loss: 0.0740 - val_acc: 0.9675
Epoch 5/40
 - 5s - loss: 0.0456 - acc: 0.9750 - val_loss: 0.0777 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0435 - acc: 0.9752 - val_loss: 0.0792 - val_acc: 0.9680
Epoch 7/40
 - 5s - loss: 0.0423 - acc: 0.9759 - val_loss: 0.0788 - val_acc: 0.9675
Epoch 8/40
 - 5s - loss: 0.0411 - acc: 0.9757 - val_loss: 0.0825 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0399 - acc: 0.9760 - val_loss: 0.0859 - val_acc: 0.9661
Epoch 10/40
 - 5s - loss: 0.0391 - acc: 0.9763 - val_loss: 0.0860 - val_acc: 0.9668
Epoch 11/40
 - 5s - loss: 0.0382 - acc: 0.9765 - val_loss: 0.0905 - val_acc: 0.9666
Epoch 12/40
 - 5s - loss: 0.0377 - acc: 0.9769 - val_loss: 0.0933 - val_acc: 0.9677
Epoch 13/40
 - 5s - loss: 0.0370 - acc: 0.9768 - val_loss: 0.0911 - val_acc: 0.9670
Epoch 14/40
 - 5s - loss: 0.0365 - acc: 0.9775 - val_loss: 0.0989 - val_acc: 0.9678
Epoch 15/40
 - 5s - loss: 0.0361 - acc: 0.9769 - val_loss: 0.0933 - val_acc: 0.9673
Epoch 16/40
 - 5s - loss: 0.0356 - acc: 0.9774 - val_loss: 0.0944 - val_acc: 0.9675
Epoch 17/40
 - 5s - loss: 0.0353 - acc: 0.9772 - val_loss: 0.0947 - val_acc: 0.9671
Epoch 18/40
 - 5s - loss: 0.0349 - acc: 0.9776 - val_loss: 0.0944 - val_acc: 0.9674
Epoch 19/40
 - 5s - loss: 0.0346 - acc: 0.9774 - val_loss: 0.0969 - val_acc: 0.9671
Epoch 20/40
 - 5s - loss: 0.0340 - acc: 0.9780 - val_loss: 0.1012 - val_acc: 0.9673
Epoch 21/40
 - 5s - loss: 0.0342 - acc: 0.9782 - val_loss: 0.0979 - val_acc: 0.9668
Epoch 22/40
 - 5s - loss: 0.0335 - acc: 0.9781 - val_loss: 0.1027 - val_acc: 0.9669
Epoch 23/40
 - 5s - loss: 0.0335 - acc: 0.9781 - val_loss: 0.1044 - val_acc: 0.9660
Epoch 24/40
 - 5s - loss: 0.0332 - acc: 0.9781 - val_loss: 0.1030 - val_acc: 0.9676
Epoch 25/40
 - 5s - loss: 0.0333 - acc: 0.9783 - val_loss: 0.1062 - val_acc: 0.9666
Epoch 26/40
 - 5s - loss: 0.0329 - acc: 0.9784 - val_loss: 0.1044 - val_acc: 0.9672
Epoch 27/40
 - 5s - loss: 0.0327 - acc: 0.9784 - val_loss: 0.1038 - val_acc: 0.9663
Epoch 28/40
 - 5s - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1025 - val_acc: 0.9644
Epoch 29/40
 - 5s - loss: 0.0326 - acc: 0.9786 - val_loss: 0.1077 - val_acc: 0.9653
Epoch 30/40
 - 5s - loss: 0.0324 - acc: 0.9787 - val_loss: 0.1046 - val_acc: 0.9666
Epoch 31/40
 - 5s - loss: 0.0322 - acc: 0.9781 - val_loss: 0.1140 - val_acc: 0.9650
Epoch 32/40
 - 5s - loss: 0.0322 - acc: 0.9791 - val_loss: 0.1138 - val_acc: 0.9663
Epoch 33/40
 - 5s - loss: 0.0320 - acc: 0.9786 - val_loss: 0.1122 - val_acc: 0.9656
Epoch 34/40
 - 5s - loss: 0.0319 - acc: 0.9791 - val_loss: 0.1146 - val_acc: 0.9643
Epoch 35/40
 - 5s - loss: 0.0320 - acc: 0.9789 - val_loss: 0.1124 - val_acc: 0.9654
Epoch 36/40
 - 5s - loss: 0.0318 - acc: 0.9788 - val_loss: 0.1133 - val_acc: 0.9651
Epoch 37/40
 - 5s - loss: 0.0316 - acc: 0.9789 - val_loss: 0.1122 - val_acc: 0.9635
Epoch 38/40
 - 5s - loss: 0.0317 - acc: 0.9790 - val_loss: 0.1144 - val_acc: 0.9639
Epoch 39/40
 - 5s - loss: 0.0317 - acc: 0.9793 - val_loss: 0.1183 - val_acc: 0.9628
Epoch 40/40
 - 5s - loss: 0.0315 - acc: 0.9793 - val_loss: 0.1166 - val_acc: 0.9650
Epoch 1/40
 - 1s - loss: 0.0963 - acc: 0.9640
Epoch 2/40
 - 1s - loss: 0.0710 - acc: 0.9677
Epoch 3/40
 - 1s - loss: 0.0615 - acc: 0.9694
Epoch 4/40
 - 1s - loss: 0.0561 - acc: 0.9708
Epoch 5/40
 - 1s - loss: 0.0527 - acc: 0.9715
Epoch 6/40
 - 1s - loss: 0.0503 - acc: 0.9723
Epoch 7/40
 - 1s - loss: 0.0481 - acc: 0.9743
Epoch 8/40
 - 1s - loss: 0.0454 - acc: 0.9742
Epoch 9/40
 - 1s - loss: 0.0440 - acc: 0.9763
Epoch 10/40
 - 1s - loss: 0.0426 - acc: 0.9753
Epoch 11/40
 - 1s - loss: 0.0419 - acc: 0.9758
Epoch 12/40
 - 1s - loss: 0.0403 - acc: 0.9756
Epoch 13/40
 - 1s - loss: 0.0399 - acc: 0.9766
Epoch 14/40
 - 1s - loss: 0.0391 - acc: 0.9769
Epoch 15/40
 - 1s - loss: 0.0378 - acc: 0.9774
Epoch 16/40
 - 1s - loss: 0.0374 - acc: 0.9767
Epoch 17/40
 - 1s - loss: 0.0360 - acc: 0.9775
Epoch 18/40
 - 1s - loss: 0.0360 - acc: 0.9783
Epoch 19/40
 - 1s - loss: 0.0354 - acc: 0.9774
Epoch 20/40
 - 1s - loss: 0.0351 - acc: 0.9775
Epoch 21/40
 - 1s - loss: 0.0352 - acc: 0.9773
Epoch 22/40
 - 1s - loss: 0.0350 - acc: 0.9787
Epoch 23/40
 - 1s - loss: 0.0345 - acc: 0.9783
Epoch 24/40
 - 1s - loss: 0.0344 - acc: 0.9779
Epoch 25/40
 - 1s - loss: 0.0336 - acc: 0.9772
Epoch 26/40
 - 1s - loss: 0.0329 - acc: 0.9789
Epoch 27/40
 - 1s - loss: 0.0324 - acc: 0.9786
Epoch 28/40
 - 1s - loss: 0.0336 - acc: 0.9798
Epoch 29/40
 - 1s - loss: 0.0335 - acc: 0.9765
Epoch 30/40
 - 1s - loss: 0.0327 - acc: 0.9787
Epoch 31/40
 - 1s - loss: 0.0321 - acc: 0.9779
Epoch 32/40
 - 1s - loss: 0.0327 - acc: 0.9783
Epoch 33/40
 - 1s - loss: 0.0320 - acc: 0.9786
Epoch 34/40
 - 1s - loss: 0.0319 - acc: 0.9782
Epoch 35/40
 - 1s - loss: 0.0322 - acc: 0.9784
Epoch 36/40
 - 1s - loss: 0.0321 - acc: 0.9784
Epoch 37/40
 - 1s - loss: 0.0319 - acc: 0.9795
Epoch 38/40
 - 1s - loss: 0.0320 - acc: 0.9793
Epoch 39/40
 - 1s - loss: 0.0316 - acc: 0.9787
Epoch 40/40
 - 1s - loss: 0.0312 - acc: 0.9785
# Training time = 0:03:54.893723
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_107 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_108 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_107 (Embedding)       (None, 4, 48)        705264      input_107[0][0]                  
__________________________________________________________________________________________________
embedding_108 (Embedding)       (None, 4, 24)        5640        input_108[0][0]                  
__________________________________________________________________________________________________
flatten_107 (Flatten)           (None, 192)          0           embedding_107[0][0]              
__________________________________________________________________________________________________
flatten_108 (Flatten)           (None, 96)           0           embedding_108[0][0]              
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 288)          0           flatten_107[0][0]                
                                                                 flatten_108[0][0]                
__________________________________________________________________________________________________
dense_107 (Dense)               (None, 24)           6936        concatenate_54[0][0]             
__________________________________________________________________________________________________
dropout_54 (Dropout)            (None, 24)           0           dense_107[0][0]                  
__________________________________________________________________________________________________
dense_108 (Dense)               (None, 8)            200         dropout_54[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1868 - acc: 0.9450 - val_loss: 0.0793 - val_acc: 0.9671
Epoch 2/40
 - 5s - loss: 0.0684 - acc: 0.9688 - val_loss: 0.0720 - val_acc: 0.9679
Epoch 3/40
 - 5s - loss: 0.0559 - acc: 0.9719 - val_loss: 0.0713 - val_acc: 0.9679
Epoch 4/40
 - 5s - loss: 0.0499 - acc: 0.9732 - val_loss: 0.0749 - val_acc: 0.9674
Epoch 5/40
 - 5s - loss: 0.0458 - acc: 0.9748 - val_loss: 0.0773 - val_acc: 0.9671
Epoch 6/40
 - 5s - loss: 0.0433 - acc: 0.9753 - val_loss: 0.0790 - val_acc: 0.9677
Epoch 7/40
 - 5s - loss: 0.0418 - acc: 0.9757 - val_loss: 0.0818 - val_acc: 0.9680
Epoch 8/40
 - 5s - loss: 0.0402 - acc: 0.9759 - val_loss: 0.0865 - val_acc: 0.9674
Epoch 9/40
 - 5s - loss: 0.0396 - acc: 0.9761 - val_loss: 0.0856 - val_acc: 0.9675
Epoch 10/40
 - 5s - loss: 0.0386 - acc: 0.9772 - val_loss: 0.0873 - val_acc: 0.9666
Epoch 11/40
 - 5s - loss: 0.0381 - acc: 0.9774 - val_loss: 0.0893 - val_acc: 0.9674
Epoch 12/40
 - 5s - loss: 0.0374 - acc: 0.9773 - val_loss: 0.0904 - val_acc: 0.9676
Epoch 13/40
 - 5s - loss: 0.0367 - acc: 0.9776 - val_loss: 0.0931 - val_acc: 0.9677
Epoch 14/40
 - 5s - loss: 0.0365 - acc: 0.9772 - val_loss: 0.0939 - val_acc: 0.9674
Epoch 15/40
 - 5s - loss: 0.0360 - acc: 0.9773 - val_loss: 0.1007 - val_acc: 0.9678
Epoch 16/40
 - 5s - loss: 0.0355 - acc: 0.9773 - val_loss: 0.0958 - val_acc: 0.9674
Epoch 17/40
 - 5s - loss: 0.0353 - acc: 0.9774 - val_loss: 0.1001 - val_acc: 0.9677
Epoch 18/40
 - 5s - loss: 0.0345 - acc: 0.9783 - val_loss: 0.1016 - val_acc: 0.9673
Epoch 19/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.1023 - val_acc: 0.9683
Epoch 20/40
 - 5s - loss: 0.0341 - acc: 0.9779 - val_loss: 0.1020 - val_acc: 0.9671
Epoch 21/40
 - 5s - loss: 0.0338 - acc: 0.9781 - val_loss: 0.1067 - val_acc: 0.9666
Epoch 22/40
 - 5s - loss: 0.0336 - acc: 0.9781 - val_loss: 0.1073 - val_acc: 0.9679
Epoch 23/40
 - 5s - loss: 0.0334 - acc: 0.9780 - val_loss: 0.1081 - val_acc: 0.9661
Epoch 24/40
 - 5s - loss: 0.0333 - acc: 0.9787 - val_loss: 0.1056 - val_acc: 0.9666
Epoch 25/40
 - 5s - loss: 0.0331 - acc: 0.9778 - val_loss: 0.1135 - val_acc: 0.9667
Epoch 26/40
 - 5s - loss: 0.0327 - acc: 0.9784 - val_loss: 0.1136 - val_acc: 0.9653
Epoch 27/40
 - 5s - loss: 0.0326 - acc: 0.9784 - val_loss: 0.1107 - val_acc: 0.9663
Epoch 28/40
 - 5s - loss: 0.0325 - acc: 0.9778 - val_loss: 0.1168 - val_acc: 0.9667
Epoch 29/40
 - 5s - loss: 0.0326 - acc: 0.9782 - val_loss: 0.1165 - val_acc: 0.9673
Epoch 30/40
 - 5s - loss: 0.0323 - acc: 0.9787 - val_loss: 0.1180 - val_acc: 0.9659
Epoch 31/40
 - 5s - loss: 0.0323 - acc: 0.9789 - val_loss: 0.1174 - val_acc: 0.9643
Epoch 32/40
 - 5s - loss: 0.0322 - acc: 0.9783 - val_loss: 0.1185 - val_acc: 0.9658
Epoch 33/40
 - 5s - loss: 0.0321 - acc: 0.9787 - val_loss: 0.1182 - val_acc: 0.9669
Epoch 34/40
 - 5s - loss: 0.0322 - acc: 0.9788 - val_loss: 0.1178 - val_acc: 0.9657
Epoch 35/40
 - 5s - loss: 0.0319 - acc: 0.9785 - val_loss: 0.1205 - val_acc: 0.9654
Epoch 36/40
 - 5s - loss: 0.0319 - acc: 0.9790 - val_loss: 0.1238 - val_acc: 0.9642
Epoch 37/40
 - 5s - loss: 0.0319 - acc: 0.9791 - val_loss: 0.1195 - val_acc: 0.9637
Epoch 38/40
 - 5s - loss: 0.0317 - acc: 0.9789 - val_loss: 0.1237 - val_acc: 0.9653
Epoch 39/40
 - 5s - loss: 0.0317 - acc: 0.9789 - val_loss: 0.1197 - val_acc: 0.9667
Epoch 40/40
 - 5s - loss: 0.0319 - acc: 0.9790 - val_loss: 0.1234 - val_acc: 0.9663
Epoch 1/40
 - 1s - loss: 0.1005 - acc: 0.9651
Epoch 2/40
 - 1s - loss: 0.0717 - acc: 0.9681
Epoch 3/40
 - 1s - loss: 0.0620 - acc: 0.9697
Epoch 4/40
 - 1s - loss: 0.0576 - acc: 0.9721
Epoch 5/40
 - 1s - loss: 0.0524 - acc: 0.9717
Epoch 6/40
 - 1s - loss: 0.0506 - acc: 0.9725
Epoch 7/40
 - 1s - loss: 0.0487 - acc: 0.9745
Epoch 8/40
 - 1s - loss: 0.0467 - acc: 0.9733
Epoch 9/40
 - 1s - loss: 0.0438 - acc: 0.9746
Epoch 10/40
 - 1s - loss: 0.0436 - acc: 0.9754
Epoch 11/40
 - 1s - loss: 0.0416 - acc: 0.9753
Epoch 12/40
 - 1s - loss: 0.0408 - acc: 0.9756
Epoch 13/40
 - 1s - loss: 0.0398 - acc: 0.9764
Epoch 14/40
 - 1s - loss: 0.0386 - acc: 0.9775
Epoch 15/40
 - 1s - loss: 0.0376 - acc: 0.9775
Epoch 16/40
 - 1s - loss: 0.0371 - acc: 0.9767
Epoch 17/40
 - 1s - loss: 0.0368 - acc: 0.9770
Epoch 18/40
 - 1s - loss: 0.0364 - acc: 0.9770
Epoch 19/40
 - 1s - loss: 0.0361 - acc: 0.9778
Epoch 20/40
 - 1s - loss: 0.0356 - acc: 0.9780
Epoch 21/40
 - 1s - loss: 0.0359 - acc: 0.9784
Epoch 22/40
 - 1s - loss: 0.0343 - acc: 0.9790
Epoch 23/40
 - 1s - loss: 0.0352 - acc: 0.9778
Epoch 24/40
 - 1s - loss: 0.0335 - acc: 0.9771
Epoch 25/40
 - 1s - loss: 0.0332 - acc: 0.9784
Epoch 26/40
 - 1s - loss: 0.0332 - acc: 0.9779
Epoch 27/40
 - 1s - loss: 0.0332 - acc: 0.9773
Epoch 28/40
 - 1s - loss: 0.0338 - acc: 0.9777
Epoch 29/40
 - 1s - loss: 0.0326 - acc: 0.9782
Epoch 30/40
 - 1s - loss: 0.0329 - acc: 0.9786
Epoch 31/40
 - 1s - loss: 0.0319 - acc: 0.9794
Epoch 32/40
 - 1s - loss: 0.0324 - acc: 0.9785
Epoch 33/40
 - 1s - loss: 0.0327 - acc: 0.9786
Epoch 34/40
 - 1s - loss: 0.0326 - acc: 0.9794
Epoch 35/40
 - 1s - loss: 0.0321 - acc: 0.9792
Epoch 36/40
 - 1s - loss: 0.0318 - acc: 0.9792
Epoch 37/40
 - 1s - loss: 0.0321 - acc: 0.9784
Epoch 38/40
 - 1s - loss: 0.0320 - acc: 0.9786
Epoch 39/40
 - 1s - loss: 0.0314 - acc: 0.9786
Epoch 40/40
 - 1s - loss: 0.0311 - acc: 0.9784
# Training time = 0:03:49.760198
# F-Score(Ordinary) = 0.12, Recall: 0.829, Precision: 0.065
# F-Score(lvc) = 0.352, Recall: 0.879, Precision: 0.22
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_109 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_110 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_109 (Embedding)       (None, 4, 48)        705264      input_109[0][0]                  
__________________________________________________________________________________________________
embedding_110 (Embedding)       (None, 4, 24)        5640        input_110[0][0]                  
__________________________________________________________________________________________________
flatten_109 (Flatten)           (None, 192)          0           embedding_109[0][0]              
__________________________________________________________________________________________________
flatten_110 (Flatten)           (None, 96)           0           embedding_110[0][0]              
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 288)          0           flatten_109[0][0]                
                                                                 flatten_110[0][0]                
__________________________________________________________________________________________________
dense_109 (Dense)               (None, 24)           6936        concatenate_55[0][0]             
__________________________________________________________________________________________________
dropout_55 (Dropout)            (None, 24)           0           dense_109[0][0]                  
__________________________________________________________________________________________________
dense_110 (Dense)               (None, 8)            200         dropout_55[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.0005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1796 - acc: 0.9443 - val_loss: 0.0825 - val_acc: 0.9676
Epoch 2/40
 - 5s - loss: 0.0704 - acc: 0.9689 - val_loss: 0.0734 - val_acc: 0.9681
Epoch 3/40
 - 5s - loss: 0.0568 - acc: 0.9716 - val_loss: 0.0724 - val_acc: 0.9679
Epoch 4/40
 - 5s - loss: 0.0503 - acc: 0.9736 - val_loss: 0.0718 - val_acc: 0.9684
Epoch 5/40
 - 5s - loss: 0.0467 - acc: 0.9746 - val_loss: 0.0767 - val_acc: 0.9677
Epoch 6/40
 - 5s - loss: 0.0443 - acc: 0.9745 - val_loss: 0.0783 - val_acc: 0.9680
Epoch 7/40
 - 5s - loss: 0.0427 - acc: 0.9754 - val_loss: 0.0807 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0413 - acc: 0.9757 - val_loss: 0.0832 - val_acc: 0.9671
Epoch 9/40
 - 5s - loss: 0.0401 - acc: 0.9763 - val_loss: 0.0864 - val_acc: 0.9665
Epoch 10/40
 - 5s - loss: 0.0391 - acc: 0.9766 - val_loss: 0.0868 - val_acc: 0.9674
Epoch 11/40
 - 5s - loss: 0.0386 - acc: 0.9765 - val_loss: 0.0917 - val_acc: 0.9671
Epoch 12/40
 - 5s - loss: 0.0379 - acc: 0.9765 - val_loss: 0.0875 - val_acc: 0.9673
Epoch 13/40
 - 5s - loss: 0.0372 - acc: 0.9767 - val_loss: 0.0924 - val_acc: 0.9666
Epoch 14/40
 - 5s - loss: 0.0368 - acc: 0.9770 - val_loss: 0.0933 - val_acc: 0.9678
Epoch 15/40
 - 5s - loss: 0.0362 - acc: 0.9768 - val_loss: 0.0923 - val_acc: 0.9680
Epoch 16/40
 - 5s - loss: 0.0358 - acc: 0.9771 - val_loss: 0.0953 - val_acc: 0.9671
Epoch 17/40
 - 5s - loss: 0.0354 - acc: 0.9773 - val_loss: 0.0945 - val_acc: 0.9673
Epoch 18/40
 - 5s - loss: 0.0351 - acc: 0.9777 - val_loss: 0.0968 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0349 - acc: 0.9778 - val_loss: 0.0968 - val_acc: 0.9668
Epoch 20/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.0964 - val_acc: 0.9674
Epoch 21/40
 - 5s - loss: 0.0343 - acc: 0.9772 - val_loss: 0.0993 - val_acc: 0.9659
Epoch 22/40
 - 5s - loss: 0.0343 - acc: 0.9781 - val_loss: 0.0981 - val_acc: 0.9644
Epoch 23/40
 - 5s - loss: 0.0339 - acc: 0.9780 - val_loss: 0.0991 - val_acc: 0.9672
Epoch 24/40
 - 5s - loss: 0.0335 - acc: 0.9781 - val_loss: 0.1000 - val_acc: 0.9655
Epoch 25/40
 - 5s - loss: 0.0334 - acc: 0.9784 - val_loss: 0.1004 - val_acc: 0.9658
Epoch 26/40
 - 5s - loss: 0.0335 - acc: 0.9786 - val_loss: 0.1060 - val_acc: 0.9659
Epoch 27/40
 - 5s - loss: 0.0332 - acc: 0.9785 - val_loss: 0.1033 - val_acc: 0.9649
Epoch 28/40
 - 5s - loss: 0.0329 - acc: 0.9785 - val_loss: 0.1057 - val_acc: 0.9654
Epoch 29/40
 - 5s - loss: 0.0329 - acc: 0.9786 - val_loss: 0.1088 - val_acc: 0.9662
Epoch 30/40
 - 5s - loss: 0.0328 - acc: 0.9783 - val_loss: 0.1079 - val_acc: 0.9651
Epoch 31/40
 - 5s - loss: 0.0325 - acc: 0.9789 - val_loss: 0.1092 - val_acc: 0.9650
Epoch 32/40
 - 5s - loss: 0.0325 - acc: 0.9787 - val_loss: 0.1084 - val_acc: 0.9636
Epoch 33/40
 - 5s - loss: 0.0324 - acc: 0.9784 - val_loss: 0.1096 - val_acc: 0.9633
Epoch 34/40
 - 5s - loss: 0.0323 - acc: 0.9787 - val_loss: 0.1114 - val_acc: 0.9655
Epoch 35/40
 - 5s - loss: 0.0322 - acc: 0.9785 - val_loss: 0.1079 - val_acc: 0.9662
Epoch 36/40
 - 5s - loss: 0.0322 - acc: 0.9791 - val_loss: 0.1135 - val_acc: 0.9651
Epoch 37/40
 - 5s - loss: 0.0320 - acc: 0.9787 - val_loss: 0.1146 - val_acc: 0.9637
Epoch 38/40
 - 5s - loss: 0.0323 - acc: 0.9789 - val_loss: 0.1114 - val_acc: 0.9633
Epoch 39/40
 - 5s - loss: 0.0318 - acc: 0.9788 - val_loss: 0.1141 - val_acc: 0.9634
Epoch 40/40
 - 5s - loss: 0.0320 - acc: 0.9787 - val_loss: 0.1123 - val_acc: 0.9639
Epoch 1/40
 - 1s - loss: 0.0981 - acc: 0.9647
Epoch 2/40
 - 1s - loss: 0.0701 - acc: 0.9679
Epoch 3/40
 - 1s - loss: 0.0617 - acc: 0.9699
Epoch 4/40
 - 1s - loss: 0.0582 - acc: 0.9710
Epoch 5/40
 - 1s - loss: 0.0544 - acc: 0.9717
Epoch 6/40
 - 1s - loss: 0.0521 - acc: 0.9726
Epoch 7/40
 - 1s - loss: 0.0495 - acc: 0.9744
Epoch 8/40
 - 1s - loss: 0.0485 - acc: 0.9732
Epoch 9/40
 - 1s - loss: 0.0466 - acc: 0.9743
Epoch 10/40
 - 1s - loss: 0.0457 - acc: 0.9747
Epoch 11/40
 - 1s - loss: 0.0448 - acc: 0.9755
Epoch 12/40
 - 1s - loss: 0.0434 - acc: 0.9759
Epoch 13/40
 - 1s - loss: 0.0427 - acc: 0.9755
Epoch 14/40
 - 1s - loss: 0.0412 - acc: 0.9771
Epoch 15/40
 - 1s - loss: 0.0405 - acc: 0.9780
Epoch 16/40
 - 1s - loss: 0.0403 - acc: 0.9776
Epoch 17/40
 - 1s - loss: 0.0389 - acc: 0.9778
Epoch 18/40
 - 1s - loss: 0.0376 - acc: 0.9772
Epoch 19/40
 - 1s - loss: 0.0370 - acc: 0.9783
Epoch 20/40
 - 1s - loss: 0.0360 - acc: 0.9778
Epoch 21/40
 - 1s - loss: 0.0358 - acc: 0.9777
Epoch 22/40
 - 1s - loss: 0.0346 - acc: 0.9787
Epoch 23/40
 - 1s - loss: 0.0341 - acc: 0.9774
Epoch 24/40
 - 1s - loss: 0.0339 - acc: 0.9770
Epoch 25/40
 - 1s - loss: 0.0338 - acc: 0.9780
Epoch 26/40
 - 1s - loss: 0.0337 - acc: 0.9789
Epoch 27/40
 - 1s - loss: 0.0339 - acc: 0.9775
Epoch 28/40
 - 1s - loss: 0.0331 - acc: 0.9786
Epoch 29/40
 - 1s - loss: 0.0330 - acc: 0.9787
Epoch 30/40
 - 1s - loss: 0.0333 - acc: 0.9786
Epoch 31/40
 - 1s - loss: 0.0320 - acc: 0.9784
Epoch 32/40
 - 1s - loss: 0.0322 - acc: 0.9785
Epoch 33/40
 - 1s - loss: 0.0318 - acc: 0.9789
Epoch 34/40
 - 1s - loss: 0.0318 - acc: 0.9784
Epoch 35/40
 - 1s - loss: 0.0322 - acc: 0.9785
Epoch 36/40
 - 1s - loss: 0.0315 - acc: 0.9787
Epoch 37/40
 - 1s - loss: 0.0320 - acc: 0.9794
Epoch 38/40
 - 1s - loss: 0.0316 - acc: 0.9798
Epoch 39/40
 - 1s - loss: 0.0319 - acc: 0.9792
Epoch 40/40
 - 1s - loss: 0.0315 - acc: 0.9792
# Training time = 0:03:49.920754
# F-Score(Ordinary) = 0.592, Recall: 0.763, Precision: 0.483
# F-Score(lvc) = 0.431, Recall: 0.667, Precision: 0.318
# F-Score(ireflv) = 0.625, Recall: 0.756, Precision: 0.533
# F-Score(id) = 0.654, Recall: 0.799, Precision: 0.554
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_111 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_112 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_111 (Embedding)       (None, 4, 48)        705264      input_111[0][0]                  
__________________________________________________________________________________________________
embedding_112 (Embedding)       (None, 4, 24)        5640        input_112[0][0]                  
__________________________________________________________________________________________________
flatten_111 (Flatten)           (None, 192)          0           embedding_111[0][0]              
__________________________________________________________________________________________________
flatten_112 (Flatten)           (None, 96)           0           embedding_112[0][0]              
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 288)          0           flatten_111[0][0]                
                                                                 flatten_112[0][0]                
__________________________________________________________________________________________________
dense_111 (Dense)               (None, 24)           6936        concatenate_56[0][0]             
__________________________________________________________________________________________________
dropout_56 (Dropout)            (None, 24)           0           dense_111[0][0]                  
__________________________________________________________________________________________________
dense_112 (Dense)               (None, 8)            200         dropout_56[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1312 - acc: 0.9571 - val_loss: 0.0764 - val_acc: 0.9670
Epoch 2/40
 - 5s - loss: 0.0607 - acc: 0.9706 - val_loss: 0.0715 - val_acc: 0.9676
Epoch 3/40
 - 5s - loss: 0.0501 - acc: 0.9733 - val_loss: 0.0747 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0456 - acc: 0.9751 - val_loss: 0.0775 - val_acc: 0.9660
Epoch 5/40
 - 5s - loss: 0.0431 - acc: 0.9747 - val_loss: 0.0819 - val_acc: 0.9682
Epoch 6/40
 - 5s - loss: 0.0414 - acc: 0.9756 - val_loss: 0.0835 - val_acc: 0.9679
Epoch 7/40
 - 5s - loss: 0.0402 - acc: 0.9761 - val_loss: 0.0851 - val_acc: 0.9679
Epoch 8/40
 - 5s - loss: 0.0388 - acc: 0.9763 - val_loss: 0.0894 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0380 - acc: 0.9770 - val_loss: 0.0897 - val_acc: 0.9674
Epoch 10/40
 - 5s - loss: 0.0375 - acc: 0.9768 - val_loss: 0.0912 - val_acc: 0.9671
Epoch 11/40
 - 5s - loss: 0.0368 - acc: 0.9771 - val_loss: 0.0988 - val_acc: 0.9669
Epoch 12/40
 - 5s - loss: 0.0363 - acc: 0.9775 - val_loss: 0.0943 - val_acc: 0.9683
Epoch 13/40
 - 5s - loss: 0.0361 - acc: 0.9770 - val_loss: 0.0963 - val_acc: 0.9677
Epoch 14/40
 - 5s - loss: 0.0355 - acc: 0.9775 - val_loss: 0.0977 - val_acc: 0.9672
Epoch 15/40
 - 5s - loss: 0.0349 - acc: 0.9777 - val_loss: 0.1009 - val_acc: 0.9671
Epoch 16/40
 - 5s - loss: 0.0351 - acc: 0.9774 - val_loss: 0.1016 - val_acc: 0.9664
Epoch 17/40
 - 5s - loss: 0.0347 - acc: 0.9778 - val_loss: 0.1046 - val_acc: 0.9671
Epoch 18/40
 - 5s - loss: 0.0339 - acc: 0.9784 - val_loss: 0.1089 - val_acc: 0.9654
Epoch 19/40
 - 5s - loss: 0.0342 - acc: 0.9778 - val_loss: 0.1030 - val_acc: 0.9667
Epoch 20/40
 - 5s - loss: 0.0345 - acc: 0.9775 - val_loss: 0.1011 - val_acc: 0.9678
Epoch 21/40
 - 5s - loss: 0.0337 - acc: 0.9783 - val_loss: 0.0997 - val_acc: 0.9674
Epoch 22/40
 - 5s - loss: 0.0336 - acc: 0.9783 - val_loss: 0.1046 - val_acc: 0.9648
Epoch 23/40
 - 5s - loss: 0.0334 - acc: 0.9778 - val_loss: 0.1052 - val_acc: 0.9664
Epoch 24/40
 - 5s - loss: 0.0335 - acc: 0.9784 - val_loss: 0.1056 - val_acc: 0.9658
Epoch 25/40
 - 5s - loss: 0.0329 - acc: 0.9780 - val_loss: 0.1115 - val_acc: 0.9679
Epoch 26/40
 - 5s - loss: 0.0329 - acc: 0.9786 - val_loss: 0.1141 - val_acc: 0.9670
Epoch 27/40
 - 5s - loss: 0.0330 - acc: 0.9782 - val_loss: 0.1115 - val_acc: 0.9669
Epoch 28/40
 - 5s - loss: 0.0334 - acc: 0.9787 - val_loss: 0.1075 - val_acc: 0.9676
Epoch 29/40
 - 5s - loss: 0.0328 - acc: 0.9785 - val_loss: 0.1131 - val_acc: 0.9667
Epoch 30/40
 - 5s - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1141 - val_acc: 0.9654
Epoch 31/40
 - 5s - loss: 0.0328 - acc: 0.9783 - val_loss: 0.1082 - val_acc: 0.9675
Epoch 32/40
 - 5s - loss: 0.0330 - acc: 0.9786 - val_loss: 0.1123 - val_acc: 0.9648
Epoch 33/40
 - 5s - loss: 0.0326 - acc: 0.9789 - val_loss: 0.1196 - val_acc: 0.9676
Epoch 34/40
 - 5s - loss: 0.0325 - acc: 0.9787 - val_loss: 0.1193 - val_acc: 0.9646
Epoch 35/40
 - 5s - loss: 0.0325 - acc: 0.9785 - val_loss: 0.1102 - val_acc: 0.9641
Epoch 36/40
 - 5s - loss: 0.0324 - acc: 0.9790 - val_loss: 0.1121 - val_acc: 0.9650
Epoch 37/40
 - 5s - loss: 0.0326 - acc: 0.9787 - val_loss: 0.1144 - val_acc: 0.9678
Epoch 38/40
 - 5s - loss: 0.0323 - acc: 0.9787 - val_loss: 0.1141 - val_acc: 0.9658
Epoch 39/40
 - 5s - loss: 0.0327 - acc: 0.9783 - val_loss: 0.1119 - val_acc: 0.9673
Epoch 40/40
 - 5s - loss: 0.0321 - acc: 0.9792 - val_loss: 0.1162 - val_acc: 0.9677
Epoch 1/40
 - 1s - loss: 0.0979 - acc: 0.9652
Epoch 2/40
 - 1s - loss: 0.0731 - acc: 0.9683
Epoch 3/40
 - 1s - loss: 0.0626 - acc: 0.9699
Epoch 4/40
 - 1s - loss: 0.0569 - acc: 0.9706
Epoch 5/40
 - 1s - loss: 0.0541 - acc: 0.9729
Epoch 6/40
 - 1s - loss: 0.0516 - acc: 0.9735
Epoch 7/40
 - 1s - loss: 0.0499 - acc: 0.9726
Epoch 8/40
 - 1s - loss: 0.0479 - acc: 0.9740
Epoch 9/40
 - 1s - loss: 0.0459 - acc: 0.9752
Epoch 10/40
 - 1s - loss: 0.0448 - acc: 0.9750
Epoch 11/40
 - 1s - loss: 0.0438 - acc: 0.9761
Epoch 12/40
 - 1s - loss: 0.0427 - acc: 0.9761
Epoch 13/40
 - 1s - loss: 0.0427 - acc: 0.9763
Epoch 14/40
 - 1s - loss: 0.0410 - acc: 0.9772
Epoch 15/40
 - 1s - loss: 0.0408 - acc: 0.9758
Epoch 16/40
 - 1s - loss: 0.0399 - acc: 0.9774
Epoch 17/40
 - 1s - loss: 0.0390 - acc: 0.9778
Epoch 18/40
 - 1s - loss: 0.0395 - acc: 0.9770
Epoch 19/40
 - 1s - loss: 0.0375 - acc: 0.9774
Epoch 20/40
 - 1s - loss: 0.0389 - acc: 0.9774
Epoch 21/40
 - 1s - loss: 0.0391 - acc: 0.9773
Epoch 22/40
 - 1s - loss: 0.0379 - acc: 0.9779
Epoch 23/40
 - 1s - loss: 0.0371 - acc: 0.9780
Epoch 24/40
 - 1s - loss: 0.0379 - acc: 0.9780
Epoch 25/40
 - 1s - loss: 0.0387 - acc: 0.9773
Epoch 26/40
 - 1s - loss: 0.0371 - acc: 0.9771
Epoch 27/40
 - 1s - loss: 0.0371 - acc: 0.9778
Epoch 28/40
 - 1s - loss: 0.0346 - acc: 0.9775
Epoch 29/40
 - 1s - loss: 0.0353 - acc: 0.9787
Epoch 30/40
 - 1s - loss: 0.0350 - acc: 0.9779
Epoch 31/40
 - 1s - loss: 0.0350 - acc: 0.9795
Epoch 32/40
 - 1s - loss: 0.0352 - acc: 0.9789
Epoch 33/40
 - 1s - loss: 0.0347 - acc: 0.9784
Epoch 34/40
 - 1s - loss: 0.0335 - acc: 0.9794
Epoch 35/40
 - 1s - loss: 0.0347 - acc: 0.9788
Epoch 36/40
 - 0s - loss: 0.0349 - acc: 0.9784
Epoch 37/40
 - 1s - loss: 0.0338 - acc: 0.9786
Epoch 38/40
 - 1s - loss: 0.0338 - acc: 0.9784
Epoch 39/40
 - 1s - loss: 0.0344 - acc: 0.9776
Epoch 40/40
 - 1s - loss: 0.0330 - acc: 0.9782
# Training time = 0:03:48.696895
# F-Score(Ordinary) = 0.071, Recall: 0.5, Precision: 0.038
# F-Score(ireflv) = 0.076, Recall: 0.556, Precision: 0.041
# F-Score(id) = 0.101, Recall: 0.458, Precision: 0.057
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_113 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_114 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_113 (Embedding)       (None, 4, 48)        705264      input_113[0][0]                  
__________________________________________________________________________________________________
embedding_114 (Embedding)       (None, 4, 24)        5640        input_114[0][0]                  
__________________________________________________________________________________________________
flatten_113 (Flatten)           (None, 192)          0           embedding_113[0][0]              
__________________________________________________________________________________________________
flatten_114 (Flatten)           (None, 96)           0           embedding_114[0][0]              
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 288)          0           flatten_113[0][0]                
                                                                 flatten_114[0][0]                
__________________________________________________________________________________________________
dense_113 (Dense)               (None, 24)           6936        concatenate_57[0][0]             
__________________________________________________________________________________________________
dropout_57 (Dropout)            (None, 24)           0           dense_113[0][0]                  
__________________________________________________________________________________________________
dense_114 (Dense)               (None, 8)            200         dropout_57[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1322 - acc: 0.9583 - val_loss: 0.0698 - val_acc: 0.9683
Epoch 2/40
 - 5s - loss: 0.0590 - acc: 0.9715 - val_loss: 0.0701 - val_acc: 0.9676
Epoch 3/40
 - 5s - loss: 0.0500 - acc: 0.9737 - val_loss: 0.0740 - val_acc: 0.9675
Epoch 4/40
 - 5s - loss: 0.0460 - acc: 0.9752 - val_loss: 0.0765 - val_acc: 0.9677
Epoch 5/40
 - 5s - loss: 0.0434 - acc: 0.9756 - val_loss: 0.0825 - val_acc: 0.9673
Epoch 6/40
 - 5s - loss: 0.0415 - acc: 0.9762 - val_loss: 0.0845 - val_acc: 0.9678
Epoch 7/40
 - 5s - loss: 0.0401 - acc: 0.9763 - val_loss: 0.0864 - val_acc: 0.9670
Epoch 8/40
 - 5s - loss: 0.0390 - acc: 0.9766 - val_loss: 0.0905 - val_acc: 0.9675
Epoch 9/40
 - 5s - loss: 0.0383 - acc: 0.9771 - val_loss: 0.0917 - val_acc: 0.9669
Epoch 10/40
 - 5s - loss: 0.0375 - acc: 0.9767 - val_loss: 0.0947 - val_acc: 0.9669
Epoch 11/40
 - 5s - loss: 0.0369 - acc: 0.9776 - val_loss: 0.0980 - val_acc: 0.9677
Epoch 12/40
 - 5s - loss: 0.0362 - acc: 0.9775 - val_loss: 0.0995 - val_acc: 0.9678
Epoch 13/40
 - 5s - loss: 0.0361 - acc: 0.9773 - val_loss: 0.0991 - val_acc: 0.9660
Epoch 14/40
 - 5s - loss: 0.0354 - acc: 0.9773 - val_loss: 0.1037 - val_acc: 0.9680
Epoch 15/40
 - 5s - loss: 0.0353 - acc: 0.9776 - val_loss: 0.1000 - val_acc: 0.9667
Epoch 16/40
 - 5s - loss: 0.0348 - acc: 0.9779 - val_loss: 0.1005 - val_acc: 0.9664
Epoch 17/40
 - 5s - loss: 0.0348 - acc: 0.9774 - val_loss: 0.1012 - val_acc: 0.9656
Epoch 18/40
 - 5s - loss: 0.0346 - acc: 0.9780 - val_loss: 0.1039 - val_acc: 0.9661
Epoch 19/40
 - 5s - loss: 0.0342 - acc: 0.9778 - val_loss: 0.1056 - val_acc: 0.9657
Epoch 20/40
 - 5s - loss: 0.0340 - acc: 0.9779 - val_loss: 0.1050 - val_acc: 0.9664
Epoch 21/40
 - 5s - loss: 0.0337 - acc: 0.9779 - val_loss: 0.1083 - val_acc: 0.9657
Epoch 22/40
 - 5s - loss: 0.0334 - acc: 0.9781 - val_loss: 0.1197 - val_acc: 0.9649
Epoch 23/40
 - 5s - loss: 0.0334 - acc: 0.9779 - val_loss: 0.1098 - val_acc: 0.9649
Epoch 24/40
 - 5s - loss: 0.0331 - acc: 0.9786 - val_loss: 0.1144 - val_acc: 0.9656
Epoch 25/40
 - 5s - loss: 0.0333 - acc: 0.9785 - val_loss: 0.1152 - val_acc: 0.9650
Epoch 26/40
 - 5s - loss: 0.0327 - acc: 0.9787 - val_loss: 0.1154 - val_acc: 0.9663
Epoch 27/40
 - 5s - loss: 0.0331 - acc: 0.9785 - val_loss: 0.1143 - val_acc: 0.9651
Epoch 28/40
 - 5s - loss: 0.0328 - acc: 0.9783 - val_loss: 0.1118 - val_acc: 0.9647
Epoch 29/40
 - 5s - loss: 0.0328 - acc: 0.9782 - val_loss: 0.1154 - val_acc: 0.9653
Epoch 30/40
 - 5s - loss: 0.0327 - acc: 0.9785 - val_loss: 0.1109 - val_acc: 0.9662
Epoch 31/40
 - 5s - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1172 - val_acc: 0.9660
Epoch 32/40
 - 5s - loss: 0.0324 - acc: 0.9788 - val_loss: 0.1106 - val_acc: 0.9677
Epoch 33/40
 - 5s - loss: 0.0324 - acc: 0.9788 - val_loss: 0.1130 - val_acc: 0.9672
Epoch 34/40
 - 5s - loss: 0.0322 - acc: 0.9788 - val_loss: 0.1133 - val_acc: 0.9663
Epoch 35/40
 - 5s - loss: 0.0325 - acc: 0.9788 - val_loss: 0.1148 - val_acc: 0.9652
Epoch 36/40
 - 5s - loss: 0.0324 - acc: 0.9785 - val_loss: 0.1148 - val_acc: 0.9664
Epoch 37/40
 - 5s - loss: 0.0324 - acc: 0.9792 - val_loss: 0.1110 - val_acc: 0.9642
Epoch 38/40
 - 5s - loss: 0.0324 - acc: 0.9787 - val_loss: 0.1148 - val_acc: 0.9659
Epoch 39/40
 - 5s - loss: 0.0320 - acc: 0.9787 - val_loss: 0.1147 - val_acc: 0.9679
Epoch 40/40
 - 5s - loss: 0.0322 - acc: 0.9785 - val_loss: 0.1147 - val_acc: 0.9672
Epoch 1/40
 - 0s - loss: 0.1023 - acc: 0.9645
Epoch 2/40
 - 0s - loss: 0.0726 - acc: 0.9682
Epoch 3/40
 - 0s - loss: 0.0639 - acc: 0.9709
Epoch 4/40
 - 0s - loss: 0.0567 - acc: 0.9717
Epoch 5/40
 - 0s - loss: 0.0545 - acc: 0.9732
Epoch 6/40
 - 0s - loss: 0.0523 - acc: 0.9728
Epoch 7/40
 - 0s - loss: 0.0505 - acc: 0.9738
Epoch 8/40
 - 0s - loss: 0.0490 - acc: 0.9757
Epoch 9/40
 - 0s - loss: 0.0477 - acc: 0.9742
Epoch 10/40
 - 1s - loss: 0.0472 - acc: 0.9762
Epoch 11/40
 - 1s - loss: 0.0459 - acc: 0.9762
Epoch 12/40
 - 1s - loss: 0.0432 - acc: 0.9767
Epoch 13/40
 - 0s - loss: 0.0422 - acc: 0.9762
Epoch 14/40
 - 0s - loss: 0.0416 - acc: 0.9759
Epoch 15/40
 - 1s - loss: 0.0414 - acc: 0.9754
Epoch 16/40
 - 0s - loss: 0.0399 - acc: 0.9771
Epoch 17/40
 - 0s - loss: 0.0412 - acc: 0.9772
Epoch 18/40
 - 1s - loss: 0.0406 - acc: 0.9767
Epoch 19/40
 - 0s - loss: 0.0404 - acc: 0.9768
Epoch 20/40
 - 1s - loss: 0.0402 - acc: 0.9770
Epoch 21/40
 - 1s - loss: 0.0399 - acc: 0.9772
Epoch 22/40
 - 1s - loss: 0.0375 - acc: 0.9779
Epoch 23/40
 - 1s - loss: 0.0373 - acc: 0.9781
Epoch 24/40
 - 0s - loss: 0.0385 - acc: 0.9779
Epoch 25/40
 - 0s - loss: 0.0360 - acc: 0.9794
Epoch 26/40
 - 1s - loss: 0.0363 - acc: 0.9788
Epoch 27/40
 - 1s - loss: 0.0360 - acc: 0.9780
Epoch 28/40
 - 0s - loss: 0.0359 - acc: 0.9783
Epoch 29/40
 - 1s - loss: 0.0362 - acc: 0.9781
Epoch 30/40
 - 0s - loss: 0.0356 - acc: 0.9781
Epoch 31/40
 - 1s - loss: 0.0353 - acc: 0.9786
Epoch 32/40
 - 0s - loss: 0.0359 - acc: 0.9782
Epoch 33/40
 - 1s - loss: 0.0352 - acc: 0.9799
Epoch 34/40
 - 0s - loss: 0.0352 - acc: 0.9783
Epoch 35/40
 - 1s - loss: 0.0344 - acc: 0.9781
Epoch 36/40
 - 0s - loss: 0.0350 - acc: 0.9788
Epoch 37/40
 - 0s - loss: 0.0352 - acc: 0.9777
Epoch 38/40
 - 0s - loss: 0.0338 - acc: 0.9789
Epoch 39/40
 - 1s - loss: 0.0341 - acc: 0.9777
Epoch 40/40
 - 1s - loss: 0.0347 - acc: 0.9785
# Training time = 0:03:48.446793
# F-Score(Ordinary) = 0.031, Recall: 0.7, Precision: 0.016
# F-Score(lvc) = 0.099, Recall: 0.778, Precision: 0.053
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_115 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_116 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_115 (Embedding)       (None, 4, 48)        705264      input_115[0][0]                  
__________________________________________________________________________________________________
embedding_116 (Embedding)       (None, 4, 24)        5640        input_116[0][0]                  
__________________________________________________________________________________________________
flatten_115 (Flatten)           (None, 192)          0           embedding_115[0][0]              
__________________________________________________________________________________________________
flatten_116 (Flatten)           (None, 96)           0           embedding_116[0][0]              
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 288)          0           flatten_115[0][0]                
                                                                 flatten_116[0][0]                
__________________________________________________________________________________________________
dense_115 (Dense)               (None, 24)           6936        concatenate_58[0][0]             
__________________________________________________________________________________________________
dropout_58 (Dropout)            (None, 24)           0           dense_115[0][0]                  
__________________________________________________________________________________________________
dense_116 (Dense)               (None, 8)            200         dropout_58[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1291 - acc: 0.9583 - val_loss: 0.0729 - val_acc: 0.9668
Epoch 2/40
 - 5s - loss: 0.0588 - acc: 0.9715 - val_loss: 0.0706 - val_acc: 0.9674
Epoch 3/40
 - 5s - loss: 0.0498 - acc: 0.9739 - val_loss: 0.0746 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0456 - acc: 0.9747 - val_loss: 0.0766 - val_acc: 0.9681
Epoch 5/40
 - 5s - loss: 0.0427 - acc: 0.9753 - val_loss: 0.0813 - val_acc: 0.9678
Epoch 6/40
 - 5s - loss: 0.0410 - acc: 0.9757 - val_loss: 0.0822 - val_acc: 0.9677
Epoch 7/40
 - 5s - loss: 0.0399 - acc: 0.9767 - val_loss: 0.0836 - val_acc: 0.9675
Epoch 8/40
 - 5s - loss: 0.0387 - acc: 0.9769 - val_loss: 0.0896 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0376 - acc: 0.9767 - val_loss: 0.0928 - val_acc: 0.9668
Epoch 10/40
 - 5s - loss: 0.0372 - acc: 0.9771 - val_loss: 0.0910 - val_acc: 0.9663
Epoch 11/40
 - 5s - loss: 0.0364 - acc: 0.9771 - val_loss: 0.0957 - val_acc: 0.9667
Epoch 12/40
 - 5s - loss: 0.0358 - acc: 0.9774 - val_loss: 0.0969 - val_acc: 0.9682
Epoch 13/40
 - 5s - loss: 0.0356 - acc: 0.9774 - val_loss: 0.0976 - val_acc: 0.9670
Epoch 14/40
 - 5s - loss: 0.0350 - acc: 0.9777 - val_loss: 0.1024 - val_acc: 0.9677
Epoch 15/40
 - 5s - loss: 0.0349 - acc: 0.9776 - val_loss: 0.0988 - val_acc: 0.9659
Epoch 16/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.0953 - val_acc: 0.9670
Epoch 17/40
 - 5s - loss: 0.0343 - acc: 0.9778 - val_loss: 0.1012 - val_acc: 0.9669
Epoch 18/40
 - 5s - loss: 0.0342 - acc: 0.9780 - val_loss: 0.1013 - val_acc: 0.9666
Epoch 19/40
 - 5s - loss: 0.0340 - acc: 0.9779 - val_loss: 0.1027 - val_acc: 0.9666
Epoch 20/40
 - 5s - loss: 0.0337 - acc: 0.9783 - val_loss: 0.1048 - val_acc: 0.9664
Epoch 21/40
 - 5s - loss: 0.0338 - acc: 0.9783 - val_loss: 0.1012 - val_acc: 0.9665
Epoch 22/40
 - 5s - loss: 0.0333 - acc: 0.9782 - val_loss: 0.1063 - val_acc: 0.9666
Epoch 23/40
 - 5s - loss: 0.0334 - acc: 0.9783 - val_loss: 0.1086 - val_acc: 0.9656
Epoch 24/40
 - 5s - loss: 0.0331 - acc: 0.9786 - val_loss: 0.1103 - val_acc: 0.9659
Epoch 25/40
 - 5s - loss: 0.0333 - acc: 0.9784 - val_loss: 0.1116 - val_acc: 0.9652
Epoch 26/40
 - 5s - loss: 0.0331 - acc: 0.9783 - val_loss: 0.1058 - val_acc: 0.9660
Epoch 27/40
 - 5s - loss: 0.0327 - acc: 0.9786 - val_loss: 0.1118 - val_acc: 0.9668
Epoch 28/40
 - 5s - loss: 0.0327 - acc: 0.9789 - val_loss: 0.1082 - val_acc: 0.9649
Epoch 29/40
 - 5s - loss: 0.0326 - acc: 0.9787 - val_loss: 0.1094 - val_acc: 0.9659
Epoch 30/40
 - 5s - loss: 0.0327 - acc: 0.9788 - val_loss: 0.1076 - val_acc: 0.9665
Epoch 31/40
 - 5s - loss: 0.0324 - acc: 0.9785 - val_loss: 0.1157 - val_acc: 0.9645
Epoch 32/40
 - 5s - loss: 0.0324 - acc: 0.9792 - val_loss: 0.1161 - val_acc: 0.9673
Epoch 33/40
 - 5s - loss: 0.0325 - acc: 0.9783 - val_loss: 0.1127 - val_acc: 0.9669
Epoch 34/40
 - 5s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1095 - val_acc: 0.9627
Epoch 35/40
 - 5s - loss: 0.0321 - acc: 0.9787 - val_loss: 0.1149 - val_acc: 0.9665
Epoch 36/40
 - 5s - loss: 0.0320 - acc: 0.9789 - val_loss: 0.1138 - val_acc: 0.9658
Epoch 37/40
 - 5s - loss: 0.0320 - acc: 0.9789 - val_loss: 0.1156 - val_acc: 0.9655
Epoch 38/40
 - 5s - loss: 0.0322 - acc: 0.9788 - val_loss: 0.1197 - val_acc: 0.9651
Epoch 39/40
 - 5s - loss: 0.0322 - acc: 0.9786 - val_loss: 0.1195 - val_acc: 0.9639
Epoch 40/40
 - 5s - loss: 0.0323 - acc: 0.9787 - val_loss: 0.1171 - val_acc: 0.9655
Epoch 1/40
 - 1s - loss: 0.1001 - acc: 0.9632
Epoch 2/40
 - 1s - loss: 0.0713 - acc: 0.9685
Epoch 3/40
 - 1s - loss: 0.0615 - acc: 0.9706
Epoch 4/40
 - 1s - loss: 0.0559 - acc: 0.9728
Epoch 5/40
 - 1s - loss: 0.0527 - acc: 0.9728
Epoch 6/40
 - 1s - loss: 0.0501 - acc: 0.9734
Epoch 7/40
 - 1s - loss: 0.0463 - acc: 0.9758
Epoch 8/40
 - 1s - loss: 0.0449 - acc: 0.9753
Epoch 9/40
 - 1s - loss: 0.0446 - acc: 0.9761
Epoch 10/40
 - 1s - loss: 0.0427 - acc: 0.9760
Epoch 11/40
 - 1s - loss: 0.0422 - acc: 0.9757
Epoch 12/40
 - 1s - loss: 0.0416 - acc: 0.9759
Epoch 13/40
 - 1s - loss: 0.0394 - acc: 0.9781
Epoch 14/40
 - 1s - loss: 0.0397 - acc: 0.9770
Epoch 15/40
 - 1s - loss: 0.0399 - acc: 0.9771
Epoch 16/40
 - 1s - loss: 0.0382 - acc: 0.9784
Epoch 17/40
 - 1s - loss: 0.0387 - acc: 0.9772
Epoch 18/40
 - 1s - loss: 0.0369 - acc: 0.9788
Epoch 19/40
 - 1s - loss: 0.0367 - acc: 0.9770
Epoch 20/40
 - 1s - loss: 0.0364 - acc: 0.9782
Epoch 21/40
 - 1s - loss: 0.0360 - acc: 0.9775
Epoch 22/40
 - 1s - loss: 0.0357 - acc: 0.9792
Epoch 23/40
 - 1s - loss: 0.0364 - acc: 0.9781
Epoch 24/40
 - 1s - loss: 0.0361 - acc: 0.9779
Epoch 25/40
 - 1s - loss: 0.0356 - acc: 0.9772
Epoch 26/40
 - 1s - loss: 0.0353 - acc: 0.9789
Epoch 27/40
 - 1s - loss: 0.0351 - acc: 0.9786
Epoch 28/40
 - 1s - loss: 0.0349 - acc: 0.9792
Epoch 29/40
 - 1s - loss: 0.0359 - acc: 0.9778
Epoch 30/40
 - 1s - loss: 0.0353 - acc: 0.9790
Epoch 31/40
 - 1s - loss: 0.0344 - acc: 0.9791
Epoch 32/40
 - 1s - loss: 0.0342 - acc: 0.9786
Epoch 33/40
 - 1s - loss: 0.0342 - acc: 0.9793
Epoch 34/40
 - 1s - loss: 0.0341 - acc: 0.9777
Epoch 35/40
 - 1s - loss: 0.0335 - acc: 0.9782
Epoch 36/40
 - 1s - loss: 0.0343 - acc: 0.9776
Epoch 37/40
 - 1s - loss: 0.0336 - acc: 0.9793
Epoch 38/40
 - 1s - loss: 0.0335 - acc: 0.9793
Epoch 39/40
 - 1s - loss: 0.0329 - acc: 0.9786
Epoch 40/40
 - 1s - loss: 0.0328 - acc: 0.9787
# Training time = 0:03:55.057309
# F-Score(Ordinary) = 0.009, Recall: 0.2, Precision: 0.004
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_117 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_118 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_117 (Embedding)       (None, 4, 48)        705264      input_117[0][0]                  
__________________________________________________________________________________________________
embedding_118 (Embedding)       (None, 4, 24)        5640        input_118[0][0]                  
__________________________________________________________________________________________________
flatten_117 (Flatten)           (None, 192)          0           embedding_117[0][0]              
__________________________________________________________________________________________________
flatten_118 (Flatten)           (None, 96)           0           embedding_118[0][0]              
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 288)          0           flatten_117[0][0]                
                                                                 flatten_118[0][0]                
__________________________________________________________________________________________________
dense_117 (Dense)               (None, 24)           6936        concatenate_59[0][0]             
__________________________________________________________________________________________________
dropout_59 (Dropout)            (None, 24)           0           dense_117[0][0]                  
__________________________________________________________________________________________________
dense_118 (Dense)               (None, 8)            200         dropout_59[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1380 - acc: 0.9541 - val_loss: 0.0722 - val_acc: 0.9676
Epoch 2/40
 - 5s - loss: 0.0602 - acc: 0.9706 - val_loss: 0.0716 - val_acc: 0.9684
Epoch 3/40
 - 5s - loss: 0.0501 - acc: 0.9734 - val_loss: 0.0732 - val_acc: 0.9679
Epoch 4/40
 - 5s - loss: 0.0455 - acc: 0.9746 - val_loss: 0.0810 - val_acc: 0.9673
Epoch 5/40
 - 5s - loss: 0.0427 - acc: 0.9759 - val_loss: 0.0825 - val_acc: 0.9673
Epoch 6/40
 - 5s - loss: 0.0411 - acc: 0.9757 - val_loss: 0.0837 - val_acc: 0.9681
Epoch 7/40
 - 5s - loss: 0.0399 - acc: 0.9762 - val_loss: 0.0851 - val_acc: 0.9678
Epoch 8/40
 - 5s - loss: 0.0386 - acc: 0.9763 - val_loss: 0.0923 - val_acc: 0.9673
Epoch 9/40
 - 5s - loss: 0.0378 - acc: 0.9767 - val_loss: 0.0934 - val_acc: 0.9672
Epoch 10/40
 - 5s - loss: 0.0370 - acc: 0.9774 - val_loss: 0.0937 - val_acc: 0.9662
Epoch 11/40
 - 5s - loss: 0.0370 - acc: 0.9773 - val_loss: 0.0964 - val_acc: 0.9672
Epoch 12/40
 - 5s - loss: 0.0363 - acc: 0.9776 - val_loss: 0.0995 - val_acc: 0.9672
Epoch 13/40
 - 5s - loss: 0.0358 - acc: 0.9776 - val_loss: 0.1020 - val_acc: 0.9680
Epoch 14/40
 - 5s - loss: 0.0356 - acc: 0.9772 - val_loss: 0.1036 - val_acc: 0.9677
Epoch 15/40
 - 5s - loss: 0.0352 - acc: 0.9774 - val_loss: 0.1046 - val_acc: 0.9676
Epoch 16/40
 - 5s - loss: 0.0349 - acc: 0.9777 - val_loss: 0.0991 - val_acc: 0.9667
Epoch 17/40
 - 5s - loss: 0.0348 - acc: 0.9774 - val_loss: 0.1051 - val_acc: 0.9656
Epoch 18/40
 - 5s - loss: 0.0341 - acc: 0.9784 - val_loss: 0.1096 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0340 - acc: 0.9780 - val_loss: 0.1136 - val_acc: 0.9671
Epoch 20/40
 - 5s - loss: 0.0340 - acc: 0.9778 - val_loss: 0.1111 - val_acc: 0.9657
Epoch 21/40
 - 5s - loss: 0.0339 - acc: 0.9784 - val_loss: 0.1147 - val_acc: 0.9660
Epoch 22/40
 - 5s - loss: 0.0338 - acc: 0.9779 - val_loss: 0.1141 - val_acc: 0.9673
Epoch 23/40
 - 5s - loss: 0.0336 - acc: 0.9778 - val_loss: 0.1164 - val_acc: 0.9656
Epoch 24/40
 - 5s - loss: 0.0337 - acc: 0.9787 - val_loss: 0.1157 - val_acc: 0.9648
Epoch 25/40
 - 5s - loss: 0.0331 - acc: 0.9779 - val_loss: 0.1215 - val_acc: 0.9664
Epoch 26/40
 - 5s - loss: 0.0332 - acc: 0.9787 - val_loss: 0.1202 - val_acc: 0.9636
Epoch 27/40
 - 5s - loss: 0.0331 - acc: 0.9780 - val_loss: 0.1145 - val_acc: 0.9668
Epoch 28/40
 - 5s - loss: 0.0329 - acc: 0.9781 - val_loss: 0.1143 - val_acc: 0.9677
Epoch 29/40
 - 5s - loss: 0.0334 - acc: 0.9780 - val_loss: 0.1212 - val_acc: 0.9678
Epoch 30/40
 - 5s - loss: 0.0327 - acc: 0.9785 - val_loss: 0.1206 - val_acc: 0.9656
Epoch 31/40
 - 5s - loss: 0.0329 - acc: 0.9788 - val_loss: 0.1164 - val_acc: 0.9670
Epoch 32/40
 - 5s - loss: 0.0328 - acc: 0.9784 - val_loss: 0.1215 - val_acc: 0.9673
Epoch 33/40
 - 5s - loss: 0.0326 - acc: 0.9785 - val_loss: 0.1206 - val_acc: 0.9682
Epoch 34/40
 - 5s - loss: 0.0328 - acc: 0.9788 - val_loss: 0.1254 - val_acc: 0.9676
Epoch 35/40
 - 5s - loss: 0.0324 - acc: 0.9785 - val_loss: 0.1260 - val_acc: 0.9668
Epoch 36/40
 - 5s - loss: 0.0325 - acc: 0.9789 - val_loss: 0.1273 - val_acc: 0.9661
Epoch 37/40
 - 5s - loss: 0.0324 - acc: 0.9788 - val_loss: 0.1229 - val_acc: 0.9672
Epoch 38/40
 - 5s - loss: 0.0326 - acc: 0.9785 - val_loss: 0.1278 - val_acc: 0.9664
Epoch 39/40
 - 5s - loss: 0.0325 - acc: 0.9784 - val_loss: 0.1252 - val_acc: 0.9683
Epoch 40/40
 - 5s - loss: 0.0323 - acc: 0.9787 - val_loss: 0.1202 - val_acc: 0.9686
Epoch 1/40
 - 1s - loss: 0.0959 - acc: 0.9649
Epoch 2/40
 - 1s - loss: 0.0709 - acc: 0.9679
Epoch 3/40
 - 1s - loss: 0.0625 - acc: 0.9697
Epoch 4/40
 - 1s - loss: 0.0579 - acc: 0.9712
Epoch 5/40
 - 1s - loss: 0.0539 - acc: 0.9719
Epoch 6/40
 - 1s - loss: 0.0515 - acc: 0.9720
Epoch 7/40
 - 1s - loss: 0.0500 - acc: 0.9736
Epoch 8/40
 - 1s - loss: 0.0490 - acc: 0.9741
Epoch 9/40
 - 1s - loss: 0.0465 - acc: 0.9746
Epoch 10/40
 - 1s - loss: 0.0462 - acc: 0.9749
Epoch 11/40
 - 1s - loss: 0.0447 - acc: 0.9758
Epoch 12/40
 - 1s - loss: 0.0439 - acc: 0.9761
Epoch 13/40
 - 1s - loss: 0.0423 - acc: 0.9764
Epoch 14/40
 - 1s - loss: 0.0423 - acc: 0.9766
Epoch 15/40
 - 1s - loss: 0.0422 - acc: 0.9778
Epoch 16/40
 - 1s - loss: 0.0418 - acc: 0.9768
Epoch 17/40
 - 1s - loss: 0.0412 - acc: 0.9770
Epoch 18/40
 - 1s - loss: 0.0391 - acc: 0.9779
Epoch 19/40
 - 1s - loss: 0.0384 - acc: 0.9786
Epoch 20/40
 - 1s - loss: 0.0389 - acc: 0.9785
Epoch 21/40
 - 1s - loss: 0.0383 - acc: 0.9778
Epoch 22/40
 - 1s - loss: 0.0379 - acc: 0.9786
Epoch 23/40
 - 1s - loss: 0.0377 - acc: 0.9773
Epoch 24/40
 - 1s - loss: 0.0361 - acc: 0.9781
Epoch 25/40
 - 1s - loss: 0.0366 - acc: 0.9785
Epoch 26/40
 - 1s - loss: 0.0365 - acc: 0.9782
Epoch 27/40
 - 1s - loss: 0.0364 - acc: 0.9787
Epoch 28/40
 - 1s - loss: 0.0365 - acc: 0.9777
Epoch 29/40
 - 1s - loss: 0.0354 - acc: 0.9793
Epoch 30/40
 - 1s - loss: 0.0360 - acc: 0.9785
Epoch 31/40
 - 1s - loss: 0.0364 - acc: 0.9795
Epoch 32/40
 - 1s - loss: 0.0361 - acc: 0.9783
Epoch 33/40
 - 1s - loss: 0.0361 - acc: 0.9780
Epoch 34/40
 - 1s - loss: 0.0349 - acc: 0.9796
Epoch 35/40
 - 1s - loss: 0.0349 - acc: 0.9784
Epoch 36/40
 - 1s - loss: 0.0347 - acc: 0.9787
Epoch 37/40
 - 1s - loss: 0.0347 - acc: 0.9787
Epoch 38/40
 - 1s - loss: 0.0347 - acc: 0.9781
Epoch 39/40
 - 1s - loss: 0.0354 - acc: 0.9778
Epoch 40/40
 - 1s - loss: 0.0342 - acc: 0.9790
# Training time = 0:03:52.006433
# F-Score(Ordinary) = 0.122, Recall: 0.405, Precision: 0.072
# F-Score(lvc) = 0.238, Recall: 0.343, Precision: 0.182
# F-Score(id) = 0.079, Recall: 0.889, Precision: 0.041
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_119 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_120 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_119 (Embedding)       (None, 4, 48)        705264      input_119[0][0]                  
__________________________________________________________________________________________________
embedding_120 (Embedding)       (None, 4, 24)        5640        input_120[0][0]                  
__________________________________________________________________________________________________
flatten_119 (Flatten)           (None, 192)          0           embedding_119[0][0]              
__________________________________________________________________________________________________
flatten_120 (Flatten)           (None, 96)           0           embedding_120[0][0]              
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 288)          0           flatten_119[0][0]                
                                                                 flatten_120[0][0]                
__________________________________________________________________________________________________
dense_119 (Dense)               (None, 24)           6936        concatenate_60[0][0]             
__________________________________________________________________________________________________
dropout_60 (Dropout)            (None, 24)           0           dense_119[0][0]                  
__________________________________________________________________________________________________
dense_120 (Dense)               (None, 8)            200         dropout_60[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1350 - acc: 0.9545 - val_loss: 0.0725 - val_acc: 0.9679
Epoch 2/40
 - 5s - loss: 0.0606 - acc: 0.9707 - val_loss: 0.0742 - val_acc: 0.9679
Epoch 3/40
 - 5s - loss: 0.0513 - acc: 0.9736 - val_loss: 0.0773 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0466 - acc: 0.9741 - val_loss: 0.0739 - val_acc: 0.9685
Epoch 5/40
 - 5s - loss: 0.0441 - acc: 0.9750 - val_loss: 0.0798 - val_acc: 0.9680
Epoch 6/40
 - 5s - loss: 0.0417 - acc: 0.9755 - val_loss: 0.0817 - val_acc: 0.9679
Epoch 7/40
 - 5s - loss: 0.0408 - acc: 0.9756 - val_loss: 0.0857 - val_acc: 0.9672
Epoch 8/40
 - 5s - loss: 0.0396 - acc: 0.9760 - val_loss: 0.0879 - val_acc: 0.9673
Epoch 9/40
 - 5s - loss: 0.0387 - acc: 0.9766 - val_loss: 0.0950 - val_acc: 0.9669
Epoch 10/40
 - 5s - loss: 0.0379 - acc: 0.9769 - val_loss: 0.0946 - val_acc: 0.9680
Epoch 11/40
 - 5s - loss: 0.0375 - acc: 0.9769 - val_loss: 0.0957 - val_acc: 0.9672
Epoch 12/40
 - 5s - loss: 0.0371 - acc: 0.9771 - val_loss: 0.0915 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0365 - acc: 0.9771 - val_loss: 0.0981 - val_acc: 0.9667
Epoch 14/40
 - 5s - loss: 0.0360 - acc: 0.9773 - val_loss: 0.1016 - val_acc: 0.9681
Epoch 15/40
 - 5s - loss: 0.0354 - acc: 0.9773 - val_loss: 0.0977 - val_acc: 0.9673
Epoch 16/40
 - 5s - loss: 0.0353 - acc: 0.9775 - val_loss: 0.1035 - val_acc: 0.9669
Epoch 17/40
 - 5s - loss: 0.0347 - acc: 0.9782 - val_loss: 0.1019 - val_acc: 0.9657
Epoch 18/40
 - 5s - loss: 0.0349 - acc: 0.9778 - val_loss: 0.1042 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0345 - acc: 0.9777 - val_loss: 0.1051 - val_acc: 0.9666
Epoch 20/40
 - 5s - loss: 0.0344 - acc: 0.9776 - val_loss: 0.1052 - val_acc: 0.9651
Epoch 21/40
 - 5s - loss: 0.0343 - acc: 0.9776 - val_loss: 0.1042 - val_acc: 0.9648
Epoch 22/40
 - 5s - loss: 0.0343 - acc: 0.9780 - val_loss: 0.1061 - val_acc: 0.9650
Epoch 23/40
 - 5s - loss: 0.0339 - acc: 0.9782 - val_loss: 0.1045 - val_acc: 0.9675
Epoch 24/40
 - 5s - loss: 0.0337 - acc: 0.9778 - val_loss: 0.1074 - val_acc: 0.9672
Epoch 25/40
 - 5s - loss: 0.0337 - acc: 0.9787 - val_loss: 0.1060 - val_acc: 0.9652
Epoch 26/40
 - 5s - loss: 0.0335 - acc: 0.9784 - val_loss: 0.1190 - val_acc: 0.9658
Epoch 27/40
 - 5s - loss: 0.0335 - acc: 0.9785 - val_loss: 0.1114 - val_acc: 0.9656
Epoch 28/40
 - 5s - loss: 0.0329 - acc: 0.9790 - val_loss: 0.1091 - val_acc: 0.9657
Epoch 29/40
 - 5s - loss: 0.0333 - acc: 0.9786 - val_loss: 0.1100 - val_acc: 0.9666
Epoch 30/40
 - 5s - loss: 0.0330 - acc: 0.9787 - val_loss: 0.1128 - val_acc: 0.9660
Epoch 31/40
 - 5s - loss: 0.0328 - acc: 0.9789 - val_loss: 0.1154 - val_acc: 0.9659
Epoch 32/40
 - 5s - loss: 0.0329 - acc: 0.9787 - val_loss: 0.1171 - val_acc: 0.9633
Epoch 33/40
 - 5s - loss: 0.0330 - acc: 0.9785 - val_loss: 0.1105 - val_acc: 0.9640
Epoch 34/40
 - 5s - loss: 0.0328 - acc: 0.9790 - val_loss: 0.1119 - val_acc: 0.9659
Epoch 35/40
 - 5s - loss: 0.0326 - acc: 0.9789 - val_loss: 0.1139 - val_acc: 0.9677
Epoch 36/40
 - 5s - loss: 0.0329 - acc: 0.9788 - val_loss: 0.1187 - val_acc: 0.9660
Epoch 37/40
 - 5s - loss: 0.0328 - acc: 0.9788 - val_loss: 0.1144 - val_acc: 0.9662
Epoch 38/40
 - 5s - loss: 0.0327 - acc: 0.9789 - val_loss: 0.1166 - val_acc: 0.9661
Epoch 39/40
 - 5s - loss: 0.0324 - acc: 0.9790 - val_loss: 0.1193 - val_acc: 0.9654
Epoch 40/40
 - 5s - loss: 0.0325 - acc: 0.9787 - val_loss: 0.1228 - val_acc: 0.9668
Epoch 1/40
 - 1s - loss: 0.1003 - acc: 0.9649
Epoch 2/40
 - 1s - loss: 0.0687 - acc: 0.9681
Epoch 3/40
 - 1s - loss: 0.0605 - acc: 0.9687
Epoch 4/40
 - 1s - loss: 0.0570 - acc: 0.9717
Epoch 5/40
 - 1s - loss: 0.0518 - acc: 0.9732
Epoch 6/40
 - 1s - loss: 0.0511 - acc: 0.9736
Epoch 7/40
 - 1s - loss: 0.0476 - acc: 0.9741
Epoch 8/40
 - 1s - loss: 0.0464 - acc: 0.9750
Epoch 9/40
 - 1s - loss: 0.0451 - acc: 0.9756
Epoch 10/40
 - 1s - loss: 0.0429 - acc: 0.9759
Epoch 11/40
 - 1s - loss: 0.0431 - acc: 0.9763
Epoch 12/40
 - 1s - loss: 0.0419 - acc: 0.9762
Epoch 13/40
 - 1s - loss: 0.0406 - acc: 0.9768
Epoch 14/40
 - 1s - loss: 0.0393 - acc: 0.9770
Epoch 15/40
 - 1s - loss: 0.0384 - acc: 0.9772
Epoch 16/40
 - 1s - loss: 0.0391 - acc: 0.9782
Epoch 17/40
 - 1s - loss: 0.0381 - acc: 0.9773
Epoch 18/40
 - 1s - loss: 0.0375 - acc: 0.9770
Epoch 19/40
 - 1s - loss: 0.0391 - acc: 0.9770
Epoch 20/40
 - 1s - loss: 0.0367 - acc: 0.9787
Epoch 21/40
 - 1s - loss: 0.0366 - acc: 0.9782
Epoch 22/40
 - 1s - loss: 0.0364 - acc: 0.9792
Epoch 23/40
 - 1s - loss: 0.0353 - acc: 0.9778
Epoch 24/40
 - 1s - loss: 0.0363 - acc: 0.9777
Epoch 25/40
 - 1s - loss: 0.0363 - acc: 0.9785
Epoch 26/40
 - 1s - loss: 0.0354 - acc: 0.9791
Epoch 27/40
 - 1s - loss: 0.0366 - acc: 0.9776
Epoch 28/40
 - 1s - loss: 0.0348 - acc: 0.9778
Epoch 29/40
 - 1s - loss: 0.0350 - acc: 0.9787
Epoch 30/40
 - 1s - loss: 0.0348 - acc: 0.9789
Epoch 31/40
 - 1s - loss: 0.0341 - acc: 0.9773
Epoch 32/40
 - 1s - loss: 0.0350 - acc: 0.9790
Epoch 33/40
 - 1s - loss: 0.0328 - acc: 0.9790
Epoch 34/40
 - 1s - loss: 0.0332 - acc: 0.9786
Epoch 35/40
 - 1s - loss: 0.0336 - acc: 0.9781
Epoch 36/40
 - 1s - loss: 0.0330 - acc: 0.9788
Epoch 37/40
 - 1s - loss: 0.0335 - acc: 0.9786
Epoch 38/40
 - 1s - loss: 0.0330 - acc: 0.9790
Epoch 39/40
 - 1s - loss: 0.0341 - acc: 0.9783
Epoch 40/40
 - 1s - loss: 0.0326 - acc: 0.9787
# Training time = 0:03:55.317840
# F-Score(Ordinary) = 0.558, Recall: 0.623, Precision: 0.506
# F-Score(lvc) = 0.46, Recall: 0.411, Precision: 0.523
# F-Score(ireflv) = 0.763, Recall: 0.714, Precision: 0.82
# F-Score(id) = 0.427, Recall: 0.964, Precision: 0.275
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_121 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_122 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_121 (Embedding)       (None, 4, 48)        705264      input_121[0][0]                  
__________________________________________________________________________________________________
embedding_122 (Embedding)       (None, 4, 24)        5640        input_122[0][0]                  
__________________________________________________________________________________________________
flatten_121 (Flatten)           (None, 192)          0           embedding_121[0][0]              
__________________________________________________________________________________________________
flatten_122 (Flatten)           (None, 96)           0           embedding_122[0][0]              
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 288)          0           flatten_121[0][0]                
                                                                 flatten_122[0][0]                
__________________________________________________________________________________________________
dense_121 (Dense)               (None, 24)           6936        concatenate_61[0][0]             
__________________________________________________________________________________________________
dropout_61 (Dropout)            (None, 24)           0           dense_121[0][0]                  
__________________________________________________________________________________________________
dense_122 (Dense)               (None, 8)            200         dropout_61[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1070 - acc: 0.9612 - val_loss: 0.0738 - val_acc: 0.9668
Epoch 2/40
 - 5s - loss: 0.0573 - acc: 0.9710 - val_loss: 0.0738 - val_acc: 0.9662
Epoch 3/40
 - 5s - loss: 0.0486 - acc: 0.9738 - val_loss: 0.0829 - val_acc: 0.9657
Epoch 4/40
 - 5s - loss: 0.0451 - acc: 0.9749 - val_loss: 0.0827 - val_acc: 0.9655
Epoch 5/40
 - 5s - loss: 0.0432 - acc: 0.9750 - val_loss: 0.0879 - val_acc: 0.9686
Epoch 6/40
 - 5s - loss: 0.0418 - acc: 0.9751 - val_loss: 0.0869 - val_acc: 0.9670
Epoch 7/40
 - 5s - loss: 0.0408 - acc: 0.9760 - val_loss: 0.0869 - val_acc: 0.9677
Epoch 8/40
 - 5s - loss: 0.0396 - acc: 0.9760 - val_loss: 0.0948 - val_acc: 0.9682
Epoch 9/40
 - 5s - loss: 0.0388 - acc: 0.9769 - val_loss: 0.0935 - val_acc: 0.9676
Epoch 10/40
 - 5s - loss: 0.0384 - acc: 0.9763 - val_loss: 0.0900 - val_acc: 0.9676
Epoch 11/40
 - 5s - loss: 0.0379 - acc: 0.9769 - val_loss: 0.1029 - val_acc: 0.9657
Epoch 12/40
 - 5s - loss: 0.0376 - acc: 0.9769 - val_loss: 0.0959 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0371 - acc: 0.9771 - val_loss: 0.0982 - val_acc: 0.9680
Epoch 14/40
 - 5s - loss: 0.0368 - acc: 0.9771 - val_loss: 0.1045 - val_acc: 0.9682
Epoch 15/40
 - 5s - loss: 0.0363 - acc: 0.9776 - val_loss: 0.1081 - val_acc: 0.9682
Epoch 16/40
 - 5s - loss: 0.0363 - acc: 0.9773 - val_loss: 0.1036 - val_acc: 0.9673
Epoch 17/40
 - 5s - loss: 0.0363 - acc: 0.9777 - val_loss: 0.1080 - val_acc: 0.9671
Epoch 18/40
 - 5s - loss: 0.0358 - acc: 0.9777 - val_loss: 0.1098 - val_acc: 0.9674
Epoch 19/40
 - 5s - loss: 0.0362 - acc: 0.9773 - val_loss: 0.1125 - val_acc: 0.9667
Epoch 20/40
 - 5s - loss: 0.0361 - acc: 0.9778 - val_loss: 0.1179 - val_acc: 0.9675
Epoch 21/40
 - 5s - loss: 0.0354 - acc: 0.9784 - val_loss: 0.1118 - val_acc: 0.9662
Epoch 22/40
 - 5s - loss: 0.0360 - acc: 0.9779 - val_loss: 0.1064 - val_acc: 0.9670
Epoch 23/40
 - 5s - loss: 0.0351 - acc: 0.9778 - val_loss: 0.1087 - val_acc: 0.9677
Epoch 24/40
 - 5s - loss: 0.0355 - acc: 0.9779 - val_loss: 0.1148 - val_acc: 0.9665
Epoch 25/40
 - 5s - loss: 0.0349 - acc: 0.9781 - val_loss: 0.1169 - val_acc: 0.9683
Epoch 26/40
 - 5s - loss: 0.0352 - acc: 0.9783 - val_loss: 0.1143 - val_acc: 0.9675
Epoch 27/40
 - 5s - loss: 0.0354 - acc: 0.9781 - val_loss: 0.1178 - val_acc: 0.9666
Epoch 28/40
 - 5s - loss: 0.0348 - acc: 0.9781 - val_loss: 0.1179 - val_acc: 0.9683
Epoch 29/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1223 - val_acc: 0.9679
Epoch 30/40
 - 5s - loss: 0.0346 - acc: 0.9782 - val_loss: 0.1223 - val_acc: 0.9670
Epoch 31/40
 - 5s - loss: 0.0349 - acc: 0.9782 - val_loss: 0.1165 - val_acc: 0.9658
Epoch 32/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1207 - val_acc: 0.9649
Epoch 33/40
 - 5s - loss: 0.0347 - acc: 0.9784 - val_loss: 0.1237 - val_acc: 0.9679
Epoch 34/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1210 - val_acc: 0.9664
Epoch 35/40
 - 5s - loss: 0.0344 - acc: 0.9783 - val_loss: 0.1159 - val_acc: 0.9677
Epoch 36/40
 - 5s - loss: 0.0350 - acc: 0.9786 - val_loss: 0.1193 - val_acc: 0.9685
Epoch 37/40
 - 5s - loss: 0.0344 - acc: 0.9782 - val_loss: 0.1204 - val_acc: 0.9683
Epoch 38/40
 - 5s - loss: 0.0344 - acc: 0.9784 - val_loss: 0.1105 - val_acc: 0.9682
Epoch 39/40
 - 5s - loss: 0.0345 - acc: 0.9784 - val_loss: 0.1202 - val_acc: 0.9675
Epoch 40/40
 - 5s - loss: 0.0344 - acc: 0.9786 - val_loss: 0.1238 - val_acc: 0.9687
Epoch 1/40
 - 1s - loss: 0.1084 - acc: 0.9656
Epoch 2/40
 - 1s - loss: 0.0733 - acc: 0.9696
Epoch 3/40
 - 1s - loss: 0.0656 - acc: 0.9712
Epoch 4/40
 - 1s - loss: 0.0578 - acc: 0.9710
Epoch 5/40
 - 1s - loss: 0.0540 - acc: 0.9737
Epoch 6/40
 - 1s - loss: 0.0507 - acc: 0.9752
Epoch 7/40
 - 1s - loss: 0.0487 - acc: 0.9744
Epoch 8/40
 - 1s - loss: 0.0482 - acc: 0.9753
Epoch 9/40
 - 1s - loss: 0.0460 - acc: 0.9756
Epoch 10/40
 - 1s - loss: 0.0447 - acc: 0.9760
Epoch 11/40
 - 1s - loss: 0.0449 - acc: 0.9768
Epoch 12/40
 - 1s - loss: 0.0439 - acc: 0.9771
Epoch 13/40
 - 1s - loss: 0.0439 - acc: 0.9774
Epoch 14/40
 - 1s - loss: 0.0448 - acc: 0.9779
Epoch 15/40
 - 1s - loss: 0.0434 - acc: 0.9766
Epoch 16/40
 - 1s - loss: 0.0424 - acc: 0.9767
Epoch 17/40
 - 1s - loss: 0.0418 - acc: 0.9780
Epoch 18/40
 - 1s - loss: 0.0402 - acc: 0.9769
Epoch 19/40
 - 1s - loss: 0.0390 - acc: 0.9782
Epoch 20/40
 - 1s - loss: 0.0397 - acc: 0.9775
Epoch 21/40
 - 1s - loss: 0.0394 - acc: 0.9774
Epoch 22/40
 - 1s - loss: 0.0392 - acc: 0.9777
Epoch 23/40
 - 1s - loss: 0.0388 - acc: 0.9786
Epoch 24/40
 - 1s - loss: 0.0404 - acc: 0.9778
Epoch 25/40
 - 1s - loss: 0.0379 - acc: 0.9766
Epoch 26/40
 - 1s - loss: 0.0390 - acc: 0.9778
Epoch 27/40
 - 1s - loss: 0.0395 - acc: 0.9783
Epoch 28/40
 - 1s - loss: 0.0377 - acc: 0.9786
Epoch 29/40
 - 1s - loss: 0.0394 - acc: 0.9778
Epoch 30/40
 - 1s - loss: 0.0380 - acc: 0.9786
Epoch 31/40
 - 1s - loss: 0.0366 - acc: 0.9786
Epoch 32/40
 - 1s - loss: 0.0370 - acc: 0.9798
Epoch 33/40
 - 1s - loss: 0.0385 - acc: 0.9788
Epoch 34/40
 - 1s - loss: 0.0391 - acc: 0.9799
Epoch 35/40
 - 1s - loss: 0.0373 - acc: 0.9784
Epoch 36/40
 - 1s - loss: 0.0377 - acc: 0.9777
Epoch 37/40
 - 1s - loss: 0.0364 - acc: 0.9786
Epoch 38/40
 - 1s - loss: 0.0368 - acc: 0.9793
Epoch 39/40
 - 1s - loss: 0.0373 - acc: 0.9769
Epoch 40/40
 - 1s - loss: 0.0392 - acc: 0.9776
# Training time = 0:03:49.003691
# F-Score(Ordinary) = 0.216, Recall: 0.713, Precision: 0.128
# F-Score(ireflv) = 0.472, Recall: 0.75, Precision: 0.344
# F-Score(id) = 0.12, Recall: 0.565, Precision: 0.067
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_123 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_124 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_123 (Embedding)       (None, 4, 48)        705264      input_123[0][0]                  
__________________________________________________________________________________________________
embedding_124 (Embedding)       (None, 4, 24)        5640        input_124[0][0]                  
__________________________________________________________________________________________________
flatten_123 (Flatten)           (None, 192)          0           embedding_123[0][0]              
__________________________________________________________________________________________________
flatten_124 (Flatten)           (None, 96)           0           embedding_124[0][0]              
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 288)          0           flatten_123[0][0]                
                                                                 flatten_124[0][0]                
__________________________________________________________________________________________________
dense_123 (Dense)               (None, 24)           6936        concatenate_62[0][0]             
__________________________________________________________________________________________________
dropout_62 (Dropout)            (None, 24)           0           dense_123[0][0]                  
__________________________________________________________________________________________________
dense_124 (Dense)               (None, 8)            200         dropout_62[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1075 - acc: 0.9614 - val_loss: 0.0686 - val_acc: 0.9679
Epoch 2/40
 - 5s - loss: 0.0565 - acc: 0.9717 - val_loss: 0.0727 - val_acc: 0.9673
Epoch 3/40
 - 5s - loss: 0.0494 - acc: 0.9736 - val_loss: 0.0779 - val_acc: 0.9670
Epoch 4/40
 - 5s - loss: 0.0452 - acc: 0.9751 - val_loss: 0.0795 - val_acc: 0.9682
Epoch 5/40
 - 5s - loss: 0.0433 - acc: 0.9757 - val_loss: 0.0849 - val_acc: 0.9677
Epoch 6/40
 - 5s - loss: 0.0419 - acc: 0.9761 - val_loss: 0.0856 - val_acc: 0.9677
Epoch 7/40
 - 5s - loss: 0.0401 - acc: 0.9762 - val_loss: 0.0884 - val_acc: 0.9673
Epoch 8/40
 - 5s - loss: 0.0396 - acc: 0.9764 - val_loss: 0.0907 - val_acc: 0.9683
Epoch 9/40
 - 5s - loss: 0.0389 - acc: 0.9767 - val_loss: 0.0884 - val_acc: 0.9667
Epoch 10/40
 - 5s - loss: 0.0385 - acc: 0.9763 - val_loss: 0.0980 - val_acc: 0.9681
Epoch 11/40
 - 5s - loss: 0.0380 - acc: 0.9769 - val_loss: 0.0980 - val_acc: 0.9679
Epoch 12/40
 - 5s - loss: 0.0378 - acc: 0.9771 - val_loss: 0.1007 - val_acc: 0.9673
Epoch 13/40
 - 5s - loss: 0.0374 - acc: 0.9770 - val_loss: 0.1006 - val_acc: 0.9667
Epoch 14/40
 - 5s - loss: 0.0368 - acc: 0.9768 - val_loss: 0.1088 - val_acc: 0.9678
Epoch 15/40
 - 5s - loss: 0.0366 - acc: 0.9772 - val_loss: 0.1020 - val_acc: 0.9677
Epoch 16/40
 - 5s - loss: 0.0366 - acc: 0.9774 - val_loss: 0.0986 - val_acc: 0.9677
Epoch 17/40
 - 5s - loss: 0.0363 - acc: 0.9771 - val_loss: 0.1062 - val_acc: 0.9654
Epoch 18/40
 - 5s - loss: 0.0363 - acc: 0.9779 - val_loss: 0.1030 - val_acc: 0.9673
Epoch 19/40
 - 5s - loss: 0.0358 - acc: 0.9778 - val_loss: 0.1058 - val_acc: 0.9670
Epoch 20/40
 - 5s - loss: 0.0360 - acc: 0.9775 - val_loss: 0.1094 - val_acc: 0.9673
Epoch 21/40
 - 5s - loss: 0.0354 - acc: 0.9783 - val_loss: 0.1075 - val_acc: 0.9673
Epoch 22/40
 - 5s - loss: 0.0355 - acc: 0.9776 - val_loss: 0.1067 - val_acc: 0.9666
Epoch 23/40
 - 5s - loss: 0.0356 - acc: 0.9776 - val_loss: 0.1074 - val_acc: 0.9670
Epoch 24/40
 - 5s - loss: 0.0351 - acc: 0.9779 - val_loss: 0.1184 - val_acc: 0.9643
Epoch 25/40
 - 5s - loss: 0.0354 - acc: 0.9776 - val_loss: 0.1146 - val_acc: 0.9669
Epoch 26/40
 - 5s - loss: 0.0353 - acc: 0.9776 - val_loss: 0.1240 - val_acc: 0.9674
Epoch 27/40
 - 5s - loss: 0.0358 - acc: 0.9774 - val_loss: 0.1089 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0349 - acc: 0.9781 - val_loss: 0.1105 - val_acc: 0.9655
Epoch 29/40
 - 5s - loss: 0.0353 - acc: 0.9782 - val_loss: 0.1234 - val_acc: 0.9668
Epoch 30/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1131 - val_acc: 0.9685
Epoch 31/40
 - 5s - loss: 0.0345 - acc: 0.9783 - val_loss: 0.1141 - val_acc: 0.9686
Epoch 32/40
 - 5s - loss: 0.0350 - acc: 0.9783 - val_loss: 0.1193 - val_acc: 0.9677
Epoch 33/40
 - 5s - loss: 0.0355 - acc: 0.9780 - val_loss: 0.1228 - val_acc: 0.9683
Epoch 34/40
 - 5s - loss: 0.0351 - acc: 0.9777 - val_loss: 0.1185 - val_acc: 0.9679
Epoch 35/40
 - 5s - loss: 0.0353 - acc: 0.9779 - val_loss: 0.1151 - val_acc: 0.9663
Epoch 36/40
 - 5s - loss: 0.0351 - acc: 0.9782 - val_loss: 0.1142 - val_acc: 0.9656
Epoch 37/40
 - 5s - loss: 0.0350 - acc: 0.9784 - val_loss: 0.1174 - val_acc: 0.9679
Epoch 38/40
 - 5s - loss: 0.0347 - acc: 0.9778 - val_loss: 0.1119 - val_acc: 0.9683
Epoch 39/40
 - 5s - loss: 0.0346 - acc: 0.9782 - val_loss: 0.1208 - val_acc: 0.9665
Epoch 40/40
 - 5s - loss: 0.0348 - acc: 0.9783 - val_loss: 0.1173 - val_acc: 0.9679
Epoch 1/40
 - 1s - loss: 0.1015 - acc: 0.9664
Epoch 2/40
 - 1s - loss: 0.0725 - acc: 0.9680
Epoch 3/40
 - 1s - loss: 0.0612 - acc: 0.9709
Epoch 4/40
 - 1s - loss: 0.0557 - acc: 0.9728
Epoch 5/40
 - 1s - loss: 0.0537 - acc: 0.9732
Epoch 6/40
 - 1s - loss: 0.0526 - acc: 0.9734
Epoch 7/40
 - 1s - loss: 0.0499 - acc: 0.9738
Epoch 8/40
 - 1s - loss: 0.0473 - acc: 0.9758
Epoch 9/40
 - 1s - loss: 0.0443 - acc: 0.9756
Epoch 10/40
 - 1s - loss: 0.0441 - acc: 0.9752
Epoch 11/40
 - 1s - loss: 0.0437 - acc: 0.9755
Epoch 12/40
 - 1s - loss: 0.0439 - acc: 0.9765
Epoch 13/40
 - 1s - loss: 0.0409 - acc: 0.9768
Epoch 14/40
 - 1s - loss: 0.0405 - acc: 0.9770
Epoch 15/40
 - 1s - loss: 0.0396 - acc: 0.9776
Epoch 16/40
 - 1s - loss: 0.0383 - acc: 0.9789
Epoch 17/40
 - 1s - loss: 0.0402 - acc: 0.9769
Epoch 18/40
 - 1s - loss: 0.0372 - acc: 0.9781
Epoch 19/40
 - 1s - loss: 0.0385 - acc: 0.9776
Epoch 20/40
 - 1s - loss: 0.0372 - acc: 0.9766
Epoch 21/40
 - 1s - loss: 0.0382 - acc: 0.9764
Epoch 22/40
 - 1s - loss: 0.0364 - acc: 0.9785
Epoch 23/40
 - 1s - loss: 0.0371 - acc: 0.9778
Epoch 24/40
 - 1s - loss: 0.0359 - acc: 0.9782
Epoch 25/40
 - 1s - loss: 0.0358 - acc: 0.9779
Epoch 26/40
 - 1s - loss: 0.0368 - acc: 0.9779
Epoch 27/40
 - 1s - loss: 0.0358 - acc: 0.9778
Epoch 28/40
 - 1s - loss: 0.0356 - acc: 0.9790
Epoch 29/40
 - 1s - loss: 0.0356 - acc: 0.9784
Epoch 30/40
 - 1s - loss: 0.0356 - acc: 0.9783
Epoch 31/40
 - 1s - loss: 0.0352 - acc: 0.9785
Epoch 32/40
 - 1s - loss: 0.0362 - acc: 0.9782
Epoch 33/40
 - 1s - loss: 0.0354 - acc: 0.9790
Epoch 34/40
 - 1s - loss: 0.0354 - acc: 0.9786
Epoch 35/40
 - 1s - loss: 0.0346 - acc: 0.9787
Epoch 36/40
 - 1s - loss: 0.0350 - acc: 0.9786
Epoch 37/40
 - 1s - loss: 0.0357 - acc: 0.9779
Epoch 38/40
 - 1s - loss: 0.0345 - acc: 0.9795
Epoch 39/40
 - 1s - loss: 0.0356 - acc: 0.9784
Epoch 40/40
 - 1s - loss: 0.0361 - acc: 0.9786
# Training time = 0:03:50.418304
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_125 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_126 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_125 (Embedding)       (None, 4, 48)        705264      input_125[0][0]                  
__________________________________________________________________________________________________
embedding_126 (Embedding)       (None, 4, 24)        5640        input_126[0][0]                  
__________________________________________________________________________________________________
flatten_125 (Flatten)           (None, 192)          0           embedding_125[0][0]              
__________________________________________________________________________________________________
flatten_126 (Flatten)           (None, 96)           0           embedding_126[0][0]              
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 288)          0           flatten_125[0][0]                
                                                                 flatten_126[0][0]                
__________________________________________________________________________________________________
dense_125 (Dense)               (None, 24)           6936        concatenate_63[0][0]             
__________________________________________________________________________________________________
dropout_63 (Dropout)            (None, 24)           0           dense_125[0][0]                  
__________________________________________________________________________________________________
dense_126 (Dense)               (None, 8)            200         dropout_63[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1046 - acc: 0.9619 - val_loss: 0.0735 - val_acc: 0.9670
Epoch 2/40
 - 5s - loss: 0.0560 - acc: 0.9714 - val_loss: 0.0720 - val_acc: 0.9674
Epoch 3/40
 - 5s - loss: 0.0481 - acc: 0.9741 - val_loss: 0.0792 - val_acc: 0.9665
Epoch 4/40
 - 5s - loss: 0.0450 - acc: 0.9747 - val_loss: 0.0820 - val_acc: 0.9671
Epoch 5/40
 - 5s - loss: 0.0425 - acc: 0.9757 - val_loss: 0.0880 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0413 - acc: 0.9757 - val_loss: 0.0881 - val_acc: 0.9677
Epoch 7/40
 - 5s - loss: 0.0397 - acc: 0.9766 - val_loss: 0.0864 - val_acc: 0.9675
Epoch 8/40
 - 5s - loss: 0.0391 - acc: 0.9770 - val_loss: 0.0990 - val_acc: 0.9689
Epoch 9/40
 - 5s - loss: 0.0386 - acc: 0.9767 - val_loss: 0.0958 - val_acc: 0.9681
Epoch 10/40
 - 5s - loss: 0.0384 - acc: 0.9769 - val_loss: 0.0939 - val_acc: 0.9668
Epoch 11/40
 - 5s - loss: 0.0377 - acc: 0.9771 - val_loss: 0.0972 - val_acc: 0.9667
Epoch 12/40
 - 5s - loss: 0.0371 - acc: 0.9772 - val_loss: 0.1045 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0368 - acc: 0.9772 - val_loss: 0.1025 - val_acc: 0.9671
Epoch 14/40
 - 5s - loss: 0.0368 - acc: 0.9775 - val_loss: 0.1034 - val_acc: 0.9678
Epoch 15/40
 - 5s - loss: 0.0371 - acc: 0.9770 - val_loss: 0.0985 - val_acc: 0.9667
Epoch 16/40
 - 5s - loss: 0.0360 - acc: 0.9774 - val_loss: 0.0977 - val_acc: 0.9684
Epoch 17/40
 - 5s - loss: 0.0359 - acc: 0.9777 - val_loss: 0.1104 - val_acc: 0.9662
Epoch 18/40
 - 5s - loss: 0.0363 - acc: 0.9777 - val_loss: 0.1072 - val_acc: 0.9659
Epoch 19/40
 - 5s - loss: 0.0354 - acc: 0.9775 - val_loss: 0.1098 - val_acc: 0.9673
Epoch 20/40
 - 5s - loss: 0.0356 - acc: 0.9773 - val_loss: 0.1055 - val_acc: 0.9674
Epoch 21/40
 - 5s - loss: 0.0356 - acc: 0.9778 - val_loss: 0.1073 - val_acc: 0.9671
Epoch 22/40
 - 5s - loss: 0.0354 - acc: 0.9777 - val_loss: 0.1130 - val_acc: 0.9668
Epoch 23/40
 - 5s - loss: 0.0354 - acc: 0.9784 - val_loss: 0.1117 - val_acc: 0.9671
Epoch 24/40
 - 5s - loss: 0.0355 - acc: 0.9780 - val_loss: 0.1140 - val_acc: 0.9684
Epoch 25/40
 - 5s - loss: 0.0348 - acc: 0.9783 - val_loss: 0.1169 - val_acc: 0.9654
Epoch 26/40
 - 5s - loss: 0.0356 - acc: 0.9776 - val_loss: 0.1161 - val_acc: 0.9668
Epoch 27/40
 - 5s - loss: 0.0351 - acc: 0.9782 - val_loss: 0.1126 - val_acc: 0.9666
Epoch 28/40
 - 5s - loss: 0.0348 - acc: 0.9784 - val_loss: 0.1159 - val_acc: 0.9634
Epoch 29/40
 - 5s - loss: 0.0348 - acc: 0.9781 - val_loss: 0.1147 - val_acc: 0.9656
Epoch 30/40
 - 5s - loss: 0.0348 - acc: 0.9785 - val_loss: 0.1184 - val_acc: 0.9668
Epoch 31/40
 - 5s - loss: 0.0345 - acc: 0.9782 - val_loss: 0.1265 - val_acc: 0.9671
Epoch 32/40
 - 5s - loss: 0.0352 - acc: 0.9783 - val_loss: 0.1146 - val_acc: 0.9646
Epoch 33/40
 - 5s - loss: 0.0350 - acc: 0.9781 - val_loss: 0.1219 - val_acc: 0.9663
Epoch 34/40
 - 5s - loss: 0.0350 - acc: 0.9780 - val_loss: 0.1220 - val_acc: 0.9645
Epoch 35/40
 - 5s - loss: 0.0346 - acc: 0.9783 - val_loss: 0.1211 - val_acc: 0.9661
Epoch 36/40
 - 5s - loss: 0.0347 - acc: 0.9780 - val_loss: 0.1259 - val_acc: 0.9670
Epoch 37/40
 - 5s - loss: 0.0347 - acc: 0.9785 - val_loss: 0.1212 - val_acc: 0.9648
Epoch 38/40
 - 5s - loss: 0.0351 - acc: 0.9782 - val_loss: 0.1202 - val_acc: 0.9678
Epoch 39/40
 - 5s - loss: 0.0347 - acc: 0.9777 - val_loss: 0.1252 - val_acc: 0.9606
Epoch 40/40
 - 5s - loss: 0.0346 - acc: 0.9782 - val_loss: 0.1237 - val_acc: 0.9670
Epoch 1/40
 - 1s - loss: 0.1048 - acc: 0.9644
Epoch 2/40
 - 1s - loss: 0.0778 - acc: 0.9682
Epoch 3/40
 - 1s - loss: 0.0629 - acc: 0.9714
Epoch 4/40
 - 1s - loss: 0.0573 - acc: 0.9722
Epoch 5/40
 - 1s - loss: 0.0533 - acc: 0.9737
Epoch 6/40
 - 1s - loss: 0.0490 - acc: 0.9737
Epoch 7/40
 - 1s - loss: 0.0465 - acc: 0.9756
Epoch 8/40
 - 1s - loss: 0.0451 - acc: 0.9749
Epoch 9/40
 - 1s - loss: 0.0451 - acc: 0.9757
Epoch 10/40
 - 1s - loss: 0.0429 - acc: 0.9769
Epoch 11/40
 - 1s - loss: 0.0422 - acc: 0.9772
Epoch 12/40
 - 1s - loss: 0.0411 - acc: 0.9769
Epoch 13/40
 - 1s - loss: 0.0414 - acc: 0.9769
Epoch 14/40
 - 1s - loss: 0.0403 - acc: 0.9764
Epoch 15/40
 - 1s - loss: 0.0401 - acc: 0.9767
Epoch 16/40
 - 1s - loss: 0.0384 - acc: 0.9777
Epoch 17/40
 - 1s - loss: 0.0383 - acc: 0.9773
Epoch 18/40
 - 1s - loss: 0.0367 - acc: 0.9782
Epoch 19/40
 - 1s - loss: 0.0371 - acc: 0.9786
Epoch 20/40
 - 1s - loss: 0.0386 - acc: 0.9764
Epoch 21/40
 - 1s - loss: 0.0371 - acc: 0.9781
Epoch 22/40
 - 1s - loss: 0.0366 - acc: 0.9796
Epoch 23/40
 - 1s - loss: 0.0369 - acc: 0.9773
Epoch 24/40
 - 1s - loss: 0.0362 - acc: 0.9780
Epoch 25/40
 - 1s - loss: 0.0355 - acc: 0.9785
Epoch 26/40
 - 1s - loss: 0.0359 - acc: 0.9786
Epoch 27/40
 - 1s - loss: 0.0350 - acc: 0.9788
Epoch 28/40
 - 1s - loss: 0.0357 - acc: 0.9786
Epoch 29/40
 - 1s - loss: 0.0354 - acc: 0.9777
Epoch 30/40
 - 1s - loss: 0.0355 - acc: 0.9788
Epoch 31/40
 - 1s - loss: 0.0362 - acc: 0.9776
Epoch 32/40
 - 1s - loss: 0.0354 - acc: 0.9781
Epoch 33/40
 - 1s - loss: 0.0341 - acc: 0.9796
Epoch 34/40
 - 1s - loss: 0.0356 - acc: 0.9778
Epoch 35/40
 - 1s - loss: 0.0357 - acc: 0.9792
Epoch 36/40
 - 1s - loss: 0.0350 - acc: 0.9783
Epoch 37/40
 - 1s - loss: 0.0345 - acc: 0.9797
Epoch 38/40
 - 1s - loss: 0.0352 - acc: 0.9791
Epoch 39/40
 - 1s - loss: 0.0342 - acc: 0.9788
Epoch 40/40
 - 1s - loss: 0.0342 - acc: 0.9790
# Training time = 0:03:49.470079
# F-Score(Ordinary) = 0.345, Recall: 0.758, Precision: 0.224
# F-Score(id) = 0.603, Recall: 0.742, Precision: 0.508
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_127 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_128 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_127 (Embedding)       (None, 4, 48)        705264      input_127[0][0]                  
__________________________________________________________________________________________________
embedding_128 (Embedding)       (None, 4, 24)        5640        input_128[0][0]                  
__________________________________________________________________________________________________
flatten_127 (Flatten)           (None, 192)          0           embedding_127[0][0]              
__________________________________________________________________________________________________
flatten_128 (Flatten)           (None, 96)           0           embedding_128[0][0]              
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 288)          0           flatten_127[0][0]                
                                                                 flatten_128[0][0]                
__________________________________________________________________________________________________
dense_127 (Dense)               (None, 24)           6936        concatenate_64[0][0]             
__________________________________________________________________________________________________
dropout_64 (Dropout)            (None, 24)           0           dense_127[0][0]                  
__________________________________________________________________________________________________
dense_128 (Dense)               (None, 8)            200         dropout_64[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1108 - acc: 0.9590 - val_loss: 0.0700 - val_acc: 0.9680
Epoch 2/40
 - 5s - loss: 0.0565 - acc: 0.9715 - val_loss: 0.0734 - val_acc: 0.9679
Epoch 3/40
 - 5s - loss: 0.0486 - acc: 0.9738 - val_loss: 0.0764 - val_acc: 0.9665
Epoch 4/40
 - 5s - loss: 0.0451 - acc: 0.9748 - val_loss: 0.0818 - val_acc: 0.9675
Epoch 5/40
 - 5s - loss: 0.0428 - acc: 0.9758 - val_loss: 0.0904 - val_acc: 0.9676
Epoch 6/40
 - 5s - loss: 0.0414 - acc: 0.9763 - val_loss: 0.0844 - val_acc: 0.9684
Epoch 7/40
 - 5s - loss: 0.0401 - acc: 0.9766 - val_loss: 0.0885 - val_acc: 0.9681
Epoch 8/40
 - 5s - loss: 0.0392 - acc: 0.9763 - val_loss: 0.0912 - val_acc: 0.9669
Epoch 9/40
 - 5s - loss: 0.0391 - acc: 0.9766 - val_loss: 0.0907 - val_acc: 0.9679
Epoch 10/40
 - 5s - loss: 0.0378 - acc: 0.9771 - val_loss: 0.0920 - val_acc: 0.9680
Epoch 11/40
 - 5s - loss: 0.0381 - acc: 0.9767 - val_loss: 0.0979 - val_acc: 0.9680
Epoch 12/40
 - 5s - loss: 0.0373 - acc: 0.9775 - val_loss: 0.0946 - val_acc: 0.9678
Epoch 13/40
 - 5s - loss: 0.0372 - acc: 0.9771 - val_loss: 0.0955 - val_acc: 0.9687
Epoch 14/40
 - 5s - loss: 0.0371 - acc: 0.9770 - val_loss: 0.0989 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0364 - acc: 0.9774 - val_loss: 0.1079 - val_acc: 0.9679
Epoch 16/40
 - 5s - loss: 0.0362 - acc: 0.9772 - val_loss: 0.0992 - val_acc: 0.9677
Epoch 17/40
 - 5s - loss: 0.0364 - acc: 0.9774 - val_loss: 0.1012 - val_acc: 0.9683
Epoch 18/40
 - 5s - loss: 0.0362 - acc: 0.9776 - val_loss: 0.1096 - val_acc: 0.9687
Epoch 19/40
 - 5s - loss: 0.0357 - acc: 0.9778 - val_loss: 0.1112 - val_acc: 0.9679
Epoch 20/40
 - 5s - loss: 0.0359 - acc: 0.9778 - val_loss: 0.1037 - val_acc: 0.9678
Epoch 21/40
 - 5s - loss: 0.0359 - acc: 0.9783 - val_loss: 0.1149 - val_acc: 0.9678
Epoch 22/40
 - 5s - loss: 0.0359 - acc: 0.9776 - val_loss: 0.1139 - val_acc: 0.9683
Epoch 23/40
 - 5s - loss: 0.0354 - acc: 0.9777 - val_loss: 0.1170 - val_acc: 0.9680
Epoch 24/40
 - 5s - loss: 0.0352 - acc: 0.9781 - val_loss: 0.1093 - val_acc: 0.9681
Epoch 25/40
 - 5s - loss: 0.0350 - acc: 0.9777 - val_loss: 0.1221 - val_acc: 0.9669
Epoch 26/40
 - 5s - loss: 0.0348 - acc: 0.9785 - val_loss: 0.1163 - val_acc: 0.9669
Epoch 27/40
 - 5s - loss: 0.0354 - acc: 0.9780 - val_loss: 0.1219 - val_acc: 0.9681
Epoch 28/40
 - 5s - loss: 0.0354 - acc: 0.9779 - val_loss: 0.1161 - val_acc: 0.9685
Epoch 29/40
 - 5s - loss: 0.0356 - acc: 0.9779 - val_loss: 0.1187 - val_acc: 0.9673
Epoch 30/40
 - 5s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.1208 - val_acc: 0.9675
Epoch 31/40
 - 5s - loss: 0.0344 - acc: 0.9782 - val_loss: 0.1173 - val_acc: 0.9682
Epoch 32/40
 - 5s - loss: 0.0349 - acc: 0.9782 - val_loss: 0.1200 - val_acc: 0.9674
Epoch 33/40
 - 5s - loss: 0.0343 - acc: 0.9781 - val_loss: 0.1231 - val_acc: 0.9678
Epoch 34/40
 - 5s - loss: 0.0342 - acc: 0.9782 - val_loss: 0.1260 - val_acc: 0.9668
Epoch 35/40
 - 5s - loss: 0.0345 - acc: 0.9782 - val_loss: 0.1221 - val_acc: 0.9675
Epoch 36/40
 - 5s - loss: 0.0345 - acc: 0.9782 - val_loss: 0.1261 - val_acc: 0.9675
Epoch 37/40
 - 5s - loss: 0.0342 - acc: 0.9782 - val_loss: 0.1196 - val_acc: 0.9688
Epoch 38/40
 - 5s - loss: 0.0340 - acc: 0.9784 - val_loss: 0.1280 - val_acc: 0.9662
Epoch 39/40
 - 5s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.1200 - val_acc: 0.9676
Epoch 40/40
 - 5s - loss: 0.0340 - acc: 0.9783 - val_loss: 0.1222 - val_acc: 0.9678
Epoch 1/40
 - 1s - loss: 0.1026 - acc: 0.9636
Epoch 2/40
 - 1s - loss: 0.0745 - acc: 0.9683
Epoch 3/40
 - 1s - loss: 0.0647 - acc: 0.9701
Epoch 4/40
 - 1s - loss: 0.0613 - acc: 0.9714
Epoch 5/40
 - 1s - loss: 0.0546 - acc: 0.9717
Epoch 6/40
 - 1s - loss: 0.0516 - acc: 0.9739
Epoch 7/40
 - 1s - loss: 0.0505 - acc: 0.9741
Epoch 8/40
 - 1s - loss: 0.0471 - acc: 0.9747
Epoch 9/40
 - 1s - loss: 0.0463 - acc: 0.9762
Epoch 10/40
 - 1s - loss: 0.0474 - acc: 0.9759
Epoch 11/40
 - 1s - loss: 0.0447 - acc: 0.9763
Epoch 12/40
 - 1s - loss: 0.0432 - acc: 0.9756
Epoch 13/40
 - 1s - loss: 0.0439 - acc: 0.9766
Epoch 14/40
 - 1s - loss: 0.0416 - acc: 0.9771
Epoch 15/40
 - 1s - loss: 0.0414 - acc: 0.9776
Epoch 16/40
 - 1s - loss: 0.0432 - acc: 0.9779
Epoch 17/40
 - 1s - loss: 0.0415 - acc: 0.9770
Epoch 18/40
 - 1s - loss: 0.0390 - acc: 0.9779
Epoch 19/40
 - 1s - loss: 0.0394 - acc: 0.9783
Epoch 20/40
 - 1s - loss: 0.0399 - acc: 0.9772
Epoch 21/40
 - 1s - loss: 0.0380 - acc: 0.9780
Epoch 22/40
 - 1s - loss: 0.0381 - acc: 0.9777
Epoch 23/40
 - 1s - loss: 0.0385 - acc: 0.9767
Epoch 24/40
 - 1s - loss: 0.0375 - acc: 0.9779
Epoch 25/40
 - 1s - loss: 0.0383 - acc: 0.9775
Epoch 26/40
 - 1s - loss: 0.0373 - acc: 0.9783
Epoch 27/40
 - 1s - loss: 0.0376 - acc: 0.9774
Epoch 28/40
 - 1s - loss: 0.0376 - acc: 0.9784
Epoch 29/40
 - 1s - loss: 0.0371 - acc: 0.9791
Epoch 30/40
 - 1s - loss: 0.0378 - acc: 0.9783
Epoch 31/40
 - 1s - loss: 0.0402 - acc: 0.9785
Epoch 32/40
 - 1s - loss: 0.0373 - acc: 0.9787
Epoch 33/40
 - 1s - loss: 0.0371 - acc: 0.9782
Epoch 34/40
 - 1s - loss: 0.0366 - acc: 0.9775
Epoch 35/40
 - 1s - loss: 0.0364 - acc: 0.9790
Epoch 36/40
 - 1s - loss: 0.0373 - acc: 0.9775
Epoch 37/40
 - 1s - loss: 0.0368 - acc: 0.9785
Epoch 38/40
 - 1s - loss: 0.0370 - acc: 0.9785
Epoch 39/40
 - 1s - loss: 0.0400 - acc: 0.9777
Epoch 40/40
 - 1s - loss: 0.0379 - acc: 0.9781
# Training time = 0:03:51.759534
# F-Score(Ordinary) = 0.251, Recall: 0.446, Precision: 0.174
# F-Score(lvc) = 0.452, Recall: 0.399, Precision: 0.523
# F-Score(id) = 0.01, Recall: 0.5, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_129 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_130 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_129 (Embedding)       (None, 4, 48)        705264      input_129[0][0]                  
__________________________________________________________________________________________________
embedding_130 (Embedding)       (None, 4, 24)        5640        input_130[0][0]                  
__________________________________________________________________________________________________
flatten_129 (Flatten)           (None, 192)          0           embedding_129[0][0]              
__________________________________________________________________________________________________
flatten_130 (Flatten)           (None, 96)           0           embedding_130[0][0]              
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 288)          0           flatten_129[0][0]                
                                                                 flatten_130[0][0]                
__________________________________________________________________________________________________
dense_129 (Dense)               (None, 24)           6936        concatenate_65[0][0]             
__________________________________________________________________________________________________
dropout_65 (Dropout)            (None, 24)           0           dense_129[0][0]                  
__________________________________________________________________________________________________
dense_130 (Dense)               (None, 8)            200         dropout_65[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1095 - acc: 0.9588 - val_loss: 0.0737 - val_acc: 0.9676
Epoch 2/40
 - 5s - loss: 0.0577 - acc: 0.9713 - val_loss: 0.0772 - val_acc: 0.9684
Epoch 3/40
 - 5s - loss: 0.0500 - acc: 0.9734 - val_loss: 0.0806 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0462 - acc: 0.9740 - val_loss: 0.0784 - val_acc: 0.9679
Epoch 5/40
 - 5s - loss: 0.0445 - acc: 0.9745 - val_loss: 0.0863 - val_acc: 0.9674
Epoch 6/40
 - 5s - loss: 0.0425 - acc: 0.9751 - val_loss: 0.0874 - val_acc: 0.9672
Epoch 7/40
 - 5s - loss: 0.0413 - acc: 0.9756 - val_loss: 0.0893 - val_acc: 0.9672
Epoch 8/40
 - 5s - loss: 0.0409 - acc: 0.9757 - val_loss: 0.0927 - val_acc: 0.9679
Epoch 9/40
 - 5s - loss: 0.0394 - acc: 0.9765 - val_loss: 0.0957 - val_acc: 0.9670
Epoch 10/40
 - 5s - loss: 0.0389 - acc: 0.9764 - val_loss: 0.0938 - val_acc: 0.9681
Epoch 11/40
 - 5s - loss: 0.0387 - acc: 0.9768 - val_loss: 0.1004 - val_acc: 0.9670
Epoch 12/40
 - 5s - loss: 0.0383 - acc: 0.9766 - val_loss: 0.0941 - val_acc: 0.9675
Epoch 13/40
 - 5s - loss: 0.0382 - acc: 0.9770 - val_loss: 0.0925 - val_acc: 0.9675
Epoch 14/40
 - 5s - loss: 0.0378 - acc: 0.9775 - val_loss: 0.1001 - val_acc: 0.9663
Epoch 15/40
 - 5s - loss: 0.0376 - acc: 0.9769 - val_loss: 0.0962 - val_acc: 0.9680
Epoch 16/40
 - 5s - loss: 0.0368 - acc: 0.9773 - val_loss: 0.1012 - val_acc: 0.9663
Epoch 17/40
 - 5s - loss: 0.0367 - acc: 0.9775 - val_loss: 0.1027 - val_acc: 0.9656
Epoch 18/40
 - 5s - loss: 0.0367 - acc: 0.9774 - val_loss: 0.1006 - val_acc: 0.9673
Epoch 19/40
 - 5s - loss: 0.0371 - acc: 0.9775 - val_loss: 0.1014 - val_acc: 0.9663
Epoch 20/40
 - 5s - loss: 0.0365 - acc: 0.9775 - val_loss: 0.1018 - val_acc: 0.9661
Epoch 21/40
 - 5s - loss: 0.0363 - acc: 0.9775 - val_loss: 0.1068 - val_acc: 0.9657
Epoch 22/40
 - 5s - loss: 0.0361 - acc: 0.9775 - val_loss: 0.1000 - val_acc: 0.9671
Epoch 23/40
 - 5s - loss: 0.0362 - acc: 0.9777 - val_loss: 0.1058 - val_acc: 0.9663
Epoch 24/40
 - 5s - loss: 0.0362 - acc: 0.9776 - val_loss: 0.1096 - val_acc: 0.9679
Epoch 25/40
 - 5s - loss: 0.0358 - acc: 0.9781 - val_loss: 0.1023 - val_acc: 0.9654
Epoch 26/40
 - 5s - loss: 0.0363 - acc: 0.9776 - val_loss: 0.1057 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0352 - acc: 0.9785 - val_loss: 0.1156 - val_acc: 0.9678
Epoch 28/40
 - 5s - loss: 0.0361 - acc: 0.9779 - val_loss: 0.1047 - val_acc: 0.9674
Epoch 29/40
 - 5s - loss: 0.0357 - acc: 0.9781 - val_loss: 0.1135 - val_acc: 0.9644
Epoch 30/40
 - 5s - loss: 0.0357 - acc: 0.9780 - val_loss: 0.1120 - val_acc: 0.9676
Epoch 31/40
 - 5s - loss: 0.0357 - acc: 0.9783 - val_loss: 0.1138 - val_acc: 0.9654
Epoch 32/40
 - 5s - loss: 0.0351 - acc: 0.9783 - val_loss: 0.1127 - val_acc: 0.9629
Epoch 33/40
 - 5s - loss: 0.0358 - acc: 0.9775 - val_loss: 0.1133 - val_acc: 0.9673
Epoch 34/40
 - 5s - loss: 0.0355 - acc: 0.9782 - val_loss: 0.1169 - val_acc: 0.9664
Epoch 35/40
 - 5s - loss: 0.0352 - acc: 0.9778 - val_loss: 0.1212 - val_acc: 0.9676
Epoch 36/40
 - 5s - loss: 0.0356 - acc: 0.9778 - val_loss: 0.1186 - val_acc: 0.9656
Epoch 37/40
 - 5s - loss: 0.0352 - acc: 0.9775 - val_loss: 0.1147 - val_acc: 0.9680
Epoch 38/40
 - 5s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.1176 - val_acc: 0.9660
Epoch 39/40
 - 5s - loss: 0.0355 - acc: 0.9781 - val_loss: 0.1208 - val_acc: 0.9647
Epoch 40/40
 - 5s - loss: 0.0351 - acc: 0.9783 - val_loss: 0.1130 - val_acc: 0.9676
Epoch 1/40
 - 1s - loss: 0.1004 - acc: 0.9648
Epoch 2/40
 - 1s - loss: 0.0726 - acc: 0.9687
Epoch 3/40
 - 1s - loss: 0.0631 - acc: 0.9709
Epoch 4/40
 - 1s - loss: 0.0599 - acc: 0.9728
Epoch 5/40
 - 1s - loss: 0.0541 - acc: 0.9722
Epoch 6/40
 - 1s - loss: 0.0530 - acc: 0.9741
Epoch 7/40
 - 1s - loss: 0.0496 - acc: 0.9754
Epoch 8/40
 - 1s - loss: 0.0491 - acc: 0.9752
Epoch 9/40
 - 1s - loss: 0.0465 - acc: 0.9752
Epoch 10/40
 - 1s - loss: 0.0466 - acc: 0.9753
Epoch 11/40
 - 1s - loss: 0.0460 - acc: 0.9764
Epoch 12/40
 - 1s - loss: 0.0451 - acc: 0.9753
Epoch 13/40
 - 1s - loss: 0.0430 - acc: 0.9766
Epoch 14/40
 - 1s - loss: 0.0420 - acc: 0.9775
Epoch 15/40
 - 1s - loss: 0.0402 - acc: 0.9776
Epoch 16/40
 - 1s - loss: 0.0407 - acc: 0.9772
Epoch 17/40
 - 1s - loss: 0.0411 - acc: 0.9766
Epoch 18/40
 - 1s - loss: 0.0407 - acc: 0.9776
Epoch 19/40
 - 1s - loss: 0.0401 - acc: 0.9777
Epoch 20/40
 - 1s - loss: 0.0388 - acc: 0.9790
Epoch 21/40
 - 1s - loss: 0.0405 - acc: 0.9772
Epoch 22/40
 - 1s - loss: 0.0396 - acc: 0.9787
Epoch 23/40
 - 1s - loss: 0.0390 - acc: 0.9786
Epoch 24/40
 - 1s - loss: 0.0394 - acc: 0.9774
Epoch 25/40
 - 1s - loss: 0.0394 - acc: 0.9774
Epoch 26/40
 - 1s - loss: 0.0377 - acc: 0.9788
Epoch 27/40
 - 1s - loss: 0.0378 - acc: 0.9768
Epoch 28/40
 - 1s - loss: 0.0370 - acc: 0.9786
Epoch 29/40
 - 1s - loss: 0.0371 - acc: 0.9785
Epoch 30/40
 - 1s - loss: 0.0382 - acc: 0.9786
Epoch 31/40
 - 1s - loss: 0.0366 - acc: 0.9784
Epoch 32/40
 - 1s - loss: 0.0375 - acc: 0.9782
Epoch 33/40
 - 1s - loss: 0.0362 - acc: 0.9788
Epoch 34/40
 - 1s - loss: 0.0373 - acc: 0.9771
Epoch 35/40
 - 1s - loss: 0.0376 - acc: 0.9782
Epoch 36/40
 - 1s - loss: 0.0373 - acc: 0.9792
Epoch 37/40
 - 1s - loss: 0.0362 - acc: 0.9786
Epoch 38/40
 - 1s - loss: 0.0368 - acc: 0.9793
Epoch 39/40
 - 1s - loss: 0.0373 - acc: 0.9781
Epoch 40/40
 - 1s - loss: 0.0383 - acc: 0.9778
# Training time = 0:03:50.043449
# F-Score(Ordinary) = 0.394, Recall: 0.4, Precision: 0.387
# F-Score(lvc) = 0.181, Recall: 0.225, Precision: 0.152
# F-Score(ireflv) = 0.385, Recall: 0.795, Precision: 0.254
# F-Score(id) = 0.467, Recall: 0.382, Precision: 0.601
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_131 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_132 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_131 (Embedding)       (None, 4, 48)        705264      input_131[0][0]                  
__________________________________________________________________________________________________
embedding_132 (Embedding)       (None, 4, 24)        5640        input_132[0][0]                  
__________________________________________________________________________________________________
flatten_131 (Flatten)           (None, 192)          0           embedding_131[0][0]              
__________________________________________________________________________________________________
flatten_132 (Flatten)           (None, 96)           0           embedding_132[0][0]              
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 288)          0           flatten_131[0][0]                
                                                                 flatten_132[0][0]                
__________________________________________________________________________________________________
dense_131 (Dense)               (None, 24)           6936        concatenate_66[0][0]             
__________________________________________________________________________________________________
dropout_66 (Dropout)            (None, 24)           0           dense_131[0][0]                  
__________________________________________________________________________________________________
dense_132 (Dense)               (None, 8)            200         dropout_66[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0942 - acc: 0.9624 - val_loss: 0.0748 - val_acc: 0.9665
Epoch 2/40
 - 5s - loss: 0.0597 - acc: 0.9702 - val_loss: 0.0804 - val_acc: 0.9673
Epoch 3/40
 - 5s - loss: 0.0524 - acc: 0.9728 - val_loss: 0.0833 - val_acc: 0.9672
Epoch 4/40
 - 5s - loss: 0.0505 - acc: 0.9739 - val_loss: 0.0836 - val_acc: 0.9676
Epoch 5/40
 - 5s - loss: 0.0481 - acc: 0.9742 - val_loss: 0.0933 - val_acc: 0.9671
Epoch 6/40
 - 5s - loss: 0.0474 - acc: 0.9745 - val_loss: 0.0920 - val_acc: 0.9685
Epoch 7/40
 - 5s - loss: 0.0469 - acc: 0.9751 - val_loss: 0.0951 - val_acc: 0.9671
Epoch 8/40
 - 5s - loss: 0.0450 - acc: 0.9750 - val_loss: 0.1029 - val_acc: 0.9673
Epoch 9/40
 - 5s - loss: 0.0451 - acc: 0.9754 - val_loss: 0.1003 - val_acc: 0.9671
Epoch 10/40
 - 5s - loss: 0.0456 - acc: 0.9754 - val_loss: 0.1032 - val_acc: 0.9665
Epoch 11/40
 - 5s - loss: 0.0444 - acc: 0.9760 - val_loss: 0.1122 - val_acc: 0.9655
Epoch 12/40
 - 5s - loss: 0.0436 - acc: 0.9763 - val_loss: 0.1095 - val_acc: 0.9672
Epoch 13/40
 - 5s - loss: 0.0433 - acc: 0.9762 - val_loss: 0.1064 - val_acc: 0.9674
Epoch 14/40
 - 5s - loss: 0.0435 - acc: 0.9758 - val_loss: 0.1251 - val_acc: 0.9674
Epoch 15/40
 - 5s - loss: 0.0439 - acc: 0.9764 - val_loss: 0.1176 - val_acc: 0.9673
Epoch 16/40
 - 5s - loss: 0.0438 - acc: 0.9760 - val_loss: 0.1197 - val_acc: 0.9656
Epoch 17/40
 - 5s - loss: 0.0437 - acc: 0.9762 - val_loss: 0.1161 - val_acc: 0.9668
Epoch 18/40
 - 5s - loss: 0.0432 - acc: 0.9762 - val_loss: 0.1200 - val_acc: 0.9675
Epoch 19/40
 - 5s - loss: 0.0441 - acc: 0.9764 - val_loss: 0.1232 - val_acc: 0.9661
Epoch 20/40
 - 5s - loss: 0.0441 - acc: 0.9765 - val_loss: 0.1330 - val_acc: 0.9672
Epoch 21/40
 - 5s - loss: 0.0431 - acc: 0.9764 - val_loss: 0.1213 - val_acc: 0.9648
Epoch 22/40
 - 5s - loss: 0.0426 - acc: 0.9766 - val_loss: 0.1170 - val_acc: 0.9682
Epoch 23/40
 - 5s - loss: 0.0444 - acc: 0.9764 - val_loss: 0.1258 - val_acc: 0.9673
Epoch 24/40
 - 5s - loss: 0.0458 - acc: 0.9767 - val_loss: 0.1215 - val_acc: 0.9667
Epoch 25/40
 - 5s - loss: 0.0439 - acc: 0.9770 - val_loss: 0.1244 - val_acc: 0.9672
Epoch 26/40
 - 5s - loss: 0.0432 - acc: 0.9767 - val_loss: 0.1325 - val_acc: 0.9670
Epoch 27/40
 - 5s - loss: 0.0446 - acc: 0.9767 - val_loss: 0.1290 - val_acc: 0.9667
Epoch 28/40
 - 5s - loss: 0.0448 - acc: 0.9768 - val_loss: 0.1304 - val_acc: 0.9654
Epoch 29/40
 - 5s - loss: 0.0436 - acc: 0.9768 - val_loss: 0.1382 - val_acc: 0.9667
Epoch 30/40
 - 5s - loss: 0.0453 - acc: 0.9766 - val_loss: 0.1410 - val_acc: 0.9676
Epoch 31/40
 - 5s - loss: 0.0441 - acc: 0.9769 - val_loss: 0.1336 - val_acc: 0.9667
Epoch 32/40
 - 5s - loss: 0.0436 - acc: 0.9771 - val_loss: 0.1349 - val_acc: 0.9670
Epoch 33/40
 - 5s - loss: 0.0442 - acc: 0.9767 - val_loss: 0.1393 - val_acc: 0.9665
Epoch 34/40
 - 5s - loss: 0.0439 - acc: 0.9771 - val_loss: 0.1314 - val_acc: 0.9673
Epoch 35/40
 - 5s - loss: 0.0444 - acc: 0.9767 - val_loss: 0.1317 - val_acc: 0.9679
Epoch 36/40
 - 5s - loss: 0.0438 - acc: 0.9771 - val_loss: 0.1279 - val_acc: 0.9678
Epoch 37/40
 - 5s - loss: 0.0443 - acc: 0.9773 - val_loss: 0.1365 - val_acc: 0.9669
Epoch 38/40
 - 5s - loss: 0.0444 - acc: 0.9771 - val_loss: 0.1262 - val_acc: 0.9671
Epoch 39/40
 - 5s - loss: 0.0439 - acc: 0.9772 - val_loss: 0.1276 - val_acc: 0.9651
Epoch 40/40
 - 5s - loss: 0.0441 - acc: 0.9765 - val_loss: 0.1376 - val_acc: 0.9667
Epoch 1/40
 - 1s - loss: 0.1189 - acc: 0.9637
Epoch 2/40
 - 1s - loss: 0.0939 - acc: 0.9689
Epoch 3/40
 - 1s - loss: 0.0764 - acc: 0.9693
Epoch 4/40
 - 1s - loss: 0.0677 - acc: 0.9700
Epoch 5/40
 - 1s - loss: 0.0636 - acc: 0.9715
Epoch 6/40
 - 1s - loss: 0.0578 - acc: 0.9724
Epoch 7/40
 - 1s - loss: 0.0556 - acc: 0.9742
Epoch 8/40
 - 1s - loss: 0.0545 - acc: 0.9750
Epoch 9/40
 - 1s - loss: 0.0536 - acc: 0.9750
Epoch 10/40
 - 1s - loss: 0.0557 - acc: 0.9751
Epoch 11/40
 - 1s - loss: 0.0509 - acc: 0.9758
Epoch 12/40
 - 1s - loss: 0.0499 - acc: 0.9757
Epoch 13/40
 - 1s - loss: 0.0505 - acc: 0.9765
Epoch 14/40
 - 1s - loss: 0.0490 - acc: 0.9764
Epoch 15/40
 - 1s - loss: 0.0501 - acc: 0.9759
Epoch 16/40
 - 1s - loss: 0.0481 - acc: 0.9754
Epoch 17/40
 - 1s - loss: 0.0446 - acc: 0.9768
Epoch 18/40
 - 1s - loss: 0.0462 - acc: 0.9765
Epoch 19/40
 - 1s - loss: 0.0469 - acc: 0.9773
Epoch 20/40
 - 1s - loss: 0.0455 - acc: 0.9776
Epoch 21/40
 - 1s - loss: 0.0426 - acc: 0.9778
Epoch 22/40
 - 1s - loss: 0.0468 - acc: 0.9770
Epoch 23/40
 - 1s - loss: 0.0453 - acc: 0.9779
Epoch 24/40
 - 1s - loss: 0.0468 - acc: 0.9775
Epoch 25/40
 - 1s - loss: 0.0449 - acc: 0.9773
Epoch 26/40
 - 1s - loss: 0.0424 - acc: 0.9788
Epoch 27/40
 - 1s - loss: 0.0479 - acc: 0.9797
Epoch 28/40
 - 1s - loss: 0.0458 - acc: 0.9775
Epoch 29/40
 - 1s - loss: 0.0441 - acc: 0.9783
Epoch 30/40
 - 1s - loss: 0.0466 - acc: 0.9783
Epoch 31/40
 - 1s - loss: 0.0446 - acc: 0.9780
Epoch 32/40
 - 1s - loss: 0.0423 - acc: 0.9792
Epoch 33/40
 - 1s - loss: 0.0438 - acc: 0.9779
Epoch 34/40
 - 1s - loss: 0.0485 - acc: 0.9774
Epoch 35/40
 - 1s - loss: 0.0446 - acc: 0.9771
Epoch 36/40
 - 1s - loss: 0.0454 - acc: 0.9777
Epoch 37/40
 - 1s - loss: 0.0410 - acc: 0.9778
Epoch 38/40
 - 1s - loss: 0.0414 - acc: 0.9784
Epoch 39/40
 - 1s - loss: 0.0440 - acc: 0.9779
Epoch 40/40
 - 1s - loss: 0.0456 - acc: 0.9773
# Training time = 0:03:49.144123
# F-Score(Ordinary) = 0.092, Recall: 0.418, Precision: 0.051
# F-Score(lvc) = 0.195, Recall: 0.682, Precision: 0.114
# F-Score(id) = 0.071, Recall: 0.242, Precision: 0.041
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_133 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_134 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_133 (Embedding)       (None, 4, 48)        705264      input_133[0][0]                  
__________________________________________________________________________________________________
embedding_134 (Embedding)       (None, 4, 24)        5640        input_134[0][0]                  
__________________________________________________________________________________________________
flatten_133 (Flatten)           (None, 192)          0           embedding_133[0][0]              
__________________________________________________________________________________________________
flatten_134 (Flatten)           (None, 96)           0           embedding_134[0][0]              
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 288)          0           flatten_133[0][0]                
                                                                 flatten_134[0][0]                
__________________________________________________________________________________________________
dense_133 (Dense)               (None, 24)           6936        concatenate_67[0][0]             
__________________________________________________________________________________________________
dropout_67 (Dropout)            (None, 24)           0           dense_133[0][0]                  
__________________________________________________________________________________________________
dense_134 (Dense)               (None, 8)            200         dropout_67[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0962 - acc: 0.9631 - val_loss: 0.0712 - val_acc: 0.9676
Epoch 2/40
 - 5s - loss: 0.0601 - acc: 0.9711 - val_loss: 0.0797 - val_acc: 0.9668
Epoch 3/40
 - 5s - loss: 0.0536 - acc: 0.9725 - val_loss: 0.0820 - val_acc: 0.9673
Epoch 4/40
 - 5s - loss: 0.0514 - acc: 0.9738 - val_loss: 0.0802 - val_acc: 0.9688
Epoch 5/40
 - 5s - loss: 0.0484 - acc: 0.9745 - val_loss: 0.0885 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0470 - acc: 0.9744 - val_loss: 0.0898 - val_acc: 0.9676
Epoch 7/40
 - 5s - loss: 0.0464 - acc: 0.9748 - val_loss: 0.0921 - val_acc: 0.9678
Epoch 8/40
 - 5s - loss: 0.0459 - acc: 0.9752 - val_loss: 0.1001 - val_acc: 0.9667
Epoch 9/40
 - 5s - loss: 0.0457 - acc: 0.9747 - val_loss: 0.0939 - val_acc: 0.9678
Epoch 10/40
 - 5s - loss: 0.0456 - acc: 0.9754 - val_loss: 0.1042 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0440 - acc: 0.9759 - val_loss: 0.1020 - val_acc: 0.9686
Epoch 12/40
 - 5s - loss: 0.0445 - acc: 0.9758 - val_loss: 0.1127 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0439 - acc: 0.9760 - val_loss: 0.1056 - val_acc: 0.9672
Epoch 14/40
 - 5s - loss: 0.0443 - acc: 0.9759 - val_loss: 0.1094 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0436 - acc: 0.9760 - val_loss: 0.1069 - val_acc: 0.9674
Epoch 16/40
 - 5s - loss: 0.0443 - acc: 0.9764 - val_loss: 0.1077 - val_acc: 0.9668
Epoch 17/40
 - 5s - loss: 0.0446 - acc: 0.9761 - val_loss: 0.1115 - val_acc: 0.9666
Epoch 18/40
 - 5s - loss: 0.0441 - acc: 0.9767 - val_loss: 0.1132 - val_acc: 0.9681
Epoch 19/40
 - 5s - loss: 0.0449 - acc: 0.9765 - val_loss: 0.1105 - val_acc: 0.9682
Epoch 20/40
 - 5s - loss: 0.0436 - acc: 0.9764 - val_loss: 0.1121 - val_acc: 0.9663
Epoch 21/40
 - 5s - loss: 0.0441 - acc: 0.9767 - val_loss: 0.1190 - val_acc: 0.9670
Epoch 22/40
 - 5s - loss: 0.0437 - acc: 0.9766 - val_loss: 0.1114 - val_acc: 0.9678
Epoch 23/40
 - 5s - loss: 0.0431 - acc: 0.9769 - val_loss: 0.1194 - val_acc: 0.9689
Epoch 24/40
 - 5s - loss: 0.0448 - acc: 0.9764 - val_loss: 0.1178 - val_acc: 0.9677
Epoch 25/40
 - 5s - loss: 0.0448 - acc: 0.9761 - val_loss: 0.1207 - val_acc: 0.9682
Epoch 26/40
 - 5s - loss: 0.0442 - acc: 0.9765 - val_loss: 0.1235 - val_acc: 0.9685
Epoch 27/40
 - 5s - loss: 0.0439 - acc: 0.9769 - val_loss: 0.1279 - val_acc: 0.9682
Epoch 28/40
 - 5s - loss: 0.0441 - acc: 0.9766 - val_loss: 0.1254 - val_acc: 0.9664
Epoch 29/40
 - 5s - loss: 0.0439 - acc: 0.9772 - val_loss: 0.1232 - val_acc: 0.9678
Epoch 30/40
 - 5s - loss: 0.0448 - acc: 0.9770 - val_loss: 0.1243 - val_acc: 0.9674
Epoch 31/40
 - 5s - loss: 0.0446 - acc: 0.9769 - val_loss: 0.1267 - val_acc: 0.9679
Epoch 32/40
 - 5s - loss: 0.0451 - acc: 0.9766 - val_loss: 0.1273 - val_acc: 0.9676
Epoch 33/40
 - 5s - loss: 0.0443 - acc: 0.9772 - val_loss: 0.1298 - val_acc: 0.9625
Epoch 34/40
 - 5s - loss: 0.0452 - acc: 0.9768 - val_loss: 0.1308 - val_acc: 0.9674
Epoch 35/40
 - 5s - loss: 0.0457 - acc: 0.9769 - val_loss: 0.1208 - val_acc: 0.9678
Epoch 36/40
 - 5s - loss: 0.0459 - acc: 0.9774 - val_loss: 0.1216 - val_acc: 0.9679
Epoch 37/40
 - 5s - loss: 0.0458 - acc: 0.9772 - val_loss: 0.1276 - val_acc: 0.9680
Epoch 38/40
 - 5s - loss: 0.0454 - acc: 0.9770 - val_loss: 0.1389 - val_acc: 0.9676
Epoch 39/40
 - 5s - loss: 0.0456 - acc: 0.9773 - val_loss: 0.1299 - val_acc: 0.9679
Epoch 40/40
 - 5s - loss: 0.0457 - acc: 0.9771 - val_loss: 0.1371 - val_acc: 0.9641
Epoch 1/40
 - 1s - loss: 0.1139 - acc: 0.9644
Epoch 2/40
 - 1s - loss: 0.0924 - acc: 0.9674
Epoch 3/40
 - 1s - loss: 0.0790 - acc: 0.9688
Epoch 4/40
 - 1s - loss: 0.0701 - acc: 0.9702
Epoch 5/40
 - 1s - loss: 0.0698 - acc: 0.9740
Epoch 6/40
 - 1s - loss: 0.0667 - acc: 0.9731
Epoch 7/40
 - 1s - loss: 0.0547 - acc: 0.9731
Epoch 8/40
 - 1s - loss: 0.0538 - acc: 0.9745
Epoch 9/40
 - 1s - loss: 0.0543 - acc: 0.9741
Epoch 10/40
 - 1s - loss: 0.0512 - acc: 0.9757
Epoch 11/40
 - 1s - loss: 0.0485 - acc: 0.9758
Epoch 12/40
 - 1s - loss: 0.0491 - acc: 0.9769
Epoch 13/40
 - 1s - loss: 0.0517 - acc: 0.9764
Epoch 14/40
 - 1s - loss: 0.0501 - acc: 0.9757
Epoch 15/40
 - 1s - loss: 0.0494 - acc: 0.9764
Epoch 16/40
 - 1s - loss: 0.0455 - acc: 0.9778
Epoch 17/40
 - 1s - loss: 0.0449 - acc: 0.9774
Epoch 18/40
 - 1s - loss: 0.0468 - acc: 0.9772
Epoch 19/40
 - 1s - loss: 0.0475 - acc: 0.9766
Epoch 20/40
 - 1s - loss: 0.0465 - acc: 0.9755
Epoch 21/40
 - 1s - loss: 0.0471 - acc: 0.9768
Epoch 22/40
 - 1s - loss: 0.0458 - acc: 0.9778
Epoch 23/40
 - 1s - loss: 0.0471 - acc: 0.9773
Epoch 24/40
 - 1s - loss: 0.0450 - acc: 0.9776
Epoch 25/40
 - 1s - loss: 0.0479 - acc: 0.9774
Epoch 26/40
 - 1s - loss: 0.0449 - acc: 0.9776
Epoch 27/40
 - 1s - loss: 0.0534 - acc: 0.9776
Epoch 28/40
 - 1s - loss: 0.0466 - acc: 0.9777
Epoch 29/40
 - 1s - loss: 0.0503 - acc: 0.9770
Epoch 30/40
 - 1s - loss: 0.0460 - acc: 0.9783
Epoch 31/40
 - 1s - loss: 0.0448 - acc: 0.9781
Epoch 32/40
 - 1s - loss: 0.0444 - acc: 0.9776
Epoch 33/40
 - 1s - loss: 0.0419 - acc: 0.9797
Epoch 34/40
 - 1s - loss: 0.0423 - acc: 0.9777
Epoch 35/40
 - 1s - loss: 0.0450 - acc: 0.9777
Epoch 36/40
 - 1s - loss: 0.0436 - acc: 0.9773
Epoch 37/40
 - 1s - loss: 0.0421 - acc: 0.9775
Epoch 38/40
 - 1s - loss: 0.0444 - acc: 0.9777
Epoch 39/40
 - 1s - loss: 0.0444 - acc: 0.9785
Epoch 40/40
 - 1s - loss: 0.0467 - acc: 0.9778
# Training time = 0:03:51.028968
# F-Score(Ordinary) = 0.191, Recall: 0.742, Precision: 0.11
# F-Score(id) = 0.378, Recall: 0.742, Precision: 0.254
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_135 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_136 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_135 (Embedding)       (None, 4, 48)        705264      input_135[0][0]                  
__________________________________________________________________________________________________
embedding_136 (Embedding)       (None, 4, 24)        5640        input_136[0][0]                  
__________________________________________________________________________________________________
flatten_135 (Flatten)           (None, 192)          0           embedding_135[0][0]              
__________________________________________________________________________________________________
flatten_136 (Flatten)           (None, 96)           0           embedding_136[0][0]              
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 288)          0           flatten_135[0][0]                
                                                                 flatten_136[0][0]                
__________________________________________________________________________________________________
dense_135 (Dense)               (None, 24)           6936        concatenate_68[0][0]             
__________________________________________________________________________________________________
dropout_68 (Dropout)            (None, 24)           0           dense_135[0][0]                  
__________________________________________________________________________________________________
dense_136 (Dense)               (None, 8)            200         dropout_68[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0933 - acc: 0.9633 - val_loss: 0.0762 - val_acc: 0.9677
Epoch 2/40
 - 5s - loss: 0.0589 - acc: 0.9708 - val_loss: 0.0800 - val_acc: 0.9649
Epoch 3/40
 - 5s - loss: 0.0532 - acc: 0.9729 - val_loss: 0.0846 - val_acc: 0.9657
Epoch 4/40
 - 5s - loss: 0.0502 - acc: 0.9742 - val_loss: 0.0865 - val_acc: 0.9680
Epoch 5/40
 - 5s - loss: 0.0484 - acc: 0.9746 - val_loss: 0.0982 - val_acc: 0.9682
Epoch 6/40
 - 5s - loss: 0.0475 - acc: 0.9743 - val_loss: 0.0964 - val_acc: 0.9681
Epoch 7/40
 - 5s - loss: 0.0478 - acc: 0.9748 - val_loss: 0.0841 - val_acc: 0.9681
Epoch 8/40
 - 5s - loss: 0.0457 - acc: 0.9752 - val_loss: 0.1033 - val_acc: 0.9683
Epoch 9/40
 - 5s - loss: 0.0460 - acc: 0.9751 - val_loss: 0.0934 - val_acc: 0.9671
Epoch 10/40
 - 5s - loss: 0.0454 - acc: 0.9754 - val_loss: 0.0985 - val_acc: 0.9665
Epoch 11/40
 - 5s - loss: 0.0445 - acc: 0.9758 - val_loss: 0.1016 - val_acc: 0.9675
Epoch 12/40
 - 5s - loss: 0.0447 - acc: 0.9757 - val_loss: 0.1099 - val_acc: 0.9668
Epoch 13/40
 - 5s - loss: 0.0443 - acc: 0.9761 - val_loss: 0.1056 - val_acc: 0.9685
Epoch 14/40
 - 5s - loss: 0.0443 - acc: 0.9762 - val_loss: 0.1095 - val_acc: 0.9652
Epoch 15/40
 - 5s - loss: 0.0436 - acc: 0.9765 - val_loss: 0.1014 - val_acc: 0.9691
Epoch 16/40
 - 5s - loss: 0.0444 - acc: 0.9760 - val_loss: 0.1090 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0447 - acc: 0.9765 - val_loss: 0.1087 - val_acc: 0.9670
Epoch 18/40
 - 5s - loss: 0.0433 - acc: 0.9761 - val_loss: 0.1077 - val_acc: 0.9662
Epoch 19/40
 - 5s - loss: 0.0442 - acc: 0.9758 - val_loss: 0.1207 - val_acc: 0.9681
Epoch 20/40
 - 5s - loss: 0.0438 - acc: 0.9765 - val_loss: 0.1132 - val_acc: 0.9677
Epoch 21/40
 - 5s - loss: 0.0437 - acc: 0.9768 - val_loss: 0.1126 - val_acc: 0.9665
Epoch 22/40
 - 5s - loss: 0.0437 - acc: 0.9764 - val_loss: 0.1320 - val_acc: 0.9653
Epoch 23/40
 - 5s - loss: 0.0447 - acc: 0.9766 - val_loss: 0.1189 - val_acc: 0.9664
Epoch 24/40
 - 5s - loss: 0.0432 - acc: 0.9765 - val_loss: 0.1194 - val_acc: 0.9668
Epoch 25/40
 - 5s - loss: 0.0443 - acc: 0.9764 - val_loss: 0.1265 - val_acc: 0.9676
Epoch 26/40
 - 5s - loss: 0.0437 - acc: 0.9769 - val_loss: 0.1274 - val_acc: 0.9660
Epoch 27/40
 - 5s - loss: 0.0440 - acc: 0.9766 - val_loss: 0.1293 - val_acc: 0.9680
Epoch 28/40
 - 5s - loss: 0.0440 - acc: 0.9767 - val_loss: 0.1369 - val_acc: 0.9628
Epoch 29/40
 - 5s - loss: 0.0445 - acc: 0.9764 - val_loss: 0.1334 - val_acc: 0.9669
Epoch 30/40
 - 5s - loss: 0.0447 - acc: 0.9766 - val_loss: 0.1356 - val_acc: 0.9645
Epoch 31/40
 - 5s - loss: 0.0441 - acc: 0.9766 - val_loss: 0.1378 - val_acc: 0.9666
Epoch 32/40
 - 5s - loss: 0.0443 - acc: 0.9770 - val_loss: 0.1374 - val_acc: 0.9678
Epoch 33/40
 - 5s - loss: 0.0440 - acc: 0.9775 - val_loss: 0.1396 - val_acc: 0.9684
Epoch 34/40
 - 5s - loss: 0.0444 - acc: 0.9766 - val_loss: 0.1260 - val_acc: 0.9671
Epoch 35/40
 - 5s - loss: 0.0445 - acc: 0.9763 - val_loss: 0.1277 - val_acc: 0.9671
Epoch 36/40
 - 5s - loss: 0.0440 - acc: 0.9769 - val_loss: 0.1326 - val_acc: 0.9668
Epoch 37/40
 - 5s - loss: 0.0428 - acc: 0.9775 - val_loss: 0.1375 - val_acc: 0.9672
Epoch 38/40
 - 5s - loss: 0.0448 - acc: 0.9768 - val_loss: 0.1414 - val_acc: 0.9671
Epoch 39/40
 - 5s - loss: 0.0446 - acc: 0.9769 - val_loss: 0.1335 - val_acc: 0.9642
Epoch 40/40
 - 5s - loss: 0.0451 - acc: 0.9771 - val_loss: 0.1330 - val_acc: 0.9675
Epoch 1/40
 - 0s - loss: 0.1175 - acc: 0.9647
Epoch 2/40
 - 0s - loss: 0.0934 - acc: 0.9683
Epoch 3/40
 - 0s - loss: 0.0794 - acc: 0.9706
Epoch 4/40
 - 0s - loss: 0.0770 - acc: 0.9709
Epoch 5/40
 - 0s - loss: 0.0681 - acc: 0.9711
Epoch 6/40
 - 0s - loss: 0.0628 - acc: 0.9737
Epoch 7/40
 - 0s - loss: 0.0590 - acc: 0.9745
Epoch 8/40
 - 0s - loss: 0.0563 - acc: 0.9749
Epoch 9/40
 - 0s - loss: 0.0572 - acc: 0.9754
Epoch 10/40
 - 0s - loss: 0.0521 - acc: 0.9753
Epoch 11/40
 - 0s - loss: 0.0483 - acc: 0.9764
Epoch 12/40
 - 0s - loss: 0.0506 - acc: 0.9762
Epoch 13/40
 - 0s - loss: 0.0523 - acc: 0.9760
Epoch 14/40
 - 0s - loss: 0.0479 - acc: 0.9758
Epoch 15/40
 - 0s - loss: 0.0467 - acc: 0.9778
Epoch 16/40
 - 0s - loss: 0.0464 - acc: 0.9772
Epoch 17/40
 - 0s - loss: 0.0486 - acc: 0.9783
Epoch 18/40
 - 0s - loss: 0.0466 - acc: 0.9770
Epoch 19/40
 - 0s - loss: 0.0474 - acc: 0.9781
Epoch 20/40
 - 0s - loss: 0.0464 - acc: 0.9778
Epoch 21/40
 - 0s - loss: 0.0463 - acc: 0.9768
Epoch 22/40
 - 0s - loss: 0.0468 - acc: 0.9787
Epoch 23/40
 - 0s - loss: 0.0452 - acc: 0.9775
Epoch 24/40
 - 0s - loss: 0.0445 - acc: 0.9781
Epoch 25/40
 - 0s - loss: 0.0450 - acc: 0.9775
Epoch 26/40
 - 0s - loss: 0.0503 - acc: 0.9780
Epoch 27/40
 - 0s - loss: 0.0432 - acc: 0.9782
Epoch 28/40
 - 0s - loss: 0.0436 - acc: 0.9778
Epoch 29/40
 - 0s - loss: 0.0441 - acc: 0.9769
Epoch 30/40
 - 0s - loss: 0.0458 - acc: 0.9778
Epoch 31/40
 - 0s - loss: 0.0480 - acc: 0.9774
Epoch 32/40
 - 0s - loss: 0.0473 - acc: 0.9777
Epoch 33/40
 - 0s - loss: 0.0420 - acc: 0.9783
Epoch 34/40
 - 0s - loss: 0.0462 - acc: 0.9784
Epoch 35/40
 - 0s - loss: 0.0467 - acc: 0.9777
Epoch 36/40
 - 0s - loss: 0.0491 - acc: 0.9769
Epoch 37/40
 - 0s - loss: 0.0451 - acc: 0.9787
Epoch 38/40
 - 0s - loss: 0.0448 - acc: 0.9779
Epoch 39/40
 - 0s - loss: 0.0437 - acc: 0.9772
Epoch 40/40
 - 0s - loss: 0.0445 - acc: 0.9780
# Training time = 0:03:47.656015
# F-Score(Ordinary) = 0.257, Recall: 0.457, Precision: 0.179
# F-Score(lvc) = 0.03, Recall: 1.0, Precision: 0.015
# F-Score(id) = 0.426, Recall: 0.451, Precision: 0.404
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_137 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_138 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_137 (Embedding)       (None, 4, 48)        705264      input_137[0][0]                  
__________________________________________________________________________________________________
embedding_138 (Embedding)       (None, 4, 24)        5640        input_138[0][0]                  
__________________________________________________________________________________________________
flatten_137 (Flatten)           (None, 192)          0           embedding_137[0][0]              
__________________________________________________________________________________________________
flatten_138 (Flatten)           (None, 96)           0           embedding_138[0][0]              
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 288)          0           flatten_137[0][0]                
                                                                 flatten_138[0][0]                
__________________________________________________________________________________________________
dense_137 (Dense)               (None, 24)           6936        concatenate_69[0][0]             
__________________________________________________________________________________________________
dropout_69 (Dropout)            (None, 24)           0           dense_137[0][0]                  
__________________________________________________________________________________________________
dense_138 (Dense)               (None, 8)            200         dropout_69[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0958 - acc: 0.9622 - val_loss: 0.0722 - val_acc: 0.9670
Epoch 2/40
 - 5s - loss: 0.0590 - acc: 0.9713 - val_loss: 0.0769 - val_acc: 0.9663
Epoch 3/40
 - 5s - loss: 0.0533 - acc: 0.9727 - val_loss: 0.0788 - val_acc: 0.9679
Epoch 4/40
 - 5s - loss: 0.0503 - acc: 0.9738 - val_loss: 0.0895 - val_acc: 0.9675
Epoch 5/40
 - 5s - loss: 0.0486 - acc: 0.9747 - val_loss: 0.0892 - val_acc: 0.9669
Epoch 6/40
 - 5s - loss: 0.0466 - acc: 0.9753 - val_loss: 0.0977 - val_acc: 0.9681
Epoch 7/40
 - 5s - loss: 0.0475 - acc: 0.9750 - val_loss: 0.1018 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0462 - acc: 0.9750 - val_loss: 0.1024 - val_acc: 0.9661
Epoch 9/40
 - 5s - loss: 0.0448 - acc: 0.9757 - val_loss: 0.0976 - val_acc: 0.9673
Epoch 10/40
 - 5s - loss: 0.0448 - acc: 0.9757 - val_loss: 0.1072 - val_acc: 0.9661
Epoch 11/40
 - 5s - loss: 0.0451 - acc: 0.9756 - val_loss: 0.0984 - val_acc: 0.9678
Epoch 12/40
 - 5s - loss: 0.0448 - acc: 0.9757 - val_loss: 0.0977 - val_acc: 0.9684
Epoch 13/40
 - 5s - loss: 0.0435 - acc: 0.9760 - val_loss: 0.1051 - val_acc: 0.9680
Epoch 14/40
 - 5s - loss: 0.0450 - acc: 0.9755 - val_loss: 0.1088 - val_acc: 0.9680
Epoch 15/40
 - 5s - loss: 0.0444 - acc: 0.9760 - val_loss: 0.1091 - val_acc: 0.9692
Epoch 16/40
 - 5s - loss: 0.0438 - acc: 0.9761 - val_loss: 0.1094 - val_acc: 0.9685
Epoch 17/40
 - 5s - loss: 0.0443 - acc: 0.9760 - val_loss: 0.1033 - val_acc: 0.9681
Epoch 18/40
 - 5s - loss: 0.0441 - acc: 0.9765 - val_loss: 0.1158 - val_acc: 0.9679
Epoch 19/40
 - 5s - loss: 0.0440 - acc: 0.9763 - val_loss: 0.1194 - val_acc: 0.9677
Epoch 20/40
 - 5s - loss: 0.0445 - acc: 0.9764 - val_loss: 0.1148 - val_acc: 0.9678
Epoch 21/40
 - 5s - loss: 0.0432 - acc: 0.9764 - val_loss: 0.1202 - val_acc: 0.9672
Epoch 22/40
 - 5s - loss: 0.0438 - acc: 0.9761 - val_loss: 0.1213 - val_acc: 0.9666
Epoch 23/40
 - 5s - loss: 0.0433 - acc: 0.9767 - val_loss: 0.1234 - val_acc: 0.9673
Epoch 24/40
 - 5s - loss: 0.0432 - acc: 0.9766 - val_loss: 0.1175 - val_acc: 0.9676
Epoch 25/40
 - 5s - loss: 0.0440 - acc: 0.9766 - val_loss: 0.1292 - val_acc: 0.9667
Epoch 26/40
 - 5s - loss: 0.0445 - acc: 0.9767 - val_loss: 0.1191 - val_acc: 0.9665
Epoch 27/40
 - 5s - loss: 0.0434 - acc: 0.9769 - val_loss: 0.1218 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0445 - acc: 0.9767 - val_loss: 0.1235 - val_acc: 0.9674
Epoch 29/40
 - 5s - loss: 0.0438 - acc: 0.9764 - val_loss: 0.1279 - val_acc: 0.9664
Epoch 30/40
 - 5s - loss: 0.0439 - acc: 0.9767 - val_loss: 0.1245 - val_acc: 0.9663
Epoch 31/40
 - 5s - loss: 0.0435 - acc: 0.9765 - val_loss: 0.1317 - val_acc: 0.9647
Epoch 32/40
 - 5s - loss: 0.0444 - acc: 0.9767 - val_loss: 0.1233 - val_acc: 0.9669
Epoch 33/40
 - 5s - loss: 0.0441 - acc: 0.9768 - val_loss: 0.1368 - val_acc: 0.9673
Epoch 34/40
 - 5s - loss: 0.0447 - acc: 0.9766 - val_loss: 0.1233 - val_acc: 0.9670
Epoch 35/40
 - 5s - loss: 0.0446 - acc: 0.9771 - val_loss: 0.1276 - val_acc: 0.9677
Epoch 36/40
 - 5s - loss: 0.0452 - acc: 0.9768 - val_loss: 0.1380 - val_acc: 0.9663
Epoch 37/40
 - 5s - loss: 0.0445 - acc: 0.9767 - val_loss: 0.1332 - val_acc: 0.9673
Epoch 38/40
 - 5s - loss: 0.0448 - acc: 0.9767 - val_loss: 0.1242 - val_acc: 0.9674
Epoch 39/40
 - 5s - loss: 0.0457 - acc: 0.9768 - val_loss: 0.1346 - val_acc: 0.9678
Epoch 40/40
 - 5s - loss: 0.0441 - acc: 0.9775 - val_loss: 0.1364 - val_acc: 0.9670
Epoch 1/40
 - 1s - loss: 0.1189 - acc: 0.9633
Epoch 2/40
 - 1s - loss: 0.0848 - acc: 0.9686
Epoch 3/40
 - 1s - loss: 0.0802 - acc: 0.9681
Epoch 4/40
 - 1s - loss: 0.0718 - acc: 0.9712
Epoch 5/40
 - 1s - loss: 0.0669 - acc: 0.9729
Epoch 6/40
 - 1s - loss: 0.0679 - acc: 0.9737
Epoch 7/40
 - 1s - loss: 0.0626 - acc: 0.9728
Epoch 8/40
 - 1s - loss: 0.0595 - acc: 0.9751
Epoch 9/40
 - 1s - loss: 0.0560 - acc: 0.9761
Epoch 10/40
 - 1s - loss: 0.0567 - acc: 0.9757
Epoch 11/40
 - 1s - loss: 0.0556 - acc: 0.9759
Epoch 12/40
 - 1s - loss: 0.0557 - acc: 0.9761
Epoch 13/40
 - 1s - loss: 0.0575 - acc: 0.9751
Epoch 14/40
 - 1s - loss: 0.0525 - acc: 0.9765
Epoch 15/40
 - 1s - loss: 0.0528 - acc: 0.9770
Epoch 16/40
 - 1s - loss: 0.0561 - acc: 0.9773
Epoch 17/40
 - 1s - loss: 0.0568 - acc: 0.9771
Epoch 18/40
 - 1s - loss: 0.0567 - acc: 0.9770
Epoch 19/40
 - 1s - loss: 0.0551 - acc: 0.9767
Epoch 20/40
 - 1s - loss: 0.0494 - acc: 0.9762
Epoch 21/40
 - 1s - loss: 0.0492 - acc: 0.9767
Epoch 22/40
 - 1s - loss: 0.0516 - acc: 0.9769
Epoch 23/40
 - 1s - loss: 0.0523 - acc: 0.9771
Epoch 24/40
 - 1s - loss: 0.0546 - acc: 0.9770
Epoch 25/40
 - 1s - loss: 0.0545 - acc: 0.9767
Epoch 26/40
 - 1s - loss: 0.0549 - acc: 0.9769
Epoch 27/40
 - 1s - loss: 0.0534 - acc: 0.9778
Epoch 28/40
 - 1s - loss: 0.0496 - acc: 0.9765
Epoch 29/40
 - 1s - loss: 0.0467 - acc: 0.9781
Epoch 30/40
 - 1s - loss: 0.0477 - acc: 0.9772
Epoch 31/40
 - 1s - loss: 0.0506 - acc: 0.9778
Epoch 32/40
 - 1s - loss: 0.0492 - acc: 0.9772
Epoch 33/40
 - 1s - loss: 0.0462 - acc: 0.9775
Epoch 34/40
 - 1s - loss: 0.0463 - acc: 0.9777
Epoch 35/40
 - 1s - loss: 0.0473 - acc: 0.9783
Epoch 36/40
 - 1s - loss: 0.0514 - acc: 0.9769
Epoch 37/40
 - 1s - loss: 0.0463 - acc: 0.9781
Epoch 38/40
 - 1s - loss: 0.0460 - acc: 0.9782
Epoch 39/40
 - 1s - loss: 0.0500 - acc: 0.9770
Epoch 40/40
 - 1s - loss: 0.0487 - acc: 0.9770
# Training time = 0:03:49.665886
# F-Score(Ordinary) = 0.068, Recall: 0.696, Precision: 0.036
# F-Score(lvc) = 0.183, Recall: 0.667, Precision: 0.106
# F-Score(ireflv) = 0.032, Recall: 1.0, Precision: 0.016
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_139 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_140 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_139 (Embedding)       (None, 4, 48)        705264      input_139[0][0]                  
__________________________________________________________________________________________________
embedding_140 (Embedding)       (None, 4, 24)        5640        input_140[0][0]                  
__________________________________________________________________________________________________
flatten_139 (Flatten)           (None, 192)          0           embedding_139[0][0]              
__________________________________________________________________________________________________
flatten_140 (Flatten)           (None, 96)           0           embedding_140[0][0]              
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 288)          0           flatten_139[0][0]                
                                                                 flatten_140[0][0]                
__________________________________________________________________________________________________
dense_139 (Dense)               (None, 24)           6936        concatenate_70[0][0]             
__________________________________________________________________________________________________
dropout_70 (Dropout)            (None, 24)           0           dense_139[0][0]                  
__________________________________________________________________________________________________
dense_140 (Dense)               (None, 8)            200         dropout_70[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0973 - acc: 0.9615 - val_loss: 0.0765 - val_acc: 0.9658
Epoch 2/40
 - 5s - loss: 0.0604 - acc: 0.9708 - val_loss: 0.0799 - val_acc: 0.9663
Epoch 3/40
 - 5s - loss: 0.0539 - acc: 0.9722 - val_loss: 0.0831 - val_acc: 0.9676
Epoch 4/40
 - 5s - loss: 0.0495 - acc: 0.9737 - val_loss: 0.0809 - val_acc: 0.9675
Epoch 5/40
 - 5s - loss: 0.0491 - acc: 0.9743 - val_loss: 0.0874 - val_acc: 0.9668
Epoch 6/40
 - 5s - loss: 0.0480 - acc: 0.9743 - val_loss: 0.0891 - val_acc: 0.9666
Epoch 7/40
 - 5s - loss: 0.0469 - acc: 0.9741 - val_loss: 0.0958 - val_acc: 0.9674
Epoch 8/40
 - 5s - loss: 0.0471 - acc: 0.9749 - val_loss: 0.1067 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0458 - acc: 0.9756 - val_loss: 0.0962 - val_acc: 0.9676
Epoch 10/40
 - 5s - loss: 0.0460 - acc: 0.9749 - val_loss: 0.1026 - val_acc: 0.9667
Epoch 11/40
 - 5s - loss: 0.0456 - acc: 0.9756 - val_loss: 0.1032 - val_acc: 0.9666
Epoch 12/40
 - 5s - loss: 0.0448 - acc: 0.9756 - val_loss: 0.1038 - val_acc: 0.9676
Epoch 13/40
 - 5s - loss: 0.0445 - acc: 0.9758 - val_loss: 0.1052 - val_acc: 0.9677
Epoch 14/40
 - 5s - loss: 0.0446 - acc: 0.9760 - val_loss: 0.1098 - val_acc: 0.9665
Epoch 15/40
 - 5s - loss: 0.0449 - acc: 0.9757 - val_loss: 0.0986 - val_acc: 0.9677
Epoch 16/40
 - 5s - loss: 0.0439 - acc: 0.9762 - val_loss: 0.1199 - val_acc: 0.9670
Epoch 17/40
 - 5s - loss: 0.0442 - acc: 0.9758 - val_loss: 0.1106 - val_acc: 0.9654
Epoch 18/40
 - 5s - loss: 0.0447 - acc: 0.9759 - val_loss: 0.1084 - val_acc: 0.9667
Epoch 19/40
 - 5s - loss: 0.0434 - acc: 0.9761 - val_loss: 0.1069 - val_acc: 0.9680
Epoch 20/40
 - 5s - loss: 0.0432 - acc: 0.9763 - val_loss: 0.1113 - val_acc: 0.9654
Epoch 21/40
 - 5s - loss: 0.0442 - acc: 0.9760 - val_loss: 0.1070 - val_acc: 0.9666
Epoch 22/40
 - 5s - loss: 0.0441 - acc: 0.9757 - val_loss: 0.1106 - val_acc: 0.9672
Epoch 23/40
 - 5s - loss: 0.0435 - acc: 0.9763 - val_loss: 0.1126 - val_acc: 0.9662
Epoch 24/40
 - 5s - loss: 0.0443 - acc: 0.9759 - val_loss: 0.1303 - val_acc: 0.9669
Epoch 25/40
 - 5s - loss: 0.0442 - acc: 0.9767 - val_loss: 0.1161 - val_acc: 0.9668
Epoch 26/40
 - 5s - loss: 0.0446 - acc: 0.9764 - val_loss: 0.1199 - val_acc: 0.9670
Epoch 27/40
 - 5s - loss: 0.0438 - acc: 0.9762 - val_loss: 0.1173 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0434 - acc: 0.9765 - val_loss: 0.1230 - val_acc: 0.9660
Epoch 29/40
 - 5s - loss: 0.0444 - acc: 0.9766 - val_loss: 0.1269 - val_acc: 0.9678
Epoch 30/40
 - 5s - loss: 0.0439 - acc: 0.9766 - val_loss: 0.1161 - val_acc: 0.9680
Epoch 31/40
 - 5s - loss: 0.0446 - acc: 0.9764 - val_loss: 0.1216 - val_acc: 0.9676
Epoch 32/40
 - 5s - loss: 0.0438 - acc: 0.9768 - val_loss: 0.1235 - val_acc: 0.9665
Epoch 33/40
 - 5s - loss: 0.0444 - acc: 0.9764 - val_loss: 0.1219 - val_acc: 0.9673
Epoch 34/40
 - 5s - loss: 0.0434 - acc: 0.9766 - val_loss: 0.1282 - val_acc: 0.9664
Epoch 35/40
 - 5s - loss: 0.0435 - acc: 0.9765 - val_loss: 0.1322 - val_acc: 0.9679
Epoch 36/40
 - 5s - loss: 0.0440 - acc: 0.9763 - val_loss: 0.1180 - val_acc: 0.9676
Epoch 37/40
 - 5s - loss: 0.0442 - acc: 0.9764 - val_loss: 0.1337 - val_acc: 0.9682
Epoch 38/40
 - 5s - loss: 0.0436 - acc: 0.9768 - val_loss: 0.1286 - val_acc: 0.9671
Epoch 39/40
 - 5s - loss: 0.0452 - acc: 0.9769 - val_loss: 0.1311 - val_acc: 0.9673
Epoch 40/40
 - 5s - loss: 0.0449 - acc: 0.9766 - val_loss: 0.1389 - val_acc: 0.9676
Epoch 1/40
 - 1s - loss: 0.1264 - acc: 0.9642
Epoch 2/40
 - 1s - loss: 0.0892 - acc: 0.9665
Epoch 3/40
 - 1s - loss: 0.0771 - acc: 0.9697
Epoch 4/40
 - 1s - loss: 0.0704 - acc: 0.9706
Epoch 5/40
 - 1s - loss: 0.0647 - acc: 0.9714
Epoch 6/40
 - 1s - loss: 0.0614 - acc: 0.9713
Epoch 7/40
 - 1s - loss: 0.0565 - acc: 0.9734
Epoch 8/40
 - 1s - loss: 0.0561 - acc: 0.9737
Epoch 9/40
 - 1s - loss: 0.0549 - acc: 0.9732
Epoch 10/40
 - 1s - loss: 0.0575 - acc: 0.9737
Epoch 11/40
 - 1s - loss: 0.0513 - acc: 0.9751
Epoch 12/40
 - 1s - loss: 0.0537 - acc: 0.9741
Epoch 13/40
 - 1s - loss: 0.0529 - acc: 0.9764
Epoch 14/40
 - 1s - loss: 0.0522 - acc: 0.9758
Epoch 15/40
 - 1s - loss: 0.0513 - acc: 0.9758
Epoch 16/40
 - 1s - loss: 0.0510 - acc: 0.9753
Epoch 17/40
 - 1s - loss: 0.0486 - acc: 0.9746
Epoch 18/40
 - 1s - loss: 0.0466 - acc: 0.9770
Epoch 19/40
 - 1s - loss: 0.0481 - acc: 0.9759
Epoch 20/40
 - 1s - loss: 0.0482 - acc: 0.9754
Epoch 21/40
 - 1s - loss: 0.0541 - acc: 0.9742
Epoch 22/40
 - 1s - loss: 0.0617 - acc: 0.9764
Epoch 23/40
 - 1s - loss: 0.0481 - acc: 0.9765
Epoch 24/40
 - 1s - loss: 0.0468 - acc: 0.9766
Epoch 25/40
 - 1s - loss: 0.0460 - acc: 0.9762
Epoch 26/40
 - 1s - loss: 0.0471 - acc: 0.9768
Epoch 27/40
 - 1s - loss: 0.0468 - acc: 0.9757
Epoch 28/40
 - 1s - loss: 0.0438 - acc: 0.9768
Epoch 29/40
 - 1s - loss: 0.0453 - acc: 0.9767
Epoch 30/40
 - 1s - loss: 0.0446 - acc: 0.9772
Epoch 31/40
 - 1s - loss: 0.0480 - acc: 0.9769
Epoch 32/40
 - 1s - loss: 0.0450 - acc: 0.9780
Epoch 33/40
 - 1s - loss: 0.0463 - acc: 0.9771
Epoch 34/40
 - 1s - loss: 0.0459 - acc: 0.9770
Epoch 35/40
 - 1s - loss: 0.0430 - acc: 0.9788
Epoch 36/40
 - 1s - loss: 0.0423 - acc: 0.9781
Epoch 37/40
 - 1s - loss: 0.0430 - acc: 0.9783
Epoch 38/40
 - 1s - loss: 0.0450 - acc: 0.9773
Epoch 39/40
 - 1s - loss: 0.0447 - acc: 0.9767
Epoch 40/40
 - 1s - loss: 0.0453 - acc: 0.9762
# Training time = 0:03:50.469534
# F-Score(Ordinary) = 0.487, Recall: 0.82, Precision: 0.347
# F-Score(lvc) = 0.495, Recall: 0.774, Precision: 0.364
# F-Score(ireflv) = 0.225, Recall: 0.8, Precision: 0.131
# F-Score(id) = 0.607, Recall: 0.85, Precision: 0.472
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_141 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_142 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_141 (Embedding)       (None, 4, 48)        705264      input_141[0][0]                  
__________________________________________________________________________________________________
embedding_142 (Embedding)       (None, 4, 24)        5640        input_142[0][0]                  
__________________________________________________________________________________________________
flatten_141 (Flatten)           (None, 192)          0           embedding_141[0][0]              
__________________________________________________________________________________________________
flatten_142 (Flatten)           (None, 96)           0           embedding_142[0][0]              
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 288)          0           flatten_141[0][0]                
                                                                 flatten_142[0][0]                
__________________________________________________________________________________________________
dense_141 (Dense)               (None, 24)           6936        concatenate_71[0][0]             
__________________________________________________________________________________________________
dropout_71 (Dropout)            (None, 24)           0           dense_141[0][0]                  
__________________________________________________________________________________________________
dense_142 (Dense)               (None, 8)            200         dropout_71[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0985 - acc: 0.9618 - val_loss: 0.0895 - val_acc: 0.9633
Epoch 2/40
 - 5s - loss: 0.0722 - acc: 0.9686 - val_loss: 0.0885 - val_acc: 0.9666
Epoch 3/40
 - 5s - loss: 0.0668 - acc: 0.9703 - val_loss: 0.0959 - val_acc: 0.9662
Epoch 4/40
 - 5s - loss: 0.0655 - acc: 0.9717 - val_loss: 0.0943 - val_acc: 0.9658
Epoch 5/40
 - 5s - loss: 0.0636 - acc: 0.9719 - val_loss: 0.1131 - val_acc: 0.9646
Epoch 6/40
 - 5s - loss: 0.0655 - acc: 0.9718 - val_loss: 0.1117 - val_acc: 0.9673
Epoch 7/40
 - 5s - loss: 0.0647 - acc: 0.9723 - val_loss: 0.1056 - val_acc: 0.9646
Epoch 8/40
 - 5s - loss: 0.0633 - acc: 0.9732 - val_loss: 0.1106 - val_acc: 0.9670
Epoch 9/40
 - 5s - loss: 0.0631 - acc: 0.9736 - val_loss: 0.1133 - val_acc: 0.9673
Epoch 10/40
 - 5s - loss: 0.0640 - acc: 0.9733 - val_loss: 0.1197 - val_acc: 0.9663
Epoch 11/40
 - 5s - loss: 0.0618 - acc: 0.9739 - val_loss: 0.1220 - val_acc: 0.9671
Epoch 12/40
 - 5s - loss: 0.0693 - acc: 0.9734 - val_loss: 0.1380 - val_acc: 0.9664
Epoch 13/40
 - 5s - loss: 0.0670 - acc: 0.9733 - val_loss: 0.1407 - val_acc: 0.9654
Epoch 14/40
 - 5s - loss: 0.0654 - acc: 0.9736 - val_loss: 0.1526 - val_acc: 0.9660
Epoch 15/40
 - 5s - loss: 0.0681 - acc: 0.9729 - val_loss: 0.1277 - val_acc: 0.9673
Epoch 16/40
 - 5s - loss: 0.0671 - acc: 0.9734 - val_loss: 0.1305 - val_acc: 0.9648
Epoch 17/40
 - 5s - loss: 0.0693 - acc: 0.9738 - val_loss: 0.1403 - val_acc: 0.9656
Epoch 18/40
 - 5s - loss: 0.0677 - acc: 0.9733 - val_loss: 0.1642 - val_acc: 0.9661
Epoch 19/40
 - 5s - loss: 0.0720 - acc: 0.9740 - val_loss: 0.1489 - val_acc: 0.9667
Epoch 20/40
 - 5s - loss: 0.0726 - acc: 0.9738 - val_loss: 0.1503 - val_acc: 0.9654
Epoch 21/40
 - 5s - loss: 0.0745 - acc: 0.9739 - val_loss: 0.1408 - val_acc: 0.9671
Epoch 22/40
 - 5s - loss: 0.0724 - acc: 0.9737 - val_loss: 0.1483 - val_acc: 0.9663
Epoch 23/40
 - 5s - loss: 0.0734 - acc: 0.9742 - val_loss: 0.1810 - val_acc: 0.9645
Epoch 24/40
 - 5s - loss: 0.0780 - acc: 0.9738 - val_loss: 0.1562 - val_acc: 0.9649
Epoch 25/40
 - 5s - loss: 0.0782 - acc: 0.9728 - val_loss: 0.1608 - val_acc: 0.9670
Epoch 26/40
 - 5s - loss: 0.0775 - acc: 0.9741 - val_loss: 0.1517 - val_acc: 0.9651
Epoch 27/40
 - 5s - loss: 0.0787 - acc: 0.9734 - val_loss: 0.1669 - val_acc: 0.9655
Epoch 28/40
 - 5s - loss: 0.0838 - acc: 0.9739 - val_loss: 0.1558 - val_acc: 0.9664
Epoch 29/40
 - 5s - loss: 0.0849 - acc: 0.9740 - val_loss: 0.1732 - val_acc: 0.9655
Epoch 30/40
 - 5s - loss: 0.0822 - acc: 0.9749 - val_loss: 0.1974 - val_acc: 0.9643
Epoch 31/40
 - 5s - loss: 0.0852 - acc: 0.9741 - val_loss: 0.1861 - val_acc: 0.9657
Epoch 32/40
 - 5s - loss: 0.0908 - acc: 0.9740 - val_loss: 0.1928 - val_acc: 0.9642
Epoch 33/40
 - 5s - loss: 0.0873 - acc: 0.9734 - val_loss: 0.1824 - val_acc: 0.9634
Epoch 34/40
 - 5s - loss: 0.0841 - acc: 0.9741 - val_loss: 0.1996 - val_acc: 0.9649
Epoch 35/40
 - 5s - loss: 0.0938 - acc: 0.9737 - val_loss: 0.1604 - val_acc: 0.9663
Epoch 36/40
 - 5s - loss: 0.0953 - acc: 0.9745 - val_loss: 0.1932 - val_acc: 0.9662
Epoch 37/40
 - 5s - loss: 0.0948 - acc: 0.9743 - val_loss: 0.2057 - val_acc: 0.9657
Epoch 38/40
 - 5s - loss: 0.0982 - acc: 0.9749 - val_loss: 0.1938 - val_acc: 0.9654
Epoch 39/40
 - 5s - loss: 0.1033 - acc: 0.9741 - val_loss: 0.1865 - val_acc: 0.9649
Epoch 40/40
 - 5s - loss: 0.0953 - acc: 0.9738 - val_loss: 0.1914 - val_acc: 0.9652
Epoch 1/40
 - 1s - loss: 0.1887 - acc: 0.9621
Epoch 2/40
 - 1s - loss: 0.1723 - acc: 0.9635
Epoch 3/40
 - 1s - loss: 0.1672 - acc: 0.9655
Epoch 4/40
 - 1s - loss: 0.1549 - acc: 0.9664
Epoch 5/40
 - 1s - loss: 0.1468 - acc: 0.9673
Epoch 6/40
 - 1s - loss: 0.1379 - acc: 0.9699
Epoch 7/40
 - 1s - loss: 0.1444 - acc: 0.9679
Epoch 8/40
 - 1s - loss: 0.1402 - acc: 0.9697
Epoch 9/40
 - 1s - loss: 0.1524 - acc: 0.9684
Epoch 10/40
 - 1s - loss: 0.1506 - acc: 0.9698
Epoch 11/40
 - 1s - loss: 0.1378 - acc: 0.9710
Epoch 12/40
 - 1s - loss: 0.1433 - acc: 0.9708
Epoch 13/40
 - 1s - loss: 0.1350 - acc: 0.9717
Epoch 14/40
 - 1s - loss: 0.1392 - acc: 0.9720
Epoch 15/40
 - 1s - loss: 0.1331 - acc: 0.9717
Epoch 16/40
 - 1s - loss: 0.1417 - acc: 0.9717
Epoch 17/40
 - 1s - loss: 0.1320 - acc: 0.9717
Epoch 18/40
 - 1s - loss: 0.1374 - acc: 0.9722
Epoch 19/40
 - 1s - loss: 0.1367 - acc: 0.9705
Epoch 20/40
 - 1s - loss: 0.1369 - acc: 0.9721
Epoch 21/40
 - 1s - loss: 0.1328 - acc: 0.9726
Epoch 22/40
 - 1s - loss: 0.1359 - acc: 0.9726
Epoch 23/40
 - 1s - loss: 0.1371 - acc: 0.9722
Epoch 24/40
 - 1s - loss: 0.1473 - acc: 0.9695
Epoch 25/40
 - 1s - loss: 0.1584 - acc: 0.9708
Epoch 26/40
 - 1s - loss: 0.1541 - acc: 0.9727
Epoch 27/40
 - 1s - loss: 0.1582 - acc: 0.9728
Epoch 28/40
 - 1s - loss: 0.1348 - acc: 0.9721
Epoch 29/40
 - 1s - loss: 0.1288 - acc: 0.9731
Epoch 30/40
 - 1s - loss: 0.1433 - acc: 0.9724
Epoch 31/40
 - 1s - loss: 0.1441 - acc: 0.9729
Epoch 32/40
 - 1s - loss: 0.1429 - acc: 0.9741
Epoch 33/40
 - 1s - loss: 0.1531 - acc: 0.9748
Epoch 34/40
 - 1s - loss: 0.1494 - acc: 0.9743
Epoch 35/40
 - 1s - loss: 0.1499 - acc: 0.9723
Epoch 36/40
 - 1s - loss: 0.1453 - acc: 0.9739
Epoch 37/40
 - 1s - loss: 0.1373 - acc: 0.9748
Epoch 38/40
 - 1s - loss: 0.1372 - acc: 0.9743
Epoch 39/40
 - 1s - loss: 0.1433 - acc: 0.9729
Epoch 40/40
 - 1s - loss: 0.1428 - acc: 0.9739
# Training time = 0:03:51.612755
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_143 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_144 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_143 (Embedding)       (None, 4, 48)        705264      input_143[0][0]                  
__________________________________________________________________________________________________
embedding_144 (Embedding)       (None, 4, 24)        5640        input_144[0][0]                  
__________________________________________________________________________________________________
flatten_143 (Flatten)           (None, 192)          0           embedding_143[0][0]              
__________________________________________________________________________________________________
flatten_144 (Flatten)           (None, 96)           0           embedding_144[0][0]              
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 288)          0           flatten_143[0][0]                
                                                                 flatten_144[0][0]                
__________________________________________________________________________________________________
dense_143 (Dense)               (None, 24)           6936        concatenate_72[0][0]             
__________________________________________________________________________________________________
dropout_72 (Dropout)            (None, 24)           0           dense_143[0][0]                  
__________________________________________________________________________________________________
dense_144 (Dense)               (None, 8)            200         dropout_72[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0996 - acc: 0.9616 - val_loss: 0.0791 - val_acc: 0.9651
Epoch 2/40
 - 5s - loss: 0.0724 - acc: 0.9685 - val_loss: 0.0926 - val_acc: 0.9640
Epoch 3/40
 - 5s - loss: 0.0676 - acc: 0.9700 - val_loss: 0.0917 - val_acc: 0.9661
Epoch 4/40
 - 5s - loss: 0.0661 - acc: 0.9709 - val_loss: 0.0983 - val_acc: 0.9651
Epoch 5/40
 - 5s - loss: 0.0648 - acc: 0.9716 - val_loss: 0.1159 - val_acc: 0.9664
Epoch 6/40
 - 5s - loss: 0.0643 - acc: 0.9723 - val_loss: 0.1111 - val_acc: 0.9654
Epoch 7/40
 - 5s - loss: 0.0638 - acc: 0.9728 - val_loss: 0.1143 - val_acc: 0.9672
Epoch 8/40
 - 5s - loss: 0.0639 - acc: 0.9729 - val_loss: 0.1237 - val_acc: 0.9662
Epoch 9/40
 - 5s - loss: 0.0663 - acc: 0.9729 - val_loss: 0.1186 - val_acc: 0.9678
Epoch 10/40
 - 5s - loss: 0.0646 - acc: 0.9734 - val_loss: 0.1213 - val_acc: 0.9675
Epoch 11/40
 - 5s - loss: 0.0661 - acc: 0.9733 - val_loss: 0.1330 - val_acc: 0.9671
Epoch 12/40
 - 5s - loss: 0.0669 - acc: 0.9735 - val_loss: 0.1363 - val_acc: 0.9671
Epoch 13/40
 - 5s - loss: 0.0689 - acc: 0.9731 - val_loss: 0.1369 - val_acc: 0.9658
Epoch 14/40
 - 5s - loss: 0.0681 - acc: 0.9737 - val_loss: 0.1743 - val_acc: 0.9673
Epoch 15/40
 - 5s - loss: 0.0672 - acc: 0.9743 - val_loss: 0.1361 - val_acc: 0.9663
Epoch 16/40
 - 5s - loss: 0.0655 - acc: 0.9745 - val_loss: 0.1344 - val_acc: 0.9656
Epoch 17/40
 - 5s - loss: 0.0690 - acc: 0.9732 - val_loss: 0.1356 - val_acc: 0.9668
Epoch 18/40
 - 5s - loss: 0.0674 - acc: 0.9739 - val_loss: 0.1505 - val_acc: 0.9665
Epoch 19/40
 - 5s - loss: 0.0690 - acc: 0.9744 - val_loss: 0.1469 - val_acc: 0.9656
Epoch 20/40
 - 5s - loss: 0.0729 - acc: 0.9736 - val_loss: 0.1580 - val_acc: 0.9663
Epoch 21/40
 - 5s - loss: 0.0735 - acc: 0.9741 - val_loss: 0.1543 - val_acc: 0.9676
Epoch 22/40
 - 5s - loss: 0.0743 - acc: 0.9738 - val_loss: 0.1661 - val_acc: 0.9657
Epoch 23/40
 - 5s - loss: 0.0744 - acc: 0.9729 - val_loss: 0.1640 - val_acc: 0.9650
Epoch 24/40
 - 5s - loss: 0.0769 - acc: 0.9736 - val_loss: 0.1781 - val_acc: 0.9639
Epoch 25/40
 - 5s - loss: 0.0761 - acc: 0.9736 - val_loss: 0.1524 - val_acc: 0.9660
Epoch 26/40
 - 5s - loss: 0.0739 - acc: 0.9733 - val_loss: 0.1547 - val_acc: 0.9654
Epoch 27/40
 - 5s - loss: 0.0754 - acc: 0.9730 - val_loss: 0.1823 - val_acc: 0.9652
Epoch 28/40
 - 5s - loss: 0.0842 - acc: 0.9724 - val_loss: 0.1556 - val_acc: 0.9654
Epoch 29/40
 - 5s - loss: 0.0771 - acc: 0.9735 - val_loss: 0.1652 - val_acc: 0.9623
Epoch 30/40
 - 5s - loss: 0.0796 - acc: 0.9735 - val_loss: 0.1569 - val_acc: 0.9662
Epoch 31/40
 - 5s - loss: 0.0886 - acc: 0.9732 - val_loss: 0.1695 - val_acc: 0.9638
Epoch 32/40
 - 5s - loss: 0.0882 - acc: 0.9735 - val_loss: 0.2172 - val_acc: 0.9650
Epoch 33/40
 - 5s - loss: 0.0955 - acc: 0.9734 - val_loss: 0.1780 - val_acc: 0.9661
Epoch 34/40
 - 5s - loss: 0.0955 - acc: 0.9737 - val_loss: 0.1764 - val_acc: 0.9660
Epoch 35/40
 - 5s - loss: 0.0957 - acc: 0.9738 - val_loss: 0.2202 - val_acc: 0.9625
Epoch 36/40
 - 5s - loss: 0.0998 - acc: 0.9736 - val_loss: 0.2143 - val_acc: 0.9640
Epoch 37/40
 - 5s - loss: 0.0947 - acc: 0.9740 - val_loss: 0.1865 - val_acc: 0.9606
Epoch 38/40
 - 5s - loss: 0.0977 - acc: 0.9739 - val_loss: 0.1830 - val_acc: 0.9654
Epoch 39/40
 - 5s - loss: 0.1117 - acc: 0.9738 - val_loss: 0.1820 - val_acc: 0.9644
Epoch 40/40
 - 5s - loss: 0.1014 - acc: 0.9746 - val_loss: 0.2037 - val_acc: 0.9643
Epoch 1/40
 - 1s - loss: 0.2235 - acc: 0.9627
Epoch 2/40
 - 1s - loss: 0.1773 - acc: 0.9648
Epoch 3/40
 - 1s - loss: 0.1664 - acc: 0.9656
Epoch 4/40
 - 1s - loss: 0.1706 - acc: 0.9666
Epoch 5/40
 - 1s - loss: 0.1598 - acc: 0.9672
Epoch 6/40
 - 1s - loss: 0.1518 - acc: 0.9688
Epoch 7/40
 - 1s - loss: 0.1459 - acc: 0.9703
Epoch 8/40
 - 1s - loss: 0.1574 - acc: 0.9670
Epoch 9/40
 - 1s - loss: 0.1534 - acc: 0.9685
Epoch 10/40
 - 1s - loss: 0.1528 - acc: 0.9705
Epoch 11/40
 - 1s - loss: 0.1417 - acc: 0.9718
Epoch 12/40
 - 1s - loss: 0.1408 - acc: 0.9723
Epoch 13/40
 - 1s - loss: 0.1347 - acc: 0.9718
Epoch 14/40
 - 1s - loss: 0.1419 - acc: 0.9704
Epoch 15/40
 - 1s - loss: 0.1347 - acc: 0.9727
Epoch 16/40
 - 1s - loss: 0.1300 - acc: 0.9733
Epoch 17/40
 - 1s - loss: 0.1339 - acc: 0.9728
Epoch 18/40
 - 1s - loss: 0.1606 - acc: 0.9723
Epoch 19/40
 - 1s - loss: 0.1608 - acc: 0.9701
Epoch 20/40
 - 1s - loss: 0.1494 - acc: 0.9719
Epoch 21/40
 - 1s - loss: 0.1386 - acc: 0.9729
Epoch 22/40
 - 1s - loss: 0.1385 - acc: 0.9727
Epoch 23/40
 - 1s - loss: 0.1423 - acc: 0.9725
Epoch 24/40
 - 1s - loss: 0.1574 - acc: 0.9721
Epoch 25/40
 - 1s - loss: 0.1567 - acc: 0.9722
Epoch 26/40
 - 1s - loss: 0.1718 - acc: 0.9720
Epoch 27/40
 - 1s - loss: 0.1455 - acc: 0.9736
Epoch 28/40
 - 1s - loss: 0.1509 - acc: 0.9736
Epoch 29/40
 - 1s - loss: 0.1457 - acc: 0.9740
Epoch 30/40
 - 1s - loss: 0.1463 - acc: 0.9737
Epoch 31/40
 - 1s - loss: 0.1462 - acc: 0.9742
Epoch 32/40
 - 1s - loss: 0.1446 - acc: 0.9744
Epoch 33/40
 - 1s - loss: 0.1436 - acc: 0.9748
Epoch 34/40
 - 1s - loss: 0.1491 - acc: 0.9737
Epoch 35/40
 - 1s - loss: 0.1790 - acc: 0.9727
Epoch 36/40
 - 1s - loss: 0.1668 - acc: 0.9728
Epoch 37/40
 - 1s - loss: 0.1516 - acc: 0.9739
Epoch 38/40
 - 1s - loss: 0.1468 - acc: 0.9742
Epoch 39/40
 - 1s - loss: 0.1499 - acc: 0.9744
Epoch 40/40
 - 1s - loss: 0.1565 - acc: 0.9741
# Training time = 0:03:50.644162
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_145 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_146 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_145 (Embedding)       (None, 4, 48)        705264      input_145[0][0]                  
__________________________________________________________________________________________________
embedding_146 (Embedding)       (None, 4, 24)        5640        input_146[0][0]                  
__________________________________________________________________________________________________
flatten_145 (Flatten)           (None, 192)          0           embedding_145[0][0]              
__________________________________________________________________________________________________
flatten_146 (Flatten)           (None, 96)           0           embedding_146[0][0]              
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 288)          0           flatten_145[0][0]                
                                                                 flatten_146[0][0]                
__________________________________________________________________________________________________
dense_145 (Dense)               (None, 24)           6936        concatenate_73[0][0]             
__________________________________________________________________________________________________
dropout_73 (Dropout)            (None, 24)           0           dense_145[0][0]                  
__________________________________________________________________________________________________
dense_146 (Dense)               (None, 8)            200         dropout_73[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1014 - acc: 0.9617 - val_loss: 0.0855 - val_acc: 0.9662
Epoch 2/40
 - 5s - loss: 0.0735 - acc: 0.9686 - val_loss: 0.0908 - val_acc: 0.9642
Epoch 3/40
 - 5s - loss: 0.0703 - acc: 0.9702 - val_loss: 0.0970 - val_acc: 0.9651
Epoch 4/40
 - 5s - loss: 0.0670 - acc: 0.9716 - val_loss: 0.1028 - val_acc: 0.9653
Epoch 5/40
 - 5s - loss: 0.0666 - acc: 0.9718 - val_loss: 0.1208 - val_acc: 0.9674
Epoch 6/40
 - 5s - loss: 0.0683 - acc: 0.9717 - val_loss: 0.1277 - val_acc: 0.9660
Epoch 7/40
 - 5s - loss: 0.0656 - acc: 0.9728 - val_loss: 0.1053 - val_acc: 0.9665
Epoch 8/40
 - 5s - loss: 0.0667 - acc: 0.9729 - val_loss: 0.1177 - val_acc: 0.9666
Epoch 9/40
 - 5s - loss: 0.0682 - acc: 0.9725 - val_loss: 0.1139 - val_acc: 0.9669
Epoch 10/40
 - 5s - loss: 0.0662 - acc: 0.9731 - val_loss: 0.1324 - val_acc: 0.9659
Epoch 11/40
 - 5s - loss: 0.0684 - acc: 0.9728 - val_loss: 0.1377 - val_acc: 0.9667
Epoch 12/40
 - 5s - loss: 0.0689 - acc: 0.9731 - val_loss: 0.1261 - val_acc: 0.9674
Epoch 13/40
 - 5s - loss: 0.0699 - acc: 0.9732 - val_loss: 0.1796 - val_acc: 0.9652
Epoch 14/40
 - 5s - loss: 0.0685 - acc: 0.9729 - val_loss: 0.1326 - val_acc: 0.9631
Epoch 15/40
 - 5s - loss: 0.0675 - acc: 0.9734 - val_loss: 0.1268 - val_acc: 0.9664
Epoch 16/40
 - 5s - loss: 0.0687 - acc: 0.9735 - val_loss: 0.1445 - val_acc: 0.9657
Epoch 17/40
 - 5s - loss: 0.0701 - acc: 0.9731 - val_loss: 0.1441 - val_acc: 0.9659
Epoch 18/40
 - 5s - loss: 0.0707 - acc: 0.9731 - val_loss: 0.1269 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0756 - acc: 0.9731 - val_loss: 0.1612 - val_acc: 0.9662
Epoch 20/40
 - 5s - loss: 0.0786 - acc: 0.9729 - val_loss: 0.1292 - val_acc: 0.9671
Epoch 21/40
 - 5s - loss: 0.0761 - acc: 0.9734 - val_loss: 0.1458 - val_acc: 0.9645
Epoch 22/40
 - 5s - loss: 0.0803 - acc: 0.9731 - val_loss: 0.1488 - val_acc: 0.9666
Epoch 23/40
 - 5s - loss: 0.0804 - acc: 0.9736 - val_loss: 0.1755 - val_acc: 0.9672
Epoch 24/40
 - 5s - loss: 0.0785 - acc: 0.9734 - val_loss: 0.1520 - val_acc: 0.9671
Epoch 25/40
 - 5s - loss: 0.0790 - acc: 0.9728 - val_loss: 0.1624 - val_acc: 0.9675
Epoch 26/40
 - 5s - loss: 0.0858 - acc: 0.9737 - val_loss: 0.1735 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0858 - acc: 0.9736 - val_loss: 0.1741 - val_acc: 0.9662
Epoch 28/40
 - 5s - loss: 0.0807 - acc: 0.9734 - val_loss: 0.1635 - val_acc: 0.9657
Epoch 29/40
 - 5s - loss: 0.0804 - acc: 0.9739 - val_loss: 0.1600 - val_acc: 0.9653
Epoch 30/40
 - 5s - loss: 0.0882 - acc: 0.9736 - val_loss: 0.1682 - val_acc: 0.9658
Epoch 31/40
 - 5s - loss: 0.0846 - acc: 0.9731 - val_loss: 0.1979 - val_acc: 0.9673
Epoch 32/40
 - 5s - loss: 0.0897 - acc: 0.9735 - val_loss: 0.2015 - val_acc: 0.9667
Epoch 33/40
 - 5s - loss: 0.0908 - acc: 0.9738 - val_loss: 0.1708 - val_acc: 0.9673
Epoch 34/40
 - 5s - loss: 0.0922 - acc: 0.9741 - val_loss: 0.1652 - val_acc: 0.9661
Epoch 35/40
 - 5s - loss: 0.0945 - acc: 0.9743 - val_loss: 0.1868 - val_acc: 0.9655
Epoch 36/40
 - 5s - loss: 0.0996 - acc: 0.9743 - val_loss: 0.1812 - val_acc: 0.9647
Epoch 37/40
 - 5s - loss: 0.1018 - acc: 0.9740 - val_loss: 0.1633 - val_acc: 0.9646
Epoch 38/40
 - 5s - loss: 0.0980 - acc: 0.9733 - val_loss: 0.1702 - val_acc: 0.9654
Epoch 39/40
 - 5s - loss: 0.1030 - acc: 0.9735 - val_loss: 0.1987 - val_acc: 0.9655
Epoch 40/40
 - 5s - loss: 0.1027 - acc: 0.9742 - val_loss: 0.1938 - val_acc: 0.9663
Epoch 1/40
 - 1s - loss: 0.1861 - acc: 0.9645
Epoch 2/40
 - 1s - loss: 0.1728 - acc: 0.9630
Epoch 3/40
 - 1s - loss: 0.1680 - acc: 0.9654
Epoch 4/40
 - 1s - loss: 0.1577 - acc: 0.9660
Epoch 5/40
 - 1s - loss: 0.1507 - acc: 0.9656
Epoch 6/40
 - 1s - loss: 0.1497 - acc: 0.9676
Epoch 7/40
 - 1s - loss: 0.1527 - acc: 0.9674
Epoch 8/40
 - 1s - loss: 0.1515 - acc: 0.9680
Epoch 9/40
 - 1s - loss: 0.1438 - acc: 0.9690
Epoch 10/40
 - 1s - loss: 0.1384 - acc: 0.9707
Epoch 11/40
 - 1s - loss: 0.1378 - acc: 0.9698
Epoch 12/40
 - 1s - loss: 0.1363 - acc: 0.9698
Epoch 13/40
 - 1s - loss: 0.1347 - acc: 0.9715
Epoch 14/40
 - 1s - loss: 0.1455 - acc: 0.9709
Epoch 15/40
 - 1s - loss: 0.1363 - acc: 0.9707
Epoch 16/40
 - 1s - loss: 0.1369 - acc: 0.9720
Epoch 17/40
 - 1s - loss: 0.1439 - acc: 0.9729
Epoch 18/40
 - 1s - loss: 0.1420 - acc: 0.9724
Epoch 19/40
 - 1s - loss: 0.1266 - acc: 0.9736
Epoch 20/40
 - 1s - loss: 0.1346 - acc: 0.9733
Epoch 21/40
 - 1s - loss: 0.1322 - acc: 0.9731
Epoch 22/40
 - 1s - loss: 0.1271 - acc: 0.9738
Epoch 23/40
 - 1s - loss: 0.1342 - acc: 0.9733
Epoch 24/40
 - 1s - loss: 0.1438 - acc: 0.9715
Epoch 25/40
 - 1s - loss: 0.1352 - acc: 0.9726
Epoch 26/40
 - 1s - loss: 0.1246 - acc: 0.9734
Epoch 27/40
 - 1s - loss: 0.1264 - acc: 0.9733
Epoch 28/40
 - 1s - loss: 0.1264 - acc: 0.9735
Epoch 29/40
 - 1s - loss: 0.1297 - acc: 0.9730
Epoch 30/40
 - 1s - loss: 0.1230 - acc: 0.9732
Epoch 31/40
 - 1s - loss: 0.1198 - acc: 0.9736
Epoch 32/40
 - 1s - loss: 0.1248 - acc: 0.9745
Epoch 33/40
 - 1s - loss: 0.1304 - acc: 0.9738
Epoch 34/40
 - 1s - loss: 0.1290 - acc: 0.9739
Epoch 35/40
 - 1s - loss: 0.1185 - acc: 0.9740
Epoch 36/40
 - 1s - loss: 0.1222 - acc: 0.9749
Epoch 37/40
 - 1s - loss: 0.1288 - acc: 0.9741
Epoch 38/40
 - 1s - loss: 0.1250 - acc: 0.9748
Epoch 39/40
 - 1s - loss: 0.1126 - acc: 0.9753
Epoch 40/40
 - 1s - loss: 0.1158 - acc: 0.9745
# Training time = 0:03:49.471725
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_147 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_148 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_147 (Embedding)       (None, 4, 48)        705264      input_147[0][0]                  
__________________________________________________________________________________________________
embedding_148 (Embedding)       (None, 4, 24)        5640        input_148[0][0]                  
__________________________________________________________________________________________________
flatten_147 (Flatten)           (None, 192)          0           embedding_147[0][0]              
__________________________________________________________________________________________________
flatten_148 (Flatten)           (None, 96)           0           embedding_148[0][0]              
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 288)          0           flatten_147[0][0]                
                                                                 flatten_148[0][0]                
__________________________________________________________________________________________________
dense_147 (Dense)               (None, 24)           6936        concatenate_74[0][0]             
__________________________________________________________________________________________________
dropout_74 (Dropout)            (None, 24)           0           dense_147[0][0]                  
__________________________________________________________________________________________________
dense_148 (Dense)               (None, 8)            200         dropout_74[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1001 - acc: 0.9614 - val_loss: 0.0812 - val_acc: 0.9648
Epoch 2/40
 - 5s - loss: 0.0729 - acc: 0.9688 - val_loss: 0.0942 - val_acc: 0.9636
Epoch 3/40
 - 5s - loss: 0.0692 - acc: 0.9702 - val_loss: 0.0885 - val_acc: 0.9661
Epoch 4/40
 - 5s - loss: 0.0668 - acc: 0.9711 - val_loss: 0.1113 - val_acc: 0.9658
Epoch 5/40
 - 5s - loss: 0.0650 - acc: 0.9716 - val_loss: 0.1108 - val_acc: 0.9668
Epoch 6/40
 - 5s - loss: 0.0642 - acc: 0.9724 - val_loss: 0.1190 - val_acc: 0.9678
Epoch 7/40
 - 5s - loss: 0.0653 - acc: 0.9725 - val_loss: 0.1146 - val_acc: 0.9670
Epoch 8/40
 - 5s - loss: 0.0650 - acc: 0.9729 - val_loss: 0.1257 - val_acc: 0.9652
Epoch 9/40
 - 5s - loss: 0.0640 - acc: 0.9729 - val_loss: 0.1105 - val_acc: 0.9679
Epoch 10/40
 - 5s - loss: 0.0631 - acc: 0.9737 - val_loss: 0.1207 - val_acc: 0.9661
Epoch 11/40
 - 5s - loss: 0.0633 - acc: 0.9732 - val_loss: 0.1347 - val_acc: 0.9646
Epoch 12/40
 - 5s - loss: 0.0630 - acc: 0.9736 - val_loss: 0.1273 - val_acc: 0.9660
Epoch 13/40
 - 5s - loss: 0.0687 - acc: 0.9736 - val_loss: 0.1413 - val_acc: 0.9678
Epoch 14/40
 - 5s - loss: 0.0698 - acc: 0.9736 - val_loss: 0.1325 - val_acc: 0.9663
Epoch 15/40
 - 5s - loss: 0.0684 - acc: 0.9738 - val_loss: 0.1450 - val_acc: 0.9660
Epoch 16/40
 - 5s - loss: 0.0676 - acc: 0.9737 - val_loss: 0.1365 - val_acc: 0.9666
Epoch 17/40
 - 5s - loss: 0.0684 - acc: 0.9737 - val_loss: 0.1371 - val_acc: 0.9648
Epoch 18/40
 - 5s - loss: 0.0677 - acc: 0.9735 - val_loss: 0.1444 - val_acc: 0.9663
Epoch 19/40
 - 5s - loss: 0.0733 - acc: 0.9736 - val_loss: 0.1610 - val_acc: 0.9657
Epoch 20/40
 - 5s - loss: 0.0730 - acc: 0.9735 - val_loss: 0.1567 - val_acc: 0.9646
Epoch 21/40
 - 5s - loss: 0.0723 - acc: 0.9734 - val_loss: 0.1480 - val_acc: 0.9668
Epoch 22/40
 - 5s - loss: 0.0708 - acc: 0.9743 - val_loss: 0.1548 - val_acc: 0.9671
Epoch 23/40
 - 5s - loss: 0.0724 - acc: 0.9737 - val_loss: 0.1571 - val_acc: 0.9671
Epoch 24/40
 - 5s - loss: 0.0718 - acc: 0.9741 - val_loss: 0.1457 - val_acc: 0.9654
Epoch 25/40
 - 5s - loss: 0.0750 - acc: 0.9742 - val_loss: 0.1616 - val_acc: 0.9668
Epoch 26/40
 - 5s - loss: 0.0767 - acc: 0.9744 - val_loss: 0.1596 - val_acc: 0.9663
Epoch 27/40
 - 5s - loss: 0.0783 - acc: 0.9742 - val_loss: 0.1630 - val_acc: 0.9664
Epoch 28/40
 - 5s - loss: 0.0857 - acc: 0.9745 - val_loss: 0.1676 - val_acc: 0.9660
Epoch 29/40
 - 5s - loss: 0.1034 - acc: 0.9744 - val_loss: 0.2585 - val_acc: 0.9659
Epoch 30/40
 - 5s - loss: 0.1167 - acc: 0.9740 - val_loss: 0.1605 - val_acc: 0.9658
Epoch 31/40
 - 5s - loss: 0.1033 - acc: 0.9747 - val_loss: 0.1976 - val_acc: 0.9665
Epoch 32/40
 - 5s - loss: 0.1074 - acc: 0.9746 - val_loss: 0.1970 - val_acc: 0.9662
Epoch 33/40
 - 5s - loss: 0.1043 - acc: 0.9744 - val_loss: 0.1961 - val_acc: 0.9661
Epoch 34/40
 - 5s - loss: 0.1097 - acc: 0.9744 - val_loss: 0.2313 - val_acc: 0.9612
Epoch 35/40
 - 5s - loss: 0.1044 - acc: 0.9732 - val_loss: 0.1861 - val_acc: 0.9652
Epoch 36/40
 - 5s - loss: 0.0964 - acc: 0.9737 - val_loss: 0.1901 - val_acc: 0.9655
Epoch 37/40
 - 5s - loss: 0.0942 - acc: 0.9741 - val_loss: 0.1873 - val_acc: 0.9649
Epoch 38/40
 - 5s - loss: 0.1027 - acc: 0.9734 - val_loss: 0.1979 - val_acc: 0.9665
Epoch 39/40
 - 5s - loss: 0.0994 - acc: 0.9744 - val_loss: 0.1721 - val_acc: 0.9671
Epoch 40/40
 - 5s - loss: 0.0994 - acc: 0.9740 - val_loss: 0.2167 - val_acc: 0.9649
Epoch 1/40
 - 0s - loss: 0.1976 - acc: 0.9646
Epoch 2/40
 - 0s - loss: 0.1765 - acc: 0.9650
Epoch 3/40
 - 0s - loss: 0.1721 - acc: 0.9661
Epoch 4/40
 - 0s - loss: 0.1694 - acc: 0.9674
Epoch 5/40
 - 0s - loss: 0.1514 - acc: 0.9689
Epoch 6/40
 - 0s - loss: 0.1492 - acc: 0.9690
Epoch 7/40
 - 0s - loss: 0.1502 - acc: 0.9695
Epoch 8/40
 - 0s - loss: 0.1418 - acc: 0.9704
Epoch 9/40
 - 0s - loss: 0.1412 - acc: 0.9697
Epoch 10/40
 - 0s - loss: 0.1431 - acc: 0.9701
Epoch 11/40
 - 0s - loss: 0.1400 - acc: 0.9707
Epoch 12/40
 - 0s - loss: 0.1396 - acc: 0.9706
Epoch 13/40
 - 0s - loss: 0.1434 - acc: 0.9713
Epoch 14/40
 - 0s - loss: 0.1435 - acc: 0.9705
Epoch 15/40
 - 0s - loss: 0.1347 - acc: 0.9710
Epoch 16/40
 - 0s - loss: 0.1390 - acc: 0.9717
Epoch 17/40
 - 0s - loss: 0.1487 - acc: 0.9715
Epoch 18/40
 - 0s - loss: 0.1392 - acc: 0.9722
Epoch 19/40
 - 0s - loss: 0.1334 - acc: 0.9723
Epoch 20/40
 - 0s - loss: 0.1375 - acc: 0.9719
Epoch 21/40
 - 0s - loss: 0.1426 - acc: 0.9720
Epoch 22/40
 - 0s - loss: 0.1347 - acc: 0.9721
Epoch 23/40
 - 0s - loss: 0.1345 - acc: 0.9726
Epoch 24/40
 - 0s - loss: 0.1525 - acc: 0.9726
Epoch 25/40
 - 0s - loss: 0.2124 - acc: 0.9708
Epoch 26/40
 - 0s - loss: 0.1982 - acc: 0.9713
Epoch 27/40
 - 0s - loss: 0.1879 - acc: 0.9723
Epoch 28/40
 - 0s - loss: 0.1454 - acc: 0.9731
Epoch 29/40
 - 0s - loss: 0.1398 - acc: 0.9724
Epoch 30/40
 - 0s - loss: 0.1393 - acc: 0.9728
Epoch 31/40
 - 0s - loss: 0.1369 - acc: 0.9729
Epoch 32/40
 - 0s - loss: 0.1534 - acc: 0.9721
Epoch 33/40
 - 0s - loss: 0.1561 - acc: 0.9721
Epoch 34/40
 - 0s - loss: 0.1546 - acc: 0.9725
Epoch 35/40
 - 0s - loss: 0.1395 - acc: 0.9721
Epoch 36/40
 - 0s - loss: 0.1518 - acc: 0.9722
Epoch 37/40
 - 0s - loss: 0.1606 - acc: 0.9723
Epoch 38/40
 - 0s - loss: 0.1383 - acc: 0.9719
Epoch 39/40
 - 0s - loss: 0.1243 - acc: 0.9737
Epoch 40/40
 - 0s - loss: 0.1263 - acc: 0.9741
# Training time = 0:03:47.377588
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_149 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_150 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_149 (Embedding)       (None, 4, 48)        705264      input_149[0][0]                  
__________________________________________________________________________________________________
embedding_150 (Embedding)       (None, 4, 24)        5640        input_150[0][0]                  
__________________________________________________________________________________________________
flatten_149 (Flatten)           (None, 192)          0           embedding_149[0][0]              
__________________________________________________________________________________________________
flatten_150 (Flatten)           (None, 96)           0           embedding_150[0][0]              
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 288)          0           flatten_149[0][0]                
                                                                 flatten_150[0][0]                
__________________________________________________________________________________________________
dense_149 (Dense)               (None, 24)           6936        concatenate_75[0][0]             
__________________________________________________________________________________________________
dropout_75 (Dropout)            (None, 24)           0           dense_149[0][0]                  
__________________________________________________________________________________________________
dense_150 (Dense)               (None, 8)            200         dropout_75[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1024 - acc: 0.9609 - val_loss: 0.0803 - val_acc: 0.9667
Epoch 2/40
 - 5s - loss: 0.0709 - acc: 0.9691 - val_loss: 0.0960 - val_acc: 0.9651
Epoch 3/40
 - 5s - loss: 0.0673 - acc: 0.9701 - val_loss: 0.1118 - val_acc: 0.9664
Epoch 4/40
 - 5s - loss: 0.0658 - acc: 0.9707 - val_loss: 0.0934 - val_acc: 0.9656
Epoch 5/40
 - 5s - loss: 0.0653 - acc: 0.9714 - val_loss: 0.1020 - val_acc: 0.9649
Epoch 6/40
 - 5s - loss: 0.0649 - acc: 0.9717 - val_loss: 0.1084 - val_acc: 0.9670
Epoch 7/40
 - 5s - loss: 0.0651 - acc: 0.9717 - val_loss: 0.1132 - val_acc: 0.9639
Epoch 8/40
 - 5s - loss: 0.0634 - acc: 0.9727 - val_loss: 0.1187 - val_acc: 0.9658
Epoch 9/40
 - 5s - loss: 0.0630 - acc: 0.9730 - val_loss: 0.1171 - val_acc: 0.9658
Epoch 10/40
 - 5s - loss: 0.0663 - acc: 0.9731 - val_loss: 0.1220 - val_acc: 0.9669
Epoch 11/40
 - 5s - loss: 0.0647 - acc: 0.9729 - val_loss: 0.1344 - val_acc: 0.9667
Epoch 12/40
 - 5s - loss: 0.0673 - acc: 0.9728 - val_loss: 0.1345 - val_acc: 0.9663
Epoch 13/40
 - 5s - loss: 0.0680 - acc: 0.9731 - val_loss: 0.1287 - val_acc: 0.9665
Epoch 14/40
 - 5s - loss: 0.0657 - acc: 0.9740 - val_loss: 0.1221 - val_acc: 0.9668
Epoch 15/40
 - 5s - loss: 0.0674 - acc: 0.9733 - val_loss: 0.1374 - val_acc: 0.9660
Epoch 16/40
 - 5s - loss: 0.0679 - acc: 0.9734 - val_loss: 0.1414 - val_acc: 0.9651
Epoch 17/40
 - 5s - loss: 0.0702 - acc: 0.9734 - val_loss: 0.1490 - val_acc: 0.9669
Epoch 18/40
 - 5s - loss: 0.0687 - acc: 0.9738 - val_loss: 0.1464 - val_acc: 0.9650
Epoch 19/40
 - 5s - loss: 0.0710 - acc: 0.9740 - val_loss: 0.1460 - val_acc: 0.9658
Epoch 20/40
 - 5s - loss: 0.0728 - acc: 0.9736 - val_loss: 0.1641 - val_acc: 0.9662
Epoch 21/40
 - 5s - loss: 0.0729 - acc: 0.9736 - val_loss: 0.1254 - val_acc: 0.9659
Epoch 22/40
 - 5s - loss: 0.0725 - acc: 0.9737 - val_loss: 0.1513 - val_acc: 0.9655
Epoch 23/40
 - 5s - loss: 0.0738 - acc: 0.9737 - val_loss: 0.1514 - val_acc: 0.9665
Epoch 24/40
 - 5s - loss: 0.0721 - acc: 0.9735 - val_loss: 0.1378 - val_acc: 0.9667
Epoch 25/40
 - 5s - loss: 0.0701 - acc: 0.9740 - val_loss: 0.1420 - val_acc: 0.9655
Epoch 26/40
 - 5s - loss: 0.0755 - acc: 0.9736 - val_loss: 0.1722 - val_acc: 0.9610
Epoch 27/40
 - 5s - loss: 0.0779 - acc: 0.9734 - val_loss: 0.1658 - val_acc: 0.9663
Epoch 28/40
 - 5s - loss: 0.0773 - acc: 0.9736 - val_loss: 0.1557 - val_acc: 0.9679
Epoch 29/40
 - 5s - loss: 0.0847 - acc: 0.9735 - val_loss: 0.1490 - val_acc: 0.9672
Epoch 30/40
 - 5s - loss: 0.0792 - acc: 0.9738 - val_loss: 0.1632 - val_acc: 0.9637
Epoch 31/40
 - 5s - loss: 0.0801 - acc: 0.9730 - val_loss: 0.1518 - val_acc: 0.9643
Epoch 32/40
 - 5s - loss: 0.0804 - acc: 0.9740 - val_loss: 0.1741 - val_acc: 0.9654
Epoch 33/40
 - 5s - loss: 0.0795 - acc: 0.9730 - val_loss: 0.2683 - val_acc: 0.9646
Epoch 34/40
 - 5s - loss: 0.0888 - acc: 0.9735 - val_loss: 0.1890 - val_acc: 0.9644
Epoch 35/40
 - 5s - loss: 0.0860 - acc: 0.9738 - val_loss: 0.2027 - val_acc: 0.9663
Epoch 36/40
 - 5s - loss: 0.0912 - acc: 0.9737 - val_loss: 0.1658 - val_acc: 0.9652
Epoch 37/40
 - 5s - loss: 0.0854 - acc: 0.9742 - val_loss: 0.1715 - val_acc: 0.9657
Epoch 38/40
 - 5s - loss: 0.0834 - acc: 0.9739 - val_loss: 0.1854 - val_acc: 0.9661
Epoch 39/40
 - 5s - loss: 0.0828 - acc: 0.9746 - val_loss: 0.1522 - val_acc: 0.9670
Epoch 40/40
 - 5s - loss: 0.0863 - acc: 0.9738 - val_loss: 0.1703 - val_acc: 0.9649
Epoch 1/40
 - 1s - loss: 0.1828 - acc: 0.9631
Epoch 2/40
 - 1s - loss: 0.1506 - acc: 0.9634
Epoch 3/40
 - 1s - loss: 0.1308 - acc: 0.9650
Epoch 4/40
 - 1s - loss: 0.1246 - acc: 0.9672
Epoch 5/40
 - 1s - loss: 0.1305 - acc: 0.9648
Epoch 6/40
 - 1s - loss: 0.1248 - acc: 0.9665
Epoch 7/40
 - 1s - loss: 0.1230 - acc: 0.9691
Epoch 8/40
 - 1s - loss: 0.1168 - acc: 0.9696
Epoch 9/40
 - 1s - loss: 0.1161 - acc: 0.9698
Epoch 10/40
 - 1s - loss: 0.1118 - acc: 0.9698
Epoch 11/40
 - 1s - loss: 0.1173 - acc: 0.9708
Epoch 12/40
 - 1s - loss: 0.1161 - acc: 0.9713
Epoch 13/40
 - 1s - loss: 0.1265 - acc: 0.9718
Epoch 14/40
 - 1s - loss: 0.1224 - acc: 0.9713
Epoch 15/40
 - 1s - loss: 0.1191 - acc: 0.9717
Epoch 16/40
 - 1s - loss: 0.1211 - acc: 0.9717
Epoch 17/40
 - 1s - loss: 0.1169 - acc: 0.9716
Epoch 18/40
 - 1s - loss: 0.1114 - acc: 0.9721
Epoch 19/40
 - 1s - loss: 0.1128 - acc: 0.9706
Epoch 20/40
 - 1s - loss: 0.1268 - acc: 0.9688
Epoch 21/40
 - 1s - loss: 0.1042 - acc: 0.9721
Epoch 22/40
 - 1s - loss: 0.1119 - acc: 0.9713
Epoch 23/40
 - 1s - loss: 0.1031 - acc: 0.9733
Epoch 24/40
 - 1s - loss: 0.0996 - acc: 0.9732
Epoch 25/40
 - 1s - loss: 0.1045 - acc: 0.9731
Epoch 26/40
 - 1s - loss: 0.0915 - acc: 0.9738
Epoch 27/40
 - 1s - loss: 0.0919 - acc: 0.9735
Epoch 28/40
 - 1s - loss: 0.0933 - acc: 0.9735
Epoch 29/40
 - 1s - loss: 0.0909 - acc: 0.9742
Epoch 30/40
 - 1s - loss: 0.0951 - acc: 0.9735
Epoch 31/40
 - 1s - loss: 0.0897 - acc: 0.9744
Epoch 32/40
 - 1s - loss: 0.0891 - acc: 0.9746
Epoch 33/40
 - 1s - loss: 0.0870 - acc: 0.9746
Epoch 34/40
 - 1s - loss: 0.0950 - acc: 0.9744
Epoch 35/40
 - 1s - loss: 0.1015 - acc: 0.9751
Epoch 36/40
 - 1s - loss: 0.0998 - acc: 0.9754
Epoch 37/40
 - 1s - loss: 0.1117 - acc: 0.9749
Epoch 38/40
 - 1s - loss: 0.1056 - acc: 0.9753
Epoch 39/40
 - 1s - loss: 0.0985 - acc: 0.9751
Epoch 40/40
 - 1s - loss: 0.1004 - acc: 0.9750
# Training time = 0:03:49.265518
# F-Score(Ordinary) = 0.004, Recall: 1.0, Precision: 0.002
# F-Score(ireflv) = 0.016, Recall: 1.0, Precision: 0.008
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_151 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_152 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_151 (Embedding)       (None, 4, 48)        705264      input_151[0][0]                  
__________________________________________________________________________________________________
embedding_152 (Embedding)       (None, 4, 24)        5640        input_152[0][0]                  
__________________________________________________________________________________________________
flatten_151 (Flatten)           (None, 192)          0           embedding_151[0][0]              
__________________________________________________________________________________________________
flatten_152 (Flatten)           (None, 96)           0           embedding_152[0][0]              
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 288)          0           flatten_151[0][0]                
                                                                 flatten_152[0][0]                
__________________________________________________________________________________________________
dense_151 (Dense)               (None, 24)           6936        concatenate_76[0][0]             
__________________________________________________________________________________________________
dropout_76 (Dropout)            (None, 24)           0           dense_151[0][0]                  
__________________________________________________________________________________________________
dense_152 (Dense)               (None, 8)            200         dropout_76[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1673 - acc: 0.9507 - val_loss: 0.1029 - val_acc: 0.9638
Epoch 2/40
 - 4s - loss: 0.0987 - acc: 0.9638 - val_loss: 0.0915 - val_acc: 0.9651
Epoch 3/40
 - 4s - loss: 0.0866 - acc: 0.9666 - val_loss: 0.0861 - val_acc: 0.9663
Epoch 4/40
 - 4s - loss: 0.0806 - acc: 0.9675 - val_loss: 0.0829 - val_acc: 0.9664
Epoch 5/40
 - 4s - loss: 0.0757 - acc: 0.9691 - val_loss: 0.0809 - val_acc: 0.9667
Epoch 6/40
 - 4s - loss: 0.0719 - acc: 0.9697 - val_loss: 0.0791 - val_acc: 0.9669
Epoch 7/40
 - 4s - loss: 0.0692 - acc: 0.9703 - val_loss: 0.0778 - val_acc: 0.9670
Epoch 8/40
 - 4s - loss: 0.0668 - acc: 0.9706 - val_loss: 0.0769 - val_acc: 0.9673
Epoch 9/40
 - 4s - loss: 0.0647 - acc: 0.9715 - val_loss: 0.0759 - val_acc: 0.9675
Epoch 10/40
 - 4s - loss: 0.0629 - acc: 0.9719 - val_loss: 0.0755 - val_acc: 0.9675
Epoch 11/40
 - 4s - loss: 0.0615 - acc: 0.9724 - val_loss: 0.0747 - val_acc: 0.9676
Epoch 12/40
 - 4s - loss: 0.0597 - acc: 0.9729 - val_loss: 0.0745 - val_acc: 0.9675
Epoch 13/40
 - 4s - loss: 0.0585 - acc: 0.9730 - val_loss: 0.0742 - val_acc: 0.9677
Epoch 14/40
 - 4s - loss: 0.0573 - acc: 0.9733 - val_loss: 0.0739 - val_acc: 0.9679
Epoch 15/40
 - 4s - loss: 0.0565 - acc: 0.9734 - val_loss: 0.0737 - val_acc: 0.9677
Epoch 16/40
 - 4s - loss: 0.0555 - acc: 0.9735 - val_loss: 0.0735 - val_acc: 0.9679
Epoch 17/40
 - 4s - loss: 0.0550 - acc: 0.9736 - val_loss: 0.0737 - val_acc: 0.9678
Epoch 18/40
 - 4s - loss: 0.0536 - acc: 0.9744 - val_loss: 0.0736 - val_acc: 0.9679
Epoch 19/40
 - 4s - loss: 0.0533 - acc: 0.9744 - val_loss: 0.0733 - val_acc: 0.9682
Epoch 20/40
 - 4s - loss: 0.0529 - acc: 0.9745 - val_loss: 0.0735 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0520 - acc: 0.9746 - val_loss: 0.0736 - val_acc: 0.9682
Epoch 22/40
 - 4s - loss: 0.0519 - acc: 0.9744 - val_loss: 0.0734 - val_acc: 0.9681
Epoch 23/40
 - 4s - loss: 0.0512 - acc: 0.9747 - val_loss: 0.0736 - val_acc: 0.9680
Epoch 24/40
 - 4s - loss: 0.0510 - acc: 0.9743 - val_loss: 0.0736 - val_acc: 0.9681
Epoch 25/40
 - 4s - loss: 0.0500 - acc: 0.9752 - val_loss: 0.0737 - val_acc: 0.9682
Epoch 26/40
 - 4s - loss: 0.0498 - acc: 0.9748 - val_loss: 0.0737 - val_acc: 0.9681
Epoch 27/40
 - 4s - loss: 0.0493 - acc: 0.9752 - val_loss: 0.0739 - val_acc: 0.9680
Epoch 28/40
 - 4s - loss: 0.0489 - acc: 0.9751 - val_loss: 0.0740 - val_acc: 0.9679
Epoch 29/40
 - 4s - loss: 0.0489 - acc: 0.9754 - val_loss: 0.0741 - val_acc: 0.9680
Epoch 30/40
 - 4s - loss: 0.0483 - acc: 0.9755 - val_loss: 0.0742 - val_acc: 0.9680
Epoch 31/40
 - 4s - loss: 0.0482 - acc: 0.9753 - val_loss: 0.0742 - val_acc: 0.9679
Epoch 32/40
 - 4s - loss: 0.0476 - acc: 0.9757 - val_loss: 0.0744 - val_acc: 0.9679
Epoch 33/40
 - 4s - loss: 0.0476 - acc: 0.9756 - val_loss: 0.0748 - val_acc: 0.9681
Epoch 34/40
 - 4s - loss: 0.0469 - acc: 0.9755 - val_loss: 0.0746 - val_acc: 0.9681
Epoch 35/40
 - 4s - loss: 0.0469 - acc: 0.9756 - val_loss: 0.0745 - val_acc: 0.9680
Epoch 36/40
 - 4s - loss: 0.0468 - acc: 0.9758 - val_loss: 0.0749 - val_acc: 0.9682
Epoch 37/40
 - 4s - loss: 0.0466 - acc: 0.9757 - val_loss: 0.0750 - val_acc: 0.9682
Epoch 38/40
 - 4s - loss: 0.0464 - acc: 0.9755 - val_loss: 0.0749 - val_acc: 0.9680
Epoch 39/40
 - 4s - loss: 0.0464 - acc: 0.9756 - val_loss: 0.0751 - val_acc: 0.9681
Epoch 40/40
 - 4s - loss: 0.0461 - acc: 0.9757 - val_loss: 0.0754 - val_acc: 0.9682
Epoch 1/40
 - 0s - loss: 0.0777 - acc: 0.9668
Epoch 2/40
 - 0s - loss: 0.0698 - acc: 0.9679
Epoch 3/40
 - 0s - loss: 0.0664 - acc: 0.9701
Epoch 4/40
 - 0s - loss: 0.0620 - acc: 0.9720
Epoch 5/40
 - 0s - loss: 0.0607 - acc: 0.9715
Epoch 6/40
 - 0s - loss: 0.0590 - acc: 0.9721
Epoch 7/40
 - 0s - loss: 0.0590 - acc: 0.9716
Epoch 8/40
 - 0s - loss: 0.0571 - acc: 0.9719
Epoch 9/40
 - 0s - loss: 0.0563 - acc: 0.9721
Epoch 10/40
 - 0s - loss: 0.0549 - acc: 0.9729
Epoch 11/40
 - 0s - loss: 0.0543 - acc: 0.9742
Epoch 12/40
 - 0s - loss: 0.0530 - acc: 0.9749
Epoch 13/40
 - 0s - loss: 0.0525 - acc: 0.9744
Epoch 14/40
 - 0s - loss: 0.0534 - acc: 0.9735
Epoch 15/40
 - 0s - loss: 0.0516 - acc: 0.9756
Epoch 16/40
 - 0s - loss: 0.0517 - acc: 0.9745
Epoch 17/40
 - 0s - loss: 0.0510 - acc: 0.9745
Epoch 18/40
 - 0s - loss: 0.0501 - acc: 0.9746
Epoch 19/40
 - 0s - loss: 0.0504 - acc: 0.9746
Epoch 20/40
 - 0s - loss: 0.0503 - acc: 0.9743
Epoch 21/40
 - 0s - loss: 0.0500 - acc: 0.9753
Epoch 22/40
 - 0s - loss: 0.0490 - acc: 0.9765
Epoch 23/40
 - 0s - loss: 0.0480 - acc: 0.9757
Epoch 24/40
 - 0s - loss: 0.0478 - acc: 0.9761
Epoch 25/40
 - 0s - loss: 0.0478 - acc: 0.9750
Epoch 26/40
 - 0s - loss: 0.0476 - acc: 0.9756
Epoch 27/40
 - 0s - loss: 0.0477 - acc: 0.9742
Epoch 28/40
 - 0s - loss: 0.0474 - acc: 0.9751
Epoch 29/40
 - 0s - loss: 0.0468 - acc: 0.9760
Epoch 30/40
 - 0s - loss: 0.0466 - acc: 0.9750
Epoch 31/40
 - 0s - loss: 0.0450 - acc: 0.9775
Epoch 32/40
 - 0s - loss: 0.0467 - acc: 0.9753
Epoch 33/40
 - 0s - loss: 0.0464 - acc: 0.9753
Epoch 34/40
 - 0s - loss: 0.0464 - acc: 0.9745
Epoch 35/40
 - 0s - loss: 0.0455 - acc: 0.9758
Epoch 36/40
 - 0s - loss: 0.0452 - acc: 0.9760
Epoch 37/40
 - 0s - loss: 0.0460 - acc: 0.9737
Epoch 38/40
 - 0s - loss: 0.0455 - acc: 0.9754
Epoch 39/40
 - 0s - loss: 0.0447 - acc: 0.9764
Epoch 40/40
 - 0s - loss: 0.0445 - acc: 0.9750
# Training time = 0:03:29.933439
# F-Score(Ordinary) = 0.288, Recall: 0.885, Precision: 0.172
# F-Score(lvc) = 0.369, Recall: 0.861, Precision: 0.235
# F-Score(ireflv) = 0.286, Recall: 0.84, Precision: 0.172
# F-Score(id) = 0.228, Recall: 0.962, Precision: 0.13
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_153 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_154 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_153 (Embedding)       (None, 4, 48)        705264      input_153[0][0]                  
__________________________________________________________________________________________________
embedding_154 (Embedding)       (None, 4, 24)        5640        input_154[0][0]                  
__________________________________________________________________________________________________
flatten_153 (Flatten)           (None, 192)          0           embedding_153[0][0]              
__________________________________________________________________________________________________
flatten_154 (Flatten)           (None, 96)           0           embedding_154[0][0]              
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 288)          0           flatten_153[0][0]                
                                                                 flatten_154[0][0]                
__________________________________________________________________________________________________
dense_153 (Dense)               (None, 24)           6936        concatenate_77[0][0]             
__________________________________________________________________________________________________
dropout_77 (Dropout)            (None, 24)           0           dense_153[0][0]                  
__________________________________________________________________________________________________
dense_154 (Dense)               (None, 8)            200         dropout_77[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1685 - acc: 0.9511 - val_loss: 0.1024 - val_acc: 0.9644
Epoch 2/40
 - 4s - loss: 0.0997 - acc: 0.9643 - val_loss: 0.0925 - val_acc: 0.9661
Epoch 3/40
 - 4s - loss: 0.0885 - acc: 0.9670 - val_loss: 0.0871 - val_acc: 0.9671
Epoch 4/40
 - 4s - loss: 0.0816 - acc: 0.9681 - val_loss: 0.0831 - val_acc: 0.9673
Epoch 5/40
 - 4s - loss: 0.0761 - acc: 0.9694 - val_loss: 0.0799 - val_acc: 0.9676
Epoch 6/40
 - 4s - loss: 0.0717 - acc: 0.9697 - val_loss: 0.0776 - val_acc: 0.9676
Epoch 7/40
 - 4s - loss: 0.0683 - acc: 0.9708 - val_loss: 0.0758 - val_acc: 0.9680
Epoch 8/40
 - 4s - loss: 0.0651 - acc: 0.9713 - val_loss: 0.0745 - val_acc: 0.9680
Epoch 9/40
 - 4s - loss: 0.0633 - acc: 0.9715 - val_loss: 0.0736 - val_acc: 0.9682
Epoch 10/40
 - 4s - loss: 0.0618 - acc: 0.9718 - val_loss: 0.0731 - val_acc: 0.9682
Epoch 11/40
 - 4s - loss: 0.0599 - acc: 0.9723 - val_loss: 0.0726 - val_acc: 0.9681
Epoch 12/40
 - 4s - loss: 0.0588 - acc: 0.9724 - val_loss: 0.0721 - val_acc: 0.9679
Epoch 13/40
 - 4s - loss: 0.0575 - acc: 0.9726 - val_loss: 0.0717 - val_acc: 0.9682
Epoch 14/40
 - 4s - loss: 0.0563 - acc: 0.9733 - val_loss: 0.0719 - val_acc: 0.9680
Epoch 15/40
 - 4s - loss: 0.0554 - acc: 0.9734 - val_loss: 0.0714 - val_acc: 0.9683
Epoch 16/40
 - 4s - loss: 0.0546 - acc: 0.9734 - val_loss: 0.0715 - val_acc: 0.9679
Epoch 17/40
 - 4s - loss: 0.0539 - acc: 0.9733 - val_loss: 0.0714 - val_acc: 0.9683
Epoch 18/40
 - 4s - loss: 0.0533 - acc: 0.9734 - val_loss: 0.0715 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0526 - acc: 0.9739 - val_loss: 0.0713 - val_acc: 0.9684
Epoch 20/40
 - 4s - loss: 0.0518 - acc: 0.9743 - val_loss: 0.0715 - val_acc: 0.9685
Epoch 21/40
 - 4s - loss: 0.0513 - acc: 0.9742 - val_loss: 0.0715 - val_acc: 0.9684
Epoch 22/40
 - 4s - loss: 0.0507 - acc: 0.9753 - val_loss: 0.0716 - val_acc: 0.9686
Epoch 23/40
 - 4s - loss: 0.0506 - acc: 0.9743 - val_loss: 0.0715 - val_acc: 0.9685
Epoch 24/40
 - 4s - loss: 0.0500 - acc: 0.9750 - val_loss: 0.0717 - val_acc: 0.9684
Epoch 25/40
 - 4s - loss: 0.0496 - acc: 0.9750 - val_loss: 0.0718 - val_acc: 0.9686
Epoch 26/40
 - 4s - loss: 0.0489 - acc: 0.9750 - val_loss: 0.0718 - val_acc: 0.9686
Epoch 27/40
 - 4s - loss: 0.0489 - acc: 0.9750 - val_loss: 0.0719 - val_acc: 0.9686
Epoch 28/40
 - 4s - loss: 0.0483 - acc: 0.9751 - val_loss: 0.0722 - val_acc: 0.9686
Epoch 29/40
 - 4s - loss: 0.0483 - acc: 0.9750 - val_loss: 0.0723 - val_acc: 0.9685
Epoch 30/40
 - 4s - loss: 0.0476 - acc: 0.9755 - val_loss: 0.0725 - val_acc: 0.9686
Epoch 31/40
 - 4s - loss: 0.0474 - acc: 0.9752 - val_loss: 0.0726 - val_acc: 0.9684
Epoch 32/40
 - 4s - loss: 0.0473 - acc: 0.9756 - val_loss: 0.0726 - val_acc: 0.9683
Epoch 33/40
 - 4s - loss: 0.0471 - acc: 0.9759 - val_loss: 0.0729 - val_acc: 0.9683
Epoch 34/40
 - 4s - loss: 0.0469 - acc: 0.9754 - val_loss: 0.0727 - val_acc: 0.9681
Epoch 35/40
 - 4s - loss: 0.0467 - acc: 0.9757 - val_loss: 0.0727 - val_acc: 0.9682
Epoch 36/40
 - 4s - loss: 0.0463 - acc: 0.9756 - val_loss: 0.0731 - val_acc: 0.9683
Epoch 37/40
 - 4s - loss: 0.0459 - acc: 0.9757 - val_loss: 0.0733 - val_acc: 0.9681
Epoch 38/40
 - 4s - loss: 0.0461 - acc: 0.9755 - val_loss: 0.0735 - val_acc: 0.9684
Epoch 39/40
 - 4s - loss: 0.0456 - acc: 0.9757 - val_loss: 0.0735 - val_acc: 0.9682
Epoch 40/40
 - 4s - loss: 0.0454 - acc: 0.9760 - val_loss: 0.0737 - val_acc: 0.9683
Epoch 1/40
 - 0s - loss: 0.0751 - acc: 0.9664
Epoch 2/40
 - 0s - loss: 0.0676 - acc: 0.9693
Epoch 3/40
 - 0s - loss: 0.0639 - acc: 0.9707
Epoch 4/40
 - 0s - loss: 0.0625 - acc: 0.9687
Epoch 5/40
 - 0s - loss: 0.0595 - acc: 0.9716
Epoch 6/40
 - 0s - loss: 0.0577 - acc: 0.9720
Epoch 7/40
 - 0s - loss: 0.0567 - acc: 0.9720
Epoch 8/40
 - 0s - loss: 0.0562 - acc: 0.9718
Epoch 9/40
 - 0s - loss: 0.0552 - acc: 0.9723
Epoch 10/40
 - 0s - loss: 0.0550 - acc: 0.9726
Epoch 11/40
 - 0s - loss: 0.0540 - acc: 0.9738
Epoch 12/40
 - 0s - loss: 0.0530 - acc: 0.9734
Epoch 13/40
 - 0s - loss: 0.0516 - acc: 0.9741
Epoch 14/40
 - 0s - loss: 0.0522 - acc: 0.9743
Epoch 15/40
 - 0s - loss: 0.0509 - acc: 0.9742
Epoch 16/40
 - 0s - loss: 0.0505 - acc: 0.9748
Epoch 17/40
 - 0s - loss: 0.0508 - acc: 0.9750
Epoch 18/40
 - 0s - loss: 0.0496 - acc: 0.9752
Epoch 19/40
 - 0s - loss: 0.0495 - acc: 0.9744
Epoch 20/40
 - 0s - loss: 0.0491 - acc: 0.9743
Epoch 21/40
 - 0s - loss: 0.0486 - acc: 0.9753
Epoch 22/40
 - 0s - loss: 0.0484 - acc: 0.9749
Epoch 23/40
 - 0s - loss: 0.0476 - acc: 0.9755
Epoch 24/40
 - 0s - loss: 0.0471 - acc: 0.9754
Epoch 25/40
 - 0s - loss: 0.0478 - acc: 0.9749
Epoch 26/40
 - 0s - loss: 0.0475 - acc: 0.9751
Epoch 27/40
 - 0s - loss: 0.0467 - acc: 0.9762
Epoch 28/40
 - 0s - loss: 0.0463 - acc: 0.9759
Epoch 29/40
 - 0s - loss: 0.0469 - acc: 0.9762
Epoch 30/40
 - 0s - loss: 0.0467 - acc: 0.9745
Epoch 31/40
 - 0s - loss: 0.0456 - acc: 0.9761
Epoch 32/40
 - 0s - loss: 0.0458 - acc: 0.9749
Epoch 33/40
 - 0s - loss: 0.0462 - acc: 0.9758
Epoch 34/40
 - 0s - loss: 0.0457 - acc: 0.9758
Epoch 35/40
 - 0s - loss: 0.0459 - acc: 0.9763
Epoch 36/40
 - 0s - loss: 0.0449 - acc: 0.9760
Epoch 37/40
 - 0s - loss: 0.0449 - acc: 0.9758
Epoch 38/40
 - 0s - loss: 0.0439 - acc: 0.9763
Epoch 39/40
 - 0s - loss: 0.0447 - acc: 0.9761
Epoch 40/40
 - 0s - loss: 0.0445 - acc: 0.9768
# Training time = 0:03:30.807304
# F-Score(Ordinary) = 0.399, Recall: 0.779, Precision: 0.268
# F-Score(lvc) = 0.217, Recall: 0.442, Precision: 0.144
# F-Score(ireflv) = 0.378, Recall: 0.738, Precision: 0.254
# F-Score(id) = 0.519, Recall: 0.986, Precision: 0.352
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_155 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_156 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_155 (Embedding)       (None, 4, 48)        705264      input_155[0][0]                  
__________________________________________________________________________________________________
embedding_156 (Embedding)       (None, 4, 24)        5640        input_156[0][0]                  
__________________________________________________________________________________________________
flatten_155 (Flatten)           (None, 192)          0           embedding_155[0][0]              
__________________________________________________________________________________________________
flatten_156 (Flatten)           (None, 96)           0           embedding_156[0][0]              
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 288)          0           flatten_155[0][0]                
                                                                 flatten_156[0][0]                
__________________________________________________________________________________________________
dense_155 (Dense)               (None, 24)           6936        concatenate_78[0][0]             
__________________________________________________________________________________________________
dropout_78 (Dropout)            (None, 24)           0           dense_155[0][0]                  
__________________________________________________________________________________________________
dense_156 (Dense)               (None, 8)            200         dropout_78[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1684 - acc: 0.9506 - val_loss: 0.1046 - val_acc: 0.9649
Epoch 2/40
 - 4s - loss: 0.1006 - acc: 0.9640 - val_loss: 0.0934 - val_acc: 0.9659
Epoch 3/40
 - 4s - loss: 0.0897 - acc: 0.9664 - val_loss: 0.0880 - val_acc: 0.9667
Epoch 4/40
 - 4s - loss: 0.0825 - acc: 0.9684 - val_loss: 0.0845 - val_acc: 0.9672
Epoch 5/40
 - 4s - loss: 0.0768 - acc: 0.9695 - val_loss: 0.0820 - val_acc: 0.9671
Epoch 6/40
 - 4s - loss: 0.0730 - acc: 0.9705 - val_loss: 0.0794 - val_acc: 0.9674
Epoch 7/40
 - 4s - loss: 0.0695 - acc: 0.9712 - val_loss: 0.0778 - val_acc: 0.9675
Epoch 8/40
 - 4s - loss: 0.0665 - acc: 0.9716 - val_loss: 0.0765 - val_acc: 0.9678
Epoch 9/40
 - 4s - loss: 0.0643 - acc: 0.9720 - val_loss: 0.0754 - val_acc: 0.9679
Epoch 10/40
 - 4s - loss: 0.0621 - acc: 0.9721 - val_loss: 0.0748 - val_acc: 0.9679
Epoch 11/40
 - 4s - loss: 0.0603 - acc: 0.9729 - val_loss: 0.0741 - val_acc: 0.9681
Epoch 12/40
 - 4s - loss: 0.0589 - acc: 0.9729 - val_loss: 0.0738 - val_acc: 0.9680
Epoch 13/40
 - 4s - loss: 0.0575 - acc: 0.9735 - val_loss: 0.0733 - val_acc: 0.9681
Epoch 14/40
 - 4s - loss: 0.0565 - acc: 0.9735 - val_loss: 0.0733 - val_acc: 0.9681
Epoch 15/40
 - 4s - loss: 0.0554 - acc: 0.9739 - val_loss: 0.0728 - val_acc: 0.9683
Epoch 16/40
 - 4s - loss: 0.0549 - acc: 0.9737 - val_loss: 0.0727 - val_acc: 0.9682
Epoch 17/40
 - 4s - loss: 0.0538 - acc: 0.9746 - val_loss: 0.0726 - val_acc: 0.9682
Epoch 18/40
 - 4s - loss: 0.0533 - acc: 0.9742 - val_loss: 0.0726 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0524 - acc: 0.9747 - val_loss: 0.0726 - val_acc: 0.9683
Epoch 20/40
 - 4s - loss: 0.0518 - acc: 0.9747 - val_loss: 0.0726 - val_acc: 0.9682
Epoch 21/40
 - 4s - loss: 0.0515 - acc: 0.9745 - val_loss: 0.0728 - val_acc: 0.9681
Epoch 22/40
 - 4s - loss: 0.0504 - acc: 0.9750 - val_loss: 0.0728 - val_acc: 0.9684
Epoch 23/40
 - 4s - loss: 0.0501 - acc: 0.9749 - val_loss: 0.0728 - val_acc: 0.9682
Epoch 24/40
 - 4s - loss: 0.0497 - acc: 0.9750 - val_loss: 0.0730 - val_acc: 0.9683
Epoch 25/40
 - 4s - loss: 0.0495 - acc: 0.9752 - val_loss: 0.0731 - val_acc: 0.9683
Epoch 26/40
 - 4s - loss: 0.0491 - acc: 0.9753 - val_loss: 0.0730 - val_acc: 0.9684
Epoch 27/40
 - 4s - loss: 0.0485 - acc: 0.9752 - val_loss: 0.0732 - val_acc: 0.9684
Epoch 28/40
 - 4s - loss: 0.0483 - acc: 0.9753 - val_loss: 0.0734 - val_acc: 0.9685
Epoch 29/40
 - 4s - loss: 0.0479 - acc: 0.9754 - val_loss: 0.0735 - val_acc: 0.9683
Epoch 30/40
 - 4s - loss: 0.0477 - acc: 0.9754 - val_loss: 0.0737 - val_acc: 0.9683
Epoch 31/40
 - 4s - loss: 0.0470 - acc: 0.9758 - val_loss: 0.0738 - val_acc: 0.9683
Epoch 32/40
 - 4s - loss: 0.0470 - acc: 0.9758 - val_loss: 0.0740 - val_acc: 0.9682
Epoch 33/40
 - 4s - loss: 0.0468 - acc: 0.9755 - val_loss: 0.0742 - val_acc: 0.9683
Epoch 34/40
 - 4s - loss: 0.0467 - acc: 0.9759 - val_loss: 0.0742 - val_acc: 0.9682
Epoch 35/40
 - 4s - loss: 0.0462 - acc: 0.9758 - val_loss: 0.0744 - val_acc: 0.9683
Epoch 36/40
 - 4s - loss: 0.0462 - acc: 0.9758 - val_loss: 0.0745 - val_acc: 0.9683
Epoch 37/40
 - 4s - loss: 0.0458 - acc: 0.9758 - val_loss: 0.0748 - val_acc: 0.9683
Epoch 38/40
 - 4s - loss: 0.0458 - acc: 0.9760 - val_loss: 0.0747 - val_acc: 0.9683
Epoch 39/40
 - 4s - loss: 0.0455 - acc: 0.9758 - val_loss: 0.0747 - val_acc: 0.9682
Epoch 40/40
 - 4s - loss: 0.0458 - acc: 0.9755 - val_loss: 0.0749 - val_acc: 0.9683
Epoch 1/40
 - 0s - loss: 0.0749 - acc: 0.9682
Epoch 2/40
 - 0s - loss: 0.0695 - acc: 0.9686
Epoch 3/40
 - 0s - loss: 0.0650 - acc: 0.9696
Epoch 4/40
 - 0s - loss: 0.0621 - acc: 0.9712
Epoch 5/40
 - 0s - loss: 0.0605 - acc: 0.9706
Epoch 6/40
 - 0s - loss: 0.0586 - acc: 0.9733
Epoch 7/40
 - 0s - loss: 0.0575 - acc: 0.9716
Epoch 8/40
 - 0s - loss: 0.0561 - acc: 0.9720
Epoch 9/40
 - 0s - loss: 0.0555 - acc: 0.9738
Epoch 10/40
 - 0s - loss: 0.0546 - acc: 0.9735
Epoch 11/40
 - 0s - loss: 0.0527 - acc: 0.9737
Epoch 12/40
 - 0s - loss: 0.0527 - acc: 0.9739
Epoch 13/40
 - 0s - loss: 0.0520 - acc: 0.9740
Epoch 14/40
 - 0s - loss: 0.0521 - acc: 0.9736
Epoch 15/40
 - 0s - loss: 0.0514 - acc: 0.9737
Epoch 16/40
 - 0s - loss: 0.0505 - acc: 0.9744
Epoch 17/40
 - 0s - loss: 0.0499 - acc: 0.9741
Epoch 18/40
 - 0s - loss: 0.0492 - acc: 0.9743
Epoch 19/40
 - 0s - loss: 0.0496 - acc: 0.9739
Epoch 20/40
 - 0s - loss: 0.0495 - acc: 0.9758
Epoch 21/40
 - 0s - loss: 0.0488 - acc: 0.9763
Epoch 22/40
 - 0s - loss: 0.0485 - acc: 0.9758
Epoch 23/40
 - 0s - loss: 0.0483 - acc: 0.9757
Epoch 24/40
 - 0s - loss: 0.0486 - acc: 0.9748
Epoch 25/40
 - 0s - loss: 0.0474 - acc: 0.9759
Epoch 26/40
 - 0s - loss: 0.0470 - acc: 0.9753
Epoch 27/40
 - 0s - loss: 0.0466 - acc: 0.9759
Epoch 28/40
 - 0s - loss: 0.0475 - acc: 0.9749
Epoch 29/40
 - 0s - loss: 0.0470 - acc: 0.9752
Epoch 30/40
 - 0s - loss: 0.0458 - acc: 0.9765
Epoch 31/40
 - 0s - loss: 0.0465 - acc: 0.9757
Epoch 32/40
 - 0s - loss: 0.0458 - acc: 0.9762
Epoch 33/40
 - 0s - loss: 0.0452 - acc: 0.9759
Epoch 34/40
 - 0s - loss: 0.0449 - acc: 0.9767
Epoch 35/40
 - 0s - loss: 0.0458 - acc: 0.9764
Epoch 36/40
 - 0s - loss: 0.0447 - acc: 0.9753
Epoch 37/40
 - 0s - loss: 0.0446 - acc: 0.9767
Epoch 38/40
 - 0s - loss: 0.0442 - acc: 0.9767
Epoch 39/40
 - 0s - loss: 0.0447 - acc: 0.9772
Epoch 40/40
 - 0s - loss: 0.0437 - acc: 0.9770
# Training time = 0:03:30.967310
# F-Score(Ordinary) = 0.395, Recall: 0.733, Precision: 0.271
# F-Score(lvc) = 0.338, Recall: 0.493, Precision: 0.258
# F-Score(ireflv) = 0.452, Recall: 0.826, Precision: 0.311
# F-Score(id) = 0.395, Recall: 0.96, Precision: 0.249
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_157 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_158 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_157 (Embedding)       (None, 4, 48)        705264      input_157[0][0]                  
__________________________________________________________________________________________________
embedding_158 (Embedding)       (None, 4, 24)        5640        input_158[0][0]                  
__________________________________________________________________________________________________
flatten_157 (Flatten)           (None, 192)          0           embedding_157[0][0]              
__________________________________________________________________________________________________
flatten_158 (Flatten)           (None, 96)           0           embedding_158[0][0]              
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 288)          0           flatten_157[0][0]                
                                                                 flatten_158[0][0]                
__________________________________________________________________________________________________
dense_157 (Dense)               (None, 24)           6936        concatenate_79[0][0]             
__________________________________________________________________________________________________
dropout_79 (Dropout)            (None, 24)           0           dense_157[0][0]                  
__________________________________________________________________________________________________
dense_158 (Dense)               (None, 8)            200         dropout_79[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1786 - acc: 0.9484 - val_loss: 0.1065 - val_acc: 0.9647
Epoch 2/40
 - 4s - loss: 0.1047 - acc: 0.9628 - val_loss: 0.0953 - val_acc: 0.9649
Epoch 3/40
 - 4s - loss: 0.0924 - acc: 0.9657 - val_loss: 0.0893 - val_acc: 0.9660
Epoch 4/40
 - 4s - loss: 0.0845 - acc: 0.9676 - val_loss: 0.0858 - val_acc: 0.9662
Epoch 5/40
 - 4s - loss: 0.0788 - acc: 0.9685 - val_loss: 0.0829 - val_acc: 0.9666
Epoch 6/40
 - 4s - loss: 0.0751 - acc: 0.9691 - val_loss: 0.0807 - val_acc: 0.9668
Epoch 7/40
 - 4s - loss: 0.0720 - acc: 0.9699 - val_loss: 0.0791 - val_acc: 0.9674
Epoch 8/40
 - 4s - loss: 0.0688 - acc: 0.9704 - val_loss: 0.0779 - val_acc: 0.9675
Epoch 9/40
 - 4s - loss: 0.0662 - acc: 0.9710 - val_loss: 0.0766 - val_acc: 0.9678
Epoch 10/40
 - 4s - loss: 0.0647 - acc: 0.9713 - val_loss: 0.0758 - val_acc: 0.9675
Epoch 11/40
 - 4s - loss: 0.0626 - acc: 0.9720 - val_loss: 0.0752 - val_acc: 0.9677
Epoch 12/40
 - 4s - loss: 0.0614 - acc: 0.9717 - val_loss: 0.0745 - val_acc: 0.9678
Epoch 13/40
 - 4s - loss: 0.0597 - acc: 0.9725 - val_loss: 0.0742 - val_acc: 0.9678
Epoch 14/40
 - 4s - loss: 0.0586 - acc: 0.9729 - val_loss: 0.0739 - val_acc: 0.9677
Epoch 15/40
 - 4s - loss: 0.0573 - acc: 0.9731 - val_loss: 0.0737 - val_acc: 0.9679
Epoch 16/40
 - 4s - loss: 0.0564 - acc: 0.9732 - val_loss: 0.0734 - val_acc: 0.9679
Epoch 17/40
 - 4s - loss: 0.0554 - acc: 0.9738 - val_loss: 0.0737 - val_acc: 0.9678
Epoch 18/40
 - 4s - loss: 0.0544 - acc: 0.9738 - val_loss: 0.0737 - val_acc: 0.9680
Epoch 19/40
 - 4s - loss: 0.0539 - acc: 0.9740 - val_loss: 0.0736 - val_acc: 0.9678
Epoch 20/40
 - 4s - loss: 0.0531 - acc: 0.9741 - val_loss: 0.0734 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0525 - acc: 0.9745 - val_loss: 0.0735 - val_acc: 0.9678
Epoch 22/40
 - 4s - loss: 0.0520 - acc: 0.9746 - val_loss: 0.0735 - val_acc: 0.9678
Epoch 23/40
 - 4s - loss: 0.0513 - acc: 0.9746 - val_loss: 0.0734 - val_acc: 0.9679
Epoch 24/40
 - 4s - loss: 0.0513 - acc: 0.9743 - val_loss: 0.0736 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0500 - acc: 0.9748 - val_loss: 0.0738 - val_acc: 0.9678
Epoch 26/40
 - 4s - loss: 0.0499 - acc: 0.9749 - val_loss: 0.0737 - val_acc: 0.9679
Epoch 27/40
 - 4s - loss: 0.0495 - acc: 0.9753 - val_loss: 0.0737 - val_acc: 0.9678
Epoch 28/40
 - 4s - loss: 0.0492 - acc: 0.9749 - val_loss: 0.0742 - val_acc: 0.9678
Epoch 29/40
 - 4s - loss: 0.0490 - acc: 0.9749 - val_loss: 0.0739 - val_acc: 0.9679
Epoch 30/40
 - 4s - loss: 0.0485 - acc: 0.9754 - val_loss: 0.0740 - val_acc: 0.9677
Epoch 31/40
 - 4s - loss: 0.0480 - acc: 0.9757 - val_loss: 0.0741 - val_acc: 0.9677
Epoch 32/40
 - 4s - loss: 0.0477 - acc: 0.9756 - val_loss: 0.0744 - val_acc: 0.9678
Epoch 33/40
 - 4s - loss: 0.0474 - acc: 0.9756 - val_loss: 0.0745 - val_acc: 0.9677
Epoch 34/40
 - 4s - loss: 0.0472 - acc: 0.9756 - val_loss: 0.0747 - val_acc: 0.9678
Epoch 35/40
 - 4s - loss: 0.0470 - acc: 0.9756 - val_loss: 0.0747 - val_acc: 0.9677
Epoch 36/40
 - 4s - loss: 0.0465 - acc: 0.9756 - val_loss: 0.0747 - val_acc: 0.9678
Epoch 37/40
 - 4s - loss: 0.0463 - acc: 0.9757 - val_loss: 0.0748 - val_acc: 0.9678
Epoch 38/40
 - 4s - loss: 0.0464 - acc: 0.9757 - val_loss: 0.0748 - val_acc: 0.9678
Epoch 39/40
 - 4s - loss: 0.0459 - acc: 0.9760 - val_loss: 0.0750 - val_acc: 0.9678
Epoch 40/40
 - 4s - loss: 0.0458 - acc: 0.9758 - val_loss: 0.0750 - val_acc: 0.9679
Epoch 1/40
 - 0s - loss: 0.0768 - acc: 0.9669
Epoch 2/40
 - 0s - loss: 0.0693 - acc: 0.9683
Epoch 3/40
 - 0s - loss: 0.0649 - acc: 0.9696
Epoch 4/40
 - 0s - loss: 0.0635 - acc: 0.9702
Epoch 5/40
 - 0s - loss: 0.0602 - acc: 0.9719
Epoch 6/40
 - 0s - loss: 0.0588 - acc: 0.9720
Epoch 7/40
 - 0s - loss: 0.0575 - acc: 0.9730
Epoch 8/40
 - 0s - loss: 0.0568 - acc: 0.9722
Epoch 9/40
 - 0s - loss: 0.0562 - acc: 0.9736
Epoch 10/40
 - 0s - loss: 0.0550 - acc: 0.9737
Epoch 11/40
 - 0s - loss: 0.0535 - acc: 0.9734
Epoch 12/40
 - 0s - loss: 0.0536 - acc: 0.9738
Epoch 13/40
 - 0s - loss: 0.0531 - acc: 0.9744
Epoch 14/40
 - 0s - loss: 0.0524 - acc: 0.9740
Epoch 15/40
 - 0s - loss: 0.0514 - acc: 0.9746
Epoch 16/40
 - 0s - loss: 0.0509 - acc: 0.9750
Epoch 17/40
 - 0s - loss: 0.0514 - acc: 0.9737
Epoch 18/40
 - 0s - loss: 0.0507 - acc: 0.9748
Epoch 19/40
 - 0s - loss: 0.0499 - acc: 0.9745
Epoch 20/40
 - 0s - loss: 0.0499 - acc: 0.9744
Epoch 21/40
 - 0s - loss: 0.0498 - acc: 0.9733
Epoch 22/40
 - 0s - loss: 0.0490 - acc: 0.9751
Epoch 23/40
 - 0s - loss: 0.0493 - acc: 0.9743
Epoch 24/40
 - 0s - loss: 0.0485 - acc: 0.9754
Epoch 25/40
 - 0s - loss: 0.0478 - acc: 0.9759
Epoch 26/40
 - 0s - loss: 0.0485 - acc: 0.9753
Epoch 27/40
 - 0s - loss: 0.0475 - acc: 0.9756
Epoch 28/40
 - 0s - loss: 0.0471 - acc: 0.9766
Epoch 29/40
 - 0s - loss: 0.0464 - acc: 0.9755
Epoch 30/40
 - 0s - loss: 0.0469 - acc: 0.9750
Epoch 31/40
 - 0s - loss: 0.0464 - acc: 0.9758
Epoch 32/40
 - 0s - loss: 0.0457 - acc: 0.9756
Epoch 33/40
 - 0s - loss: 0.0463 - acc: 0.9750
Epoch 34/40
 - 0s - loss: 0.0465 - acc: 0.9756
Epoch 35/40
 - 0s - loss: 0.0467 - acc: 0.9754
Epoch 36/40
 - 0s - loss: 0.0451 - acc: 0.9778
Epoch 37/40
 - 0s - loss: 0.0454 - acc: 0.9754
Epoch 38/40
 - 0s - loss: 0.0441 - acc: 0.9767
Epoch 39/40
 - 0s - loss: 0.0450 - acc: 0.9764
Epoch 40/40
 - 0s - loss: 0.0451 - acc: 0.9761
# Training time = 0:03:29.168018
# F-Score(Ordinary) = 0.394, Recall: 0.864, Precision: 0.255
# F-Score(lvc) = 0.279, Recall: 0.697, Precision: 0.174
# F-Score(ireflv) = 0.383, Recall: 0.775, Precision: 0.254
# F-Score(id) = 0.468, Recall: 1.0, Precision: 0.306
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_159 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_160 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_159 (Embedding)       (None, 4, 48)        705264      input_159[0][0]                  
__________________________________________________________________________________________________
embedding_160 (Embedding)       (None, 4, 24)        5640        input_160[0][0]                  
__________________________________________________________________________________________________
flatten_159 (Flatten)           (None, 192)          0           embedding_159[0][0]              
__________________________________________________________________________________________________
flatten_160 (Flatten)           (None, 96)           0           embedding_160[0][0]              
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 288)          0           flatten_159[0][0]                
                                                                 flatten_160[0][0]                
__________________________________________________________________________________________________
dense_159 (Dense)               (None, 24)           6936        concatenate_80[0][0]             
__________________________________________________________________________________________________
dropout_80 (Dropout)            (None, 24)           0           dense_159[0][0]                  
__________________________________________________________________________________________________
dense_160 (Dense)               (None, 8)            200         dropout_80[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1709 - acc: 0.9485 - val_loss: 0.1061 - val_acc: 0.9643
Epoch 2/40
 - 4s - loss: 0.1039 - acc: 0.9628 - val_loss: 0.0954 - val_acc: 0.9656
Epoch 3/40
 - 4s - loss: 0.0923 - acc: 0.9659 - val_loss: 0.0906 - val_acc: 0.9661
Epoch 4/40
 - 4s - loss: 0.0859 - acc: 0.9677 - val_loss: 0.0874 - val_acc: 0.9665
Epoch 5/40
 - 4s - loss: 0.0811 - acc: 0.9688 - val_loss: 0.0849 - val_acc: 0.9673
Epoch 6/40
 - 4s - loss: 0.0769 - acc: 0.9697 - val_loss: 0.0827 - val_acc: 0.9676
Epoch 7/40
 - 4s - loss: 0.0739 - acc: 0.9699 - val_loss: 0.0809 - val_acc: 0.9677
Epoch 8/40
 - 4s - loss: 0.0708 - acc: 0.9710 - val_loss: 0.0795 - val_acc: 0.9675
Epoch 9/40
 - 4s - loss: 0.0689 - acc: 0.9707 - val_loss: 0.0783 - val_acc: 0.9678
Epoch 10/40
 - 4s - loss: 0.0664 - acc: 0.9716 - val_loss: 0.0773 - val_acc: 0.9679
Epoch 11/40
 - 4s - loss: 0.0648 - acc: 0.9718 - val_loss: 0.0766 - val_acc: 0.9680
Epoch 12/40
 - 4s - loss: 0.0633 - acc: 0.9720 - val_loss: 0.0760 - val_acc: 0.9679
Epoch 13/40
 - 4s - loss: 0.0616 - acc: 0.9725 - val_loss: 0.0752 - val_acc: 0.9681
Epoch 14/40
 - 4s - loss: 0.0605 - acc: 0.9728 - val_loss: 0.0751 - val_acc: 0.9682
Epoch 15/40
 - 4s - loss: 0.0590 - acc: 0.9738 - val_loss: 0.0746 - val_acc: 0.9683
Epoch 16/40
 - 4s - loss: 0.0582 - acc: 0.9735 - val_loss: 0.0743 - val_acc: 0.9681
Epoch 17/40
 - 4s - loss: 0.0575 - acc: 0.9731 - val_loss: 0.0740 - val_acc: 0.9684
Epoch 18/40
 - 4s - loss: 0.0564 - acc: 0.9735 - val_loss: 0.0738 - val_acc: 0.9683
Epoch 19/40
 - 4s - loss: 0.0560 - acc: 0.9735 - val_loss: 0.0735 - val_acc: 0.9683
Epoch 20/40
 - 4s - loss: 0.0553 - acc: 0.9737 - val_loss: 0.0734 - val_acc: 0.9683
Epoch 21/40
 - 4s - loss: 0.0545 - acc: 0.9742 - val_loss: 0.0734 - val_acc: 0.9683
Epoch 22/40
 - 4s - loss: 0.0538 - acc: 0.9745 - val_loss: 0.0734 - val_acc: 0.9684
Epoch 23/40
 - 4s - loss: 0.0534 - acc: 0.9744 - val_loss: 0.0736 - val_acc: 0.9683
Epoch 24/40
 - 4s - loss: 0.0526 - acc: 0.9746 - val_loss: 0.0734 - val_acc: 0.9682
Epoch 25/40
 - 4s - loss: 0.0521 - acc: 0.9745 - val_loss: 0.0735 - val_acc: 0.9682
Epoch 26/40
 - 4s - loss: 0.0517 - acc: 0.9745 - val_loss: 0.0735 - val_acc: 0.9683
Epoch 27/40
 - 4s - loss: 0.0515 - acc: 0.9747 - val_loss: 0.0736 - val_acc: 0.9683
Epoch 28/40
 - 4s - loss: 0.0509 - acc: 0.9745 - val_loss: 0.0736 - val_acc: 0.9683
Epoch 29/40
 - 4s - loss: 0.0506 - acc: 0.9750 - val_loss: 0.0737 - val_acc: 0.9680
Epoch 30/40
 - 4s - loss: 0.0502 - acc: 0.9752 - val_loss: 0.0737 - val_acc: 0.9682
Epoch 31/40
 - 4s - loss: 0.0500 - acc: 0.9748 - val_loss: 0.0737 - val_acc: 0.9682
Epoch 32/40
 - 4s - loss: 0.0496 - acc: 0.9750 - val_loss: 0.0738 - val_acc: 0.9681
Epoch 33/40
 - 4s - loss: 0.0487 - acc: 0.9753 - val_loss: 0.0738 - val_acc: 0.9681
Epoch 34/40
 - 4s - loss: 0.0489 - acc: 0.9751 - val_loss: 0.0741 - val_acc: 0.9680
Epoch 35/40
 - 4s - loss: 0.0490 - acc: 0.9753 - val_loss: 0.0741 - val_acc: 0.9680
Epoch 36/40
 - 4s - loss: 0.0487 - acc: 0.9748 - val_loss: 0.0741 - val_acc: 0.9682
Epoch 37/40
 - 4s - loss: 0.0481 - acc: 0.9755 - val_loss: 0.0742 - val_acc: 0.9680
Epoch 38/40
 - 4s - loss: 0.0482 - acc: 0.9753 - val_loss: 0.0743 - val_acc: 0.9682
Epoch 39/40
 - 4s - loss: 0.0478 - acc: 0.9753 - val_loss: 0.0742 - val_acc: 0.9680
Epoch 40/40
 - 4s - loss: 0.0475 - acc: 0.9758 - val_loss: 0.0744 - val_acc: 0.9680
Epoch 1/40
 - 0s - loss: 0.0793 - acc: 0.9650
Epoch 2/40
 - 0s - loss: 0.0713 - acc: 0.9668
Epoch 3/40
 - 0s - loss: 0.0667 - acc: 0.9680
Epoch 4/40
 - 0s - loss: 0.0646 - acc: 0.9706
Epoch 5/40
 - 0s - loss: 0.0621 - acc: 0.9715
Epoch 6/40
 - 0s - loss: 0.0599 - acc: 0.9715
Epoch 7/40
 - 0s - loss: 0.0589 - acc: 0.9716
Epoch 8/40
 - 0s - loss: 0.0580 - acc: 0.9717
Epoch 9/40
 - 0s - loss: 0.0570 - acc: 0.9722
Epoch 10/40
 - 0s - loss: 0.0560 - acc: 0.9730
Epoch 11/40
 - 0s - loss: 0.0559 - acc: 0.9735
Epoch 12/40
 - 0s - loss: 0.0547 - acc: 0.9731
Epoch 13/40
 - 0s - loss: 0.0549 - acc: 0.9722
Epoch 14/40
 - 0s - loss: 0.0536 - acc: 0.9740
Epoch 15/40
 - 0s - loss: 0.0530 - acc: 0.9748
Epoch 16/40
 - 0s - loss: 0.0527 - acc: 0.9746
Epoch 17/40
 - 0s - loss: 0.0529 - acc: 0.9744
Epoch 18/40
 - 0s - loss: 0.0515 - acc: 0.9744
Epoch 19/40
 - 0s - loss: 0.0523 - acc: 0.9743
Epoch 20/40
 - 0s - loss: 0.0511 - acc: 0.9742
Epoch 21/40
 - 0s - loss: 0.0511 - acc: 0.9755
Epoch 22/40
 - 0s - loss: 0.0497 - acc: 0.9745
Epoch 23/40
 - 0s - loss: 0.0496 - acc: 0.9757
Epoch 24/40
 - 0s - loss: 0.0501 - acc: 0.9751
Epoch 25/40
 - 0s - loss: 0.0498 - acc: 0.9755
Epoch 26/40
 - 0s - loss: 0.0490 - acc: 0.9755
Epoch 27/40
 - 0s - loss: 0.0496 - acc: 0.9752
Epoch 28/40
 - 0s - loss: 0.0488 - acc: 0.9755
Epoch 29/40
 - 0s - loss: 0.0483 - acc: 0.9753
Epoch 30/40
 - 0s - loss: 0.0482 - acc: 0.9765
Epoch 31/40
 - 0s - loss: 0.0477 - acc: 0.9760
Epoch 32/40
 - 0s - loss: 0.0473 - acc: 0.9754
Epoch 33/40
 - 0s - loss: 0.0475 - acc: 0.9757
Epoch 34/40
 - 0s - loss: 0.0473 - acc: 0.9750
Epoch 35/40
 - 0s - loss: 0.0459 - acc: 0.9769
Epoch 36/40
 - 0s - loss: 0.0462 - acc: 0.9755
Epoch 37/40
 - 0s - loss: 0.0458 - acc: 0.9771
Epoch 38/40
 - 0s - loss: 0.0472 - acc: 0.9755
Epoch 39/40
 - 0s - loss: 0.0465 - acc: 0.9766
Epoch 40/40
 - 0s - loss: 0.0473 - acc: 0.9745
# Training time = 0:03:28.012686
# F-Score(Ordinary) = 0.336, Recall: 0.692, Precision: 0.221
# F-Score(lvc) = 0.266, Recall: 0.808, Precision: 0.159
# F-Score(ireflv) = 0.347, Recall: 0.459, Precision: 0.279
# F-Score(id) = 0.364, Recall: 1.0, Precision: 0.223
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_161 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_162 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_161 (Embedding)       (None, 4, 48)        705264      input_161[0][0]                  
__________________________________________________________________________________________________
embedding_162 (Embedding)       (None, 4, 24)        5640        input_162[0][0]                  
__________________________________________________________________________________________________
flatten_161 (Flatten)           (None, 192)          0           embedding_161[0][0]              
__________________________________________________________________________________________________
flatten_162 (Flatten)           (None, 96)           0           embedding_162[0][0]              
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 288)          0           flatten_161[0][0]                
                                                                 flatten_162[0][0]                
__________________________________________________________________________________________________
dense_161 (Dense)               (None, 24)           6936        concatenate_81[0][0]             
__________________________________________________________________________________________________
dropout_81 (Dropout)            (None, 24)           0           dense_161[0][0]                  
__________________________________________________________________________________________________
dense_162 (Dense)               (None, 8)            200         dropout_81[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1183 - acc: 0.9589 - val_loss: 0.0849 - val_acc: 0.9662
Epoch 2/40
 - 4s - loss: 0.0765 - acc: 0.9680 - val_loss: 0.0779 - val_acc: 0.9668
Epoch 3/40
 - 4s - loss: 0.0663 - acc: 0.9706 - val_loss: 0.0746 - val_acc: 0.9672
Epoch 4/40
 - 4s - loss: 0.0611 - acc: 0.9717 - val_loss: 0.0731 - val_acc: 0.9673
Epoch 5/40
 - 4s - loss: 0.0572 - acc: 0.9731 - val_loss: 0.0727 - val_acc: 0.9678
Epoch 6/40
 - 4s - loss: 0.0544 - acc: 0.9735 - val_loss: 0.0726 - val_acc: 0.9682
Epoch 7/40
 - 4s - loss: 0.0524 - acc: 0.9738 - val_loss: 0.0726 - val_acc: 0.9683
Epoch 8/40
 - 4s - loss: 0.0507 - acc: 0.9744 - val_loss: 0.0732 - val_acc: 0.9682
Epoch 9/40
 - 4s - loss: 0.0495 - acc: 0.9749 - val_loss: 0.0733 - val_acc: 0.9683
Epoch 10/40
 - 4s - loss: 0.0482 - acc: 0.9749 - val_loss: 0.0740 - val_acc: 0.9679
Epoch 11/40
 - 4s - loss: 0.0474 - acc: 0.9751 - val_loss: 0.0743 - val_acc: 0.9680
Epoch 12/40
 - 4s - loss: 0.0462 - acc: 0.9753 - val_loss: 0.0747 - val_acc: 0.9682
Epoch 13/40
 - 4s - loss: 0.0457 - acc: 0.9756 - val_loss: 0.0753 - val_acc: 0.9678
Epoch 14/40
 - 4s - loss: 0.0450 - acc: 0.9758 - val_loss: 0.0756 - val_acc: 0.9679
Epoch 15/40
 - 4s - loss: 0.0447 - acc: 0.9758 - val_loss: 0.0756 - val_acc: 0.9679
Epoch 16/40
 - 4s - loss: 0.0442 - acc: 0.9758 - val_loss: 0.0760 - val_acc: 0.9678
Epoch 17/40
 - 4s - loss: 0.0438 - acc: 0.9759 - val_loss: 0.0771 - val_acc: 0.9678
Epoch 18/40
 - 4s - loss: 0.0432 - acc: 0.9763 - val_loss: 0.0772 - val_acc: 0.9679
Epoch 19/40
 - 4s - loss: 0.0431 - acc: 0.9765 - val_loss: 0.0770 - val_acc: 0.9679
Epoch 20/40
 - 4s - loss: 0.0430 - acc: 0.9760 - val_loss: 0.0779 - val_acc: 0.9678
Epoch 21/40
 - 4s - loss: 0.0424 - acc: 0.9766 - val_loss: 0.0781 - val_acc: 0.9681
Epoch 22/40
 - 4s - loss: 0.0424 - acc: 0.9763 - val_loss: 0.0779 - val_acc: 0.9679
Epoch 23/40
 - 4s - loss: 0.0421 - acc: 0.9765 - val_loss: 0.0786 - val_acc: 0.9680
Epoch 24/40
 - 4s - loss: 0.0420 - acc: 0.9763 - val_loss: 0.0786 - val_acc: 0.9680
Epoch 25/40
 - 4s - loss: 0.0415 - acc: 0.9769 - val_loss: 0.0791 - val_acc: 0.9681
Epoch 26/40
 - 4s - loss: 0.0414 - acc: 0.9766 - val_loss: 0.0795 - val_acc: 0.9679
Epoch 27/40
 - 4s - loss: 0.0410 - acc: 0.9771 - val_loss: 0.0798 - val_acc: 0.9679
Epoch 28/40
 - 4s - loss: 0.0410 - acc: 0.9766 - val_loss: 0.0800 - val_acc: 0.9678
Epoch 29/40
 - 4s - loss: 0.0410 - acc: 0.9769 - val_loss: 0.0800 - val_acc: 0.9680
Epoch 30/40
 - 4s - loss: 0.0407 - acc: 0.9769 - val_loss: 0.0805 - val_acc: 0.9679
Epoch 31/40
 - 4s - loss: 0.0407 - acc: 0.9765 - val_loss: 0.0805 - val_acc: 0.9680
Epoch 32/40
 - 4s - loss: 0.0404 - acc: 0.9770 - val_loss: 0.0809 - val_acc: 0.9680
Epoch 33/40
 - 4s - loss: 0.0405 - acc: 0.9769 - val_loss: 0.0814 - val_acc: 0.9679
Epoch 34/40
 - 4s - loss: 0.0401 - acc: 0.9770 - val_loss: 0.0810 - val_acc: 0.9681
Epoch 35/40
 - 4s - loss: 0.0400 - acc: 0.9770 - val_loss: 0.0810 - val_acc: 0.9681
Epoch 36/40
 - 4s - loss: 0.0400 - acc: 0.9771 - val_loss: 0.0819 - val_acc: 0.9679
Epoch 37/40
 - 4s - loss: 0.0400 - acc: 0.9769 - val_loss: 0.0821 - val_acc: 0.9679
Epoch 38/40
 - 4s - loss: 0.0399 - acc: 0.9767 - val_loss: 0.0820 - val_acc: 0.9681
Epoch 39/40
 - 4s - loss: 0.0398 - acc: 0.9769 - val_loss: 0.0823 - val_acc: 0.9680
Epoch 40/40
 - 4s - loss: 0.0397 - acc: 0.9768 - val_loss: 0.0832 - val_acc: 0.9680
Epoch 1/40
 - 0s - loss: 0.0793 - acc: 0.9659
Epoch 2/40
 - 0s - loss: 0.0633 - acc: 0.9698
Epoch 3/40
 - 0s - loss: 0.0582 - acc: 0.9716
Epoch 4/40
 - 0s - loss: 0.0537 - acc: 0.9741
Epoch 5/40
 - 0s - loss: 0.0520 - acc: 0.9745
Epoch 6/40
 - 0s - loss: 0.0504 - acc: 0.9744
Epoch 7/40
 - 0s - loss: 0.0495 - acc: 0.9741
Epoch 8/40
 - 0s - loss: 0.0475 - acc: 0.9749
Epoch 9/40
 - 0s - loss: 0.0468 - acc: 0.9744
Epoch 10/40
 - 0s - loss: 0.0459 - acc: 0.9752
Epoch 11/40
 - 0s - loss: 0.0454 - acc: 0.9757
Epoch 12/40
 - 0s - loss: 0.0439 - acc: 0.9768
Epoch 13/40
 - 0s - loss: 0.0434 - acc: 0.9769
Epoch 14/40
 - 0s - loss: 0.0438 - acc: 0.9756
Epoch 15/40
 - 0s - loss: 0.0425 - acc: 0.9763
Epoch 16/40
 - 0s - loss: 0.0424 - acc: 0.9762
Epoch 17/40
 - 0s - loss: 0.0418 - acc: 0.9768
Epoch 18/40
 - 0s - loss: 0.0409 - acc: 0.9766
Epoch 19/40
 - 0s - loss: 0.0411 - acc: 0.9763
Epoch 20/40
 - 0s - loss: 0.0409 - acc: 0.9767
Epoch 21/40
 - 0s - loss: 0.0404 - acc: 0.9769
Epoch 22/40
 - 0s - loss: 0.0398 - acc: 0.9784
Epoch 23/40
 - 0s - loss: 0.0391 - acc: 0.9776
Epoch 24/40
 - 0s - loss: 0.0387 - acc: 0.9785
Epoch 25/40
 - 0s - loss: 0.0387 - acc: 0.9777
Epoch 26/40
 - 0s - loss: 0.0385 - acc: 0.9769
Epoch 27/40
 - 0s - loss: 0.0389 - acc: 0.9769
Epoch 28/40
 - 0s - loss: 0.0387 - acc: 0.9772
Epoch 29/40
 - 0s - loss: 0.0380 - acc: 0.9782
Epoch 30/40
 - 0s - loss: 0.0382 - acc: 0.9775
Epoch 31/40
 - 0s - loss: 0.0367 - acc: 0.9799
Epoch 32/40
 - 0s - loss: 0.0380 - acc: 0.9775
Epoch 33/40
 - 0s - loss: 0.0378 - acc: 0.9772
Epoch 34/40
 - 0s - loss: 0.0375 - acc: 0.9768
Epoch 35/40
 - 0s - loss: 0.0371 - acc: 0.9778
Epoch 36/40
 - 0s - loss: 0.0368 - acc: 0.9776
Epoch 37/40
 - 0s - loss: 0.0374 - acc: 0.9765
Epoch 38/40
 - 0s - loss: 0.0371 - acc: 0.9771
Epoch 39/40
 - 0s - loss: 0.0364 - acc: 0.9780
Epoch 40/40
 - 0s - loss: 0.0362 - acc: 0.9774
# Training time = 0:03:30.249034
# F-Score(Ordinary) = 0.407, Recall: 0.887, Precision: 0.264
# F-Score(lvc) = 0.319, Recall: 0.839, Precision: 0.197
# F-Score(ireflv) = 0.625, Recall: 0.857, Precision: 0.492
# F-Score(id) = 0.284, Recall: 1.0, Precision: 0.166
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_163 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_164 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_163 (Embedding)       (None, 4, 48)        705264      input_163[0][0]                  
__________________________________________________________________________________________________
embedding_164 (Embedding)       (None, 4, 24)        5640        input_164[0][0]                  
__________________________________________________________________________________________________
flatten_163 (Flatten)           (None, 192)          0           embedding_163[0][0]              
__________________________________________________________________________________________________
flatten_164 (Flatten)           (None, 96)           0           embedding_164[0][0]              
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 288)          0           flatten_163[0][0]                
                                                                 flatten_164[0][0]                
__________________________________________________________________________________________________
dense_163 (Dense)               (None, 24)           6936        concatenate_82[0][0]             
__________________________________________________________________________________________________
dropout_82 (Dropout)            (None, 24)           0           dense_163[0][0]                  
__________________________________________________________________________________________________
dense_164 (Dense)               (None, 8)            200         dropout_82[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1204 - acc: 0.9595 - val_loss: 0.0862 - val_acc: 0.9674
Epoch 2/40
 - 4s - loss: 0.0784 - acc: 0.9687 - val_loss: 0.0785 - val_acc: 0.9684
Epoch 3/40
 - 4s - loss: 0.0677 - acc: 0.9703 - val_loss: 0.0740 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0613 - acc: 0.9717 - val_loss: 0.0719 - val_acc: 0.9682
Epoch 5/40
 - 4s - loss: 0.0570 - acc: 0.9727 - val_loss: 0.0711 - val_acc: 0.9679
Epoch 6/40
 - 4s - loss: 0.0538 - acc: 0.9733 - val_loss: 0.0710 - val_acc: 0.9682
Epoch 7/40
 - 4s - loss: 0.0518 - acc: 0.9742 - val_loss: 0.0709 - val_acc: 0.9684
Epoch 8/40
 - 4s - loss: 0.0497 - acc: 0.9745 - val_loss: 0.0714 - val_acc: 0.9686
Epoch 9/40
 - 4s - loss: 0.0488 - acc: 0.9747 - val_loss: 0.0716 - val_acc: 0.9683
Epoch 10/40
 - 4s - loss: 0.0479 - acc: 0.9754 - val_loss: 0.0722 - val_acc: 0.9684
Epoch 11/40
 - 4s - loss: 0.0469 - acc: 0.9755 - val_loss: 0.0729 - val_acc: 0.9682
Epoch 12/40
 - 4s - loss: 0.0463 - acc: 0.9752 - val_loss: 0.0730 - val_acc: 0.9684
Epoch 13/40
 - 4s - loss: 0.0456 - acc: 0.9756 - val_loss: 0.0732 - val_acc: 0.9682
Epoch 14/40
 - 4s - loss: 0.0452 - acc: 0.9759 - val_loss: 0.0741 - val_acc: 0.9681
Epoch 15/40
 - 4s - loss: 0.0445 - acc: 0.9762 - val_loss: 0.0739 - val_acc: 0.9680
Epoch 16/40
 - 4s - loss: 0.0442 - acc: 0.9758 - val_loss: 0.0746 - val_acc: 0.9679
Epoch 17/40
 - 4s - loss: 0.0438 - acc: 0.9758 - val_loss: 0.0746 - val_acc: 0.9680
Epoch 18/40
 - 4s - loss: 0.0435 - acc: 0.9761 - val_loss: 0.0751 - val_acc: 0.9680
Epoch 19/40
 - 4s - loss: 0.0431 - acc: 0.9762 - val_loss: 0.0753 - val_acc: 0.9681
Epoch 20/40
 - 4s - loss: 0.0429 - acc: 0.9762 - val_loss: 0.0758 - val_acc: 0.9681
Epoch 21/40
 - 4s - loss: 0.0425 - acc: 0.9763 - val_loss: 0.0760 - val_acc: 0.9680
Epoch 22/40
 - 4s - loss: 0.0423 - acc: 0.9769 - val_loss: 0.0766 - val_acc: 0.9679
Epoch 23/40
 - 4s - loss: 0.0423 - acc: 0.9765 - val_loss: 0.0766 - val_acc: 0.9680
Epoch 24/40
 - 4s - loss: 0.0420 - acc: 0.9769 - val_loss: 0.0771 - val_acc: 0.9683
Epoch 25/40
 - 4s - loss: 0.0418 - acc: 0.9769 - val_loss: 0.0775 - val_acc: 0.9682
Epoch 26/40
 - 4s - loss: 0.0413 - acc: 0.9767 - val_loss: 0.0777 - val_acc: 0.9682
Epoch 27/40
 - 4s - loss: 0.0415 - acc: 0.9766 - val_loss: 0.0775 - val_acc: 0.9680
Epoch 28/40
 - 4s - loss: 0.0412 - acc: 0.9769 - val_loss: 0.0782 - val_acc: 0.9680
Epoch 29/40
 - 4s - loss: 0.0410 - acc: 0.9769 - val_loss: 0.0786 - val_acc: 0.9681
Epoch 30/40
 - 4s - loss: 0.0408 - acc: 0.9771 - val_loss: 0.0788 - val_acc: 0.9679
Epoch 31/40
 - 4s - loss: 0.0406 - acc: 0.9771 - val_loss: 0.0789 - val_acc: 0.9679
Epoch 32/40
 - 4s - loss: 0.0405 - acc: 0.9771 - val_loss: 0.0793 - val_acc: 0.9679
Epoch 33/40
 - 4s - loss: 0.0406 - acc: 0.9771 - val_loss: 0.0796 - val_acc: 0.9679
Epoch 34/40
 - 4s - loss: 0.0403 - acc: 0.9770 - val_loss: 0.0792 - val_acc: 0.9682
Epoch 35/40
 - 4s - loss: 0.0403 - acc: 0.9772 - val_loss: 0.0791 - val_acc: 0.9681
Epoch 36/40
 - 4s - loss: 0.0401 - acc: 0.9772 - val_loss: 0.0801 - val_acc: 0.9680
Epoch 37/40
 - 4s - loss: 0.0400 - acc: 0.9770 - val_loss: 0.0802 - val_acc: 0.9682
Epoch 38/40
 - 4s - loss: 0.0399 - acc: 0.9770 - val_loss: 0.0807 - val_acc: 0.9678
Epoch 39/40
 - 4s - loss: 0.0398 - acc: 0.9770 - val_loss: 0.0807 - val_acc: 0.9681
Epoch 40/40
 - 4s - loss: 0.0397 - acc: 0.9770 - val_loss: 0.0808 - val_acc: 0.9682
Epoch 1/40
 - 0s - loss: 0.0759 - acc: 0.9674
Epoch 2/40
 - 0s - loss: 0.0623 - acc: 0.9699
Epoch 3/40
 - 0s - loss: 0.0570 - acc: 0.9727
Epoch 4/40
 - 0s - loss: 0.0539 - acc: 0.9727
Epoch 5/40
 - 0s - loss: 0.0519 - acc: 0.9740
Epoch 6/40
 - 0s - loss: 0.0501 - acc: 0.9741
Epoch 7/40
 - 0s - loss: 0.0485 - acc: 0.9746
Epoch 8/40
 - 0s - loss: 0.0476 - acc: 0.9745
Epoch 9/40
 - 0s - loss: 0.0463 - acc: 0.9754
Epoch 10/40
 - 0s - loss: 0.0461 - acc: 0.9758
Epoch 11/40
 - 0s - loss: 0.0457 - acc: 0.9756
Epoch 12/40
 - 0s - loss: 0.0443 - acc: 0.9761
Epoch 13/40
 - 0s - loss: 0.0433 - acc: 0.9757
Epoch 14/40
 - 0s - loss: 0.0431 - acc: 0.9776
Epoch 15/40
 - 0s - loss: 0.0423 - acc: 0.9762
Epoch 16/40
 - 0s - loss: 0.0421 - acc: 0.9769
Epoch 17/40
 - 0s - loss: 0.0419 - acc: 0.9770
Epoch 18/40
 - 0s - loss: 0.0407 - acc: 0.9770
Epoch 19/40
 - 0s - loss: 0.0405 - acc: 0.9772
Epoch 20/40
 - 0s - loss: 0.0401 - acc: 0.9771
Epoch 21/40
 - 0s - loss: 0.0401 - acc: 0.9777
Epoch 22/40
 - 0s - loss: 0.0397 - acc: 0.9776
Epoch 23/40
 - 0s - loss: 0.0392 - acc: 0.9777
Epoch 24/40
 - 0s - loss: 0.0390 - acc: 0.9771
Epoch 25/40
 - 0s - loss: 0.0390 - acc: 0.9774
Epoch 26/40
 - 0s - loss: 0.0387 - acc: 0.9780
Epoch 27/40
 - 0s - loss: 0.0383 - acc: 0.9782
Epoch 28/40
 - 0s - loss: 0.0378 - acc: 0.9788
Epoch 29/40
 - 0s - loss: 0.0380 - acc: 0.9778
Epoch 30/40
 - 0s - loss: 0.0380 - acc: 0.9776
Epoch 31/40
 - 0s - loss: 0.0377 - acc: 0.9781
Epoch 32/40
 - 0s - loss: 0.0377 - acc: 0.9778
Epoch 33/40
 - 0s - loss: 0.0380 - acc: 0.9771
Epoch 34/40
 - 0s - loss: 0.0376 - acc: 0.9778
Epoch 35/40
 - 0s - loss: 0.0372 - acc: 0.9789
Epoch 36/40
 - 0s - loss: 0.0369 - acc: 0.9781
Epoch 37/40
 - 0s - loss: 0.0373 - acc: 0.9776
Epoch 38/40
 - 0s - loss: 0.0365 - acc: 0.9778
Epoch 39/40
 - 0s - loss: 0.0373 - acc: 0.9773
Epoch 40/40
 - 0s - loss: 0.0371 - acc: 0.9787
# Training time = 0:03:30.386107
# F-Score(Ordinary) = 0.484, Recall: 0.799, Precision: 0.347
# F-Score(lvc) = 0.337, Recall: 0.552, Precision: 0.242
# F-Score(ireflv) = 0.477, Recall: 0.778, Precision: 0.344
# F-Score(id) = 0.582, Recall: 0.976, Precision: 0.415
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_165 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_166 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_165 (Embedding)       (None, 4, 48)        705264      input_165[0][0]                  
__________________________________________________________________________________________________
embedding_166 (Embedding)       (None, 4, 24)        5640        input_166[0][0]                  
__________________________________________________________________________________________________
flatten_165 (Flatten)           (None, 192)          0           embedding_165[0][0]              
__________________________________________________________________________________________________
flatten_166 (Flatten)           (None, 96)           0           embedding_166[0][0]              
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 288)          0           flatten_165[0][0]                
                                                                 flatten_166[0][0]                
__________________________________________________________________________________________________
dense_165 (Dense)               (None, 24)           6936        concatenate_83[0][0]             
__________________________________________________________________________________________________
dropout_83 (Dropout)            (None, 24)           0           dense_165[0][0]                  
__________________________________________________________________________________________________
dense_166 (Dense)               (None, 8)            200         dropout_83[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1198 - acc: 0.9598 - val_loss: 0.0858 - val_acc: 0.9666
Epoch 2/40
 - 4s - loss: 0.0760 - acc: 0.9692 - val_loss: 0.0764 - val_acc: 0.9679
Epoch 3/40
 - 4s - loss: 0.0660 - acc: 0.9710 - val_loss: 0.0728 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0600 - acc: 0.9721 - val_loss: 0.0718 - val_acc: 0.9680
Epoch 5/40
 - 4s - loss: 0.0558 - acc: 0.9733 - val_loss: 0.0722 - val_acc: 0.9682
Epoch 6/40
 - 4s - loss: 0.0532 - acc: 0.9738 - val_loss: 0.0716 - val_acc: 0.9684
Epoch 7/40
 - 4s - loss: 0.0513 - acc: 0.9746 - val_loss: 0.0718 - val_acc: 0.9683
Epoch 8/40
 - 4s - loss: 0.0497 - acc: 0.9744 - val_loss: 0.0724 - val_acc: 0.9684
Epoch 9/40
 - 4s - loss: 0.0485 - acc: 0.9750 - val_loss: 0.0727 - val_acc: 0.9683
Epoch 10/40
 - 4s - loss: 0.0473 - acc: 0.9752 - val_loss: 0.0738 - val_acc: 0.9681
Epoch 11/40
 - 4s - loss: 0.0463 - acc: 0.9757 - val_loss: 0.0743 - val_acc: 0.9682
Epoch 12/40
 - 4s - loss: 0.0458 - acc: 0.9755 - val_loss: 0.0749 - val_acc: 0.9682
Epoch 13/40
 - 4s - loss: 0.0452 - acc: 0.9761 - val_loss: 0.0749 - val_acc: 0.9679
Epoch 14/40
 - 4s - loss: 0.0447 - acc: 0.9758 - val_loss: 0.0761 - val_acc: 0.9679
Epoch 15/40
 - 4s - loss: 0.0441 - acc: 0.9759 - val_loss: 0.0757 - val_acc: 0.9682
Epoch 16/40
 - 4s - loss: 0.0440 - acc: 0.9764 - val_loss: 0.0760 - val_acc: 0.9681
Epoch 17/40
 - 4s - loss: 0.0435 - acc: 0.9765 - val_loss: 0.0763 - val_acc: 0.9685
Epoch 18/40
 - 4s - loss: 0.0432 - acc: 0.9761 - val_loss: 0.0766 - val_acc: 0.9679
Epoch 19/40
 - 4s - loss: 0.0428 - acc: 0.9764 - val_loss: 0.0772 - val_acc: 0.9680
Epoch 20/40
 - 4s - loss: 0.0423 - acc: 0.9767 - val_loss: 0.0777 - val_acc: 0.9680
Epoch 21/40
 - 4s - loss: 0.0422 - acc: 0.9766 - val_loss: 0.0779 - val_acc: 0.9680
Epoch 22/40
 - 4s - loss: 0.0417 - acc: 0.9767 - val_loss: 0.0783 - val_acc: 0.9680
Epoch 23/40
 - 4s - loss: 0.0417 - acc: 0.9764 - val_loss: 0.0786 - val_acc: 0.9681
Epoch 24/40
 - 4s - loss: 0.0414 - acc: 0.9765 - val_loss: 0.0790 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0414 - acc: 0.9768 - val_loss: 0.0793 - val_acc: 0.9679
Epoch 26/40
 - 4s - loss: 0.0411 - acc: 0.9767 - val_loss: 0.0792 - val_acc: 0.9682
Epoch 27/40
 - 4s - loss: 0.0409 - acc: 0.9766 - val_loss: 0.0798 - val_acc: 0.9682
Epoch 28/40
 - 4s - loss: 0.0409 - acc: 0.9766 - val_loss: 0.0796 - val_acc: 0.9682
Epoch 29/40
 - 4s - loss: 0.0406 - acc: 0.9768 - val_loss: 0.0801 - val_acc: 0.9682
Epoch 30/40
 - 4s - loss: 0.0407 - acc: 0.9766 - val_loss: 0.0806 - val_acc: 0.9682
Epoch 31/40
 - 4s - loss: 0.0401 - acc: 0.9772 - val_loss: 0.0810 - val_acc: 0.9683
Epoch 32/40
 - 4s - loss: 0.0401 - acc: 0.9774 - val_loss: 0.0815 - val_acc: 0.9682
Epoch 33/40
 - 4s - loss: 0.0401 - acc: 0.9771 - val_loss: 0.0818 - val_acc: 0.9681
Epoch 34/40
 - 4s - loss: 0.0402 - acc: 0.9770 - val_loss: 0.0811 - val_acc: 0.9683
Epoch 35/40
 - 4s - loss: 0.0397 - acc: 0.9771 - val_loss: 0.0817 - val_acc: 0.9678
Epoch 36/40
 - 4s - loss: 0.0398 - acc: 0.9768 - val_loss: 0.0819 - val_acc: 0.9680
Epoch 37/40
 - 4s - loss: 0.0396 - acc: 0.9772 - val_loss: 0.0825 - val_acc: 0.9678
Epoch 38/40
 - 4s - loss: 0.0396 - acc: 0.9771 - val_loss: 0.0826 - val_acc: 0.9679
Epoch 39/40
 - 4s - loss: 0.0393 - acc: 0.9770 - val_loss: 0.0824 - val_acc: 0.9680
Epoch 40/40
 - 4s - loss: 0.0396 - acc: 0.9767 - val_loss: 0.0827 - val_acc: 0.9679
Epoch 1/40
 - 0s - loss: 0.0769 - acc: 0.9673
Epoch 2/40
 - 0s - loss: 0.0632 - acc: 0.9701
Epoch 3/40
 - 0s - loss: 0.0574 - acc: 0.9713
Epoch 4/40
 - 0s - loss: 0.0543 - acc: 0.9730
Epoch 5/40
 - 0s - loss: 0.0520 - acc: 0.9726
Epoch 6/40
 - 0s - loss: 0.0501 - acc: 0.9753
Epoch 7/40
 - 0s - loss: 0.0486 - acc: 0.9746
Epoch 8/40
 - 0s - loss: 0.0475 - acc: 0.9739
Epoch 9/40
 - 0s - loss: 0.0464 - acc: 0.9749
Epoch 10/40
 - 0s - loss: 0.0455 - acc: 0.9755
Epoch 11/40
 - 0s - loss: 0.0441 - acc: 0.9755
Epoch 12/40
 - 0s - loss: 0.0442 - acc: 0.9759
Epoch 13/40
 - 0s - loss: 0.0434 - acc: 0.9758
Epoch 14/40
 - 0s - loss: 0.0429 - acc: 0.9756
Epoch 15/40
 - 0s - loss: 0.0425 - acc: 0.9763
Epoch 16/40
 - 0s - loss: 0.0418 - acc: 0.9763
Epoch 17/40
 - 0s - loss: 0.0410 - acc: 0.9771
Epoch 18/40
 - 0s - loss: 0.0405 - acc: 0.9774
Epoch 19/40
 - 0s - loss: 0.0410 - acc: 0.9762
Epoch 20/40
 - 0s - loss: 0.0406 - acc: 0.9770
Epoch 21/40
 - 0s - loss: 0.0398 - acc: 0.9775
Epoch 22/40
 - 0s - loss: 0.0394 - acc: 0.9786
Epoch 23/40
 - 0s - loss: 0.0393 - acc: 0.9772
Epoch 24/40
 - 0s - loss: 0.0392 - acc: 0.9767
Epoch 25/40
 - 0s - loss: 0.0385 - acc: 0.9772
Epoch 26/40
 - 0s - loss: 0.0382 - acc: 0.9780
Epoch 27/40
 - 0s - loss: 0.0379 - acc: 0.9771
Epoch 28/40
 - 0s - loss: 0.0386 - acc: 0.9770
Epoch 29/40
 - 0s - loss: 0.0381 - acc: 0.9768
Epoch 30/40
 - 0s - loss: 0.0372 - acc: 0.9783
Epoch 31/40
 - 0s - loss: 0.0380 - acc: 0.9771
Epoch 32/40
 - 0s - loss: 0.0371 - acc: 0.9781
Epoch 33/40
 - 0s - loss: 0.0370 - acc: 0.9779
Epoch 34/40
 - 0s - loss: 0.0365 - acc: 0.9788
Epoch 35/40
 - 0s - loss: 0.0371 - acc: 0.9781
Epoch 36/40
 - 0s - loss: 0.0368 - acc: 0.9767
Epoch 37/40
 - 0s - loss: 0.0366 - acc: 0.9780
Epoch 38/40
 - 0s - loss: 0.0363 - acc: 0.9788
Epoch 39/40
 - 0s - loss: 0.0364 - acc: 0.9786
Epoch 40/40
 - 0s - loss: 0.0366 - acc: 0.9781
# Training time = 0:03:31.421457
# F-Score(Ordinary) = 0.378, Recall: 0.793, Precision: 0.248
# F-Score(lvc) = 0.304, Recall: 0.538, Precision: 0.212
# F-Score(ireflv) = 0.442, Recall: 0.878, Precision: 0.295
# F-Score(id) = 0.375, Recall: 0.957, Precision: 0.233
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_167 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_168 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_167 (Embedding)       (None, 4, 48)        705264      input_167[0][0]                  
__________________________________________________________________________________________________
embedding_168 (Embedding)       (None, 4, 24)        5640        input_168[0][0]                  
__________________________________________________________________________________________________
flatten_167 (Flatten)           (None, 192)          0           embedding_167[0][0]              
__________________________________________________________________________________________________
flatten_168 (Flatten)           (None, 96)           0           embedding_168[0][0]              
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 288)          0           flatten_167[0][0]                
                                                                 flatten_168[0][0]                
__________________________________________________________________________________________________
dense_167 (Dense)               (None, 24)           6936        concatenate_84[0][0]             
__________________________________________________________________________________________________
dropout_84 (Dropout)            (None, 24)           0           dense_167[0][0]                  
__________________________________________________________________________________________________
dense_168 (Dense)               (None, 8)            200         dropout_84[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1257 - acc: 0.9581 - val_loss: 0.0874 - val_acc: 0.9665
Epoch 2/40
 - 4s - loss: 0.0797 - acc: 0.9680 - val_loss: 0.0788 - val_acc: 0.9671
Epoch 3/40
 - 4s - loss: 0.0690 - acc: 0.9700 - val_loss: 0.0750 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0634 - acc: 0.9712 - val_loss: 0.0740 - val_acc: 0.9679
Epoch 5/40
 - 4s - loss: 0.0591 - acc: 0.9725 - val_loss: 0.0732 - val_acc: 0.9679
Epoch 6/40
 - 4s - loss: 0.0564 - acc: 0.9731 - val_loss: 0.0728 - val_acc: 0.9678
Epoch 7/40
 - 4s - loss: 0.0543 - acc: 0.9736 - val_loss: 0.0729 - val_acc: 0.9678
Epoch 8/40
 - 4s - loss: 0.0520 - acc: 0.9739 - val_loss: 0.0733 - val_acc: 0.9680
Epoch 9/40
 - 4s - loss: 0.0505 - acc: 0.9747 - val_loss: 0.0732 - val_acc: 0.9683
Epoch 10/40
 - 4s - loss: 0.0495 - acc: 0.9746 - val_loss: 0.0734 - val_acc: 0.9680
Epoch 11/40
 - 4s - loss: 0.0484 - acc: 0.9754 - val_loss: 0.0738 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0478 - acc: 0.9751 - val_loss: 0.0737 - val_acc: 0.9680
Epoch 13/40
 - 4s - loss: 0.0466 - acc: 0.9757 - val_loss: 0.0741 - val_acc: 0.9677
Epoch 14/40
 - 4s - loss: 0.0463 - acc: 0.9756 - val_loss: 0.0745 - val_acc: 0.9681
Epoch 15/40
 - 4s - loss: 0.0456 - acc: 0.9758 - val_loss: 0.0752 - val_acc: 0.9679
Epoch 16/40
 - 4s - loss: 0.0451 - acc: 0.9758 - val_loss: 0.0751 - val_acc: 0.9679
Epoch 17/40
 - 4s - loss: 0.0445 - acc: 0.9763 - val_loss: 0.0759 - val_acc: 0.9673
Epoch 18/40
 - 4s - loss: 0.0440 - acc: 0.9761 - val_loss: 0.0766 - val_acc: 0.9678
Epoch 19/40
 - 4s - loss: 0.0439 - acc: 0.9760 - val_loss: 0.0768 - val_acc: 0.9677
Epoch 20/40
 - 4s - loss: 0.0435 - acc: 0.9760 - val_loss: 0.0769 - val_acc: 0.9673
Epoch 21/40
 - 4s - loss: 0.0432 - acc: 0.9766 - val_loss: 0.0777 - val_acc: 0.9673
Epoch 22/40
 - 4s - loss: 0.0430 - acc: 0.9762 - val_loss: 0.0777 - val_acc: 0.9675
Epoch 23/40
 - 4s - loss: 0.0426 - acc: 0.9764 - val_loss: 0.0778 - val_acc: 0.9675
Epoch 24/40
 - 4s - loss: 0.0428 - acc: 0.9760 - val_loss: 0.0780 - val_acc: 0.9673
Epoch 25/40
 - 4s - loss: 0.0419 - acc: 0.9768 - val_loss: 0.0788 - val_acc: 0.9672
Epoch 26/40
 - 4s - loss: 0.0418 - acc: 0.9764 - val_loss: 0.0786 - val_acc: 0.9676
Epoch 27/40
 - 4s - loss: 0.0415 - acc: 0.9768 - val_loss: 0.0790 - val_acc: 0.9672
Epoch 28/40
 - 4s - loss: 0.0415 - acc: 0.9766 - val_loss: 0.0802 - val_acc: 0.9671
Epoch 29/40
 - 4s - loss: 0.0414 - acc: 0.9769 - val_loss: 0.0795 - val_acc: 0.9675
Epoch 30/40
 - 4s - loss: 0.0412 - acc: 0.9768 - val_loss: 0.0796 - val_acc: 0.9674
Epoch 31/40
 - 4s - loss: 0.0408 - acc: 0.9770 - val_loss: 0.0797 - val_acc: 0.9676
Epoch 32/40
 - 4s - loss: 0.0407 - acc: 0.9771 - val_loss: 0.0803 - val_acc: 0.9675
Epoch 33/40
 - 4s - loss: 0.0405 - acc: 0.9767 - val_loss: 0.0810 - val_acc: 0.9675
Epoch 34/40
 - 4s - loss: 0.0405 - acc: 0.9767 - val_loss: 0.0809 - val_acc: 0.9674
Epoch 35/40
 - 4s - loss: 0.0402 - acc: 0.9768 - val_loss: 0.0810 - val_acc: 0.9674
Epoch 36/40
 - 4s - loss: 0.0399 - acc: 0.9768 - val_loss: 0.0813 - val_acc: 0.9676
Epoch 37/40
 - 4s - loss: 0.0400 - acc: 0.9771 - val_loss: 0.0815 - val_acc: 0.9676
Epoch 38/40
 - 4s - loss: 0.0400 - acc: 0.9769 - val_loss: 0.0814 - val_acc: 0.9676
Epoch 39/40
 - 4s - loss: 0.0397 - acc: 0.9775 - val_loss: 0.0818 - val_acc: 0.9674
Epoch 40/40
 - 4s - loss: 0.0397 - acc: 0.9768 - val_loss: 0.0819 - val_acc: 0.9674
Epoch 1/40
 - 0s - loss: 0.0770 - acc: 0.9679
Epoch 2/40
 - 0s - loss: 0.0629 - acc: 0.9695
Epoch 3/40
 - 0s - loss: 0.0569 - acc: 0.9718
Epoch 4/40
 - 0s - loss: 0.0550 - acc: 0.9724
Epoch 5/40
 - 0s - loss: 0.0516 - acc: 0.9737
Epoch 6/40
 - 0s - loss: 0.0506 - acc: 0.9732
Epoch 7/40
 - 0s - loss: 0.0491 - acc: 0.9747
Epoch 8/40
 - 0s - loss: 0.0484 - acc: 0.9728
Epoch 9/40
 - 0s - loss: 0.0469 - acc: 0.9753
Epoch 10/40
 - 0s - loss: 0.0460 - acc: 0.9754
Epoch 11/40
 - 0s - loss: 0.0447 - acc: 0.9751
Epoch 12/40
 - 0s - loss: 0.0445 - acc: 0.9763
Epoch 13/40
 - 0s - loss: 0.0440 - acc: 0.9763
Epoch 14/40
 - 0s - loss: 0.0432 - acc: 0.9769
Epoch 15/40
 - 0s - loss: 0.0426 - acc: 0.9761
Epoch 16/40
 - 0s - loss: 0.0418 - acc: 0.9770
Epoch 17/40
 - 0s - loss: 0.0420 - acc: 0.9758
Epoch 18/40
 - 0s - loss: 0.0411 - acc: 0.9778
Epoch 19/40
 - 0s - loss: 0.0409 - acc: 0.9764
Epoch 20/40
 - 0s - loss: 0.0405 - acc: 0.9767
Epoch 21/40
 - 0s - loss: 0.0404 - acc: 0.9763
Epoch 22/40
 - 0s - loss: 0.0395 - acc: 0.9777
Epoch 23/40
 - 0s - loss: 0.0396 - acc: 0.9764
Epoch 24/40
 - 0s - loss: 0.0394 - acc: 0.9774
Epoch 25/40
 - 0s - loss: 0.0385 - acc: 0.9780
Epoch 26/40
 - 0s - loss: 0.0390 - acc: 0.9767
Epoch 27/40
 - 0s - loss: 0.0383 - acc: 0.9774
Epoch 28/40
 - 0s - loss: 0.0382 - acc: 0.9774
Epoch 29/40
 - 0s - loss: 0.0379 - acc: 0.9777
Epoch 30/40
 - 0s - loss: 0.0382 - acc: 0.9768
Epoch 31/40
 - 0s - loss: 0.0376 - acc: 0.9783
Epoch 32/40
 - 0s - loss: 0.0368 - acc: 0.9784
Epoch 33/40
 - 0s - loss: 0.0374 - acc: 0.9778
Epoch 34/40
 - 0s - loss: 0.0372 - acc: 0.9785
Epoch 35/40
 - 0s - loss: 0.0374 - acc: 0.9781
Epoch 36/40
 - 0s - loss: 0.0365 - acc: 0.9788
Epoch 37/40
 - 0s - loss: 0.0368 - acc: 0.9776
Epoch 38/40
 - 0s - loss: 0.0363 - acc: 0.9780
Epoch 39/40
 - 0s - loss: 0.0366 - acc: 0.9791
Epoch 40/40
 - 0s - loss: 0.0370 - acc: 0.9773
# Training time = 0:03:30.037113
# F-Score(Ordinary) = 0.444, Recall: 0.898, Precision: 0.295
# F-Score(lvc) = 0.425, Recall: 0.809, Precision: 0.288
# F-Score(ireflv) = 0.491, Recall: 0.911, Precision: 0.336
# F-Score(id) = 0.427, Recall: 0.964, Precision: 0.275
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_169 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_170 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_169 (Embedding)       (None, 4, 48)        705264      input_169[0][0]                  
__________________________________________________________________________________________________
embedding_170 (Embedding)       (None, 4, 24)        5640        input_170[0][0]                  
__________________________________________________________________________________________________
flatten_169 (Flatten)           (None, 192)          0           embedding_169[0][0]              
__________________________________________________________________________________________________
flatten_170 (Flatten)           (None, 96)           0           embedding_170[0][0]              
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 288)          0           flatten_169[0][0]                
                                                                 flatten_170[0][0]                
__________________________________________________________________________________________________
dense_169 (Dense)               (None, 24)           6936        concatenate_85[0][0]             
__________________________________________________________________________________________________
dropout_85 (Dropout)            (None, 24)           0           dense_169[0][0]                  
__________________________________________________________________________________________________
dense_170 (Dense)               (None, 8)            200         dropout_85[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1249 - acc: 0.9578 - val_loss: 0.0894 - val_acc: 0.9666
Epoch 2/40
 - 4s - loss: 0.0825 - acc: 0.9678 - val_loss: 0.0817 - val_acc: 0.9675
Epoch 3/40
 - 4s - loss: 0.0718 - acc: 0.9698 - val_loss: 0.0771 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0654 - acc: 0.9711 - val_loss: 0.0745 - val_acc: 0.9679
Epoch 5/40
 - 4s - loss: 0.0608 - acc: 0.9721 - val_loss: 0.0735 - val_acc: 0.9681
Epoch 6/40
 - 4s - loss: 0.0574 - acc: 0.9725 - val_loss: 0.0727 - val_acc: 0.9685
Epoch 7/40
 - 4s - loss: 0.0549 - acc: 0.9735 - val_loss: 0.0726 - val_acc: 0.9683
Epoch 8/40
 - 4s - loss: 0.0528 - acc: 0.9743 - val_loss: 0.0727 - val_acc: 0.9683
Epoch 9/40
 - 4s - loss: 0.0515 - acc: 0.9746 - val_loss: 0.0729 - val_acc: 0.9681
Epoch 10/40
 - 4s - loss: 0.0500 - acc: 0.9745 - val_loss: 0.0732 - val_acc: 0.9682
Epoch 11/40
 - 4s - loss: 0.0492 - acc: 0.9747 - val_loss: 0.0737 - val_acc: 0.9681
Epoch 12/40
 - 4s - loss: 0.0483 - acc: 0.9749 - val_loss: 0.0740 - val_acc: 0.9684
Epoch 13/40
 - 4s - loss: 0.0474 - acc: 0.9751 - val_loss: 0.0741 - val_acc: 0.9681
Epoch 14/40
 - 4s - loss: 0.0469 - acc: 0.9755 - val_loss: 0.0745 - val_acc: 0.9685
Epoch 15/40
 - 4s - loss: 0.0460 - acc: 0.9757 - val_loss: 0.0752 - val_acc: 0.9685
Epoch 16/40
 - 4s - loss: 0.0457 - acc: 0.9757 - val_loss: 0.0752 - val_acc: 0.9683
Epoch 17/40
 - 4s - loss: 0.0455 - acc: 0.9757 - val_loss: 0.0753 - val_acc: 0.9681
Epoch 18/40
 - 4s - loss: 0.0447 - acc: 0.9763 - val_loss: 0.0759 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0447 - acc: 0.9760 - val_loss: 0.0760 - val_acc: 0.9680
Epoch 20/40
 - 4s - loss: 0.0442 - acc: 0.9759 - val_loss: 0.0760 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0438 - acc: 0.9765 - val_loss: 0.0766 - val_acc: 0.9681
Epoch 22/40
 - 4s - loss: 0.0436 - acc: 0.9763 - val_loss: 0.0770 - val_acc: 0.9682
Epoch 23/40
 - 4s - loss: 0.0433 - acc: 0.9766 - val_loss: 0.0774 - val_acc: 0.9681
Epoch 24/40
 - 4s - loss: 0.0428 - acc: 0.9765 - val_loss: 0.0776 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0427 - acc: 0.9768 - val_loss: 0.0775 - val_acc: 0.9680
Epoch 26/40
 - 4s - loss: 0.0424 - acc: 0.9764 - val_loss: 0.0780 - val_acc: 0.9681
Epoch 27/40
 - 4s - loss: 0.0423 - acc: 0.9767 - val_loss: 0.0780 - val_acc: 0.9679
Epoch 28/40
 - 4s - loss: 0.0421 - acc: 0.9766 - val_loss: 0.0784 - val_acc: 0.9678
Epoch 29/40
 - 4s - loss: 0.0417 - acc: 0.9766 - val_loss: 0.0788 - val_acc: 0.9677
Epoch 30/40
 - 4s - loss: 0.0416 - acc: 0.9768 - val_loss: 0.0788 - val_acc: 0.9679
Epoch 31/40
 - 4s - loss: 0.0414 - acc: 0.9769 - val_loss: 0.0788 - val_acc: 0.9677
Epoch 32/40
 - 4s - loss: 0.0414 - acc: 0.9771 - val_loss: 0.0793 - val_acc: 0.9677
Epoch 33/40
 - 4s - loss: 0.0409 - acc: 0.9766 - val_loss: 0.0797 - val_acc: 0.9676
Epoch 34/40
 - 4s - loss: 0.0410 - acc: 0.9767 - val_loss: 0.0801 - val_acc: 0.9676
Epoch 35/40
 - 4s - loss: 0.0411 - acc: 0.9768 - val_loss: 0.0805 - val_acc: 0.9675
Epoch 36/40
 - 4s - loss: 0.0411 - acc: 0.9764 - val_loss: 0.0802 - val_acc: 0.9675
Epoch 37/40
 - 4s - loss: 0.0407 - acc: 0.9770 - val_loss: 0.0806 - val_acc: 0.9677
Epoch 38/40
 - 4s - loss: 0.0408 - acc: 0.9768 - val_loss: 0.0808 - val_acc: 0.9675
Epoch 39/40
 - 4s - loss: 0.0405 - acc: 0.9770 - val_loss: 0.0807 - val_acc: 0.9677
Epoch 40/40
 - 4s - loss: 0.0405 - acc: 0.9768 - val_loss: 0.0814 - val_acc: 0.9675
Epoch 1/40
 - 0s - loss: 0.0793 - acc: 0.9652
Epoch 2/40
 - 0s - loss: 0.0641 - acc: 0.9690
Epoch 3/40
 - 0s - loss: 0.0584 - acc: 0.9715
Epoch 4/40
 - 0s - loss: 0.0559 - acc: 0.9725
Epoch 5/40
 - 0s - loss: 0.0530 - acc: 0.9734
Epoch 6/40
 - 0s - loss: 0.0512 - acc: 0.9737
Epoch 7/40
 - 0s - loss: 0.0497 - acc: 0.9745
Epoch 8/40
 - 0s - loss: 0.0484 - acc: 0.9754
Epoch 9/40
 - 0s - loss: 0.0475 - acc: 0.9738
Epoch 10/40
 - 0s - loss: 0.0463 - acc: 0.9763
Epoch 11/40
 - 0s - loss: 0.0464 - acc: 0.9764
Epoch 12/40
 - 0s - loss: 0.0455 - acc: 0.9753
Epoch 13/40
 - 0s - loss: 0.0450 - acc: 0.9753
Epoch 14/40
 - 0s - loss: 0.0444 - acc: 0.9756
Epoch 15/40
 - 0s - loss: 0.0434 - acc: 0.9768
Epoch 16/40
 - 0s - loss: 0.0431 - acc: 0.9763
Epoch 17/40
 - 0s - loss: 0.0431 - acc: 0.9764
Epoch 18/40
 - 0s - loss: 0.0420 - acc: 0.9770
Epoch 19/40
 - 0s - loss: 0.0428 - acc: 0.9761
Epoch 20/40
 - 0s - loss: 0.0414 - acc: 0.9764
Epoch 21/40
 - 0s - loss: 0.0415 - acc: 0.9770
Epoch 22/40
 - 0s - loss: 0.0406 - acc: 0.9761
Epoch 23/40
 - 0s - loss: 0.0409 - acc: 0.9774
Epoch 24/40
 - 0s - loss: 0.0410 - acc: 0.9766
Epoch 25/40
 - 0s - loss: 0.0403 - acc: 0.9776
Epoch 26/40
 - 0s - loss: 0.0399 - acc: 0.9768
Epoch 27/40
 - 0s - loss: 0.0400 - acc: 0.9779
Epoch 28/40
 - 0s - loss: 0.0391 - acc: 0.9773
Epoch 29/40
 - 0s - loss: 0.0393 - acc: 0.9761
Epoch 30/40
 - 0s - loss: 0.0389 - acc: 0.9779
Epoch 31/40
 - 0s - loss: 0.0386 - acc: 0.9779
Epoch 32/40
 - 0s - loss: 0.0382 - acc: 0.9780
Epoch 33/40
 - 0s - loss: 0.0385 - acc: 0.9767
Epoch 34/40
 - 0s - loss: 0.0378 - acc: 0.9777
Epoch 35/40
 - 0s - loss: 0.0373 - acc: 0.9791
Epoch 36/40
 - 0s - loss: 0.0372 - acc: 0.9782
Epoch 37/40
 - 0s - loss: 0.0371 - acc: 0.9787
Epoch 38/40
 - 0s - loss: 0.0378 - acc: 0.9778
Epoch 39/40
 - 0s - loss: 0.0374 - acc: 0.9777
Epoch 40/40
 - 0s - loss: 0.0378 - acc: 0.9765
# Training time = 0:03:27.409589
# F-Score(Ordinary) = 0.381, Recall: 0.816, Precision: 0.248
# F-Score(lvc) = 0.267, Recall: 0.667, Precision: 0.167
# F-Score(ireflv) = 0.486, Recall: 0.746, Precision: 0.361
# F-Score(id) = 0.363, Recall: 0.977, Precision: 0.223
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_171 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_172 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_171 (Embedding)       (None, 4, 48)        705264      input_171[0][0]                  
__________________________________________________________________________________________________
embedding_172 (Embedding)       (None, 4, 24)        5640        input_172[0][0]                  
__________________________________________________________________________________________________
flatten_171 (Flatten)           (None, 192)          0           embedding_171[0][0]              
__________________________________________________________________________________________________
flatten_172 (Flatten)           (None, 96)           0           embedding_172[0][0]              
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 288)          0           flatten_171[0][0]                
                                                                 flatten_172[0][0]                
__________________________________________________________________________________________________
dense_171 (Dense)               (None, 24)           6936        concatenate_86[0][0]             
__________________________________________________________________________________________________
dropout_86 (Dropout)            (None, 24)           0           dense_171[0][0]                  
__________________________________________________________________________________________________
dense_172 (Dense)               (None, 8)            200         dropout_86[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0961 - acc: 0.9635 - val_loss: 0.0762 - val_acc: 0.9674
Epoch 2/40
 - 4s - loss: 0.0627 - acc: 0.9706 - val_loss: 0.0726 - val_acc: 0.9678
Epoch 3/40
 - 4s - loss: 0.0539 - acc: 0.9731 - val_loss: 0.0727 - val_acc: 0.9678
Epoch 4/40
 - 4s - loss: 0.0501 - acc: 0.9740 - val_loss: 0.0743 - val_acc: 0.9677
Epoch 5/40
 - 4s - loss: 0.0474 - acc: 0.9750 - val_loss: 0.0749 - val_acc: 0.9680
Epoch 6/40
 - 4s - loss: 0.0459 - acc: 0.9749 - val_loss: 0.0760 - val_acc: 0.9683
Epoch 7/40
 - 4s - loss: 0.0446 - acc: 0.9755 - val_loss: 0.0766 - val_acc: 0.9680
Epoch 8/40
 - 4s - loss: 0.0435 - acc: 0.9760 - val_loss: 0.0780 - val_acc: 0.9681
Epoch 9/40
 - 4s - loss: 0.0428 - acc: 0.9761 - val_loss: 0.0790 - val_acc: 0.9679
Epoch 10/40
 - 4s - loss: 0.0421 - acc: 0.9764 - val_loss: 0.0804 - val_acc: 0.9680
Epoch 11/40
 - 4s - loss: 0.0414 - acc: 0.9761 - val_loss: 0.0811 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0410 - acc: 0.9765 - val_loss: 0.0816 - val_acc: 0.9680
Epoch 13/40
 - 4s - loss: 0.0407 - acc: 0.9764 - val_loss: 0.0819 - val_acc: 0.9676
Epoch 14/40
 - 4s - loss: 0.0402 - acc: 0.9772 - val_loss: 0.0835 - val_acc: 0.9680
Epoch 15/40
 - 4s - loss: 0.0400 - acc: 0.9767 - val_loss: 0.0828 - val_acc: 0.9682
Epoch 16/40
 - 4s - loss: 0.0395 - acc: 0.9770 - val_loss: 0.0835 - val_acc: 0.9682
Epoch 17/40
 - 4s - loss: 0.0395 - acc: 0.9768 - val_loss: 0.0854 - val_acc: 0.9676
Epoch 18/40
 - 4s - loss: 0.0392 - acc: 0.9771 - val_loss: 0.0855 - val_acc: 0.9682
Epoch 19/40
 - 4s - loss: 0.0391 - acc: 0.9771 - val_loss: 0.0850 - val_acc: 0.9680
Epoch 20/40
 - 4s - loss: 0.0390 - acc: 0.9772 - val_loss: 0.0862 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0386 - acc: 0.9772 - val_loss: 0.0867 - val_acc: 0.9679
Epoch 22/40
 - 4s - loss: 0.0385 - acc: 0.9774 - val_loss: 0.0864 - val_acc: 0.9678
Epoch 23/40
 - 4s - loss: 0.0384 - acc: 0.9771 - val_loss: 0.0876 - val_acc: 0.9680
Epoch 24/40
 - 4s - loss: 0.0382 - acc: 0.9771 - val_loss: 0.0878 - val_acc: 0.9680
Epoch 25/40
 - 4s - loss: 0.0378 - acc: 0.9776 - val_loss: 0.0891 - val_acc: 0.9681
Epoch 26/40
 - 4s - loss: 0.0378 - acc: 0.9770 - val_loss: 0.0893 - val_acc: 0.9679
Epoch 27/40
 - 4s - loss: 0.0376 - acc: 0.9776 - val_loss: 0.0899 - val_acc: 0.9680
Epoch 28/40
 - 4s - loss: 0.0375 - acc: 0.9774 - val_loss: 0.0899 - val_acc: 0.9680
Epoch 29/40
 - 4s - loss: 0.0374 - acc: 0.9774 - val_loss: 0.0892 - val_acc: 0.9680
Epoch 30/40
 - 4s - loss: 0.0372 - acc: 0.9779 - val_loss: 0.0901 - val_acc: 0.9681
Epoch 31/40
 - 4s - loss: 0.0371 - acc: 0.9772 - val_loss: 0.0900 - val_acc: 0.9681
Epoch 32/40
 - 4s - loss: 0.0370 - acc: 0.9774 - val_loss: 0.0906 - val_acc: 0.9681
Epoch 33/40
 - 4s - loss: 0.0370 - acc: 0.9777 - val_loss: 0.0910 - val_acc: 0.9683
Epoch 34/40
 - 4s - loss: 0.0368 - acc: 0.9775 - val_loss: 0.0915 - val_acc: 0.9682
Epoch 35/40
 - 4s - loss: 0.0366 - acc: 0.9779 - val_loss: 0.0908 - val_acc: 0.9680
Epoch 36/40
 - 4s - loss: 0.0366 - acc: 0.9779 - val_loss: 0.0924 - val_acc: 0.9679
Epoch 37/40
 - 4s - loss: 0.0366 - acc: 0.9777 - val_loss: 0.0930 - val_acc: 0.9681
Epoch 38/40
 - 4s - loss: 0.0365 - acc: 0.9777 - val_loss: 0.0925 - val_acc: 0.9682
Epoch 39/40
 - 4s - loss: 0.0363 - acc: 0.9778 - val_loss: 0.0936 - val_acc: 0.9679
Epoch 40/40
 - 4s - loss: 0.0363 - acc: 0.9775 - val_loss: 0.0942 - val_acc: 0.9679
Epoch 1/40
 - 0s - loss: 0.0804 - acc: 0.9675
Epoch 2/40
 - 0s - loss: 0.0576 - acc: 0.9721
Epoch 3/40
 - 0s - loss: 0.0533 - acc: 0.9735
Epoch 4/40
 - 0s - loss: 0.0485 - acc: 0.9745
Epoch 5/40
 - 0s - loss: 0.0472 - acc: 0.9746
Epoch 6/40
 - 0s - loss: 0.0453 - acc: 0.9754
Epoch 7/40
 - 0s - loss: 0.0442 - acc: 0.9759
Epoch 8/40
 - 0s - loss: 0.0428 - acc: 0.9759
Epoch 9/40
 - 0s - loss: 0.0419 - acc: 0.9754
Epoch 10/40
 - 0s - loss: 0.0413 - acc: 0.9759
Epoch 11/40
 - 0s - loss: 0.0409 - acc: 0.9775
Epoch 12/40
 - 0s - loss: 0.0395 - acc: 0.9776
Epoch 13/40
 - 0s - loss: 0.0391 - acc: 0.9778
Epoch 14/40
 - 0s - loss: 0.0388 - acc: 0.9774
Epoch 15/40
 - 0s - loss: 0.0380 - acc: 0.9776
Epoch 16/40
 - 0s - loss: 0.0376 - acc: 0.9781
Epoch 17/40
 - 0s - loss: 0.0367 - acc: 0.9773
Epoch 18/40
 - 0s - loss: 0.0362 - acc: 0.9778
Epoch 19/40
 - 0s - loss: 0.0364 - acc: 0.9776
Epoch 20/40
 - 0s - loss: 0.0359 - acc: 0.9781
Epoch 21/40
 - 0s - loss: 0.0356 - acc: 0.9780
Epoch 22/40
 - 0s - loss: 0.0349 - acc: 0.9782
Epoch 23/40
 - 0s - loss: 0.0347 - acc: 0.9777
Epoch 24/40
 - 0s - loss: 0.0347 - acc: 0.9788
Epoch 25/40
 - 0s - loss: 0.0342 - acc: 0.9786
Epoch 26/40
 - 0s - loss: 0.0343 - acc: 0.9787
Epoch 27/40
 - 0s - loss: 0.0346 - acc: 0.9785
Epoch 28/40
 - 0s - loss: 0.0341 - acc: 0.9787
Epoch 29/40
 - 0s - loss: 0.0340 - acc: 0.9794
Epoch 30/40
 - 0s - loss: 0.0339 - acc: 0.9782
Epoch 31/40
 - 0s - loss: 0.0330 - acc: 0.9804
Epoch 32/40
 - 0s - loss: 0.0336 - acc: 0.9787
Epoch 33/40
 - 0s - loss: 0.0337 - acc: 0.9782
Epoch 34/40
 - 0s - loss: 0.0333 - acc: 0.9784
Epoch 35/40
 - 0s - loss: 0.0331 - acc: 0.9782
Epoch 36/40
 - 0s - loss: 0.0331 - acc: 0.9790
Epoch 37/40
 - 0s - loss: 0.0334 - acc: 0.9790
Epoch 38/40
 - 0s - loss: 0.0331 - acc: 0.9780
Epoch 39/40
 - 0s - loss: 0.0330 - acc: 0.9775
Epoch 40/40
 - 0s - loss: 0.0327 - acc: 0.9791
# Training time = 0:03:29.840147
# F-Score(Ordinary) = 0.37, Recall: 0.748, Precision: 0.246
# F-Score(lvc) = 0.533, Recall: 0.825, Precision: 0.394
# F-Score(ireflv) = 0.201, Recall: 0.824, Precision: 0.115
# F-Score(id) = 0.338, Recall: 0.657, Precision: 0.228
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_173 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_174 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_173 (Embedding)       (None, 4, 48)        705264      input_173[0][0]                  
__________________________________________________________________________________________________
embedding_174 (Embedding)       (None, 4, 24)        5640        input_174[0][0]                  
__________________________________________________________________________________________________
flatten_173 (Flatten)           (None, 192)          0           embedding_173[0][0]              
__________________________________________________________________________________________________
flatten_174 (Flatten)           (None, 96)           0           embedding_174[0][0]              
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 288)          0           flatten_173[0][0]                
                                                                 flatten_174[0][0]                
__________________________________________________________________________________________________
dense_173 (Dense)               (None, 24)           6936        concatenate_87[0][0]             
__________________________________________________________________________________________________
dropout_87 (Dropout)            (None, 24)           0           dense_173[0][0]                  
__________________________________________________________________________________________________
dense_174 (Dense)               (None, 8)            200         dropout_87[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0988 - acc: 0.9632 - val_loss: 0.0750 - val_acc: 0.9686
Epoch 2/40
 - 4s - loss: 0.0635 - acc: 0.9706 - val_loss: 0.0714 - val_acc: 0.9681
Epoch 3/40
 - 4s - loss: 0.0550 - acc: 0.9726 - val_loss: 0.0707 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0508 - acc: 0.9741 - val_loss: 0.0717 - val_acc: 0.9683
Epoch 5/40
 - 4s - loss: 0.0481 - acc: 0.9751 - val_loss: 0.0731 - val_acc: 0.9685
Epoch 6/40
 - 4s - loss: 0.0459 - acc: 0.9752 - val_loss: 0.0746 - val_acc: 0.9683
Epoch 7/40
 - 4s - loss: 0.0448 - acc: 0.9756 - val_loss: 0.0747 - val_acc: 0.9679
Epoch 8/40
 - 4s - loss: 0.0436 - acc: 0.9759 - val_loss: 0.0761 - val_acc: 0.9682
Epoch 9/40
 - 4s - loss: 0.0430 - acc: 0.9758 - val_loss: 0.0767 - val_acc: 0.9679
Epoch 10/40
 - 4s - loss: 0.0424 - acc: 0.9764 - val_loss: 0.0782 - val_acc: 0.9679
Epoch 11/40
 - 4s - loss: 0.0417 - acc: 0.9765 - val_loss: 0.0795 - val_acc: 0.9680
Epoch 12/40
 - 4s - loss: 0.0414 - acc: 0.9764 - val_loss: 0.0799 - val_acc: 0.9680
Epoch 13/40
 - 4s - loss: 0.0411 - acc: 0.9769 - val_loss: 0.0796 - val_acc: 0.9679
Epoch 14/40
 - 4s - loss: 0.0408 - acc: 0.9770 - val_loss: 0.0815 - val_acc: 0.9680
Epoch 15/40
 - 4s - loss: 0.0403 - acc: 0.9772 - val_loss: 0.0809 - val_acc: 0.9681
Epoch 16/40
 - 4s - loss: 0.0401 - acc: 0.9764 - val_loss: 0.0822 - val_acc: 0.9680
Epoch 17/40
 - 4s - loss: 0.0398 - acc: 0.9766 - val_loss: 0.0817 - val_acc: 0.9680
Epoch 18/40
 - 4s - loss: 0.0395 - acc: 0.9768 - val_loss: 0.0828 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0392 - acc: 0.9770 - val_loss: 0.0833 - val_acc: 0.9680
Epoch 20/40
 - 4s - loss: 0.0391 - acc: 0.9767 - val_loss: 0.0833 - val_acc: 0.9681
Epoch 21/40
 - 4s - loss: 0.0388 - acc: 0.9773 - val_loss: 0.0833 - val_acc: 0.9681
Epoch 22/40
 - 4s - loss: 0.0387 - acc: 0.9772 - val_loss: 0.0847 - val_acc: 0.9679
Epoch 23/40
 - 4s - loss: 0.0385 - acc: 0.9775 - val_loss: 0.0845 - val_acc: 0.9679
Epoch 24/40
 - 4s - loss: 0.0383 - acc: 0.9776 - val_loss: 0.0857 - val_acc: 0.9680
Epoch 25/40
 - 4s - loss: 0.0382 - acc: 0.9773 - val_loss: 0.0865 - val_acc: 0.9677
Epoch 26/40
 - 4s - loss: 0.0378 - acc: 0.9774 - val_loss: 0.0866 - val_acc: 0.9679
Epoch 27/40
 - 4s - loss: 0.0379 - acc: 0.9772 - val_loss: 0.0861 - val_acc: 0.9677
Epoch 28/40
 - 4s - loss: 0.0377 - acc: 0.9773 - val_loss: 0.0870 - val_acc: 0.9678
Epoch 29/40
 - 4s - loss: 0.0374 - acc: 0.9777 - val_loss: 0.0885 - val_acc: 0.9678
Epoch 30/40
 - 4s - loss: 0.0372 - acc: 0.9776 - val_loss: 0.0883 - val_acc: 0.9679
Epoch 31/40
 - 4s - loss: 0.0372 - acc: 0.9777 - val_loss: 0.0879 - val_acc: 0.9679
Epoch 32/40
 - 4s - loss: 0.0371 - acc: 0.9778 - val_loss: 0.0888 - val_acc: 0.9676
Epoch 33/40
 - 4s - loss: 0.0372 - acc: 0.9778 - val_loss: 0.0894 - val_acc: 0.9679
Epoch 34/40
 - 4s - loss: 0.0369 - acc: 0.9773 - val_loss: 0.0881 - val_acc: 0.9679
Epoch 35/40
 - 4s - loss: 0.0367 - acc: 0.9778 - val_loss: 0.0882 - val_acc: 0.9677
Epoch 36/40
 - 4s - loss: 0.0366 - acc: 0.9780 - val_loss: 0.0902 - val_acc: 0.9679
Epoch 37/40
 - 4s - loss: 0.0365 - acc: 0.9779 - val_loss: 0.0908 - val_acc: 0.9678
Epoch 38/40
 - 4s - loss: 0.0363 - acc: 0.9778 - val_loss: 0.0917 - val_acc: 0.9679
Epoch 39/40
 - 4s - loss: 0.0363 - acc: 0.9777 - val_loss: 0.0914 - val_acc: 0.9680
Epoch 40/40
 - 4s - loss: 0.0362 - acc: 0.9780 - val_loss: 0.0912 - val_acc: 0.9680
Epoch 1/40
 - 0s - loss: 0.0781 - acc: 0.9668
Epoch 2/40
 - 0s - loss: 0.0571 - acc: 0.9718
Epoch 3/40
 - 0s - loss: 0.0515 - acc: 0.9730
Epoch 4/40
 - 0s - loss: 0.0479 - acc: 0.9733
Epoch 5/40
 - 0s - loss: 0.0462 - acc: 0.9763
Epoch 6/40
 - 0s - loss: 0.0443 - acc: 0.9753
Epoch 7/40
 - 0s - loss: 0.0430 - acc: 0.9752
Epoch 8/40
 - 0s - loss: 0.0420 - acc: 0.9763
Epoch 9/40
 - 0s - loss: 0.0410 - acc: 0.9764
Epoch 10/40
 - 0s - loss: 0.0406 - acc: 0.9770
Epoch 11/40
 - 0s - loss: 0.0399 - acc: 0.9771
Epoch 12/40
 - 0s - loss: 0.0386 - acc: 0.9775
Epoch 13/40
 - 0s - loss: 0.0377 - acc: 0.9771
Epoch 14/40
 - 0s - loss: 0.0374 - acc: 0.9775
Epoch 15/40
 - 0s - loss: 0.0368 - acc: 0.9766
Epoch 16/40
 - 0s - loss: 0.0364 - acc: 0.9789
Epoch 17/40
 - 0s - loss: 0.0362 - acc: 0.9782
Epoch 18/40
 - 0s - loss: 0.0355 - acc: 0.9785
Epoch 19/40
 - 0s - loss: 0.0353 - acc: 0.9784
Epoch 20/40
 - 0s - loss: 0.0348 - acc: 0.9789
Epoch 21/40
 - 0s - loss: 0.0352 - acc: 0.9782
Epoch 22/40
 - 0s - loss: 0.0347 - acc: 0.9775
Epoch 23/40
 - 0s - loss: 0.0346 - acc: 0.9784
Epoch 24/40
 - 0s - loss: 0.0344 - acc: 0.9784
Epoch 25/40
 - 0s - loss: 0.0342 - acc: 0.9792
Epoch 26/40
 - 0s - loss: 0.0340 - acc: 0.9786
Epoch 27/40
 - 0s - loss: 0.0336 - acc: 0.9787
Epoch 28/40
 - 0s - loss: 0.0336 - acc: 0.9792
Epoch 29/40
 - 0s - loss: 0.0335 - acc: 0.9801
Epoch 30/40
 - 0s - loss: 0.0335 - acc: 0.9783
Epoch 31/40
 - 0s - loss: 0.0337 - acc: 0.9781
Epoch 32/40
 - 0s - loss: 0.0333 - acc: 0.9796
Epoch 33/40
 - 0s - loss: 0.0335 - acc: 0.9786
Epoch 34/40
 - 0s - loss: 0.0333 - acc: 0.9789
Epoch 35/40
 - 0s - loss: 0.0332 - acc: 0.9794
Epoch 36/40
 - 0s - loss: 0.0330 - acc: 0.9790
Epoch 37/40
 - 0s - loss: 0.0333 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0326 - acc: 0.9794
Epoch 39/40
 - 0s - loss: 0.0335 - acc: 0.9782
Epoch 40/40
 - 0s - loss: 0.0329 - acc: 0.9787
# Training time = 0:03:33.673147
# F-Score(Ordinary) = 0.43, Recall: 0.778, Precision: 0.298
# F-Score(lvc) = 0.259, Recall: 0.579, Precision: 0.167
# F-Score(ireflv) = 0.44, Recall: 0.667, Precision: 0.328
# F-Score(id) = 0.526, Recall: 0.959, Precision: 0.363
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_175 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_176 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_175 (Embedding)       (None, 4, 48)        705264      input_175[0][0]                  
__________________________________________________________________________________________________
embedding_176 (Embedding)       (None, 4, 24)        5640        input_176[0][0]                  
__________________________________________________________________________________________________
flatten_175 (Flatten)           (None, 192)          0           embedding_175[0][0]              
__________________________________________________________________________________________________
flatten_176 (Flatten)           (None, 96)           0           embedding_176[0][0]              
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 288)          0           flatten_175[0][0]                
                                                                 flatten_176[0][0]                
__________________________________________________________________________________________________
dense_175 (Dense)               (None, 24)           6936        concatenate_88[0][0]             
__________________________________________________________________________________________________
dropout_88 (Dropout)            (None, 24)           0           dense_175[0][0]                  
__________________________________________________________________________________________________
dense_176 (Dense)               (None, 8)            200         dropout_88[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0964 - acc: 0.9640 - val_loss: 0.0738 - val_acc: 0.9679
Epoch 2/40
 - 4s - loss: 0.0619 - acc: 0.9709 - val_loss: 0.0705 - val_acc: 0.9687
Epoch 3/40
 - 4s - loss: 0.0543 - acc: 0.9733 - val_loss: 0.0706 - val_acc: 0.9682
Epoch 4/40
 - 4s - loss: 0.0503 - acc: 0.9739 - val_loss: 0.0724 - val_acc: 0.9680
Epoch 5/40
 - 4s - loss: 0.0477 - acc: 0.9746 - val_loss: 0.0742 - val_acc: 0.9679
Epoch 6/40
 - 4s - loss: 0.0459 - acc: 0.9753 - val_loss: 0.0746 - val_acc: 0.9684
Epoch 7/40
 - 4s - loss: 0.0447 - acc: 0.9759 - val_loss: 0.0753 - val_acc: 0.9681
Epoch 8/40
 - 4s - loss: 0.0438 - acc: 0.9757 - val_loss: 0.0767 - val_acc: 0.9683
Epoch 9/40
 - 4s - loss: 0.0429 - acc: 0.9761 - val_loss: 0.0775 - val_acc: 0.9677
Epoch 10/40
 - 4s - loss: 0.0421 - acc: 0.9763 - val_loss: 0.0791 - val_acc: 0.9674
Epoch 11/40
 - 4s - loss: 0.0416 - acc: 0.9764 - val_loss: 0.0798 - val_acc: 0.9678
Epoch 12/40
 - 4s - loss: 0.0413 - acc: 0.9763 - val_loss: 0.0815 - val_acc: 0.9677
Epoch 13/40
 - 4s - loss: 0.0407 - acc: 0.9766 - val_loss: 0.0807 - val_acc: 0.9679
Epoch 14/40
 - 4s - loss: 0.0403 - acc: 0.9768 - val_loss: 0.0827 - val_acc: 0.9677
Epoch 15/40
 - 4s - loss: 0.0400 - acc: 0.9766 - val_loss: 0.0818 - val_acc: 0.9681
Epoch 16/40
 - 4s - loss: 0.0399 - acc: 0.9767 - val_loss: 0.0826 - val_acc: 0.9678
Epoch 17/40
 - 4s - loss: 0.0397 - acc: 0.9769 - val_loss: 0.0827 - val_acc: 0.9681
Epoch 18/40
 - 4s - loss: 0.0392 - acc: 0.9773 - val_loss: 0.0830 - val_acc: 0.9676
Epoch 19/40
 - 4s - loss: 0.0392 - acc: 0.9767 - val_loss: 0.0841 - val_acc: 0.9677
Epoch 20/40
 - 4s - loss: 0.0387 - acc: 0.9769 - val_loss: 0.0844 - val_acc: 0.9678
Epoch 21/40
 - 4s - loss: 0.0386 - acc: 0.9772 - val_loss: 0.0847 - val_acc: 0.9679
Epoch 22/40
 - 4s - loss: 0.0381 - acc: 0.9770 - val_loss: 0.0854 - val_acc: 0.9680
Epoch 23/40
 - 4s - loss: 0.0382 - acc: 0.9769 - val_loss: 0.0859 - val_acc: 0.9679
Epoch 24/40
 - 4s - loss: 0.0379 - acc: 0.9772 - val_loss: 0.0856 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0380 - acc: 0.9774 - val_loss: 0.0871 - val_acc: 0.9678
Epoch 26/40
 - 4s - loss: 0.0377 - acc: 0.9771 - val_loss: 0.0867 - val_acc: 0.9677
Epoch 27/40
 - 4s - loss: 0.0375 - acc: 0.9773 - val_loss: 0.0874 - val_acc: 0.9679
Epoch 28/40
 - 4s - loss: 0.0374 - acc: 0.9769 - val_loss: 0.0863 - val_acc: 0.9681
Epoch 29/40
 - 4s - loss: 0.0372 - acc: 0.9775 - val_loss: 0.0874 - val_acc: 0.9680
Epoch 30/40
 - 4s - loss: 0.0374 - acc: 0.9772 - val_loss: 0.0880 - val_acc: 0.9679
Epoch 31/40
 - 4s - loss: 0.0366 - acc: 0.9772 - val_loss: 0.0897 - val_acc: 0.9679
Epoch 32/40
 - 4s - loss: 0.0365 - acc: 0.9778 - val_loss: 0.0892 - val_acc: 0.9681
Epoch 33/40
 - 4s - loss: 0.0367 - acc: 0.9776 - val_loss: 0.0901 - val_acc: 0.9681
Epoch 34/40
 - 4s - loss: 0.0367 - acc: 0.9777 - val_loss: 0.0888 - val_acc: 0.9680
Epoch 35/40
 - 4s - loss: 0.0364 - acc: 0.9774 - val_loss: 0.0901 - val_acc: 0.9680
Epoch 36/40
 - 4s - loss: 0.0363 - acc: 0.9773 - val_loss: 0.0895 - val_acc: 0.9681
Epoch 37/40
 - 4s - loss: 0.0362 - acc: 0.9776 - val_loss: 0.0905 - val_acc: 0.9682
Epoch 38/40
 - 4s - loss: 0.0360 - acc: 0.9779 - val_loss: 0.0913 - val_acc: 0.9684
Epoch 39/40
 - 4s - loss: 0.0358 - acc: 0.9775 - val_loss: 0.0909 - val_acc: 0.9682
Epoch 40/40
 - 4s - loss: 0.0361 - acc: 0.9770 - val_loss: 0.0913 - val_acc: 0.9683
Epoch 1/40
 - 0s - loss: 0.0786 - acc: 0.9674
Epoch 2/40
 - 0s - loss: 0.0586 - acc: 0.9706
Epoch 3/40
 - 0s - loss: 0.0516 - acc: 0.9722
Epoch 4/40
 - 0s - loss: 0.0487 - acc: 0.9741
Epoch 5/40
 - 0s - loss: 0.0453 - acc: 0.9749
Epoch 6/40
 - 0s - loss: 0.0442 - acc: 0.9748
Epoch 7/40
 - 0s - loss: 0.0421 - acc: 0.9756
Epoch 8/40
 - 0s - loss: 0.0410 - acc: 0.9758
Epoch 9/40
 - 0s - loss: 0.0401 - acc: 0.9771
Epoch 10/40
 - 0s - loss: 0.0388 - acc: 0.9772
Epoch 11/40
 - 0s - loss: 0.0379 - acc: 0.9774
Epoch 12/40
 - 0s - loss: 0.0380 - acc: 0.9771
Epoch 13/40
 - 0s - loss: 0.0374 - acc: 0.9766
Epoch 14/40
 - 0s - loss: 0.0368 - acc: 0.9767
Epoch 15/40
 - 0s - loss: 0.0364 - acc: 0.9774
Epoch 16/40
 - 0s - loss: 0.0358 - acc: 0.9775
Epoch 17/40
 - 0s - loss: 0.0356 - acc: 0.9781
Epoch 18/40
 - 0s - loss: 0.0351 - acc: 0.9774
Epoch 19/40
 - 0s - loss: 0.0357 - acc: 0.9776
Epoch 20/40
 - 0s - loss: 0.0350 - acc: 0.9778
Epoch 21/40
 - 0s - loss: 0.0348 - acc: 0.9784
Epoch 22/40
 - 0s - loss: 0.0345 - acc: 0.9794
Epoch 23/40
 - 0s - loss: 0.0345 - acc: 0.9768
Epoch 24/40
 - 0s - loss: 0.0341 - acc: 0.9779
Epoch 25/40
 - 0s - loss: 0.0341 - acc: 0.9779
Epoch 26/40
 - 0s - loss: 0.0337 - acc: 0.9792
Epoch 27/40
 - 0s - loss: 0.0334 - acc: 0.9783
Epoch 28/40
 - 0s - loss: 0.0339 - acc: 0.9777
Epoch 29/40
 - 0s - loss: 0.0336 - acc: 0.9781
Epoch 30/40
 - 0s - loss: 0.0335 - acc: 0.9781
Epoch 31/40
 - 0s - loss: 0.0334 - acc: 0.9781
Epoch 32/40
 - 0s - loss: 0.0330 - acc: 0.9792
Epoch 33/40
 - 0s - loss: 0.0330 - acc: 0.9788
Epoch 34/40
 - 0s - loss: 0.0329 - acc: 0.9795
Epoch 35/40
 - 0s - loss: 0.0331 - acc: 0.9780
Epoch 36/40
 - 0s - loss: 0.0328 - acc: 0.9786
Epoch 37/40
 - 0s - loss: 0.0329 - acc: 0.9792
Epoch 38/40
 - 0s - loss: 0.0326 - acc: 0.9790
Epoch 39/40
 - 0s - loss: 0.0328 - acc: 0.9787
Epoch 40/40
 - 0s - loss: 0.0324 - acc: 0.9791
# Training time = 0:03:28.703614
# F-Score(Ordinary) = 0.442, Recall: 0.81, Precision: 0.304
# F-Score(lvc) = 0.256, Recall: 0.833, Precision: 0.152
# F-Score(ireflv) = 0.594, Recall: 0.7, Precision: 0.516
# F-Score(id) = 0.421, Recall: 0.963, Precision: 0.269
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_177 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_178 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_177 (Embedding)       (None, 4, 48)        705264      input_177[0][0]                  
__________________________________________________________________________________________________
embedding_178 (Embedding)       (None, 4, 24)        5640        input_178[0][0]                  
__________________________________________________________________________________________________
flatten_177 (Flatten)           (None, 192)          0           embedding_177[0][0]              
__________________________________________________________________________________________________
flatten_178 (Flatten)           (None, 96)           0           embedding_178[0][0]              
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 288)          0           flatten_177[0][0]                
                                                                 flatten_178[0][0]                
__________________________________________________________________________________________________
dense_177 (Dense)               (None, 24)           6936        concatenate_89[0][0]             
__________________________________________________________________________________________________
dropout_89 (Dropout)            (None, 24)           0           dense_177[0][0]                  
__________________________________________________________________________________________________
dense_178 (Dense)               (None, 8)            200         dropout_89[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0999 - acc: 0.9627 - val_loss: 0.0745 - val_acc: 0.9683
Epoch 2/40
 - 4s - loss: 0.0634 - acc: 0.9708 - val_loss: 0.0712 - val_acc: 0.9685
Epoch 3/40
 - 4s - loss: 0.0551 - acc: 0.9730 - val_loss: 0.0704 - val_acc: 0.9684
Epoch 4/40
 - 4s - loss: 0.0511 - acc: 0.9735 - val_loss: 0.0717 - val_acc: 0.9680
Epoch 5/40
 - 4s - loss: 0.0477 - acc: 0.9748 - val_loss: 0.0726 - val_acc: 0.9680
Epoch 6/40
 - 4s - loss: 0.0459 - acc: 0.9751 - val_loss: 0.0733 - val_acc: 0.9687
Epoch 7/40
 - 4s - loss: 0.0447 - acc: 0.9750 - val_loss: 0.0741 - val_acc: 0.9684
Epoch 8/40
 - 4s - loss: 0.0433 - acc: 0.9757 - val_loss: 0.0758 - val_acc: 0.9682
Epoch 9/40
 - 4s - loss: 0.0424 - acc: 0.9762 - val_loss: 0.0760 - val_acc: 0.9684
Epoch 10/40
 - 4s - loss: 0.0417 - acc: 0.9764 - val_loss: 0.0766 - val_acc: 0.9681
Epoch 11/40
 - 4s - loss: 0.0412 - acc: 0.9764 - val_loss: 0.0770 - val_acc: 0.9681
Epoch 12/40
 - 4s - loss: 0.0408 - acc: 0.9762 - val_loss: 0.0766 - val_acc: 0.9681
Epoch 13/40
 - 4s - loss: 0.0402 - acc: 0.9769 - val_loss: 0.0785 - val_acc: 0.9681
Epoch 14/40
 - 4s - loss: 0.0397 - acc: 0.9769 - val_loss: 0.0782 - val_acc: 0.9683
Epoch 15/40
 - 4s - loss: 0.0395 - acc: 0.9772 - val_loss: 0.0799 - val_acc: 0.9684
Epoch 16/40
 - 4s - loss: 0.0391 - acc: 0.9769 - val_loss: 0.0798 - val_acc: 0.9682
Epoch 17/40
 - 4s - loss: 0.0388 - acc: 0.9771 - val_loss: 0.0809 - val_acc: 0.9683
Epoch 18/40
 - 4s - loss: 0.0383 - acc: 0.9772 - val_loss: 0.0822 - val_acc: 0.9680
Epoch 19/40
 - 4s - loss: 0.0383 - acc: 0.9770 - val_loss: 0.0831 - val_acc: 0.9681
Epoch 20/40
 - 4s - loss: 0.0378 - acc: 0.9774 - val_loss: 0.0827 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0376 - acc: 0.9773 - val_loss: 0.0849 - val_acc: 0.9681
Epoch 22/40
 - 4s - loss: 0.0377 - acc: 0.9769 - val_loss: 0.0847 - val_acc: 0.9679
Epoch 23/40
 - 4s - loss: 0.0373 - acc: 0.9776 - val_loss: 0.0849 - val_acc: 0.9680
Epoch 24/40
 - 4s - loss: 0.0373 - acc: 0.9770 - val_loss: 0.0848 - val_acc: 0.9680
Epoch 25/40
 - 4s - loss: 0.0369 - acc: 0.9771 - val_loss: 0.0863 - val_acc: 0.9679
Epoch 26/40
 - 4s - loss: 0.0367 - acc: 0.9772 - val_loss: 0.0858 - val_acc: 0.9681
Epoch 27/40
 - 4s - loss: 0.0367 - acc: 0.9772 - val_loss: 0.0864 - val_acc: 0.9679
Epoch 28/40
 - 4s - loss: 0.0364 - acc: 0.9778 - val_loss: 0.0890 - val_acc: 0.9679
Epoch 29/40
 - 4s - loss: 0.0365 - acc: 0.9776 - val_loss: 0.0877 - val_acc: 0.9677
Epoch 30/40
 - 4s - loss: 0.0363 - acc: 0.9772 - val_loss: 0.0884 - val_acc: 0.9677
Epoch 31/40
 - 4s - loss: 0.0360 - acc: 0.9776 - val_loss: 0.0880 - val_acc: 0.9678
Epoch 32/40
 - 4s - loss: 0.0361 - acc: 0.9776 - val_loss: 0.0899 - val_acc: 0.9679
Epoch 33/40
 - 4s - loss: 0.0358 - acc: 0.9777 - val_loss: 0.0909 - val_acc: 0.9679
Epoch 34/40
 - 4s - loss: 0.0359 - acc: 0.9777 - val_loss: 0.0902 - val_acc: 0.9680
Epoch 35/40
 - 4s - loss: 0.0356 - acc: 0.9779 - val_loss: 0.0908 - val_acc: 0.9680
Epoch 36/40
 - 4s - loss: 0.0353 - acc: 0.9779 - val_loss: 0.0920 - val_acc: 0.9681
Epoch 37/40
 - 4s - loss: 0.0353 - acc: 0.9779 - val_loss: 0.0915 - val_acc: 0.9680
Epoch 38/40
 - 4s - loss: 0.0353 - acc: 0.9780 - val_loss: 0.0915 - val_acc: 0.9680
Epoch 39/40
 - 4s - loss: 0.0351 - acc: 0.9783 - val_loss: 0.0916 - val_acc: 0.9680
Epoch 40/40
 - 4s - loss: 0.0351 - acc: 0.9782 - val_loss: 0.0928 - val_acc: 0.9682
Epoch 1/40
 - 0s - loss: 0.0806 - acc: 0.9675
Epoch 2/40
 - 0s - loss: 0.0564 - acc: 0.9713
Epoch 3/40
 - 0s - loss: 0.0505 - acc: 0.9728
Epoch 4/40
 - 0s - loss: 0.0476 - acc: 0.9748
Epoch 5/40
 - 0s - loss: 0.0452 - acc: 0.9757
Epoch 6/40
 - 0s - loss: 0.0447 - acc: 0.9750
Epoch 7/40
 - 0s - loss: 0.0433 - acc: 0.9764
Epoch 8/40
 - 0s - loss: 0.0419 - acc: 0.9762
Epoch 9/40
 - 0s - loss: 0.0408 - acc: 0.9768
Epoch 10/40
 - 0s - loss: 0.0396 - acc: 0.9764
Epoch 11/40
 - 0s - loss: 0.0389 - acc: 0.9761
Epoch 12/40
 - 0s - loss: 0.0383 - acc: 0.9769
Epoch 13/40
 - 0s - loss: 0.0374 - acc: 0.9782
Epoch 14/40
 - 0s - loss: 0.0371 - acc: 0.9786
Epoch 15/40
 - 0s - loss: 0.0366 - acc: 0.9780
Epoch 16/40
 - 0s - loss: 0.0360 - acc: 0.9775
Epoch 17/40
 - 0s - loss: 0.0363 - acc: 0.9769
Epoch 18/40
 - 0s - loss: 0.0359 - acc: 0.9778
Epoch 19/40
 - 0s - loss: 0.0353 - acc: 0.9782
Epoch 20/40
 - 0s - loss: 0.0348 - acc: 0.9780
Epoch 21/40
 - 0s - loss: 0.0347 - acc: 0.9786
Epoch 22/40
 - 0s - loss: 0.0343 - acc: 0.9786
Epoch 23/40
 - 0s - loss: 0.0342 - acc: 0.9787
Epoch 24/40
 - 0s - loss: 0.0344 - acc: 0.9785
Epoch 25/40
 - 0s - loss: 0.0340 - acc: 0.9789
Epoch 26/40
 - 0s - loss: 0.0337 - acc: 0.9781
Epoch 27/40
 - 0s - loss: 0.0336 - acc: 0.9790
Epoch 28/40
 - 0s - loss: 0.0341 - acc: 0.9784
Epoch 29/40
 - 0s - loss: 0.0332 - acc: 0.9795
Epoch 30/40
 - 0s - loss: 0.0332 - acc: 0.9782
Epoch 31/40
 - 0s - loss: 0.0330 - acc: 0.9792
Epoch 32/40
 - 0s - loss: 0.0327 - acc: 0.9786
Epoch 33/40
 - 0s - loss: 0.0329 - acc: 0.9786
Epoch 34/40
 - 0s - loss: 0.0328 - acc: 0.9779
Epoch 35/40
 - 0s - loss: 0.0330 - acc: 0.9789
Epoch 36/40
 - 0s - loss: 0.0323 - acc: 0.9789
Epoch 37/40
 - 0s - loss: 0.0325 - acc: 0.9789
Epoch 38/40
 - 0s - loss: 0.0324 - acc: 0.9791
Epoch 39/40
 - 0s - loss: 0.0324 - acc: 0.9794
Epoch 40/40
 - 0s - loss: 0.0325 - acc: 0.9778
# Training time = 0:03:30.032506
# F-Score(Ordinary) = 0.371, Recall: 0.92, Precision: 0.233
# F-Score(lvc) = 0.412, Recall: 0.921, Precision: 0.265
# F-Score(ireflv) = 0.442, Recall: 0.878, Precision: 0.295
# F-Score(id) = 0.291, Recall: 0.971, Precision: 0.171
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_179 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_180 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_179 (Embedding)       (None, 4, 48)        705264      input_179[0][0]                  
__________________________________________________________________________________________________
embedding_180 (Embedding)       (None, 4, 24)        5640        input_180[0][0]                  
__________________________________________________________________________________________________
flatten_179 (Flatten)           (None, 192)          0           embedding_179[0][0]              
__________________________________________________________________________________________________
flatten_180 (Flatten)           (None, 96)           0           embedding_180[0][0]              
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 288)          0           flatten_179[0][0]                
                                                                 flatten_180[0][0]                
__________________________________________________________________________________________________
dense_179 (Dense)               (None, 24)           6936        concatenate_90[0][0]             
__________________________________________________________________________________________________
dropout_90 (Dropout)            (None, 24)           0           dense_179[0][0]                  
__________________________________________________________________________________________________
dense_180 (Dense)               (None, 8)            200         dropout_90[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.02
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1010 - acc: 0.9629 - val_loss: 0.0761 - val_acc: 0.9679
Epoch 2/40
 - 4s - loss: 0.0645 - acc: 0.9703 - val_loss: 0.0718 - val_acc: 0.9684
Epoch 3/40
 - 4s - loss: 0.0560 - acc: 0.9721 - val_loss: 0.0714 - val_acc: 0.9684
Epoch 4/40
 - 4s - loss: 0.0515 - acc: 0.9739 - val_loss: 0.0722 - val_acc: 0.9685
Epoch 5/40
 - 4s - loss: 0.0489 - acc: 0.9743 - val_loss: 0.0725 - val_acc: 0.9681
Epoch 6/40
 - 4s - loss: 0.0467 - acc: 0.9747 - val_loss: 0.0732 - val_acc: 0.9680
Epoch 7/40
 - 4s - loss: 0.0456 - acc: 0.9747 - val_loss: 0.0749 - val_acc: 0.9679
Epoch 8/40
 - 4s - loss: 0.0444 - acc: 0.9758 - val_loss: 0.0752 - val_acc: 0.9675
Epoch 9/40
 - 4s - loss: 0.0433 - acc: 0.9762 - val_loss: 0.0764 - val_acc: 0.9677
Epoch 10/40
 - 4s - loss: 0.0425 - acc: 0.9763 - val_loss: 0.0763 - val_acc: 0.9680
Epoch 11/40
 - 4s - loss: 0.0421 - acc: 0.9761 - val_loss: 0.0779 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0415 - acc: 0.9766 - val_loss: 0.0779 - val_acc: 0.9685
Epoch 13/40
 - 4s - loss: 0.0409 - acc: 0.9768 - val_loss: 0.0788 - val_acc: 0.9680
Epoch 14/40
 - 4s - loss: 0.0408 - acc: 0.9767 - val_loss: 0.0794 - val_acc: 0.9680
Epoch 15/40
 - 4s - loss: 0.0403 - acc: 0.9768 - val_loss: 0.0799 - val_acc: 0.9685
Epoch 16/40
 - 4s - loss: 0.0400 - acc: 0.9769 - val_loss: 0.0806 - val_acc: 0.9683
Epoch 17/40
 - 4s - loss: 0.0397 - acc: 0.9769 - val_loss: 0.0809 - val_acc: 0.9684
Epoch 18/40
 - 4s - loss: 0.0392 - acc: 0.9772 - val_loss: 0.0827 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0392 - acc: 0.9769 - val_loss: 0.0823 - val_acc: 0.9679
Epoch 20/40
 - 4s - loss: 0.0390 - acc: 0.9767 - val_loss: 0.0828 - val_acc: 0.9679
Epoch 21/40
 - 4s - loss: 0.0387 - acc: 0.9773 - val_loss: 0.0837 - val_acc: 0.9681
Epoch 22/40
 - 4s - loss: 0.0386 - acc: 0.9769 - val_loss: 0.0834 - val_acc: 0.9682
Epoch 23/40
 - 4s - loss: 0.0383 - acc: 0.9776 - val_loss: 0.0850 - val_acc: 0.9683
Epoch 24/40
 - 4s - loss: 0.0380 - acc: 0.9772 - val_loss: 0.0858 - val_acc: 0.9679
Epoch 25/40
 - 4s - loss: 0.0379 - acc: 0.9774 - val_loss: 0.0858 - val_acc: 0.9683
Epoch 26/40
 - 4s - loss: 0.0376 - acc: 0.9776 - val_loss: 0.0869 - val_acc: 0.9683
Epoch 27/40
 - 4s - loss: 0.0375 - acc: 0.9773 - val_loss: 0.0862 - val_acc: 0.9681
Epoch 28/40
 - 4s - loss: 0.0373 - acc: 0.9774 - val_loss: 0.0874 - val_acc: 0.9681
Epoch 29/40
 - 4s - loss: 0.0372 - acc: 0.9777 - val_loss: 0.0881 - val_acc: 0.9681
Epoch 30/40
 - 4s - loss: 0.0371 - acc: 0.9778 - val_loss: 0.0880 - val_acc: 0.9681
Epoch 31/40
 - 4s - loss: 0.0370 - acc: 0.9775 - val_loss: 0.0886 - val_acc: 0.9680
Epoch 32/40
 - 4s - loss: 0.0370 - acc: 0.9775 - val_loss: 0.0892 - val_acc: 0.9679
Epoch 33/40
 - 4s - loss: 0.0368 - acc: 0.9774 - val_loss: 0.0896 - val_acc: 0.9679
Epoch 34/40
 - 4s - loss: 0.0367 - acc: 0.9777 - val_loss: 0.0902 - val_acc: 0.9680
Epoch 35/40
 - 4s - loss: 0.0366 - acc: 0.9775 - val_loss: 0.0908 - val_acc: 0.9680
Epoch 36/40
 - 4s - loss: 0.0366 - acc: 0.9773 - val_loss: 0.0907 - val_acc: 0.9679
Epoch 37/40
 - 4s - loss: 0.0363 - acc: 0.9777 - val_loss: 0.0917 - val_acc: 0.9679
Epoch 38/40
 - 4s - loss: 0.0364 - acc: 0.9774 - val_loss: 0.0913 - val_acc: 0.9677
Epoch 39/40
 - 4s - loss: 0.0362 - acc: 0.9773 - val_loss: 0.0915 - val_acc: 0.9677
Epoch 40/40
 - 4s - loss: 0.0363 - acc: 0.9778 - val_loss: 0.0921 - val_acc: 0.9681
Epoch 1/40
 - 0s - loss: 0.0804 - acc: 0.9662
Epoch 2/40
 - 0s - loss: 0.0583 - acc: 0.9712
Epoch 3/40
 - 0s - loss: 0.0516 - acc: 0.9727
Epoch 4/40
 - 0s - loss: 0.0492 - acc: 0.9730
Epoch 5/40
 - 0s - loss: 0.0462 - acc: 0.9751
Epoch 6/40
 - 0s - loss: 0.0449 - acc: 0.9744
Epoch 7/40
 - 0s - loss: 0.0431 - acc: 0.9753
Epoch 8/40
 - 0s - loss: 0.0416 - acc: 0.9758
Epoch 9/40
 - 0s - loss: 0.0409 - acc: 0.9766
Epoch 10/40
 - 0s - loss: 0.0402 - acc: 0.9752
Epoch 11/40
 - 0s - loss: 0.0404 - acc: 0.9758
Epoch 12/40
 - 0s - loss: 0.0388 - acc: 0.9759
Epoch 13/40
 - 0s - loss: 0.0385 - acc: 0.9772
Epoch 14/40
 - 0s - loss: 0.0381 - acc: 0.9768
Epoch 15/40
 - 0s - loss: 0.0376 - acc: 0.9775
Epoch 16/40
 - 0s - loss: 0.0374 - acc: 0.9763
Epoch 17/40
 - 0s - loss: 0.0370 - acc: 0.9776
Epoch 18/40
 - 0s - loss: 0.0364 - acc: 0.9773
Epoch 19/40
 - 0s - loss: 0.0366 - acc: 0.9778
Epoch 20/40
 - 0s - loss: 0.0358 - acc: 0.9790
Epoch 21/40
 - 0s - loss: 0.0363 - acc: 0.9778
Epoch 22/40
 - 0s - loss: 0.0355 - acc: 0.9772
Epoch 23/40
 - 0s - loss: 0.0355 - acc: 0.9778
Epoch 24/40
 - 0s - loss: 0.0358 - acc: 0.9784
Epoch 25/40
 - 0s - loss: 0.0359 - acc: 0.9770
Epoch 26/40
 - 0s - loss: 0.0354 - acc: 0.9784
Epoch 27/40
 - 0s - loss: 0.0354 - acc: 0.9770
Epoch 28/40
 - 0s - loss: 0.0350 - acc: 0.9783
Epoch 29/40
 - 0s - loss: 0.0347 - acc: 0.9776
Epoch 30/40
 - 0s - loss: 0.0348 - acc: 0.9778
Epoch 31/40
 - 0s - loss: 0.0342 - acc: 0.9792
Epoch 32/40
 - 0s - loss: 0.0342 - acc: 0.9787
Epoch 33/40
 - 0s - loss: 0.0346 - acc: 0.9781
Epoch 34/40
 - 0s - loss: 0.0339 - acc: 0.9782
Epoch 35/40
 - 0s - loss: 0.0335 - acc: 0.9788
Epoch 36/40
 - 0s - loss: 0.0337 - acc: 0.9789
Epoch 37/40
 - 0s - loss: 0.0335 - acc: 0.9787
Epoch 38/40
 - 0s - loss: 0.0335 - acc: 0.9787
Epoch 39/40
 - 0s - loss: 0.0335 - acc: 0.9783
Epoch 40/40
 - 0s - loss: 0.0332 - acc: 0.9787
# Training time = 0:03:30.738580
# F-Score(Ordinary) = 0.423, Recall: 0.83, Precision: 0.284
# F-Score(lvc) = 0.469, Recall: 0.75, Precision: 0.341
# F-Score(ireflv) = 0.39, Recall: 0.762, Precision: 0.262
# F-Score(id) = 0.41, Recall: 0.98, Precision: 0.259
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_181 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_182 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_181 (Embedding)       (None, 4, 48)        705264      input_181[0][0]                  
__________________________________________________________________________________________________
embedding_182 (Embedding)       (None, 4, 24)        5640        input_182[0][0]                  
__________________________________________________________________________________________________
flatten_181 (Flatten)           (None, 192)          0           embedding_181[0][0]              
__________________________________________________________________________________________________
flatten_182 (Flatten)           (None, 96)           0           embedding_182[0][0]              
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 288)          0           flatten_181[0][0]                
                                                                 flatten_182[0][0]                
__________________________________________________________________________________________________
dense_181 (Dense)               (None, 24)           6936        concatenate_91[0][0]             
__________________________________________________________________________________________________
dropout_91 (Dropout)            (None, 24)           0           dense_181[0][0]                  
__________________________________________________________________________________________________
dense_182 (Dense)               (None, 8)            200         dropout_91[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0844 - acc: 0.9649 - val_loss: 0.0713 - val_acc: 0.9681
Epoch 2/40
 - 4s - loss: 0.0541 - acc: 0.9726 - val_loss: 0.0708 - val_acc: 0.9676
Epoch 3/40
 - 4s - loss: 0.0462 - acc: 0.9748 - val_loss: 0.0739 - val_acc: 0.9676
Epoch 4/40
 - 4s - loss: 0.0431 - acc: 0.9760 - val_loss: 0.0771 - val_acc: 0.9672
Epoch 5/40
 - 4s - loss: 0.0409 - acc: 0.9765 - val_loss: 0.0805 - val_acc: 0.9682
Epoch 6/40
 - 4s - loss: 0.0396 - acc: 0.9769 - val_loss: 0.0823 - val_acc: 0.9684
Epoch 7/40
 - 4s - loss: 0.0387 - acc: 0.9770 - val_loss: 0.0838 - val_acc: 0.9678
Epoch 8/40
 - 4s - loss: 0.0379 - acc: 0.9767 - val_loss: 0.0863 - val_acc: 0.9681
Epoch 9/40
 - 4s - loss: 0.0371 - acc: 0.9778 - val_loss: 0.0871 - val_acc: 0.9685
Epoch 10/40
 - 4s - loss: 0.0365 - acc: 0.9777 - val_loss: 0.0886 - val_acc: 0.9682
Epoch 11/40
 - 4s - loss: 0.0358 - acc: 0.9776 - val_loss: 0.0909 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0355 - acc: 0.9782 - val_loss: 0.0909 - val_acc: 0.9681
Epoch 13/40
 - 4s - loss: 0.0351 - acc: 0.9778 - val_loss: 0.0936 - val_acc: 0.9684
Epoch 14/40
 - 4s - loss: 0.0348 - acc: 0.9780 - val_loss: 0.0951 - val_acc: 0.9683
Epoch 15/40
 - 4s - loss: 0.0345 - acc: 0.9778 - val_loss: 0.0961 - val_acc: 0.9682
Epoch 16/40
 - 4s - loss: 0.0344 - acc: 0.9781 - val_loss: 0.0957 - val_acc: 0.9684
Epoch 17/40
 - 4s - loss: 0.0342 - acc: 0.9782 - val_loss: 0.0990 - val_acc: 0.9679
Epoch 18/40
 - 4s - loss: 0.0340 - acc: 0.9782 - val_loss: 0.0988 - val_acc: 0.9684
Epoch 19/40
 - 4s - loss: 0.0338 - acc: 0.9783 - val_loss: 0.0992 - val_acc: 0.9681
Epoch 20/40
 - 4s - loss: 0.0336 - acc: 0.9784 - val_loss: 0.1015 - val_acc: 0.9685
Epoch 21/40
 - 4s - loss: 0.0334 - acc: 0.9785 - val_loss: 0.1016 - val_acc: 0.9682
Epoch 22/40
 - 4s - loss: 0.0333 - acc: 0.9790 - val_loss: 0.1016 - val_acc: 0.9682
Epoch 23/40
 - 4s - loss: 0.0331 - acc: 0.9783 - val_loss: 0.1050 - val_acc: 0.9688
Epoch 24/40
 - 4s - loss: 0.0330 - acc: 0.9785 - val_loss: 0.1041 - val_acc: 0.9684
Epoch 25/40
 - 4s - loss: 0.0327 - acc: 0.9786 - val_loss: 0.1065 - val_acc: 0.9687
Epoch 26/40
 - 4s - loss: 0.0328 - acc: 0.9790 - val_loss: 0.1066 - val_acc: 0.9686
Epoch 27/40
 - 4s - loss: 0.0327 - acc: 0.9784 - val_loss: 0.1072 - val_acc: 0.9687
Epoch 28/40
 - 4s - loss: 0.0325 - acc: 0.9785 - val_loss: 0.1075 - val_acc: 0.9687
Epoch 29/40
 - 4s - loss: 0.0323 - acc: 0.9788 - val_loss: 0.1071 - val_acc: 0.9685
Epoch 30/40
 - 4s - loss: 0.0323 - acc: 0.9786 - val_loss: 0.1094 - val_acc: 0.9690
Epoch 31/40
 - 4s - loss: 0.0321 - acc: 0.9786 - val_loss: 0.1074 - val_acc: 0.9686
Epoch 32/40
 - 4s - loss: 0.0322 - acc: 0.9788 - val_loss: 0.1095 - val_acc: 0.9685
Epoch 33/40
 - 4s - loss: 0.0322 - acc: 0.9785 - val_loss: 0.1096 - val_acc: 0.9689
Epoch 34/40
 - 4s - loss: 0.0320 - acc: 0.9789 - val_loss: 0.1099 - val_acc: 0.9685
Epoch 35/40
 - 4s - loss: 0.0318 - acc: 0.9793 - val_loss: 0.1096 - val_acc: 0.9688
Epoch 36/40
 - 4s - loss: 0.0319 - acc: 0.9790 - val_loss: 0.1112 - val_acc: 0.9684
Epoch 37/40
 - 4s - loss: 0.0318 - acc: 0.9788 - val_loss: 0.1134 - val_acc: 0.9684
Epoch 38/40
 - 4s - loss: 0.0319 - acc: 0.9791 - val_loss: 0.1112 - val_acc: 0.9685
Epoch 39/40
 - 4s - loss: 0.0316 - acc: 0.9788 - val_loss: 0.1128 - val_acc: 0.9685
Epoch 40/40
 - 4s - loss: 0.0316 - acc: 0.9788 - val_loss: 0.1130 - val_acc: 0.9689
Epoch 1/40
 - 0s - loss: 0.0939 - acc: 0.9669
Epoch 2/40
 - 0s - loss: 0.0565 - acc: 0.9729
Epoch 3/40
 - 0s - loss: 0.0501 - acc: 0.9733
Epoch 4/40
 - 0s - loss: 0.0460 - acc: 0.9747
Epoch 5/40
 - 0s - loss: 0.0449 - acc: 0.9750
Epoch 6/40
 - 0s - loss: 0.0418 - acc: 0.9756
Epoch 7/40
 - 0s - loss: 0.0422 - acc: 0.9755
Epoch 8/40
 - 0s - loss: 0.0395 - acc: 0.9770
Epoch 9/40
 - 0s - loss: 0.0390 - acc: 0.9772
Epoch 10/40
 - 0s - loss: 0.0386 - acc: 0.9767
Epoch 11/40
 - 0s - loss: 0.0378 - acc: 0.9770
Epoch 12/40
 - 0s - loss: 0.0372 - acc: 0.9769
Epoch 13/40
 - 0s - loss: 0.0365 - acc: 0.9773
Epoch 14/40
 - 0s - loss: 0.0361 - acc: 0.9778
Epoch 15/40
 - 0s - loss: 0.0356 - acc: 0.9768
Epoch 16/40
 - 0s - loss: 0.0348 - acc: 0.9766
Epoch 17/40
 - 0s - loss: 0.0347 - acc: 0.9783
Epoch 18/40
 - 0s - loss: 0.0346 - acc: 0.9771
Epoch 19/40
 - 0s - loss: 0.0335 - acc: 0.9782
Epoch 20/40
 - 0s - loss: 0.0339 - acc: 0.9787
Epoch 21/40
 - 0s - loss: 0.0331 - acc: 0.9786
Epoch 22/40
 - 0s - loss: 0.0323 - acc: 0.9786
Epoch 23/40
 - 0s - loss: 0.0322 - acc: 0.9795
Epoch 24/40
 - 0s - loss: 0.0327 - acc: 0.9770
Epoch 25/40
 - 0s - loss: 0.0319 - acc: 0.9782
Epoch 26/40
 - 0s - loss: 0.0322 - acc: 0.9784
Epoch 27/40
 - 0s - loss: 0.0318 - acc: 0.9788
Epoch 28/40
 - 0s - loss: 0.0318 - acc: 0.9786
Epoch 29/40
 - 0s - loss: 0.0314 - acc: 0.9800
Epoch 30/40
 - 0s - loss: 0.0312 - acc: 0.9782
Epoch 31/40
 - 0s - loss: 0.0314 - acc: 0.9787
Epoch 32/40
 - 0s - loss: 0.0315 - acc: 0.9789
Epoch 33/40
 - 0s - loss: 0.0315 - acc: 0.9784
Epoch 34/40
 - 0s - loss: 0.0309 - acc: 0.9784
Epoch 35/40
 - 0s - loss: 0.0311 - acc: 0.9783
Epoch 36/40
 - 0s - loss: 0.0312 - acc: 0.9776
Epoch 37/40
 - 0s - loss: 0.0309 - acc: 0.9794
Epoch 38/40
 - 0s - loss: 0.0311 - acc: 0.9799
Epoch 39/40
 - 0s - loss: 0.0308 - acc: 0.9786
Epoch 40/40
 - 0s - loss: 0.0309 - acc: 0.9784
# Training time = 0:03:28.566069
# F-Score(Ordinary) = 0.095, Recall: 0.622, Precision: 0.051
# F-Score(lvc) = 0.086, Recall: 0.75, Precision: 0.045
# F-Score(ireflv) = 0.139, Recall: 0.455, Precision: 0.082
# F-Score(id) = 0.07, Recall: 1.0, Precision: 0.036
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_183 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_184 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_183 (Embedding)       (None, 4, 48)        705264      input_183[0][0]                  
__________________________________________________________________________________________________
embedding_184 (Embedding)       (None, 4, 24)        5640        input_184[0][0]                  
__________________________________________________________________________________________________
flatten_183 (Flatten)           (None, 192)          0           embedding_183[0][0]              
__________________________________________________________________________________________________
flatten_184 (Flatten)           (None, 96)           0           embedding_184[0][0]              
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 288)          0           flatten_183[0][0]                
                                                                 flatten_184[0][0]                
__________________________________________________________________________________________________
dense_183 (Dense)               (None, 24)           6936        concatenate_92[0][0]             
__________________________________________________________________________________________________
dropout_92 (Dropout)            (None, 24)           0           dense_183[0][0]                  
__________________________________________________________________________________________________
dense_184 (Dense)               (None, 8)            200         dropout_92[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0858 - acc: 0.9652 - val_loss: 0.0690 - val_acc: 0.9688
Epoch 2/40
 - 4s - loss: 0.0546 - acc: 0.9727 - val_loss: 0.0711 - val_acc: 0.9677
Epoch 3/40
 - 4s - loss: 0.0479 - acc: 0.9744 - val_loss: 0.0725 - val_acc: 0.9680
Epoch 4/40
 - 4s - loss: 0.0447 - acc: 0.9757 - val_loss: 0.0759 - val_acc: 0.9679
Epoch 5/40
 - 4s - loss: 0.0423 - acc: 0.9763 - val_loss: 0.0781 - val_acc: 0.9680
Epoch 6/40
 - 4s - loss: 0.0407 - acc: 0.9764 - val_loss: 0.0793 - val_acc: 0.9685
Epoch 7/40
 - 4s - loss: 0.0398 - acc: 0.9767 - val_loss: 0.0818 - val_acc: 0.9685
Epoch 8/40
 - 4s - loss: 0.0390 - acc: 0.9767 - val_loss: 0.0855 - val_acc: 0.9687
Epoch 9/40
 - 4s - loss: 0.0384 - acc: 0.9771 - val_loss: 0.0850 - val_acc: 0.9686
Epoch 10/40
 - 4s - loss: 0.0377 - acc: 0.9775 - val_loss: 0.0876 - val_acc: 0.9687
Epoch 11/40
 - 4s - loss: 0.0373 - acc: 0.9776 - val_loss: 0.0905 - val_acc: 0.9683
Epoch 12/40
 - 4s - loss: 0.0368 - acc: 0.9775 - val_loss: 0.0907 - val_acc: 0.9684
Epoch 13/40
 - 4s - loss: 0.0364 - acc: 0.9776 - val_loss: 0.0913 - val_acc: 0.9682
Epoch 14/40
 - 4s - loss: 0.0363 - acc: 0.9777 - val_loss: 0.0968 - val_acc: 0.9684
Epoch 15/40
 - 4s - loss: 0.0361 - acc: 0.9778 - val_loss: 0.0932 - val_acc: 0.9684
Epoch 16/40
 - 4s - loss: 0.0357 - acc: 0.9778 - val_loss: 0.0960 - val_acc: 0.9683
Epoch 17/40
 - 4s - loss: 0.0355 - acc: 0.9777 - val_loss: 0.0964 - val_acc: 0.9682
Epoch 18/40
 - 4s - loss: 0.0350 - acc: 0.9781 - val_loss: 0.0980 - val_acc: 0.9684
Epoch 19/40
 - 4s - loss: 0.0350 - acc: 0.9780 - val_loss: 0.0983 - val_acc: 0.9684
Epoch 20/40
 - 4s - loss: 0.0349 - acc: 0.9778 - val_loss: 0.0984 - val_acc: 0.9687
Epoch 21/40
 - 4s - loss: 0.0346 - acc: 0.9782 - val_loss: 0.0987 - val_acc: 0.9685
Epoch 22/40
 - 4s - loss: 0.0346 - acc: 0.9779 - val_loss: 0.1015 - val_acc: 0.9681
Epoch 23/40
 - 4s - loss: 0.0346 - acc: 0.9782 - val_loss: 0.1009 - val_acc: 0.9685
Epoch 24/40
 - 4s - loss: 0.0342 - acc: 0.9786 - val_loss: 0.1046 - val_acc: 0.9682
Epoch 25/40
 - 4s - loss: 0.0340 - acc: 0.9783 - val_loss: 0.1047 - val_acc: 0.9684
Epoch 26/40
 - 4s - loss: 0.0338 - acc: 0.9786 - val_loss: 0.1045 - val_acc: 0.9682
Epoch 27/40
 - 4s - loss: 0.0337 - acc: 0.9785 - val_loss: 0.1048 - val_acc: 0.9682
Epoch 28/40
 - 4s - loss: 0.0336 - acc: 0.9784 - val_loss: 0.1057 - val_acc: 0.9680
Epoch 29/40
 - 4s - loss: 0.0336 - acc: 0.9784 - val_loss: 0.1086 - val_acc: 0.9688
Epoch 30/40
 - 4s - loss: 0.0336 - acc: 0.9785 - val_loss: 0.1076 - val_acc: 0.9684
Epoch 31/40
 - 4s - loss: 0.0333 - acc: 0.9785 - val_loss: 0.1077 - val_acc: 0.9687
Epoch 32/40
 - 4s - loss: 0.0333 - acc: 0.9787 - val_loss: 0.1083 - val_acc: 0.9685
Epoch 33/40
 - 4s - loss: 0.0333 - acc: 0.9787 - val_loss: 0.1108 - val_acc: 0.9685
Epoch 34/40
 - 4s - loss: 0.0332 - acc: 0.9786 - val_loss: 0.1098 - val_acc: 0.9685
Epoch 35/40
 - 4s - loss: 0.0330 - acc: 0.9789 - val_loss: 0.1093 - val_acc: 0.9683
Epoch 36/40
 - 4s - loss: 0.0328 - acc: 0.9788 - val_loss: 0.1116 - val_acc: 0.9684
Epoch 37/40
 - 4s - loss: 0.0329 - acc: 0.9791 - val_loss: 0.1124 - val_acc: 0.9685
Epoch 38/40
 - 4s - loss: 0.0327 - acc: 0.9792 - val_loss: 0.1138 - val_acc: 0.9683
Epoch 39/40
 - 4s - loss: 0.0328 - acc: 0.9789 - val_loss: 0.1126 - val_acc: 0.9685
Epoch 40/40
 - 4s - loss: 0.0328 - acc: 0.9789 - val_loss: 0.1134 - val_acc: 0.9684
Epoch 1/40
 - 0s - loss: 0.0870 - acc: 0.9665
Epoch 2/40
 - 0s - loss: 0.0548 - acc: 0.9733
Epoch 3/40
 - 0s - loss: 0.0478 - acc: 0.9749
Epoch 4/40
 - 0s - loss: 0.0442 - acc: 0.9746
Epoch 5/40
 - 0s - loss: 0.0432 - acc: 0.9767
Epoch 6/40
 - 0s - loss: 0.0417 - acc: 0.9757
Epoch 7/40
 - 0s - loss: 0.0399 - acc: 0.9765
Epoch 8/40
 - 0s - loss: 0.0392 - acc: 0.9776
Epoch 9/40
 - 0s - loss: 0.0375 - acc: 0.9776
Epoch 10/40
 - 0s - loss: 0.0386 - acc: 0.9771
Epoch 11/40
 - 0s - loss: 0.0378 - acc: 0.9776
Epoch 12/40
 - 0s - loss: 0.0374 - acc: 0.9772
Epoch 13/40
 - 0s - loss: 0.0368 - acc: 0.9779
Epoch 14/40
 - 0s - loss: 0.0366 - acc: 0.9775
Epoch 15/40
 - 0s - loss: 0.0364 - acc: 0.9773
Epoch 16/40
 - 0s - loss: 0.0361 - acc: 0.9785
Epoch 17/40
 - 0s - loss: 0.0364 - acc: 0.9770
Epoch 18/40
 - 0s - loss: 0.0359 - acc: 0.9768
Epoch 19/40
 - 0s - loss: 0.0359 - acc: 0.9782
Epoch 20/40
 - 0s - loss: 0.0359 - acc: 0.9780
Epoch 21/40
 - 0s - loss: 0.0349 - acc: 0.9777
Epoch 22/40
 - 0s - loss: 0.0341 - acc: 0.9778
Epoch 23/40
 - 0s - loss: 0.0341 - acc: 0.9775
Epoch 24/40
 - 0s - loss: 0.0333 - acc: 0.9787
Epoch 25/40
 - 0s - loss: 0.0333 - acc: 0.9791
Epoch 26/40
 - 0s - loss: 0.0333 - acc: 0.9783
Epoch 27/40
 - 0s - loss: 0.0329 - acc: 0.9788
Epoch 28/40
 - 0s - loss: 0.0327 - acc: 0.9786
Epoch 29/40
 - 0s - loss: 0.0325 - acc: 0.9789
Epoch 30/40
 - 0s - loss: 0.0327 - acc: 0.9787
Epoch 31/40
 - 0s - loss: 0.0324 - acc: 0.9776
Epoch 32/40
 - 0s - loss: 0.0323 - acc: 0.9795
Epoch 33/40
 - 0s - loss: 0.0326 - acc: 0.9788
Epoch 34/40
 - 0s - loss: 0.0325 - acc: 0.9780
Epoch 35/40
 - 0s - loss: 0.0324 - acc: 0.9788
Epoch 36/40
 - 0s - loss: 0.0324 - acc: 0.9786
Epoch 37/40
 - 0s - loss: 0.0325 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0323 - acc: 0.9792
Epoch 39/40
 - 0s - loss: 0.0326 - acc: 0.9783
Epoch 40/40
 - 0s - loss: 0.0324 - acc: 0.9793
# Training time = 0:03:27.804061
# F-Score(Ordinary) = 0.324, Recall: 0.864, Precision: 0.199
# F-Score(lvc) = 0.127, Recall: 0.9, Precision: 0.068
# F-Score(ireflv) = 0.475, Recall: 0.764, Precision: 0.344
# F-Score(id) = 0.329, Recall: 1.0, Precision: 0.197
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_185 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_186 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_185 (Embedding)       (None, 4, 48)        705264      input_185[0][0]                  
__________________________________________________________________________________________________
embedding_186 (Embedding)       (None, 4, 24)        5640        input_186[0][0]                  
__________________________________________________________________________________________________
flatten_185 (Flatten)           (None, 192)          0           embedding_185[0][0]              
__________________________________________________________________________________________________
flatten_186 (Flatten)           (None, 96)           0           embedding_186[0][0]              
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 288)          0           flatten_185[0][0]                
                                                                 flatten_186[0][0]                
__________________________________________________________________________________________________
dense_185 (Dense)               (None, 24)           6936        concatenate_93[0][0]             
__________________________________________________________________________________________________
dropout_93 (Dropout)            (None, 24)           0           dense_185[0][0]                  
__________________________________________________________________________________________________
dense_186 (Dense)               (None, 8)            200         dropout_93[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0835 - acc: 0.9656 - val_loss: 0.0696 - val_acc: 0.9685
Epoch 2/40
 - 4s - loss: 0.0545 - acc: 0.9726 - val_loss: 0.0698 - val_acc: 0.9683
Epoch 3/40
 - 4s - loss: 0.0477 - acc: 0.9744 - val_loss: 0.0729 - val_acc: 0.9673
Epoch 4/40
 - 4s - loss: 0.0439 - acc: 0.9755 - val_loss: 0.0767 - val_acc: 0.9684
Epoch 5/40
 - 4s - loss: 0.0417 - acc: 0.9760 - val_loss: 0.0799 - val_acc: 0.9680
Epoch 6/40
 - 4s - loss: 0.0402 - acc: 0.9770 - val_loss: 0.0808 - val_acc: 0.9680
Epoch 7/40
 - 4s - loss: 0.0392 - acc: 0.9772 - val_loss: 0.0824 - val_acc: 0.9677
Epoch 8/40
 - 4s - loss: 0.0380 - acc: 0.9770 - val_loss: 0.0857 - val_acc: 0.9684
Epoch 9/40
 - 4s - loss: 0.0374 - acc: 0.9772 - val_loss: 0.0873 - val_acc: 0.9682
Epoch 10/40
 - 4s - loss: 0.0371 - acc: 0.9773 - val_loss: 0.0880 - val_acc: 0.9681
Epoch 11/40
 - 4s - loss: 0.0364 - acc: 0.9775 - val_loss: 0.0893 - val_acc: 0.9685
Epoch 12/40
 - 4s - loss: 0.0360 - acc: 0.9776 - val_loss: 0.0928 - val_acc: 0.9680
Epoch 13/40
 - 4s - loss: 0.0355 - acc: 0.9776 - val_loss: 0.0922 - val_acc: 0.9681
Epoch 14/40
 - 4s - loss: 0.0353 - acc: 0.9781 - val_loss: 0.0938 - val_acc: 0.9680
Epoch 15/40
 - 4s - loss: 0.0350 - acc: 0.9777 - val_loss: 0.0928 - val_acc: 0.9685
Epoch 16/40
 - 4s - loss: 0.0350 - acc: 0.9778 - val_loss: 0.0954 - val_acc: 0.9683
Epoch 17/40
 - 4s - loss: 0.0344 - acc: 0.9784 - val_loss: 0.0962 - val_acc: 0.9682
Epoch 18/40
 - 4s - loss: 0.0342 - acc: 0.9784 - val_loss: 0.0967 - val_acc: 0.9681
Epoch 19/40
 - 4s - loss: 0.0343 - acc: 0.9783 - val_loss: 0.0977 - val_acc: 0.9682
Epoch 20/40
 - 4s - loss: 0.0338 - acc: 0.9787 - val_loss: 0.0987 - val_acc: 0.9682
Epoch 21/40
 - 4s - loss: 0.0337 - acc: 0.9788 - val_loss: 0.0997 - val_acc: 0.9686
Epoch 22/40
 - 4s - loss: 0.0334 - acc: 0.9784 - val_loss: 0.1010 - val_acc: 0.9682
Epoch 23/40
 - 4s - loss: 0.0334 - acc: 0.9783 - val_loss: 0.1020 - val_acc: 0.9681
Epoch 24/40
 - 4s - loss: 0.0332 - acc: 0.9789 - val_loss: 0.1021 - val_acc: 0.9684
Epoch 25/40
 - 4s - loss: 0.0334 - acc: 0.9787 - val_loss: 0.1035 - val_acc: 0.9682
Epoch 26/40
 - 4s - loss: 0.0330 - acc: 0.9785 - val_loss: 0.1031 - val_acc: 0.9683
Epoch 27/40
 - 4s - loss: 0.0329 - acc: 0.9790 - val_loss: 0.1039 - val_acc: 0.9684
Epoch 28/40
 - 4s - loss: 0.0328 - acc: 0.9788 - val_loss: 0.1028 - val_acc: 0.9686
Epoch 29/40
 - 4s - loss: 0.0329 - acc: 0.9788 - val_loss: 0.1051 - val_acc: 0.9688
Epoch 30/40
 - 4s - loss: 0.0327 - acc: 0.9790 - val_loss: 0.1057 - val_acc: 0.9687
Epoch 31/40
 - 4s - loss: 0.0325 - acc: 0.9792 - val_loss: 0.1071 - val_acc: 0.9687
Epoch 32/40
 - 4s - loss: 0.0324 - acc: 0.9793 - val_loss: 0.1081 - val_acc: 0.9685
Epoch 33/40
 - 4s - loss: 0.0324 - acc: 0.9791 - val_loss: 0.1089 - val_acc: 0.9686
Epoch 34/40
 - 4s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1074 - val_acc: 0.9683
Epoch 35/40
 - 4s - loss: 0.0325 - acc: 0.9788 - val_loss: 0.1081 - val_acc: 0.9685
Epoch 36/40
 - 4s - loss: 0.0323 - acc: 0.9786 - val_loss: 0.1086 - val_acc: 0.9688
Epoch 37/40
 - 4s - loss: 0.0320 - acc: 0.9790 - val_loss: 0.1095 - val_acc: 0.9689
Epoch 38/40
 - 4s - loss: 0.0321 - acc: 0.9791 - val_loss: 0.1116 - val_acc: 0.9686
Epoch 39/40
 - 4s - loss: 0.0320 - acc: 0.9789 - val_loss: 0.1097 - val_acc: 0.9686
Epoch 40/40
 - 4s - loss: 0.0322 - acc: 0.9790 - val_loss: 0.1104 - val_acc: 0.9688
Epoch 1/40
 - 0s - loss: 0.0901 - acc: 0.9668
Epoch 2/40
 - 0s - loss: 0.0553 - acc: 0.9719
Epoch 3/40
 - 0s - loss: 0.0477 - acc: 0.9745
Epoch 4/40
 - 0s - loss: 0.0444 - acc: 0.9749
Epoch 5/40
 - 0s - loss: 0.0411 - acc: 0.9761
Epoch 6/40
 - 0s - loss: 0.0391 - acc: 0.9757
Epoch 7/40
 - 0s - loss: 0.0375 - acc: 0.9772
Epoch 8/40
 - 0s - loss: 0.0361 - acc: 0.9780
Epoch 9/40
 - 0s - loss: 0.0360 - acc: 0.9777
Epoch 10/40
 - 0s - loss: 0.0350 - acc: 0.9771
Epoch 11/40
 - 0s - loss: 0.0347 - acc: 0.9792
Epoch 12/40
 - 0s - loss: 0.0339 - acc: 0.9791
Epoch 13/40
 - 0s - loss: 0.0337 - acc: 0.9782
Epoch 14/40
 - 0s - loss: 0.0338 - acc: 0.9788
Epoch 15/40
 - 0s - loss: 0.0332 - acc: 0.9782
Epoch 16/40
 - 0s - loss: 0.0332 - acc: 0.9797
Epoch 17/40
 - 0s - loss: 0.0330 - acc: 0.9783
Epoch 18/40
 - 0s - loss: 0.0323 - acc: 0.9793
Epoch 19/40
 - 0s - loss: 0.0329 - acc: 0.9783
Epoch 20/40
 - 0s - loss: 0.0324 - acc: 0.9783
Epoch 21/40
 - 0s - loss: 0.0322 - acc: 0.9794
Epoch 22/40
 - 0s - loss: 0.0320 - acc: 0.9791
Epoch 23/40
 - 0s - loss: 0.0323 - acc: 0.9783
Epoch 24/40
 - 0s - loss: 0.0323 - acc: 0.9789
Epoch 25/40
 - 0s - loss: 0.0319 - acc: 0.9791
Epoch 26/40
 - 0s - loss: 0.0318 - acc: 0.9790
Epoch 27/40
 - 0s - loss: 0.0312 - acc: 0.9790
Epoch 28/40
 - 0s - loss: 0.0318 - acc: 0.9782
Epoch 29/40
 - 0s - loss: 0.0319 - acc: 0.9786
Epoch 30/40
 - 0s - loss: 0.0316 - acc: 0.9789
Epoch 31/40
 - 0s - loss: 0.0311 - acc: 0.9804
Epoch 32/40
 - 0s - loss: 0.0316 - acc: 0.9791
Epoch 33/40
 - 0s - loss: 0.0309 - acc: 0.9786
Epoch 34/40
 - 0s - loss: 0.0314 - acc: 0.9790
Epoch 35/40
 - 0s - loss: 0.0311 - acc: 0.9787
Epoch 36/40
 - 0s - loss: 0.0312 - acc: 0.9807
Epoch 37/40
 - 0s - loss: 0.0309 - acc: 0.9803
Epoch 38/40
 - 0s - loss: 0.0308 - acc: 0.9802
Epoch 39/40
 - 0s - loss: 0.0308 - acc: 0.9798
Epoch 40/40
 - 0s - loss: 0.0307 - acc: 0.9793
# Training time = 0:03:31.422255
# F-Score(Ordinary) = 0.334, Recall: 0.606, Precision: 0.23
# F-Score(lvc) = 0.164, Recall: 0.359, Precision: 0.106
# F-Score(ireflv) = 0.615, Recall: 0.643, Precision: 0.59
# F-Score(id) = 0.151, Recall: 0.842, Precision: 0.083
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_187 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_188 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_187 (Embedding)       (None, 4, 48)        705264      input_187[0][0]                  
__________________________________________________________________________________________________
embedding_188 (Embedding)       (None, 4, 24)        5640        input_188[0][0]                  
__________________________________________________________________________________________________
flatten_187 (Flatten)           (None, 192)          0           embedding_187[0][0]              
__________________________________________________________________________________________________
flatten_188 (Flatten)           (None, 96)           0           embedding_188[0][0]              
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 288)          0           flatten_187[0][0]                
                                                                 flatten_188[0][0]                
__________________________________________________________________________________________________
dense_187 (Dense)               (None, 24)           6936        concatenate_94[0][0]             
__________________________________________________________________________________________________
dropout_94 (Dropout)            (None, 24)           0           dense_187[0][0]                  
__________________________________________________________________________________________________
dense_188 (Dense)               (None, 8)            200         dropout_94[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0841 - acc: 0.9646 - val_loss: 0.0683 - val_acc: 0.9689
Epoch 2/40
 - 4s - loss: 0.0535 - acc: 0.9727 - val_loss: 0.0688 - val_acc: 0.9687
Epoch 3/40
 - 4s - loss: 0.0463 - acc: 0.9740 - val_loss: 0.0701 - val_acc: 0.9690
Epoch 4/40
 - 4s - loss: 0.0428 - acc: 0.9761 - val_loss: 0.0749 - val_acc: 0.9685
Epoch 5/40
 - 4s - loss: 0.0406 - acc: 0.9769 - val_loss: 0.0783 - val_acc: 0.9687
Epoch 6/40
 - 4s - loss: 0.0393 - acc: 0.9769 - val_loss: 0.0810 - val_acc: 0.9691
Epoch 7/40
 - 4s - loss: 0.0383 - acc: 0.9769 - val_loss: 0.0811 - val_acc: 0.9691
Epoch 8/40
 - 4s - loss: 0.0374 - acc: 0.9776 - val_loss: 0.0836 - val_acc: 0.9686
Epoch 9/40
 - 4s - loss: 0.0369 - acc: 0.9773 - val_loss: 0.0869 - val_acc: 0.9691
Epoch 10/40
 - 4s - loss: 0.0362 - acc: 0.9779 - val_loss: 0.0859 - val_acc: 0.9690
Epoch 11/40
 - 4s - loss: 0.0357 - acc: 0.9778 - val_loss: 0.0888 - val_acc: 0.9686
Epoch 12/40
 - 4s - loss: 0.0356 - acc: 0.9778 - val_loss: 0.0887 - val_acc: 0.9692
Epoch 13/40
 - 4s - loss: 0.0349 - acc: 0.9779 - val_loss: 0.0923 - val_acc: 0.9687
Epoch 14/40
 - 4s - loss: 0.0347 - acc: 0.9778 - val_loss: 0.0928 - val_acc: 0.9690
Epoch 15/40
 - 4s - loss: 0.0345 - acc: 0.9780 - val_loss: 0.0954 - val_acc: 0.9687
Epoch 16/40
 - 4s - loss: 0.0341 - acc: 0.9782 - val_loss: 0.0956 - val_acc: 0.9685
Epoch 17/40
 - 4s - loss: 0.0340 - acc: 0.9783 - val_loss: 0.0971 - val_acc: 0.9687
Epoch 18/40
 - 4s - loss: 0.0336 - acc: 0.9788 - val_loss: 0.0986 - val_acc: 0.9689
Epoch 19/40
 - 4s - loss: 0.0336 - acc: 0.9784 - val_loss: 0.0999 - val_acc: 0.9690
Epoch 20/40
 - 4s - loss: 0.0335 - acc: 0.9787 - val_loss: 0.0994 - val_acc: 0.9691
Epoch 21/40
 - 4s - loss: 0.0331 - acc: 0.9787 - val_loss: 0.1024 - val_acc: 0.9688
Epoch 22/40
 - 4s - loss: 0.0332 - acc: 0.9788 - val_loss: 0.1010 - val_acc: 0.9684
Epoch 23/40
 - 4s - loss: 0.0329 - acc: 0.9784 - val_loss: 0.1022 - val_acc: 0.9691
Epoch 24/40
 - 4s - loss: 0.0329 - acc: 0.9788 - val_loss: 0.1028 - val_acc: 0.9693
Epoch 25/40
 - 4s - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1060 - val_acc: 0.9687
Epoch 26/40
 - 4s - loss: 0.0325 - acc: 0.9789 - val_loss: 0.1040 - val_acc: 0.9684
Epoch 27/40
 - 4s - loss: 0.0324 - acc: 0.9788 - val_loss: 0.1060 - val_acc: 0.9689
Epoch 28/40
 - 4s - loss: 0.0324 - acc: 0.9787 - val_loss: 0.1077 - val_acc: 0.9690
Epoch 29/40
 - 4s - loss: 0.0323 - acc: 0.9789 - val_loss: 0.1091 - val_acc: 0.9683
Epoch 30/40
 - 4s - loss: 0.0322 - acc: 0.9786 - val_loss: 0.1083 - val_acc: 0.9689
Epoch 31/40
 - 4s - loss: 0.0322 - acc: 0.9790 - val_loss: 0.1076 - val_acc: 0.9689
Epoch 32/40
 - 4s - loss: 0.0322 - acc: 0.9788 - val_loss: 0.1093 - val_acc: 0.9692
Epoch 33/40
 - 4s - loss: 0.0321 - acc: 0.9791 - val_loss: 0.1111 - val_acc: 0.9686
Epoch 34/40
 - 4s - loss: 0.0320 - acc: 0.9791 - val_loss: 0.1102 - val_acc: 0.9687
Epoch 35/40
 - 4s - loss: 0.0317 - acc: 0.9792 - val_loss: 0.1099 - val_acc: 0.9688
Epoch 36/40
 - 4s - loss: 0.0315 - acc: 0.9794 - val_loss: 0.1137 - val_acc: 0.9693
Epoch 37/40
 - 4s - loss: 0.0317 - acc: 0.9796 - val_loss: 0.1097 - val_acc: 0.9688
Epoch 38/40
 - 4s - loss: 0.0316 - acc: 0.9790 - val_loss: 0.1123 - val_acc: 0.9687
Epoch 39/40
 - 4s - loss: 0.0316 - acc: 0.9790 - val_loss: 0.1122 - val_acc: 0.9687
Epoch 40/40
 - 4s - loss: 0.0315 - acc: 0.9794 - val_loss: 0.1131 - val_acc: 0.9690
Epoch 1/40
 - 0s - loss: 0.0892 - acc: 0.9682
Epoch 2/40
 - 0s - loss: 0.0555 - acc: 0.9727
Epoch 3/40
 - 0s - loss: 0.0474 - acc: 0.9741
Epoch 4/40
 - 0s - loss: 0.0437 - acc: 0.9740
Epoch 5/40
 - 0s - loss: 0.0406 - acc: 0.9766
Epoch 6/40
 - 0s - loss: 0.0401 - acc: 0.9769
Epoch 7/40
 - 0s - loss: 0.0385 - acc: 0.9769
Epoch 8/40
 - 0s - loss: 0.0368 - acc: 0.9784
Epoch 9/40
 - 0s - loss: 0.0358 - acc: 0.9771
Epoch 10/40
 - 0s - loss: 0.0352 - acc: 0.9781
Epoch 11/40
 - 0s - loss: 0.0341 - acc: 0.9779
Epoch 12/40
 - 0s - loss: 0.0346 - acc: 0.9781
Epoch 13/40
 - 0s - loss: 0.0335 - acc: 0.9792
Epoch 14/40
 - 0s - loss: 0.0332 - acc: 0.9793
Epoch 15/40
 - 0s - loss: 0.0335 - acc: 0.9792
Epoch 16/40
 - 0s - loss: 0.0328 - acc: 0.9782
Epoch 17/40
 - 0s - loss: 0.0324 - acc: 0.9789
Epoch 18/40
 - 0s - loss: 0.0322 - acc: 0.9785
Epoch 19/40
 - 0s - loss: 0.0320 - acc: 0.9792
Epoch 20/40
 - 0s - loss: 0.0317 - acc: 0.9791
Epoch 21/40
 - 0s - loss: 0.0315 - acc: 0.9790
Epoch 22/40
 - 0s - loss: 0.0316 - acc: 0.9792
Epoch 23/40
 - 0s - loss: 0.0312 - acc: 0.9800
Epoch 24/40
 - 0s - loss: 0.0315 - acc: 0.9793
Epoch 25/40
 - 0s - loss: 0.0317 - acc: 0.9785
Epoch 26/40
 - 0s - loss: 0.0313 - acc: 0.9797
Epoch 27/40
 - 0s - loss: 0.0314 - acc: 0.9797
Epoch 28/40
 - 0s - loss: 0.0318 - acc: 0.9787
Epoch 29/40
 - 0s - loss: 0.0313 - acc: 0.9783
Epoch 30/40
 - 0s - loss: 0.0315 - acc: 0.9790
Epoch 31/40
 - 0s - loss: 0.0307 - acc: 0.9796
Epoch 32/40
 - 0s - loss: 0.0306 - acc: 0.9796
Epoch 33/40
 - 0s - loss: 0.0311 - acc: 0.9788
Epoch 34/40
 - 0s - loss: 0.0308 - acc: 0.9800
Epoch 35/40
 - 0s - loss: 0.0305 - acc: 0.9802
Epoch 36/40
 - 0s - loss: 0.0303 - acc: 0.9790
Epoch 37/40
 - 0s - loss: 0.0306 - acc: 0.9787
Epoch 38/40
 - 0s - loss: 0.0306 - acc: 0.9793
Epoch 39/40
 - 0s - loss: 0.0309 - acc: 0.9795
Epoch 40/40
 - 0s - loss: 0.0303 - acc: 0.9793
# Training time = 0:03:28.106069
# F-Score(Ordinary) = 0.358, Recall: 0.805, Precision: 0.23
# F-Score(lvc) = 0.516, Recall: 0.889, Precision: 0.364
# F-Score(ireflv) = 0.554, Recall: 0.74, Precision: 0.443
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_189 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_190 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_189 (Embedding)       (None, 4, 48)        705264      input_189[0][0]                  
__________________________________________________________________________________________________
embedding_190 (Embedding)       (None, 4, 24)        5640        input_190[0][0]                  
__________________________________________________________________________________________________
flatten_189 (Flatten)           (None, 192)          0           embedding_189[0][0]              
__________________________________________________________________________________________________
flatten_190 (Flatten)           (None, 96)           0           embedding_190[0][0]              
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 288)          0           flatten_189[0][0]                
                                                                 flatten_190[0][0]                
__________________________________________________________________________________________________
dense_189 (Dense)               (None, 24)           6936        concatenate_95[0][0]             
__________________________________________________________________________________________________
dropout_95 (Dropout)            (None, 24)           0           dense_189[0][0]                  
__________________________________________________________________________________________________
dense_190 (Dense)               (None, 8)            200         dropout_95[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.05
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0871 - acc: 0.9651 - val_loss: 0.0698 - val_acc: 0.9677
Epoch 2/40
 - 4s - loss: 0.0548 - acc: 0.9721 - val_loss: 0.0716 - val_acc: 0.9683
Epoch 3/40
 - 4s - loss: 0.0476 - acc: 0.9745 - val_loss: 0.0730 - val_acc: 0.9679
Epoch 4/40
 - 4s - loss: 0.0440 - acc: 0.9756 - val_loss: 0.0733 - val_acc: 0.9688
Epoch 5/40
 - 4s - loss: 0.0418 - acc: 0.9760 - val_loss: 0.0765 - val_acc: 0.9688
Epoch 6/40
 - 4s - loss: 0.0400 - acc: 0.9764 - val_loss: 0.0781 - val_acc: 0.9682
Epoch 7/40
 - 4s - loss: 0.0393 - acc: 0.9764 - val_loss: 0.0810 - val_acc: 0.9687
Epoch 8/40
 - 4s - loss: 0.0383 - acc: 0.9768 - val_loss: 0.0817 - val_acc: 0.9687
Epoch 9/40
 - 4s - loss: 0.0376 - acc: 0.9772 - val_loss: 0.0862 - val_acc: 0.9684
Epoch 10/40
 - 4s - loss: 0.0370 - acc: 0.9773 - val_loss: 0.0858 - val_acc: 0.9694
Epoch 11/40
 - 4s - loss: 0.0366 - acc: 0.9777 - val_loss: 0.0873 - val_acc: 0.9688
Epoch 12/40
 - 4s - loss: 0.0361 - acc: 0.9774 - val_loss: 0.0890 - val_acc: 0.9683
Epoch 13/40
 - 4s - loss: 0.0356 - acc: 0.9778 - val_loss: 0.0883 - val_acc: 0.9683
Epoch 14/40
 - 4s - loss: 0.0355 - acc: 0.9777 - val_loss: 0.0902 - val_acc: 0.9686
Epoch 15/40
 - 4s - loss: 0.0351 - acc: 0.9782 - val_loss: 0.0915 - val_acc: 0.9688
Epoch 16/40
 - 4s - loss: 0.0349 - acc: 0.9777 - val_loss: 0.0921 - val_acc: 0.9685
Epoch 17/40
 - 4s - loss: 0.0347 - acc: 0.9778 - val_loss: 0.0926 - val_acc: 0.9689
Epoch 18/40
 - 4s - loss: 0.0344 - acc: 0.9784 - val_loss: 0.0946 - val_acc: 0.9689
Epoch 19/40
 - 4s - loss: 0.0342 - acc: 0.9786 - val_loss: 0.0950 - val_acc: 0.9685
Epoch 20/40
 - 4s - loss: 0.0341 - acc: 0.9778 - val_loss: 0.0933 - val_acc: 0.9684
Epoch 21/40
 - 4s - loss: 0.0339 - acc: 0.9782 - val_loss: 0.0950 - val_acc: 0.9683
Epoch 22/40
 - 4s - loss: 0.0337 - acc: 0.9786 - val_loss: 0.0959 - val_acc: 0.9681
Epoch 23/40
 - 4s - loss: 0.0337 - acc: 0.9783 - val_loss: 0.0970 - val_acc: 0.9687
Epoch 24/40
 - 4s - loss: 0.0332 - acc: 0.9785 - val_loss: 0.0982 - val_acc: 0.9685
Epoch 25/40
 - 4s - loss: 0.0333 - acc: 0.9786 - val_loss: 0.0987 - val_acc: 0.9686
Epoch 26/40
 - 4s - loss: 0.0331 - acc: 0.9790 - val_loss: 0.1007 - val_acc: 0.9686
Epoch 27/40
 - 4s - loss: 0.0331 - acc: 0.9787 - val_loss: 0.0993 - val_acc: 0.9684
Epoch 28/40
 - 4s - loss: 0.0330 - acc: 0.9788 - val_loss: 0.1004 - val_acc: 0.9687
Epoch 29/40
 - 4s - loss: 0.0329 - acc: 0.9785 - val_loss: 0.1020 - val_acc: 0.9688
Epoch 30/40
 - 4s - loss: 0.0329 - acc: 0.9787 - val_loss: 0.1009 - val_acc: 0.9681
Epoch 31/40
 - 4s - loss: 0.0329 - acc: 0.9791 - val_loss: 0.1015 - val_acc: 0.9685
Epoch 32/40
 - 4s - loss: 0.0327 - acc: 0.9788 - val_loss: 0.1031 - val_acc: 0.9685
Epoch 33/40
 - 4s - loss: 0.0325 - acc: 0.9789 - val_loss: 0.1031 - val_acc: 0.9688
Epoch 34/40
 - 4s - loss: 0.0325 - acc: 0.9787 - val_loss: 0.1045 - val_acc: 0.9688
Epoch 35/40
 - 4s - loss: 0.0325 - acc: 0.9791 - val_loss: 0.1062 - val_acc: 0.9690
Epoch 36/40
 - 4s - loss: 0.0325 - acc: 0.9785 - val_loss: 0.1065 - val_acc: 0.9686
Epoch 37/40
 - 4s - loss: 0.0324 - acc: 0.9790 - val_loss: 0.1062 - val_acc: 0.9686
Epoch 38/40
 - 4s - loss: 0.0324 - acc: 0.9790 - val_loss: 0.1060 - val_acc: 0.9687
Epoch 39/40
 - 4s - loss: 0.0322 - acc: 0.9788 - val_loss: 0.1053 - val_acc: 0.9685
Epoch 40/40
 - 4s - loss: 0.0323 - acc: 0.9791 - val_loss: 0.1078 - val_acc: 0.9685
Epoch 1/40
 - 0s - loss: 0.0868 - acc: 0.9670
Epoch 2/40
 - 0s - loss: 0.0549 - acc: 0.9730
Epoch 3/40
 - 0s - loss: 0.0468 - acc: 0.9749
Epoch 4/40
 - 0s - loss: 0.0448 - acc: 0.9757
Epoch 5/40
 - 0s - loss: 0.0412 - acc: 0.9770
Epoch 6/40
 - 0s - loss: 0.0405 - acc: 0.9769
Epoch 7/40
 - 0s - loss: 0.0385 - acc: 0.9769
Epoch 8/40
 - 0s - loss: 0.0369 - acc: 0.9771
Epoch 9/40
 - 0s - loss: 0.0362 - acc: 0.9782
Epoch 10/40
 - 0s - loss: 0.0358 - acc: 0.9782
Epoch 11/40
 - 0s - loss: 0.0355 - acc: 0.9776
Epoch 12/40
 - 0s - loss: 0.0350 - acc: 0.9779
Epoch 13/40
 - 0s - loss: 0.0348 - acc: 0.9777
Epoch 14/40
 - 0s - loss: 0.0342 - acc: 0.9779
Epoch 15/40
 - 0s - loss: 0.0344 - acc: 0.9786
Epoch 16/40
 - 0s - loss: 0.0339 - acc: 0.9781
Epoch 17/40
 - 0s - loss: 0.0336 - acc: 0.9799
Epoch 18/40
 - 0s - loss: 0.0333 - acc: 0.9788
Epoch 19/40
 - 0s - loss: 0.0334 - acc: 0.9772
Epoch 20/40
 - 0s - loss: 0.0332 - acc: 0.9786
Epoch 21/40
 - 0s - loss: 0.0328 - acc: 0.9785
Epoch 22/40
 - 0s - loss: 0.0325 - acc: 0.9786
Epoch 23/40
 - 0s - loss: 0.0322 - acc: 0.9785
Epoch 24/40
 - 0s - loss: 0.0329 - acc: 0.9782
Epoch 25/40
 - 0s - loss: 0.0324 - acc: 0.9787
Epoch 26/40
 - 0s - loss: 0.0325 - acc: 0.9786
Epoch 27/40
 - 0s - loss: 0.0324 - acc: 0.9782
Epoch 28/40
 - 0s - loss: 0.0320 - acc: 0.9793
Epoch 29/40
 - 0s - loss: 0.0322 - acc: 0.9792
Epoch 30/40
 - 0s - loss: 0.0320 - acc: 0.9791
Epoch 31/40
 - 0s - loss: 0.0316 - acc: 0.9795
Epoch 32/40
 - 0s - loss: 0.0322 - acc: 0.9787
Epoch 33/40
 - 0s - loss: 0.0320 - acc: 0.9797
Epoch 34/40
 - 0s - loss: 0.0318 - acc: 0.9786
Epoch 35/40
 - 0s - loss: 0.0313 - acc: 0.9782
Epoch 36/40
 - 0s - loss: 0.0314 - acc: 0.9794
Epoch 37/40
 - 0s - loss: 0.0321 - acc: 0.9795
Epoch 38/40
 - 0s - loss: 0.0316 - acc: 0.9794
Epoch 39/40
 - 0s - loss: 0.0316 - acc: 0.9778
Epoch 40/40
 - 0s - loss: 0.0317 - acc: 0.9801
# Training time = 0:03:27.214372
# F-Score(Ordinary) = 0.481, Recall: 0.776, Precision: 0.349
# F-Score(lvc) = 0.537, Recall: 0.626, Precision: 0.47
# F-Score(ireflv) = 0.56, Recall: 0.85, Precision: 0.418
# F-Score(id) = 0.34, Recall: 0.952, Precision: 0.207
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_191 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_192 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_191 (Embedding)       (None, 4, 48)        705264      input_191[0][0]                  
__________________________________________________________________________________________________
embedding_192 (Embedding)       (None, 4, 24)        5640        input_192[0][0]                  
__________________________________________________________________________________________________
flatten_191 (Flatten)           (None, 192)          0           embedding_191[0][0]              
__________________________________________________________________________________________________
flatten_192 (Flatten)           (None, 96)           0           embedding_192[0][0]              
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 288)          0           flatten_191[0][0]                
                                                                 flatten_192[0][0]                
__________________________________________________________________________________________________
dense_191 (Dense)               (None, 24)           6936        concatenate_96[0][0]             
__________________________________________________________________________________________________
dropout_96 (Dropout)            (None, 24)           0           dense_191[0][0]                  
__________________________________________________________________________________________________
dense_192 (Dense)               (None, 8)            200         dropout_96[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0841 - acc: 0.9654 - val_loss: 0.0703 - val_acc: 0.9684
Epoch 2/40
 - 4s - loss: 0.0518 - acc: 0.9736 - val_loss: 0.0713 - val_acc: 0.9690
Epoch 3/40
 - 4s - loss: 0.0446 - acc: 0.9752 - val_loss: 0.0781 - val_acc: 0.9689
Epoch 4/40
 - 4s - loss: 0.0414 - acc: 0.9765 - val_loss: 0.0791 - val_acc: 0.9687
Epoch 5/40
 - 4s - loss: 0.0392 - acc: 0.9765 - val_loss: 0.0847 - val_acc: 0.9695
Epoch 6/40
 - 4s - loss: 0.0376 - acc: 0.9770 - val_loss: 0.0855 - val_acc: 0.9686
Epoch 7/40
 - 4s - loss: 0.0367 - acc: 0.9774 - val_loss: 0.0875 - val_acc: 0.9690
Epoch 8/40
 - 4s - loss: 0.0359 - acc: 0.9771 - val_loss: 0.0945 - val_acc: 0.9693
Epoch 9/40
 - 4s - loss: 0.0354 - acc: 0.9777 - val_loss: 0.0933 - val_acc: 0.9682
Epoch 10/40
 - 4s - loss: 0.0348 - acc: 0.9777 - val_loss: 0.0934 - val_acc: 0.9684
Epoch 11/40
 - 4s - loss: 0.0342 - acc: 0.9778 - val_loss: 0.0985 - val_acc: 0.9679
Epoch 12/40
 - 4s - loss: 0.0337 - acc: 0.9783 - val_loss: 0.0969 - val_acc: 0.9685
Epoch 13/40
 - 4s - loss: 0.0337 - acc: 0.9782 - val_loss: 0.0973 - val_acc: 0.9687
Epoch 14/40
 - 4s - loss: 0.0332 - acc: 0.9785 - val_loss: 0.1002 - val_acc: 0.9690
Epoch 15/40
 - 4s - loss: 0.0332 - acc: 0.9783 - val_loss: 0.1024 - val_acc: 0.9687
Epoch 16/40
 - 4s - loss: 0.0328 - acc: 0.9780 - val_loss: 0.1009 - val_acc: 0.9681
Epoch 17/40
 - 4s - loss: 0.0329 - acc: 0.9787 - val_loss: 0.1011 - val_acc: 0.9684
Epoch 18/40
 - 4s - loss: 0.0327 - acc: 0.9788 - val_loss: 0.1035 - val_acc: 0.9684
Epoch 19/40
 - 4s - loss: 0.0325 - acc: 0.9784 - val_loss: 0.1037 - val_acc: 0.9686
Epoch 20/40
 - 4s - loss: 0.0323 - acc: 0.9787 - val_loss: 0.1042 - val_acc: 0.9692
Epoch 21/40
 - 4s - loss: 0.0321 - acc: 0.9790 - val_loss: 0.1054 - val_acc: 0.9685
Epoch 22/40
 - 4s - loss: 0.0319 - acc: 0.9793 - val_loss: 0.1054 - val_acc: 0.9687
Epoch 23/40
 - 4s - loss: 0.0318 - acc: 0.9788 - val_loss: 0.1075 - val_acc: 0.9684
Epoch 24/40
 - 4s - loss: 0.0319 - acc: 0.9788 - val_loss: 0.1080 - val_acc: 0.9689
Epoch 25/40
 - 4s - loss: 0.0319 - acc: 0.9788 - val_loss: 0.1067 - val_acc: 0.9694
Epoch 26/40
 - 4s - loss: 0.0316 - acc: 0.9792 - val_loss: 0.1103 - val_acc: 0.9687
Epoch 27/40
 - 4s - loss: 0.0315 - acc: 0.9790 - val_loss: 0.1111 - val_acc: 0.9687
Epoch 28/40
 - 4s - loss: 0.0315 - acc: 0.9790 - val_loss: 0.1119 - val_acc: 0.9687
Epoch 29/40
 - 4s - loss: 0.0315 - acc: 0.9790 - val_loss: 0.1097 - val_acc: 0.9688
Epoch 30/40
 - 4s - loss: 0.0313 - acc: 0.9790 - val_loss: 0.1108 - val_acc: 0.9694
Epoch 31/40
 - 4s - loss: 0.0311 - acc: 0.9789 - val_loss: 0.1104 - val_acc: 0.9688
Epoch 32/40
 - 4s - loss: 0.0312 - acc: 0.9792 - val_loss: 0.1128 - val_acc: 0.9687
Epoch 33/40
 - 4s - loss: 0.0313 - acc: 0.9788 - val_loss: 0.1125 - val_acc: 0.9692
Epoch 34/40
 - 4s - loss: 0.0311 - acc: 0.9788 - val_loss: 0.1142 - val_acc: 0.9693
Epoch 35/40
 - 4s - loss: 0.0311 - acc: 0.9796 - val_loss: 0.1127 - val_acc: 0.9690
Epoch 36/40
 - 4s - loss: 0.0309 - acc: 0.9796 - val_loss: 0.1124 - val_acc: 0.9688
Epoch 37/40
 - 4s - loss: 0.0309 - acc: 0.9791 - val_loss: 0.1154 - val_acc: 0.9689
Epoch 38/40
 - 4s - loss: 0.0310 - acc: 0.9793 - val_loss: 0.1138 - val_acc: 0.9690
Epoch 39/40
 - 4s - loss: 0.0310 - acc: 0.9789 - val_loss: 0.1129 - val_acc: 0.9686
Epoch 40/40
 - 4s - loss: 0.0310 - acc: 0.9793 - val_loss: 0.1168 - val_acc: 0.9689
Epoch 1/40
 - 0s - loss: 0.0936 - acc: 0.9668
Epoch 2/40
 - 0s - loss: 0.0559 - acc: 0.9728
Epoch 3/40
 - 0s - loss: 0.0490 - acc: 0.9741
Epoch 4/40
 - 0s - loss: 0.0448 - acc: 0.9747
Epoch 5/40
 - 0s - loss: 0.0430 - acc: 0.9759
Epoch 6/40
 - 0s - loss: 0.0409 - acc: 0.9750
Epoch 7/40
 - 0s - loss: 0.0395 - acc: 0.9751
Epoch 8/40
 - 0s - loss: 0.0394 - acc: 0.9764
Epoch 9/40
 - 0s - loss: 0.0381 - acc: 0.9769
Epoch 10/40
 - 0s - loss: 0.0376 - acc: 0.9772
Epoch 11/40
 - 0s - loss: 0.0366 - acc: 0.9768
Epoch 12/40
 - 0s - loss: 0.0360 - acc: 0.9784
Epoch 13/40
 - 0s - loss: 0.0356 - acc: 0.9779
Epoch 14/40
 - 0s - loss: 0.0346 - acc: 0.9766
Epoch 15/40
 - 0s - loss: 0.0341 - acc: 0.9773
Epoch 16/40
 - 0s - loss: 0.0340 - acc: 0.9777
Epoch 17/40
 - 0s - loss: 0.0334 - acc: 0.9775
Epoch 18/40
 - 0s - loss: 0.0337 - acc: 0.9778
Epoch 19/40
 - 0s - loss: 0.0332 - acc: 0.9771
Epoch 20/40
 - 0s - loss: 0.0332 - acc: 0.9789
Epoch 21/40
 - 0s - loss: 0.0327 - acc: 0.9785
Epoch 22/40
 - 0s - loss: 0.0336 - acc: 0.9779
Epoch 23/40
 - 0s - loss: 0.0325 - acc: 0.9785
Epoch 24/40
 - 0s - loss: 0.0329 - acc: 0.9767
Epoch 25/40
 - 0s - loss: 0.0326 - acc: 0.9776
Epoch 26/40
 - 0s - loss: 0.0324 - acc: 0.9784
Epoch 27/40
 - 0s - loss: 0.0326 - acc: 0.9786
Epoch 28/40
 - 0s - loss: 0.0322 - acc: 0.9778
Epoch 29/40
 - 0s - loss: 0.0325 - acc: 0.9785
Epoch 30/40
 - 0s - loss: 0.0322 - acc: 0.9777
Epoch 31/40
 - 0s - loss: 0.0319 - acc: 0.9794
Epoch 32/40
 - 0s - loss: 0.0323 - acc: 0.9778
Epoch 33/40
 - 0s - loss: 0.0322 - acc: 0.9785
Epoch 34/40
 - 0s - loss: 0.0318 - acc: 0.9792
Epoch 35/40
 - 0s - loss: 0.0321 - acc: 0.9780
Epoch 36/40
 - 0s - loss: 0.0317 - acc: 0.9790
Epoch 37/40
 - 0s - loss: 0.0315 - acc: 0.9796
Epoch 38/40
 - 0s - loss: 0.0319 - acc: 0.9789
Epoch 39/40
 - 0s - loss: 0.0315 - acc: 0.9788
Epoch 40/40
 - 0s - loss: 0.0318 - acc: 0.9780
# Training time = 0:03:28.131772
# F-Score(Ordinary) = 0.009, Recall: 1.0, Precision: 0.004
# F-Score(ireflv) = 0.032, Recall: 1.0, Precision: 0.016
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_193 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_194 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_193 (Embedding)       (None, 4, 48)        705264      input_193[0][0]                  
__________________________________________________________________________________________________
embedding_194 (Embedding)       (None, 4, 24)        5640        input_194[0][0]                  
__________________________________________________________________________________________________
flatten_193 (Flatten)           (None, 192)          0           embedding_193[0][0]              
__________________________________________________________________________________________________
flatten_194 (Flatten)           (None, 96)           0           embedding_194[0][0]              
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 288)          0           flatten_193[0][0]                
                                                                 flatten_194[0][0]                
__________________________________________________________________________________________________
dense_193 (Dense)               (None, 24)           6936        concatenate_97[0][0]             
__________________________________________________________________________________________________
dropout_97 (Dropout)            (None, 24)           0           dense_193[0][0]                  
__________________________________________________________________________________________________
dense_194 (Dense)               (None, 8)            200         dropout_97[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0896 - acc: 0.9645 - val_loss: 0.0697 - val_acc: 0.9681
Epoch 2/40
 - 4s - loss: 0.0551 - acc: 0.9729 - val_loss: 0.0723 - val_acc: 0.9685
Epoch 3/40
 - 4s - loss: 0.0474 - acc: 0.9743 - val_loss: 0.0743 - val_acc: 0.9681
Epoch 4/40
 - 4s - loss: 0.0437 - acc: 0.9761 - val_loss: 0.0791 - val_acc: 0.9686
Epoch 5/40
 - 4s - loss: 0.0409 - acc: 0.9766 - val_loss: 0.0822 - val_acc: 0.9686
Epoch 6/40
 - 4s - loss: 0.0392 - acc: 0.9771 - val_loss: 0.0821 - val_acc: 0.9689
Epoch 7/40
 - 4s - loss: 0.0381 - acc: 0.9771 - val_loss: 0.0852 - val_acc: 0.9683
Epoch 8/40
 - 4s - loss: 0.0373 - acc: 0.9772 - val_loss: 0.0894 - val_acc: 0.9692
Epoch 9/40
 - 4s - loss: 0.0364 - acc: 0.9777 - val_loss: 0.0882 - val_acc: 0.9689
Epoch 10/40
 - 4s - loss: 0.0359 - acc: 0.9775 - val_loss: 0.0929 - val_acc: 0.9693
Epoch 11/40
 - 4s - loss: 0.0356 - acc: 0.9778 - val_loss: 0.0962 - val_acc: 0.9694
Epoch 12/40
 - 4s - loss: 0.0352 - acc: 0.9778 - val_loss: 0.0964 - val_acc: 0.9691
Epoch 13/40
 - 4s - loss: 0.0349 - acc: 0.9780 - val_loss: 0.0967 - val_acc: 0.9688
Epoch 14/40
 - 4s - loss: 0.0344 - acc: 0.9782 - val_loss: 0.1013 - val_acc: 0.9691
Epoch 15/40
 - 4s - loss: 0.0344 - acc: 0.9780 - val_loss: 0.0978 - val_acc: 0.9693
Epoch 16/40
 - 4s - loss: 0.0341 - acc: 0.9779 - val_loss: 0.1015 - val_acc: 0.9692
Epoch 17/40
 - 4s - loss: 0.0337 - acc: 0.9780 - val_loss: 0.1042 - val_acc: 0.9688
Epoch 18/40
 - 4s - loss: 0.0335 - acc: 0.9790 - val_loss: 0.1061 - val_acc: 0.9688
Epoch 19/40
 - 4s - loss: 0.0336 - acc: 0.9786 - val_loss: 0.1043 - val_acc: 0.9686
Epoch 20/40
 - 4s - loss: 0.0333 - acc: 0.9785 - val_loss: 0.1070 - val_acc: 0.9688
Epoch 21/40
 - 4s - loss: 0.0330 - acc: 0.9784 - val_loss: 0.1066 - val_acc: 0.9690
Epoch 22/40
 - 4s - loss: 0.0331 - acc: 0.9788 - val_loss: 0.1081 - val_acc: 0.9688
Epoch 23/40
 - 4s - loss: 0.0330 - acc: 0.9787 - val_loss: 0.1079 - val_acc: 0.9691
Epoch 24/40
 - 4s - loss: 0.0329 - acc: 0.9788 - val_loss: 0.1116 - val_acc: 0.9686
Epoch 25/40
 - 4s - loss: 0.0327 - acc: 0.9787 - val_loss: 0.1107 - val_acc: 0.9687
Epoch 26/40
 - 4s - loss: 0.0325 - acc: 0.9788 - val_loss: 0.1120 - val_acc: 0.9694
Epoch 27/40
 - 4s - loss: 0.0327 - acc: 0.9785 - val_loss: 0.1125 - val_acc: 0.9689
Epoch 28/40
 - 4s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1113 - val_acc: 0.9688
Epoch 29/40
 - 4s - loss: 0.0321 - acc: 0.9789 - val_loss: 0.1140 - val_acc: 0.9690
Epoch 30/40
 - 4s - loss: 0.0323 - acc: 0.9791 - val_loss: 0.1129 - val_acc: 0.9687
Epoch 31/40
 - 4s - loss: 0.0320 - acc: 0.9792 - val_loss: 0.1123 - val_acc: 0.9692
Epoch 32/40
 - 4s - loss: 0.0320 - acc: 0.9791 - val_loss: 0.1117 - val_acc: 0.9690
Epoch 33/40
 - 4s - loss: 0.0319 - acc: 0.9788 - val_loss: 0.1131 - val_acc: 0.9688
Epoch 34/40
 - 4s - loss: 0.0317 - acc: 0.9792 - val_loss: 0.1153 - val_acc: 0.9690
Epoch 35/40
 - 4s - loss: 0.0317 - acc: 0.9788 - val_loss: 0.1142 - val_acc: 0.9686
Epoch 36/40
 - 4s - loss: 0.0314 - acc: 0.9791 - val_loss: 0.1167 - val_acc: 0.9685
Epoch 37/40
 - 4s - loss: 0.0316 - acc: 0.9791 - val_loss: 0.1190 - val_acc: 0.9686
Epoch 38/40
 - 4s - loss: 0.0315 - acc: 0.9789 - val_loss: 0.1182 - val_acc: 0.9689
Epoch 39/40
 - 4s - loss: 0.0313 - acc: 0.9790 - val_loss: 0.1187 - val_acc: 0.9692
Epoch 40/40
 - 4s - loss: 0.0313 - acc: 0.9792 - val_loss: 0.1178 - val_acc: 0.9688
Epoch 1/40
 - 0s - loss: 0.0916 - acc: 0.9679
Epoch 2/40
 - 0s - loss: 0.0596 - acc: 0.9712
Epoch 3/40
 - 0s - loss: 0.0487 - acc: 0.9743
Epoch 4/40
 - 0s - loss: 0.0460 - acc: 0.9750
Epoch 5/40
 - 0s - loss: 0.0435 - acc: 0.9766
Epoch 6/40
 - 0s - loss: 0.0422 - acc: 0.9762
Epoch 7/40
 - 0s - loss: 0.0416 - acc: 0.9765
Epoch 8/40
 - 0s - loss: 0.0396 - acc: 0.9775
Epoch 9/40
 - 0s - loss: 0.0375 - acc: 0.9771
Epoch 10/40
 - 0s - loss: 0.0373 - acc: 0.9778
Epoch 11/40
 - 0s - loss: 0.0370 - acc: 0.9770
Epoch 12/40
 - 0s - loss: 0.0365 - acc: 0.9772
Epoch 13/40
 - 0s - loss: 0.0350 - acc: 0.9777
Epoch 14/40
 - 0s - loss: 0.0345 - acc: 0.9773
Epoch 15/40
 - 0s - loss: 0.0346 - acc: 0.9774
Epoch 16/40
 - 0s - loss: 0.0348 - acc: 0.9773
Epoch 17/40
 - 0s - loss: 0.0340 - acc: 0.9781
Epoch 18/40
 - 0s - loss: 0.0334 - acc: 0.9788
Epoch 19/40
 - 0s - loss: 0.0347 - acc: 0.9779
Epoch 20/40
 - 0s - loss: 0.0335 - acc: 0.9772
Epoch 21/40
 - 0s - loss: 0.0329 - acc: 0.9781
Epoch 22/40
 - 0s - loss: 0.0334 - acc: 0.9794
Epoch 23/40
 - 0s - loss: 0.0333 - acc: 0.9774
Epoch 24/40
 - 0s - loss: 0.0330 - acc: 0.9777
Epoch 25/40
 - 0s - loss: 0.0329 - acc: 0.9777
Epoch 26/40
 - 0s - loss: 0.0331 - acc: 0.9786
Epoch 27/40
 - 0s - loss: 0.0326 - acc: 0.9780
Epoch 28/40
 - 0s - loss: 0.0317 - acc: 0.9788
Epoch 29/40
 - 0s - loss: 0.0313 - acc: 0.9790
Epoch 30/40
 - 0s - loss: 0.0315 - acc: 0.9790
Epoch 31/40
 - 0s - loss: 0.0315 - acc: 0.9788
Epoch 32/40
 - 0s - loss: 0.0311 - acc: 0.9786
Epoch 33/40
 - 0s - loss: 0.0315 - acc: 0.9794
Epoch 34/40
 - 0s - loss: 0.0317 - acc: 0.9789
Epoch 35/40
 - 0s - loss: 0.0318 - acc: 0.9794
Epoch 36/40
 - 0s - loss: 0.0316 - acc: 0.9775
Epoch 37/40
 - 0s - loss: 0.0313 - acc: 0.9787
Epoch 38/40
 - 0s - loss: 0.0311 - acc: 0.9787
Epoch 39/40
 - 0s - loss: 0.0315 - acc: 0.9788
Epoch 40/40
 - 0s - loss: 0.0316 - acc: 0.9788
# Training time = 0:03:28.921722
# F-Score(Ordinary) = 0.222, Recall: 0.559, Precision: 0.139
# F-Score(lvc) = 0.104, Recall: 0.164, Precision: 0.076
# F-Score(ireflv) = 0.032, Recall: 0.667, Precision: 0.016
# F-Score(id) = 0.367, Recall: 0.936, Precision: 0.228
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_195 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_196 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_195 (Embedding)       (None, 4, 48)        705264      input_195[0][0]                  
__________________________________________________________________________________________________
embedding_196 (Embedding)       (None, 4, 24)        5640        input_196[0][0]                  
__________________________________________________________________________________________________
flatten_195 (Flatten)           (None, 192)          0           embedding_195[0][0]              
__________________________________________________________________________________________________
flatten_196 (Flatten)           (None, 96)           0           embedding_196[0][0]              
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 288)          0           flatten_195[0][0]                
                                                                 flatten_196[0][0]                
__________________________________________________________________________________________________
dense_195 (Dense)               (None, 24)           6936        concatenate_98[0][0]             
__________________________________________________________________________________________________
dropout_98 (Dropout)            (None, 24)           0           dense_195[0][0]                  
__________________________________________________________________________________________________
dense_196 (Dense)               (None, 8)            200         dropout_98[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0841 - acc: 0.9650 - val_loss: 0.0692 - val_acc: 0.9688
Epoch 2/40
 - 4s - loss: 0.0524 - acc: 0.9727 - val_loss: 0.0698 - val_acc: 0.9687
Epoch 3/40
 - 4s - loss: 0.0456 - acc: 0.9751 - val_loss: 0.0742 - val_acc: 0.9682
Epoch 4/40
 - 4s - loss: 0.0424 - acc: 0.9757 - val_loss: 0.0799 - val_acc: 0.9693
Epoch 5/40
 - 4s - loss: 0.0399 - acc: 0.9766 - val_loss: 0.0807 - val_acc: 0.9688
Epoch 6/40
 - 4s - loss: 0.0382 - acc: 0.9769 - val_loss: 0.0836 - val_acc: 0.9693
Epoch 7/40
 - 4s - loss: 0.0375 - acc: 0.9779 - val_loss: 0.0874 - val_acc: 0.9693
Epoch 8/40
 - 4s - loss: 0.0365 - acc: 0.9776 - val_loss: 0.0895 - val_acc: 0.9694
Epoch 9/40
 - 4s - loss: 0.0361 - acc: 0.9775 - val_loss: 0.0900 - val_acc: 0.9690
Epoch 10/40
 - 4s - loss: 0.0354 - acc: 0.9779 - val_loss: 0.0923 - val_acc: 0.9687
Epoch 11/40
 - 4s - loss: 0.0351 - acc: 0.9781 - val_loss: 0.0933 - val_acc: 0.9695
Epoch 12/40
 - 4s - loss: 0.0343 - acc: 0.9784 - val_loss: 0.0993 - val_acc: 0.9691
Epoch 13/40
 - 4s - loss: 0.0342 - acc: 0.9781 - val_loss: 0.0972 - val_acc: 0.9695
Epoch 14/40
 - 4s - loss: 0.0339 - acc: 0.9787 - val_loss: 0.1003 - val_acc: 0.9694
Epoch 15/40
 - 4s - loss: 0.0338 - acc: 0.9782 - val_loss: 0.0986 - val_acc: 0.9688
Epoch 16/40
 - 4s - loss: 0.0334 - acc: 0.9783 - val_loss: 0.1007 - val_acc: 0.9692
Epoch 17/40
 - 4s - loss: 0.0332 - acc: 0.9784 - val_loss: 0.1007 - val_acc: 0.9687
Epoch 18/40
 - 4s - loss: 0.0328 - acc: 0.9785 - val_loss: 0.1020 - val_acc: 0.9695
Epoch 19/40
 - 4s - loss: 0.0326 - acc: 0.9784 - val_loss: 0.1043 - val_acc: 0.9689
Epoch 20/40
 - 4s - loss: 0.0325 - acc: 0.9788 - val_loss: 0.1038 - val_acc: 0.9690
Epoch 21/40
 - 4s - loss: 0.0321 - acc: 0.9792 - val_loss: 0.1062 - val_acc: 0.9689
Epoch 22/40
 - 4s - loss: 0.0321 - acc: 0.9786 - val_loss: 0.1070 - val_acc: 0.9693
Epoch 23/40
 - 4s - loss: 0.0322 - acc: 0.9790 - val_loss: 0.1064 - val_acc: 0.9688
Epoch 24/40
 - 4s - loss: 0.0319 - acc: 0.9790 - val_loss: 0.1066 - val_acc: 0.9699
Epoch 25/40
 - 4s - loss: 0.0317 - acc: 0.9793 - val_loss: 0.1091 - val_acc: 0.9695
Epoch 26/40
 - 4s - loss: 0.0316 - acc: 0.9787 - val_loss: 0.1100 - val_acc: 0.9694
Epoch 27/40
 - 4s - loss: 0.0317 - acc: 0.9790 - val_loss: 0.1089 - val_acc: 0.9695
Epoch 28/40
 - 4s - loss: 0.0314 - acc: 0.9795 - val_loss: 0.1109 - val_acc: 0.9690
Epoch 29/40
 - 4s - loss: 0.0318 - acc: 0.9793 - val_loss: 0.1105 - val_acc: 0.9691
Epoch 30/40
 - 4s - loss: 0.0314 - acc: 0.9794 - val_loss: 0.1107 - val_acc: 0.9694
Epoch 31/40
 - 4s - loss: 0.0316 - acc: 0.9790 - val_loss: 0.1117 - val_acc: 0.9695
Epoch 32/40
 - 4s - loss: 0.0314 - acc: 0.9793 - val_loss: 0.1115 - val_acc: 0.9694
Epoch 33/40
 - 4s - loss: 0.0312 - acc: 0.9792 - val_loss: 0.1131 - val_acc: 0.9695
Epoch 34/40
 - 4s - loss: 0.0310 - acc: 0.9798 - val_loss: 0.1143 - val_acc: 0.9692
Epoch 35/40
 - 4s - loss: 0.0310 - acc: 0.9794 - val_loss: 0.1121 - val_acc: 0.9691
Epoch 36/40
 - 4s - loss: 0.0312 - acc: 0.9791 - val_loss: 0.1131 - val_acc: 0.9695
Epoch 37/40
 - 4s - loss: 0.0308 - acc: 0.9796 - val_loss: 0.1144 - val_acc: 0.9692
Epoch 38/40
 - 4s - loss: 0.0309 - acc: 0.9794 - val_loss: 0.1171 - val_acc: 0.9695
Epoch 39/40
 - 4s - loss: 0.0309 - acc: 0.9795 - val_loss: 0.1160 - val_acc: 0.9689
Epoch 40/40
 - 4s - loss: 0.0310 - acc: 0.9794 - val_loss: 0.1151 - val_acc: 0.9694
Epoch 1/40
 - 0s - loss: 0.0922 - acc: 0.9668
Epoch 2/40
 - 0s - loss: 0.0567 - acc: 0.9731
Epoch 3/40
 - 0s - loss: 0.0458 - acc: 0.9750
Epoch 4/40
 - 0s - loss: 0.0434 - acc: 0.9747
Epoch 5/40
 - 0s - loss: 0.0404 - acc: 0.9759
Epoch 6/40
 - 0s - loss: 0.0391 - acc: 0.9758
Epoch 7/40
 - 0s - loss: 0.0378 - acc: 0.9760
Epoch 8/40
 - 0s - loss: 0.0369 - acc: 0.9771
Epoch 9/40
 - 0s - loss: 0.0359 - acc: 0.9782
Epoch 10/40
 - 0s - loss: 0.0356 - acc: 0.9783
Epoch 11/40
 - 0s - loss: 0.0350 - acc: 0.9775
Epoch 12/40
 - 0s - loss: 0.0344 - acc: 0.9779
Epoch 13/40
 - 0s - loss: 0.0341 - acc: 0.9789
Epoch 14/40
 - 0s - loss: 0.0334 - acc: 0.9784
Epoch 15/40
 - 0s - loss: 0.0329 - acc: 0.9786
Epoch 16/40
 - 0s - loss: 0.0327 - acc: 0.9783
Epoch 17/40
 - 0s - loss: 0.0323 - acc: 0.9787
Epoch 18/40
 - 0s - loss: 0.0328 - acc: 0.9783
Epoch 19/40
 - 0s - loss: 0.0322 - acc: 0.9791
Epoch 20/40
 - 0s - loss: 0.0324 - acc: 0.9786
Epoch 21/40
 - 0s - loss: 0.0321 - acc: 0.9786
Epoch 22/40
 - 0s - loss: 0.0321 - acc: 0.9797
Epoch 23/40
 - 0s - loss: 0.0315 - acc: 0.9793
Epoch 24/40
 - 0s - loss: 0.0319 - acc: 0.9782
Epoch 25/40
 - 0s - loss: 0.0321 - acc: 0.9781
Epoch 26/40
 - 0s - loss: 0.0317 - acc: 0.9786
Epoch 27/40
 - 0s - loss: 0.0317 - acc: 0.9790
Epoch 28/40
 - 0s - loss: 0.0318 - acc: 0.9788
Epoch 29/40
 - 0s - loss: 0.0319 - acc: 0.9781
Epoch 30/40
 - 0s - loss: 0.0314 - acc: 0.9792
Epoch 31/40
 - 0s - loss: 0.0311 - acc: 0.9791
Epoch 32/40
 - 0s - loss: 0.0315 - acc: 0.9782
Epoch 33/40
 - 0s - loss: 0.0311 - acc: 0.9794
Epoch 34/40
 - 0s - loss: 0.0310 - acc: 0.9791
Epoch 35/40
 - 0s - loss: 0.0315 - acc: 0.9788
Epoch 36/40
 - 0s - loss: 0.0312 - acc: 0.9786
Epoch 37/40
 - 0s - loss: 0.0314 - acc: 0.9797
Epoch 38/40
 - 0s - loss: 0.0311 - acc: 0.9800
Epoch 39/40
 - 0s - loss: 0.0310 - acc: 0.9789
Epoch 40/40
 - 0s - loss: 0.0309 - acc: 0.9788
# Training time = 0:03:29.171730
# F-Score(Ordinary) = 0.076, Recall: 0.643, Precision: 0.04
# F-Score(lvc) = 0.059, Recall: 1.0, Precision: 0.03
# F-Score(ireflv) = 0.162, Recall: 0.786, Precision: 0.09
# F-Score(id) = 0.03, Recall: 0.3, Precision: 0.016
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_197 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_198 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_197 (Embedding)       (None, 4, 48)        705264      input_197[0][0]                  
__________________________________________________________________________________________________
embedding_198 (Embedding)       (None, 4, 24)        5640        input_198[0][0]                  
__________________________________________________________________________________________________
flatten_197 (Flatten)           (None, 192)          0           embedding_197[0][0]              
__________________________________________________________________________________________________
flatten_198 (Flatten)           (None, 96)           0           embedding_198[0][0]              
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 288)          0           flatten_197[0][0]                
                                                                 flatten_198[0][0]                
__________________________________________________________________________________________________
dense_197 (Dense)               (None, 24)           6936        concatenate_99[0][0]             
__________________________________________________________________________________________________
dropout_99 (Dropout)            (None, 24)           0           dense_197[0][0]                  
__________________________________________________________________________________________________
dense_198 (Dense)               (None, 8)            200         dropout_99[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0855 - acc: 0.9644 - val_loss: 0.0688 - val_acc: 0.9688
Epoch 2/40
 - 4s - loss: 0.0543 - acc: 0.9728 - val_loss: 0.0709 - val_acc: 0.9687
Epoch 3/40
 - 4s - loss: 0.0468 - acc: 0.9745 - val_loss: 0.0733 - val_acc: 0.9688
Epoch 4/40
 - 4s - loss: 0.0430 - acc: 0.9759 - val_loss: 0.0800 - val_acc: 0.9687
Epoch 5/40
 - 4s - loss: 0.0404 - acc: 0.9772 - val_loss: 0.0815 - val_acc: 0.9692
Epoch 6/40
 - 4s - loss: 0.0390 - acc: 0.9771 - val_loss: 0.0843 - val_acc: 0.9695
Epoch 7/40
 - 4s - loss: 0.0381 - acc: 0.9772 - val_loss: 0.0869 - val_acc: 0.9688
Epoch 8/40
 - 4s - loss: 0.0370 - acc: 0.9775 - val_loss: 0.0877 - val_acc: 0.9692
Epoch 9/40
 - 4s - loss: 0.0360 - acc: 0.9775 - val_loss: 0.0915 - val_acc: 0.9694
Epoch 10/40
 - 4s - loss: 0.0357 - acc: 0.9780 - val_loss: 0.0913 - val_acc: 0.9688
Epoch 11/40
 - 4s - loss: 0.0351 - acc: 0.9780 - val_loss: 0.0934 - val_acc: 0.9692
Epoch 12/40
 - 4s - loss: 0.0350 - acc: 0.9782 - val_loss: 0.0926 - val_acc: 0.9690
Epoch 13/40
 - 4s - loss: 0.0343 - acc: 0.9780 - val_loss: 0.0977 - val_acc: 0.9692
Epoch 14/40
 - 4s - loss: 0.0342 - acc: 0.9782 - val_loss: 0.0990 - val_acc: 0.9690
Epoch 15/40
 - 4s - loss: 0.0339 - acc: 0.9782 - val_loss: 0.1019 - val_acc: 0.9690
Epoch 16/40
 - 4s - loss: 0.0337 - acc: 0.9784 - val_loss: 0.1015 - val_acc: 0.9691
Epoch 17/40
 - 4s - loss: 0.0336 - acc: 0.9784 - val_loss: 0.1019 - val_acc: 0.9692
Epoch 18/40
 - 4s - loss: 0.0334 - acc: 0.9790 - val_loss: 0.1058 - val_acc: 0.9689
Epoch 19/40
 - 4s - loss: 0.0334 - acc: 0.9787 - val_loss: 0.1039 - val_acc: 0.9694
Epoch 20/40
 - 4s - loss: 0.0329 - acc: 0.9789 - val_loss: 0.1059 - val_acc: 0.9688
Epoch 21/40
 - 4s - loss: 0.0328 - acc: 0.9792 - val_loss: 0.1065 - val_acc: 0.9691
Epoch 22/40
 - 4s - loss: 0.0329 - acc: 0.9789 - val_loss: 0.1087 - val_acc: 0.9690
Epoch 23/40
 - 4s - loss: 0.0329 - acc: 0.9785 - val_loss: 0.1069 - val_acc: 0.9692
Epoch 24/40
 - 4s - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1075 - val_acc: 0.9691
Epoch 25/40
 - 4s - loss: 0.0325 - acc: 0.9788 - val_loss: 0.1111 - val_acc: 0.9693
Epoch 26/40
 - 4s - loss: 0.0324 - acc: 0.9792 - val_loss: 0.1102 - val_acc: 0.9691
Epoch 27/40
 - 4s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1104 - val_acc: 0.9692
Epoch 28/40
 - 4s - loss: 0.0322 - acc: 0.9788 - val_loss: 0.1115 - val_acc: 0.9691
Epoch 29/40
 - 4s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1123 - val_acc: 0.9694
Epoch 30/40
 - 4s - loss: 0.0320 - acc: 0.9791 - val_loss: 0.1115 - val_acc: 0.9694
Epoch 31/40
 - 4s - loss: 0.0319 - acc: 0.9793 - val_loss: 0.1111 - val_acc: 0.9690
Epoch 32/40
 - 4s - loss: 0.0321 - acc: 0.9790 - val_loss: 0.1150 - val_acc: 0.9693
Epoch 33/40
 - 4s - loss: 0.0318 - acc: 0.9792 - val_loss: 0.1151 - val_acc: 0.9689
Epoch 34/40
 - 4s - loss: 0.0319 - acc: 0.9793 - val_loss: 0.1135 - val_acc: 0.9694
Epoch 35/40
 - 4s - loss: 0.0317 - acc: 0.9792 - val_loss: 0.1152 - val_acc: 0.9691
Epoch 36/40
 - 4s - loss: 0.0319 - acc: 0.9794 - val_loss: 0.1163 - val_acc: 0.9695
Epoch 37/40
 - 4s - loss: 0.0317 - acc: 0.9795 - val_loss: 0.1152 - val_acc: 0.9690
Epoch 38/40
 - 4s - loss: 0.0317 - acc: 0.9790 - val_loss: 0.1161 - val_acc: 0.9690
Epoch 39/40
 - 4s - loss: 0.0316 - acc: 0.9793 - val_loss: 0.1170 - val_acc: 0.9693
Epoch 40/40
 - 4s - loss: 0.0317 - acc: 0.9793 - val_loss: 0.1189 - val_acc: 0.9693
Epoch 1/40
 - 0s - loss: 0.0908 - acc: 0.9675
Epoch 2/40
 - 0s - loss: 0.0589 - acc: 0.9732
Epoch 3/40
 - 0s - loss: 0.0494 - acc: 0.9739
Epoch 4/40
 - 0s - loss: 0.0446 - acc: 0.9749
Epoch 5/40
 - 0s - loss: 0.0410 - acc: 0.9757
Epoch 6/40
 - 0s - loss: 0.0403 - acc: 0.9761
Epoch 7/40
 - 0s - loss: 0.0389 - acc: 0.9756
Epoch 8/40
 - 0s - loss: 0.0374 - acc: 0.9770
Epoch 9/40
 - 0s - loss: 0.0359 - acc: 0.9763
Epoch 10/40
 - 0s - loss: 0.0365 - acc: 0.9773
Epoch 11/40
 - 0s - loss: 0.0357 - acc: 0.9767
Epoch 12/40
 - 0s - loss: 0.0353 - acc: 0.9781
Epoch 13/40
 - 0s - loss: 0.0348 - acc: 0.9778
Epoch 14/40
 - 0s - loss: 0.0352 - acc: 0.9775
Epoch 15/40
 - 0s - loss: 0.0341 - acc: 0.9771
Epoch 16/40
 - 0s - loss: 0.0337 - acc: 0.9782
Epoch 17/40
 - 0s - loss: 0.0341 - acc: 0.9779
Epoch 18/40
 - 0s - loss: 0.0346 - acc: 0.9776
Epoch 19/40
 - 0s - loss: 0.0345 - acc: 0.9783
Epoch 20/40
 - 0s - loss: 0.0343 - acc: 0.9779
Epoch 21/40
 - 0s - loss: 0.0334 - acc: 0.9787
Epoch 22/40
 - 0s - loss: 0.0332 - acc: 0.9792
Epoch 23/40
 - 0s - loss: 0.0329 - acc: 0.9785
Epoch 24/40
 - 0s - loss: 0.0328 - acc: 0.9778
Epoch 25/40
 - 0s - loss: 0.0328 - acc: 0.9793
Epoch 26/40
 - 0s - loss: 0.0324 - acc: 0.9785
Epoch 27/40
 - 0s - loss: 0.0329 - acc: 0.9779
Epoch 28/40
 - 0s - loss: 0.0327 - acc: 0.9781
Epoch 29/40
 - 0s - loss: 0.0325 - acc: 0.9795
Epoch 30/40
 - 0s - loss: 0.0328 - acc: 0.9781
Epoch 31/40
 - 0s - loss: 0.0320 - acc: 0.9795
Epoch 32/40
 - 0s - loss: 0.0323 - acc: 0.9787
Epoch 33/40
 - 0s - loss: 0.0324 - acc: 0.9787
Epoch 34/40
 - 0s - loss: 0.0321 - acc: 0.9786
Epoch 35/40
 - 0s - loss: 0.0326 - acc: 0.9790
Epoch 36/40
 - 0s - loss: 0.0320 - acc: 0.9794
Epoch 37/40
 - 0s - loss: 0.0319 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0316 - acc: 0.9780
Epoch 39/40
 - 0s - loss: 0.0323 - acc: 0.9796
Epoch 40/40
 - 0s - loss: 0.0314 - acc: 0.9795
# Training time = 0:03:29.671256
# F-Score(Ordinary) = 0.079, Recall: 0.514, Precision: 0.043
# F-Score(lvc) = 0.176, Recall: 0.813, Precision: 0.098
# F-Score(ireflv) = 0.059, Recall: 0.308, Precision: 0.033
# F-Score(id) = 0.02, Recall: 0.25, Precision: 0.01
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_199 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_200 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_199 (Embedding)       (None, 4, 48)        705264      input_199[0][0]                  
__________________________________________________________________________________________________
embedding_200 (Embedding)       (None, 4, 24)        5640        input_200[0][0]                  
__________________________________________________________________________________________________
flatten_199 (Flatten)           (None, 192)          0           embedding_199[0][0]              
__________________________________________________________________________________________________
flatten_200 (Flatten)           (None, 96)           0           embedding_200[0][0]              
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 288)          0           flatten_199[0][0]                
                                                                 flatten_200[0][0]                
__________________________________________________________________________________________________
dense_199 (Dense)               (None, 24)           6936        concatenate_100[0][0]            
__________________________________________________________________________________________________
dropout_100 (Dropout)           (None, 24)           0           dense_199[0][0]                  
__________________________________________________________________________________________________
dense_200 (Dense)               (None, 8)            200         dropout_100[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adagrad, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0858 - acc: 0.9650 - val_loss: 0.0699 - val_acc: 0.9681
Epoch 2/40
 - 4s - loss: 0.0523 - acc: 0.9731 - val_loss: 0.0740 - val_acc: 0.9684
Epoch 3/40
 - 4s - loss: 0.0449 - acc: 0.9749 - val_loss: 0.0755 - val_acc: 0.9687
Epoch 4/40
 - 4s - loss: 0.0414 - acc: 0.9756 - val_loss: 0.0782 - val_acc: 0.9690
Epoch 5/40
 - 4s - loss: 0.0395 - acc: 0.9763 - val_loss: 0.0804 - val_acc: 0.9684
Epoch 6/40
 - 4s - loss: 0.0381 - acc: 0.9766 - val_loss: 0.0834 - val_acc: 0.9689
Epoch 7/40
 - 4s - loss: 0.0373 - acc: 0.9769 - val_loss: 0.0876 - val_acc: 0.9695
Epoch 8/40
 - 4s - loss: 0.0362 - acc: 0.9773 - val_loss: 0.0880 - val_acc: 0.9688
Epoch 9/40
 - 4s - loss: 0.0356 - acc: 0.9778 - val_loss: 0.0920 - val_acc: 0.9684
Epoch 10/40
 - 4s - loss: 0.0350 - acc: 0.9775 - val_loss: 0.0929 - val_acc: 0.9690
Epoch 11/40
 - 4s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.0970 - val_acc: 0.9690
Epoch 12/40
 - 4s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.0932 - val_acc: 0.9684
Epoch 13/40
 - 4s - loss: 0.0338 - acc: 0.9785 - val_loss: 0.0952 - val_acc: 0.9688
Epoch 14/40
 - 4s - loss: 0.0338 - acc: 0.9778 - val_loss: 0.1014 - val_acc: 0.9686
Epoch 15/40
 - 4s - loss: 0.0334 - acc: 0.9783 - val_loss: 0.0985 - val_acc: 0.9689
Epoch 16/40
 - 4s - loss: 0.0331 - acc: 0.9784 - val_loss: 0.1006 - val_acc: 0.9693
Epoch 17/40
 - 4s - loss: 0.0329 - acc: 0.9785 - val_loss: 0.1009 - val_acc: 0.9690
Epoch 18/40
 - 4s - loss: 0.0326 - acc: 0.9787 - val_loss: 0.1044 - val_acc: 0.9687
Epoch 19/40
 - 4s - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1021 - val_acc: 0.9685
Epoch 20/40
 - 4s - loss: 0.0324 - acc: 0.9785 - val_loss: 0.1011 - val_acc: 0.9685
Epoch 21/40
 - 4s - loss: 0.0324 - acc: 0.9786 - val_loss: 0.1029 - val_acc: 0.9685
Epoch 22/40
 - 4s - loss: 0.0321 - acc: 0.9787 - val_loss: 0.1049 - val_acc: 0.9683
Epoch 23/40
 - 4s - loss: 0.0320 - acc: 0.9791 - val_loss: 0.1056 - val_acc: 0.9694
Epoch 24/40
 - 4s - loss: 0.0317 - acc: 0.9791 - val_loss: 0.1066 - val_acc: 0.9688
Epoch 25/40
 - 4s - loss: 0.0316 - acc: 0.9791 - val_loss: 0.1065 - val_acc: 0.9687
Epoch 26/40
 - 4s - loss: 0.0316 - acc: 0.9793 - val_loss: 0.1098 - val_acc: 0.9688
Epoch 27/40
 - 4s - loss: 0.0317 - acc: 0.9791 - val_loss: 0.1095 - val_acc: 0.9690
Epoch 28/40
 - 4s - loss: 0.0312 - acc: 0.9792 - val_loss: 0.1113 - val_acc: 0.9691
Epoch 29/40
 - 4s - loss: 0.0313 - acc: 0.9792 - val_loss: 0.1141 - val_acc: 0.9691
Epoch 30/40
 - 4s - loss: 0.0316 - acc: 0.9790 - val_loss: 0.1110 - val_acc: 0.9692
Epoch 31/40
 - 4s - loss: 0.0313 - acc: 0.9793 - val_loss: 0.1123 - val_acc: 0.9688
Epoch 32/40
 - 4s - loss: 0.0312 - acc: 0.9791 - val_loss: 0.1112 - val_acc: 0.9689
Epoch 33/40
 - 4s - loss: 0.0311 - acc: 0.9792 - val_loss: 0.1132 - val_acc: 0.9689
Epoch 34/40
 - 4s - loss: 0.0308 - acc: 0.9794 - val_loss: 0.1144 - val_acc: 0.9691
Epoch 35/40
 - 4s - loss: 0.0309 - acc: 0.9790 - val_loss: 0.1157 - val_acc: 0.9693
Epoch 36/40
 - 4s - loss: 0.0311 - acc: 0.9791 - val_loss: 0.1163 - val_acc: 0.9687
Epoch 37/40
 - 4s - loss: 0.0310 - acc: 0.9792 - val_loss: 0.1169 - val_acc: 0.9690
Epoch 38/40
 - 4s - loss: 0.0309 - acc: 0.9794 - val_loss: 0.1158 - val_acc: 0.9688
Epoch 39/40
 - 4s - loss: 0.0309 - acc: 0.9792 - val_loss: 0.1176 - val_acc: 0.9689
Epoch 40/40
 - 4s - loss: 0.0309 - acc: 0.9791 - val_loss: 0.1176 - val_acc: 0.9687
Epoch 1/40
 - 0s - loss: 0.0908 - acc: 0.9668
Epoch 2/40
 - 0s - loss: 0.0543 - acc: 0.9731
Epoch 3/40
 - 0s - loss: 0.0490 - acc: 0.9745
Epoch 4/40
 - 0s - loss: 0.0457 - acc: 0.9748
Epoch 5/40
 - 0s - loss: 0.0444 - acc: 0.9766
Epoch 6/40
 - 0s - loss: 0.0418 - acc: 0.9757
Epoch 7/40
 - 0s - loss: 0.0412 - acc: 0.9760
Epoch 8/40
 - 0s - loss: 0.0400 - acc: 0.9774
Epoch 9/40
 - 0s - loss: 0.0393 - acc: 0.9754
Epoch 10/40
 - 0s - loss: 0.0379 - acc: 0.9763
Epoch 11/40
 - 0s - loss: 0.0373 - acc: 0.9774
Epoch 12/40
 - 0s - loss: 0.0360 - acc: 0.9765
Epoch 13/40
 - 0s - loss: 0.0358 - acc: 0.9771
Epoch 14/40
 - 0s - loss: 0.0351 - acc: 0.9770
Epoch 15/40
 - 0s - loss: 0.0347 - acc: 0.9783
Epoch 16/40
 - 0s - loss: 0.0344 - acc: 0.9786
Epoch 17/40
 - 0s - loss: 0.0342 - acc: 0.9770
Epoch 18/40
 - 0s - loss: 0.0337 - acc: 0.9783
Epoch 19/40
 - 0s - loss: 0.0333 - acc: 0.9777
Epoch 20/40
 - 0s - loss: 0.0333 - acc: 0.9790
Epoch 21/40
 - 0s - loss: 0.0335 - acc: 0.9781
Epoch 22/40
 - 0s - loss: 0.0329 - acc: 0.9783
Epoch 23/40
 - 0s - loss: 0.0329 - acc: 0.9785
Epoch 24/40
 - 0s - loss: 0.0330 - acc: 0.9791
Epoch 25/40
 - 0s - loss: 0.0331 - acc: 0.9788
Epoch 26/40
 - 0s - loss: 0.0325 - acc: 0.9782
Epoch 27/40
 - 0s - loss: 0.0325 - acc: 0.9778
Epoch 28/40
 - 0s - loss: 0.0325 - acc: 0.9780
Epoch 29/40
 - 0s - loss: 0.0325 - acc: 0.9785
Epoch 30/40
 - 0s - loss: 0.0325 - acc: 0.9791
Epoch 31/40
 - 0s - loss: 0.0328 - acc: 0.9777
Epoch 32/40
 - 0s - loss: 0.0321 - acc: 0.9780
Epoch 33/40
 - 0s - loss: 0.0327 - acc: 0.9780
Epoch 34/40
 - 0s - loss: 0.0321 - acc: 0.9793
Epoch 35/40
 - 0s - loss: 0.0319 - acc: 0.9785
Epoch 36/40
 - 0s - loss: 0.0322 - acc: 0.9792
Epoch 37/40
 - 0s - loss: 0.0323 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0315 - acc: 0.9784
Epoch 39/40
 - 0s - loss: 0.0315 - acc: 0.9781
Epoch 40/40
 - 0s - loss: 0.0316 - acc: 0.9783
# Training time = 0:03:30.414041
# F-Score(Ordinary) = 0.53, Recall: 0.711, Precision: 0.423
# F-Score(lvc) = 0.538, Recall: 0.517, Precision: 0.561
# F-Score(ireflv) = 0.673, Recall: 0.87, Precision: 0.549
# F-Score(id) = 0.36, Recall: 0.935, Precision: 0.223
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_201 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_202 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_201 (Embedding)       (None, 4, 48)        705264      input_201[0][0]                  
__________________________________________________________________________________________________
embedding_202 (Embedding)       (None, 4, 24)        5640        input_202[0][0]                  
__________________________________________________________________________________________________
flatten_201 (Flatten)           (None, 192)          0           embedding_201[0][0]              
__________________________________________________________________________________________________
flatten_202 (Flatten)           (None, 96)           0           embedding_202[0][0]              
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 288)          0           flatten_201[0][0]                
                                                                 flatten_202[0][0]                
__________________________________________________________________________________________________
dense_201 (Dense)               (None, 24)           6936        concatenate_101[0][0]            
__________________________________________________________________________________________________
dropout_101 (Dropout)           (None, 24)           0           dense_201[0][0]                  
__________________________________________________________________________________________________
dense_202 (Dense)               (None, 8)            200         dropout_101[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.3163 - acc: 0.9114 - val_loss: 0.1562 - val_acc: 0.9526
Epoch 2/40
 - 5s - loss: 0.1428 - acc: 0.9553 - val_loss: 0.1181 - val_acc: 0.9602
Epoch 3/40
 - 5s - loss: 0.1184 - acc: 0.9593 - val_loss: 0.1061 - val_acc: 0.9621
Epoch 4/40
 - 5s - loss: 0.1082 - acc: 0.9611 - val_loss: 0.0990 - val_acc: 0.9633
Epoch 5/40
 - 5s - loss: 0.1004 - acc: 0.9626 - val_loss: 0.0937 - val_acc: 0.9640
Epoch 6/40
 - 5s - loss: 0.0946 - acc: 0.9640 - val_loss: 0.0901 - val_acc: 0.9653
Epoch 7/40
 - 5s - loss: 0.0908 - acc: 0.9645 - val_loss: 0.0878 - val_acc: 0.9657
Epoch 8/40
 - 5s - loss: 0.0876 - acc: 0.9654 - val_loss: 0.0854 - val_acc: 0.9657
Epoch 9/40
 - 5s - loss: 0.0852 - acc: 0.9657 - val_loss: 0.0837 - val_acc: 0.9660
Epoch 10/40
 - 5s - loss: 0.0826 - acc: 0.9662 - val_loss: 0.0828 - val_acc: 0.9659
Epoch 11/40
 - 5s - loss: 0.0810 - acc: 0.9665 - val_loss: 0.0822 - val_acc: 0.9662
Epoch 12/40
 - 5s - loss: 0.0788 - acc: 0.9673 - val_loss: 0.0806 - val_acc: 0.9663
Epoch 13/40
 - 5s - loss: 0.0774 - acc: 0.9676 - val_loss: 0.0795 - val_acc: 0.9662
Epoch 14/40
 - 5s - loss: 0.0759 - acc: 0.9678 - val_loss: 0.0789 - val_acc: 0.9663
Epoch 15/40
 - 5s - loss: 0.0749 - acc: 0.9678 - val_loss: 0.0784 - val_acc: 0.9666
Epoch 16/40
 - 5s - loss: 0.0736 - acc: 0.9681 - val_loss: 0.0776 - val_acc: 0.9665
Epoch 17/40
 - 5s - loss: 0.0727 - acc: 0.9684 - val_loss: 0.0771 - val_acc: 0.9664
Epoch 18/40
 - 5s - loss: 0.0715 - acc: 0.9688 - val_loss: 0.0765 - val_acc: 0.9665
Epoch 19/40
 - 5s - loss: 0.0707 - acc: 0.9689 - val_loss: 0.0762 - val_acc: 0.9670
Epoch 20/40
 - 5s - loss: 0.0703 - acc: 0.9690 - val_loss: 0.0756 - val_acc: 0.9667
Epoch 21/40
 - 5s - loss: 0.0689 - acc: 0.9693 - val_loss: 0.0749 - val_acc: 0.9671
Epoch 22/40
 - 5s - loss: 0.0685 - acc: 0.9694 - val_loss: 0.0745 - val_acc: 0.9672
Epoch 23/40
 - 5s - loss: 0.0678 - acc: 0.9696 - val_loss: 0.0740 - val_acc: 0.9671
Epoch 24/40
 - 5s - loss: 0.0670 - acc: 0.9699 - val_loss: 0.0737 - val_acc: 0.9671
Epoch 25/40
 - 5s - loss: 0.0660 - acc: 0.9705 - val_loss: 0.0733 - val_acc: 0.9673
Epoch 26/40
 - 5s - loss: 0.0656 - acc: 0.9700 - val_loss: 0.0731 - val_acc: 0.9673
Epoch 27/40
 - 5s - loss: 0.0649 - acc: 0.9701 - val_loss: 0.0725 - val_acc: 0.9674
Epoch 28/40
 - 5s - loss: 0.0646 - acc: 0.9704 - val_loss: 0.0725 - val_acc: 0.9679
Epoch 29/40
 - 5s - loss: 0.0642 - acc: 0.9706 - val_loss: 0.0721 - val_acc: 0.9681
Epoch 30/40
 - 5s - loss: 0.0634 - acc: 0.9708 - val_loss: 0.0720 - val_acc: 0.9680
Epoch 31/40
 - 5s - loss: 0.0630 - acc: 0.9706 - val_loss: 0.0717 - val_acc: 0.9679
Epoch 32/40
 - 5s - loss: 0.0624 - acc: 0.9711 - val_loss: 0.0717 - val_acc: 0.9676
Epoch 33/40
 - 5s - loss: 0.0622 - acc: 0.9711 - val_loss: 0.0711 - val_acc: 0.9678
Epoch 34/40
 - 5s - loss: 0.0616 - acc: 0.9714 - val_loss: 0.0710 - val_acc: 0.9679
Epoch 35/40
 - 5s - loss: 0.0611 - acc: 0.9714 - val_loss: 0.0708 - val_acc: 0.9680
Epoch 36/40
 - 5s - loss: 0.0610 - acc: 0.9716 - val_loss: 0.0710 - val_acc: 0.9677
Epoch 37/40
 - 5s - loss: 0.0607 - acc: 0.9715 - val_loss: 0.0705 - val_acc: 0.9678
Epoch 38/40
 - 5s - loss: 0.0604 - acc: 0.9714 - val_loss: 0.0702 - val_acc: 0.9681
Epoch 39/40
 - 5s - loss: 0.0599 - acc: 0.9717 - val_loss: 0.0701 - val_acc: 0.9681
Epoch 40/40
 - 5s - loss: 0.0596 - acc: 0.9718 - val_loss: 0.0706 - val_acc: 0.9682
Epoch 1/40
 - 1s - loss: 0.0742 - acc: 0.9672
Epoch 2/40
 - 1s - loss: 0.0718 - acc: 0.9676
Epoch 3/40
 - 1s - loss: 0.0713 - acc: 0.9682
Epoch 4/40
 - 1s - loss: 0.0697 - acc: 0.9696
Epoch 5/40
 - 1s - loss: 0.0691 - acc: 0.9683
Epoch 6/40
 - 1s - loss: 0.0683 - acc: 0.9684
Epoch 7/40
 - 1s - loss: 0.0679 - acc: 0.9692
Epoch 8/40
 - 1s - loss: 0.0670 - acc: 0.9688
Epoch 9/40
 - 1s - loss: 0.0671 - acc: 0.9678
Epoch 10/40
 - 1s - loss: 0.0652 - acc: 0.9702
Epoch 11/40
 - 1s - loss: 0.0665 - acc: 0.9684
Epoch 12/40
 - 1s - loss: 0.0633 - acc: 0.9698
Epoch 13/40
 - 1s - loss: 0.0638 - acc: 0.9696
Epoch 14/40
 - 1s - loss: 0.0643 - acc: 0.9696
Epoch 15/40
 - 1s - loss: 0.0627 - acc: 0.9701
Epoch 16/40
 - 1s - loss: 0.0624 - acc: 0.9698
Epoch 17/40
 - 1s - loss: 0.0629 - acc: 0.9701
Epoch 18/40
 - 1s - loss: 0.0617 - acc: 0.9695
Epoch 19/40
 - 1s - loss: 0.0613 - acc: 0.9715
Epoch 20/40
 - 1s - loss: 0.0616 - acc: 0.9711
Epoch 21/40
 - 1s - loss: 0.0604 - acc: 0.9710
Epoch 22/40
 - 1s - loss: 0.0599 - acc: 0.9725
Epoch 23/40
 - 1s - loss: 0.0589 - acc: 0.9710
Epoch 24/40
 - 1s - loss: 0.0586 - acc: 0.9720
Epoch 25/40
 - 1s - loss: 0.0591 - acc: 0.9715
Epoch 26/40
 - 1s - loss: 0.0593 - acc: 0.9709
Epoch 27/40
 - 1s - loss: 0.0586 - acc: 0.9709
Epoch 28/40
 - 1s - loss: 0.0576 - acc: 0.9710
Epoch 29/40
 - 1s - loss: 0.0570 - acc: 0.9718
Epoch 30/40
 - 1s - loss: 0.0570 - acc: 0.9713
Epoch 31/40
 - 1s - loss: 0.0560 - acc: 0.9732
Epoch 32/40
 - 1s - loss: 0.0565 - acc: 0.9720
Epoch 33/40
 - 1s - loss: 0.0568 - acc: 0.9725
Epoch 34/40
 - 1s - loss: 0.0565 - acc: 0.9731
Epoch 35/40
 - 1s - loss: 0.0559 - acc: 0.9718
Epoch 36/40
 - 1s - loss: 0.0557 - acc: 0.9720
Epoch 37/40
 - 1s - loss: 0.0559 - acc: 0.9716
Epoch 38/40
 - 1s - loss: 0.0553 - acc: 0.9727
Epoch 39/40
 - 1s - loss: 0.0547 - acc: 0.9731
Epoch 40/40
 - 1s - loss: 0.0545 - acc: 0.9715
# Training time = 0:04:05.562663
# F-Score(Ordinary) = 0.412, Recall: 0.592, Precision: 0.315
# F-Score(lvc) = 0.397, Recall: 0.386, Precision: 0.409
# F-Score(ireflv) = 0.437, Recall: 0.731, Precision: 0.311
# F-Score(id) = 0.351, Recall: 0.913, Precision: 0.218
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_203 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_204 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_203 (Embedding)       (None, 4, 48)        705264      input_203[0][0]                  
__________________________________________________________________________________________________
embedding_204 (Embedding)       (None, 4, 24)        5640        input_204[0][0]                  
__________________________________________________________________________________________________
flatten_203 (Flatten)           (None, 192)          0           embedding_203[0][0]              
__________________________________________________________________________________________________
flatten_204 (Flatten)           (None, 96)           0           embedding_204[0][0]              
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 288)          0           flatten_203[0][0]                
                                                                 flatten_204[0][0]                
__________________________________________________________________________________________________
dense_203 (Dense)               (None, 24)           6936        concatenate_102[0][0]            
__________________________________________________________________________________________________
dropout_102 (Dropout)           (None, 24)           0           dense_203[0][0]                  
__________________________________________________________________________________________________
dense_204 (Dense)               (None, 8)            200         dropout_102[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.3235 - acc: 0.9071 - val_loss: 0.1522 - val_acc: 0.9519
Epoch 2/40
 - 5s - loss: 0.1410 - acc: 0.9535 - val_loss: 0.1140 - val_acc: 0.9601
Epoch 3/40
 - 5s - loss: 0.1177 - acc: 0.9592 - val_loss: 0.1045 - val_acc: 0.9621
Epoch 4/40
 - 5s - loss: 0.1092 - acc: 0.9613 - val_loss: 0.0995 - val_acc: 0.9641
Epoch 5/40
 - 5s - loss: 0.1033 - acc: 0.9627 - val_loss: 0.0961 - val_acc: 0.9647
Epoch 6/40
 - 5s - loss: 0.0987 - acc: 0.9640 - val_loss: 0.0928 - val_acc: 0.9651
Epoch 7/40
 - 5s - loss: 0.0948 - acc: 0.9649 - val_loss: 0.0903 - val_acc: 0.9653
Epoch 8/40
 - 5s - loss: 0.0907 - acc: 0.9654 - val_loss: 0.0870 - val_acc: 0.9660
Epoch 9/40
 - 5s - loss: 0.0878 - acc: 0.9660 - val_loss: 0.0847 - val_acc: 0.9663
Epoch 10/40
 - 5s - loss: 0.0847 - acc: 0.9667 - val_loss: 0.0825 - val_acc: 0.9665
Epoch 11/40
 - 5s - loss: 0.0820 - acc: 0.9672 - val_loss: 0.0809 - val_acc: 0.9666
Epoch 12/40
 - 5s - loss: 0.0804 - acc: 0.9675 - val_loss: 0.0796 - val_acc: 0.9671
Epoch 13/40
 - 5s - loss: 0.0781 - acc: 0.9676 - val_loss: 0.0786 - val_acc: 0.9671
Epoch 14/40
 - 5s - loss: 0.0768 - acc: 0.9681 - val_loss: 0.0780 - val_acc: 0.9672
Epoch 15/40
 - 5s - loss: 0.0754 - acc: 0.9681 - val_loss: 0.0767 - val_acc: 0.9675
Epoch 16/40
 - 5s - loss: 0.0741 - acc: 0.9684 - val_loss: 0.0760 - val_acc: 0.9672
Epoch 17/40
 - 5s - loss: 0.0733 - acc: 0.9684 - val_loss: 0.0753 - val_acc: 0.9675
Epoch 18/40
 - 5s - loss: 0.0720 - acc: 0.9689 - val_loss: 0.0750 - val_acc: 0.9677
Epoch 19/40
 - 5s - loss: 0.0712 - acc: 0.9687 - val_loss: 0.0743 - val_acc: 0.9675
Epoch 20/40
 - 5s - loss: 0.0702 - acc: 0.9693 - val_loss: 0.0739 - val_acc: 0.9678
Epoch 21/40
 - 5s - loss: 0.0692 - acc: 0.9696 - val_loss: 0.0736 - val_acc: 0.9673
Epoch 22/40
 - 5s - loss: 0.0686 - acc: 0.9699 - val_loss: 0.0734 - val_acc: 0.9676
Epoch 23/40
 - 5s - loss: 0.0682 - acc: 0.9697 - val_loss: 0.0727 - val_acc: 0.9678
Epoch 24/40
 - 5s - loss: 0.0677 - acc: 0.9702 - val_loss: 0.0728 - val_acc: 0.9676
Epoch 25/40
 - 5s - loss: 0.0671 - acc: 0.9698 - val_loss: 0.0723 - val_acc: 0.9681
Epoch 26/40
 - 5s - loss: 0.0659 - acc: 0.9701 - val_loss: 0.0719 - val_acc: 0.9680
Epoch 27/40
 - 5s - loss: 0.0658 - acc: 0.9705 - val_loss: 0.0718 - val_acc: 0.9678
Epoch 28/40
 - 5s - loss: 0.0654 - acc: 0.9705 - val_loss: 0.0716 - val_acc: 0.9679
Epoch 29/40
 - 5s - loss: 0.0648 - acc: 0.9706 - val_loss: 0.0720 - val_acc: 0.9679
Epoch 30/40
 - 5s - loss: 0.0643 - acc: 0.9709 - val_loss: 0.0713 - val_acc: 0.9679
Epoch 31/40
 - 5s - loss: 0.0638 - acc: 0.9708 - val_loss: 0.0710 - val_acc: 0.9677
Epoch 32/40
 - 5s - loss: 0.0638 - acc: 0.9710 - val_loss: 0.0708 - val_acc: 0.9678
Epoch 33/40
 - 5s - loss: 0.0632 - acc: 0.9709 - val_loss: 0.0710 - val_acc: 0.9679
Epoch 34/40
 - 5s - loss: 0.0627 - acc: 0.9711 - val_loss: 0.0702 - val_acc: 0.9680
Epoch 35/40
 - 5s - loss: 0.0626 - acc: 0.9707 - val_loss: 0.0701 - val_acc: 0.9678
Epoch 36/40
 - 5s - loss: 0.0619 - acc: 0.9714 - val_loss: 0.0707 - val_acc: 0.9676
Epoch 37/40
 - 5s - loss: 0.0613 - acc: 0.9713 - val_loss: 0.0708 - val_acc: 0.9679
Epoch 38/40
 - 5s - loss: 0.0614 - acc: 0.9714 - val_loss: 0.0705 - val_acc: 0.9679
Epoch 39/40
 - 5s - loss: 0.0607 - acc: 0.9716 - val_loss: 0.0702 - val_acc: 0.9679
Epoch 40/40
 - 5s - loss: 0.0605 - acc: 0.9714 - val_loss: 0.0701 - val_acc: 0.9682
Epoch 1/40
 - 1s - loss: 0.0734 - acc: 0.9670
Epoch 2/40
 - 1s - loss: 0.0722 - acc: 0.9672
Epoch 3/40
 - 1s - loss: 0.0710 - acc: 0.9687
Epoch 4/40
 - 1s - loss: 0.0712 - acc: 0.9673
Epoch 5/40
 - 1s - loss: 0.0709 - acc: 0.9685
Epoch 6/40
 - 1s - loss: 0.0686 - acc: 0.9690
Epoch 7/40
 - 1s - loss: 0.0684 - acc: 0.9684
Epoch 8/40
 - 1s - loss: 0.0681 - acc: 0.9683
Epoch 9/40
 - 1s - loss: 0.0684 - acc: 0.9681
Epoch 10/40
 - 1s - loss: 0.0659 - acc: 0.9695
Epoch 11/40
 - 1s - loss: 0.0668 - acc: 0.9689
Epoch 12/40
 - 1s - loss: 0.0655 - acc: 0.9688
Epoch 13/40
 - 1s - loss: 0.0643 - acc: 0.9688
Epoch 14/40
 - 1s - loss: 0.0645 - acc: 0.9697
Epoch 15/40
 - 1s - loss: 0.0638 - acc: 0.9703
Epoch 16/40
 - 1s - loss: 0.0634 - acc: 0.9704
Epoch 17/40
 - 1s - loss: 0.0635 - acc: 0.9704
Epoch 18/40
 - 1s - loss: 0.0621 - acc: 0.9707
Epoch 19/40
 - 1s - loss: 0.0627 - acc: 0.9704
Epoch 20/40
 - 1s - loss: 0.0613 - acc: 0.9710
Epoch 21/40
 - 1s - loss: 0.0608 - acc: 0.9708
Epoch 22/40
 - 1s - loss: 0.0610 - acc: 0.9723
Epoch 23/40
 - 1s - loss: 0.0598 - acc: 0.9714
Epoch 24/40
 - 1s - loss: 0.0597 - acc: 0.9707
Epoch 25/40
 - 1s - loss: 0.0605 - acc: 0.9699
Epoch 26/40
 - 1s - loss: 0.0592 - acc: 0.9718
Epoch 27/40
 - 1s - loss: 0.0591 - acc: 0.9725
Epoch 28/40
 - 1s - loss: 0.0583 - acc: 0.9730
Epoch 29/40
 - 1s - loss: 0.0591 - acc: 0.9718
Epoch 30/40
 - 1s - loss: 0.0582 - acc: 0.9720
Epoch 31/40
 - 1s - loss: 0.0570 - acc: 0.9723
Epoch 32/40
 - 1s - loss: 0.0575 - acc: 0.9728
Epoch 33/40
 - 1s - loss: 0.0580 - acc: 0.9722
Epoch 34/40
 - 1s - loss: 0.0574 - acc: 0.9725
Epoch 35/40
 - 1s - loss: 0.0579 - acc: 0.9718
Epoch 36/40
 - 1s - loss: 0.0566 - acc: 0.9720
Epoch 37/40
 - 1s - loss: 0.0566 - acc: 0.9718
Epoch 38/40
 - 1s - loss: 0.0552 - acc: 0.9726
Epoch 39/40
 - 1s - loss: 0.0562 - acc: 0.9726
Epoch 40/40
 - 1s - loss: 0.0558 - acc: 0.9740
# Training time = 0:04:04.881932
# F-Score(Ordinary) = 0.426, Recall: 0.537, Precision: 0.353
# F-Score(lvc) = 0.369, Recall: 0.402, Precision: 0.341
# F-Score(ireflv) = 0.541, Recall: 0.5, Precision: 0.59
# F-Score(id) = 0.312, Recall: 0.947, Precision: 0.187
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_205 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_206 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_205 (Embedding)       (None, 4, 48)        705264      input_205[0][0]                  
__________________________________________________________________________________________________
embedding_206 (Embedding)       (None, 4, 24)        5640        input_206[0][0]                  
__________________________________________________________________________________________________
flatten_205 (Flatten)           (None, 192)          0           embedding_205[0][0]              
__________________________________________________________________________________________________
flatten_206 (Flatten)           (None, 96)           0           embedding_206[0][0]              
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 288)          0           flatten_205[0][0]                
                                                                 flatten_206[0][0]                
__________________________________________________________________________________________________
dense_205 (Dense)               (None, 24)           6936        concatenate_103[0][0]            
__________________________________________________________________________________________________
dropout_103 (Dropout)           (None, 24)           0           dense_205[0][0]                  
__________________________________________________________________________________________________
dense_206 (Dense)               (None, 8)            200         dropout_103[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.3182 - acc: 0.9173 - val_loss: 0.1573 - val_acc: 0.9502
Epoch 2/40
 - 5s - loss: 0.1411 - acc: 0.9552 - val_loss: 0.1157 - val_acc: 0.9601
Epoch 3/40
 - 5s - loss: 0.1176 - acc: 0.9595 - val_loss: 0.1054 - val_acc: 0.9628
Epoch 4/40
 - 5s - loss: 0.1083 - acc: 0.9612 - val_loss: 0.1007 - val_acc: 0.9635
Epoch 5/40
 - 5s - loss: 0.1021 - acc: 0.9630 - val_loss: 0.0968 - val_acc: 0.9644
Epoch 6/40
 - 5s - loss: 0.0978 - acc: 0.9642 - val_loss: 0.0934 - val_acc: 0.9650
Epoch 7/40
 - 5s - loss: 0.0938 - acc: 0.9649 - val_loss: 0.0905 - val_acc: 0.9654
Epoch 8/40
 - 5s - loss: 0.0898 - acc: 0.9658 - val_loss: 0.0878 - val_acc: 0.9659
Epoch 9/40
 - 5s - loss: 0.0862 - acc: 0.9666 - val_loss: 0.0852 - val_acc: 0.9662
Epoch 10/40
 - 5s - loss: 0.0833 - acc: 0.9666 - val_loss: 0.0831 - val_acc: 0.9663
Epoch 11/40
 - 5s - loss: 0.0811 - acc: 0.9671 - val_loss: 0.0817 - val_acc: 0.9667
Epoch 12/40
 - 5s - loss: 0.0787 - acc: 0.9677 - val_loss: 0.0804 - val_acc: 0.9669
Epoch 13/40
 - 5s - loss: 0.0774 - acc: 0.9678 - val_loss: 0.0793 - val_acc: 0.9669
Epoch 14/40
 - 5s - loss: 0.0758 - acc: 0.9684 - val_loss: 0.0787 - val_acc: 0.9669
Epoch 15/40
 - 5s - loss: 0.0748 - acc: 0.9683 - val_loss: 0.0778 - val_acc: 0.9670
Epoch 16/40
 - 5s - loss: 0.0738 - acc: 0.9684 - val_loss: 0.0769 - val_acc: 0.9673
Epoch 17/40
 - 5s - loss: 0.0726 - acc: 0.9687 - val_loss: 0.0764 - val_acc: 0.9672
Epoch 18/40
 - 5s - loss: 0.0720 - acc: 0.9687 - val_loss: 0.0757 - val_acc: 0.9673
Epoch 19/40
 - 5s - loss: 0.0711 - acc: 0.9691 - val_loss: 0.0753 - val_acc: 0.9675
Epoch 20/40
 - 5s - loss: 0.0698 - acc: 0.9694 - val_loss: 0.0747 - val_acc: 0.9676
Epoch 21/40
 - 5s - loss: 0.0693 - acc: 0.9693 - val_loss: 0.0746 - val_acc: 0.9678
Epoch 22/40
 - 5s - loss: 0.0682 - acc: 0.9699 - val_loss: 0.0740 - val_acc: 0.9678
Epoch 23/40
 - 5s - loss: 0.0678 - acc: 0.9694 - val_loss: 0.0737 - val_acc: 0.9680
Epoch 24/40
 - 5s - loss: 0.0672 - acc: 0.9699 - val_loss: 0.0733 - val_acc: 0.9679
Epoch 25/40
 - 5s - loss: 0.0667 - acc: 0.9700 - val_loss: 0.0734 - val_acc: 0.9678
Epoch 26/40
 - 5s - loss: 0.0659 - acc: 0.9702 - val_loss: 0.0726 - val_acc: 0.9677
Epoch 27/40
 - 5s - loss: 0.0650 - acc: 0.9705 - val_loss: 0.0725 - val_acc: 0.9677
Epoch 28/40
 - 5s - loss: 0.0645 - acc: 0.9703 - val_loss: 0.0721 - val_acc: 0.9683
Epoch 29/40
 - 5s - loss: 0.0644 - acc: 0.9706 - val_loss: 0.0721 - val_acc: 0.9678
Epoch 30/40
 - 5s - loss: 0.0640 - acc: 0.9704 - val_loss: 0.0714 - val_acc: 0.9678
Epoch 31/40
 - 5s - loss: 0.0631 - acc: 0.9708 - val_loss: 0.0719 - val_acc: 0.9679
Epoch 32/40
 - 5s - loss: 0.0629 - acc: 0.9710 - val_loss: 0.0717 - val_acc: 0.9681
Epoch 33/40
 - 5s - loss: 0.0624 - acc: 0.9708 - val_loss: 0.0714 - val_acc: 0.9678
Epoch 34/40
 - 5s - loss: 0.0622 - acc: 0.9712 - val_loss: 0.0708 - val_acc: 0.9679
Epoch 35/40
 - 5s - loss: 0.0614 - acc: 0.9713 - val_loss: 0.0709 - val_acc: 0.9681
Epoch 36/40
 - 5s - loss: 0.0610 - acc: 0.9717 - val_loss: 0.0707 - val_acc: 0.9683
Epoch 37/40
 - 5s - loss: 0.0607 - acc: 0.9714 - val_loss: 0.0703 - val_acc: 0.9682
Epoch 38/40
 - 5s - loss: 0.0601 - acc: 0.9714 - val_loss: 0.0710 - val_acc: 0.9681
Epoch 39/40
 - 5s - loss: 0.0601 - acc: 0.9718 - val_loss: 0.0705 - val_acc: 0.9682
Epoch 40/40
 - 5s - loss: 0.0600 - acc: 0.9714 - val_loss: 0.0699 - val_acc: 0.9685
Epoch 1/40
 - 1s - loss: 0.0721 - acc: 0.9682
Epoch 2/40
 - 1s - loss: 0.0724 - acc: 0.9682
Epoch 3/40
 - 1s - loss: 0.0714 - acc: 0.9678
Epoch 4/40
 - 1s - loss: 0.0709 - acc: 0.9670
Epoch 5/40
 - 1s - loss: 0.0697 - acc: 0.9682
Epoch 6/40
 - 1s - loss: 0.0683 - acc: 0.9681
Epoch 7/40
 - 1s - loss: 0.0681 - acc: 0.9686
Epoch 8/40
 - 1s - loss: 0.0671 - acc: 0.9683
Epoch 9/40
 - 1s - loss: 0.0668 - acc: 0.9687
Epoch 10/40
 - 1s - loss: 0.0655 - acc: 0.9703
Epoch 11/40
 - 1s - loss: 0.0652 - acc: 0.9703
Epoch 12/40
 - 1s - loss: 0.0648 - acc: 0.9699
Epoch 13/40
 - 1s - loss: 0.0645 - acc: 0.9680
Epoch 14/40
 - 1s - loss: 0.0634 - acc: 0.9691
Epoch 15/40
 - 1s - loss: 0.0644 - acc: 0.9690
Epoch 16/40
 - 1s - loss: 0.0633 - acc: 0.9692
Epoch 17/40
 - 1s - loss: 0.0614 - acc: 0.9699
Epoch 18/40
 - 1s - loss: 0.0613 - acc: 0.9708
Epoch 19/40
 - 1s - loss: 0.0623 - acc: 0.9702
Epoch 20/40
 - 1s - loss: 0.0620 - acc: 0.9702
Epoch 21/40
 - 1s - loss: 0.0607 - acc: 0.9709
Epoch 22/40
 - 1s - loss: 0.0599 - acc: 0.9721
Epoch 23/40
 - 1s - loss: 0.0597 - acc: 0.9704
Epoch 24/40
 - 1s - loss: 0.0600 - acc: 0.9709
Epoch 25/40
 - 1s - loss: 0.0595 - acc: 0.9705
Epoch 26/40
 - 1s - loss: 0.0581 - acc: 0.9724
Epoch 27/40
 - 1s - loss: 0.0580 - acc: 0.9704
Epoch 28/40
 - 1s - loss: 0.0591 - acc: 0.9707
Epoch 29/40
 - 1s - loss: 0.0590 - acc: 0.9711
Epoch 30/40
 - 1s - loss: 0.0570 - acc: 0.9717
Epoch 31/40
 - 1s - loss: 0.0572 - acc: 0.9722
Epoch 32/40
 - 1s - loss: 0.0572 - acc: 0.9731
Epoch 33/40
 - 1s - loss: 0.0559 - acc: 0.9734
Epoch 34/40
 - 1s - loss: 0.0566 - acc: 0.9724
Epoch 35/40
 - 1s - loss: 0.0565 - acc: 0.9731
Epoch 36/40
 - 1s - loss: 0.0552 - acc: 0.9720
Epoch 37/40
 - 1s - loss: 0.0558 - acc: 0.9728
Epoch 38/40
 - 1s - loss: 0.0554 - acc: 0.9734
Epoch 39/40
 - 1s - loss: 0.0553 - acc: 0.9727
Epoch 40/40
 - 1s - loss: 0.0556 - acc: 0.9735
# Training time = 0:04:04.922574
# F-Score(Ordinary) = 0.341, Recall: 0.714, Precision: 0.224
# F-Score(lvc) = 0.167, Recall: 0.389, Precision: 0.106
# F-Score(ireflv) = 0.379, Recall: 0.635, Precision: 0.27
# F-Score(id) = 0.416, Recall: 0.981, Precision: 0.264
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_207 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_208 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_207 (Embedding)       (None, 4, 48)        705264      input_207[0][0]                  
__________________________________________________________________________________________________
embedding_208 (Embedding)       (None, 4, 24)        5640        input_208[0][0]                  
__________________________________________________________________________________________________
flatten_207 (Flatten)           (None, 192)          0           embedding_207[0][0]              
__________________________________________________________________________________________________
flatten_208 (Flatten)           (None, 96)           0           embedding_208[0][0]              
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 288)          0           flatten_207[0][0]                
                                                                 flatten_208[0][0]                
__________________________________________________________________________________________________
dense_207 (Dense)               (None, 24)           6936        concatenate_104[0][0]            
__________________________________________________________________________________________________
dropout_104 (Dropout)           (None, 24)           0           dense_207[0][0]                  
__________________________________________________________________________________________________
dense_208 (Dense)               (None, 8)            200         dropout_104[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.3368 - acc: 0.9045 - val_loss: 0.1617 - val_acc: 0.9537
Epoch 2/40
 - 5s - loss: 0.1506 - acc: 0.9535 - val_loss: 0.1196 - val_acc: 0.9596
Epoch 3/40
 - 5s - loss: 0.1241 - acc: 0.9582 - val_loss: 0.1078 - val_acc: 0.9624
Epoch 4/40
 - 5s - loss: 0.1120 - acc: 0.9609 - val_loss: 0.1014 - val_acc: 0.9639
Epoch 5/40
 - 5s - loss: 0.1049 - acc: 0.9623 - val_loss: 0.0970 - val_acc: 0.9650
Epoch 6/40
 - 5s - loss: 0.0996 - acc: 0.9633 - val_loss: 0.0930 - val_acc: 0.9653
Epoch 7/40
 - 5s - loss: 0.0952 - acc: 0.9645 - val_loss: 0.0898 - val_acc: 0.9660
Epoch 8/40
 - 5s - loss: 0.0912 - acc: 0.9651 - val_loss: 0.0875 - val_acc: 0.9663
Epoch 9/40
 - 5s - loss: 0.0880 - acc: 0.9658 - val_loss: 0.0854 - val_acc: 0.9662
Epoch 10/40
 - 5s - loss: 0.0858 - acc: 0.9664 - val_loss: 0.0838 - val_acc: 0.9668
Epoch 11/40
 - 5s - loss: 0.0833 - acc: 0.9670 - val_loss: 0.0824 - val_acc: 0.9670
Epoch 12/40
 - 5s - loss: 0.0821 - acc: 0.9672 - val_loss: 0.0817 - val_acc: 0.9670
Epoch 13/40
 - 5s - loss: 0.0804 - acc: 0.9676 - val_loss: 0.0804 - val_acc: 0.9672
Epoch 14/40
 - 5s - loss: 0.0788 - acc: 0.9678 - val_loss: 0.0794 - val_acc: 0.9674
Epoch 15/40
 - 5s - loss: 0.0770 - acc: 0.9684 - val_loss: 0.0788 - val_acc: 0.9676
Epoch 16/40
 - 5s - loss: 0.0762 - acc: 0.9682 - val_loss: 0.0778 - val_acc: 0.9677
Epoch 17/40
 - 5s - loss: 0.0749 - acc: 0.9684 - val_loss: 0.0775 - val_acc: 0.9677
Epoch 18/40
 - 5s - loss: 0.0740 - acc: 0.9686 - val_loss: 0.0768 - val_acc: 0.9677
Epoch 19/40
 - 5s - loss: 0.0729 - acc: 0.9691 - val_loss: 0.0765 - val_acc: 0.9679
Epoch 20/40
 - 5s - loss: 0.0721 - acc: 0.9692 - val_loss: 0.0759 - val_acc: 0.9678
Epoch 21/40
 - 5s - loss: 0.0712 - acc: 0.9689 - val_loss: 0.0756 - val_acc: 0.9680
Epoch 22/40
 - 5s - loss: 0.0705 - acc: 0.9694 - val_loss: 0.0753 - val_acc: 0.9680
Epoch 23/40
 - 5s - loss: 0.0701 - acc: 0.9696 - val_loss: 0.0746 - val_acc: 0.9682
Epoch 24/40
 - 5s - loss: 0.0698 - acc: 0.9700 - val_loss: 0.0743 - val_acc: 0.9682
Epoch 25/40
 - 5s - loss: 0.0686 - acc: 0.9700 - val_loss: 0.0741 - val_acc: 0.9683
Epoch 26/40
 - 5s - loss: 0.0678 - acc: 0.9699 - val_loss: 0.0738 - val_acc: 0.9681
Epoch 27/40
 - 5s - loss: 0.0672 - acc: 0.9703 - val_loss: 0.0733 - val_acc: 0.9680
Epoch 28/40
 - 5s - loss: 0.0668 - acc: 0.9703 - val_loss: 0.0734 - val_acc: 0.9681
Epoch 29/40
 - 5s - loss: 0.0664 - acc: 0.9706 - val_loss: 0.0729 - val_acc: 0.9682
Epoch 30/40
 - 5s - loss: 0.0659 - acc: 0.9701 - val_loss: 0.0725 - val_acc: 0.9682
Epoch 31/40
 - 5s - loss: 0.0651 - acc: 0.9708 - val_loss: 0.0725 - val_acc: 0.9682
Epoch 32/40
 - 5s - loss: 0.0648 - acc: 0.9711 - val_loss: 0.0725 - val_acc: 0.9682
Epoch 33/40
 - 5s - loss: 0.0642 - acc: 0.9710 - val_loss: 0.0722 - val_acc: 0.9683
Epoch 34/40
 - 5s - loss: 0.0639 - acc: 0.9711 - val_loss: 0.0717 - val_acc: 0.9680
Epoch 35/40
 - 5s - loss: 0.0635 - acc: 0.9717 - val_loss: 0.0715 - val_acc: 0.9682
Epoch 36/40
 - 5s - loss: 0.0626 - acc: 0.9711 - val_loss: 0.0719 - val_acc: 0.9685
Epoch 37/40
 - 5s - loss: 0.0625 - acc: 0.9716 - val_loss: 0.0715 - val_acc: 0.9683
Epoch 38/40
 - 5s - loss: 0.0621 - acc: 0.9716 - val_loss: 0.0711 - val_acc: 0.9683
Epoch 39/40
 - 5s - loss: 0.0620 - acc: 0.9715 - val_loss: 0.0711 - val_acc: 0.9681
Epoch 40/40
 - 5s - loss: 0.0611 - acc: 0.9719 - val_loss: 0.0707 - val_acc: 0.9685
Epoch 1/40
 - 1s - loss: 0.0755 - acc: 0.9671
Epoch 2/40
 - 1s - loss: 0.0737 - acc: 0.9678
Epoch 3/40
 - 1s - loss: 0.0731 - acc: 0.9674
Epoch 4/40
 - 1s - loss: 0.0728 - acc: 0.9679
Epoch 5/40
 - 1s - loss: 0.0709 - acc: 0.9679
Epoch 6/40
 - 1s - loss: 0.0699 - acc: 0.9688
Epoch 7/40
 - 1s - loss: 0.0696 - acc: 0.9690
Epoch 8/40
 - 1s - loss: 0.0697 - acc: 0.9670
Epoch 9/40
 - 1s - loss: 0.0691 - acc: 0.9693
Epoch 10/40
 - 1s - loss: 0.0681 - acc: 0.9690
Epoch 11/40
 - 1s - loss: 0.0668 - acc: 0.9696
Epoch 12/40
 - 1s - loss: 0.0667 - acc: 0.9700
Epoch 13/40
 - 1s - loss: 0.0663 - acc: 0.9694
Epoch 14/40
 - 1s - loss: 0.0655 - acc: 0.9703
Epoch 15/40
 - 1s - loss: 0.0649 - acc: 0.9697
Epoch 16/40
 - 1s - loss: 0.0649 - acc: 0.9710
Epoch 17/40
 - 1s - loss: 0.0642 - acc: 0.9701
Epoch 18/40
 - 1s - loss: 0.0634 - acc: 0.9715
Epoch 19/40
 - 1s - loss: 0.0636 - acc: 0.9695
Epoch 20/40
 - 1s - loss: 0.0627 - acc: 0.9709
Epoch 21/40
 - 1s - loss: 0.0617 - acc: 0.9719
Epoch 22/40
 - 1s - loss: 0.0619 - acc: 0.9717
Epoch 23/40
 - 1s - loss: 0.0619 - acc: 0.9706
Epoch 24/40
 - 1s - loss: 0.0622 - acc: 0.9706
Epoch 25/40
 - 1s - loss: 0.0608 - acc: 0.9728
Epoch 26/40
 - 1s - loss: 0.0612 - acc: 0.9720
Epoch 27/40
 - 1s - loss: 0.0607 - acc: 0.9722
Epoch 28/40
 - 1s - loss: 0.0604 - acc: 0.9719
Epoch 29/40
 - 1s - loss: 0.0595 - acc: 0.9723
Epoch 30/40
 - 1s - loss: 0.0593 - acc: 0.9716
Epoch 31/40
 - 1s - loss: 0.0583 - acc: 0.9722
Epoch 32/40
 - 1s - loss: 0.0582 - acc: 0.9735
Epoch 33/40
 - 1s - loss: 0.0587 - acc: 0.9727
Epoch 34/40
 - 1s - loss: 0.0578 - acc: 0.9726
Epoch 35/40
 - 1s - loss: 0.0587 - acc: 0.9721
Epoch 36/40
 - 1s - loss: 0.0573 - acc: 0.9732
Epoch 37/40
 - 1s - loss: 0.0575 - acc: 0.9729
Epoch 38/40
 - 1s - loss: 0.0561 - acc: 0.9728
Epoch 39/40
 - 1s - loss: 0.0572 - acc: 0.9726
Epoch 40/40
 - 1s - loss: 0.0565 - acc: 0.9727
# Training time = 0:04:07.549149
# F-Score(Ordinary) = 0.319, Recall: 0.52, Precision: 0.23
# F-Score(lvc) = 0.255, Recall: 0.279, Precision: 0.235
# F-Score(ireflv) = 0.422, Recall: 0.655, Precision: 0.311
# F-Score(id) = 0.252, Recall: 0.966, Precision: 0.145
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_209 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_210 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_209 (Embedding)       (None, 4, 48)        705264      input_209[0][0]                  
__________________________________________________________________________________________________
embedding_210 (Embedding)       (None, 4, 24)        5640        input_210[0][0]                  
__________________________________________________________________________________________________
flatten_209 (Flatten)           (None, 192)          0           embedding_209[0][0]              
__________________________________________________________________________________________________
flatten_210 (Flatten)           (None, 96)           0           embedding_210[0][0]              
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 288)          0           flatten_209[0][0]                
                                                                 flatten_210[0][0]                
__________________________________________________________________________________________________
dense_209 (Dense)               (None, 24)           6936        concatenate_105[0][0]            
__________________________________________________________________________________________________
dropout_105 (Dropout)           (None, 24)           0           dense_209[0][0]                  
__________________________________________________________________________________________________
dense_210 (Dense)               (None, 8)            200         dropout_105[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.1
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.3256 - acc: 0.9072 - val_loss: 0.1494 - val_acc: 0.9547
Epoch 2/40
 - 5s - loss: 0.1409 - acc: 0.9544 - val_loss: 0.1157 - val_acc: 0.9601
Epoch 3/40
 - 5s - loss: 0.1193 - acc: 0.9587 - val_loss: 0.1059 - val_acc: 0.9621
Epoch 4/40
 - 5s - loss: 0.1105 - acc: 0.9605 - val_loss: 0.1010 - val_acc: 0.9630
Epoch 5/40
 - 5s - loss: 0.1043 - acc: 0.9623 - val_loss: 0.0973 - val_acc: 0.9647
Epoch 6/40
 - 5s - loss: 0.0989 - acc: 0.9640 - val_loss: 0.0936 - val_acc: 0.9651
Epoch 7/40
 - 5s - loss: 0.0946 - acc: 0.9646 - val_loss: 0.0903 - val_acc: 0.9656
Epoch 8/40
 - 5s - loss: 0.0901 - acc: 0.9654 - val_loss: 0.0870 - val_acc: 0.9658
Epoch 9/40
 - 5s - loss: 0.0867 - acc: 0.9658 - val_loss: 0.0844 - val_acc: 0.9660
Epoch 10/40
 - 5s - loss: 0.0838 - acc: 0.9662 - val_loss: 0.0826 - val_acc: 0.9662
Epoch 11/40
 - 5s - loss: 0.0817 - acc: 0.9667 - val_loss: 0.0813 - val_acc: 0.9660
Epoch 12/40
 - 5s - loss: 0.0802 - acc: 0.9669 - val_loss: 0.0800 - val_acc: 0.9667
Epoch 13/40
 - 5s - loss: 0.0781 - acc: 0.9674 - val_loss: 0.0794 - val_acc: 0.9666
Epoch 14/40
 - 5s - loss: 0.0771 - acc: 0.9672 - val_loss: 0.0785 - val_acc: 0.9669
Epoch 15/40
 - 5s - loss: 0.0755 - acc: 0.9680 - val_loss: 0.0778 - val_acc: 0.9670
Epoch 16/40
 - 5s - loss: 0.0746 - acc: 0.9681 - val_loss: 0.0775 - val_acc: 0.9671
Epoch 17/40
 - 5s - loss: 0.0738 - acc: 0.9681 - val_loss: 0.0767 - val_acc: 0.9671
Epoch 18/40
 - 5s - loss: 0.0726 - acc: 0.9689 - val_loss: 0.0763 - val_acc: 0.9672
Epoch 19/40
 - 5s - loss: 0.0718 - acc: 0.9688 - val_loss: 0.0756 - val_acc: 0.9672
Epoch 20/40
 - 5s - loss: 0.0709 - acc: 0.9689 - val_loss: 0.0752 - val_acc: 0.9676
Epoch 21/40
 - 5s - loss: 0.0700 - acc: 0.9697 - val_loss: 0.0748 - val_acc: 0.9676
Epoch 22/40
 - 5s - loss: 0.0695 - acc: 0.9694 - val_loss: 0.0744 - val_acc: 0.9674
Epoch 23/40
 - 5s - loss: 0.0686 - acc: 0.9696 - val_loss: 0.0744 - val_acc: 0.9675
Epoch 24/40
 - 5s - loss: 0.0678 - acc: 0.9696 - val_loss: 0.0737 - val_acc: 0.9674
Epoch 25/40
 - 5s - loss: 0.0672 - acc: 0.9704 - val_loss: 0.0735 - val_acc: 0.9679
Epoch 26/40
 - 5s - loss: 0.0667 - acc: 0.9699 - val_loss: 0.0732 - val_acc: 0.9680
Epoch 27/40
 - 5s - loss: 0.0662 - acc: 0.9704 - val_loss: 0.0726 - val_acc: 0.9680
Epoch 28/40
 - 5s - loss: 0.0652 - acc: 0.9704 - val_loss: 0.0725 - val_acc: 0.9681
Epoch 29/40
 - 5s - loss: 0.0649 - acc: 0.9705 - val_loss: 0.0721 - val_acc: 0.9680
Epoch 30/40
 - 5s - loss: 0.0643 - acc: 0.9709 - val_loss: 0.0718 - val_acc: 0.9683
Epoch 31/40
 - 5s - loss: 0.0639 - acc: 0.9706 - val_loss: 0.0715 - val_acc: 0.9680
Epoch 32/40
 - 5s - loss: 0.0632 - acc: 0.9711 - val_loss: 0.0715 - val_acc: 0.9681
Epoch 33/40
 - 5s - loss: 0.0627 - acc: 0.9711 - val_loss: 0.0713 - val_acc: 0.9679
Epoch 34/40
 - 5s - loss: 0.0625 - acc: 0.9711 - val_loss: 0.0711 - val_acc: 0.9679
Epoch 35/40
 - 5s - loss: 0.0623 - acc: 0.9709 - val_loss: 0.0709 - val_acc: 0.9683
Epoch 36/40
 - 5s - loss: 0.0619 - acc: 0.9711 - val_loss: 0.0702 - val_acc: 0.9684
Epoch 37/40
 - 5s - loss: 0.0612 - acc: 0.9718 - val_loss: 0.0705 - val_acc: 0.9684
Epoch 38/40
 - 5s - loss: 0.0612 - acc: 0.9716 - val_loss: 0.0704 - val_acc: 0.9684
Epoch 39/40
 - 5s - loss: 0.0606 - acc: 0.9718 - val_loss: 0.0700 - val_acc: 0.9682
Epoch 40/40
 - 5s - loss: 0.0606 - acc: 0.9721 - val_loss: 0.0702 - val_acc: 0.9683
Epoch 1/40
 - 1s - loss: 0.0755 - acc: 0.9665
Epoch 2/40
 - 1s - loss: 0.0726 - acc: 0.9666
Epoch 3/40
 - 1s - loss: 0.0710 - acc: 0.9673
Epoch 4/40
 - 1s - loss: 0.0708 - acc: 0.9671
Epoch 5/40
 - 1s - loss: 0.0698 - acc: 0.9685
Epoch 6/40
 - 1s - loss: 0.0688 - acc: 0.9687
Epoch 7/40
 - 1s - loss: 0.0683 - acc: 0.9687
Epoch 8/40
 - 1s - loss: 0.0673 - acc: 0.9687
Epoch 9/40
 - 1s - loss: 0.0669 - acc: 0.9690
Epoch 10/40
 - 1s - loss: 0.0662 - acc: 0.9692
Epoch 11/40
 - 1s - loss: 0.0657 - acc: 0.9708
Epoch 12/40
 - 1s - loss: 0.0648 - acc: 0.9698
Epoch 13/40
 - 1s - loss: 0.0646 - acc: 0.9689
Epoch 14/40
 - 1s - loss: 0.0642 - acc: 0.9697
Epoch 15/40
 - 1s - loss: 0.0633 - acc: 0.9711
Epoch 16/40
 - 1s - loss: 0.0624 - acc: 0.9707
Epoch 17/40
 - 1s - loss: 0.0627 - acc: 0.9700
Epoch 18/40
 - 1s - loss: 0.0613 - acc: 0.9719
Epoch 19/40
 - 1s - loss: 0.0624 - acc: 0.9695
Epoch 20/40
 - 1s - loss: 0.0610 - acc: 0.9709
Epoch 21/40
 - 1s - loss: 0.0609 - acc: 0.9704
Epoch 22/40
 - 1s - loss: 0.0602 - acc: 0.9713
Epoch 23/40
 - 1s - loss: 0.0590 - acc: 0.9713
Epoch 24/40
 - 1s - loss: 0.0595 - acc: 0.9704
Epoch 25/40
 - 1s - loss: 0.0595 - acc: 0.9714
Epoch 26/40
 - 1s - loss: 0.0586 - acc: 0.9720
Epoch 27/40
 - 1s - loss: 0.0587 - acc: 0.9703
Epoch 28/40
 - 1s - loss: 0.0588 - acc: 0.9712
Epoch 29/40
 - 1s - loss: 0.0575 - acc: 0.9730
Epoch 30/40
 - 1s - loss: 0.0577 - acc: 0.9720
Epoch 31/40
 - 1s - loss: 0.0574 - acc: 0.9718
Epoch 32/40
 - 1s - loss: 0.0566 - acc: 0.9721
Epoch 33/40
 - 1s - loss: 0.0565 - acc: 0.9722
Epoch 34/40
 - 1s - loss: 0.0567 - acc: 0.9714
Epoch 35/40
 - 1s - loss: 0.0554 - acc: 0.9725
Epoch 36/40
 - 1s - loss: 0.0559 - acc: 0.9724
Epoch 37/40
 - 1s - loss: 0.0554 - acc: 0.9730
Epoch 38/40
 - 1s - loss: 0.0560 - acc: 0.9722
Epoch 39/40
 - 1s - loss: 0.0554 - acc: 0.9737
Epoch 40/40
 - 1s - loss: 0.0555 - acc: 0.9719
# Training time = 0:04:07.790711
# F-Score(Ordinary) = 0.362, Recall: 0.585, Precision: 0.262
# F-Score(lvc) = 0.237, Recall: 0.299, Precision: 0.197
# F-Score(ireflv) = 0.567, Recall: 0.686, Precision: 0.484
# F-Score(id) = 0.245, Recall: 1.0, Precision: 0.14
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_211 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_212 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_211 (Embedding)       (None, 4, 48)        705264      input_211[0][0]                  
__________________________________________________________________________________________________
embedding_212 (Embedding)       (None, 4, 24)        5640        input_212[0][0]                  
__________________________________________________________________________________________________
flatten_211 (Flatten)           (None, 192)          0           embedding_211[0][0]              
__________________________________________________________________________________________________
flatten_212 (Flatten)           (None, 96)           0           embedding_212[0][0]              
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 288)          0           flatten_211[0][0]                
                                                                 flatten_212[0][0]                
__________________________________________________________________________________________________
dense_211 (Dense)               (None, 24)           6936        concatenate_106[0][0]            
__________________________________________________________________________________________________
dropout_106 (Dropout)           (None, 24)           0           dense_211[0][0]                  
__________________________________________________________________________________________________
dense_212 (Dense)               (None, 8)            200         dropout_106[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1598 - acc: 0.9485 - val_loss: 0.0947 - val_acc: 0.9638
Epoch 2/40
 - 5s - loss: 0.0907 - acc: 0.9645 - val_loss: 0.0833 - val_acc: 0.9654
Epoch 3/40
 - 5s - loss: 0.0795 - acc: 0.9669 - val_loss: 0.0788 - val_acc: 0.9665
Epoch 4/40
 - 5s - loss: 0.0739 - acc: 0.9679 - val_loss: 0.0761 - val_acc: 0.9671
Epoch 5/40
 - 5s - loss: 0.0697 - acc: 0.9690 - val_loss: 0.0744 - val_acc: 0.9670
Epoch 6/40
 - 5s - loss: 0.0666 - acc: 0.9696 - val_loss: 0.0728 - val_acc: 0.9674
Epoch 7/40
 - 5s - loss: 0.0641 - acc: 0.9702 - val_loss: 0.0728 - val_acc: 0.9677
Epoch 8/40
 - 5s - loss: 0.0624 - acc: 0.9710 - val_loss: 0.0702 - val_acc: 0.9682
Epoch 9/40
 - 5s - loss: 0.0603 - acc: 0.9714 - val_loss: 0.0693 - val_acc: 0.9684
Epoch 10/40
 - 5s - loss: 0.0592 - acc: 0.9712 - val_loss: 0.0695 - val_acc: 0.9680
Epoch 11/40
 - 5s - loss: 0.0577 - acc: 0.9718 - val_loss: 0.0701 - val_acc: 0.9679
Epoch 12/40
 - 5s - loss: 0.0563 - acc: 0.9723 - val_loss: 0.0685 - val_acc: 0.9680
Epoch 13/40
 - 5s - loss: 0.0558 - acc: 0.9728 - val_loss: 0.0683 - val_acc: 0.9684
Epoch 14/40
 - 5s - loss: 0.0548 - acc: 0.9735 - val_loss: 0.0694 - val_acc: 0.9688
Epoch 15/40
 - 5s - loss: 0.0543 - acc: 0.9732 - val_loss: 0.0688 - val_acc: 0.9682
Epoch 16/40
 - 5s - loss: 0.0533 - acc: 0.9731 - val_loss: 0.0684 - val_acc: 0.9680
Epoch 17/40
 - 5s - loss: 0.0530 - acc: 0.9735 - val_loss: 0.0693 - val_acc: 0.9687
Epoch 18/40
 - 5s - loss: 0.0523 - acc: 0.9740 - val_loss: 0.0699 - val_acc: 0.9687
Epoch 19/40
 - 5s - loss: 0.0519 - acc: 0.9738 - val_loss: 0.0719 - val_acc: 0.9685
Epoch 20/40
 - 5s - loss: 0.0518 - acc: 0.9741 - val_loss: 0.0711 - val_acc: 0.9687
Epoch 21/40
 - 5s - loss: 0.0510 - acc: 0.9743 - val_loss: 0.0709 - val_acc: 0.9678
Epoch 22/40
 - 5s - loss: 0.0509 - acc: 0.9745 - val_loss: 0.0714 - val_acc: 0.9683
Epoch 23/40
 - 5s - loss: 0.0506 - acc: 0.9743 - val_loss: 0.0701 - val_acc: 0.9686
Epoch 24/40
 - 5s - loss: 0.0503 - acc: 0.9742 - val_loss: 0.0714 - val_acc: 0.9681
Epoch 25/40
 - 5s - loss: 0.0499 - acc: 0.9745 - val_loss: 0.0726 - val_acc: 0.9685
Epoch 26/40
 - 5s - loss: 0.0494 - acc: 0.9747 - val_loss: 0.0737 - val_acc: 0.9680
Epoch 27/40
 - 5s - loss: 0.0495 - acc: 0.9748 - val_loss: 0.0728 - val_acc: 0.9681
Epoch 28/40
 - 5s - loss: 0.0493 - acc: 0.9744 - val_loss: 0.0720 - val_acc: 0.9684
Epoch 29/40
 - 5s - loss: 0.0491 - acc: 0.9748 - val_loss: 0.0733 - val_acc: 0.9690
Epoch 30/40
 - 5s - loss: 0.0488 - acc: 0.9746 - val_loss: 0.0733 - val_acc: 0.9683
Epoch 31/40
 - 5s - loss: 0.0488 - acc: 0.9747 - val_loss: 0.0721 - val_acc: 0.9685
Epoch 32/40
 - 5s - loss: 0.0483 - acc: 0.9750 - val_loss: 0.0735 - val_acc: 0.9679
Epoch 33/40
 - 5s - loss: 0.0486 - acc: 0.9750 - val_loss: 0.0739 - val_acc: 0.9682
Epoch 34/40
 - 5s - loss: 0.0480 - acc: 0.9751 - val_loss: 0.0751 - val_acc: 0.9688
Epoch 35/40
 - 5s - loss: 0.0480 - acc: 0.9751 - val_loss: 0.0732 - val_acc: 0.9688
Epoch 36/40
 - 5s - loss: 0.0480 - acc: 0.9753 - val_loss: 0.0760 - val_acc: 0.9682
Epoch 37/40
 - 5s - loss: 0.0479 - acc: 0.9754 - val_loss: 0.0759 - val_acc: 0.9690
Epoch 38/40
 - 5s - loss: 0.0479 - acc: 0.9755 - val_loss: 0.0780 - val_acc: 0.9683
Epoch 39/40
 - 5s - loss: 0.0476 - acc: 0.9755 - val_loss: 0.0748 - val_acc: 0.9680
Epoch 40/40
 - 5s - loss: 0.0472 - acc: 0.9755 - val_loss: 0.0793 - val_acc: 0.9688
Epoch 1/40
 - 1s - loss: 0.0752 - acc: 0.9671
Epoch 2/40
 - 1s - loss: 0.0680 - acc: 0.9696
Epoch 3/40
 - 1s - loss: 0.0667 - acc: 0.9691
Epoch 4/40
 - 1s - loss: 0.0625 - acc: 0.9702
Epoch 5/40
 - 1s - loss: 0.0620 - acc: 0.9703
Epoch 6/40
 - 1s - loss: 0.0594 - acc: 0.9705
Epoch 7/40
 - 1s - loss: 0.0586 - acc: 0.9714
Epoch 8/40
 - 1s - loss: 0.0577 - acc: 0.9703
Epoch 9/40
 - 1s - loss: 0.0554 - acc: 0.9708
Epoch 10/40
 - 1s - loss: 0.0558 - acc: 0.9725
Epoch 11/40
 - 1s - loss: 0.0538 - acc: 0.9722
Epoch 12/40
 - 1s - loss: 0.0515 - acc: 0.9732
Epoch 13/40
 - 1s - loss: 0.0505 - acc: 0.9732
Epoch 14/40
 - 1s - loss: 0.0514 - acc: 0.9738
Epoch 15/40
 - 1s - loss: 0.0501 - acc: 0.9726
Epoch 16/40
 - 1s - loss: 0.0492 - acc: 0.9739
Epoch 17/40
 - 1s - loss: 0.0498 - acc: 0.9740
Epoch 18/40
 - 1s - loss: 0.0477 - acc: 0.9742
Epoch 19/40
 - 1s - loss: 0.0473 - acc: 0.9737
Epoch 20/40
 - 1s - loss: 0.0482 - acc: 0.9742
Epoch 21/40
 - 1s - loss: 0.0465 - acc: 0.9748
Epoch 22/40
 - 1s - loss: 0.0459 - acc: 0.9754
Epoch 23/40
 - 1s - loss: 0.0438 - acc: 0.9750
Epoch 24/40
 - 1s - loss: 0.0445 - acc: 0.9744
Epoch 25/40
 - 1s - loss: 0.0436 - acc: 0.9750
Epoch 26/40
 - 1s - loss: 0.0445 - acc: 0.9756
Epoch 27/40
 - 1s - loss: 0.0439 - acc: 0.9751
Epoch 28/40
 - 1s - loss: 0.0431 - acc: 0.9749
Epoch 29/40
 - 1s - loss: 0.0432 - acc: 0.9750
Epoch 30/40
 - 1s - loss: 0.0421 - acc: 0.9761
Epoch 31/40
 - 1s - loss: 0.0410 - acc: 0.9775
Epoch 32/40
 - 1s - loss: 0.0429 - acc: 0.9748
Epoch 33/40
 - 1s - loss: 0.0411 - acc: 0.9766
Epoch 34/40
 - 1s - loss: 0.0412 - acc: 0.9756
Epoch 35/40
 - 1s - loss: 0.0398 - acc: 0.9760
Epoch 36/40
 - 1s - loss: 0.0405 - acc: 0.9762
Epoch 37/40
 - 1s - loss: 0.0409 - acc: 0.9763
Epoch 38/40
 - 1s - loss: 0.0397 - acc: 0.9762
Epoch 39/40
 - 1s - loss: 0.0397 - acc: 0.9764
Epoch 40/40
 - 1s - loss: 0.0394 - acc: 0.9766
# Training time = 0:04:08.442488
# F-Score(Ordinary) = 0.275, Recall: 0.509, Precision: 0.188
# F-Score(lvc) = 0.128, Recall: 1.0, Precision: 0.068
# F-Score(ireflv) = 0.353, Recall: 0.871, Precision: 0.221
# F-Score(id) = 0.302, Recall: 0.384, Precision: 0.249
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_213 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_214 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_213 (Embedding)       (None, 4, 48)        705264      input_213[0][0]                  
__________________________________________________________________________________________________
embedding_214 (Embedding)       (None, 4, 24)        5640        input_214[0][0]                  
__________________________________________________________________________________________________
flatten_213 (Flatten)           (None, 192)          0           embedding_213[0][0]              
__________________________________________________________________________________________________
flatten_214 (Flatten)           (None, 96)           0           embedding_214[0][0]              
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 288)          0           flatten_213[0][0]                
                                                                 flatten_214[0][0]                
__________________________________________________________________________________________________
dense_213 (Dense)               (None, 24)           6936        concatenate_107[0][0]            
__________________________________________________________________________________________________
dropout_107 (Dropout)           (None, 24)           0           dense_213[0][0]                  
__________________________________________________________________________________________________
dense_214 (Dense)               (None, 8)            200         dropout_107[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1639 - acc: 0.9483 - val_loss: 0.0966 - val_acc: 0.9647
Epoch 2/40
 - 5s - loss: 0.0937 - acc: 0.9649 - val_loss: 0.0844 - val_acc: 0.9660
Epoch 3/40
 - 5s - loss: 0.0806 - acc: 0.9670 - val_loss: 0.0793 - val_acc: 0.9666
Epoch 4/40
 - 5s - loss: 0.0743 - acc: 0.9682 - val_loss: 0.0755 - val_acc: 0.9671
Epoch 5/40
 - 5s - loss: 0.0702 - acc: 0.9691 - val_loss: 0.0739 - val_acc: 0.9671
Epoch 6/40
 - 5s - loss: 0.0671 - acc: 0.9698 - val_loss: 0.0716 - val_acc: 0.9675
Epoch 7/40
 - 5s - loss: 0.0653 - acc: 0.9703 - val_loss: 0.0709 - val_acc: 0.9675
Epoch 8/40
 - 5s - loss: 0.0629 - acc: 0.9707 - val_loss: 0.0703 - val_acc: 0.9682
Epoch 9/40
 - 5s - loss: 0.0614 - acc: 0.9712 - val_loss: 0.0706 - val_acc: 0.9685
Epoch 10/40
 - 5s - loss: 0.0606 - acc: 0.9717 - val_loss: 0.0700 - val_acc: 0.9682
Epoch 11/40
 - 5s - loss: 0.0590 - acc: 0.9717 - val_loss: 0.0719 - val_acc: 0.9671
Epoch 12/40
 - 5s - loss: 0.0579 - acc: 0.9720 - val_loss: 0.0701 - val_acc: 0.9676
Epoch 13/40
 - 5s - loss: 0.0570 - acc: 0.9728 - val_loss: 0.0698 - val_acc: 0.9683
Epoch 14/40
 - 5s - loss: 0.0562 - acc: 0.9726 - val_loss: 0.0717 - val_acc: 0.9686
Epoch 15/40
 - 5s - loss: 0.0555 - acc: 0.9728 - val_loss: 0.0693 - val_acc: 0.9685
Epoch 16/40
 - 5s - loss: 0.0546 - acc: 0.9731 - val_loss: 0.0689 - val_acc: 0.9682
Epoch 17/40
 - 5s - loss: 0.0537 - acc: 0.9731 - val_loss: 0.0687 - val_acc: 0.9682
Epoch 18/40
 - 5s - loss: 0.0531 - acc: 0.9737 - val_loss: 0.0697 - val_acc: 0.9684
Epoch 19/40
 - 5s - loss: 0.0530 - acc: 0.9736 - val_loss: 0.0695 - val_acc: 0.9686
Epoch 20/40
 - 5s - loss: 0.0522 - acc: 0.9738 - val_loss: 0.0699 - val_acc: 0.9689
Epoch 21/40
 - 5s - loss: 0.0517 - acc: 0.9740 - val_loss: 0.0702 - val_acc: 0.9684
Epoch 22/40
 - 5s - loss: 0.0513 - acc: 0.9739 - val_loss: 0.0705 - val_acc: 0.9679
Epoch 23/40
 - 5s - loss: 0.0511 - acc: 0.9744 - val_loss: 0.0700 - val_acc: 0.9684
Epoch 24/40
 - 5s - loss: 0.0509 - acc: 0.9742 - val_loss: 0.0733 - val_acc: 0.9677
Epoch 25/40
 - 5s - loss: 0.0508 - acc: 0.9741 - val_loss: 0.0737 - val_acc: 0.9679
Epoch 26/40
 - 5s - loss: 0.0502 - acc: 0.9748 - val_loss: 0.0735 - val_acc: 0.9687
Epoch 27/40
 - 5s - loss: 0.0503 - acc: 0.9748 - val_loss: 0.0726 - val_acc: 0.9684
Epoch 28/40
 - 5s - loss: 0.0501 - acc: 0.9742 - val_loss: 0.0722 - val_acc: 0.9680
Epoch 29/40
 - 5s - loss: 0.0497 - acc: 0.9750 - val_loss: 0.0751 - val_acc: 0.9680
Epoch 30/40
 - 5s - loss: 0.0496 - acc: 0.9751 - val_loss: 0.0729 - val_acc: 0.9681
Epoch 31/40
 - 5s - loss: 0.0493 - acc: 0.9749 - val_loss: 0.0752 - val_acc: 0.9676
Epoch 32/40
 - 5s - loss: 0.0494 - acc: 0.9751 - val_loss: 0.0743 - val_acc: 0.9683
Epoch 33/40
 - 5s - loss: 0.0492 - acc: 0.9749 - val_loss: 0.0774 - val_acc: 0.9681
Epoch 34/40
 - 5s - loss: 0.0489 - acc: 0.9749 - val_loss: 0.0748 - val_acc: 0.9672
Epoch 35/40
 - 5s - loss: 0.0489 - acc: 0.9748 - val_loss: 0.0733 - val_acc: 0.9678
Epoch 36/40
 - 5s - loss: 0.0487 - acc: 0.9751 - val_loss: 0.0745 - val_acc: 0.9668
Epoch 37/40
 - 5s - loss: 0.0481 - acc: 0.9755 - val_loss: 0.0753 - val_acc: 0.9669
Epoch 38/40
 - 5s - loss: 0.0485 - acc: 0.9752 - val_loss: 0.0757 - val_acc: 0.9688
Epoch 39/40
 - 5s - loss: 0.0482 - acc: 0.9751 - val_loss: 0.0759 - val_acc: 0.9679
Epoch 40/40
 - 5s - loss: 0.0484 - acc: 0.9752 - val_loss: 0.0760 - val_acc: 0.9686
Epoch 1/40
 - 1s - loss: 0.0739 - acc: 0.9671
Epoch 2/40
 - 1s - loss: 0.0696 - acc: 0.9673
Epoch 3/40
 - 1s - loss: 0.0660 - acc: 0.9692
Epoch 4/40
 - 1s - loss: 0.0639 - acc: 0.9685
Epoch 5/40
 - 1s - loss: 0.0632 - acc: 0.9693
Epoch 6/40
 - 1s - loss: 0.0586 - acc: 0.9718
Epoch 7/40
 - 1s - loss: 0.0589 - acc: 0.9704
Epoch 8/40
 - 1s - loss: 0.0567 - acc: 0.9719
Epoch 9/40
 - 1s - loss: 0.0556 - acc: 0.9720
Epoch 10/40
 - 1s - loss: 0.0549 - acc: 0.9716
Epoch 11/40
 - 1s - loss: 0.0545 - acc: 0.9718
Epoch 12/40
 - 1s - loss: 0.0536 - acc: 0.9735
Epoch 13/40
 - 1s - loss: 0.0521 - acc: 0.9722
Epoch 14/40
 - 1s - loss: 0.0520 - acc: 0.9728
Epoch 15/40
 - 1s - loss: 0.0514 - acc: 0.9731
Epoch 16/40
 - 1s - loss: 0.0494 - acc: 0.9746
Epoch 17/40
 - 1s - loss: 0.0493 - acc: 0.9742
Epoch 18/40
 - 1s - loss: 0.0495 - acc: 0.9735
Epoch 19/40
 - 1s - loss: 0.0484 - acc: 0.9745
Epoch 20/40
 - 1s - loss: 0.0476 - acc: 0.9739
Epoch 21/40
 - 1s - loss: 0.0475 - acc: 0.9740
Epoch 22/40
 - 1s - loss: 0.0467 - acc: 0.9754
Epoch 23/40
 - 1s - loss: 0.0463 - acc: 0.9745
Epoch 24/40
 - 1s - loss: 0.0463 - acc: 0.9753
Epoch 25/40
 - 1s - loss: 0.0464 - acc: 0.9752
Epoch 26/40
 - 1s - loss: 0.0451 - acc: 0.9751
Epoch 27/40
 - 1s - loss: 0.0453 - acc: 0.9759
Epoch 28/40
 - 1s - loss: 0.0445 - acc: 0.9764
Epoch 29/40
 - 1s - loss: 0.0444 - acc: 0.9765
Epoch 30/40
 - 1s - loss: 0.0439 - acc: 0.9762
Epoch 31/40
 - 1s - loss: 0.0429 - acc: 0.9763
Epoch 32/40
 - 1s - loss: 0.0429 - acc: 0.9756
Epoch 33/40
 - 1s - loss: 0.0435 - acc: 0.9762
Epoch 34/40
 - 1s - loss: 0.0422 - acc: 0.9761
Epoch 35/40
 - 1s - loss: 0.0432 - acc: 0.9764
Epoch 36/40
 - 1s - loss: 0.0414 - acc: 0.9763
Epoch 37/40
 - 1s - loss: 0.0416 - acc: 0.9755
Epoch 38/40
 - 1s - loss: 0.0406 - acc: 0.9767
Epoch 39/40
 - 1s - loss: 0.0416 - acc: 0.9760
Epoch 40/40
 - 1s - loss: 0.0406 - acc: 0.9764
# Training time = 0:04:08.472790
# F-Score(Ordinary) = 0.276, Recall: 0.558, Precision: 0.183
# F-Score(lvc) = 0.063, Recall: 0.103, Precision: 0.045
# F-Score(ireflv) = 0.256, Recall: 0.588, Precision: 0.164
# F-Score(id) = 0.427, Recall: 0.964, Precision: 0.275
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_215 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_216 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_215 (Embedding)       (None, 4, 48)        705264      input_215[0][0]                  
__________________________________________________________________________________________________
embedding_216 (Embedding)       (None, 4, 24)        5640        input_216[0][0]                  
__________________________________________________________________________________________________
flatten_215 (Flatten)           (None, 192)          0           embedding_215[0][0]              
__________________________________________________________________________________________________
flatten_216 (Flatten)           (None, 96)           0           embedding_216[0][0]              
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 288)          0           flatten_215[0][0]                
                                                                 flatten_216[0][0]                
__________________________________________________________________________________________________
dense_215 (Dense)               (None, 24)           6936        concatenate_108[0][0]            
__________________________________________________________________________________________________
dropout_108 (Dropout)           (None, 24)           0           dense_215[0][0]                  
__________________________________________________________________________________________________
dense_216 (Dense)               (None, 8)            200         dropout_108[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1619 - acc: 0.9506 - val_loss: 0.0971 - val_acc: 0.9646
Epoch 2/40
 - 5s - loss: 0.0915 - acc: 0.9649 - val_loss: 0.0833 - val_acc: 0.9667
Epoch 3/40
 - 5s - loss: 0.0796 - acc: 0.9670 - val_loss: 0.0773 - val_acc: 0.9671
Epoch 4/40
 - 5s - loss: 0.0731 - acc: 0.9679 - val_loss: 0.0756 - val_acc: 0.9671
Epoch 5/40
 - 5s - loss: 0.0692 - acc: 0.9693 - val_loss: 0.0732 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0661 - acc: 0.9697 - val_loss: 0.0715 - val_acc: 0.9682
Epoch 7/40
 - 5s - loss: 0.0639 - acc: 0.9707 - val_loss: 0.0705 - val_acc: 0.9680
Epoch 8/40
 - 5s - loss: 0.0620 - acc: 0.9708 - val_loss: 0.0705 - val_acc: 0.9686
Epoch 9/40
 - 5s - loss: 0.0603 - acc: 0.9712 - val_loss: 0.0703 - val_acc: 0.9679
Epoch 10/40
 - 5s - loss: 0.0590 - acc: 0.9721 - val_loss: 0.0689 - val_acc: 0.9680
Epoch 11/40
 - 5s - loss: 0.0578 - acc: 0.9722 - val_loss: 0.0700 - val_acc: 0.9682
Epoch 12/40
 - 5s - loss: 0.0566 - acc: 0.9724 - val_loss: 0.0698 - val_acc: 0.9687
Epoch 13/40
 - 5s - loss: 0.0558 - acc: 0.9729 - val_loss: 0.0683 - val_acc: 0.9684
Epoch 14/40
 - 5s - loss: 0.0552 - acc: 0.9729 - val_loss: 0.0703 - val_acc: 0.9685
Epoch 15/40
 - 5s - loss: 0.0544 - acc: 0.9732 - val_loss: 0.0686 - val_acc: 0.9683
Epoch 16/40
 - 5s - loss: 0.0540 - acc: 0.9735 - val_loss: 0.0683 - val_acc: 0.9684
Epoch 17/40
 - 5s - loss: 0.0532 - acc: 0.9736 - val_loss: 0.0692 - val_acc: 0.9686
Epoch 18/40
 - 5s - loss: 0.0530 - acc: 0.9738 - val_loss: 0.0701 - val_acc: 0.9686
Epoch 19/40
 - 5s - loss: 0.0523 - acc: 0.9740 - val_loss: 0.0694 - val_acc: 0.9685
Epoch 20/40
 - 5s - loss: 0.0515 - acc: 0.9742 - val_loss: 0.0693 - val_acc: 0.9686
Epoch 21/40
 - 5s - loss: 0.0516 - acc: 0.9745 - val_loss: 0.0709 - val_acc: 0.9683
Epoch 22/40
 - 5s - loss: 0.0508 - acc: 0.9747 - val_loss: 0.0706 - val_acc: 0.9682
Epoch 23/40
 - 5s - loss: 0.0510 - acc: 0.9745 - val_loss: 0.0707 - val_acc: 0.9686
Epoch 24/40
 - 5s - loss: 0.0506 - acc: 0.9745 - val_loss: 0.0710 - val_acc: 0.9685
Epoch 25/40
 - 5s - loss: 0.0502 - acc: 0.9748 - val_loss: 0.0733 - val_acc: 0.9682
Epoch 26/40
 - 5s - loss: 0.0498 - acc: 0.9749 - val_loss: 0.0709 - val_acc: 0.9676
Epoch 27/40
 - 5s - loss: 0.0493 - acc: 0.9747 - val_loss: 0.0732 - val_acc: 0.9683
Epoch 28/40
 - 5s - loss: 0.0494 - acc: 0.9750 - val_loss: 0.0711 - val_acc: 0.9677
Epoch 29/40
 - 5s - loss: 0.0494 - acc: 0.9753 - val_loss: 0.0718 - val_acc: 0.9686
Epoch 30/40
 - 5s - loss: 0.0493 - acc: 0.9750 - val_loss: 0.0716 - val_acc: 0.9677
Epoch 31/40
 - 5s - loss: 0.0486 - acc: 0.9753 - val_loss: 0.0737 - val_acc: 0.9683
Epoch 32/40
 - 5s - loss: 0.0488 - acc: 0.9751 - val_loss: 0.0750 - val_acc: 0.9686
Epoch 33/40
 - 5s - loss: 0.0484 - acc: 0.9751 - val_loss: 0.0757 - val_acc: 0.9691
Epoch 34/40
 - 5s - loss: 0.0486 - acc: 0.9750 - val_loss: 0.0756 - val_acc: 0.9664
Epoch 35/40
 - 5s - loss: 0.0478 - acc: 0.9752 - val_loss: 0.0764 - val_acc: 0.9676
Epoch 36/40
 - 5s - loss: 0.0483 - acc: 0.9754 - val_loss: 0.0766 - val_acc: 0.9686
Epoch 37/40
 - 5s - loss: 0.0480 - acc: 0.9758 - val_loss: 0.0751 - val_acc: 0.9676
Epoch 38/40
 - 5s - loss: 0.0479 - acc: 0.9755 - val_loss: 0.0775 - val_acc: 0.9681
Epoch 39/40
 - 5s - loss: 0.0478 - acc: 0.9757 - val_loss: 0.0757 - val_acc: 0.9673
Epoch 40/40
 - 5s - loss: 0.0483 - acc: 0.9757 - val_loss: 0.0753 - val_acc: 0.9687
Epoch 1/40
 - 1s - loss: 0.0746 - acc: 0.9658
Epoch 2/40
 - 1s - loss: 0.0707 - acc: 0.9683
Epoch 3/40
 - 1s - loss: 0.0674 - acc: 0.9697
Epoch 4/40
 - 1s - loss: 0.0643 - acc: 0.9707
Epoch 5/40
 - 1s - loss: 0.0614 - acc: 0.9716
Epoch 6/40
 - 1s - loss: 0.0604 - acc: 0.9710
Epoch 7/40
 - 1s - loss: 0.0595 - acc: 0.9719
Epoch 8/40
 - 1s - loss: 0.0569 - acc: 0.9722
Epoch 9/40
 - 1s - loss: 0.0555 - acc: 0.9723
Epoch 10/40
 - 1s - loss: 0.0528 - acc: 0.9735
Epoch 11/40
 - 1s - loss: 0.0535 - acc: 0.9724
Epoch 12/40
 - 1s - loss: 0.0520 - acc: 0.9732
Epoch 13/40
 - 1s - loss: 0.0504 - acc: 0.9739
Epoch 14/40
 - 1s - loss: 0.0499 - acc: 0.9745
Epoch 15/40
 - 1s - loss: 0.0495 - acc: 0.9740
Epoch 16/40
 - 1s - loss: 0.0494 - acc: 0.9741
Epoch 17/40
 - 1s - loss: 0.0480 - acc: 0.9747
Epoch 18/40
 - 1s - loss: 0.0471 - acc: 0.9754
Epoch 19/40
 - 1s - loss: 0.0474 - acc: 0.9742
Epoch 20/40
 - 1s - loss: 0.0457 - acc: 0.9748
Epoch 21/40
 - 1s - loss: 0.0462 - acc: 0.9756
Epoch 22/40
 - 1s - loss: 0.0451 - acc: 0.9750
Epoch 23/40
 - 1s - loss: 0.0448 - acc: 0.9765
Epoch 24/40
 - 1s - loss: 0.0438 - acc: 0.9760
Epoch 25/40
 - 1s - loss: 0.0429 - acc: 0.9749
Epoch 26/40
 - 1s - loss: 0.0419 - acc: 0.9768
Epoch 27/40
 - 1s - loss: 0.0419 - acc: 0.9756
Epoch 28/40
 - 1s - loss: 0.0428 - acc: 0.9751
Epoch 29/40
 - 1s - loss: 0.0427 - acc: 0.9750
Epoch 30/40
 - 1s - loss: 0.0411 - acc: 0.9770
Epoch 31/40
 - 1s - loss: 0.0417 - acc: 0.9757
Epoch 32/40
 - 1s - loss: 0.0416 - acc: 0.9755
Epoch 33/40
 - 1s - loss: 0.0406 - acc: 0.9773
Epoch 34/40
 - 1s - loss: 0.0401 - acc: 0.9771
Epoch 35/40
 - 1s - loss: 0.0402 - acc: 0.9766
Epoch 36/40
 - 1s - loss: 0.0386 - acc: 0.9774
Epoch 37/40
 - 1s - loss: 0.0398 - acc: 0.9775
Epoch 38/40
 - 1s - loss: 0.0387 - acc: 0.9782
Epoch 39/40
 - 1s - loss: 0.0386 - acc: 0.9767
Epoch 40/40
 - 1s - loss: 0.0387 - acc: 0.9774
# Training time = 0:04:09.118627
# F-Score(Ordinary) = 0.284, Recall: 0.718, Precision: 0.177
# F-Score(lvc) = 0.296, Recall: 0.491, Precision: 0.212
# F-Score(ireflv) = 0.358, Recall: 0.931, Precision: 0.221
# F-Score(id) = 0.221, Recall: 1.0, Precision: 0.124
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_217 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_218 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_217 (Embedding)       (None, 4, 48)        705264      input_217[0][0]                  
__________________________________________________________________________________________________
embedding_218 (Embedding)       (None, 4, 24)        5640        input_218[0][0]                  
__________________________________________________________________________________________________
flatten_217 (Flatten)           (None, 192)          0           embedding_217[0][0]              
__________________________________________________________________________________________________
flatten_218 (Flatten)           (None, 96)           0           embedding_218[0][0]              
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 288)          0           flatten_217[0][0]                
                                                                 flatten_218[0][0]                
__________________________________________________________________________________________________
dense_217 (Dense)               (None, 24)           6936        concatenate_109[0][0]            
__________________________________________________________________________________________________
dropout_109 (Dropout)           (None, 24)           0           dense_217[0][0]                  
__________________________________________________________________________________________________
dense_218 (Dense)               (None, 8)            200         dropout_109[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1715 - acc: 0.9464 - val_loss: 0.0971 - val_acc: 0.9647
Epoch 2/40
 - 5s - loss: 0.0942 - acc: 0.9643 - val_loss: 0.0844 - val_acc: 0.9663
Epoch 3/40
 - 5s - loss: 0.0824 - acc: 0.9669 - val_loss: 0.0792 - val_acc: 0.9672
Epoch 4/40
 - 5s - loss: 0.0761 - acc: 0.9680 - val_loss: 0.0773 - val_acc: 0.9677
Epoch 5/40
 - 5s - loss: 0.0715 - acc: 0.9690 - val_loss: 0.0752 - val_acc: 0.9676
Epoch 6/40
 - 5s - loss: 0.0687 - acc: 0.9699 - val_loss: 0.0732 - val_acc: 0.9684
Epoch 7/40
 - 5s - loss: 0.0661 - acc: 0.9705 - val_loss: 0.0720 - val_acc: 0.9683
Epoch 8/40
 - 5s - loss: 0.0637 - acc: 0.9708 - val_loss: 0.0717 - val_acc: 0.9682
Epoch 9/40
 - 5s - loss: 0.0616 - acc: 0.9710 - val_loss: 0.0706 - val_acc: 0.9686
Epoch 10/40
 - 5s - loss: 0.0602 - acc: 0.9721 - val_loss: 0.0695 - val_acc: 0.9681
Epoch 11/40
 - 5s - loss: 0.0589 - acc: 0.9721 - val_loss: 0.0690 - val_acc: 0.9681
Epoch 12/40
 - 5s - loss: 0.0580 - acc: 0.9724 - val_loss: 0.0690 - val_acc: 0.9684
Epoch 13/40
 - 5s - loss: 0.0566 - acc: 0.9729 - val_loss: 0.0694 - val_acc: 0.9682
Epoch 14/40
 - 5s - loss: 0.0559 - acc: 0.9731 - val_loss: 0.0684 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0550 - acc: 0.9729 - val_loss: 0.0697 - val_acc: 0.9688
Epoch 16/40
 - 5s - loss: 0.0546 - acc: 0.9732 - val_loss: 0.0703 - val_acc: 0.9683
Epoch 17/40
 - 5s - loss: 0.0537 - acc: 0.9740 - val_loss: 0.0704 - val_acc: 0.9686
Epoch 18/40
 - 5s - loss: 0.0528 - acc: 0.9739 - val_loss: 0.0701 - val_acc: 0.9685
Epoch 19/40
 - 5s - loss: 0.0524 - acc: 0.9737 - val_loss: 0.0720 - val_acc: 0.9680
Epoch 20/40
 - 5s - loss: 0.0523 - acc: 0.9739 - val_loss: 0.0699 - val_acc: 0.9679
Epoch 21/40
 - 5s - loss: 0.0517 - acc: 0.9743 - val_loss: 0.0724 - val_acc: 0.9680
Epoch 22/40
 - 5s - loss: 0.0513 - acc: 0.9742 - val_loss: 0.0721 - val_acc: 0.9686
Epoch 23/40
 - 5s - loss: 0.0513 - acc: 0.9740 - val_loss: 0.0711 - val_acc: 0.9681
Epoch 24/40
 - 5s - loss: 0.0512 - acc: 0.9745 - val_loss: 0.0703 - val_acc: 0.9687
Epoch 25/40
 - 5s - loss: 0.0505 - acc: 0.9748 - val_loss: 0.0716 - val_acc: 0.9686
Epoch 26/40
 - 5s - loss: 0.0501 - acc: 0.9749 - val_loss: 0.0711 - val_acc: 0.9686
Epoch 27/40
 - 5s - loss: 0.0497 - acc: 0.9751 - val_loss: 0.0710 - val_acc: 0.9687
Epoch 28/40
 - 5s - loss: 0.0498 - acc: 0.9749 - val_loss: 0.0729 - val_acc: 0.9686
Epoch 29/40
 - 5s - loss: 0.0499 - acc: 0.9750 - val_loss: 0.0710 - val_acc: 0.9687
Epoch 30/40
 - 5s - loss: 0.0493 - acc: 0.9748 - val_loss: 0.0717 - val_acc: 0.9680
Epoch 31/40
 - 5s - loss: 0.0492 - acc: 0.9747 - val_loss: 0.0731 - val_acc: 0.9682
Epoch 32/40
 - 5s - loss: 0.0492 - acc: 0.9751 - val_loss: 0.0725 - val_acc: 0.9685
Epoch 33/40
 - 5s - loss: 0.0486 - acc: 0.9752 - val_loss: 0.0749 - val_acc: 0.9685
Epoch 34/40
 - 5s - loss: 0.0486 - acc: 0.9749 - val_loss: 0.0747 - val_acc: 0.9680
Epoch 35/40
 - 5s - loss: 0.0484 - acc: 0.9754 - val_loss: 0.0748 - val_acc: 0.9686
Epoch 36/40
 - 5s - loss: 0.0479 - acc: 0.9752 - val_loss: 0.0752 - val_acc: 0.9683
Epoch 37/40
 - 5s - loss: 0.0482 - acc: 0.9752 - val_loss: 0.0733 - val_acc: 0.9677
Epoch 38/40
 - 5s - loss: 0.0478 - acc: 0.9753 - val_loss: 0.0771 - val_acc: 0.9686
Epoch 39/40
 - 5s - loss: 0.0478 - acc: 0.9755 - val_loss: 0.0779 - val_acc: 0.9678
Epoch 40/40
 - 5s - loss: 0.0474 - acc: 0.9757 - val_loss: 0.0750 - val_acc: 0.9685
Epoch 1/40
 - 1s - loss: 0.0752 - acc: 0.9670
Epoch 2/40
 - 1s - loss: 0.0694 - acc: 0.9683
Epoch 3/40
 - 1s - loss: 0.0654 - acc: 0.9695
Epoch 4/40
 - 1s - loss: 0.0649 - acc: 0.9699
Epoch 5/40
 - 1s - loss: 0.0609 - acc: 0.9692
Epoch 6/40
 - 1s - loss: 0.0597 - acc: 0.9716
Epoch 7/40
 - 1s - loss: 0.0591 - acc: 0.9712
Epoch 8/40
 - 1s - loss: 0.0575 - acc: 0.9714
Epoch 9/40
 - 1s - loss: 0.0572 - acc: 0.9720
Epoch 10/40
 - 1s - loss: 0.0547 - acc: 0.9727
Epoch 11/40
 - 1s - loss: 0.0534 - acc: 0.9732
Epoch 12/40
 - 1s - loss: 0.0525 - acc: 0.9733
Epoch 13/40
 - 1s - loss: 0.0513 - acc: 0.9732
Epoch 14/40
 - 1s - loss: 0.0499 - acc: 0.9753
Epoch 15/40
 - 1s - loss: 0.0503 - acc: 0.9746
Epoch 16/40
 - 1s - loss: 0.0498 - acc: 0.9739
Epoch 17/40
 - 1s - loss: 0.0487 - acc: 0.9740
Epoch 18/40
 - 1s - loss: 0.0482 - acc: 0.9745
Epoch 19/40
 - 1s - loss: 0.0484 - acc: 0.9747
Epoch 20/40
 - 1s - loss: 0.0471 - acc: 0.9754
Epoch 21/40
 - 1s - loss: 0.0464 - acc: 0.9756
Epoch 22/40
 - 1s - loss: 0.0453 - acc: 0.9759
Epoch 23/40
 - 1s - loss: 0.0458 - acc: 0.9753
Epoch 24/40
 - 1s - loss: 0.0449 - acc: 0.9758
Epoch 25/40
 - 1s - loss: 0.0441 - acc: 0.9766
Epoch 26/40
 - 1s - loss: 0.0447 - acc: 0.9758
Epoch 27/40
 - 1s - loss: 0.0431 - acc: 0.9753
Epoch 28/40
 - 1s - loss: 0.0438 - acc: 0.9754
Epoch 29/40
 - 1s - loss: 0.0420 - acc: 0.9755
Epoch 30/40
 - 1s - loss: 0.0434 - acc: 0.9753
Epoch 31/40
 - 1s - loss: 0.0423 - acc: 0.9769
Epoch 32/40
 - 1s - loss: 0.0425 - acc: 0.9765
Epoch 33/40
 - 1s - loss: 0.0418 - acc: 0.9761
Epoch 34/40
 - 1s - loss: 0.0417 - acc: 0.9773
Epoch 35/40
 - 1s - loss: 0.0412 - acc: 0.9766
Epoch 36/40
 - 1s - loss: 0.0409 - acc: 0.9770
Epoch 37/40
 - 1s - loss: 0.0399 - acc: 0.9777
Epoch 38/40
 - 1s - loss: 0.0397 - acc: 0.9774
Epoch 39/40
 - 1s - loss: 0.0405 - acc: 0.9768
Epoch 40/40
 - 1s - loss: 0.0396 - acc: 0.9770
# Training time = 0:04:08.237194
# F-Score(Ordinary) = 0.346, Recall: 0.52, Precision: 0.26
# F-Score(lvc) = 0.421, Recall: 0.372, Precision: 0.485
# F-Score(ireflv) = 0.183, Recall: 0.65, Precision: 0.107
# F-Score(id) = 0.277, Recall: 1.0, Precision: 0.161
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_219 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_220 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_219 (Embedding)       (None, 4, 48)        705264      input_219[0][0]                  
__________________________________________________________________________________________________
embedding_220 (Embedding)       (None, 4, 24)        5640        input_220[0][0]                  
__________________________________________________________________________________________________
flatten_219 (Flatten)           (None, 192)          0           embedding_219[0][0]              
__________________________________________________________________________________________________
flatten_220 (Flatten)           (None, 96)           0           embedding_220[0][0]              
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 288)          0           flatten_219[0][0]                
                                                                 flatten_220[0][0]                
__________________________________________________________________________________________________
dense_219 (Dense)               (None, 24)           6936        concatenate_110[0][0]            
__________________________________________________________________________________________________
dropout_110 (Dropout)           (None, 24)           0           dense_219[0][0]                  
__________________________________________________________________________________________________
dense_220 (Dense)               (None, 8)            200         dropout_110[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 0.5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1654 - acc: 0.9467 - val_loss: 0.0974 - val_acc: 0.9646
Epoch 2/40
 - 5s - loss: 0.0921 - acc: 0.9645 - val_loss: 0.0831 - val_acc: 0.9663
Epoch 3/40
 - 5s - loss: 0.0801 - acc: 0.9662 - val_loss: 0.0781 - val_acc: 0.9668
Epoch 4/40
 - 5s - loss: 0.0743 - acc: 0.9682 - val_loss: 0.0761 - val_acc: 0.9676
Epoch 5/40
 - 5s - loss: 0.0705 - acc: 0.9693 - val_loss: 0.0741 - val_acc: 0.9674
Epoch 6/40
 - 5s - loss: 0.0672 - acc: 0.9699 - val_loss: 0.0725 - val_acc: 0.9679
Epoch 7/40
 - 5s - loss: 0.0652 - acc: 0.9699 - val_loss: 0.0729 - val_acc: 0.9682
Epoch 8/40
 - 5s - loss: 0.0632 - acc: 0.9708 - val_loss: 0.0706 - val_acc: 0.9682
Epoch 9/40
 - 5s - loss: 0.0615 - acc: 0.9713 - val_loss: 0.0702 - val_acc: 0.9679
Epoch 10/40
 - 5s - loss: 0.0601 - acc: 0.9718 - val_loss: 0.0708 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0591 - acc: 0.9719 - val_loss: 0.0699 - val_acc: 0.9678
Epoch 12/40
 - 5s - loss: 0.0581 - acc: 0.9720 - val_loss: 0.0688 - val_acc: 0.9681
Epoch 13/40
 - 5s - loss: 0.0570 - acc: 0.9726 - val_loss: 0.0685 - val_acc: 0.9686
Epoch 14/40
 - 5s - loss: 0.0562 - acc: 0.9730 - val_loss: 0.0695 - val_acc: 0.9682
Epoch 15/40
 - 5s - loss: 0.0553 - acc: 0.9731 - val_loss: 0.0682 - val_acc: 0.9687
Epoch 16/40
 - 5s - loss: 0.0548 - acc: 0.9731 - val_loss: 0.0687 - val_acc: 0.9684
Epoch 17/40
 - 5s - loss: 0.0543 - acc: 0.9731 - val_loss: 0.0686 - val_acc: 0.9681
Epoch 18/40
 - 5s - loss: 0.0535 - acc: 0.9735 - val_loss: 0.0692 - val_acc: 0.9682
Epoch 19/40
 - 5s - loss: 0.0530 - acc: 0.9738 - val_loss: 0.0688 - val_acc: 0.9679
Epoch 20/40
 - 5s - loss: 0.0526 - acc: 0.9741 - val_loss: 0.0700 - val_acc: 0.9682
Epoch 21/40
 - 5s - loss: 0.0518 - acc: 0.9744 - val_loss: 0.0699 - val_acc: 0.9680
Epoch 22/40
 - 5s - loss: 0.0520 - acc: 0.9741 - val_loss: 0.0697 - val_acc: 0.9682
Epoch 23/40
 - 5s - loss: 0.0513 - acc: 0.9743 - val_loss: 0.0694 - val_acc: 0.9683
Epoch 24/40
 - 5s - loss: 0.0508 - acc: 0.9743 - val_loss: 0.0709 - val_acc: 0.9679
Epoch 25/40
 - 5s - loss: 0.0505 - acc: 0.9746 - val_loss: 0.0701 - val_acc: 0.9679
Epoch 26/40
 - 5s - loss: 0.0504 - acc: 0.9748 - val_loss: 0.0723 - val_acc: 0.9684
Epoch 27/40
 - 5s - loss: 0.0499 - acc: 0.9748 - val_loss: 0.0704 - val_acc: 0.9690
Epoch 28/40
 - 5s - loss: 0.0495 - acc: 0.9747 - val_loss: 0.0726 - val_acc: 0.9680
Epoch 29/40
 - 5s - loss: 0.0492 - acc: 0.9747 - val_loss: 0.0726 - val_acc: 0.9677
Epoch 30/40
 - 5s - loss: 0.0492 - acc: 0.9749 - val_loss: 0.0711 - val_acc: 0.9686
Epoch 31/40
 - 5s - loss: 0.0490 - acc: 0.9752 - val_loss: 0.0713 - val_acc: 0.9675
Epoch 32/40
 - 5s - loss: 0.0491 - acc: 0.9754 - val_loss: 0.0723 - val_acc: 0.9685
Epoch 33/40
 - 5s - loss: 0.0487 - acc: 0.9752 - val_loss: 0.0727 - val_acc: 0.9680
Epoch 34/40
 - 5s - loss: 0.0486 - acc: 0.9755 - val_loss: 0.0727 - val_acc: 0.9682
Epoch 35/40
 - 5s - loss: 0.0484 - acc: 0.9751 - val_loss: 0.0766 - val_acc: 0.9682
Epoch 36/40
 - 5s - loss: 0.0484 - acc: 0.9753 - val_loss: 0.0737 - val_acc: 0.9675
Epoch 37/40
 - 5s - loss: 0.0480 - acc: 0.9758 - val_loss: 0.0732 - val_acc: 0.9682
Epoch 38/40
 - 5s - loss: 0.0480 - acc: 0.9756 - val_loss: 0.0757 - val_acc: 0.9677
Epoch 39/40
 - 5s - loss: 0.0481 - acc: 0.9753 - val_loss: 0.0732 - val_acc: 0.9677
Epoch 40/40
 - 5s - loss: 0.0481 - acc: 0.9756 - val_loss: 0.0759 - val_acc: 0.9678
Epoch 1/40
 - 1s - loss: 0.0764 - acc: 0.9661
Epoch 2/40
 - 1s - loss: 0.0702 - acc: 0.9677
Epoch 3/40
 - 1s - loss: 0.0645 - acc: 0.9693
Epoch 4/40
 - 1s - loss: 0.0639 - acc: 0.9699
Epoch 5/40
 - 1s - loss: 0.0616 - acc: 0.9704
Epoch 6/40
 - 1s - loss: 0.0592 - acc: 0.9711
Epoch 7/40
 - 1s - loss: 0.0566 - acc: 0.9712
Epoch 8/40
 - 1s - loss: 0.0558 - acc: 0.9723
Epoch 9/40
 - 1s - loss: 0.0541 - acc: 0.9719
Epoch 10/40
 - 1s - loss: 0.0532 - acc: 0.9718
Epoch 11/40
 - 1s - loss: 0.0529 - acc: 0.9726
Epoch 12/40
 - 1s - loss: 0.0509 - acc: 0.9736
Epoch 13/40
 - 1s - loss: 0.0503 - acc: 0.9736
Epoch 14/40
 - 1s - loss: 0.0498 - acc: 0.9739
Epoch 15/40
 - 1s - loss: 0.0477 - acc: 0.9742
Epoch 16/40
 - 1s - loss: 0.0476 - acc: 0.9739
Epoch 17/40
 - 1s - loss: 0.0476 - acc: 0.9745
Epoch 18/40
 - 1s - loss: 0.0458 - acc: 0.9758
Epoch 19/40
 - 1s - loss: 0.0459 - acc: 0.9752
Epoch 20/40
 - 1s - loss: 0.0452 - acc: 0.9744
Epoch 21/40
 - 1s - loss: 0.0455 - acc: 0.9754
Epoch 22/40
 - 1s - loss: 0.0440 - acc: 0.9748
Epoch 23/40
 - 1s - loss: 0.0426 - acc: 0.9762
Epoch 24/40
 - 1s - loss: 0.0427 - acc: 0.9761
Epoch 25/40
 - 1s - loss: 0.0423 - acc: 0.9759
Epoch 26/40
 - 1s - loss: 0.0425 - acc: 0.9757
Epoch 27/40
 - 1s - loss: 0.0425 - acc: 0.9759
Epoch 28/40
 - 1s - loss: 0.0415 - acc: 0.9760
Epoch 29/40
 - 1s - loss: 0.0415 - acc: 0.9764
Epoch 30/40
 - 1s - loss: 0.0413 - acc: 0.9770
Epoch 31/40
 - 1s - loss: 0.0405 - acc: 0.9766
Epoch 32/40
 - 1s - loss: 0.0403 - acc: 0.9769
Epoch 33/40
 - 1s - loss: 0.0400 - acc: 0.9770
Epoch 34/40
 - 1s - loss: 0.0396 - acc: 0.9779
Epoch 35/40
 - 1s - loss: 0.0388 - acc: 0.9777
Epoch 36/40
 - 1s - loss: 0.0386 - acc: 0.9776
Epoch 37/40
 - 1s - loss: 0.0394 - acc: 0.9763
Epoch 38/40
 - 1s - loss: 0.0385 - acc: 0.9770
Epoch 39/40
 - 1s - loss: 0.0395 - acc: 0.9772
Epoch 40/40
 - 1s - loss: 0.0381 - acc: 0.9779
# Training time = 0:04:02.793318
# F-Score(Ordinary) = 0.499, Recall: 0.667, Precision: 0.398
# F-Score(lvc) = 0.469, Recall: 0.523, Precision: 0.424
# F-Score(ireflv) = 0.459, Recall: 0.566, Precision: 0.385
# F-Score(id) = 0.541, Recall: 0.948, Precision: 0.378
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_221 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_222 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_221 (Embedding)       (None, 4, 48)        705264      input_221[0][0]                  
__________________________________________________________________________________________________
embedding_222 (Embedding)       (None, 4, 24)        5640        input_222[0][0]                  
__________________________________________________________________________________________________
flatten_221 (Flatten)           (None, 192)          0           embedding_221[0][0]              
__________________________________________________________________________________________________
flatten_222 (Flatten)           (None, 96)           0           embedding_222[0][0]              
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 288)          0           flatten_221[0][0]                
                                                                 flatten_222[0][0]                
__________________________________________________________________________________________________
dense_221 (Dense)               (None, 24)           6936        concatenate_111[0][0]            
__________________________________________________________________________________________________
dropout_111 (Dropout)           (None, 24)           0           dense_221[0][0]                  
__________________________________________________________________________________________________
dense_222 (Dense)               (None, 8)            200         dropout_111[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 1.0
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1291 - acc: 0.9556 - val_loss: 0.0855 - val_acc: 0.9651
Epoch 2/40
 - 5s - loss: 0.0798 - acc: 0.9665 - val_loss: 0.0778 - val_acc: 0.9660
Epoch 3/40
 - 5s - loss: 0.0703 - acc: 0.9686 - val_loss: 0.0747 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0655 - acc: 0.9699 - val_loss: 0.0715 - val_acc: 0.9676
Epoch 5/40
 - 5s - loss: 0.0622 - acc: 0.9705 - val_loss: 0.0709 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0597 - acc: 0.9713 - val_loss: 0.0702 - val_acc: 0.9687
Epoch 7/40
 - 5s - loss: 0.0577 - acc: 0.9722 - val_loss: 0.0738 - val_acc: 0.9669
Epoch 8/40
 - 5s - loss: 0.0564 - acc: 0.9722 - val_loss: 0.0693 - val_acc: 0.9686
Epoch 9/40
 - 5s - loss: 0.0548 - acc: 0.9730 - val_loss: 0.0702 - val_acc: 0.9685
Epoch 10/40
 - 5s - loss: 0.0540 - acc: 0.9732 - val_loss: 0.0697 - val_acc: 0.9683
Epoch 11/40
 - 5s - loss: 0.0525 - acc: 0.9734 - val_loss: 0.0711 - val_acc: 0.9683
Epoch 12/40
 - 5s - loss: 0.0519 - acc: 0.9737 - val_loss: 0.0698 - val_acc: 0.9678
Epoch 13/40
 - 5s - loss: 0.0514 - acc: 0.9738 - val_loss: 0.0712 - val_acc: 0.9684
Epoch 14/40
 - 5s - loss: 0.0509 - acc: 0.9744 - val_loss: 0.0724 - val_acc: 0.9684
Epoch 15/40
 - 5s - loss: 0.0505 - acc: 0.9742 - val_loss: 0.0710 - val_acc: 0.9688
Epoch 16/40
 - 5s - loss: 0.0498 - acc: 0.9743 - val_loss: 0.0702 - val_acc: 0.9680
Epoch 17/40
 - 5s - loss: 0.0498 - acc: 0.9745 - val_loss: 0.0749 - val_acc: 0.9685
Epoch 18/40
 - 5s - loss: 0.0494 - acc: 0.9748 - val_loss: 0.0752 - val_acc: 0.9680
Epoch 19/40
 - 5s - loss: 0.0492 - acc: 0.9746 - val_loss: 0.0770 - val_acc: 0.9684
Epoch 20/40
 - 5s - loss: 0.0491 - acc: 0.9747 - val_loss: 0.0749 - val_acc: 0.9687
Epoch 21/40
 - 5s - loss: 0.0486 - acc: 0.9754 - val_loss: 0.0760 - val_acc: 0.9674
Epoch 22/40
 - 5s - loss: 0.0486 - acc: 0.9753 - val_loss: 0.0770 - val_acc: 0.9682
Epoch 23/40
 - 5s - loss: 0.0486 - acc: 0.9753 - val_loss: 0.0764 - val_acc: 0.9680
Epoch 24/40
 - 5s - loss: 0.0485 - acc: 0.9752 - val_loss: 0.0797 - val_acc: 0.9682
Epoch 25/40
 - 5s - loss: 0.0484 - acc: 0.9750 - val_loss: 0.0806 - val_acc: 0.9678
Epoch 26/40
 - 5s - loss: 0.0479 - acc: 0.9757 - val_loss: 0.0833 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0482 - acc: 0.9755 - val_loss: 0.0818 - val_acc: 0.9678
Epoch 28/40
 - 5s - loss: 0.0480 - acc: 0.9754 - val_loss: 0.0798 - val_acc: 0.9681
Epoch 29/40
 - 5s - loss: 0.0481 - acc: 0.9754 - val_loss: 0.0837 - val_acc: 0.9679
Epoch 30/40
 - 5s - loss: 0.0478 - acc: 0.9756 - val_loss: 0.0824 - val_acc: 0.9688
Epoch 31/40
 - 5s - loss: 0.0477 - acc: 0.9754 - val_loss: 0.0829 - val_acc: 0.9684
Epoch 32/40
 - 5s - loss: 0.0476 - acc: 0.9756 - val_loss: 0.0822 - val_acc: 0.9676
Epoch 33/40
 - 5s - loss: 0.0478 - acc: 0.9754 - val_loss: 0.0827 - val_acc: 0.9684
Epoch 34/40
 - 5s - loss: 0.0473 - acc: 0.9760 - val_loss: 0.0831 - val_acc: 0.9689
Epoch 35/40
 - 5s - loss: 0.0474 - acc: 0.9759 - val_loss: 0.0830 - val_acc: 0.9675
Epoch 36/40
 - 5s - loss: 0.0477 - acc: 0.9761 - val_loss: 0.0850 - val_acc: 0.9671
Epoch 37/40
 - 5s - loss: 0.0474 - acc: 0.9759 - val_loss: 0.0869 - val_acc: 0.9681
Epoch 38/40
 - 5s - loss: 0.0472 - acc: 0.9757 - val_loss: 0.0894 - val_acc: 0.9678
Epoch 39/40
 - 5s - loss: 0.0472 - acc: 0.9755 - val_loss: 0.0818 - val_acc: 0.9679
Epoch 40/40
 - 5s - loss: 0.0470 - acc: 0.9762 - val_loss: 0.0894 - val_acc: 0.9685
Epoch 1/40
 - 1s - loss: 0.0835 - acc: 0.9660
Epoch 2/40
 - 1s - loss: 0.0748 - acc: 0.9680
Epoch 3/40
 - 1s - loss: 0.0698 - acc: 0.9681
Epoch 4/40
 - 1s - loss: 0.0648 - acc: 0.9687
Epoch 5/40
 - 1s - loss: 0.0651 - acc: 0.9705
Epoch 6/40
 - 1s - loss: 0.0619 - acc: 0.9713
Epoch 7/40
 - 1s - loss: 0.0602 - acc: 0.9709
Epoch 8/40
 - 1s - loss: 0.0578 - acc: 0.9718
Epoch 9/40
 - 1s - loss: 0.0559 - acc: 0.9717
Epoch 10/40
 - 1s - loss: 0.0562 - acc: 0.9722
Epoch 11/40
 - 1s - loss: 0.0547 - acc: 0.9730
Epoch 12/40
 - 1s - loss: 0.0510 - acc: 0.9742
Epoch 13/40
 - 1s - loss: 0.0510 - acc: 0.9739
Epoch 14/40
 - 1s - loss: 0.0509 - acc: 0.9742
Epoch 15/40
 - 1s - loss: 0.0508 - acc: 0.9737
Epoch 16/40
 - 1s - loss: 0.0483 - acc: 0.9745
Epoch 17/40
 - 1s - loss: 0.0496 - acc: 0.9741
Epoch 18/40
 - 1s - loss: 0.0489 - acc: 0.9745
Epoch 19/40
 - 1s - loss: 0.0469 - acc: 0.9741
Epoch 20/40
 - 1s - loss: 0.0470 - acc: 0.9749
Epoch 21/40
 - 1s - loss: 0.0465 - acc: 0.9747
Epoch 22/40
 - 1s - loss: 0.0447 - acc: 0.9757
Epoch 23/40
 - 1s - loss: 0.0445 - acc: 0.9745
Epoch 24/40
 - 1s - loss: 0.0440 - acc: 0.9757
Epoch 25/40
 - 1s - loss: 0.0441 - acc: 0.9746
Epoch 26/40
 - 1s - loss: 0.0438 - acc: 0.9753
Epoch 27/40
 - 1s - loss: 0.0435 - acc: 0.9764
Epoch 28/40
 - 1s - loss: 0.0433 - acc: 0.9751
Epoch 29/40
 - 1s - loss: 0.0428 - acc: 0.9765
Epoch 30/40
 - 1s - loss: 0.0421 - acc: 0.9763
Epoch 31/40
 - 1s - loss: 0.0411 - acc: 0.9779
Epoch 32/40
 - 1s - loss: 0.0428 - acc: 0.9757
Epoch 33/40
 - 1s - loss: 0.0412 - acc: 0.9764
Epoch 34/40
 - 1s - loss: 0.0411 - acc: 0.9771
Epoch 35/40
 - 1s - loss: 0.0404 - acc: 0.9773
Epoch 36/40
 - 1s - loss: 0.0400 - acc: 0.9769
Epoch 37/40
 - 1s - loss: 0.0393 - acc: 0.9768
Epoch 38/40
 - 1s - loss: 0.0402 - acc: 0.9767
Epoch 39/40
 - 1s - loss: 0.0395 - acc: 0.9768
Epoch 40/40
 - 1s - loss: 0.0389 - acc: 0.9765
# Training time = 0:04:08.350641
# F-Score(Ordinary) = 0.323, Recall: 0.946, Precision: 0.195
# F-Score(ireflv) = 0.134, Recall: 0.75, Precision: 0.074
# F-Score(id) = 0.571, Recall: 0.975, Precision: 0.404
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_223 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_224 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_223 (Embedding)       (None, 4, 48)        705264      input_223[0][0]                  
__________________________________________________________________________________________________
embedding_224 (Embedding)       (None, 4, 24)        5640        input_224[0][0]                  
__________________________________________________________________________________________________
flatten_223 (Flatten)           (None, 192)          0           embedding_223[0][0]              
__________________________________________________________________________________________________
flatten_224 (Flatten)           (None, 96)           0           embedding_224[0][0]              
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 288)          0           flatten_223[0][0]                
                                                                 flatten_224[0][0]                
__________________________________________________________________________________________________
dense_223 (Dense)               (None, 24)           6936        concatenate_112[0][0]            
__________________________________________________________________________________________________
dropout_112 (Dropout)           (None, 24)           0           dense_223[0][0]                  
__________________________________________________________________________________________________
dense_224 (Dense)               (None, 8)            200         dropout_112[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 1.0
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1321 - acc: 0.9556 - val_loss: 0.0855 - val_acc: 0.9653
Epoch 2/40
 - 5s - loss: 0.0794 - acc: 0.9668 - val_loss: 0.0771 - val_acc: 0.9677
Epoch 3/40
 - 5s - loss: 0.0702 - acc: 0.9688 - val_loss: 0.0741 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0659 - acc: 0.9703 - val_loss: 0.0716 - val_acc: 0.9682
Epoch 5/40
 - 5s - loss: 0.0630 - acc: 0.9709 - val_loss: 0.0713 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0607 - acc: 0.9713 - val_loss: 0.0705 - val_acc: 0.9683
Epoch 7/40
 - 5s - loss: 0.0590 - acc: 0.9720 - val_loss: 0.0699 - val_acc: 0.9680
Epoch 8/40
 - 5s - loss: 0.0572 - acc: 0.9721 - val_loss: 0.0709 - val_acc: 0.9682
Epoch 9/40
 - 5s - loss: 0.0556 - acc: 0.9727 - val_loss: 0.0705 - val_acc: 0.9688
Epoch 10/40
 - 5s - loss: 0.0553 - acc: 0.9732 - val_loss: 0.0706 - val_acc: 0.9691
Epoch 11/40
 - 5s - loss: 0.0539 - acc: 0.9737 - val_loss: 0.0775 - val_acc: 0.9671
Epoch 12/40
 - 5s - loss: 0.0530 - acc: 0.9741 - val_loss: 0.0721 - val_acc: 0.9678
Epoch 13/40
 - 5s - loss: 0.0526 - acc: 0.9738 - val_loss: 0.0724 - val_acc: 0.9684
Epoch 14/40
 - 5s - loss: 0.0519 - acc: 0.9742 - val_loss: 0.0768 - val_acc: 0.9686
Epoch 15/40
 - 5s - loss: 0.0516 - acc: 0.9745 - val_loss: 0.0713 - val_acc: 0.9688
Epoch 16/40
 - 5s - loss: 0.0507 - acc: 0.9746 - val_loss: 0.0715 - val_acc: 0.9674
Epoch 17/40
 - 5s - loss: 0.0499 - acc: 0.9747 - val_loss: 0.0715 - val_acc: 0.9691
Epoch 18/40
 - 5s - loss: 0.0499 - acc: 0.9748 - val_loss: 0.0735 - val_acc: 0.9685
Epoch 19/40
 - 5s - loss: 0.0499 - acc: 0.9749 - val_loss: 0.0720 - val_acc: 0.9685
Epoch 20/40
 - 5s - loss: 0.0494 - acc: 0.9752 - val_loss: 0.0735 - val_acc: 0.9683
Epoch 21/40
 - 5s - loss: 0.0489 - acc: 0.9753 - val_loss: 0.0753 - val_acc: 0.9679
Epoch 22/40
 - 5s - loss: 0.0489 - acc: 0.9750 - val_loss: 0.0757 - val_acc: 0.9678
Epoch 23/40
 - 5s - loss: 0.0487 - acc: 0.9754 - val_loss: 0.0724 - val_acc: 0.9681
Epoch 24/40
 - 5s - loss: 0.0489 - acc: 0.9754 - val_loss: 0.0772 - val_acc: 0.9690
Epoch 25/40
 - 5s - loss: 0.0485 - acc: 0.9752 - val_loss: 0.0799 - val_acc: 0.9682
Epoch 26/40
 - 5s - loss: 0.0484 - acc: 0.9757 - val_loss: 0.0806 - val_acc: 0.9697
Epoch 27/40
 - 5s - loss: 0.0486 - acc: 0.9753 - val_loss: 0.0784 - val_acc: 0.9680
Epoch 28/40
 - 5s - loss: 0.0485 - acc: 0.9754 - val_loss: 0.0803 - val_acc: 0.9691
Epoch 29/40
 - 5s - loss: 0.0482 - acc: 0.9754 - val_loss: 0.0852 - val_acc: 0.9680
Epoch 30/40
 - 5s - loss: 0.0482 - acc: 0.9765 - val_loss: 0.0810 - val_acc: 0.9679
Epoch 31/40
 - 5s - loss: 0.0479 - acc: 0.9755 - val_loss: 0.0805 - val_acc: 0.9677
Epoch 32/40
 - 5s - loss: 0.0483 - acc: 0.9757 - val_loss: 0.0818 - val_acc: 0.9679
Epoch 33/40
 - 5s - loss: 0.0484 - acc: 0.9759 - val_loss: 0.0793 - val_acc: 0.9686
Epoch 34/40
 - 5s - loss: 0.0478 - acc: 0.9756 - val_loss: 0.0815 - val_acc: 0.9684
Epoch 35/40
 - 5s - loss: 0.0476 - acc: 0.9751 - val_loss: 0.0805 - val_acc: 0.9679
Epoch 36/40
 - 5s - loss: 0.0475 - acc: 0.9755 - val_loss: 0.0843 - val_acc: 0.9668
Epoch 37/40
 - 5s - loss: 0.0477 - acc: 0.9758 - val_loss: 0.0817 - val_acc: 0.9684
Epoch 38/40
 - 5s - loss: 0.0477 - acc: 0.9753 - val_loss: 0.0835 - val_acc: 0.9688
Epoch 39/40
 - 5s - loss: 0.0475 - acc: 0.9757 - val_loss: 0.0869 - val_acc: 0.9682
Epoch 40/40
 - 5s - loss: 0.0475 - acc: 0.9761 - val_loss: 0.0845 - val_acc: 0.9686
Epoch 1/40
 - 1s - loss: 0.0809 - acc: 0.9672
Epoch 2/40
 - 1s - loss: 0.0719 - acc: 0.9658
Epoch 3/40
 - 1s - loss: 0.0690 - acc: 0.9688
Epoch 4/40
 - 1s - loss: 0.0651 - acc: 0.9701
Epoch 5/40
 - 1s - loss: 0.0642 - acc: 0.9698
Epoch 6/40
 - 1s - loss: 0.0611 - acc: 0.9707
Epoch 7/40
 - 1s - loss: 0.0602 - acc: 0.9701
Epoch 8/40
 - 1s - loss: 0.0559 - acc: 0.9729
Epoch 9/40
 - 1s - loss: 0.0551 - acc: 0.9726
Epoch 10/40
 - 1s - loss: 0.0545 - acc: 0.9717
Epoch 11/40
 - 1s - loss: 0.0538 - acc: 0.9737
Epoch 12/40
 - 1s - loss: 0.0524 - acc: 0.9730
Epoch 13/40
 - 1s - loss: 0.0518 - acc: 0.9730
Epoch 14/40
 - 1s - loss: 0.0513 - acc: 0.9740
Epoch 15/40
 - 1s - loss: 0.0498 - acc: 0.9743
Epoch 16/40
 - 1s - loss: 0.0477 - acc: 0.9755
Epoch 17/40
 - 1s - loss: 0.0489 - acc: 0.9752
Epoch 18/40
 - 1s - loss: 0.0481 - acc: 0.9758
Epoch 19/40
 - 1s - loss: 0.0466 - acc: 0.9758
Epoch 20/40
 - 1s - loss: 0.0463 - acc: 0.9761
Epoch 21/40
 - 1s - loss: 0.0459 - acc: 0.9751
Epoch 22/40
 - 1s - loss: 0.0463 - acc: 0.9754
Epoch 23/40
 - 1s - loss: 0.0459 - acc: 0.9757
Epoch 24/40
 - 1s - loss: 0.0456 - acc: 0.9765
Epoch 25/40
 - 1s - loss: 0.0441 - acc: 0.9757
Epoch 26/40
 - 1s - loss: 0.0434 - acc: 0.9758
Epoch 27/40
 - 1s - loss: 0.0440 - acc: 0.9778
Epoch 28/40
 - 1s - loss: 0.0431 - acc: 0.9775
Epoch 29/40
 - 1s - loss: 0.0426 - acc: 0.9768
Epoch 30/40
 - 1s - loss: 0.0422 - acc: 0.9764
Epoch 31/40
 - 1s - loss: 0.0411 - acc: 0.9774
Epoch 32/40
 - 1s - loss: 0.0416 - acc: 0.9773
Epoch 33/40
 - 1s - loss: 0.0416 - acc: 0.9775
Epoch 34/40
 - 1s - loss: 0.0415 - acc: 0.9771
Epoch 35/40
 - 1s - loss: 0.0416 - acc: 0.9773
Epoch 36/40
 - 1s - loss: 0.0400 - acc: 0.9772
Epoch 37/40
 - 1s - loss: 0.0403 - acc: 0.9770
Epoch 38/40
 - 1s - loss: 0.0401 - acc: 0.9770
Epoch 39/40
 - 1s - loss: 0.0412 - acc: 0.9770
Epoch 40/40
 - 1s - loss: 0.0398 - acc: 0.9767
# Training time = 0:04:02.376051
# F-Score(Ordinary) = 0.098, Recall: 0.41, Precision: 0.056
# F-Score(lvc) = 0.069, Recall: 0.143, Precision: 0.045
# F-Score(id) = 0.17, Recall: 0.947, Precision: 0.093
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_225 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_226 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_225 (Embedding)       (None, 4, 48)        705264      input_225[0][0]                  
__________________________________________________________________________________________________
embedding_226 (Embedding)       (None, 4, 24)        5640        input_226[0][0]                  
__________________________________________________________________________________________________
flatten_225 (Flatten)           (None, 192)          0           embedding_225[0][0]              
__________________________________________________________________________________________________
flatten_226 (Flatten)           (None, 96)           0           embedding_226[0][0]              
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 288)          0           flatten_225[0][0]                
                                                                 flatten_226[0][0]                
__________________________________________________________________________________________________
dense_225 (Dense)               (None, 24)           6936        concatenate_113[0][0]            
__________________________________________________________________________________________________
dropout_113 (Dropout)           (None, 24)           0           dense_225[0][0]                  
__________________________________________________________________________________________________
dense_226 (Dense)               (None, 8)            200         dropout_113[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 1.0
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1304 - acc: 0.9569 - val_loss: 0.0850 - val_acc: 0.9656
Epoch 2/40
 - 5s - loss: 0.0784 - acc: 0.9669 - val_loss: 0.0756 - val_acc: 0.9674
Epoch 3/40
 - 5s - loss: 0.0697 - acc: 0.9686 - val_loss: 0.0719 - val_acc: 0.9680
Epoch 4/40
 - 5s - loss: 0.0649 - acc: 0.9697 - val_loss: 0.0710 - val_acc: 0.9682
Epoch 5/40
 - 5s - loss: 0.0616 - acc: 0.9710 - val_loss: 0.0707 - val_acc: 0.9680
Epoch 6/40
 - 5s - loss: 0.0592 - acc: 0.9717 - val_loss: 0.0687 - val_acc: 0.9688
Epoch 7/40
 - 5s - loss: 0.0574 - acc: 0.9726 - val_loss: 0.0691 - val_acc: 0.9684
Epoch 8/40
 - 5s - loss: 0.0561 - acc: 0.9725 - val_loss: 0.0706 - val_acc: 0.9689
Epoch 9/40
 - 5s - loss: 0.0546 - acc: 0.9730 - val_loss: 0.0702 - val_acc: 0.9667
Epoch 10/40
 - 5s - loss: 0.0538 - acc: 0.9737 - val_loss: 0.0689 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0529 - acc: 0.9735 - val_loss: 0.0729 - val_acc: 0.9673
Epoch 12/40
 - 5s - loss: 0.0521 - acc: 0.9739 - val_loss: 0.0713 - val_acc: 0.9686
Epoch 13/40
 - 5s - loss: 0.0517 - acc: 0.9742 - val_loss: 0.0706 - val_acc: 0.9674
Epoch 14/40
 - 5s - loss: 0.0510 - acc: 0.9743 - val_loss: 0.0733 - val_acc: 0.9680
Epoch 15/40
 - 5s - loss: 0.0507 - acc: 0.9744 - val_loss: 0.0713 - val_acc: 0.9688
Epoch 16/40
 - 5s - loss: 0.0506 - acc: 0.9745 - val_loss: 0.0715 - val_acc: 0.9684
Epoch 17/40
 - 5s - loss: 0.0500 - acc: 0.9744 - val_loss: 0.0722 - val_acc: 0.9681
Epoch 18/40
 - 5s - loss: 0.0501 - acc: 0.9747 - val_loss: 0.0742 - val_acc: 0.9679
Epoch 19/40
 - 5s - loss: 0.0496 - acc: 0.9748 - val_loss: 0.0750 - val_acc: 0.9680
Epoch 20/40
 - 5s - loss: 0.0490 - acc: 0.9749 - val_loss: 0.0761 - val_acc: 0.9682
Epoch 21/40
 - 5s - loss: 0.0492 - acc: 0.9752 - val_loss: 0.0779 - val_acc: 0.9678
Epoch 22/40
 - 5s - loss: 0.0488 - acc: 0.9750 - val_loss: 0.0769 - val_acc: 0.9691
Epoch 23/40
 - 5s - loss: 0.0489 - acc: 0.9752 - val_loss: 0.0759 - val_acc: 0.9683
Epoch 24/40
 - 5s - loss: 0.0488 - acc: 0.9750 - val_loss: 0.0752 - val_acc: 0.9688
Epoch 25/40
 - 5s - loss: 0.0484 - acc: 0.9756 - val_loss: 0.0802 - val_acc: 0.9677
Epoch 26/40
 - 5s - loss: 0.0481 - acc: 0.9753 - val_loss: 0.0803 - val_acc: 0.9661
Epoch 27/40
 - 5s - loss: 0.0480 - acc: 0.9753 - val_loss: 0.0809 - val_acc: 0.9683
Epoch 28/40
 - 5s - loss: 0.0481 - acc: 0.9758 - val_loss: 0.0774 - val_acc: 0.9664
Epoch 29/40
 - 5s - loss: 0.0485 - acc: 0.9759 - val_loss: 0.0812 - val_acc: 0.9679
Epoch 30/40
 - 5s - loss: 0.0481 - acc: 0.9757 - val_loss: 0.0770 - val_acc: 0.9689
Epoch 31/40
 - 5s - loss: 0.0477 - acc: 0.9755 - val_loss: 0.0826 - val_acc: 0.9679
Epoch 32/40
 - 5s - loss: 0.0480 - acc: 0.9757 - val_loss: 0.0828 - val_acc: 0.9687
Epoch 33/40
 - 5s - loss: 0.0478 - acc: 0.9759 - val_loss: 0.0797 - val_acc: 0.9688
Epoch 34/40
 - 5s - loss: 0.0480 - acc: 0.9758 - val_loss: 0.0843 - val_acc: 0.9650
Epoch 35/40
 - 5s - loss: 0.0475 - acc: 0.9758 - val_loss: 0.0857 - val_acc: 0.9671
Epoch 36/40
 - 5s - loss: 0.0478 - acc: 0.9754 - val_loss: 0.0847 - val_acc: 0.9689
Epoch 37/40
 - 5s - loss: 0.0477 - acc: 0.9758 - val_loss: 0.0822 - val_acc: 0.9685
Epoch 38/40
 - 5s - loss: 0.0475 - acc: 0.9759 - val_loss: 0.0877 - val_acc: 0.9679
Epoch 39/40
 - 5s - loss: 0.0476 - acc: 0.9755 - val_loss: 0.0846 - val_acc: 0.9658
Epoch 40/40
 - 5s - loss: 0.0482 - acc: 0.9760 - val_loss: 0.0856 - val_acc: 0.9670
Epoch 1/40
 - 1s - loss: 0.0823 - acc: 0.9663
Epoch 2/40
 - 1s - loss: 0.0746 - acc: 0.9682
Epoch 3/40
 - 1s - loss: 0.0702 - acc: 0.9697
Epoch 4/40
 - 1s - loss: 0.0660 - acc: 0.9704
Epoch 5/40
 - 1s - loss: 0.0639 - acc: 0.9709
Epoch 6/40
 - 1s - loss: 0.0613 - acc: 0.9704
Epoch 7/40
 - 1s - loss: 0.0588 - acc: 0.9720
Epoch 8/40
 - 1s - loss: 0.0581 - acc: 0.9721
Epoch 9/40
 - 1s - loss: 0.0559 - acc: 0.9734
Epoch 10/40
 - 1s - loss: 0.0538 - acc: 0.9735
Epoch 11/40
 - 1s - loss: 0.0538 - acc: 0.9724
Epoch 12/40
 - 1s - loss: 0.0530 - acc: 0.9732
Epoch 13/40
 - 1s - loss: 0.0508 - acc: 0.9738
Epoch 14/40
 - 1s - loss: 0.0506 - acc: 0.9741
Epoch 15/40
 - 1s - loss: 0.0509 - acc: 0.9734
Epoch 16/40
 - 1s - loss: 0.0494 - acc: 0.9739
Epoch 17/40
 - 1s - loss: 0.0484 - acc: 0.9753
Epoch 18/40
 - 1s - loss: 0.0479 - acc: 0.9761
Epoch 19/40
 - 1s - loss: 0.0480 - acc: 0.9745
Epoch 20/40
 - 1s - loss: 0.0460 - acc: 0.9753
Epoch 21/40
 - 1s - loss: 0.0477 - acc: 0.9738
Epoch 22/40
 - 1s - loss: 0.0458 - acc: 0.9769
Epoch 23/40
 - 1s - loss: 0.0455 - acc: 0.9757
Epoch 24/40
 - 1s - loss: 0.0448 - acc: 0.9756
Epoch 25/40
 - 1s - loss: 0.0434 - acc: 0.9756
Epoch 26/40
 - 1s - loss: 0.0440 - acc: 0.9765
Epoch 27/40
 - 1s - loss: 0.0447 - acc: 0.9750
Epoch 28/40
 - 1s - loss: 0.0439 - acc: 0.9768
Epoch 29/40
 - 1s - loss: 0.0445 - acc: 0.9754
Epoch 30/40
 - 1s - loss: 0.0430 - acc: 0.9767
Epoch 31/40
 - 1s - loss: 0.0424 - acc: 0.9753
Epoch 32/40
 - 1s - loss: 0.0424 - acc: 0.9774
Epoch 33/40
 - 1s - loss: 0.0424 - acc: 0.9771
Epoch 34/40
 - 1s - loss: 0.0417 - acc: 0.9776
Epoch 35/40
 - 1s - loss: 0.0415 - acc: 0.9770
Epoch 36/40
 - 1s - loss: 0.0407 - acc: 0.9767
Epoch 37/40
 - 1s - loss: 0.0408 - acc: 0.9774
Epoch 38/40
 - 1s - loss: 0.0400 - acc: 0.9782
Epoch 39/40
 - 1s - loss: 0.0402 - acc: 0.9765
Epoch 40/40
 - 1s - loss: 0.0395 - acc: 0.9781
# Training time = 0:04:08.119643
# F-Score(Ordinary) = 0.184, Recall: 0.852, Precision: 0.103
# F-Score(lvc) = 0.367, Recall: 0.838, Precision: 0.235
# F-Score(ireflv) = 0.216, Recall: 0.882, Precision: 0.123
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_227 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_228 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_227 (Embedding)       (None, 4, 48)        705264      input_227[0][0]                  
__________________________________________________________________________________________________
embedding_228 (Embedding)       (None, 4, 24)        5640        input_228[0][0]                  
__________________________________________________________________________________________________
flatten_227 (Flatten)           (None, 192)          0           embedding_227[0][0]              
__________________________________________________________________________________________________
flatten_228 (Flatten)           (None, 96)           0           embedding_228[0][0]              
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 288)          0           flatten_227[0][0]                
                                                                 flatten_228[0][0]                
__________________________________________________________________________________________________
dense_227 (Dense)               (None, 24)           6936        concatenate_114[0][0]            
__________________________________________________________________________________________________
dropout_114 (Dropout)           (None, 24)           0           dense_227[0][0]                  
__________________________________________________________________________________________________
dense_228 (Dense)               (None, 8)            200         dropout_114[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 1.0
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1367 - acc: 0.9543 - val_loss: 0.0842 - val_acc: 0.9663
Epoch 2/40
 - 5s - loss: 0.0812 - acc: 0.9668 - val_loss: 0.0781 - val_acc: 0.9671
Epoch 3/40
 - 5s - loss: 0.0724 - acc: 0.9691 - val_loss: 0.0736 - val_acc: 0.9674
Epoch 4/40
 - 5s - loss: 0.0674 - acc: 0.9696 - val_loss: 0.0723 - val_acc: 0.9683
Epoch 5/40
 - 5s - loss: 0.0631 - acc: 0.9713 - val_loss: 0.0725 - val_acc: 0.9682
Epoch 6/40
 - 5s - loss: 0.0606 - acc: 0.9719 - val_loss: 0.0703 - val_acc: 0.9682
Epoch 7/40
 - 5s - loss: 0.0587 - acc: 0.9724 - val_loss: 0.0691 - val_acc: 0.9685
Epoch 8/40
 - 5s - loss: 0.0565 - acc: 0.9726 - val_loss: 0.0713 - val_acc: 0.9675
Epoch 9/40
 - 5s - loss: 0.0551 - acc: 0.9733 - val_loss: 0.0698 - val_acc: 0.9687
Epoch 10/40
 - 5s - loss: 0.0544 - acc: 0.9736 - val_loss: 0.0702 - val_acc: 0.9678
Epoch 11/40
 - 5s - loss: 0.0536 - acc: 0.9738 - val_loss: 0.0706 - val_acc: 0.9682
Epoch 12/40
 - 5s - loss: 0.0531 - acc: 0.9736 - val_loss: 0.0707 - val_acc: 0.9689
Epoch 13/40
 - 5s - loss: 0.0521 - acc: 0.9741 - val_loss: 0.0734 - val_acc: 0.9682
Epoch 14/40
 - 5s - loss: 0.0517 - acc: 0.9741 - val_loss: 0.0714 - val_acc: 0.9675
Epoch 15/40
 - 5s - loss: 0.0512 - acc: 0.9741 - val_loss: 0.0734 - val_acc: 0.9683
Epoch 16/40
 - 5s - loss: 0.0511 - acc: 0.9745 - val_loss: 0.0747 - val_acc: 0.9682
Epoch 17/40
 - 5s - loss: 0.0507 - acc: 0.9745 - val_loss: 0.0730 - val_acc: 0.9688
Epoch 18/40
 - 5s - loss: 0.0498 - acc: 0.9754 - val_loss: 0.0760 - val_acc: 0.9682
Epoch 19/40
 - 5s - loss: 0.0499 - acc: 0.9748 - val_loss: 0.0781 - val_acc: 0.9686
Epoch 20/40
 - 5s - loss: 0.0496 - acc: 0.9747 - val_loss: 0.0761 - val_acc: 0.9681
Epoch 21/40
 - 5s - loss: 0.0491 - acc: 0.9751 - val_loss: 0.0812 - val_acc: 0.9680
Epoch 22/40
 - 5s - loss: 0.0493 - acc: 0.9750 - val_loss: 0.0783 - val_acc: 0.9686
Epoch 23/40
 - 5s - loss: 0.0489 - acc: 0.9749 - val_loss: 0.0784 - val_acc: 0.9687
Epoch 24/40
 - 5s - loss: 0.0491 - acc: 0.9753 - val_loss: 0.0762 - val_acc: 0.9685
Epoch 25/40
 - 5s - loss: 0.0482 - acc: 0.9754 - val_loss: 0.0776 - val_acc: 0.9691
Epoch 26/40
 - 5s - loss: 0.0481 - acc: 0.9754 - val_loss: 0.0772 - val_acc: 0.9687
Epoch 27/40
 - 5s - loss: 0.0480 - acc: 0.9754 - val_loss: 0.0802 - val_acc: 0.9686
Epoch 28/40
 - 5s - loss: 0.0480 - acc: 0.9747 - val_loss: 0.0817 - val_acc: 0.9676
Epoch 29/40
 - 5s - loss: 0.0481 - acc: 0.9756 - val_loss: 0.0775 - val_acc: 0.9677
Epoch 30/40
 - 5s - loss: 0.0477 - acc: 0.9755 - val_loss: 0.0817 - val_acc: 0.9685
Epoch 31/40
 - 5s - loss: 0.0475 - acc: 0.9758 - val_loss: 0.0840 - val_acc: 0.9680
Epoch 32/40
 - 5s - loss: 0.0475 - acc: 0.9754 - val_loss: 0.0782 - val_acc: 0.9681
Epoch 33/40
 - 5s - loss: 0.0471 - acc: 0.9755 - val_loss: 0.0808 - val_acc: 0.9693
Epoch 34/40
 - 5s - loss: 0.0472 - acc: 0.9757 - val_loss: 0.0819 - val_acc: 0.9674
Epoch 35/40
 - 5s - loss: 0.0475 - acc: 0.9756 - val_loss: 0.0820 - val_acc: 0.9685
Epoch 36/40
 - 5s - loss: 0.0467 - acc: 0.9757 - val_loss: 0.0827 - val_acc: 0.9683
Epoch 37/40
 - 5s - loss: 0.0469 - acc: 0.9759 - val_loss: 0.0840 - val_acc: 0.9672
Epoch 38/40
 - 5s - loss: 0.0466 - acc: 0.9758 - val_loss: 0.0825 - val_acc: 0.9679
Epoch 39/40
 - 5s - loss: 0.0466 - acc: 0.9760 - val_loss: 0.0863 - val_acc: 0.9680
Epoch 40/40
 - 5s - loss: 0.0470 - acc: 0.9761 - val_loss: 0.0862 - val_acc: 0.9679
Epoch 1/40
 - 1s - loss: 0.0810 - acc: 0.9664
Epoch 2/40
 - 1s - loss: 0.0728 - acc: 0.9675
Epoch 3/40
 - 1s - loss: 0.0691 - acc: 0.9695
Epoch 4/40
 - 1s - loss: 0.0654 - acc: 0.9689
Epoch 5/40
 - 1s - loss: 0.0606 - acc: 0.9701
Epoch 6/40
 - 1s - loss: 0.0589 - acc: 0.9704
Epoch 7/40
 - 1s - loss: 0.0597 - acc: 0.9715
Epoch 8/40
 - 1s - loss: 0.0563 - acc: 0.9719
Epoch 9/40
 - 1s - loss: 0.0560 - acc: 0.9727
Epoch 10/40
 - 1s - loss: 0.0525 - acc: 0.9725
Epoch 11/40
 - 1s - loss: 0.0521 - acc: 0.9731
Epoch 12/40
 - 1s - loss: 0.0512 - acc: 0.9733
Epoch 13/40
 - 1s - loss: 0.0507 - acc: 0.9750
Epoch 14/40
 - 1s - loss: 0.0498 - acc: 0.9741
Epoch 15/40
 - 1s - loss: 0.0480 - acc: 0.9751
Epoch 16/40
 - 1s - loss: 0.0488 - acc: 0.9743
Epoch 17/40
 - 1s - loss: 0.0474 - acc: 0.9748
Epoch 18/40
 - 1s - loss: 0.0471 - acc: 0.9764
Epoch 19/40
 - 1s - loss: 0.0478 - acc: 0.9765
Epoch 20/40
 - 1s - loss: 0.0462 - acc: 0.9762
Epoch 21/40
 - 1s - loss: 0.0459 - acc: 0.9756
Epoch 22/40
 - 1s - loss: 0.0449 - acc: 0.9766
Epoch 23/40
 - 1s - loss: 0.0453 - acc: 0.9759
Epoch 24/40
 - 1s - loss: 0.0442 - acc: 0.9761
Epoch 25/40
 - 1s - loss: 0.0444 - acc: 0.9760
Epoch 26/40
 - 1s - loss: 0.0443 - acc: 0.9763
Epoch 27/40
 - 1s - loss: 0.0434 - acc: 0.9760
Epoch 28/40
 - 1s - loss: 0.0430 - acc: 0.9771
Epoch 29/40
 - 1s - loss: 0.0420 - acc: 0.9763
Epoch 30/40
 - 1s - loss: 0.0430 - acc: 0.9766
Epoch 31/40
 - 1s - loss: 0.0431 - acc: 0.9771
Epoch 32/40
 - 1s - loss: 0.0416 - acc: 0.9770
Epoch 33/40
 - 1s - loss: 0.0424 - acc: 0.9771
Epoch 34/40
 - 1s - loss: 0.0421 - acc: 0.9764
Epoch 35/40
 - 1s - loss: 0.0413 - acc: 0.9775
Epoch 36/40
 - 1s - loss: 0.0415 - acc: 0.9761
Epoch 37/40
 - 1s - loss: 0.0407 - acc: 0.9768
Epoch 38/40
 - 1s - loss: 0.0402 - acc: 0.9776
Epoch 39/40
 - 1s - loss: 0.0410 - acc: 0.9771
Epoch 40/40
 - 1s - loss: 0.0402 - acc: 0.9768
# Training time = 0:04:08.869806
# F-Score(Ordinary) = 0.163, Recall: 0.478, Precision: 0.098
# F-Score(lvc) = 0.195, Recall: 0.302, Precision: 0.144
# F-Score(ireflv) = 0.245, Recall: 1.0, Precision: 0.139
# F-Score(id) = 0.078, Recall: 0.667, Precision: 0.041
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_229 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_230 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_229 (Embedding)       (None, 4, 48)        705264      input_229[0][0]                  
__________________________________________________________________________________________________
embedding_230 (Embedding)       (None, 4, 24)        5640        input_230[0][0]                  
__________________________________________________________________________________________________
flatten_229 (Flatten)           (None, 192)          0           embedding_229[0][0]              
__________________________________________________________________________________________________
flatten_230 (Flatten)           (None, 96)           0           embedding_230[0][0]              
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 288)          0           flatten_229[0][0]                
                                                                 flatten_230[0][0]                
__________________________________________________________________________________________________
dense_229 (Dense)               (None, 24)           6936        concatenate_115[0][0]            
__________________________________________________________________________________________________
dropout_115 (Dropout)           (None, 24)           0           dense_229[0][0]                  
__________________________________________________________________________________________________
dense_230 (Dense)               (None, 8)            200         dropout_115[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 1.0
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1326 - acc: 0.9546 - val_loss: 0.0857 - val_acc: 0.9656
Epoch 2/40
 - 5s - loss: 0.0790 - acc: 0.9666 - val_loss: 0.0777 - val_acc: 0.9671
Epoch 3/40
 - 5s - loss: 0.0709 - acc: 0.9682 - val_loss: 0.0734 - val_acc: 0.9677
Epoch 4/40
 - 5s - loss: 0.0660 - acc: 0.9699 - val_loss: 0.0725 - val_acc: 0.9674
Epoch 5/40
 - 5s - loss: 0.0628 - acc: 0.9708 - val_loss: 0.0705 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0600 - acc: 0.9714 - val_loss: 0.0694 - val_acc: 0.9683
Epoch 7/40
 - 5s - loss: 0.0582 - acc: 0.9717 - val_loss: 0.0724 - val_acc: 0.9684
Epoch 8/40
 - 5s - loss: 0.0563 - acc: 0.9725 - val_loss: 0.0698 - val_acc: 0.9687
Epoch 9/40
 - 5s - loss: 0.0551 - acc: 0.9730 - val_loss: 0.0689 - val_acc: 0.9684
Epoch 10/40
 - 5s - loss: 0.0539 - acc: 0.9733 - val_loss: 0.0716 - val_acc: 0.9687
Epoch 11/40
 - 5s - loss: 0.0533 - acc: 0.9737 - val_loss: 0.0708 - val_acc: 0.9681
Epoch 12/40
 - 5s - loss: 0.0523 - acc: 0.9737 - val_loss: 0.0694 - val_acc: 0.9678
Epoch 13/40
 - 5s - loss: 0.0516 - acc: 0.9742 - val_loss: 0.0703 - val_acc: 0.9686
Epoch 14/40
 - 5s - loss: 0.0510 - acc: 0.9747 - val_loss: 0.0734 - val_acc: 0.9694
Epoch 15/40
 - 5s - loss: 0.0505 - acc: 0.9745 - val_loss: 0.0698 - val_acc: 0.9686
Epoch 16/40
 - 5s - loss: 0.0502 - acc: 0.9747 - val_loss: 0.0717 - val_acc: 0.9689
Epoch 17/40
 - 5s - loss: 0.0501 - acc: 0.9745 - val_loss: 0.0727 - val_acc: 0.9689
Epoch 18/40
 - 5s - loss: 0.0495 - acc: 0.9749 - val_loss: 0.0739 - val_acc: 0.9687
Epoch 19/40
 - 5s - loss: 0.0494 - acc: 0.9748 - val_loss: 0.0720 - val_acc: 0.9681
Epoch 20/40
 - 5s - loss: 0.0492 - acc: 0.9747 - val_loss: 0.0759 - val_acc: 0.9669
Epoch 21/40
 - 5s - loss: 0.0486 - acc: 0.9753 - val_loss: 0.0771 - val_acc: 0.9680
Epoch 22/40
 - 5s - loss: 0.0486 - acc: 0.9750 - val_loss: 0.0750 - val_acc: 0.9677
Epoch 23/40
 - 5s - loss: 0.0483 - acc: 0.9756 - val_loss: 0.0756 - val_acc: 0.9684
Epoch 24/40
 - 5s - loss: 0.0481 - acc: 0.9754 - val_loss: 0.0775 - val_acc: 0.9687
Epoch 25/40
 - 5s - loss: 0.0478 - acc: 0.9757 - val_loss: 0.0739 - val_acc: 0.9690
Epoch 26/40
 - 5s - loss: 0.0479 - acc: 0.9756 - val_loss: 0.0811 - val_acc: 0.9690
Epoch 27/40
 - 5s - loss: 0.0477 - acc: 0.9754 - val_loss: 0.0778 - val_acc: 0.9687
Epoch 28/40
 - 5s - loss: 0.0472 - acc: 0.9757 - val_loss: 0.0772 - val_acc: 0.9678
Epoch 29/40
 - 5s - loss: 0.0469 - acc: 0.9760 - val_loss: 0.0821 - val_acc: 0.9683
Epoch 30/40
 - 5s - loss: 0.0471 - acc: 0.9757 - val_loss: 0.0809 - val_acc: 0.9688
Epoch 31/40
 - 5s - loss: 0.0471 - acc: 0.9761 - val_loss: 0.0770 - val_acc: 0.9683
Epoch 32/40
 - 5s - loss: 0.0473 - acc: 0.9760 - val_loss: 0.0765 - val_acc: 0.9691
Epoch 33/40
 - 5s - loss: 0.0469 - acc: 0.9756 - val_loss: 0.0794 - val_acc: 0.9687
Epoch 34/40
 - 5s - loss: 0.0470 - acc: 0.9759 - val_loss: 0.0781 - val_acc: 0.9683
Epoch 35/40
 - 5s - loss: 0.0468 - acc: 0.9761 - val_loss: 0.0853 - val_acc: 0.9677
Epoch 36/40
 - 5s - loss: 0.0471 - acc: 0.9758 - val_loss: 0.0826 - val_acc: 0.9679
Epoch 37/40
 - 5s - loss: 0.0468 - acc: 0.9761 - val_loss: 0.0793 - val_acc: 0.9686
Epoch 38/40
 - 5s - loss: 0.0472 - acc: 0.9760 - val_loss: 0.0862 - val_acc: 0.9669
Epoch 39/40
 - 5s - loss: 0.0465 - acc: 0.9763 - val_loss: 0.0844 - val_acc: 0.9669
Epoch 40/40
 - 5s - loss: 0.0470 - acc: 0.9761 - val_loss: 0.0850 - val_acc: 0.9679
Epoch 1/40
 - 1s - loss: 0.0817 - acc: 0.9666
Epoch 2/40
 - 1s - loss: 0.0740 - acc: 0.9682
Epoch 3/40
 - 1s - loss: 0.0674 - acc: 0.9695
Epoch 4/40
 - 1s - loss: 0.0656 - acc: 0.9711
Epoch 5/40
 - 1s - loss: 0.0621 - acc: 0.9709
Epoch 6/40
 - 1s - loss: 0.0594 - acc: 0.9708
Epoch 7/40
 - 1s - loss: 0.0568 - acc: 0.9723
Epoch 8/40
 - 1s - loss: 0.0559 - acc: 0.9723
Epoch 9/40
 - 1s - loss: 0.0527 - acc: 0.9727
Epoch 10/40
 - 1s - loss: 0.0531 - acc: 0.9728
Epoch 11/40
 - 1s - loss: 0.0530 - acc: 0.9727
Epoch 12/40
 - 1s - loss: 0.0513 - acc: 0.9738
Epoch 13/40
 - 1s - loss: 0.0492 - acc: 0.9744
Epoch 14/40
 - 1s - loss: 0.0483 - acc: 0.9748
Epoch 15/40
 - 1s - loss: 0.0478 - acc: 0.9745
Epoch 16/40
 - 1s - loss: 0.0472 - acc: 0.9748
Epoch 17/40
 - 1s - loss: 0.0470 - acc: 0.9748
Epoch 18/40
 - 1s - loss: 0.0454 - acc: 0.9755
Epoch 19/40
 - 1s - loss: 0.0455 - acc: 0.9763
Epoch 20/40
 - 1s - loss: 0.0439 - acc: 0.9774
Epoch 21/40
 - 1s - loss: 0.0453 - acc: 0.9756
Epoch 22/40
 - 1s - loss: 0.0435 - acc: 0.9760
Epoch 23/40
 - 1s - loss: 0.0433 - acc: 0.9755
Epoch 24/40
 - 1s - loss: 0.0429 - acc: 0.9758
Epoch 25/40
 - 1s - loss: 0.0425 - acc: 0.9775
Epoch 26/40
 - 1s - loss: 0.0422 - acc: 0.9770
Epoch 27/40
 - 1s - loss: 0.0424 - acc: 0.9756
Epoch 28/40
 - 1s - loss: 0.0415 - acc: 0.9770
Epoch 29/40
 - 1s - loss: 0.0409 - acc: 0.9767
Epoch 30/40
 - 1s - loss: 0.0413 - acc: 0.9770
Epoch 31/40
 - 1s - loss: 0.0403 - acc: 0.9763
Epoch 32/40
 - 1s - loss: 0.0407 - acc: 0.9784
Epoch 33/40
 - 1s - loss: 0.0405 - acc: 0.9770
Epoch 34/40
 - 1s - loss: 0.0407 - acc: 0.9759
Epoch 35/40
 - 1s - loss: 0.0395 - acc: 0.9775
Epoch 36/40
 - 1s - loss: 0.0390 - acc: 0.9790
Epoch 37/40
 - 1s - loss: 0.0399 - acc: 0.9775
Epoch 38/40
 - 1s - loss: 0.0391 - acc: 0.9786
Epoch 39/40
 - 1s - loss: 0.0398 - acc: 0.9774
Epoch 40/40
 - 1s - loss: 0.0385 - acc: 0.9779
# Training time = 0:04:02.498034
# F-Score(Ordinary) = 0.596, Recall: 0.689, Precision: 0.526
# F-Score(lvc) = 0.513, Recall: 0.496, Precision: 0.53
# F-Score(ireflv) = 0.703, Recall: 0.718, Precision: 0.689
# F-Score(id) = 0.572, Recall: 0.952, Precision: 0.409
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_231 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_232 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_231 (Embedding)       (None, 4, 48)        705264      input_231[0][0]                  
__________________________________________________________________________________________________
embedding_232 (Embedding)       (None, 4, 24)        5640        input_232[0][0]                  
__________________________________________________________________________________________________
flatten_231 (Flatten)           (None, 192)          0           embedding_231[0][0]              
__________________________________________________________________________________________________
flatten_232 (Flatten)           (None, 96)           0           embedding_232[0][0]              
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 288)          0           flatten_231[0][0]                
                                                                 flatten_232[0][0]                
__________________________________________________________________________________________________
dense_231 (Dense)               (None, 24)           6936        concatenate_116[0][0]            
__________________________________________________________________________________________________
dropout_116 (Dropout)           (None, 24)           0           dense_231[0][0]                  
__________________________________________________________________________________________________
dense_232 (Dense)               (None, 8)            200         dropout_116[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 2
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1096 - acc: 0.9598 - val_loss: 0.0800 - val_acc: 0.9660
Epoch 2/40
 - 5s - loss: 0.0725 - acc: 0.9681 - val_loss: 0.0734 - val_acc: 0.9672
Epoch 3/40
 - 5s - loss: 0.0647 - acc: 0.9697 - val_loss: 0.0749 - val_acc: 0.9673
Epoch 4/40
 - 5s - loss: 0.0611 - acc: 0.9714 - val_loss: 0.0719 - val_acc: 0.9678
Epoch 5/40
 - 5s - loss: 0.0587 - acc: 0.9722 - val_loss: 0.0722 - val_acc: 0.9684
Epoch 6/40
 - 5s - loss: 0.0570 - acc: 0.9724 - val_loss: 0.0722 - val_acc: 0.9685
Epoch 7/40
 - 5s - loss: 0.0562 - acc: 0.9730 - val_loss: 0.0786 - val_acc: 0.9670
Epoch 8/40
 - 5s - loss: 0.0552 - acc: 0.9728 - val_loss: 0.0747 - val_acc: 0.9688
Epoch 9/40
 - 5s - loss: 0.0537 - acc: 0.9741 - val_loss: 0.0756 - val_acc: 0.9689
Epoch 10/40
 - 5s - loss: 0.0536 - acc: 0.9737 - val_loss: 0.0757 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0522 - acc: 0.9742 - val_loss: 0.0781 - val_acc: 0.9681
Epoch 12/40
 - 5s - loss: 0.0521 - acc: 0.9745 - val_loss: 0.0785 - val_acc: 0.9678
Epoch 13/40
 - 5s - loss: 0.0516 - acc: 0.9741 - val_loss: 0.0753 - val_acc: 0.9686
Epoch 14/40
 - 5s - loss: 0.0516 - acc: 0.9748 - val_loss: 0.0855 - val_acc: 0.9689
Epoch 15/40
 - 5s - loss: 0.0510 - acc: 0.9748 - val_loss: 0.0825 - val_acc: 0.9676
Epoch 16/40
 - 5s - loss: 0.0509 - acc: 0.9747 - val_loss: 0.0779 - val_acc: 0.9679
Epoch 17/40
 - 5s - loss: 0.0510 - acc: 0.9747 - val_loss: 0.0816 - val_acc: 0.9678
Epoch 18/40
 - 5s - loss: 0.0511 - acc: 0.9747 - val_loss: 0.0842 - val_acc: 0.9686
Epoch 19/40
 - 5s - loss: 0.0509 - acc: 0.9748 - val_loss: 0.0869 - val_acc: 0.9678
Epoch 20/40
 - 5s - loss: 0.0506 - acc: 0.9752 - val_loss: 0.0885 - val_acc: 0.9679
Epoch 21/40
 - 5s - loss: 0.0499 - acc: 0.9753 - val_loss: 0.0856 - val_acc: 0.9662
Epoch 22/40
 - 5s - loss: 0.0499 - acc: 0.9755 - val_loss: 0.0940 - val_acc: 0.9678
Epoch 23/40
 - 5s - loss: 0.0506 - acc: 0.9753 - val_loss: 0.0886 - val_acc: 0.9673
Epoch 24/40
 - 5s - loss: 0.0501 - acc: 0.9751 - val_loss: 0.0924 - val_acc: 0.9682
Epoch 25/40
 - 5s - loss: 0.0500 - acc: 0.9754 - val_loss: 0.0898 - val_acc: 0.9686
Epoch 26/40
 - 5s - loss: 0.0497 - acc: 0.9755 - val_loss: 0.1006 - val_acc: 0.9672
Epoch 27/40
 - 5s - loss: 0.0506 - acc: 0.9756 - val_loss: 0.0930 - val_acc: 0.9674
Epoch 28/40
 - 5s - loss: 0.0504 - acc: 0.9755 - val_loss: 0.0896 - val_acc: 0.9685
Epoch 29/40
 - 5s - loss: 0.0506 - acc: 0.9756 - val_loss: 0.0939 - val_acc: 0.9679
Epoch 30/40
 - 5s - loss: 0.0498 - acc: 0.9752 - val_loss: 0.0947 - val_acc: 0.9680
Epoch 31/40
 - 5s - loss: 0.0500 - acc: 0.9756 - val_loss: 0.0960 - val_acc: 0.9687
Epoch 32/40
 - 5s - loss: 0.0493 - acc: 0.9755 - val_loss: 0.0967 - val_acc: 0.9675
Epoch 33/40
 - 5s - loss: 0.0499 - acc: 0.9757 - val_loss: 0.0917 - val_acc: 0.9683
Epoch 34/40
 - 5s - loss: 0.0495 - acc: 0.9758 - val_loss: 0.1005 - val_acc: 0.9675
Epoch 35/40
 - 5s - loss: 0.0501 - acc: 0.9757 - val_loss: 0.0931 - val_acc: 0.9686
Epoch 36/40
 - 5s - loss: 0.0493 - acc: 0.9754 - val_loss: 0.0985 - val_acc: 0.9671
Epoch 37/40
 - 5s - loss: 0.0499 - acc: 0.9753 - val_loss: 0.1050 - val_acc: 0.9683
Epoch 38/40
 - 5s - loss: 0.0497 - acc: 0.9753 - val_loss: 0.0983 - val_acc: 0.9673
Epoch 39/40
 - 5s - loss: 0.0489 - acc: 0.9758 - val_loss: 0.1046 - val_acc: 0.9679
Epoch 40/40
 - 5s - loss: 0.0494 - acc: 0.9762 - val_loss: 0.1000 - val_acc: 0.9687
Epoch 1/40
 - 1s - loss: 0.0905 - acc: 0.9663
Epoch 2/40
 - 1s - loss: 0.0804 - acc: 0.9678
Epoch 3/40
 - 1s - loss: 0.0751 - acc: 0.9688
Epoch 4/40
 - 1s - loss: 0.0706 - acc: 0.9688
Epoch 5/40
 - 1s - loss: 0.0695 - acc: 0.9699
Epoch 6/40
 - 1s - loss: 0.0660 - acc: 0.9708
Epoch 7/40
 - 1s - loss: 0.0647 - acc: 0.9708
Epoch 8/40
 - 1s - loss: 0.0637 - acc: 0.9704
Epoch 9/40
 - 1s - loss: 0.0610 - acc: 0.9719
Epoch 10/40
 - 1s - loss: 0.0604 - acc: 0.9727
Epoch 11/40
 - 1s - loss: 0.0582 - acc: 0.9722
Epoch 12/40
 - 1s - loss: 0.0567 - acc: 0.9739
Epoch 13/40
 - 1s - loss: 0.0550 - acc: 0.9745
Epoch 14/40
 - 1s - loss: 0.0535 - acc: 0.9743
Epoch 15/40
 - 1s - loss: 0.0540 - acc: 0.9731
Epoch 16/40
 - 1s - loss: 0.0509 - acc: 0.9750
Epoch 17/40
 - 1s - loss: 0.0516 - acc: 0.9741
Epoch 18/40
 - 1s - loss: 0.0521 - acc: 0.9736
Epoch 19/40
 - 1s - loss: 0.0491 - acc: 0.9754
Epoch 20/40
 - 1s - loss: 0.0498 - acc: 0.9753
Epoch 21/40
 - 1s - loss: 0.0491 - acc: 0.9744
Epoch 22/40
 - 1s - loss: 0.0489 - acc: 0.9757
Epoch 23/40
 - 1s - loss: 0.0471 - acc: 0.9755
Epoch 24/40
 - 1s - loss: 0.0470 - acc: 0.9753
Epoch 25/40
 - 1s - loss: 0.0459 - acc: 0.9756
Epoch 26/40
 - 1s - loss: 0.0469 - acc: 0.9758
Epoch 27/40
 - 1s - loss: 0.0456 - acc: 0.9772
Epoch 28/40
 - 1s - loss: 0.0462 - acc: 0.9759
Epoch 29/40
 - 1s - loss: 0.0451 - acc: 0.9769
Epoch 30/40
 - 1s - loss: 0.0462 - acc: 0.9773
Epoch 31/40
 - 1s - loss: 0.0435 - acc: 0.9774
Epoch 32/40
 - 1s - loss: 0.0438 - acc: 0.9773
Epoch 33/40
 - 1s - loss: 0.0452 - acc: 0.9761
Epoch 34/40
 - 1s - loss: 0.0439 - acc: 0.9776
Epoch 35/40
 - 1s - loss: 0.0419 - acc: 0.9771
Epoch 36/40
 - 1s - loss: 0.0427 - acc: 0.9773
Epoch 37/40
 - 1s - loss: 0.0424 - acc: 0.9778
Epoch 38/40
 - 1s - loss: 0.0423 - acc: 0.9768
Epoch 39/40
 - 1s - loss: 0.0419 - acc: 0.9761
Epoch 40/40
 - 1s - loss: 0.0421 - acc: 0.9764
# Training time = 0:04:09.743492
# F-Score(Ordinary) = 0.164, Recall: 0.656, Precision: 0.094
# F-Score(lvc) = 0.064, Recall: 0.2, Precision: 0.038
# F-Score(ireflv) = 0.229, Recall: 0.889, Precision: 0.131
# F-Score(id) = 0.187, Recall: 0.952, Precision: 0.104
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_233 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_234 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_233 (Embedding)       (None, 4, 48)        705264      input_233[0][0]                  
__________________________________________________________________________________________________
embedding_234 (Embedding)       (None, 4, 24)        5640        input_234[0][0]                  
__________________________________________________________________________________________________
flatten_233 (Flatten)           (None, 192)          0           embedding_233[0][0]              
__________________________________________________________________________________________________
flatten_234 (Flatten)           (None, 96)           0           embedding_234[0][0]              
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 288)          0           flatten_233[0][0]                
                                                                 flatten_234[0][0]                
__________________________________________________________________________________________________
dense_233 (Dense)               (None, 24)           6936        concatenate_117[0][0]            
__________________________________________________________________________________________________
dropout_117 (Dropout)           (None, 24)           0           dense_233[0][0]                  
__________________________________________________________________________________________________
dense_234 (Dense)               (None, 8)            200         dropout_117[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 2
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1107 - acc: 0.9599 - val_loss: 0.0781 - val_acc: 0.9671
Epoch 2/40
 - 5s - loss: 0.0719 - acc: 0.9681 - val_loss: 0.0738 - val_acc: 0.9681
Epoch 3/40
 - 5s - loss: 0.0655 - acc: 0.9699 - val_loss: 0.0713 - val_acc: 0.9674
Epoch 4/40
 - 5s - loss: 0.0623 - acc: 0.9712 - val_loss: 0.0701 - val_acc: 0.9686
Epoch 5/40
 - 5s - loss: 0.0593 - acc: 0.9721 - val_loss: 0.0724 - val_acc: 0.9678
Epoch 6/40
 - 5s - loss: 0.0576 - acc: 0.9725 - val_loss: 0.0739 - val_acc: 0.9671
Epoch 7/40
 - 5s - loss: 0.0561 - acc: 0.9728 - val_loss: 0.0715 - val_acc: 0.9678
Epoch 8/40
 - 5s - loss: 0.0546 - acc: 0.9734 - val_loss: 0.0740 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0537 - acc: 0.9737 - val_loss: 0.0730 - val_acc: 0.9686
Epoch 10/40
 - 5s - loss: 0.0534 - acc: 0.9739 - val_loss: 0.0770 - val_acc: 0.9680
Epoch 11/40
 - 5s - loss: 0.0523 - acc: 0.9744 - val_loss: 0.0850 - val_acc: 0.9660
Epoch 12/40
 - 5s - loss: 0.0517 - acc: 0.9746 - val_loss: 0.0771 - val_acc: 0.9675
Epoch 13/40
 - 5s - loss: 0.0516 - acc: 0.9744 - val_loss: 0.0772 - val_acc: 0.9660
Epoch 14/40
 - 5s - loss: 0.0507 - acc: 0.9746 - val_loss: 0.0840 - val_acc: 0.9680
Epoch 15/40
 - 5s - loss: 0.0506 - acc: 0.9745 - val_loss: 0.0823 - val_acc: 0.9678
Epoch 16/40
 - 5s - loss: 0.0502 - acc: 0.9749 - val_loss: 0.0784 - val_acc: 0.9674
Epoch 17/40
 - 5s - loss: 0.0502 - acc: 0.9748 - val_loss: 0.0799 - val_acc: 0.9686
Epoch 18/40
 - 5s - loss: 0.0500 - acc: 0.9754 - val_loss: 0.0825 - val_acc: 0.9677
Epoch 19/40
 - 5s - loss: 0.0504 - acc: 0.9753 - val_loss: 0.0794 - val_acc: 0.9678
Epoch 20/40
 - 5s - loss: 0.0497 - acc: 0.9750 - val_loss: 0.0836 - val_acc: 0.9685
Epoch 21/40
 - 5s - loss: 0.0496 - acc: 0.9755 - val_loss: 0.0844 - val_acc: 0.9685
Epoch 22/40
 - 5s - loss: 0.0496 - acc: 0.9750 - val_loss: 0.0865 - val_acc: 0.9673
Epoch 23/40
 - 5s - loss: 0.0499 - acc: 0.9753 - val_loss: 0.0838 - val_acc: 0.9672
Epoch 24/40
 - 5s - loss: 0.0495 - acc: 0.9755 - val_loss: 0.0931 - val_acc: 0.9674
Epoch 25/40
 - 5s - loss: 0.0494 - acc: 0.9754 - val_loss: 0.0889 - val_acc: 0.9680
Epoch 26/40
 - 5s - loss: 0.0496 - acc: 0.9758 - val_loss: 0.0916 - val_acc: 0.9687
Epoch 27/40
 - 5s - loss: 0.0492 - acc: 0.9758 - val_loss: 0.0936 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0497 - acc: 0.9753 - val_loss: 0.0862 - val_acc: 0.9681
Epoch 29/40
 - 5s - loss: 0.0496 - acc: 0.9756 - val_loss: 0.0915 - val_acc: 0.9678
Epoch 30/40
 - 5s - loss: 0.0497 - acc: 0.9759 - val_loss: 0.0839 - val_acc: 0.9680
Epoch 31/40
 - 5s - loss: 0.0495 - acc: 0.9756 - val_loss: 0.0899 - val_acc: 0.9684
Epoch 32/40
 - 5s - loss: 0.0492 - acc: 0.9759 - val_loss: 0.1002 - val_acc: 0.9671
Epoch 33/40
 - 5s - loss: 0.0496 - acc: 0.9760 - val_loss: 0.0983 - val_acc: 0.9679
Epoch 34/40
 - 5s - loss: 0.0495 - acc: 0.9756 - val_loss: 0.0921 - val_acc: 0.9677
Epoch 35/40
 - 5s - loss: 0.0491 - acc: 0.9759 - val_loss: 0.0909 - val_acc: 0.9667
Epoch 36/40
 - 5s - loss: 0.0484 - acc: 0.9761 - val_loss: 0.0924 - val_acc: 0.9674
Epoch 37/40
 - 5s - loss: 0.0497 - acc: 0.9761 - val_loss: 0.0978 - val_acc: 0.9682
Epoch 38/40
 - 5s - loss: 0.0493 - acc: 0.9761 - val_loss: 0.1012 - val_acc: 0.9685
Epoch 39/40
 - 5s - loss: 0.0499 - acc: 0.9756 - val_loss: 0.1088 - val_acc: 0.9676
Epoch 40/40
 - 5s - loss: 0.0496 - acc: 0.9759 - val_loss: 0.1050 - val_acc: 0.9679
Epoch 1/40
 - 1s - loss: 0.0902 - acc: 0.9662
Epoch 2/40
 - 1s - loss: 0.0814 - acc: 0.9658
Epoch 3/40
 - 1s - loss: 0.0758 - acc: 0.9682
Epoch 4/40
 - 1s - loss: 0.0713 - acc: 0.9695
Epoch 5/40
 - 1s - loss: 0.0707 - acc: 0.9687
Epoch 6/40
 - 1s - loss: 0.0629 - acc: 0.9714
Epoch 7/40
 - 1s - loss: 0.0638 - acc: 0.9712
Epoch 8/40
 - 1s - loss: 0.0615 - acc: 0.9728
Epoch 9/40
 - 1s - loss: 0.0588 - acc: 0.9721
Epoch 10/40
 - 1s - loss: 0.0576 - acc: 0.9723
Epoch 11/40
 - 1s - loss: 0.0557 - acc: 0.9736
Epoch 12/40
 - 1s - loss: 0.0559 - acc: 0.9737
Epoch 13/40
 - 1s - loss: 0.0546 - acc: 0.9738
Epoch 14/40
 - 1s - loss: 0.0525 - acc: 0.9737
Epoch 15/40
 - 1s - loss: 0.0511 - acc: 0.9745
Epoch 16/40
 - 1s - loss: 0.0508 - acc: 0.9751
Epoch 17/40
 - 1s - loss: 0.0496 - acc: 0.9754
Epoch 18/40
 - 1s - loss: 0.0504 - acc: 0.9754
Epoch 19/40
 - 1s - loss: 0.0488 - acc: 0.9753
Epoch 20/40
 - 1s - loss: 0.0484 - acc: 0.9756
Epoch 21/40
 - 1s - loss: 0.0473 - acc: 0.9765
Epoch 22/40
 - 1s - loss: 0.0472 - acc: 0.9768
Epoch 23/40
 - 1s - loss: 0.0472 - acc: 0.9760
Epoch 24/40
 - 1s - loss: 0.0460 - acc: 0.9773
Epoch 25/40
 - 1s - loss: 0.0472 - acc: 0.9771
Epoch 26/40
 - 1s - loss: 0.0465 - acc: 0.9766
Epoch 27/40
 - 1s - loss: 0.0446 - acc: 0.9779
Epoch 28/40
 - 1s - loss: 0.0470 - acc: 0.9767
Epoch 29/40
 - 1s - loss: 0.0452 - acc: 0.9769
Epoch 30/40
 - 1s - loss: 0.0447 - acc: 0.9763
Epoch 31/40
 - 1s - loss: 0.0432 - acc: 0.9766
Epoch 32/40
 - 1s - loss: 0.0442 - acc: 0.9772
Epoch 33/40
 - 1s - loss: 0.0456 - acc: 0.9781
Epoch 34/40
 - 1s - loss: 0.0442 - acc: 0.9763
Epoch 35/40
 - 1s - loss: 0.0438 - acc: 0.9770
Epoch 36/40
 - 1s - loss: 0.0437 - acc: 0.9773
Epoch 37/40
 - 1s - loss: 0.0442 - acc: 0.9777
Epoch 38/40
 - 1s - loss: 0.0428 - acc: 0.9769
Epoch 39/40
 - 1s - loss: 0.0430 - acc: 0.9778
Epoch 40/40
 - 1s - loss: 0.0427 - acc: 0.9772
# Training time = 0:04:08.650291
# F-Score(Ordinary) = 0.302, Recall: 0.91, Precision: 0.181
# F-Score(ireflv) = 0.105, Recall: 0.636, Precision: 0.057
# F-Score(id) = 0.541, Recall: 0.948, Precision: 0.378
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_235 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_236 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_235 (Embedding)       (None, 4, 48)        705264      input_235[0][0]                  
__________________________________________________________________________________________________
embedding_236 (Embedding)       (None, 4, 24)        5640        input_236[0][0]                  
__________________________________________________________________________________________________
flatten_235 (Flatten)           (None, 192)          0           embedding_235[0][0]              
__________________________________________________________________________________________________
flatten_236 (Flatten)           (None, 96)           0           embedding_236[0][0]              
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 288)          0           flatten_235[0][0]                
                                                                 flatten_236[0][0]                
__________________________________________________________________________________________________
dense_235 (Dense)               (None, 24)           6936        concatenate_118[0][0]            
__________________________________________________________________________________________________
dropout_118 (Dropout)           (None, 24)           0           dense_235[0][0]                  
__________________________________________________________________________________________________
dense_236 (Dense)               (None, 8)            200         dropout_118[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 2
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1091 - acc: 0.9606 - val_loss: 0.0793 - val_acc: 0.9666
Epoch 2/40
 - 5s - loss: 0.0710 - acc: 0.9680 - val_loss: 0.0721 - val_acc: 0.9674
Epoch 3/40
 - 5s - loss: 0.0638 - acc: 0.9701 - val_loss: 0.0696 - val_acc: 0.9682
Epoch 4/40
 - 5s - loss: 0.0601 - acc: 0.9713 - val_loss: 0.0708 - val_acc: 0.9685
Epoch 5/40
 - 5s - loss: 0.0574 - acc: 0.9719 - val_loss: 0.0721 - val_acc: 0.9681
Epoch 6/40
 - 5s - loss: 0.0558 - acc: 0.9728 - val_loss: 0.0731 - val_acc: 0.9687
Epoch 7/40
 - 5s - loss: 0.0547 - acc: 0.9737 - val_loss: 0.0722 - val_acc: 0.9686
Epoch 8/40
 - 5s - loss: 0.0536 - acc: 0.9737 - val_loss: 0.0758 - val_acc: 0.9694
Epoch 9/40
 - 5s - loss: 0.0526 - acc: 0.9737 - val_loss: 0.0772 - val_acc: 0.9660
Epoch 10/40
 - 5s - loss: 0.0523 - acc: 0.9742 - val_loss: 0.0766 - val_acc: 0.9687
Epoch 11/40
 - 5s - loss: 0.0515 - acc: 0.9743 - val_loss: 0.0807 - val_acc: 0.9669
Epoch 12/40
 - 5s - loss: 0.0510 - acc: 0.9744 - val_loss: 0.0767 - val_acc: 0.9686
Epoch 13/40
 - 5s - loss: 0.0508 - acc: 0.9745 - val_loss: 0.0796 - val_acc: 0.9660
Epoch 14/40
 - 5s - loss: 0.0509 - acc: 0.9746 - val_loss: 0.0797 - val_acc: 0.9682
Epoch 15/40
 - 5s - loss: 0.0511 - acc: 0.9744 - val_loss: 0.0795 - val_acc: 0.9687
Epoch 16/40
 - 5s - loss: 0.0506 - acc: 0.9745 - val_loss: 0.0768 - val_acc: 0.9687
Epoch 17/40
 - 5s - loss: 0.0505 - acc: 0.9750 - val_loss: 0.0832 - val_acc: 0.9674
Epoch 18/40
 - 5s - loss: 0.0511 - acc: 0.9750 - val_loss: 0.0846 - val_acc: 0.9682
Epoch 19/40
 - 5s - loss: 0.0503 - acc: 0.9756 - val_loss: 0.0845 - val_acc: 0.9679
Epoch 20/40
 - 5s - loss: 0.0497 - acc: 0.9747 - val_loss: 0.0858 - val_acc: 0.9676
Epoch 21/40
 - 5s - loss: 0.0503 - acc: 0.9754 - val_loss: 0.0885 - val_acc: 0.9664
Epoch 22/40
 - 5s - loss: 0.0505 - acc: 0.9751 - val_loss: 0.0915 - val_acc: 0.9685
Epoch 23/40
 - 5s - loss: 0.0510 - acc: 0.9753 - val_loss: 0.0851 - val_acc: 0.9676
Epoch 24/40
 - 5s - loss: 0.0506 - acc: 0.9749 - val_loss: 0.0819 - val_acc: 0.9682
Epoch 25/40
 - 5s - loss: 0.0507 - acc: 0.9754 - val_loss: 0.1016 - val_acc: 0.9679
Epoch 26/40
 - 5s - loss: 0.0504 - acc: 0.9750 - val_loss: 0.0919 - val_acc: 0.9660
Epoch 27/40
 - 5s - loss: 0.0502 - acc: 0.9755 - val_loss: 0.0902 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0502 - acc: 0.9754 - val_loss: 0.0918 - val_acc: 0.9626
Epoch 29/40
 - 5s - loss: 0.0505 - acc: 0.9758 - val_loss: 0.0911 - val_acc: 0.9673
Epoch 30/40
 - 5s - loss: 0.0503 - acc: 0.9755 - val_loss: 0.0955 - val_acc: 0.9686
Epoch 31/40
 - 5s - loss: 0.0498 - acc: 0.9755 - val_loss: 0.0979 - val_acc: 0.9671
Epoch 32/40
 - 5s - loss: 0.0502 - acc: 0.9759 - val_loss: 0.1026 - val_acc: 0.9675
Epoch 33/40
 - 5s - loss: 0.0497 - acc: 0.9758 - val_loss: 0.0944 - val_acc: 0.9684
Epoch 34/40
 - 5s - loss: 0.0505 - acc: 0.9757 - val_loss: 0.0970 - val_acc: 0.9668
Epoch 35/40
 - 5s - loss: 0.0500 - acc: 0.9759 - val_loss: 0.1064 - val_acc: 0.9679
Epoch 36/40
 - 5s - loss: 0.0496 - acc: 0.9759 - val_loss: 0.1004 - val_acc: 0.9673
Epoch 37/40
 - 5s - loss: 0.0508 - acc: 0.9760 - val_loss: 0.0920 - val_acc: 0.9682
Epoch 38/40
 - 5s - loss: 0.0495 - acc: 0.9756 - val_loss: 0.1129 - val_acc: 0.9657
Epoch 39/40
 - 5s - loss: 0.0505 - acc: 0.9756 - val_loss: 0.0981 - val_acc: 0.9673
Epoch 40/40
 - 5s - loss: 0.0501 - acc: 0.9756 - val_loss: 0.0985 - val_acc: 0.9671
Epoch 1/40
 - 1s - loss: 0.0947 - acc: 0.9663
Epoch 2/40
 - 1s - loss: 0.0850 - acc: 0.9675
Epoch 3/40
 - 1s - loss: 0.0784 - acc: 0.9697
Epoch 4/40
 - 1s - loss: 0.0735 - acc: 0.9697
Epoch 5/40
 - 1s - loss: 0.0707 - acc: 0.9693
Epoch 6/40
 - 1s - loss: 0.0699 - acc: 0.9704
Epoch 7/40
 - 1s - loss: 0.0657 - acc: 0.9719
Epoch 8/40
 - 1s - loss: 0.0635 - acc: 0.9721
Epoch 9/40
 - 1s - loss: 0.0604 - acc: 0.9737
Epoch 10/40
 - 1s - loss: 0.0577 - acc: 0.9728
Epoch 11/40
 - 1s - loss: 0.0579 - acc: 0.9741
Epoch 12/40
 - 1s - loss: 0.0579 - acc: 0.9740
Epoch 13/40
 - 1s - loss: 0.0545 - acc: 0.9740
Epoch 14/40
 - 1s - loss: 0.0531 - acc: 0.9749
Epoch 15/40
 - 1s - loss: 0.0542 - acc: 0.9750
Epoch 16/40
 - 1s - loss: 0.0522 - acc: 0.9743
Epoch 17/40
 - 1s - loss: 0.0508 - acc: 0.9758
Epoch 18/40
 - 1s - loss: 0.0492 - acc: 0.9765
Epoch 19/40
 - 1s - loss: 0.0519 - acc: 0.9754
Epoch 20/40
 - 1s - loss: 0.0483 - acc: 0.9761
Epoch 21/40
 - 1s - loss: 0.0503 - acc: 0.9765
Epoch 22/40
 - 1s - loss: 0.0482 - acc: 0.9771
Epoch 23/40
 - 1s - loss: 0.0478 - acc: 0.9762
Epoch 24/40
 - 1s - loss: 0.0466 - acc: 0.9770
Epoch 25/40
 - 1s - loss: 0.0483 - acc: 0.9759
Epoch 26/40
 - 1s - loss: 0.0462 - acc: 0.9772
Epoch 27/40
 - 1s - loss: 0.0462 - acc: 0.9758
Epoch 28/40
 - 1s - loss: 0.0470 - acc: 0.9771
Epoch 29/40
 - 1s - loss: 0.0449 - acc: 0.9760
Epoch 30/40
 - 1s - loss: 0.0439 - acc: 0.9767
Epoch 31/40
 - 1s - loss: 0.0444 - acc: 0.9765
Epoch 32/40
 - 1s - loss: 0.0442 - acc: 0.9768
Epoch 33/40
 - 1s - loss: 0.0440 - acc: 0.9771
Epoch 34/40
 - 1s - loss: 0.0442 - acc: 0.9779
Epoch 35/40
 - 1s - loss: 0.0427 - acc: 0.9788
Epoch 36/40
 - 1s - loss: 0.0426 - acc: 0.9770
Epoch 37/40
 - 1s - loss: 0.0424 - acc: 0.9775
Epoch 38/40
 - 1s - loss: 0.0429 - acc: 0.9786
Epoch 39/40
 - 1s - loss: 0.0440 - acc: 0.9772
Epoch 40/40
 - 1s - loss: 0.0428 - acc: 0.9778
# Training time = 0:04:08.794426
# F-Score(Ordinary) = 0.101, Recall: 0.923, Precision: 0.054
# F-Score(lvc) = 0.286, Recall: 1.0, Precision: 0.167
# F-Score(ireflv) = 0.016, Recall: 0.333, Precision: 0.008
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_237 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_238 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_237 (Embedding)       (None, 4, 48)        705264      input_237[0][0]                  
__________________________________________________________________________________________________
embedding_238 (Embedding)       (None, 4, 24)        5640        input_238[0][0]                  
__________________________________________________________________________________________________
flatten_237 (Flatten)           (None, 192)          0           embedding_237[0][0]              
__________________________________________________________________________________________________
flatten_238 (Flatten)           (None, 96)           0           embedding_238[0][0]              
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 288)          0           flatten_237[0][0]                
                                                                 flatten_238[0][0]                
__________________________________________________________________________________________________
dense_237 (Dense)               (None, 24)           6936        concatenate_119[0][0]            
__________________________________________________________________________________________________
dropout_119 (Dropout)           (None, 24)           0           dense_237[0][0]                  
__________________________________________________________________________________________________
dense_238 (Dense)               (None, 8)            200         dropout_119[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 2
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1148 - acc: 0.9591 - val_loss: 0.0786 - val_acc: 0.9654
Epoch 2/40
 - 5s - loss: 0.0733 - acc: 0.9682 - val_loss: 0.0776 - val_acc: 0.9656
Epoch 3/40
 - 5s - loss: 0.0661 - acc: 0.9701 - val_loss: 0.0707 - val_acc: 0.9688
Epoch 4/40
 - 5s - loss: 0.0620 - acc: 0.9709 - val_loss: 0.0709 - val_acc: 0.9684
Epoch 5/40
 - 5s - loss: 0.0588 - acc: 0.9724 - val_loss: 0.0782 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0573 - acc: 0.9723 - val_loss: 0.0719 - val_acc: 0.9679
Epoch 7/40
 - 5s - loss: 0.0560 - acc: 0.9732 - val_loss: 0.0713 - val_acc: 0.9687
Epoch 8/40
 - 5s - loss: 0.0539 - acc: 0.9735 - val_loss: 0.0756 - val_acc: 0.9670
Epoch 9/40
 - 5s - loss: 0.0532 - acc: 0.9741 - val_loss: 0.0746 - val_acc: 0.9687
Epoch 10/40
 - 5s - loss: 0.0526 - acc: 0.9744 - val_loss: 0.0742 - val_acc: 0.9681
Epoch 11/40
 - 5s - loss: 0.0524 - acc: 0.9743 - val_loss: 0.0782 - val_acc: 0.9690
Epoch 12/40
 - 5s - loss: 0.0520 - acc: 0.9745 - val_loss: 0.0750 - val_acc: 0.9687
Epoch 13/40
 - 5s - loss: 0.0511 - acc: 0.9750 - val_loss: 0.0811 - val_acc: 0.9688
Epoch 14/40
 - 5s - loss: 0.0508 - acc: 0.9745 - val_loss: 0.0791 - val_acc: 0.9682
Epoch 15/40
 - 5s - loss: 0.0511 - acc: 0.9747 - val_loss: 0.0818 - val_acc: 0.9683
Epoch 16/40
 - 5s - loss: 0.0510 - acc: 0.9750 - val_loss: 0.0843 - val_acc: 0.9679
Epoch 17/40
 - 5s - loss: 0.0510 - acc: 0.9751 - val_loss: 0.0829 - val_acc: 0.9687
Epoch 18/40
 - 5s - loss: 0.0503 - acc: 0.9755 - val_loss: 0.0877 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0507 - acc: 0.9755 - val_loss: 0.0883 - val_acc: 0.9680
Epoch 20/40
 - 5s - loss: 0.0509 - acc: 0.9753 - val_loss: 0.0813 - val_acc: 0.9685
Epoch 21/40
 - 5s - loss: 0.0503 - acc: 0.9755 - val_loss: 0.0998 - val_acc: 0.9671
Epoch 22/40
 - 5s - loss: 0.0510 - acc: 0.9751 - val_loss: 0.0881 - val_acc: 0.9681
Epoch 23/40
 - 5s - loss: 0.0508 - acc: 0.9750 - val_loss: 0.0897 - val_acc: 0.9679
Epoch 24/40
 - 5s - loss: 0.0507 - acc: 0.9756 - val_loss: 0.0850 - val_acc: 0.9685
Epoch 25/40
 - 5s - loss: 0.0499 - acc: 0.9756 - val_loss: 0.0887 - val_acc: 0.9685
Epoch 26/40
 - 5s - loss: 0.0502 - acc: 0.9756 - val_loss: 0.0891 - val_acc: 0.9683
Epoch 27/40
 - 5s - loss: 0.0500 - acc: 0.9755 - val_loss: 0.0893 - val_acc: 0.9677
Epoch 28/40
 - 5s - loss: 0.0502 - acc: 0.9752 - val_loss: 0.0911 - val_acc: 0.9682
Epoch 29/40
 - 5s - loss: 0.0503 - acc: 0.9751 - val_loss: 0.0863 - val_acc: 0.9686
Epoch 30/40
 - 5s - loss: 0.0499 - acc: 0.9755 - val_loss: 0.0955 - val_acc: 0.9651
Epoch 31/40
 - 5s - loss: 0.0494 - acc: 0.9757 - val_loss: 0.0915 - val_acc: 0.9675
Epoch 32/40
 - 5s - loss: 0.0497 - acc: 0.9753 - val_loss: 0.0962 - val_acc: 0.9678
Epoch 33/40
 - 5s - loss: 0.0494 - acc: 0.9758 - val_loss: 0.0935 - val_acc: 0.9681
Epoch 34/40
 - 5s - loss: 0.0489 - acc: 0.9758 - val_loss: 0.0960 - val_acc: 0.9682
Epoch 35/40
 - 5s - loss: 0.0492 - acc: 0.9755 - val_loss: 0.0892 - val_acc: 0.9689
Epoch 36/40
 - 5s - loss: 0.0490 - acc: 0.9758 - val_loss: 0.0954 - val_acc: 0.9681
Epoch 37/40
 - 5s - loss: 0.0491 - acc: 0.9761 - val_loss: 0.1018 - val_acc: 0.9661
Epoch 38/40
 - 5s - loss: 0.0485 - acc: 0.9762 - val_loss: 0.1013 - val_acc: 0.9679
Epoch 39/40
 - 5s - loss: 0.0493 - acc: 0.9757 - val_loss: 0.0993 - val_acc: 0.9684
Epoch 40/40
 - 5s - loss: 0.0486 - acc: 0.9759 - val_loss: 0.0960 - val_acc: 0.9685
Epoch 1/40
 - 1s - loss: 0.0922 - acc: 0.9657
Epoch 2/40
 - 1s - loss: 0.0802 - acc: 0.9686
Epoch 3/40
 - 1s - loss: 0.0749 - acc: 0.9690
Epoch 4/40
 - 1s - loss: 0.0717 - acc: 0.9687
Epoch 5/40
 - 1s - loss: 0.0656 - acc: 0.9688
Epoch 6/40
 - 1s - loss: 0.0642 - acc: 0.9709
Epoch 7/40
 - 1s - loss: 0.0637 - acc: 0.9703
Epoch 8/40
 - 1s - loss: 0.0607 - acc: 0.9719
Epoch 9/40
 - 1s - loss: 0.0594 - acc: 0.9725
Epoch 10/40
 - 1s - loss: 0.0577 - acc: 0.9738
Epoch 11/40
 - 1s - loss: 0.0577 - acc: 0.9741
Epoch 12/40
 - 1s - loss: 0.0546 - acc: 0.9746
Epoch 13/40
 - 1s - loss: 0.0549 - acc: 0.9737
Epoch 14/40
 - 1s - loss: 0.0513 - acc: 0.9747
Epoch 15/40
 - 1s - loss: 0.0495 - acc: 0.9762
Epoch 16/40
 - 1s - loss: 0.0515 - acc: 0.9741
Epoch 17/40
 - 1s - loss: 0.0491 - acc: 0.9752
Epoch 18/40
 - 1s - loss: 0.0486 - acc: 0.9763
Epoch 19/40
 - 1s - loss: 0.0482 - acc: 0.9758
Epoch 20/40
 - 1s - loss: 0.0473 - acc: 0.9759
Epoch 21/40
 - 1s - loss: 0.0456 - acc: 0.9756
Epoch 22/40
 - 1s - loss: 0.0469 - acc: 0.9760
Epoch 23/40
 - 1s - loss: 0.0455 - acc: 0.9760
Epoch 24/40
 - 1s - loss: 0.0452 - acc: 0.9765
Epoch 25/40
 - 1s - loss: 0.0470 - acc: 0.9761
Epoch 26/40
 - 1s - loss: 0.0458 - acc: 0.9761
Epoch 27/40
 - 1s - loss: 0.0451 - acc: 0.9767
Epoch 28/40
 - 1s - loss: 0.0439 - acc: 0.9766
Epoch 29/40
 - 1s - loss: 0.0442 - acc: 0.9767
Epoch 30/40
 - 1s - loss: 0.0434 - acc: 0.9767
Epoch 31/40
 - 1s - loss: 0.0444 - acc: 0.9777
Epoch 32/40
 - 1s - loss: 0.0438 - acc: 0.9769
Epoch 33/40
 - 1s - loss: 0.0437 - acc: 0.9766
Epoch 34/40
 - 1s - loss: 0.0425 - acc: 0.9779
Epoch 35/40
 - 1s - loss: 0.0420 - acc: 0.9779
Epoch 36/40
 - 1s - loss: 0.0425 - acc: 0.9762
Epoch 37/40
 - 1s - loss: 0.0425 - acc: 0.9767
Epoch 38/40
 - 1s - loss: 0.0418 - acc: 0.9768
Epoch 39/40
 - 1s - loss: 0.0434 - acc: 0.9760
Epoch 40/40
 - 1s - loss: 0.0425 - acc: 0.9772
# Training time = 0:04:08.078705
# F-Score(Ordinary) = 0.247, Recall: 0.55, Precision: 0.159
# F-Score(lvc) = 0.496, Recall: 0.508, Precision: 0.485
# F-Score(id) = 0.031, Recall: 1.0, Precision: 0.016
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_239 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_240 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_239 (Embedding)       (None, 4, 48)        705264      input_239[0][0]                  
__________________________________________________________________________________________________
embedding_240 (Embedding)       (None, 4, 24)        5640        input_240[0][0]                  
__________________________________________________________________________________________________
flatten_239 (Flatten)           (None, 192)          0           embedding_239[0][0]              
__________________________________________________________________________________________________
flatten_240 (Flatten)           (None, 96)           0           embedding_240[0][0]              
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 288)          0           flatten_239[0][0]                
                                                                 flatten_240[0][0]                
__________________________________________________________________________________________________
dense_239 (Dense)               (None, 24)           6936        concatenate_120[0][0]            
__________________________________________________________________________________________________
dropout_120 (Dropout)           (None, 24)           0           dense_239[0][0]                  
__________________________________________________________________________________________________
dense_240 (Dense)               (None, 8)            200         dropout_120[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 2
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1118 - acc: 0.9589 - val_loss: 0.0805 - val_acc: 0.9661
Epoch 2/40
 - 5s - loss: 0.0720 - acc: 0.9682 - val_loss: 0.0738 - val_acc: 0.9674
Epoch 3/40
 - 5s - loss: 0.0649 - acc: 0.9699 - val_loss: 0.0711 - val_acc: 0.9681
Epoch 4/40
 - 5s - loss: 0.0607 - acc: 0.9710 - val_loss: 0.0699 - val_acc: 0.9676
Epoch 5/40
 - 5s - loss: 0.0580 - acc: 0.9719 - val_loss: 0.0697 - val_acc: 0.9669
Epoch 6/40
 - 5s - loss: 0.0561 - acc: 0.9726 - val_loss: 0.0697 - val_acc: 0.9695
Epoch 7/40
 - 5s - loss: 0.0547 - acc: 0.9730 - val_loss: 0.0756 - val_acc: 0.9682
Epoch 8/40
 - 5s - loss: 0.0538 - acc: 0.9734 - val_loss: 0.0723 - val_acc: 0.9692
Epoch 9/40
 - 5s - loss: 0.0523 - acc: 0.9745 - val_loss: 0.0741 - val_acc: 0.9684
Epoch 10/40
 - 5s - loss: 0.0520 - acc: 0.9742 - val_loss: 0.0757 - val_acc: 0.9686
Epoch 11/40
 - 5s - loss: 0.0513 - acc: 0.9744 - val_loss: 0.0750 - val_acc: 0.9694
Epoch 12/40
 - 5s - loss: 0.0510 - acc: 0.9743 - val_loss: 0.0744 - val_acc: 0.9678
Epoch 13/40
 - 5s - loss: 0.0501 - acc: 0.9750 - val_loss: 0.0752 - val_acc: 0.9678
Epoch 14/40
 - 5s - loss: 0.0501 - acc: 0.9752 - val_loss: 0.0777 - val_acc: 0.9695
Epoch 15/40
 - 5s - loss: 0.0496 - acc: 0.9748 - val_loss: 0.0757 - val_acc: 0.9684
Epoch 16/40
 - 5s - loss: 0.0497 - acc: 0.9745 - val_loss: 0.0790 - val_acc: 0.9689
Epoch 17/40
 - 5s - loss: 0.0496 - acc: 0.9752 - val_loss: 0.0761 - val_acc: 0.9686
Epoch 18/40
 - 5s - loss: 0.0494 - acc: 0.9754 - val_loss: 0.0792 - val_acc: 0.9682
Epoch 19/40
 - 5s - loss: 0.0492 - acc: 0.9751 - val_loss: 0.0811 - val_acc: 0.9682
Epoch 20/40
 - 5s - loss: 0.0489 - acc: 0.9751 - val_loss: 0.0842 - val_acc: 0.9673
Epoch 21/40
 - 5s - loss: 0.0483 - acc: 0.9756 - val_loss: 0.0856 - val_acc: 0.9683
Epoch 22/40
 - 5s - loss: 0.0485 - acc: 0.9752 - val_loss: 0.0851 - val_acc: 0.9676
Epoch 23/40
 - 5s - loss: 0.0488 - acc: 0.9758 - val_loss: 0.0844 - val_acc: 0.9686
Epoch 24/40
 - 5s - loss: 0.0482 - acc: 0.9755 - val_loss: 0.0892 - val_acc: 0.9684
Epoch 25/40
 - 5s - loss: 0.0483 - acc: 0.9759 - val_loss: 0.0852 - val_acc: 0.9671
Epoch 26/40
 - 5s - loss: 0.0486 - acc: 0.9756 - val_loss: 0.0917 - val_acc: 0.9678
Epoch 27/40
 - 5s - loss: 0.0488 - acc: 0.9761 - val_loss: 0.0882 - val_acc: 0.9694
Epoch 28/40
 - 5s - loss: 0.0476 - acc: 0.9760 - val_loss: 0.0883 - val_acc: 0.9680
Epoch 29/40
 - 5s - loss: 0.0478 - acc: 0.9760 - val_loss: 0.0922 - val_acc: 0.9679
Epoch 30/40
 - 5s - loss: 0.0481 - acc: 0.9759 - val_loss: 0.0887 - val_acc: 0.9687
Epoch 31/40
 - 5s - loss: 0.0478 - acc: 0.9763 - val_loss: 0.0934 - val_acc: 0.9673
Epoch 32/40
 - 5s - loss: 0.0479 - acc: 0.9762 - val_loss: 0.0845 - val_acc: 0.9679
Epoch 33/40
 - 5s - loss: 0.0476 - acc: 0.9757 - val_loss: 0.0917 - val_acc: 0.9675
Epoch 34/40
 - 5s - loss: 0.0479 - acc: 0.9763 - val_loss: 0.0919 - val_acc: 0.9676
Epoch 35/40
 - 5s - loss: 0.0475 - acc: 0.9756 - val_loss: 0.0968 - val_acc: 0.9686
Epoch 36/40
 - 5s - loss: 0.0484 - acc: 0.9763 - val_loss: 0.0965 - val_acc: 0.9677
Epoch 37/40
 - 5s - loss: 0.0478 - acc: 0.9765 - val_loss: 0.0970 - val_acc: 0.9682
Epoch 38/40
 - 5s - loss: 0.0476 - acc: 0.9766 - val_loss: 0.0985 - val_acc: 0.9667
Epoch 39/40
 - 5s - loss: 0.0472 - acc: 0.9765 - val_loss: 0.1007 - val_acc: 0.9671
Epoch 40/40
 - 5s - loss: 0.0476 - acc: 0.9765 - val_loss: 0.1172 - val_acc: 0.9670
Epoch 1/40
 - 1s - loss: 0.0939 - acc: 0.9658
Epoch 2/40
 - 1s - loss: 0.0803 - acc: 0.9675
Epoch 3/40
 - 1s - loss: 0.0734 - acc: 0.9684
Epoch 4/40
 - 1s - loss: 0.0685 - acc: 0.9701
Epoch 5/40
 - 1s - loss: 0.0664 - acc: 0.9701
Epoch 6/40
 - 1s - loss: 0.0617 - acc: 0.9709
Epoch 7/40
 - 1s - loss: 0.0610 - acc: 0.9716
Epoch 8/40
 - 1s - loss: 0.0578 - acc: 0.9713
Epoch 9/40
 - 1s - loss: 0.0547 - acc: 0.9722
Epoch 10/40
 - 1s - loss: 0.0544 - acc: 0.9728
Epoch 11/40
 - 1s - loss: 0.0533 - acc: 0.9745
Epoch 12/40
 - 1s - loss: 0.0533 - acc: 0.9739
Epoch 13/40
 - 1s - loss: 0.0524 - acc: 0.9739
Epoch 14/40
 - 1s - loss: 0.0490 - acc: 0.9755
Epoch 15/40
 - 1s - loss: 0.0504 - acc: 0.9749
Epoch 16/40
 - 1s - loss: 0.0474 - acc: 0.9748
Epoch 17/40
 - 1s - loss: 0.0471 - acc: 0.9749
Epoch 18/40
 - 1s - loss: 0.0471 - acc: 0.9753
Epoch 19/40
 - 1s - loss: 0.0479 - acc: 0.9769
Epoch 20/40
 - 1s - loss: 0.0431 - acc: 0.9769
Epoch 21/40
 - 1s - loss: 0.0441 - acc: 0.9765
Epoch 22/40
 - 1s - loss: 0.0433 - acc: 0.9766
Epoch 23/40
 - 1s - loss: 0.0455 - acc: 0.9769
Epoch 24/40
 - 1s - loss: 0.0432 - acc: 0.9768
Epoch 25/40
 - 1s - loss: 0.0437 - acc: 0.9775
Epoch 26/40
 - 1s - loss: 0.0434 - acc: 0.9770
Epoch 27/40
 - 1s - loss: 0.0434 - acc: 0.9765
Epoch 28/40
 - 1s - loss: 0.0431 - acc: 0.9775
Epoch 29/40
 - 1s - loss: 0.0418 - acc: 0.9779
Epoch 30/40
 - 1s - loss: 0.0424 - acc: 0.9775
Epoch 31/40
 - 1s - loss: 0.0418 - acc: 0.9779
Epoch 32/40
 - 1s - loss: 0.0412 - acc: 0.9777
Epoch 33/40
 - 1s - loss: 0.0416 - acc: 0.9774
Epoch 34/40
 - 1s - loss: 0.0406 - acc: 0.9782
Epoch 35/40
 - 1s - loss: 0.0407 - acc: 0.9781
Epoch 36/40
 - 1s - loss: 0.0406 - acc: 0.9788
Epoch 37/40
 - 1s - loss: 0.0390 - acc: 0.9774
Epoch 38/40
 - 1s - loss: 0.0402 - acc: 0.9798
Epoch 39/40
 - 1s - loss: 0.0403 - acc: 0.9776
Epoch 40/40
 - 1s - loss: 0.0397 - acc: 0.9779
# Training time = 0:04:03.381241
# F-Score(Ordinary) = 0.514, Recall: 0.561, Precision: 0.474
# F-Score(lvc) = 0.394, Recall: 0.317, Precision: 0.523
# F-Score(ireflv) = 0.718, Recall: 0.806, Precision: 0.648
# F-Score(id) = 0.471, Recall: 0.968, Precision: 0.311
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_241 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_242 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_241 (Embedding)       (None, 4, 48)        705264      input_241[0][0]                  
__________________________________________________________________________________________________
embedding_242 (Embedding)       (None, 4, 24)        5640        input_242[0][0]                  
__________________________________________________________________________________________________
flatten_241 (Flatten)           (None, 192)          0           embedding_241[0][0]              
__________________________________________________________________________________________________
flatten_242 (Flatten)           (None, 96)           0           embedding_242[0][0]              
__________________________________________________________________________________________________
concatenate_121 (Concatenate)   (None, 288)          0           flatten_241[0][0]                
                                                                 flatten_242[0][0]                
__________________________________________________________________________________________________
dense_241 (Dense)               (None, 24)           6936        concatenate_121[0][0]            
__________________________________________________________________________________________________
dropout_121 (Dropout)           (None, 24)           0           dense_241[0][0]                  
__________________________________________________________________________________________________
dense_242 (Dense)               (None, 8)            200         dropout_121[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0960 - acc: 0.9620 - val_loss: 0.0745 - val_acc: 0.9676
Epoch 2/40
 - 5s - loss: 0.0690 - acc: 0.9689 - val_loss: 0.0711 - val_acc: 0.9677
Epoch 3/40
 - 5s - loss: 0.0627 - acc: 0.9708 - val_loss: 0.0763 - val_acc: 0.9684
Epoch 4/40
 - 5s - loss: 0.0605 - acc: 0.9721 - val_loss: 0.0775 - val_acc: 0.9669
Epoch 5/40
 - 5s - loss: 0.0586 - acc: 0.9721 - val_loss: 0.0784 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0573 - acc: 0.9727 - val_loss: 0.0794 - val_acc: 0.9680
Epoch 7/40
 - 5s - loss: 0.0572 - acc: 0.9729 - val_loss: 0.0896 - val_acc: 0.9669
Epoch 8/40
 - 5s - loss: 0.0568 - acc: 0.9732 - val_loss: 0.0804 - val_acc: 0.9687
Epoch 9/40
 - 5s - loss: 0.0559 - acc: 0.9739 - val_loss: 0.0830 - val_acc: 0.9691
Epoch 10/40
 - 5s - loss: 0.0563 - acc: 0.9734 - val_loss: 0.0991 - val_acc: 0.9671
Epoch 11/40
 - 5s - loss: 0.0552 - acc: 0.9741 - val_loss: 0.0865 - val_acc: 0.9679
Epoch 12/40
 - 5s - loss: 0.0554 - acc: 0.9739 - val_loss: 0.1027 - val_acc: 0.9671
Epoch 13/40
 - 5s - loss: 0.0560 - acc: 0.9740 - val_loss: 0.0944 - val_acc: 0.9686
Epoch 14/40
 - 5s - loss: 0.0557 - acc: 0.9742 - val_loss: 0.0988 - val_acc: 0.9676
Epoch 15/40
 - 5s - loss: 0.0558 - acc: 0.9744 - val_loss: 0.0926 - val_acc: 0.9667
Epoch 16/40
 - 5s - loss: 0.0560 - acc: 0.9742 - val_loss: 0.0963 - val_acc: 0.9677
Epoch 17/40
 - 5s - loss: 0.0561 - acc: 0.9746 - val_loss: 0.0978 - val_acc: 0.9679
Epoch 18/40
 - 5s - loss: 0.0555 - acc: 0.9746 - val_loss: 0.1033 - val_acc: 0.9675
Epoch 19/40
 - 5s - loss: 0.0543 - acc: 0.9747 - val_loss: 0.1024 - val_acc: 0.9678
Epoch 20/40
 - 5s - loss: 0.0549 - acc: 0.9749 - val_loss: 0.1019 - val_acc: 0.9679
Epoch 21/40
 - 5s - loss: 0.0551 - acc: 0.9747 - val_loss: 0.0986 - val_acc: 0.9664
Epoch 22/40
 - 5s - loss: 0.0541 - acc: 0.9746 - val_loss: 0.1034 - val_acc: 0.9682
Epoch 23/40
 - 5s - loss: 0.0557 - acc: 0.9751 - val_loss: 0.1096 - val_acc: 0.9675
Epoch 24/40
 - 5s - loss: 0.0558 - acc: 0.9750 - val_loss: 0.1149 - val_acc: 0.9677
Epoch 25/40
 - 5s - loss: 0.0549 - acc: 0.9752 - val_loss: 0.1218 - val_acc: 0.9688
Epoch 26/40
 - 5s - loss: 0.0537 - acc: 0.9751 - val_loss: 0.1131 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0549 - acc: 0.9748 - val_loss: 0.1090 - val_acc: 0.9680
Epoch 28/40
 - 5s - loss: 0.0547 - acc: 0.9752 - val_loss: 0.1089 - val_acc: 0.9683
Epoch 29/40
 - 5s - loss: 0.0549 - acc: 0.9751 - val_loss: 0.1202 - val_acc: 0.9684
Epoch 30/40
 - 5s - loss: 0.0541 - acc: 0.9751 - val_loss: 0.1224 - val_acc: 0.9684
Epoch 31/40
 - 5s - loss: 0.0545 - acc: 0.9753 - val_loss: 0.1199 - val_acc: 0.9682
Epoch 32/40
 - 5s - loss: 0.0546 - acc: 0.9754 - val_loss: 0.1211 - val_acc: 0.9677
Epoch 33/40
 - 5s - loss: 0.0551 - acc: 0.9751 - val_loss: 0.1124 - val_acc: 0.9673
Epoch 34/40
 - 5s - loss: 0.0540 - acc: 0.9752 - val_loss: 0.1259 - val_acc: 0.9672
Epoch 35/40
 - 5s - loss: 0.0546 - acc: 0.9756 - val_loss: 0.1223 - val_acc: 0.9682
Epoch 36/40
 - 5s - loss: 0.0546 - acc: 0.9754 - val_loss: 0.1063 - val_acc: 0.9683
Epoch 37/40
 - 5s - loss: 0.0540 - acc: 0.9756 - val_loss: 0.1227 - val_acc: 0.9684
Epoch 38/40
 - 5s - loss: 0.0541 - acc: 0.9755 - val_loss: 0.1241 - val_acc: 0.9676
Epoch 39/40
 - 5s - loss: 0.0545 - acc: 0.9759 - val_loss: 0.1222 - val_acc: 0.9670
Epoch 40/40
 - 5s - loss: 0.0538 - acc: 0.9756 - val_loss: 0.1273 - val_acc: 0.9680
Epoch 1/40
 - 1s - loss: 0.1246 - acc: 0.9651
Epoch 2/40
 - 1s - loss: 0.1015 - acc: 0.9672
Epoch 3/40
 - 1s - loss: 0.0918 - acc: 0.9668
Epoch 4/40
 - 1s - loss: 0.0878 - acc: 0.9679
Epoch 5/40
 - 1s - loss: 0.0850 - acc: 0.9686
Epoch 6/40
 - 1s - loss: 0.0806 - acc: 0.9699
Epoch 7/40
 - 1s - loss: 0.0761 - acc: 0.9710
Epoch 8/40
 - 1s - loss: 0.0758 - acc: 0.9712
Epoch 9/40
 - 1s - loss: 0.0692 - acc: 0.9724
Epoch 10/40
 - 1s - loss: 0.0676 - acc: 0.9742
Epoch 11/40
 - 1s - loss: 0.0682 - acc: 0.9727
Epoch 12/40
 - 1s - loss: 0.0623 - acc: 0.9752
Epoch 13/40
 - 1s - loss: 0.0610 - acc: 0.9750
Epoch 14/40
 - 1s - loss: 0.0640 - acc: 0.9745
Epoch 15/40
 - 1s - loss: 0.0630 - acc: 0.9748
Epoch 16/40
 - 1s - loss: 0.0611 - acc: 0.9752
Epoch 17/40
 - 1s - loss: 0.0614 - acc: 0.9747
Epoch 18/40
 - 1s - loss: 0.0604 - acc: 0.9754
Epoch 19/40
 - 1s - loss: 0.0608 - acc: 0.9742
Epoch 20/40
 - 1s - loss: 0.0551 - acc: 0.9774
Epoch 21/40
 - 1s - loss: 0.0557 - acc: 0.9749
Epoch 22/40
 - 1s - loss: 0.0561 - acc: 0.9761
Epoch 23/40
 - 1s - loss: 0.0542 - acc: 0.9775
Epoch 24/40
 - 1s - loss: 0.0566 - acc: 0.9745
Epoch 25/40
 - 1s - loss: 0.0528 - acc: 0.9770
Epoch 26/40
 - 1s - loss: 0.0551 - acc: 0.9769
Epoch 27/40
 - 1s - loss: 0.0530 - acc: 0.9768
Epoch 28/40
 - 1s - loss: 0.0536 - acc: 0.9758
Epoch 29/40
 - 1s - loss: 0.0561 - acc: 0.9770
Epoch 30/40
 - 1s - loss: 0.0570 - acc: 0.9771
Epoch 31/40
 - 1s - loss: 0.0503 - acc: 0.9773
Epoch 32/40
 - 1s - loss: 0.0517 - acc: 0.9777
Epoch 33/40
 - 1s - loss: 0.0536 - acc: 0.9763
Epoch 34/40
 - 1s - loss: 0.0529 - acc: 0.9782
Epoch 35/40
 - 1s - loss: 0.0517 - acc: 0.9766
Epoch 36/40
 - 1s - loss: 0.0521 - acc: 0.9778
Epoch 37/40
 - 1s - loss: 0.0530 - acc: 0.9768
Epoch 38/40
 - 1s - loss: 0.0505 - acc: 0.9769
Epoch 39/40
 - 1s - loss: 0.0532 - acc: 0.9775
Epoch 40/40
 - 1s - loss: 0.0492 - acc: 0.9782
# Training time = 0:04:03.215834
# F-Score(Ordinary) = 0.229, Recall: 0.868, Precision: 0.132
# F-Score(ireflv) = 0.624, Recall: 0.881, Precision: 0.484
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_243 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_244 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_243 (Embedding)       (None, 4, 48)        705264      input_243[0][0]                  
__________________________________________________________________________________________________
embedding_244 (Embedding)       (None, 4, 24)        5640        input_244[0][0]                  
__________________________________________________________________________________________________
flatten_243 (Flatten)           (None, 192)          0           embedding_243[0][0]              
__________________________________________________________________________________________________
flatten_244 (Flatten)           (None, 96)           0           embedding_244[0][0]              
__________________________________________________________________________________________________
concatenate_122 (Concatenate)   (None, 288)          0           flatten_243[0][0]                
                                                                 flatten_244[0][0]                
__________________________________________________________________________________________________
dense_243 (Dense)               (None, 24)           6936        concatenate_122[0][0]            
__________________________________________________________________________________________________
dropout_122 (Dropout)           (None, 24)           0           dense_243[0][0]                  
__________________________________________________________________________________________________
dense_244 (Dense)               (None, 8)            200         dropout_122[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0970 - acc: 0.9616 - val_loss: 0.0757 - val_acc: 0.9679
Epoch 2/40
 - 5s - loss: 0.0690 - acc: 0.9689 - val_loss: 0.0786 - val_acc: 0.9658
Epoch 3/40
 - 5s - loss: 0.0638 - acc: 0.9703 - val_loss: 0.0749 - val_acc: 0.9672
Epoch 4/40
 - 5s - loss: 0.0609 - acc: 0.9716 - val_loss: 0.0733 - val_acc: 0.9682
Epoch 5/40
 - 5s - loss: 0.0581 - acc: 0.9725 - val_loss: 0.0748 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0568 - acc: 0.9732 - val_loss: 0.0824 - val_acc: 0.9674
Epoch 7/40
 - 5s - loss: 0.0553 - acc: 0.9736 - val_loss: 0.0795 - val_acc: 0.9671
Epoch 8/40
 - 5s - loss: 0.0543 - acc: 0.9740 - val_loss: 0.0847 - val_acc: 0.9673
Epoch 9/40
 - 5s - loss: 0.0541 - acc: 0.9738 - val_loss: 0.0808 - val_acc: 0.9684
Epoch 10/40
 - 5s - loss: 0.0540 - acc: 0.9741 - val_loss: 0.0880 - val_acc: 0.9693
Epoch 11/40
 - 5s - loss: 0.0535 - acc: 0.9741 - val_loss: 0.0989 - val_acc: 0.9649
Epoch 12/40
 - 5s - loss: 0.0535 - acc: 0.9743 - val_loss: 0.0925 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0540 - acc: 0.9745 - val_loss: 0.0951 - val_acc: 0.9677
Epoch 14/40
 - 5s - loss: 0.0537 - acc: 0.9748 - val_loss: 0.0981 - val_acc: 0.9685
Epoch 15/40
 - 5s - loss: 0.0541 - acc: 0.9745 - val_loss: 0.0964 - val_acc: 0.9682
Epoch 16/40
 - 5s - loss: 0.0531 - acc: 0.9750 - val_loss: 0.0946 - val_acc: 0.9669
Epoch 17/40
 - 5s - loss: 0.0525 - acc: 0.9747 - val_loss: 0.0984 - val_acc: 0.9674
Epoch 18/40
 - 5s - loss: 0.0522 - acc: 0.9754 - val_loss: 0.0984 - val_acc: 0.9679
Epoch 19/40
 - 5s - loss: 0.0528 - acc: 0.9754 - val_loss: 0.0991 - val_acc: 0.9683
Epoch 20/40
 - 5s - loss: 0.0532 - acc: 0.9752 - val_loss: 0.1047 - val_acc: 0.9656
Epoch 21/40
 - 5s - loss: 0.0527 - acc: 0.9754 - val_loss: 0.1010 - val_acc: 0.9691
Epoch 22/40
 - 5s - loss: 0.0507 - acc: 0.9757 - val_loss: 0.1154 - val_acc: 0.9672
Epoch 23/40
 - 5s - loss: 0.0525 - acc: 0.9750 - val_loss: 0.1098 - val_acc: 0.9661
Epoch 24/40
 - 5s - loss: 0.0526 - acc: 0.9755 - val_loss: 0.1215 - val_acc: 0.9677
Epoch 25/40
 - 5s - loss: 0.0522 - acc: 0.9755 - val_loss: 0.1104 - val_acc: 0.9670
Epoch 26/40
 - 5s - loss: 0.0522 - acc: 0.9751 - val_loss: 0.1176 - val_acc: 0.9665
Epoch 27/40
 - 5s - loss: 0.0515 - acc: 0.9755 - val_loss: 0.1277 - val_acc: 0.9657
Epoch 28/40
 - 5s - loss: 0.0522 - acc: 0.9754 - val_loss: 0.1205 - val_acc: 0.9671
Epoch 29/40
 - 5s - loss: 0.0520 - acc: 0.9763 - val_loss: 0.1222 - val_acc: 0.9680
Epoch 30/40
 - 5s - loss: 0.0527 - acc: 0.9759 - val_loss: 0.1009 - val_acc: 0.9673
Epoch 31/40
 - 5s - loss: 0.0527 - acc: 0.9762 - val_loss: 0.1099 - val_acc: 0.9685
Epoch 32/40
 - 5s - loss: 0.0512 - acc: 0.9756 - val_loss: 0.1121 - val_acc: 0.9677
Epoch 33/40
 - 5s - loss: 0.0505 - acc: 0.9759 - val_loss: 0.1278 - val_acc: 0.9687
Epoch 34/40
 - 5s - loss: 0.0513 - acc: 0.9760 - val_loss: 0.1311 - val_acc: 0.9643
Epoch 35/40
 - 5s - loss: 0.0514 - acc: 0.9760 - val_loss: 0.1342 - val_acc: 0.9638
Epoch 36/40
 - 5s - loss: 0.0506 - acc: 0.9761 - val_loss: 0.1183 - val_acc: 0.9667
Epoch 37/40
 - 5s - loss: 0.0503 - acc: 0.9767 - val_loss: 0.1264 - val_acc: 0.9654
Epoch 38/40
 - 5s - loss: 0.0515 - acc: 0.9761 - val_loss: 0.1201 - val_acc: 0.9673
Epoch 39/40
 - 5s - loss: 0.0508 - acc: 0.9761 - val_loss: 0.1381 - val_acc: 0.9657
Epoch 40/40
 - 5s - loss: 0.0503 - acc: 0.9761 - val_loss: 0.1241 - val_acc: 0.9670
Epoch 1/40
 - 1s - loss: 0.1088 - acc: 0.9648
Epoch 2/40
 - 1s - loss: 0.0949 - acc: 0.9658
Epoch 3/40
 - 1s - loss: 0.0905 - acc: 0.9681
Epoch 4/40
 - 1s - loss: 0.0835 - acc: 0.9674
Epoch 5/40
 - 1s - loss: 0.0852 - acc: 0.9701
Epoch 6/40
 - 1s - loss: 0.0751 - acc: 0.9716
Epoch 7/40
 - 1s - loss: 0.0724 - acc: 0.9718
Epoch 8/40
 - 1s - loss: 0.0713 - acc: 0.9726
Epoch 9/40
 - 1s - loss: 0.0695 - acc: 0.9732
Epoch 10/40
 - 1s - loss: 0.0711 - acc: 0.9724
Epoch 11/40
 - 1s - loss: 0.0649 - acc: 0.9728
Epoch 12/40
 - 1s - loss: 0.0663 - acc: 0.9739
Epoch 13/40
 - 1s - loss: 0.0660 - acc: 0.9749
Epoch 14/40
 - 1s - loss: 0.0650 - acc: 0.9754
Epoch 15/40
 - 1s - loss: 0.0625 - acc: 0.9738
Epoch 16/40
 - 1s - loss: 0.0608 - acc: 0.9750
Epoch 17/40
 - 1s - loss: 0.0599 - acc: 0.9748
Epoch 18/40
 - 1s - loss: 0.0605 - acc: 0.9752
Epoch 19/40
 - 1s - loss: 0.0604 - acc: 0.9765
Epoch 20/40
 - 1s - loss: 0.0599 - acc: 0.9749
Epoch 21/40
 - 1s - loss: 0.0555 - acc: 0.9758
Epoch 22/40
 - 1s - loss: 0.0565 - acc: 0.9768
Epoch 23/40
 - 1s - loss: 0.0599 - acc: 0.9757
Epoch 24/40
 - 1s - loss: 0.0563 - acc: 0.9767
Epoch 25/40
 - 1s - loss: 0.0564 - acc: 0.9771
Epoch 26/40
 - 1s - loss: 0.0575 - acc: 0.9759
Epoch 27/40
 - 1s - loss: 0.0566 - acc: 0.9765
Epoch 28/40
 - 1s - loss: 0.0572 - acc: 0.9764
Epoch 29/40
 - 1s - loss: 0.0525 - acc: 0.9777
Epoch 30/40
 - 1s - loss: 0.0555 - acc: 0.9755
Epoch 31/40
 - 1s - loss: 0.0550 - acc: 0.9770
Epoch 32/40
 - 1s - loss: 0.0563 - acc: 0.9772
Epoch 33/40
 - 1s - loss: 0.0539 - acc: 0.9780
Epoch 34/40
 - 1s - loss: 0.0524 - acc: 0.9775
Epoch 35/40
 - 1s - loss: 0.0556 - acc: 0.9769
Epoch 36/40
 - 1s - loss: 0.0539 - acc: 0.9773
Epoch 37/40
 - 1s - loss: 0.0569 - acc: 0.9760
Epoch 38/40
 - 1s - loss: 0.0563 - acc: 0.9774
Epoch 39/40
 - 1s - loss: 0.0533 - acc: 0.9776
Epoch 40/40
 - 1s - loss: 0.0537 - acc: 0.9770
# Training time = 0:04:07.633165
# F-Score(Ordinary) = 0.268, Recall: 0.855, Precision: 0.159
# F-Score(ireflv) = 0.062, Recall: 0.571, Precision: 0.033
# F-Score(id) = 0.491, Recall: 0.868, Precision: 0.342
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_245 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_246 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_245 (Embedding)       (None, 4, 48)        705264      input_245[0][0]                  
__________________________________________________________________________________________________
embedding_246 (Embedding)       (None, 4, 24)        5640        input_246[0][0]                  
__________________________________________________________________________________________________
flatten_245 (Flatten)           (None, 192)          0           embedding_245[0][0]              
__________________________________________________________________________________________________
flatten_246 (Flatten)           (None, 96)           0           embedding_246[0][0]              
__________________________________________________________________________________________________
concatenate_123 (Concatenate)   (None, 288)          0           flatten_245[0][0]                
                                                                 flatten_246[0][0]                
__________________________________________________________________________________________________
dense_245 (Dense)               (None, 24)           6936        concatenate_123[0][0]            
__________________________________________________________________________________________________
dropout_123 (Dropout)           (None, 24)           0           dense_245[0][0]                  
__________________________________________________________________________________________________
dense_246 (Dense)               (None, 8)            200         dropout_123[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0958 - acc: 0.9622 - val_loss: 0.0800 - val_acc: 0.9663
Epoch 2/40
 - 5s - loss: 0.0684 - acc: 0.9690 - val_loss: 0.0711 - val_acc: 0.9675
Epoch 3/40
 - 5s - loss: 0.0626 - acc: 0.9708 - val_loss: 0.0785 - val_acc: 0.9671
Epoch 4/40
 - 5s - loss: 0.0601 - acc: 0.9718 - val_loss: 0.0775 - val_acc: 0.9681
Epoch 5/40
 - 5s - loss: 0.0581 - acc: 0.9721 - val_loss: 0.0809 - val_acc: 0.9673
Epoch 6/40
 - 5s - loss: 0.0575 - acc: 0.9728 - val_loss: 0.0856 - val_acc: 0.9685
Epoch 7/40
 - 5s - loss: 0.0559 - acc: 0.9734 - val_loss: 0.0795 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0552 - acc: 0.9736 - val_loss: 0.0881 - val_acc: 0.9683
Epoch 9/40
 - 5s - loss: 0.0547 - acc: 0.9738 - val_loss: 0.0932 - val_acc: 0.9625
Epoch 10/40
 - 5s - loss: 0.0548 - acc: 0.9743 - val_loss: 0.0848 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0539 - acc: 0.9745 - val_loss: 0.0898 - val_acc: 0.9651
Epoch 12/40
 - 5s - loss: 0.0536 - acc: 0.9742 - val_loss: 0.0838 - val_acc: 0.9688
Epoch 13/40
 - 5s - loss: 0.0529 - acc: 0.9745 - val_loss: 0.0877 - val_acc: 0.9677
Epoch 14/40
 - 5s - loss: 0.0537 - acc: 0.9748 - val_loss: 0.1052 - val_acc: 0.9673
Epoch 15/40
 - 5s - loss: 0.0532 - acc: 0.9748 - val_loss: 0.0919 - val_acc: 0.9686
Epoch 16/40
 - 5s - loss: 0.0534 - acc: 0.9748 - val_loss: 0.0841 - val_acc: 0.9684
Epoch 17/40
 - 5s - loss: 0.0532 - acc: 0.9748 - val_loss: 0.0962 - val_acc: 0.9673
Epoch 18/40
 - 5s - loss: 0.0528 - acc: 0.9751 - val_loss: 0.1096 - val_acc: 0.9673
Epoch 19/40
 - 5s - loss: 0.0526 - acc: 0.9755 - val_loss: 0.0964 - val_acc: 0.9684
Epoch 20/40
 - 5s - loss: 0.0520 - acc: 0.9751 - val_loss: 0.1021 - val_acc: 0.9660
Epoch 21/40
 - 5s - loss: 0.0530 - acc: 0.9751 - val_loss: 0.0994 - val_acc: 0.9665
Epoch 22/40
 - 5s - loss: 0.0522 - acc: 0.9752 - val_loss: 0.1078 - val_acc: 0.9664
Epoch 23/40
 - 5s - loss: 0.0542 - acc: 0.9760 - val_loss: 0.1017 - val_acc: 0.9662
Epoch 24/40
 - 5s - loss: 0.0529 - acc: 0.9756 - val_loss: 0.1024 - val_acc: 0.9679
Epoch 25/40
 - 5s - loss: 0.0524 - acc: 0.9753 - val_loss: 0.1117 - val_acc: 0.9677
Epoch 26/40
 - 5s - loss: 0.0536 - acc: 0.9753 - val_loss: 0.1166 - val_acc: 0.9654
Epoch 27/40
 - 5s - loss: 0.0517 - acc: 0.9755 - val_loss: 0.1119 - val_acc: 0.9678
Epoch 28/40
 - 5s - loss: 0.0523 - acc: 0.9758 - val_loss: 0.1010 - val_acc: 0.9673
Epoch 29/40
 - 5s - loss: 0.0526 - acc: 0.9758 - val_loss: 0.1109 - val_acc: 0.9663
Epoch 30/40
 - 5s - loss: 0.0529 - acc: 0.9759 - val_loss: 0.1340 - val_acc: 0.9653
Epoch 31/40
 - 5s - loss: 0.0520 - acc: 0.9760 - val_loss: 0.1132 - val_acc: 0.9675
Epoch 32/40
 - 5s - loss: 0.0519 - acc: 0.9764 - val_loss: 0.1167 - val_acc: 0.9677
Epoch 33/40
 - 5s - loss: 0.0517 - acc: 0.9763 - val_loss: 0.1107 - val_acc: 0.9694
Epoch 34/40
 - 5s - loss: 0.0511 - acc: 0.9762 - val_loss: 0.1164 - val_acc: 0.9644
Epoch 35/40
 - 5s - loss: 0.0511 - acc: 0.9762 - val_loss: 0.1271 - val_acc: 0.9681
Epoch 36/40
 - 5s - loss: 0.0518 - acc: 0.9763 - val_loss: 0.1304 - val_acc: 0.9662
Epoch 37/40
 - 5s - loss: 0.0515 - acc: 0.9765 - val_loss: 0.1156 - val_acc: 0.9638
Epoch 38/40
 - 5s - loss: 0.0513 - acc: 0.9762 - val_loss: 0.1234 - val_acc: 0.9678
Epoch 39/40
 - 5s - loss: 0.0513 - acc: 0.9765 - val_loss: 0.1415 - val_acc: 0.9664
Epoch 40/40
 - 5s - loss: 0.0513 - acc: 0.9764 - val_loss: 0.1217 - val_acc: 0.9671
Epoch 1/40
 - 1s - loss: 0.1129 - acc: 0.9655
Epoch 2/40
 - 1s - loss: 0.1008 - acc: 0.9658
Epoch 3/40
 - 1s - loss: 0.0911 - acc: 0.9680
Epoch 4/40
 - 1s - loss: 0.0855 - acc: 0.9697
Epoch 5/40
 - 1s - loss: 0.0801 - acc: 0.9704
Epoch 6/40
 - 1s - loss: 0.0783 - acc: 0.9710
Epoch 7/40
 - 1s - loss: 0.0735 - acc: 0.9715
Epoch 8/40
 - 1s - loss: 0.0741 - acc: 0.9714
Epoch 9/40
 - 1s - loss: 0.0692 - acc: 0.9727
Epoch 10/40
 - 1s - loss: 0.0656 - acc: 0.9737
Epoch 11/40
 - 1s - loss: 0.0645 - acc: 0.9733
Epoch 12/40
 - 1s - loss: 0.0626 - acc: 0.9733
Epoch 13/40
 - 1s - loss: 0.0639 - acc: 0.9746
Epoch 14/40
 - 1s - loss: 0.0607 - acc: 0.9748
Epoch 15/40
 - 1s - loss: 0.0630 - acc: 0.9735
Epoch 16/40
 - 1s - loss: 0.0613 - acc: 0.9747
Epoch 17/40
 - 1s - loss: 0.0581 - acc: 0.9766
Epoch 18/40
 - 1s - loss: 0.0592 - acc: 0.9760
Epoch 19/40
 - 1s - loss: 0.0585 - acc: 0.9748
Epoch 20/40
 - 1s - loss: 0.0581 - acc: 0.9755
Epoch 21/40
 - 1s - loss: 0.0558 - acc: 0.9763
Epoch 22/40
 - 1s - loss: 0.0559 - acc: 0.9777
Epoch 23/40
 - 1s - loss: 0.0537 - acc: 0.9762
Epoch 24/40
 - 1s - loss: 0.0544 - acc: 0.9774
Epoch 25/40
 - 1s - loss: 0.0569 - acc: 0.9766
Epoch 26/40
 - 1s - loss: 0.0542 - acc: 0.9775
Epoch 27/40
 - 1s - loss: 0.0525 - acc: 0.9770
Epoch 28/40
 - 1s - loss: 0.0531 - acc: 0.9774
Epoch 29/40
 - 1s - loss: 0.0529 - acc: 0.9768
Epoch 30/40
 - 1s - loss: 0.0512 - acc: 0.9770
Epoch 31/40
 - 1s - loss: 0.0536 - acc: 0.9765
Epoch 32/40
 - 1s - loss: 0.0531 - acc: 0.9769
Epoch 33/40
 - 1s - loss: 0.0532 - acc: 0.9776
Epoch 34/40
 - 1s - loss: 0.0527 - acc: 0.9777
Epoch 35/40
 - 1s - loss: 0.0510 - acc: 0.9783
Epoch 36/40
 - 1s - loss: 0.0500 - acc: 0.9773
Epoch 37/40
 - 1s - loss: 0.0512 - acc: 0.9784
Epoch 38/40
 - 1s - loss: 0.0529 - acc: 0.9779
Epoch 39/40
 - 1s - loss: 0.0538 - acc: 0.9770
Epoch 40/40
 - 1s - loss: 0.0509 - acc: 0.9786
# Training time = 0:04:08.735420
# F-Score(Ordinary) = 0.319, Recall: 0.409, Precision: 0.262
# F-Score(lvc) = 0.245, Recall: 0.826, Precision: 0.144
# F-Score(id) = 0.404, Recall: 0.35, Precision: 0.477
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_247 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_248 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_247 (Embedding)       (None, 4, 48)        705264      input_247[0][0]                  
__________________________________________________________________________________________________
embedding_248 (Embedding)       (None, 4, 24)        5640        input_248[0][0]                  
__________________________________________________________________________________________________
flatten_247 (Flatten)           (None, 192)          0           embedding_247[0][0]              
__________________________________________________________________________________________________
flatten_248 (Flatten)           (None, 96)           0           embedding_248[0][0]              
__________________________________________________________________________________________________
concatenate_124 (Concatenate)   (None, 288)          0           flatten_247[0][0]                
                                                                 flatten_248[0][0]                
__________________________________________________________________________________________________
dense_247 (Dense)               (None, 24)           6936        concatenate_124[0][0]            
__________________________________________________________________________________________________
dropout_124 (Dropout)           (None, 24)           0           dense_247[0][0]                  
__________________________________________________________________________________________________
dense_248 (Dense)               (None, 8)            200         dropout_124[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0977 - acc: 0.9617 - val_loss: 0.0752 - val_acc: 0.9656
Epoch 2/40
 - 5s - loss: 0.0682 - acc: 0.9691 - val_loss: 0.0751 - val_acc: 0.9655
Epoch 3/40
 - 5s - loss: 0.0627 - acc: 0.9707 - val_loss: 0.0737 - val_acc: 0.9679
Epoch 4/40
 - 5s - loss: 0.0596 - acc: 0.9718 - val_loss: 0.0754 - val_acc: 0.9678
Epoch 5/40
 - 5s - loss: 0.0577 - acc: 0.9728 - val_loss: 0.0828 - val_acc: 0.9671
Epoch 6/40
 - 5s - loss: 0.0572 - acc: 0.9731 - val_loss: 0.0775 - val_acc: 0.9682
Epoch 7/40
 - 5s - loss: 0.0558 - acc: 0.9734 - val_loss: 0.0806 - val_acc: 0.9671
Epoch 8/40
 - 5s - loss: 0.0545 - acc: 0.9738 - val_loss: 0.0917 - val_acc: 0.9662
Epoch 9/40
 - 5s - loss: 0.0549 - acc: 0.9738 - val_loss: 0.0887 - val_acc: 0.9682
Epoch 10/40
 - 5s - loss: 0.0554 - acc: 0.9741 - val_loss: 0.0898 - val_acc: 0.9676
Epoch 11/40
 - 5s - loss: 0.0550 - acc: 0.9742 - val_loss: 0.0836 - val_acc: 0.9682
Epoch 12/40
 - 5s - loss: 0.0555 - acc: 0.9746 - val_loss: 0.0936 - val_acc: 0.9680
Epoch 13/40
 - 5s - loss: 0.0545 - acc: 0.9745 - val_loss: 0.0906 - val_acc: 0.9673
Epoch 14/40
 - 5s - loss: 0.0543 - acc: 0.9747 - val_loss: 0.0896 - val_acc: 0.9685
Epoch 15/40
 - 5s - loss: 0.0545 - acc: 0.9744 - val_loss: 0.0987 - val_acc: 0.9676
Epoch 16/40
 - 5s - loss: 0.0549 - acc: 0.9750 - val_loss: 0.0997 - val_acc: 0.9679
Epoch 17/40
 - 5s - loss: 0.0548 - acc: 0.9746 - val_loss: 0.0909 - val_acc: 0.9681
Epoch 18/40
 - 5s - loss: 0.0538 - acc: 0.9755 - val_loss: 0.1007 - val_acc: 0.9681
Epoch 19/40
 - 5s - loss: 0.0539 - acc: 0.9752 - val_loss: 0.1032 - val_acc: 0.9674
Epoch 20/40
 - 5s - loss: 0.0529 - acc: 0.9751 - val_loss: 0.0938 - val_acc: 0.9687
Epoch 21/40
 - 5s - loss: 0.0545 - acc: 0.9752 - val_loss: 0.1059 - val_acc: 0.9682
Epoch 22/40
 - 5s - loss: 0.0545 - acc: 0.9751 - val_loss: 0.1020 - val_acc: 0.9676
Epoch 23/40
 - 5s - loss: 0.0537 - acc: 0.9750 - val_loss: 0.1068 - val_acc: 0.9677
Epoch 24/40
 - 5s - loss: 0.0535 - acc: 0.9754 - val_loss: 0.1329 - val_acc: 0.9681
Epoch 25/40
 - 5s - loss: 0.0532 - acc: 0.9751 - val_loss: 0.1161 - val_acc: 0.9671
Epoch 26/40
 - 5s - loss: 0.0530 - acc: 0.9758 - val_loss: 0.1025 - val_acc: 0.9692
Epoch 27/40
 - 5s - loss: 0.0534 - acc: 0.9754 - val_loss: 0.1132 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0530 - acc: 0.9757 - val_loss: 0.1464 - val_acc: 0.9672
Epoch 29/40
 - 5s - loss: 0.0551 - acc: 0.9754 - val_loss: 0.1042 - val_acc: 0.9684
Epoch 30/40
 - 5s - loss: 0.0525 - acc: 0.9758 - val_loss: 0.1022 - val_acc: 0.9650
Epoch 31/40
 - 5s - loss: 0.0533 - acc: 0.9760 - val_loss: 0.1182 - val_acc: 0.9674
Epoch 32/40
 - 5s - loss: 0.0524 - acc: 0.9756 - val_loss: 0.1128 - val_acc: 0.9668
Epoch 33/40
 - 5s - loss: 0.0526 - acc: 0.9757 - val_loss: 0.1211 - val_acc: 0.9683
Epoch 34/40
 - 5s - loss: 0.0540 - acc: 0.9759 - val_loss: 0.1168 - val_acc: 0.9682
Epoch 35/40
 - 5s - loss: 0.0525 - acc: 0.9761 - val_loss: 0.1147 - val_acc: 0.9683
Epoch 36/40
 - 5s - loss: 0.0526 - acc: 0.9759 - val_loss: 0.1325 - val_acc: 0.9670
Epoch 37/40
 - 5s - loss: 0.0530 - acc: 0.9761 - val_loss: 0.1213 - val_acc: 0.9675
Epoch 38/40
 - 5s - loss: 0.0512 - acc: 0.9761 - val_loss: 0.1095 - val_acc: 0.9689
Epoch 39/40
 - 5s - loss: 0.0519 - acc: 0.9765 - val_loss: 0.1100 - val_acc: 0.9681
Epoch 40/40
 - 5s - loss: 0.0524 - acc: 0.9760 - val_loss: 0.1310 - val_acc: 0.9683
Epoch 1/40
 - 1s - loss: 0.1108 - acc: 0.9662
Epoch 2/40
 - 1s - loss: 0.0949 - acc: 0.9667
Epoch 3/40
 - 1s - loss: 0.0855 - acc: 0.9682
Epoch 4/40
 - 1s - loss: 0.0821 - acc: 0.9684
Epoch 5/40
 - 1s - loss: 0.0770 - acc: 0.9686
Epoch 6/40
 - 1s - loss: 0.0767 - acc: 0.9718
Epoch 7/40
 - 1s - loss: 0.0707 - acc: 0.9711
Epoch 8/40
 - 1s - loss: 0.0704 - acc: 0.9715
Epoch 9/40
 - 1s - loss: 0.0665 - acc: 0.9721
Epoch 10/40
 - 1s - loss: 0.0670 - acc: 0.9728
Epoch 11/40
 - 1s - loss: 0.0654 - acc: 0.9742
Epoch 12/40
 - 1s - loss: 0.0608 - acc: 0.9743
Epoch 13/40
 - 1s - loss: 0.0623 - acc: 0.9745
Epoch 14/40
 - 1s - loss: 0.0594 - acc: 0.9753
Epoch 15/40
 - 1s - loss: 0.0563 - acc: 0.9756
Epoch 16/40
 - 1s - loss: 0.0553 - acc: 0.9753
Epoch 17/40
 - 1s - loss: 0.0556 - acc: 0.9750
Epoch 18/40
 - 1s - loss: 0.0556 - acc: 0.9766
Epoch 19/40
 - 1s - loss: 0.0556 - acc: 0.9764
Epoch 20/40
 - 1s - loss: 0.0542 - acc: 0.9761
Epoch 21/40
 - 1s - loss: 0.0527 - acc: 0.9767
Epoch 22/40
 - 1s - loss: 0.0522 - acc: 0.9768
Epoch 23/40
 - 1s - loss: 0.0516 - acc: 0.9769
Epoch 24/40
 - 1s - loss: 0.0515 - acc: 0.9761
Epoch 25/40
 - 1s - loss: 0.0500 - acc: 0.9768
Epoch 26/40
 - 1s - loss: 0.0521 - acc: 0.9768
Epoch 27/40
 - 1s - loss: 0.0516 - acc: 0.9773
Epoch 28/40
 - 1s - loss: 0.0574 - acc: 0.9766
Epoch 29/40
 - 1s - loss: 0.0517 - acc: 0.9770
Epoch 30/40
 - 1s - loss: 0.0502 - acc: 0.9770
Epoch 31/40
 - 1s - loss: 0.0510 - acc: 0.9774
Epoch 32/40
 - 1s - loss: 0.0511 - acc: 0.9767
Epoch 33/40
 - 1s - loss: 0.0523 - acc: 0.9778
Epoch 34/40
 - 1s - loss: 0.0495 - acc: 0.9787
Epoch 35/40
 - 1s - loss: 0.0491 - acc: 0.9780
Epoch 36/40
 - 1s - loss: 0.0473 - acc: 0.9783
Epoch 37/40
 - 1s - loss: 0.0496 - acc: 0.9776
Epoch 38/40
 - 1s - loss: 0.0484 - acc: 0.9767
Epoch 39/40
 - 1s - loss: 0.0486 - acc: 0.9773
Epoch 40/40
 - 1s - loss: 0.0471 - acc: 0.9770
# Training time = 0:04:08.611037
# F-Score(Ordinary) = 0.19, Recall: 0.842, Precision: 0.107
# F-Score(lvc) = 0.492, Recall: 0.836, Precision: 0.348
# F-Score(id) = 0.021, Recall: 1.0, Precision: 0.01
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_249 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_250 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_249 (Embedding)       (None, 4, 48)        705264      input_249[0][0]                  
__________________________________________________________________________________________________
embedding_250 (Embedding)       (None, 4, 24)        5640        input_250[0][0]                  
__________________________________________________________________________________________________
flatten_249 (Flatten)           (None, 192)          0           embedding_249[0][0]              
__________________________________________________________________________________________________
flatten_250 (Flatten)           (None, 96)           0           embedding_250[0][0]              
__________________________________________________________________________________________________
concatenate_125 (Concatenate)   (None, 288)          0           flatten_249[0][0]                
                                                                 flatten_250[0][0]                
__________________________________________________________________________________________________
dense_249 (Dense)               (None, 24)           6936        concatenate_125[0][0]            
__________________________________________________________________________________________________
dropout_125 (Dropout)           (None, 24)           0           dense_249[0][0]                  
__________________________________________________________________________________________________
dense_250 (Dense)               (None, 8)            200         dropout_125[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adadelta, learning rate = 5
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0971 - acc: 0.9617 - val_loss: 0.0778 - val_acc: 0.9666
Epoch 2/40
 - 5s - loss: 0.0686 - acc: 0.9692 - val_loss: 0.0748 - val_acc: 0.9681
Epoch 3/40
 - 5s - loss: 0.0625 - acc: 0.9706 - val_loss: 0.0758 - val_acc: 0.9677
Epoch 4/40
 - 5s - loss: 0.0601 - acc: 0.9714 - val_loss: 0.0767 - val_acc: 0.9667
Epoch 5/40
 - 5s - loss: 0.0577 - acc: 0.9725 - val_loss: 0.0768 - val_acc: 0.9666
Epoch 6/40
 - 5s - loss: 0.0563 - acc: 0.9726 - val_loss: 0.0772 - val_acc: 0.9672
Epoch 7/40
 - 5s - loss: 0.0557 - acc: 0.9734 - val_loss: 0.0854 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0545 - acc: 0.9733 - val_loss: 0.0859 - val_acc: 0.9681
Epoch 9/40
 - 5s - loss: 0.0538 - acc: 0.9745 - val_loss: 0.0838 - val_acc: 0.9664
Epoch 10/40
 - 5s - loss: 0.0539 - acc: 0.9738 - val_loss: 0.0922 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0536 - acc: 0.9747 - val_loss: 0.0895 - val_acc: 0.9688
Epoch 12/40
 - 5s - loss: 0.0531 - acc: 0.9742 - val_loss: 0.0840 - val_acc: 0.9680
Epoch 13/40
 - 5s - loss: 0.0526 - acc: 0.9747 - val_loss: 0.0927 - val_acc: 0.9676
Epoch 14/40
 - 5s - loss: 0.0523 - acc: 0.9753 - val_loss: 0.0913 - val_acc: 0.9686
Epoch 15/40
 - 5s - loss: 0.0521 - acc: 0.9750 - val_loss: 0.0916 - val_acc: 0.9687
Epoch 16/40
 - 5s - loss: 0.0518 - acc: 0.9746 - val_loss: 0.0985 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0509 - acc: 0.9750 - val_loss: 0.1018 - val_acc: 0.9678
Epoch 18/40
 - 5s - loss: 0.0511 - acc: 0.9749 - val_loss: 0.0949 - val_acc: 0.9672
Epoch 19/40
 - 5s - loss: 0.0515 - acc: 0.9755 - val_loss: 0.0931 - val_acc: 0.9688
Epoch 20/40
 - 5s - loss: 0.0513 - acc: 0.9750 - val_loss: 0.1024 - val_acc: 0.9665
Epoch 21/40
 - 5s - loss: 0.0503 - acc: 0.9755 - val_loss: 0.1049 - val_acc: 0.9680
Epoch 22/40
 - 5s - loss: 0.0499 - acc: 0.9754 - val_loss: 0.1050 - val_acc: 0.9658
Epoch 23/40
 - 5s - loss: 0.0502 - acc: 0.9761 - val_loss: 0.0992 - val_acc: 0.9671
Epoch 24/40
 - 5s - loss: 0.0503 - acc: 0.9756 - val_loss: 0.1039 - val_acc: 0.9663
Epoch 25/40
 - 5s - loss: 0.0500 - acc: 0.9759 - val_loss: 0.0918 - val_acc: 0.9671
Epoch 26/40
 - 5s - loss: 0.0503 - acc: 0.9759 - val_loss: 0.1123 - val_acc: 0.9682
Epoch 27/40
 - 5s - loss: 0.0506 - acc: 0.9761 - val_loss: 0.1025 - val_acc: 0.9683
Epoch 28/40
 - 5s - loss: 0.0496 - acc: 0.9764 - val_loss: 0.1023 - val_acc: 0.9678
Epoch 29/40
 - 5s - loss: 0.0490 - acc: 0.9760 - val_loss: 0.1142 - val_acc: 0.9651
Epoch 30/40
 - 5s - loss: 0.0497 - acc: 0.9756 - val_loss: 0.1106 - val_acc: 0.9675
Epoch 31/40
 - 5s - loss: 0.0493 - acc: 0.9764 - val_loss: 0.1149 - val_acc: 0.9678
Epoch 32/40
 - 5s - loss: 0.0494 - acc: 0.9760 - val_loss: 0.0968 - val_acc: 0.9680
Epoch 33/40
 - 5s - loss: 0.0496 - acc: 0.9759 - val_loss: 0.1144 - val_acc: 0.9667
Epoch 34/40
 - 5s - loss: 0.0493 - acc: 0.9765 - val_loss: 0.1235 - val_acc: 0.9671
Epoch 35/40
 - 5s - loss: 0.0498 - acc: 0.9760 - val_loss: 0.1164 - val_acc: 0.9671
Epoch 36/40
 - 5s - loss: 0.0502 - acc: 0.9763 - val_loss: 0.1107 - val_acc: 0.9679
Epoch 37/40
 - 5s - loss: 0.0501 - acc: 0.9761 - val_loss: 0.1119 - val_acc: 0.9678
Epoch 38/40
 - 5s - loss: 0.0491 - acc: 0.9761 - val_loss: 0.1186 - val_acc: 0.9677
Epoch 39/40
 - 5s - loss: 0.0491 - acc: 0.9762 - val_loss: 0.1212 - val_acc: 0.9678
Epoch 40/40
 - 5s - loss: 0.0502 - acc: 0.9763 - val_loss: 0.1237 - val_acc: 0.9683
Epoch 1/40
 - 1s - loss: 0.1088 - acc: 0.9646
Epoch 2/40
 - 1s - loss: 0.0892 - acc: 0.9666
Epoch 3/40
 - 1s - loss: 0.0846 - acc: 0.9675
Epoch 4/40
 - 1s - loss: 0.0774 - acc: 0.9699
Epoch 5/40
 - 1s - loss: 0.0798 - acc: 0.9696
Epoch 6/40
 - 1s - loss: 0.0736 - acc: 0.9700
Epoch 7/40
 - 1s - loss: 0.0666 - acc: 0.9717
Epoch 8/40
 - 1s - loss: 0.0652 - acc: 0.9720
Epoch 9/40
 - 1s - loss: 0.0614 - acc: 0.9731
Epoch 10/40
 - 1s - loss: 0.0589 - acc: 0.9729
Epoch 11/40
 - 1s - loss: 0.0584 - acc: 0.9743
Epoch 12/40
 - 1s - loss: 0.0584 - acc: 0.9730
Epoch 13/40
 - 1s - loss: 0.0577 - acc: 0.9752
Epoch 14/40
 - 1s - loss: 0.0590 - acc: 0.9742
Epoch 15/40
 - 1s - loss: 0.0550 - acc: 0.9741
Epoch 16/40
 - 1s - loss: 0.0527 - acc: 0.9760
Epoch 17/40
 - 1s - loss: 0.0554 - acc: 0.9752
Epoch 18/40
 - 1s - loss: 0.0521 - acc: 0.9768
Epoch 19/40
 - 1s - loss: 0.0537 - acc: 0.9770
Epoch 20/40
 - 1s - loss: 0.0489 - acc: 0.9770
Epoch 21/40
 - 1s - loss: 0.0529 - acc: 0.9764
Epoch 22/40
 - 1s - loss: 0.0509 - acc: 0.9774
Epoch 23/40
 - 1s - loss: 0.0502 - acc: 0.9763
Epoch 24/40
 - 1s - loss: 0.0481 - acc: 0.9775
Epoch 25/40
 - 1s - loss: 0.0505 - acc: 0.9763
Epoch 26/40
 - 1s - loss: 0.0504 - acc: 0.9783
Epoch 27/40
 - 1s - loss: 0.0503 - acc: 0.9774
Epoch 28/40
 - 1s - loss: 0.0484 - acc: 0.9765
Epoch 29/40
 - 1s - loss: 0.0492 - acc: 0.9777
Epoch 30/40
 - 1s - loss: 0.0473 - acc: 0.9785
Epoch 31/40
 - 1s - loss: 0.0477 - acc: 0.9769
Epoch 32/40
 - 1s - loss: 0.0503 - acc: 0.9772
Epoch 33/40
 - 1s - loss: 0.0498 - acc: 0.9764
Epoch 34/40
 - 1s - loss: 0.0481 - acc: 0.9785
Epoch 35/40
 - 1s - loss: 0.0480 - acc: 0.9781
Epoch 36/40
 - 1s - loss: 0.0467 - acc: 0.9770
Epoch 37/40
 - 1s - loss: 0.0476 - acc: 0.9770
Epoch 38/40
 - 1s - loss: 0.0454 - acc: 0.9779
Epoch 39/40
 - 1s - loss: 0.0528 - acc: 0.9767
Epoch 40/40
 - 1s - loss: 0.0465 - acc: 0.9778
# Training time = 0:04:09.147716
# F-Score(Ordinary) = 0.465, Recall: 0.543, Precision: 0.407
# F-Score(lvc) = 0.473, Recall: 0.394, Precision: 0.591
# F-Score(ireflv) = 0.763, Recall: 0.726, Precision: 0.803
# F-Score(id) = 0.01, Recall: 0.5, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_251 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_252 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_251 (Embedding)       (None, 4, 48)        705264      input_251[0][0]                  
__________________________________________________________________________________________________
embedding_252 (Embedding)       (None, 4, 24)        5640        input_252[0][0]                  
__________________________________________________________________________________________________
flatten_251 (Flatten)           (None, 192)          0           embedding_251[0][0]              
__________________________________________________________________________________________________
flatten_252 (Flatten)           (None, 96)           0           embedding_252[0][0]              
__________________________________________________________________________________________________
concatenate_126 (Concatenate)   (None, 288)          0           flatten_251[0][0]                
                                                                 flatten_252[0][0]                
__________________________________________________________________________________________________
dense_251 (Dense)               (None, 24)           6936        concatenate_126[0][0]            
__________________________________________________________________________________________________
dropout_126 (Dropout)           (None, 24)           0           dense_251[0][0]                  
__________________________________________________________________________________________________
dense_252 (Dense)               (None, 8)            200         dropout_126[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1947 - acc: 0.9439 - val_loss: 0.0983 - val_acc: 0.9624
Epoch 2/40
 - 5s - loss: 0.0916 - acc: 0.9640 - val_loss: 0.0835 - val_acc: 0.9656
Epoch 3/40
 - 5s - loss: 0.0772 - acc: 0.9676 - val_loss: 0.0772 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0701 - acc: 0.9685 - val_loss: 0.0744 - val_acc: 0.9670
Epoch 5/40
 - 5s - loss: 0.0650 - acc: 0.9703 - val_loss: 0.0726 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0615 - acc: 0.9711 - val_loss: 0.0711 - val_acc: 0.9677
Epoch 7/40
 - 5s - loss: 0.0587 - acc: 0.9716 - val_loss: 0.0706 - val_acc: 0.9682
Epoch 8/40
 - 5s - loss: 0.0568 - acc: 0.9722 - val_loss: 0.0705 - val_acc: 0.9679
Epoch 9/40
 - 5s - loss: 0.0546 - acc: 0.9730 - val_loss: 0.0706 - val_acc: 0.9678
Epoch 10/40
 - 5s - loss: 0.0527 - acc: 0.9735 - val_loss: 0.0715 - val_acc: 0.9681
Epoch 11/40
 - 5s - loss: 0.0513 - acc: 0.9735 - val_loss: 0.0727 - val_acc: 0.9677
Epoch 12/40
 - 5s - loss: 0.0498 - acc: 0.9742 - val_loss: 0.0722 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0489 - acc: 0.9746 - val_loss: 0.0727 - val_acc: 0.9674
Epoch 14/40
 - 5s - loss: 0.0478 - acc: 0.9751 - val_loss: 0.0740 - val_acc: 0.9680
Epoch 15/40
 - 5s - loss: 0.0470 - acc: 0.9748 - val_loss: 0.0755 - val_acc: 0.9680
Epoch 16/40
 - 5s - loss: 0.0462 - acc: 0.9747 - val_loss: 0.0753 - val_acc: 0.9680
Epoch 17/40
 - 5s - loss: 0.0455 - acc: 0.9755 - val_loss: 0.0770 - val_acc: 0.9677
Epoch 18/40
 - 5s - loss: 0.0447 - acc: 0.9754 - val_loss: 0.0775 - val_acc: 0.9677
Epoch 19/40
 - 5s - loss: 0.0442 - acc: 0.9755 - val_loss: 0.0784 - val_acc: 0.9679
Epoch 20/40
 - 5s - loss: 0.0438 - acc: 0.9756 - val_loss: 0.0798 - val_acc: 0.9674
Epoch 21/40
 - 5s - loss: 0.0432 - acc: 0.9757 - val_loss: 0.0797 - val_acc: 0.9670
Epoch 22/40
 - 5s - loss: 0.0430 - acc: 0.9758 - val_loss: 0.0794 - val_acc: 0.9671
Epoch 23/40
 - 5s - loss: 0.0424 - acc: 0.9758 - val_loss: 0.0812 - val_acc: 0.9675
Epoch 24/40
 - 5s - loss: 0.0420 - acc: 0.9758 - val_loss: 0.0825 - val_acc: 0.9672
Epoch 25/40
 - 5s - loss: 0.0415 - acc: 0.9765 - val_loss: 0.0847 - val_acc: 0.9674
Epoch 26/40
 - 5s - loss: 0.0411 - acc: 0.9764 - val_loss: 0.0842 - val_acc: 0.9674
Epoch 27/40
 - 5s - loss: 0.0409 - acc: 0.9764 - val_loss: 0.0861 - val_acc: 0.9670
Epoch 28/40
 - 5s - loss: 0.0406 - acc: 0.9761 - val_loss: 0.0856 - val_acc: 0.9671
Epoch 29/40
 - 5s - loss: 0.0404 - acc: 0.9766 - val_loss: 0.0852 - val_acc: 0.9670
Epoch 30/40
 - 5s - loss: 0.0401 - acc: 0.9764 - val_loss: 0.0858 - val_acc: 0.9671
Epoch 31/40
 - 5s - loss: 0.0398 - acc: 0.9764 - val_loss: 0.0853 - val_acc: 0.9671
Epoch 32/40
 - 5s - loss: 0.0395 - acc: 0.9766 - val_loss: 0.0862 - val_acc: 0.9674
Epoch 33/40
 - 5s - loss: 0.0395 - acc: 0.9767 - val_loss: 0.0871 - val_acc: 0.9669
Epoch 34/40
 - 5s - loss: 0.0391 - acc: 0.9768 - val_loss: 0.0877 - val_acc: 0.9671
Epoch 35/40
 - 5s - loss: 0.0389 - acc: 0.9772 - val_loss: 0.0883 - val_acc: 0.9670
Epoch 36/40
 - 5s - loss: 0.0388 - acc: 0.9771 - val_loss: 0.0894 - val_acc: 0.9671
Epoch 37/40
 - 5s - loss: 0.0388 - acc: 0.9770 - val_loss: 0.0893 - val_acc: 0.9668
Epoch 38/40
 - 5s - loss: 0.0387 - acc: 0.9770 - val_loss: 0.0885 - val_acc: 0.9667
Epoch 39/40
 - 5s - loss: 0.0384 - acc: 0.9770 - val_loss: 0.0905 - val_acc: 0.9670
Epoch 40/40
 - 5s - loss: 0.0382 - acc: 0.9771 - val_loss: 0.0923 - val_acc: 0.9668
Epoch 1/40
 - 0s - loss: 0.0849 - acc: 0.9662
Epoch 2/40
 - 0s - loss: 0.0737 - acc: 0.9689
Epoch 3/40
 - 0s - loss: 0.0687 - acc: 0.9689
Epoch 4/40
 - 0s - loss: 0.0630 - acc: 0.9705
Epoch 5/40
 - 0s - loss: 0.0610 - acc: 0.9698
Epoch 6/40
 - 0s - loss: 0.0584 - acc: 0.9709
Epoch 7/40
 - 0s - loss: 0.0556 - acc: 0.9714
Epoch 8/40
 - 0s - loss: 0.0529 - acc: 0.9711
Epoch 9/40
 - 0s - loss: 0.0517 - acc: 0.9723
Epoch 10/40
 - 0s - loss: 0.0507 - acc: 0.9724
Epoch 11/40
 - 0s - loss: 0.0493 - acc: 0.9726
Epoch 12/40
 - 0s - loss: 0.0470 - acc: 0.9753
Epoch 13/40
 - 0s - loss: 0.0462 - acc: 0.9748
Epoch 14/40
 - 0s - loss: 0.0455 - acc: 0.9737
Epoch 15/40
 - 0s - loss: 0.0442 - acc: 0.9733
Epoch 16/40
 - 0s - loss: 0.0435 - acc: 0.9749
Epoch 17/40
 - 0s - loss: 0.0429 - acc: 0.9757
Epoch 18/40
 - 0s - loss: 0.0413 - acc: 0.9762
Epoch 19/40
 - 0s - loss: 0.0409 - acc: 0.9761
Epoch 20/40
 - 0s - loss: 0.0410 - acc: 0.9754
Epoch 21/40
 - 0s - loss: 0.0399 - acc: 0.9765
Epoch 22/40
 - 0s - loss: 0.0396 - acc: 0.9761
Epoch 23/40
 - 0s - loss: 0.0383 - acc: 0.9762
Epoch 24/40
 - 0s - loss: 0.0382 - acc: 0.9762
Epoch 25/40
 - 0s - loss: 0.0369 - acc: 0.9770
Epoch 26/40
 - 0s - loss: 0.0373 - acc: 0.9778
Epoch 27/40
 - 0s - loss: 0.0370 - acc: 0.9777
Epoch 28/40
 - 0s - loss: 0.0366 - acc: 0.9772
Epoch 29/40
 - 0s - loss: 0.0364 - acc: 0.9781
Epoch 30/40
 - 0s - loss: 0.0361 - acc: 0.9769
Epoch 31/40
 - 0s - loss: 0.0351 - acc: 0.9783
Epoch 32/40
 - 0s - loss: 0.0354 - acc: 0.9768
Epoch 33/40
 - 0s - loss: 0.0351 - acc: 0.9769
Epoch 34/40
 - 0s - loss: 0.0344 - acc: 0.9772
Epoch 35/40
 - 0s - loss: 0.0347 - acc: 0.9774
Epoch 36/40
 - 0s - loss: 0.0343 - acc: 0.9765
Epoch 37/40
 - 0s - loss: 0.0341 - acc: 0.9778
Epoch 38/40
 - 0s - loss: 0.0340 - acc: 0.9777
Epoch 39/40
 - 0s - loss: 0.0337 - acc: 0.9779
Epoch 40/40
 - 0s - loss: 0.0334 - acc: 0.9787
# Training time = 0:03:48.572764
# F-Score(Ordinary) = 0.123, Recall: 0.732, Precision: 0.067
# F-Score(lvc) = 0.113, Recall: 0.889, Precision: 0.061
# F-Score(ireflv) = 0.131, Recall: 0.6, Precision: 0.074
# F-Score(id) = 0.124, Recall: 0.765, Precision: 0.067
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_253 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_254 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_253 (Embedding)       (None, 4, 48)        705264      input_253[0][0]                  
__________________________________________________________________________________________________
embedding_254 (Embedding)       (None, 4, 24)        5640        input_254[0][0]                  
__________________________________________________________________________________________________
flatten_253 (Flatten)           (None, 192)          0           embedding_253[0][0]              
__________________________________________________________________________________________________
flatten_254 (Flatten)           (None, 96)           0           embedding_254[0][0]              
__________________________________________________________________________________________________
concatenate_127 (Concatenate)   (None, 288)          0           flatten_253[0][0]                
                                                                 flatten_254[0][0]                
__________________________________________________________________________________________________
dense_253 (Dense)               (None, 24)           6936        concatenate_127[0][0]            
__________________________________________________________________________________________________
dropout_127 (Dropout)           (None, 24)           0           dense_253[0][0]                  
__________________________________________________________________________________________________
dense_254 (Dense)               (None, 8)            200         dropout_127[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.2007 - acc: 0.9445 - val_loss: 0.0985 - val_acc: 0.9641
Epoch 2/40
 - 5s - loss: 0.0945 - acc: 0.9643 - val_loss: 0.0847 - val_acc: 0.9670
Epoch 3/40
 - 5s - loss: 0.0794 - acc: 0.9673 - val_loss: 0.0768 - val_acc: 0.9677
Epoch 4/40
 - 5s - loss: 0.0710 - acc: 0.9688 - val_loss: 0.0727 - val_acc: 0.9679
Epoch 5/40
 - 5s - loss: 0.0654 - acc: 0.9699 - val_loss: 0.0708 - val_acc: 0.9683
Epoch 6/40
 - 5s - loss: 0.0614 - acc: 0.9707 - val_loss: 0.0692 - val_acc: 0.9682
Epoch 7/40
 - 5s - loss: 0.0586 - acc: 0.9719 - val_loss: 0.0689 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0560 - acc: 0.9726 - val_loss: 0.0689 - val_acc: 0.9683
Epoch 9/40
 - 5s - loss: 0.0542 - acc: 0.9729 - val_loss: 0.0691 - val_acc: 0.9679
Epoch 10/40
 - 5s - loss: 0.0528 - acc: 0.9738 - val_loss: 0.0698 - val_acc: 0.9687
Epoch 11/40
 - 5s - loss: 0.0513 - acc: 0.9739 - val_loss: 0.0704 - val_acc: 0.9682
Epoch 12/40
 - 5s - loss: 0.0503 - acc: 0.9740 - val_loss: 0.0724 - val_acc: 0.9675
Epoch 13/40
 - 5s - loss: 0.0490 - acc: 0.9747 - val_loss: 0.0710 - val_acc: 0.9683
Epoch 14/40
 - 5s - loss: 0.0481 - acc: 0.9747 - val_loss: 0.0735 - val_acc: 0.9677
Epoch 15/40
 - 5s - loss: 0.0472 - acc: 0.9751 - val_loss: 0.0722 - val_acc: 0.9682
Epoch 16/40
 - 5s - loss: 0.0464 - acc: 0.9747 - val_loss: 0.0736 - val_acc: 0.9683
Epoch 17/40
 - 5s - loss: 0.0457 - acc: 0.9750 - val_loss: 0.0726 - val_acc: 0.9681
Epoch 18/40
 - 5s - loss: 0.0451 - acc: 0.9758 - val_loss: 0.0751 - val_acc: 0.9682
Epoch 19/40
 - 5s - loss: 0.0445 - acc: 0.9755 - val_loss: 0.0759 - val_acc: 0.9673
Epoch 20/40
 - 5s - loss: 0.0439 - acc: 0.9754 - val_loss: 0.0762 - val_acc: 0.9679
Epoch 21/40
 - 5s - loss: 0.0435 - acc: 0.9758 - val_loss: 0.0768 - val_acc: 0.9673
Epoch 22/40
 - 5s - loss: 0.0430 - acc: 0.9761 - val_loss: 0.0780 - val_acc: 0.9670
Epoch 23/40
 - 5s - loss: 0.0429 - acc: 0.9761 - val_loss: 0.0781 - val_acc: 0.9677
Epoch 24/40
 - 5s - loss: 0.0424 - acc: 0.9763 - val_loss: 0.0807 - val_acc: 0.9670
Epoch 25/40
 - 5s - loss: 0.0420 - acc: 0.9759 - val_loss: 0.0812 - val_acc: 0.9671
Epoch 26/40
 - 5s - loss: 0.0414 - acc: 0.9764 - val_loss: 0.0816 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0414 - acc: 0.9767 - val_loss: 0.0805 - val_acc: 0.9675
Epoch 28/40
 - 5s - loss: 0.0411 - acc: 0.9766 - val_loss: 0.0816 - val_acc: 0.9675
Epoch 29/40
 - 5s - loss: 0.0409 - acc: 0.9765 - val_loss: 0.0833 - val_acc: 0.9673
Epoch 30/40
 - 5s - loss: 0.0405 - acc: 0.9770 - val_loss: 0.0834 - val_acc: 0.9670
Epoch 31/40
 - 5s - loss: 0.0403 - acc: 0.9769 - val_loss: 0.0833 - val_acc: 0.9671
Epoch 32/40
 - 5s - loss: 0.0403 - acc: 0.9771 - val_loss: 0.0833 - val_acc: 0.9669
Epoch 33/40
 - 5s - loss: 0.0400 - acc: 0.9770 - val_loss: 0.0869 - val_acc: 0.9675
Epoch 34/40
 - 5s - loss: 0.0397 - acc: 0.9769 - val_loss: 0.0843 - val_acc: 0.9673
Epoch 35/40
 - 5s - loss: 0.0396 - acc: 0.9768 - val_loss: 0.0837 - val_acc: 0.9668
Epoch 36/40
 - 5s - loss: 0.0393 - acc: 0.9770 - val_loss: 0.0869 - val_acc: 0.9667
Epoch 37/40
 - 5s - loss: 0.0391 - acc: 0.9772 - val_loss: 0.0866 - val_acc: 0.9668
Epoch 38/40
 - 5s - loss: 0.0390 - acc: 0.9770 - val_loss: 0.0881 - val_acc: 0.9668
Epoch 39/40
 - 5s - loss: 0.0388 - acc: 0.9770 - val_loss: 0.0871 - val_acc: 0.9668
Epoch 40/40
 - 5s - loss: 0.0387 - acc: 0.9773 - val_loss: 0.0868 - val_acc: 0.9665
Epoch 1/40
 - 0s - loss: 0.0810 - acc: 0.9663
Epoch 2/40
 - 0s - loss: 0.0717 - acc: 0.9665
Epoch 3/40
 - 0s - loss: 0.0660 - acc: 0.9701
Epoch 4/40
 - 0s - loss: 0.0627 - acc: 0.9693
Epoch 5/40
 - 0s - loss: 0.0603 - acc: 0.9707
Epoch 6/40
 - 0s - loss: 0.0573 - acc: 0.9725
Epoch 7/40
 - 0s - loss: 0.0551 - acc: 0.9719
Epoch 8/40
 - 0s - loss: 0.0531 - acc: 0.9734
Epoch 9/40
 - 0s - loss: 0.0509 - acc: 0.9724
Epoch 10/40
 - 0s - loss: 0.0504 - acc: 0.9738
Epoch 11/40
 - 0s - loss: 0.0490 - acc: 0.9742
Epoch 12/40
 - 0s - loss: 0.0472 - acc: 0.9760
Epoch 13/40
 - 0s - loss: 0.0458 - acc: 0.9745
Epoch 14/40
 - 0s - loss: 0.0452 - acc: 0.9752
Epoch 15/40
 - 0s - loss: 0.0445 - acc: 0.9753
Epoch 16/40
 - 0s - loss: 0.0434 - acc: 0.9758
Epoch 17/40
 - 0s - loss: 0.0428 - acc: 0.9762
Epoch 18/40
 - 0s - loss: 0.0420 - acc: 0.9759
Epoch 19/40
 - 0s - loss: 0.0412 - acc: 0.9765
Epoch 20/40
 - 0s - loss: 0.0413 - acc: 0.9761
Epoch 21/40
 - 0s - loss: 0.0403 - acc: 0.9771
Epoch 22/40
 - 0s - loss: 0.0399 - acc: 0.9763
Epoch 23/40
 - 0s - loss: 0.0392 - acc: 0.9779
Epoch 24/40
 - 0s - loss: 0.0393 - acc: 0.9775
Epoch 25/40
 - 0s - loss: 0.0386 - acc: 0.9763
Epoch 26/40
 - 0s - loss: 0.0380 - acc: 0.9776
Epoch 27/40
 - 0s - loss: 0.0379 - acc: 0.9775
Epoch 28/40
 - 0s - loss: 0.0373 - acc: 0.9783
Epoch 29/40
 - 0s - loss: 0.0374 - acc: 0.9783
Epoch 30/40
 - 0s - loss: 0.0370 - acc: 0.9778
Epoch 31/40
 - 0s - loss: 0.0363 - acc: 0.9781
Epoch 32/40
 - 0s - loss: 0.0362 - acc: 0.9778
Epoch 33/40
 - 0s - loss: 0.0366 - acc: 0.9778
Epoch 34/40
 - 0s - loss: 0.0363 - acc: 0.9776
Epoch 35/40
 - 0s - loss: 0.0361 - acc: 0.9782
Epoch 36/40
 - 0s - loss: 0.0358 - acc: 0.9779
Epoch 37/40
 - 0s - loss: 0.0358 - acc: 0.9772
Epoch 38/40
 - 0s - loss: 0.0346 - acc: 0.9780
Epoch 39/40
 - 0s - loss: 0.0355 - acc: 0.9780
Epoch 40/40
 - 0s - loss: 0.0352 - acc: 0.9786
# Training time = 0:03:46.362280
# F-Score(Ordinary) = 0.245, Recall: 0.493, Precision: 0.163
# F-Score(lvc) = 0.411, Recall: 0.412, Precision: 0.409
# F-Score(ireflv) = 0.031, Recall: 0.4, Precision: 0.016
# F-Score(id) = 0.117, Recall: 1.0, Precision: 0.062
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_255 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_256 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_255 (Embedding)       (None, 4, 48)        705264      input_255[0][0]                  
__________________________________________________________________________________________________
embedding_256 (Embedding)       (None, 4, 24)        5640        input_256[0][0]                  
__________________________________________________________________________________________________
flatten_255 (Flatten)           (None, 192)          0           embedding_255[0][0]              
__________________________________________________________________________________________________
flatten_256 (Flatten)           (None, 96)           0           embedding_256[0][0]              
__________________________________________________________________________________________________
concatenate_128 (Concatenate)   (None, 288)          0           flatten_255[0][0]                
                                                                 flatten_256[0][0]                
__________________________________________________________________________________________________
dense_255 (Dense)               (None, 24)           6936        concatenate_128[0][0]            
__________________________________________________________________________________________________
dropout_128 (Dropout)           (None, 24)           0           dense_255[0][0]                  
__________________________________________________________________________________________________
dense_256 (Dense)               (None, 8)            200         dropout_128[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1945 - acc: 0.9459 - val_loss: 0.0999 - val_acc: 0.9630
Epoch 2/40
 - 5s - loss: 0.0929 - acc: 0.9645 - val_loss: 0.0844 - val_acc: 0.9664
Epoch 3/40
 - 5s - loss: 0.0782 - acc: 0.9675 - val_loss: 0.0770 - val_acc: 0.9671
Epoch 4/40
 - 5s - loss: 0.0698 - acc: 0.9690 - val_loss: 0.0734 - val_acc: 0.9673
Epoch 5/40
 - 5s - loss: 0.0647 - acc: 0.9702 - val_loss: 0.0714 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0611 - acc: 0.9711 - val_loss: 0.0697 - val_acc: 0.9679
Epoch 7/40
 - 5s - loss: 0.0584 - acc: 0.9720 - val_loss: 0.0690 - val_acc: 0.9686
Epoch 8/40
 - 5s - loss: 0.0560 - acc: 0.9723 - val_loss: 0.0699 - val_acc: 0.9683
Epoch 9/40
 - 5s - loss: 0.0541 - acc: 0.9731 - val_loss: 0.0696 - val_acc: 0.9683
Epoch 10/40
 - 5s - loss: 0.0523 - acc: 0.9736 - val_loss: 0.0699 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0507 - acc: 0.9737 - val_loss: 0.0711 - val_acc: 0.9677
Epoch 12/40
 - 5s - loss: 0.0495 - acc: 0.9739 - val_loss: 0.0728 - val_acc: 0.9677
Epoch 13/40
 - 5s - loss: 0.0485 - acc: 0.9746 - val_loss: 0.0717 - val_acc: 0.9675
Epoch 14/40
 - 5s - loss: 0.0475 - acc: 0.9747 - val_loss: 0.0750 - val_acc: 0.9675
Epoch 15/40
 - 5s - loss: 0.0465 - acc: 0.9749 - val_loss: 0.0732 - val_acc: 0.9678
Epoch 16/40
 - 5s - loss: 0.0460 - acc: 0.9753 - val_loss: 0.0748 - val_acc: 0.9677
Epoch 17/40
 - 5s - loss: 0.0452 - acc: 0.9756 - val_loss: 0.0747 - val_acc: 0.9678
Epoch 18/40
 - 5s - loss: 0.0446 - acc: 0.9755 - val_loss: 0.0746 - val_acc: 0.9676
Epoch 19/40
 - 5s - loss: 0.0440 - acc: 0.9755 - val_loss: 0.0771 - val_acc: 0.9671
Epoch 20/40
 - 5s - loss: 0.0431 - acc: 0.9758 - val_loss: 0.0776 - val_acc: 0.9676
Epoch 21/40
 - 5s - loss: 0.0431 - acc: 0.9760 - val_loss: 0.0781 - val_acc: 0.9671
Epoch 22/40
 - 5s - loss: 0.0422 - acc: 0.9762 - val_loss: 0.0798 - val_acc: 0.9669
Epoch 23/40
 - 5s - loss: 0.0420 - acc: 0.9760 - val_loss: 0.0803 - val_acc: 0.9668
Epoch 24/40
 - 5s - loss: 0.0415 - acc: 0.9766 - val_loss: 0.0808 - val_acc: 0.9675
Epoch 25/40
 - 5s - loss: 0.0414 - acc: 0.9763 - val_loss: 0.0823 - val_acc: 0.9666
Epoch 26/40
 - 5s - loss: 0.0410 - acc: 0.9762 - val_loss: 0.0806 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0405 - acc: 0.9764 - val_loss: 0.0827 - val_acc: 0.9671
Epoch 28/40
 - 5s - loss: 0.0404 - acc: 0.9766 - val_loss: 0.0804 - val_acc: 0.9668
Epoch 29/40
 - 5s - loss: 0.0400 - acc: 0.9769 - val_loss: 0.0825 - val_acc: 0.9671
Epoch 30/40
 - 5s - loss: 0.0401 - acc: 0.9767 - val_loss: 0.0833 - val_acc: 0.9671
Epoch 31/40
 - 5s - loss: 0.0394 - acc: 0.9767 - val_loss: 0.0857 - val_acc: 0.9669
Epoch 32/40
 - 5s - loss: 0.0391 - acc: 0.9771 - val_loss: 0.0863 - val_acc: 0.9669
Epoch 33/40
 - 5s - loss: 0.0391 - acc: 0.9766 - val_loss: 0.0873 - val_acc: 0.9672
Epoch 34/40
 - 5s - loss: 0.0390 - acc: 0.9771 - val_loss: 0.0853 - val_acc: 0.9668
Epoch 35/40
 - 5s - loss: 0.0386 - acc: 0.9771 - val_loss: 0.0866 - val_acc: 0.9673
Epoch 36/40
 - 5s - loss: 0.0385 - acc: 0.9769 - val_loss: 0.0855 - val_acc: 0.9677
Epoch 37/40
 - 5s - loss: 0.0382 - acc: 0.9774 - val_loss: 0.0867 - val_acc: 0.9672
Epoch 38/40
 - 5s - loss: 0.0382 - acc: 0.9770 - val_loss: 0.0895 - val_acc: 0.9674
Epoch 39/40
 - 5s - loss: 0.0380 - acc: 0.9772 - val_loss: 0.0884 - val_acc: 0.9666
Epoch 40/40
 - 5s - loss: 0.0381 - acc: 0.9773 - val_loss: 0.0889 - val_acc: 0.9670
Epoch 1/40
 - 0s - loss: 0.0827 - acc: 0.9668
Epoch 2/40
 - 0s - loss: 0.0741 - acc: 0.9663
Epoch 3/40
 - 0s - loss: 0.0679 - acc: 0.9695
Epoch 4/40
 - 0s - loss: 0.0644 - acc: 0.9695
Epoch 5/40
 - 0s - loss: 0.0594 - acc: 0.9701
Epoch 6/40
 - 0s - loss: 0.0573 - acc: 0.9706
Epoch 7/40
 - 0s - loss: 0.0543 - acc: 0.9721
Epoch 8/40
 - 0s - loss: 0.0523 - acc: 0.9732
Epoch 9/40
 - 0s - loss: 0.0511 - acc: 0.9737
Epoch 10/40
 - 0s - loss: 0.0483 - acc: 0.9750
Epoch 11/40
 - 0s - loss: 0.0477 - acc: 0.9743
Epoch 12/40
 - 0s - loss: 0.0457 - acc: 0.9746
Epoch 13/40
 - 0s - loss: 0.0449 - acc: 0.9748
Epoch 14/40
 - 0s - loss: 0.0439 - acc: 0.9760
Epoch 15/40
 - 0s - loss: 0.0433 - acc: 0.9745
Epoch 16/40
 - 0s - loss: 0.0425 - acc: 0.9761
Epoch 17/40
 - 0s - loss: 0.0410 - acc: 0.9759
Epoch 18/40
 - 0s - loss: 0.0401 - acc: 0.9766
Epoch 19/40
 - 0s - loss: 0.0408 - acc: 0.9745
Epoch 20/40
 - 0s - loss: 0.0393 - acc: 0.9758
Epoch 21/40
 - 0s - loss: 0.0395 - acc: 0.9761
Epoch 22/40
 - 0s - loss: 0.0383 - acc: 0.9772
Epoch 23/40
 - 0s - loss: 0.0378 - acc: 0.9769
Epoch 24/40
 - 0s - loss: 0.0376 - acc: 0.9765
Epoch 25/40
 - 0s - loss: 0.0366 - acc: 0.9767
Epoch 26/40
 - 0s - loss: 0.0361 - acc: 0.9766
Epoch 27/40
 - 0s - loss: 0.0356 - acc: 0.9774
Epoch 28/40
 - 0s - loss: 0.0359 - acc: 0.9770
Epoch 29/40
 - 0s - loss: 0.0360 - acc: 0.9771
Epoch 30/40
 - 0s - loss: 0.0348 - acc: 0.9783
Epoch 31/40
 - 0s - loss: 0.0354 - acc: 0.9775
Epoch 32/40
 - 0s - loss: 0.0347 - acc: 0.9774
Epoch 33/40
 - 0s - loss: 0.0340 - acc: 0.9786
Epoch 34/40
 - 0s - loss: 0.0339 - acc: 0.9788
Epoch 35/40
 - 0s - loss: 0.0337 - acc: 0.9772
Epoch 36/40
 - 0s - loss: 0.0336 - acc: 0.9786
Epoch 37/40
 - 0s - loss: 0.0335 - acc: 0.9790
Epoch 38/40
 - 0s - loss: 0.0331 - acc: 0.9782
Epoch 39/40
 - 0s - loss: 0.0333 - acc: 0.9780
Epoch 40/40
 - 0s - loss: 0.0326 - acc: 0.9782
# Training time = 0:03:47.274983
# F-Score(Ordinary) = 0.208, Recall: 0.679, Precision: 0.123
# F-Score(lvc) = 0.43, Recall: 0.632, Precision: 0.326
# F-Score(id) = 0.117, Recall: 1.0, Precision: 0.062
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_257 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_258 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_257 (Embedding)       (None, 4, 48)        705264      input_257[0][0]                  
__________________________________________________________________________________________________
embedding_258 (Embedding)       (None, 4, 24)        5640        input_258[0][0]                  
__________________________________________________________________________________________________
flatten_257 (Flatten)           (None, 192)          0           embedding_257[0][0]              
__________________________________________________________________________________________________
flatten_258 (Flatten)           (None, 96)           0           embedding_258[0][0]              
__________________________________________________________________________________________________
concatenate_129 (Concatenate)   (None, 288)          0           flatten_257[0][0]                
                                                                 flatten_258[0][0]                
__________________________________________________________________________________________________
dense_257 (Dense)               (None, 24)           6936        concatenate_129[0][0]            
__________________________________________________________________________________________________
dropout_129 (Dropout)           (None, 24)           0           dense_257[0][0]                  
__________________________________________________________________________________________________
dense_258 (Dense)               (None, 8)            200         dropout_129[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.2120 - acc: 0.9392 - val_loss: 0.0985 - val_acc: 0.9641
Epoch 2/40
 - 5s - loss: 0.0926 - acc: 0.9639 - val_loss: 0.0827 - val_acc: 0.9661
Epoch 3/40
 - 5s - loss: 0.0780 - acc: 0.9672 - val_loss: 0.0769 - val_acc: 0.9668
Epoch 4/40
 - 5s - loss: 0.0709 - acc: 0.9686 - val_loss: 0.0746 - val_acc: 0.9676
Epoch 5/40
 - 5s - loss: 0.0656 - acc: 0.9699 - val_loss: 0.0723 - val_acc: 0.9676
Epoch 6/40
 - 5s - loss: 0.0623 - acc: 0.9709 - val_loss: 0.0710 - val_acc: 0.9671
Epoch 7/40
 - 5s - loss: 0.0595 - acc: 0.9716 - val_loss: 0.0704 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0566 - acc: 0.9726 - val_loss: 0.0709 - val_acc: 0.9676
Epoch 9/40
 - 5s - loss: 0.0545 - acc: 0.9729 - val_loss: 0.0702 - val_acc: 0.9675
Epoch 10/40
 - 5s - loss: 0.0530 - acc: 0.9736 - val_loss: 0.0707 - val_acc: 0.9676
Epoch 11/40
 - 5s - loss: 0.0515 - acc: 0.9738 - val_loss: 0.0706 - val_acc: 0.9680
Epoch 12/40
 - 5s - loss: 0.0503 - acc: 0.9739 - val_loss: 0.0706 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0486 - acc: 0.9746 - val_loss: 0.0724 - val_acc: 0.9675
Epoch 14/40
 - 5s - loss: 0.0479 - acc: 0.9744 - val_loss: 0.0714 - val_acc: 0.9676
Epoch 15/40
 - 5s - loss: 0.0468 - acc: 0.9751 - val_loss: 0.0736 - val_acc: 0.9676
Epoch 16/40
 - 5s - loss: 0.0460 - acc: 0.9747 - val_loss: 0.0738 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0452 - acc: 0.9757 - val_loss: 0.0755 - val_acc: 0.9678
Epoch 18/40
 - 5s - loss: 0.0443 - acc: 0.9757 - val_loss: 0.0755 - val_acc: 0.9680
Epoch 19/40
 - 5s - loss: 0.0439 - acc: 0.9757 - val_loss: 0.0789 - val_acc: 0.9675
Epoch 20/40
 - 5s - loss: 0.0432 - acc: 0.9756 - val_loss: 0.0766 - val_acc: 0.9671
Epoch 21/40
 - 5s - loss: 0.0428 - acc: 0.9756 - val_loss: 0.0791 - val_acc: 0.9675
Epoch 22/40
 - 5s - loss: 0.0423 - acc: 0.9757 - val_loss: 0.0799 - val_acc: 0.9677
Epoch 23/40
 - 5s - loss: 0.0418 - acc: 0.9757 - val_loss: 0.0788 - val_acc: 0.9678
Epoch 24/40
 - 5s - loss: 0.0416 - acc: 0.9761 - val_loss: 0.0789 - val_acc: 0.9679
Epoch 25/40
 - 5s - loss: 0.0411 - acc: 0.9761 - val_loss: 0.0812 - val_acc: 0.9677
Epoch 26/40
 - 5s - loss: 0.0405 - acc: 0.9766 - val_loss: 0.0808 - val_acc: 0.9674
Epoch 27/40
 - 5s - loss: 0.0403 - acc: 0.9765 - val_loss: 0.0818 - val_acc: 0.9676
Epoch 28/40
 - 5s - loss: 0.0401 - acc: 0.9763 - val_loss: 0.0843 - val_acc: 0.9671
Epoch 29/40
 - 5s - loss: 0.0399 - acc: 0.9764 - val_loss: 0.0827 - val_acc: 0.9674
Epoch 30/40
 - 5s - loss: 0.0396 - acc: 0.9764 - val_loss: 0.0841 - val_acc: 0.9674
Epoch 31/40
 - 5s - loss: 0.0392 - acc: 0.9769 - val_loss: 0.0839 - val_acc: 0.9671
Epoch 32/40
 - 5s - loss: 0.0392 - acc: 0.9765 - val_loss: 0.0866 - val_acc: 0.9671
Epoch 33/40
 - 5s - loss: 0.0388 - acc: 0.9768 - val_loss: 0.0880 - val_acc: 0.9674
Epoch 34/40
 - 5s - loss: 0.0388 - acc: 0.9772 - val_loss: 0.0872 - val_acc: 0.9674
Epoch 35/40
 - 5s - loss: 0.0383 - acc: 0.9767 - val_loss: 0.0882 - val_acc: 0.9676
Epoch 36/40
 - 5s - loss: 0.0381 - acc: 0.9772 - val_loss: 0.0900 - val_acc: 0.9675
Epoch 37/40
 - 5s - loss: 0.0381 - acc: 0.9770 - val_loss: 0.0874 - val_acc: 0.9671
Epoch 38/40
 - 5s - loss: 0.0380 - acc: 0.9770 - val_loss: 0.0881 - val_acc: 0.9675
Epoch 39/40
 - 5s - loss: 0.0377 - acc: 0.9776 - val_loss: 0.0899 - val_acc: 0.9675
Epoch 40/40
 - 5s - loss: 0.0376 - acc: 0.9773 - val_loss: 0.0881 - val_acc: 0.9678
Epoch 1/40
 - 0s - loss: 0.0820 - acc: 0.9670
Epoch 2/40
 - 0s - loss: 0.0718 - acc: 0.9694
Epoch 3/40
 - 0s - loss: 0.0649 - acc: 0.9696
Epoch 4/40
 - 0s - loss: 0.0630 - acc: 0.9702
Epoch 5/40
 - 0s - loss: 0.0584 - acc: 0.9710
Epoch 6/40
 - 0s - loss: 0.0568 - acc: 0.9710
Epoch 7/40
 - 0s - loss: 0.0553 - acc: 0.9717
Epoch 8/40
 - 0s - loss: 0.0529 - acc: 0.9721
Epoch 9/40
 - 0s - loss: 0.0503 - acc: 0.9723
Epoch 10/40
 - 0s - loss: 0.0489 - acc: 0.9740
Epoch 11/40
 - 0s - loss: 0.0467 - acc: 0.9741
Epoch 12/40
 - 0s - loss: 0.0464 - acc: 0.9737
Epoch 13/40
 - 0s - loss: 0.0455 - acc: 0.9749
Epoch 14/40
 - 0s - loss: 0.0443 - acc: 0.9749
Epoch 15/40
 - 0s - loss: 0.0436 - acc: 0.9745
Epoch 16/40
 - 0s - loss: 0.0423 - acc: 0.9756
Epoch 17/40
 - 0s - loss: 0.0418 - acc: 0.9754
Epoch 18/40
 - 0s - loss: 0.0412 - acc: 0.9756
Epoch 19/40
 - 0s - loss: 0.0400 - acc: 0.9778
Epoch 20/40
 - 0s - loss: 0.0398 - acc: 0.9769
Epoch 21/40
 - 0s - loss: 0.0393 - acc: 0.9765
Epoch 22/40
 - 0s - loss: 0.0380 - acc: 0.9764
Epoch 23/40
 - 0s - loss: 0.0381 - acc: 0.9769
Epoch 24/40
 - 0s - loss: 0.0375 - acc: 0.9777
Epoch 25/40
 - 0s - loss: 0.0375 - acc: 0.9770
Epoch 26/40
 - 0s - loss: 0.0367 - acc: 0.9770
Epoch 27/40
 - 0s - loss: 0.0364 - acc: 0.9776
Epoch 28/40
 - 0s - loss: 0.0366 - acc: 0.9766
Epoch 29/40
 - 0s - loss: 0.0359 - acc: 0.9772
Epoch 30/40
 - 0s - loss: 0.0355 - acc: 0.9771
Epoch 31/40
 - 0s - loss: 0.0354 - acc: 0.9781
Epoch 32/40
 - 0s - loss: 0.0352 - acc: 0.9780
Epoch 33/40
 - 0s - loss: 0.0348 - acc: 0.9776
Epoch 34/40
 - 0s - loss: 0.0343 - acc: 0.9788
Epoch 35/40
 - 0s - loss: 0.0345 - acc: 0.9772
Epoch 36/40
 - 0s - loss: 0.0339 - acc: 0.9775
Epoch 37/40
 - 0s - loss: 0.0340 - acc: 0.9776
Epoch 38/40
 - 0s - loss: 0.0336 - acc: 0.9773
Epoch 39/40
 - 0s - loss: 0.0338 - acc: 0.9767
Epoch 40/40
 - 0s - loss: 0.0333 - acc: 0.9784
# Training time = 0:03:46.735146
# F-Score(Ordinary) = 0.283, Recall: 0.672, Precision: 0.179
# F-Score(lvc) = 0.409, Recall: 0.818, Precision: 0.273
# F-Score(ireflv) = 0.344, Recall: 0.771, Precision: 0.221
# F-Score(id) = 0.129, Recall: 0.375, Precision: 0.078
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_259 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_260 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_259 (Embedding)       (None, 4, 48)        705264      input_259[0][0]                  
__________________________________________________________________________________________________
embedding_260 (Embedding)       (None, 4, 24)        5640        input_260[0][0]                  
__________________________________________________________________________________________________
flatten_259 (Flatten)           (None, 192)          0           embedding_259[0][0]              
__________________________________________________________________________________________________
flatten_260 (Flatten)           (None, 96)           0           embedding_260[0][0]              
__________________________________________________________________________________________________
concatenate_130 (Concatenate)   (None, 288)          0           flatten_259[0][0]                
                                                                 flatten_260[0][0]                
__________________________________________________________________________________________________
dense_259 (Dense)               (None, 24)           6936        concatenate_130[0][0]            
__________________________________________________________________________________________________
dropout_130 (Dropout)           (None, 24)           0           dense_259[0][0]                  
__________________________________________________________________________________________________
dense_260 (Dense)               (None, 8)            200         dropout_130[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1998 - acc: 0.9381 - val_loss: 0.1007 - val_acc: 0.9638
Epoch 2/40
 - 5s - loss: 0.0962 - acc: 0.9641 - val_loss: 0.0868 - val_acc: 0.9660
Epoch 3/40
 - 5s - loss: 0.0800 - acc: 0.9670 - val_loss: 0.0767 - val_acc: 0.9674
Epoch 4/40
 - 5s - loss: 0.0714 - acc: 0.9690 - val_loss: 0.0731 - val_acc: 0.9679
Epoch 5/40
 - 5s - loss: 0.0664 - acc: 0.9700 - val_loss: 0.0711 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0625 - acc: 0.9705 - val_loss: 0.0695 - val_acc: 0.9679
Epoch 7/40
 - 5s - loss: 0.0598 - acc: 0.9711 - val_loss: 0.0690 - val_acc: 0.9675
Epoch 8/40
 - 5s - loss: 0.0572 - acc: 0.9720 - val_loss: 0.0685 - val_acc: 0.9681
Epoch 9/40
 - 5s - loss: 0.0553 - acc: 0.9724 - val_loss: 0.0685 - val_acc: 0.9676
Epoch 10/40
 - 5s - loss: 0.0532 - acc: 0.9730 - val_loss: 0.0694 - val_acc: 0.9681
Epoch 11/40
 - 5s - loss: 0.0521 - acc: 0.9731 - val_loss: 0.0699 - val_acc: 0.9680
Epoch 12/40
 - 5s - loss: 0.0507 - acc: 0.9739 - val_loss: 0.0701 - val_acc: 0.9680
Epoch 13/40
 - 5s - loss: 0.0494 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9677
Epoch 14/40
 - 5s - loss: 0.0486 - acc: 0.9742 - val_loss: 0.0723 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0476 - acc: 0.9750 - val_loss: 0.0713 - val_acc: 0.9680
Epoch 16/40
 - 5s - loss: 0.0470 - acc: 0.9750 - val_loss: 0.0738 - val_acc: 0.9679
Epoch 17/40
 - 5s - loss: 0.0463 - acc: 0.9749 - val_loss: 0.0738 - val_acc: 0.9677
Epoch 18/40
 - 5s - loss: 0.0455 - acc: 0.9757 - val_loss: 0.0749 - val_acc: 0.9680
Epoch 19/40
 - 5s - loss: 0.0451 - acc: 0.9756 - val_loss: 0.0759 - val_acc: 0.9672
Epoch 20/40
 - 5s - loss: 0.0446 - acc: 0.9756 - val_loss: 0.0747 - val_acc: 0.9674
Epoch 21/40
 - 5s - loss: 0.0438 - acc: 0.9757 - val_loss: 0.0774 - val_acc: 0.9672
Epoch 22/40
 - 5s - loss: 0.0437 - acc: 0.9756 - val_loss: 0.0775 - val_acc: 0.9673
Epoch 23/40
 - 5s - loss: 0.0432 - acc: 0.9760 - val_loss: 0.0784 - val_acc: 0.9672
Epoch 24/40
 - 5s - loss: 0.0425 - acc: 0.9758 - val_loss: 0.0800 - val_acc: 0.9676
Epoch 25/40
 - 5s - loss: 0.0421 - acc: 0.9767 - val_loss: 0.0791 - val_acc: 0.9671
Epoch 26/40
 - 5s - loss: 0.0419 - acc: 0.9763 - val_loss: 0.0819 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0416 - acc: 0.9764 - val_loss: 0.0806 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0411 - acc: 0.9767 - val_loss: 0.0829 - val_acc: 0.9674
Epoch 29/40
 - 5s - loss: 0.0409 - acc: 0.9771 - val_loss: 0.0832 - val_acc: 0.9671
Epoch 30/40
 - 5s - loss: 0.0407 - acc: 0.9766 - val_loss: 0.0833 - val_acc: 0.9676
Epoch 31/40
 - 5s - loss: 0.0404 - acc: 0.9771 - val_loss: 0.0831 - val_acc: 0.9672
Epoch 32/40
 - 5s - loss: 0.0403 - acc: 0.9770 - val_loss: 0.0849 - val_acc: 0.9671
Epoch 33/40
 - 5s - loss: 0.0401 - acc: 0.9766 - val_loss: 0.0857 - val_acc: 0.9669
Epoch 34/40
 - 5s - loss: 0.0397 - acc: 0.9769 - val_loss: 0.0876 - val_acc: 0.9671
Epoch 35/40
 - 5s - loss: 0.0396 - acc: 0.9767 - val_loss: 0.0883 - val_acc: 0.9674
Epoch 36/40
 - 5s - loss: 0.0395 - acc: 0.9766 - val_loss: 0.0875 - val_acc: 0.9670
Epoch 37/40
 - 5s - loss: 0.0392 - acc: 0.9769 - val_loss: 0.0886 - val_acc: 0.9670
Epoch 38/40
 - 5s - loss: 0.0392 - acc: 0.9773 - val_loss: 0.0874 - val_acc: 0.9670
Epoch 39/40
 - 5s - loss: 0.0388 - acc: 0.9774 - val_loss: 0.0880 - val_acc: 0.9665
Epoch 40/40
 - 5s - loss: 0.0388 - acc: 0.9770 - val_loss: 0.0892 - val_acc: 0.9666
Epoch 1/40
 - 0s - loss: 0.0842 - acc: 0.9650
Epoch 2/40
 - 0s - loss: 0.0737 - acc: 0.9673
Epoch 3/40
 - 0s - loss: 0.0671 - acc: 0.9693
Epoch 4/40
 - 0s - loss: 0.0646 - acc: 0.9698
Epoch 5/40
 - 0s - loss: 0.0607 - acc: 0.9704
Epoch 6/40
 - 0s - loss: 0.0579 - acc: 0.9720
Epoch 7/40
 - 0s - loss: 0.0545 - acc: 0.9728
Epoch 8/40
 - 0s - loss: 0.0528 - acc: 0.9725
Epoch 9/40
 - 0s - loss: 0.0503 - acc: 0.9737
Epoch 10/40
 - 0s - loss: 0.0494 - acc: 0.9740
Epoch 11/40
 - 0s - loss: 0.0488 - acc: 0.9725
Epoch 12/40
 - 0s - loss: 0.0470 - acc: 0.9734
Epoch 13/40
 - 0s - loss: 0.0460 - acc: 0.9740
Epoch 14/40
 - 0s - loss: 0.0443 - acc: 0.9748
Epoch 15/40
 - 0s - loss: 0.0437 - acc: 0.9746
Epoch 16/40
 - 0s - loss: 0.0428 - acc: 0.9759
Epoch 17/40
 - 0s - loss: 0.0427 - acc: 0.9755
Epoch 18/40
 - 0s - loss: 0.0412 - acc: 0.9747
Epoch 19/40
 - 0s - loss: 0.0410 - acc: 0.9759
Epoch 20/40
 - 0s - loss: 0.0401 - acc: 0.9760
Epoch 21/40
 - 0s - loss: 0.0398 - acc: 0.9758
Epoch 22/40
 - 0s - loss: 0.0386 - acc: 0.9761
Epoch 23/40
 - 0s - loss: 0.0378 - acc: 0.9777
Epoch 24/40
 - 0s - loss: 0.0382 - acc: 0.9775
Epoch 25/40
 - 0s - loss: 0.0381 - acc: 0.9770
Epoch 26/40
 - 0s - loss: 0.0373 - acc: 0.9772
Epoch 27/40
 - 0s - loss: 0.0372 - acc: 0.9778
Epoch 28/40
 - 0s - loss: 0.0369 - acc: 0.9772
Epoch 29/40
 - 0s - loss: 0.0363 - acc: 0.9781
Epoch 30/40
 - 0s - loss: 0.0361 - acc: 0.9790
Epoch 31/40
 - 0s - loss: 0.0355 - acc: 0.9774
Epoch 32/40
 - 0s - loss: 0.0355 - acc: 0.9778
Epoch 33/40
 - 0s - loss: 0.0352 - acc: 0.9781
Epoch 34/40
 - 0s - loss: 0.0350 - acc: 0.9776
Epoch 35/40
 - 0s - loss: 0.0344 - acc: 0.9774
Epoch 36/40
 - 0s - loss: 0.0343 - acc: 0.9780
Epoch 37/40
 - 0s - loss: 0.0344 - acc: 0.9785
Epoch 38/40
 - 0s - loss: 0.0343 - acc: 0.9783
Epoch 39/40
 - 0s - loss: 0.0339 - acc: 0.9789
Epoch 40/40
 - 0s - loss: 0.0338 - acc: 0.9776
# Training time = 0:03:47.322390
# F-Score(Ordinary) = 0.487, Recall: 0.743, Precision: 0.362
# F-Score(lvc) = 0.482, Recall: 0.587, Precision: 0.409
# F-Score(ireflv) = 0.557, Recall: 0.75, Precision: 0.443
# F-Score(id) = 0.429, Recall: 0.981, Precision: 0.275
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_261 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_262 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_261 (Embedding)       (None, 4, 48)        705264      input_261[0][0]                  
__________________________________________________________________________________________________
embedding_262 (Embedding)       (None, 4, 24)        5640        input_262[0][0]                  
__________________________________________________________________________________________________
flatten_261 (Flatten)           (None, 192)          0           embedding_261[0][0]              
__________________________________________________________________________________________________
flatten_262 (Flatten)           (None, 96)           0           embedding_262[0][0]              
__________________________________________________________________________________________________
concatenate_131 (Concatenate)   (None, 288)          0           flatten_261[0][0]                
                                                                 flatten_262[0][0]                
__________________________________________________________________________________________________
dense_261 (Dense)               (None, 24)           6936        concatenate_131[0][0]            
__________________________________________________________________________________________________
dropout_131 (Dropout)           (None, 24)           0           dense_261[0][0]                  
__________________________________________________________________________________________________
dense_262 (Dense)               (None, 8)            200         dropout_131[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1479 - acc: 0.9530 - val_loss: 0.0867 - val_acc: 0.9651
Epoch 2/40
 - 5s - loss: 0.0779 - acc: 0.9671 - val_loss: 0.0752 - val_acc: 0.9673
Epoch 3/40
 - 5s - loss: 0.0658 - acc: 0.9696 - val_loss: 0.0714 - val_acc: 0.9682
Epoch 4/40
 - 5s - loss: 0.0604 - acc: 0.9712 - val_loss: 0.0711 - val_acc: 0.9676
Epoch 5/40
 - 5s - loss: 0.0563 - acc: 0.9725 - val_loss: 0.0722 - val_acc: 0.9670
Epoch 6/40
 - 5s - loss: 0.0536 - acc: 0.9729 - val_loss: 0.0709 - val_acc: 0.9681
Epoch 7/40
 - 5s - loss: 0.0512 - acc: 0.9736 - val_loss: 0.0726 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0494 - acc: 0.9738 - val_loss: 0.0734 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0477 - acc: 0.9743 - val_loss: 0.0750 - val_acc: 0.9674
Epoch 10/40
 - 5s - loss: 0.0462 - acc: 0.9753 - val_loss: 0.0763 - val_acc: 0.9673
Epoch 11/40
 - 5s - loss: 0.0450 - acc: 0.9752 - val_loss: 0.0791 - val_acc: 0.9670
Epoch 12/40
 - 5s - loss: 0.0440 - acc: 0.9756 - val_loss: 0.0774 - val_acc: 0.9676
Epoch 13/40
 - 5s - loss: 0.0434 - acc: 0.9755 - val_loss: 0.0788 - val_acc: 0.9675
Epoch 14/40
 - 5s - loss: 0.0426 - acc: 0.9759 - val_loss: 0.0809 - val_acc: 0.9674
Epoch 15/40
 - 5s - loss: 0.0420 - acc: 0.9758 - val_loss: 0.0809 - val_acc: 0.9674
Epoch 16/40
 - 5s - loss: 0.0414 - acc: 0.9757 - val_loss: 0.0808 - val_acc: 0.9674
Epoch 17/40
 - 5s - loss: 0.0409 - acc: 0.9764 - val_loss: 0.0836 - val_acc: 0.9670
Epoch 18/40
 - 5s - loss: 0.0405 - acc: 0.9764 - val_loss: 0.0852 - val_acc: 0.9671
Epoch 19/40
 - 5s - loss: 0.0401 - acc: 0.9767 - val_loss: 0.0846 - val_acc: 0.9674
Epoch 20/40
 - 5s - loss: 0.0399 - acc: 0.9765 - val_loss: 0.0853 - val_acc: 0.9677
Epoch 21/40
 - 5s - loss: 0.0394 - acc: 0.9766 - val_loss: 0.0863 - val_acc: 0.9674
Epoch 22/40
 - 5s - loss: 0.0392 - acc: 0.9772 - val_loss: 0.0858 - val_acc: 0.9672
Epoch 23/40
 - 5s - loss: 0.0390 - acc: 0.9768 - val_loss: 0.0879 - val_acc: 0.9671
Epoch 24/40
 - 5s - loss: 0.0386 - acc: 0.9766 - val_loss: 0.0886 - val_acc: 0.9671
Epoch 25/40
 - 5s - loss: 0.0383 - acc: 0.9769 - val_loss: 0.0914 - val_acc: 0.9674
Epoch 26/40
 - 5s - loss: 0.0379 - acc: 0.9773 - val_loss: 0.0901 - val_acc: 0.9678
Epoch 27/40
 - 5s - loss: 0.0378 - acc: 0.9773 - val_loss: 0.0932 - val_acc: 0.9672
Epoch 28/40
 - 5s - loss: 0.0375 - acc: 0.9770 - val_loss: 0.0921 - val_acc: 0.9668
Epoch 29/40
 - 5s - loss: 0.0374 - acc: 0.9775 - val_loss: 0.0916 - val_acc: 0.9666
Epoch 30/40
 - 5s - loss: 0.0370 - acc: 0.9774 - val_loss: 0.0936 - val_acc: 0.9674
Epoch 31/40
 - 5s - loss: 0.0367 - acc: 0.9775 - val_loss: 0.0905 - val_acc: 0.9675
Epoch 32/40
 - 5s - loss: 0.0367 - acc: 0.9774 - val_loss: 0.0929 - val_acc: 0.9664
Epoch 33/40
 - 5s - loss: 0.0366 - acc: 0.9774 - val_loss: 0.0939 - val_acc: 0.9673
Epoch 34/40
 - 5s - loss: 0.0363 - acc: 0.9776 - val_loss: 0.0944 - val_acc: 0.9664
Epoch 35/40
 - 5s - loss: 0.0361 - acc: 0.9781 - val_loss: 0.0936 - val_acc: 0.9672
Epoch 36/40
 - 5s - loss: 0.0360 - acc: 0.9779 - val_loss: 0.0951 - val_acc: 0.9664
Epoch 37/40
 - 5s - loss: 0.0360 - acc: 0.9778 - val_loss: 0.0963 - val_acc: 0.9669
Epoch 38/40
 - 5s - loss: 0.0358 - acc: 0.9777 - val_loss: 0.0952 - val_acc: 0.9658
Epoch 39/40
 - 5s - loss: 0.0356 - acc: 0.9778 - val_loss: 0.0973 - val_acc: 0.9671
Epoch 40/40
 - 5s - loss: 0.0356 - acc: 0.9780 - val_loss: 0.0984 - val_acc: 0.9668
Epoch 1/40
 - 0s - loss: 0.0879 - acc: 0.9658
Epoch 2/40
 - 0s - loss: 0.0725 - acc: 0.9679
Epoch 3/40
 - 0s - loss: 0.0660 - acc: 0.9689
Epoch 4/40
 - 0s - loss: 0.0597 - acc: 0.9697
Epoch 5/40
 - 0s - loss: 0.0577 - acc: 0.9717
Epoch 6/40
 - 0s - loss: 0.0536 - acc: 0.9718
Epoch 7/40
 - 0s - loss: 0.0518 - acc: 0.9718
Epoch 8/40
 - 0s - loss: 0.0488 - acc: 0.9730
Epoch 9/40
 - 0s - loss: 0.0473 - acc: 0.9736
Epoch 10/40
 - 0s - loss: 0.0462 - acc: 0.9748
Epoch 11/40
 - 0s - loss: 0.0447 - acc: 0.9754
Epoch 12/40
 - 0s - loss: 0.0427 - acc: 0.9756
Epoch 13/40
 - 0s - loss: 0.0425 - acc: 0.9754
Epoch 14/40
 - 0s - loss: 0.0408 - acc: 0.9756
Epoch 15/40
 - 0s - loss: 0.0398 - acc: 0.9748
Epoch 16/40
 - 0s - loss: 0.0391 - acc: 0.9766
Epoch 17/40
 - 0s - loss: 0.0381 - acc: 0.9767
Epoch 18/40
 - 0s - loss: 0.0376 - acc: 0.9778
Epoch 19/40
 - 0s - loss: 0.0368 - acc: 0.9764
Epoch 20/40
 - 0s - loss: 0.0374 - acc: 0.9770
Epoch 21/40
 - 0s - loss: 0.0365 - acc: 0.9770
Epoch 22/40
 - 0s - loss: 0.0360 - acc: 0.9767
Epoch 23/40
 - 0s - loss: 0.0356 - acc: 0.9763
Epoch 24/40
 - 0s - loss: 0.0356 - acc: 0.9775
Epoch 25/40
 - 0s - loss: 0.0348 - acc: 0.9776
Epoch 26/40
 - 0s - loss: 0.0348 - acc: 0.9783
Epoch 27/40
 - 0s - loss: 0.0343 - acc: 0.9770
Epoch 28/40
 - 0s - loss: 0.0342 - acc: 0.9770
Epoch 29/40
 - 0s - loss: 0.0338 - acc: 0.9778
Epoch 30/40
 - 0s - loss: 0.0328 - acc: 0.9780
Epoch 31/40
 - 0s - loss: 0.0328 - acc: 0.9794
Epoch 32/40
 - 0s - loss: 0.0329 - acc: 0.9783
Epoch 33/40
 - 0s - loss: 0.0327 - acc: 0.9769
Epoch 34/40
 - 0s - loss: 0.0318 - acc: 0.9796
Epoch 35/40
 - 0s - loss: 0.0323 - acc: 0.9780
Epoch 36/40
 - 0s - loss: 0.0321 - acc: 0.9780
Epoch 37/40
 - 0s - loss: 0.0320 - acc: 0.9779
Epoch 38/40
 - 0s - loss: 0.0316 - acc: 0.9783
Epoch 39/40
 - 0s - loss: 0.0316 - acc: 0.9781
Epoch 40/40
 - 0s - loss: 0.0315 - acc: 0.9777
# Training time = 0:03:47.198301
# F-Score(Ordinary) = 0.088, Recall: 0.311, Precision: 0.051
# F-Score(ireflv) = 0.091, Recall: 0.6, Precision: 0.049
# F-Score(id) = 0.125, Recall: 0.25, Precision: 0.083
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_263 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_264 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_263 (Embedding)       (None, 4, 48)        705264      input_263[0][0]                  
__________________________________________________________________________________________________
embedding_264 (Embedding)       (None, 4, 24)        5640        input_264[0][0]                  
__________________________________________________________________________________________________
flatten_263 (Flatten)           (None, 192)          0           embedding_263[0][0]              
__________________________________________________________________________________________________
flatten_264 (Flatten)           (None, 96)           0           embedding_264[0][0]              
__________________________________________________________________________________________________
concatenate_132 (Concatenate)   (None, 288)          0           flatten_263[0][0]                
                                                                 flatten_264[0][0]                
__________________________________________________________________________________________________
dense_263 (Dense)               (None, 24)           6936        concatenate_132[0][0]            
__________________________________________________________________________________________________
dropout_132 (Dropout)           (None, 24)           0           dense_263[0][0]                  
__________________________________________________________________________________________________
dense_264 (Dense)               (None, 8)            200         dropout_132[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1504 - acc: 0.9535 - val_loss: 0.0851 - val_acc: 0.9664
Epoch 2/40
 - 5s - loss: 0.0777 - acc: 0.9671 - val_loss: 0.0732 - val_acc: 0.9685
Epoch 3/40
 - 5s - loss: 0.0657 - acc: 0.9695 - val_loss: 0.0703 - val_acc: 0.9677
Epoch 4/40
 - 5s - loss: 0.0600 - acc: 0.9714 - val_loss: 0.0690 - val_acc: 0.9685
Epoch 5/40
 - 5s - loss: 0.0561 - acc: 0.9723 - val_loss: 0.0699 - val_acc: 0.9681
Epoch 6/40
 - 5s - loss: 0.0529 - acc: 0.9731 - val_loss: 0.0697 - val_acc: 0.9685
Epoch 7/40
 - 5s - loss: 0.0505 - acc: 0.9740 - val_loss: 0.0709 - val_acc: 0.9680
Epoch 8/40
 - 5s - loss: 0.0485 - acc: 0.9743 - val_loss: 0.0727 - val_acc: 0.9680
Epoch 9/40
 - 5s - loss: 0.0473 - acc: 0.9748 - val_loss: 0.0730 - val_acc: 0.9674
Epoch 10/40
 - 5s - loss: 0.0461 - acc: 0.9751 - val_loss: 0.0764 - val_acc: 0.9676
Epoch 11/40
 - 5s - loss: 0.0450 - acc: 0.9751 - val_loss: 0.0789 - val_acc: 0.9681
Epoch 12/40
 - 5s - loss: 0.0443 - acc: 0.9754 - val_loss: 0.0813 - val_acc: 0.9666
Epoch 13/40
 - 5s - loss: 0.0436 - acc: 0.9761 - val_loss: 0.0783 - val_acc: 0.9672
Epoch 14/40
 - 5s - loss: 0.0429 - acc: 0.9758 - val_loss: 0.0827 - val_acc: 0.9677
Epoch 15/40
 - 5s - loss: 0.0422 - acc: 0.9759 - val_loss: 0.0791 - val_acc: 0.9678
Epoch 16/40
 - 5s - loss: 0.0416 - acc: 0.9764 - val_loss: 0.0822 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0412 - acc: 0.9761 - val_loss: 0.0795 - val_acc: 0.9676
Epoch 18/40
 - 5s - loss: 0.0408 - acc: 0.9767 - val_loss: 0.0825 - val_acc: 0.9680
Epoch 19/40
 - 5s - loss: 0.0403 - acc: 0.9767 - val_loss: 0.0836 - val_acc: 0.9671
Epoch 20/40
 - 5s - loss: 0.0399 - acc: 0.9768 - val_loss: 0.0843 - val_acc: 0.9671
Epoch 21/40
 - 5s - loss: 0.0395 - acc: 0.9768 - val_loss: 0.0850 - val_acc: 0.9665
Epoch 22/40
 - 5s - loss: 0.0392 - acc: 0.9769 - val_loss: 0.0869 - val_acc: 0.9666
Epoch 23/40
 - 5s - loss: 0.0392 - acc: 0.9770 - val_loss: 0.0855 - val_acc: 0.9670
Epoch 24/40
 - 5s - loss: 0.0387 - acc: 0.9774 - val_loss: 0.0894 - val_acc: 0.9667
Epoch 25/40
 - 5s - loss: 0.0384 - acc: 0.9770 - val_loss: 0.0905 - val_acc: 0.9666
Epoch 26/40
 - 5s - loss: 0.0381 - acc: 0.9773 - val_loss: 0.0885 - val_acc: 0.9667
Epoch 27/40
 - 5s - loss: 0.0380 - acc: 0.9770 - val_loss: 0.0887 - val_acc: 0.9663
Epoch 28/40
 - 5s - loss: 0.0378 - acc: 0.9770 - val_loss: 0.0891 - val_acc: 0.9666
Epoch 29/40
 - 5s - loss: 0.0376 - acc: 0.9772 - val_loss: 0.0905 - val_acc: 0.9672
Epoch 30/40
 - 5s - loss: 0.0373 - acc: 0.9776 - val_loss: 0.0896 - val_acc: 0.9671
Epoch 31/40
 - 5s - loss: 0.0370 - acc: 0.9777 - val_loss: 0.0916 - val_acc: 0.9671
Epoch 32/40
 - 5s - loss: 0.0371 - acc: 0.9777 - val_loss: 0.0915 - val_acc: 0.9669
Epoch 33/40
 - 5s - loss: 0.0370 - acc: 0.9777 - val_loss: 0.0960 - val_acc: 0.9670
Epoch 34/40
 - 5s - loss: 0.0367 - acc: 0.9773 - val_loss: 0.0912 - val_acc: 0.9664
Epoch 35/40
 - 5s - loss: 0.0366 - acc: 0.9774 - val_loss: 0.0913 - val_acc: 0.9662
Epoch 36/40
 - 5s - loss: 0.0363 - acc: 0.9777 - val_loss: 0.0937 - val_acc: 0.9663
Epoch 37/40
 - 5s - loss: 0.0360 - acc: 0.9780 - val_loss: 0.0940 - val_acc: 0.9654
Epoch 38/40
 - 5s - loss: 0.0360 - acc: 0.9776 - val_loss: 0.0959 - val_acc: 0.9661
Epoch 39/40
 - 5s - loss: 0.0358 - acc: 0.9778 - val_loss: 0.0958 - val_acc: 0.9658
Epoch 40/40
 - 5s - loss: 0.0359 - acc: 0.9778 - val_loss: 0.0963 - val_acc: 0.9657
Epoch 1/40
 - 0s - loss: 0.0870 - acc: 0.9663
Epoch 2/40
 - 0s - loss: 0.0713 - acc: 0.9674
Epoch 3/40
 - 0s - loss: 0.0652 - acc: 0.9695
Epoch 4/40
 - 0s - loss: 0.0607 - acc: 0.9697
Epoch 5/40
 - 0s - loss: 0.0578 - acc: 0.9705
Epoch 6/40
 - 0s - loss: 0.0544 - acc: 0.9721
Epoch 7/40
 - 0s - loss: 0.0525 - acc: 0.9719
Epoch 8/40
 - 0s - loss: 0.0501 - acc: 0.9743
Epoch 9/40
 - 0s - loss: 0.0468 - acc: 0.9735
Epoch 10/40
 - 0s - loss: 0.0467 - acc: 0.9744
Epoch 11/40
 - 0s - loss: 0.0454 - acc: 0.9748
Epoch 12/40
 - 0s - loss: 0.0436 - acc: 0.9761
Epoch 13/40
 - 0s - loss: 0.0425 - acc: 0.9753
Epoch 14/40
 - 0s - loss: 0.0416 - acc: 0.9758
Epoch 15/40
 - 0s - loss: 0.0409 - acc: 0.9759
Epoch 16/40
 - 0s - loss: 0.0396 - acc: 0.9768
Epoch 17/40
 - 0s - loss: 0.0395 - acc: 0.9771
Epoch 18/40
 - 0s - loss: 0.0387 - acc: 0.9770
Epoch 19/40
 - 0s - loss: 0.0381 - acc: 0.9769
Epoch 20/40
 - 0s - loss: 0.0378 - acc: 0.9776
Epoch 21/40
 - 0s - loss: 0.0372 - acc: 0.9782
Epoch 22/40
 - 0s - loss: 0.0367 - acc: 0.9775
Epoch 23/40
 - 0s - loss: 0.0365 - acc: 0.9790
Epoch 24/40
 - 0s - loss: 0.0361 - acc: 0.9776
Epoch 25/40
 - 0s - loss: 0.0357 - acc: 0.9770
Epoch 26/40
 - 0s - loss: 0.0354 - acc: 0.9781
Epoch 27/40
 - 0s - loss: 0.0345 - acc: 0.9783
Epoch 28/40
 - 0s - loss: 0.0345 - acc: 0.9787
Epoch 29/40
 - 0s - loss: 0.0345 - acc: 0.9782
Epoch 30/40
 - 0s - loss: 0.0344 - acc: 0.9783
Epoch 31/40
 - 0s - loss: 0.0341 - acc: 0.9786
Epoch 32/40
 - 0s - loss: 0.0338 - acc: 0.9784
Epoch 33/40
 - 0s - loss: 0.0344 - acc: 0.9787
Epoch 34/40
 - 0s - loss: 0.0337 - acc: 0.9789
Epoch 35/40
 - 0s - loss: 0.0338 - acc: 0.9786
Epoch 36/40
 - 0s - loss: 0.0336 - acc: 0.9786
Epoch 37/40
 - 0s - loss: 0.0337 - acc: 0.9780
Epoch 38/40
 - 0s - loss: 0.0329 - acc: 0.9789
Epoch 39/40
 - 0s - loss: 0.0332 - acc: 0.9779
Epoch 40/40
 - 0s - loss: 0.0334 - acc: 0.9782
# Training time = 0:03:46.821957
# F-Score(Ordinary) = 0.211, Recall: 0.466, Precision: 0.136
# F-Score(lvc) = 0.425, Recall: 0.433, Precision: 0.417
# F-Score(id) = 0.02, Recall: 0.667, Precision: 0.01
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_265 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_266 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_265 (Embedding)       (None, 4, 48)        705264      input_265[0][0]                  
__________________________________________________________________________________________________
embedding_266 (Embedding)       (None, 4, 24)        5640        input_266[0][0]                  
__________________________________________________________________________________________________
flatten_265 (Flatten)           (None, 192)          0           embedding_265[0][0]              
__________________________________________________________________________________________________
flatten_266 (Flatten)           (None, 96)           0           embedding_266[0][0]              
__________________________________________________________________________________________________
concatenate_133 (Concatenate)   (None, 288)          0           flatten_265[0][0]                
                                                                 flatten_266[0][0]                
__________________________________________________________________________________________________
dense_265 (Dense)               (None, 24)           6936        concatenate_133[0][0]            
__________________________________________________________________________________________________
dropout_133 (Dropout)           (None, 24)           0           dense_265[0][0]                  
__________________________________________________________________________________________________
dense_266 (Dense)               (None, 8)            200         dropout_133[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1478 - acc: 0.9545 - val_loss: 0.0859 - val_acc: 0.9653
Epoch 2/40
 - 5s - loss: 0.0762 - acc: 0.9675 - val_loss: 0.0729 - val_acc: 0.9680
Epoch 3/40
 - 5s - loss: 0.0654 - acc: 0.9700 - val_loss: 0.0717 - val_acc: 0.9676
Epoch 4/40
 - 5s - loss: 0.0595 - acc: 0.9714 - val_loss: 0.0697 - val_acc: 0.9680
Epoch 5/40
 - 5s - loss: 0.0553 - acc: 0.9724 - val_loss: 0.0701 - val_acc: 0.9680
Epoch 6/40
 - 5s - loss: 0.0524 - acc: 0.9732 - val_loss: 0.0697 - val_acc: 0.9683
Epoch 7/40
 - 5s - loss: 0.0501 - acc: 0.9744 - val_loss: 0.0702 - val_acc: 0.9682
Epoch 8/40
 - 5s - loss: 0.0483 - acc: 0.9745 - val_loss: 0.0735 - val_acc: 0.9679
Epoch 9/40
 - 5s - loss: 0.0468 - acc: 0.9746 - val_loss: 0.0738 - val_acc: 0.9679
Epoch 10/40
 - 5s - loss: 0.0455 - acc: 0.9753 - val_loss: 0.0738 - val_acc: 0.9674
Epoch 11/40
 - 5s - loss: 0.0443 - acc: 0.9753 - val_loss: 0.0761 - val_acc: 0.9673
Epoch 12/40
 - 5s - loss: 0.0435 - acc: 0.9754 - val_loss: 0.0792 - val_acc: 0.9676
Epoch 13/40
 - 5s - loss: 0.0427 - acc: 0.9758 - val_loss: 0.0777 - val_acc: 0.9675
Epoch 14/40
 - 5s - loss: 0.0419 - acc: 0.9759 - val_loss: 0.0833 - val_acc: 0.9676
Epoch 15/40
 - 5s - loss: 0.0414 - acc: 0.9759 - val_loss: 0.0798 - val_acc: 0.9668
Epoch 16/40
 - 5s - loss: 0.0411 - acc: 0.9764 - val_loss: 0.0827 - val_acc: 0.9679
Epoch 17/40
 - 5s - loss: 0.0404 - acc: 0.9763 - val_loss: 0.0816 - val_acc: 0.9671
Epoch 18/40
 - 5s - loss: 0.0399 - acc: 0.9767 - val_loss: 0.0800 - val_acc: 0.9671
Epoch 19/40
 - 5s - loss: 0.0394 - acc: 0.9765 - val_loss: 0.0834 - val_acc: 0.9673
Epoch 20/40
 - 5s - loss: 0.0388 - acc: 0.9770 - val_loss: 0.0844 - val_acc: 0.9668
Epoch 21/40
 - 5s - loss: 0.0388 - acc: 0.9768 - val_loss: 0.0844 - val_acc: 0.9671
Epoch 22/40
 - 5s - loss: 0.0382 - acc: 0.9770 - val_loss: 0.0869 - val_acc: 0.9665
Epoch 23/40
 - 5s - loss: 0.0381 - acc: 0.9769 - val_loss: 0.0866 - val_acc: 0.9663
Epoch 24/40
 - 5s - loss: 0.0376 - acc: 0.9775 - val_loss: 0.0878 - val_acc: 0.9672
Epoch 25/40
 - 5s - loss: 0.0376 - acc: 0.9773 - val_loss: 0.0896 - val_acc: 0.9666
Epoch 26/40
 - 5s - loss: 0.0372 - acc: 0.9770 - val_loss: 0.0883 - val_acc: 0.9658
Epoch 27/40
 - 5s - loss: 0.0370 - acc: 0.9772 - val_loss: 0.0896 - val_acc: 0.9660
Epoch 28/40
 - 5s - loss: 0.0369 - acc: 0.9775 - val_loss: 0.0868 - val_acc: 0.9654
Epoch 29/40
 - 5s - loss: 0.0367 - acc: 0.9778 - val_loss: 0.0886 - val_acc: 0.9665
Epoch 30/40
 - 5s - loss: 0.0367 - acc: 0.9778 - val_loss: 0.0900 - val_acc: 0.9660
Epoch 31/40
 - 5s - loss: 0.0361 - acc: 0.9777 - val_loss: 0.0922 - val_acc: 0.9659
Epoch 32/40
 - 5s - loss: 0.0358 - acc: 0.9781 - val_loss: 0.0924 - val_acc: 0.9661
Epoch 33/40
 - 5s - loss: 0.0358 - acc: 0.9775 - val_loss: 0.0928 - val_acc: 0.9663
Epoch 34/40
 - 5s - loss: 0.0356 - acc: 0.9780 - val_loss: 0.0930 - val_acc: 0.9652
Epoch 35/40
 - 5s - loss: 0.0354 - acc: 0.9780 - val_loss: 0.0953 - val_acc: 0.9662
Epoch 36/40
 - 5s - loss: 0.0353 - acc: 0.9777 - val_loss: 0.0934 - val_acc: 0.9659
Epoch 37/40
 - 5s - loss: 0.0351 - acc: 0.9781 - val_loss: 0.0937 - val_acc: 0.9652
Epoch 38/40
 - 5s - loss: 0.0351 - acc: 0.9781 - val_loss: 0.0981 - val_acc: 0.9654
Epoch 39/40
 - 5s - loss: 0.0348 - acc: 0.9779 - val_loss: 0.0972 - val_acc: 0.9640
Epoch 40/40
 - 5s - loss: 0.0349 - acc: 0.9781 - val_loss: 0.0966 - val_acc: 0.9654
Epoch 1/40
 - 0s - loss: 0.0889 - acc: 0.9649
Epoch 2/40
 - 0s - loss: 0.0745 - acc: 0.9680
Epoch 3/40
 - 0s - loss: 0.0666 - acc: 0.9693
Epoch 4/40
 - 0s - loss: 0.0623 - acc: 0.9686
Epoch 5/40
 - 0s - loss: 0.0573 - acc: 0.9699
Epoch 6/40
 - 0s - loss: 0.0556 - acc: 0.9714
Epoch 7/40
 - 0s - loss: 0.0518 - acc: 0.9719
Epoch 8/40
 - 0s - loss: 0.0493 - acc: 0.9732
Epoch 9/40
 - 0s - loss: 0.0479 - acc: 0.9741
Epoch 10/40
 - 0s - loss: 0.0453 - acc: 0.9749
Epoch 11/40
 - 0s - loss: 0.0447 - acc: 0.9750
Epoch 12/40
 - 0s - loss: 0.0431 - acc: 0.9750
Epoch 13/40
 - 0s - loss: 0.0427 - acc: 0.9748
Epoch 14/40
 - 0s - loss: 0.0410 - acc: 0.9768
Epoch 15/40
 - 0s - loss: 0.0409 - acc: 0.9746
Epoch 16/40
 - 0s - loss: 0.0400 - acc: 0.9770
Epoch 17/40
 - 0s - loss: 0.0382 - acc: 0.9771
Epoch 18/40
 - 0s - loss: 0.0374 - acc: 0.9786
Epoch 19/40
 - 0s - loss: 0.0376 - acc: 0.9759
Epoch 20/40
 - 0s - loss: 0.0369 - acc: 0.9764
Epoch 21/40
 - 0s - loss: 0.0369 - acc: 0.9777
Epoch 22/40
 - 0s - loss: 0.0355 - acc: 0.9779
Epoch 23/40
 - 0s - loss: 0.0352 - acc: 0.9770
Epoch 24/40
 - 0s - loss: 0.0347 - acc: 0.9778
Epoch 25/40
 - 0s - loss: 0.0342 - acc: 0.9776
Epoch 26/40
 - 0s - loss: 0.0336 - acc: 0.9783
Epoch 27/40
 - 0s - loss: 0.0336 - acc: 0.9782
Epoch 28/40
 - 0s - loss: 0.0334 - acc: 0.9794
Epoch 29/40
 - 0s - loss: 0.0337 - acc: 0.9780
Epoch 30/40
 - 0s - loss: 0.0326 - acc: 0.9792
Epoch 31/40
 - 0s - loss: 0.0332 - acc: 0.9788
Epoch 32/40
 - 0s - loss: 0.0328 - acc: 0.9779
Epoch 33/40
 - 0s - loss: 0.0316 - acc: 0.9798
Epoch 34/40
 - 0s - loss: 0.0318 - acc: 0.9794
Epoch 35/40
 - 0s - loss: 0.0319 - acc: 0.9779
Epoch 36/40
 - 0s - loss: 0.0317 - acc: 0.9791
Epoch 37/40
 - 0s - loss: 0.0320 - acc: 0.9788
Epoch 38/40
 - 0s - loss: 0.0316 - acc: 0.9793
Epoch 39/40
 - 0s - loss: 0.0311 - acc: 0.9792
Epoch 40/40
 - 0s - loss: 0.0314 - acc: 0.9786
# Training time = 0:03:45.547320
# F-Score(Ordinary) = 0.069, Recall: 0.347, Precision: 0.038
# F-Score(lvc) = 0.19, Recall: 0.362, Precision: 0.129
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_267 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_268 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_267 (Embedding)       (None, 4, 48)        705264      input_267[0][0]                  
__________________________________________________________________________________________________
embedding_268 (Embedding)       (None, 4, 24)        5640        input_268[0][0]                  
__________________________________________________________________________________________________
flatten_267 (Flatten)           (None, 192)          0           embedding_267[0][0]              
__________________________________________________________________________________________________
flatten_268 (Flatten)           (None, 96)           0           embedding_268[0][0]              
__________________________________________________________________________________________________
concatenate_134 (Concatenate)   (None, 288)          0           flatten_267[0][0]                
                                                                 flatten_268[0][0]                
__________________________________________________________________________________________________
dense_267 (Dense)               (None, 24)           6936        concatenate_134[0][0]            
__________________________________________________________________________________________________
dropout_134 (Dropout)           (None, 24)           0           dense_267[0][0]                  
__________________________________________________________________________________________________
dense_268 (Dense)               (None, 8)            200         dropout_134[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1574 - acc: 0.9506 - val_loss: 0.0842 - val_acc: 0.9660
Epoch 2/40
 - 5s - loss: 0.0777 - acc: 0.9669 - val_loss: 0.0754 - val_acc: 0.9675
Epoch 3/40
 - 5s - loss: 0.0672 - acc: 0.9696 - val_loss: 0.0715 - val_acc: 0.9678
Epoch 4/40
 - 5s - loss: 0.0612 - acc: 0.9710 - val_loss: 0.0706 - val_acc: 0.9676
Epoch 5/40
 - 5s - loss: 0.0564 - acc: 0.9724 - val_loss: 0.0711 - val_acc: 0.9677
Epoch 6/40
 - 5s - loss: 0.0533 - acc: 0.9732 - val_loss: 0.0708 - val_acc: 0.9676
Epoch 7/40
 - 5s - loss: 0.0508 - acc: 0.9737 - val_loss: 0.0719 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0485 - acc: 0.9741 - val_loss: 0.0746 - val_acc: 0.9677
Epoch 9/40
 - 5s - loss: 0.0469 - acc: 0.9745 - val_loss: 0.0738 - val_acc: 0.9681
Epoch 10/40
 - 5s - loss: 0.0456 - acc: 0.9753 - val_loss: 0.0749 - val_acc: 0.9670
Epoch 11/40
 - 5s - loss: 0.0447 - acc: 0.9751 - val_loss: 0.0758 - val_acc: 0.9679
Epoch 12/40
 - 5s - loss: 0.0436 - acc: 0.9754 - val_loss: 0.0763 - val_acc: 0.9682
Epoch 13/40
 - 5s - loss: 0.0427 - acc: 0.9758 - val_loss: 0.0788 - val_acc: 0.9678
Epoch 14/40
 - 5s - loss: 0.0423 - acc: 0.9757 - val_loss: 0.0777 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0417 - acc: 0.9759 - val_loss: 0.0810 - val_acc: 0.9675
Epoch 16/40
 - 5s - loss: 0.0409 - acc: 0.9761 - val_loss: 0.0817 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0404 - acc: 0.9766 - val_loss: 0.0819 - val_acc: 0.9679
Epoch 18/40
 - 5s - loss: 0.0397 - acc: 0.9765 - val_loss: 0.0845 - val_acc: 0.9677
Epoch 19/40
 - 5s - loss: 0.0395 - acc: 0.9765 - val_loss: 0.0865 - val_acc: 0.9679
Epoch 20/40
 - 5s - loss: 0.0391 - acc: 0.9768 - val_loss: 0.0829 - val_acc: 0.9675
Epoch 21/40
 - 5s - loss: 0.0387 - acc: 0.9768 - val_loss: 0.0884 - val_acc: 0.9674
Epoch 22/40
 - 5s - loss: 0.0385 - acc: 0.9771 - val_loss: 0.0889 - val_acc: 0.9676
Epoch 23/40
 - 5s - loss: 0.0380 - acc: 0.9767 - val_loss: 0.0876 - val_acc: 0.9675
Epoch 24/40
 - 5s - loss: 0.0379 - acc: 0.9774 - val_loss: 0.0868 - val_acc: 0.9678
Epoch 25/40
 - 5s - loss: 0.0377 - acc: 0.9770 - val_loss: 0.0899 - val_acc: 0.9677
Epoch 26/40
 - 5s - loss: 0.0372 - acc: 0.9771 - val_loss: 0.0885 - val_acc: 0.9669
Epoch 27/40
 - 5s - loss: 0.0369 - acc: 0.9773 - val_loss: 0.0901 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0368 - acc: 0.9769 - val_loss: 0.0916 - val_acc: 0.9674
Epoch 29/40
 - 5s - loss: 0.0366 - acc: 0.9773 - val_loss: 0.0919 - val_acc: 0.9674
Epoch 30/40
 - 5s - loss: 0.0365 - acc: 0.9772 - val_loss: 0.0904 - val_acc: 0.9671
Epoch 31/40
 - 5s - loss: 0.0362 - acc: 0.9776 - val_loss: 0.0907 - val_acc: 0.9674
Epoch 32/40
 - 5s - loss: 0.0362 - acc: 0.9769 - val_loss: 0.0936 - val_acc: 0.9673
Epoch 33/40
 - 5s - loss: 0.0357 - acc: 0.9775 - val_loss: 0.0966 - val_acc: 0.9675
Epoch 34/40
 - 5s - loss: 0.0358 - acc: 0.9780 - val_loss: 0.0940 - val_acc: 0.9672
Epoch 35/40
 - 5s - loss: 0.0354 - acc: 0.9778 - val_loss: 0.0963 - val_acc: 0.9667
Epoch 36/40
 - 5s - loss: 0.0353 - acc: 0.9779 - val_loss: 0.0966 - val_acc: 0.9673
Epoch 37/40
 - 5s - loss: 0.0352 - acc: 0.9779 - val_loss: 0.0965 - val_acc: 0.9673
Epoch 38/40
 - 5s - loss: 0.0352 - acc: 0.9776 - val_loss: 0.0970 - val_acc: 0.9674
Epoch 39/40
 - 5s - loss: 0.0351 - acc: 0.9778 - val_loss: 0.0985 - val_acc: 0.9670
Epoch 40/40
 - 5s - loss: 0.0349 - acc: 0.9779 - val_loss: 0.0957 - val_acc: 0.9676
Epoch 1/40
 - 0s - loss: 0.0878 - acc: 0.9678
Epoch 2/40
 - 0s - loss: 0.0728 - acc: 0.9680
Epoch 3/40
 - 0s - loss: 0.0651 - acc: 0.9708
Epoch 4/40
 - 0s - loss: 0.0629 - acc: 0.9695
Epoch 5/40
 - 0s - loss: 0.0579 - acc: 0.9707
Epoch 6/40
 - 0s - loss: 0.0551 - acc: 0.9707
Epoch 7/40
 - 0s - loss: 0.0534 - acc: 0.9725
Epoch 8/40
 - 0s - loss: 0.0507 - acc: 0.9723
Epoch 9/40
 - 0s - loss: 0.0477 - acc: 0.9737
Epoch 10/40
 - 0s - loss: 0.0468 - acc: 0.9739
Epoch 11/40
 - 0s - loss: 0.0452 - acc: 0.9741
Epoch 12/40
 - 0s - loss: 0.0445 - acc: 0.9738
Epoch 13/40
 - 0s - loss: 0.0425 - acc: 0.9762
Epoch 14/40
 - 0s - loss: 0.0419 - acc: 0.9762
Epoch 15/40
 - 0s - loss: 0.0409 - acc: 0.9761
Epoch 16/40
 - 0s - loss: 0.0401 - acc: 0.9763
Epoch 17/40
 - 0s - loss: 0.0394 - acc: 0.9755
Epoch 18/40
 - 0s - loss: 0.0387 - acc: 0.9770
Epoch 19/40
 - 0s - loss: 0.0382 - acc: 0.9767
Epoch 20/40
 - 0s - loss: 0.0379 - acc: 0.9779
Epoch 21/40
 - 0s - loss: 0.0368 - acc: 0.9777
Epoch 22/40
 - 0s - loss: 0.0364 - acc: 0.9781
Epoch 23/40
 - 0s - loss: 0.0361 - acc: 0.9786
Epoch 24/40
 - 0s - loss: 0.0359 - acc: 0.9774
Epoch 25/40
 - 0s - loss: 0.0349 - acc: 0.9781
Epoch 26/40
 - 0s - loss: 0.0350 - acc: 0.9780
Epoch 27/40
 - 0s - loss: 0.0350 - acc: 0.9779
Epoch 28/40
 - 0s - loss: 0.0347 - acc: 0.9772
Epoch 29/40
 - 0s - loss: 0.0351 - acc: 0.9783
Epoch 30/40
 - 0s - loss: 0.0347 - acc: 0.9782
Epoch 31/40
 - 0s - loss: 0.0337 - acc: 0.9784
Epoch 32/40
 - 0s - loss: 0.0337 - acc: 0.9778
Epoch 33/40
 - 0s - loss: 0.0337 - acc: 0.9784
Epoch 34/40
 - 0s - loss: 0.0334 - acc: 0.9795
Epoch 35/40
 - 0s - loss: 0.0326 - acc: 0.9786
Epoch 36/40
 - 0s - loss: 0.0327 - acc: 0.9780
Epoch 37/40
 - 0s - loss: 0.0324 - acc: 0.9778
Epoch 38/40
 - 0s - loss: 0.0321 - acc: 0.9778
Epoch 39/40
 - 0s - loss: 0.0323 - acc: 0.9776
Epoch 40/40
 - 0s - loss: 0.0317 - acc: 0.9786
# Training time = 0:03:45.937669
# F-Score(Ordinary) = 0.055, Recall: 0.448, Precision: 0.029
# F-Score(lvc) = 0.108, Recall: 0.5, Precision: 0.061
# F-Score(ireflv) = 0.06, Recall: 0.333, Precision: 0.033
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_269 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_270 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_269 (Embedding)       (None, 4, 48)        705264      input_269[0][0]                  
__________________________________________________________________________________________________
embedding_270 (Embedding)       (None, 4, 24)        5640        input_270[0][0]                  
__________________________________________________________________________________________________
flatten_269 (Flatten)           (None, 192)          0           embedding_269[0][0]              
__________________________________________________________________________________________________
flatten_270 (Flatten)           (None, 96)           0           embedding_270[0][0]              
__________________________________________________________________________________________________
concatenate_135 (Concatenate)   (None, 288)          0           flatten_269[0][0]                
                                                                 flatten_270[0][0]                
__________________________________________________________________________________________________
dense_269 (Dense)               (None, 24)           6936        concatenate_135[0][0]            
__________________________________________________________________________________________________
dropout_135 (Dropout)           (None, 24)           0           dense_269[0][0]                  
__________________________________________________________________________________________________
dense_270 (Dense)               (None, 8)            200         dropout_135[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1523 - acc: 0.9501 - val_loss: 0.0884 - val_acc: 0.9663
Epoch 2/40
 - 5s - loss: 0.0792 - acc: 0.9672 - val_loss: 0.0749 - val_acc: 0.9680
Epoch 3/40
 - 5s - loss: 0.0670 - acc: 0.9690 - val_loss: 0.0703 - val_acc: 0.9676
Epoch 4/40
 - 5s - loss: 0.0610 - acc: 0.9708 - val_loss: 0.0693 - val_acc: 0.9680
Epoch 5/40
 - 5s - loss: 0.0570 - acc: 0.9719 - val_loss: 0.0690 - val_acc: 0.9678
Epoch 6/40
 - 5s - loss: 0.0537 - acc: 0.9730 - val_loss: 0.0696 - val_acc: 0.9683
Epoch 7/40
 - 5s - loss: 0.0516 - acc: 0.9735 - val_loss: 0.0702 - val_acc: 0.9679
Epoch 8/40
 - 5s - loss: 0.0499 - acc: 0.9741 - val_loss: 0.0716 - val_acc: 0.9681
Epoch 9/40
 - 5s - loss: 0.0484 - acc: 0.9745 - val_loss: 0.0726 - val_acc: 0.9674
Epoch 10/40
 - 5s - loss: 0.0467 - acc: 0.9748 - val_loss: 0.0734 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0461 - acc: 0.9752 - val_loss: 0.0763 - val_acc: 0.9674
Epoch 12/40
 - 5s - loss: 0.0450 - acc: 0.9755 - val_loss: 0.0766 - val_acc: 0.9672
Epoch 13/40
 - 5s - loss: 0.0442 - acc: 0.9761 - val_loss: 0.0766 - val_acc: 0.9672
Epoch 14/40
 - 5s - loss: 0.0435 - acc: 0.9755 - val_loss: 0.0796 - val_acc: 0.9676
Epoch 15/40
 - 5s - loss: 0.0430 - acc: 0.9759 - val_loss: 0.0765 - val_acc: 0.9677
Epoch 16/40
 - 5s - loss: 0.0423 - acc: 0.9759 - val_loss: 0.0805 - val_acc: 0.9675
Epoch 17/40
 - 5s - loss: 0.0419 - acc: 0.9758 - val_loss: 0.0802 - val_acc: 0.9671
Epoch 18/40
 - 5s - loss: 0.0414 - acc: 0.9766 - val_loss: 0.0822 - val_acc: 0.9675
Epoch 19/40
 - 5s - loss: 0.0410 - acc: 0.9762 - val_loss: 0.0826 - val_acc: 0.9664
Epoch 20/40
 - 5s - loss: 0.0406 - acc: 0.9763 - val_loss: 0.0812 - val_acc: 0.9670
Epoch 21/40
 - 5s - loss: 0.0400 - acc: 0.9768 - val_loss: 0.0848 - val_acc: 0.9664
Epoch 22/40
 - 5s - loss: 0.0399 - acc: 0.9764 - val_loss: 0.0843 - val_acc: 0.9670
Epoch 23/40
 - 5s - loss: 0.0396 - acc: 0.9767 - val_loss: 0.0849 - val_acc: 0.9670
Epoch 24/40
 - 5s - loss: 0.0389 - acc: 0.9770 - val_loss: 0.0866 - val_acc: 0.9669
Epoch 25/40
 - 5s - loss: 0.0387 - acc: 0.9770 - val_loss: 0.0862 - val_acc: 0.9667
Epoch 26/40
 - 5s - loss: 0.0386 - acc: 0.9772 - val_loss: 0.0901 - val_acc: 0.9669
Epoch 27/40
 - 5s - loss: 0.0383 - acc: 0.9771 - val_loss: 0.0869 - val_acc: 0.9670
Epoch 28/40
 - 5s - loss: 0.0379 - acc: 0.9773 - val_loss: 0.0889 - val_acc: 0.9668
Epoch 29/40
 - 5s - loss: 0.0378 - acc: 0.9773 - val_loss: 0.0907 - val_acc: 0.9665
Epoch 30/40
 - 5s - loss: 0.0376 - acc: 0.9771 - val_loss: 0.0906 - val_acc: 0.9668
Epoch 31/40
 - 5s - loss: 0.0373 - acc: 0.9776 - val_loss: 0.0900 - val_acc: 0.9664
Epoch 32/40
 - 5s - loss: 0.0374 - acc: 0.9772 - val_loss: 0.0906 - val_acc: 0.9665
Epoch 33/40
 - 5s - loss: 0.0370 - acc: 0.9773 - val_loss: 0.0916 - val_acc: 0.9665
Epoch 34/40
 - 5s - loss: 0.0370 - acc: 0.9777 - val_loss: 0.0944 - val_acc: 0.9663
Epoch 35/40
 - 5s - loss: 0.0367 - acc: 0.9772 - val_loss: 0.0944 - val_acc: 0.9665
Epoch 36/40
 - 5s - loss: 0.0366 - acc: 0.9773 - val_loss: 0.0952 - val_acc: 0.9657
Epoch 37/40
 - 5s - loss: 0.0363 - acc: 0.9775 - val_loss: 0.0961 - val_acc: 0.9658
Epoch 38/40
 - 5s - loss: 0.0364 - acc: 0.9778 - val_loss: 0.0930 - val_acc: 0.9660
Epoch 39/40
 - 5s - loss: 0.0360 - acc: 0.9778 - val_loss: 0.0941 - val_acc: 0.9654
Epoch 40/40
 - 5s - loss: 0.0361 - acc: 0.9778 - val_loss: 0.0972 - val_acc: 0.9660
Epoch 1/40
 - 0s - loss: 0.0897 - acc: 0.9636
Epoch 2/40
 - 0s - loss: 0.0744 - acc: 0.9679
Epoch 3/40
 - 0s - loss: 0.0654 - acc: 0.9688
Epoch 4/40
 - 0s - loss: 0.0620 - acc: 0.9695
Epoch 5/40
 - 0s - loss: 0.0578 - acc: 0.9696
Epoch 6/40
 - 0s - loss: 0.0553 - acc: 0.9710
Epoch 7/40
 - 0s - loss: 0.0519 - acc: 0.9724
Epoch 8/40
 - 0s - loss: 0.0500 - acc: 0.9727
Epoch 9/40
 - 0s - loss: 0.0471 - acc: 0.9731
Epoch 10/40
 - 0s - loss: 0.0460 - acc: 0.9741
Epoch 11/40
 - 0s - loss: 0.0453 - acc: 0.9739
Epoch 12/40
 - 0s - loss: 0.0433 - acc: 0.9749
Epoch 13/40
 - 0s - loss: 0.0432 - acc: 0.9738
Epoch 14/40
 - 0s - loss: 0.0413 - acc: 0.9757
Epoch 15/40
 - 0s - loss: 0.0405 - acc: 0.9757
Epoch 16/40
 - 1s - loss: 0.0402 - acc: 0.9746
Epoch 17/40
 - 0s - loss: 0.0394 - acc: 0.9752
Epoch 18/40
 - 0s - loss: 0.0388 - acc: 0.9743
Epoch 19/40
 - 0s - loss: 0.0380 - acc: 0.9763
Epoch 20/40
 - 0s - loss: 0.0375 - acc: 0.9766
Epoch 21/40
 - 0s - loss: 0.0376 - acc: 0.9764
Epoch 22/40
 - 0s - loss: 0.0364 - acc: 0.9769
Epoch 23/40
 - 0s - loss: 0.0355 - acc: 0.9773
Epoch 24/40
 - 0s - loss: 0.0353 - acc: 0.9769
Epoch 25/40
 - 0s - loss: 0.0357 - acc: 0.9765
Epoch 26/40
 - 0s - loss: 0.0349 - acc: 0.9777
Epoch 27/40
 - 0s - loss: 0.0347 - acc: 0.9771
Epoch 28/40
 - 0s - loss: 0.0346 - acc: 0.9776
Epoch 29/40
 - 0s - loss: 0.0338 - acc: 0.9775
Epoch 30/40
 - 0s - loss: 0.0339 - acc: 0.9780
Epoch 31/40
 - 0s - loss: 0.0332 - acc: 0.9784
Epoch 32/40
 - 0s - loss: 0.0330 - acc: 0.9781
Epoch 33/40
 - 0s - loss: 0.0330 - acc: 0.9788
Epoch 34/40
 - 0s - loss: 0.0323 - acc: 0.9794
Epoch 35/40
 - 0s - loss: 0.0324 - acc: 0.9782
Epoch 36/40
 - 0s - loss: 0.0323 - acc: 0.9793
Epoch 37/40
 - 0s - loss: 0.0326 - acc: 0.9782
Epoch 38/40
 - 0s - loss: 0.0323 - acc: 0.9787
Epoch 39/40
 - 1s - loss: 0.0320 - acc: 0.9786
Epoch 40/40
 - 0s - loss: 0.0321 - acc: 0.9789
# Training time = 0:03:47.296763
# F-Score(Ordinary) = 0.575, Recall: 0.695, Precision: 0.49
# F-Score(lvc) = 0.45, Recall: 0.426, Precision: 0.477
# F-Score(ireflv) = 0.696, Recall: 0.847, Precision: 0.59
# F-Score(id) = 0.575, Recall: 0.963, Precision: 0.409
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_271 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_272 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_271 (Embedding)       (None, 4, 48)        705264      input_271[0][0]                  
__________________________________________________________________________________________________
embedding_272 (Embedding)       (None, 4, 24)        5640        input_272[0][0]                  
__________________________________________________________________________________________________
flatten_271 (Flatten)           (None, 192)          0           embedding_271[0][0]              
__________________________________________________________________________________________________
flatten_272 (Flatten)           (None, 96)           0           embedding_272[0][0]              
__________________________________________________________________________________________________
concatenate_136 (Concatenate)   (None, 288)          0           flatten_271[0][0]                
                                                                 flatten_272[0][0]                
__________________________________________________________________________________________________
dense_271 (Dense)               (None, 24)           6936        concatenate_136[0][0]            
__________________________________________________________________________________________________
dropout_136 (Dropout)           (None, 24)           0           dense_271[0][0]                  
__________________________________________________________________________________________________
dense_272 (Dense)               (None, 8)            200         dropout_136[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1107 - acc: 0.9601 - val_loss: 0.0775 - val_acc: 0.9667
Epoch 2/40
 - 5s - loss: 0.0666 - acc: 0.9691 - val_loss: 0.0703 - val_acc: 0.9672
Epoch 3/40
 - 5s - loss: 0.0573 - acc: 0.9714 - val_loss: 0.0699 - val_acc: 0.9683
Epoch 4/40
 - 5s - loss: 0.0527 - acc: 0.9732 - val_loss: 0.0722 - val_acc: 0.9674
Epoch 5/40
 - 5s - loss: 0.0491 - acc: 0.9735 - val_loss: 0.0762 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0469 - acc: 0.9741 - val_loss: 0.0745 - val_acc: 0.9682
Epoch 7/40
 - 5s - loss: 0.0452 - acc: 0.9750 - val_loss: 0.0783 - val_acc: 0.9677
Epoch 8/40
 - 5s - loss: 0.0435 - acc: 0.9751 - val_loss: 0.0815 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0422 - acc: 0.9764 - val_loss: 0.0815 - val_acc: 0.9678
Epoch 10/40
 - 5s - loss: 0.0416 - acc: 0.9760 - val_loss: 0.0823 - val_acc: 0.9676
Epoch 11/40
 - 5s - loss: 0.0403 - acc: 0.9763 - val_loss: 0.0859 - val_acc: 0.9658
Epoch 12/40
 - 5s - loss: 0.0397 - acc: 0.9769 - val_loss: 0.0827 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0392 - acc: 0.9763 - val_loss: 0.0863 - val_acc: 0.9670
Epoch 14/40
 - 5s - loss: 0.0385 - acc: 0.9769 - val_loss: 0.0893 - val_acc: 0.9676
Epoch 15/40
 - 5s - loss: 0.0379 - acc: 0.9769 - val_loss: 0.0881 - val_acc: 0.9674
Epoch 16/40
 - 5s - loss: 0.0376 - acc: 0.9767 - val_loss: 0.0886 - val_acc: 0.9671
Epoch 17/40
 - 5s - loss: 0.0373 - acc: 0.9774 - val_loss: 0.0889 - val_acc: 0.9669
Epoch 18/40
 - 5s - loss: 0.0367 - acc: 0.9775 - val_loss: 0.0915 - val_acc: 0.9668
Epoch 19/40
 - 5s - loss: 0.0365 - acc: 0.9772 - val_loss: 0.0942 - val_acc: 0.9679
Epoch 20/40
 - 5s - loss: 0.0363 - acc: 0.9776 - val_loss: 0.0927 - val_acc: 0.9677
Epoch 21/40
 - 5s - loss: 0.0358 - acc: 0.9778 - val_loss: 0.0937 - val_acc: 0.9662
Epoch 22/40
 - 5s - loss: 0.0358 - acc: 0.9780 - val_loss: 0.0943 - val_acc: 0.9659
Epoch 23/40
 - 5s - loss: 0.0353 - acc: 0.9778 - val_loss: 0.0951 - val_acc: 0.9662
Epoch 24/40
 - 5s - loss: 0.0355 - acc: 0.9776 - val_loss: 0.0978 - val_acc: 0.9647
Epoch 25/40
 - 5s - loss: 0.0348 - acc: 0.9776 - val_loss: 0.0989 - val_acc: 0.9676
Epoch 26/40
 - 5s - loss: 0.0345 - acc: 0.9784 - val_loss: 0.1010 - val_acc: 0.9660
Epoch 27/40
 - 5s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.1000 - val_acc: 0.9655
Epoch 28/40
 - 5s - loss: 0.0343 - acc: 0.9779 - val_loss: 0.0989 - val_acc: 0.9647
Epoch 29/40
 - 5s - loss: 0.0343 - acc: 0.9784 - val_loss: 0.1014 - val_acc: 0.9642
Epoch 30/40
 - 5s - loss: 0.0342 - acc: 0.9781 - val_loss: 0.1042 - val_acc: 0.9646
Epoch 31/40
 - 5s - loss: 0.0338 - acc: 0.9781 - val_loss: 0.1009 - val_acc: 0.9646
Epoch 32/40
 - 5s - loss: 0.0341 - acc: 0.9782 - val_loss: 0.1053 - val_acc: 0.9644
Epoch 33/40
 - 5s - loss: 0.0337 - acc: 0.9785 - val_loss: 0.1067 - val_acc: 0.9654
Epoch 34/40
 - 5s - loss: 0.0335 - acc: 0.9786 - val_loss: 0.1091 - val_acc: 0.9624
Epoch 35/40
 - 5s - loss: 0.0337 - acc: 0.9783 - val_loss: 0.1076 - val_acc: 0.9651
Epoch 36/40
 - 5s - loss: 0.0335 - acc: 0.9785 - val_loss: 0.1065 - val_acc: 0.9634
Epoch 37/40
 - 5s - loss: 0.0333 - acc: 0.9785 - val_loss: 0.1087 - val_acc: 0.9629
Epoch 38/40
 - 5s - loss: 0.0332 - acc: 0.9784 - val_loss: 0.1086 - val_acc: 0.9636
Epoch 39/40
 - 5s - loss: 0.0332 - acc: 0.9782 - val_loss: 0.1110 - val_acc: 0.9644
Epoch 40/40
 - 5s - loss: 0.0332 - acc: 0.9786 - val_loss: 0.1089 - val_acc: 0.9656
Epoch 1/40
 - 0s - loss: 0.0943 - acc: 0.9652
Epoch 2/40
 - 0s - loss: 0.0763 - acc: 0.9681
Epoch 3/40
 - 0s - loss: 0.0666 - acc: 0.9695
Epoch 4/40
 - 0s - loss: 0.0601 - acc: 0.9712
Epoch 5/40
 - 0s - loss: 0.0573 - acc: 0.9717
Epoch 6/40
 - 0s - loss: 0.0531 - acc: 0.9730
Epoch 7/40
 - 0s - loss: 0.0517 - acc: 0.9726
Epoch 8/40
 - 0s - loss: 0.0496 - acc: 0.9744
Epoch 9/40
 - 0s - loss: 0.0470 - acc: 0.9744
Epoch 10/40
 - 0s - loss: 0.0461 - acc: 0.9740
Epoch 11/40
 - 0s - loss: 0.0445 - acc: 0.9749
Epoch 12/40
 - 0s - loss: 0.0436 - acc: 0.9762
Epoch 13/40
 - 0s - loss: 0.0427 - acc: 0.9753
Epoch 14/40
 - 0s - loss: 0.0402 - acc: 0.9758
Epoch 15/40
 - 0s - loss: 0.0399 - acc: 0.9745
Epoch 16/40
 - 0s - loss: 0.0388 - acc: 0.9781
Epoch 17/40
 - 0s - loss: 0.0394 - acc: 0.9764
Epoch 18/40
 - 0s - loss: 0.0378 - acc: 0.9767
Epoch 19/40
 - 0s - loss: 0.0370 - acc: 0.9770
Epoch 20/40
 - 0s - loss: 0.0378 - acc: 0.9774
Epoch 21/40
 - 0s - loss: 0.0368 - acc: 0.9764
Epoch 22/40
 - 0s - loss: 0.0360 - acc: 0.9777
Epoch 23/40
 - 0s - loss: 0.0356 - acc: 0.9779
Epoch 24/40
 - 0s - loss: 0.0364 - acc: 0.9773
Epoch 25/40
 - 0s - loss: 0.0358 - acc: 0.9774
Epoch 26/40
 - 0s - loss: 0.0357 - acc: 0.9780
Epoch 27/40
 - 0s - loss: 0.0358 - acc: 0.9764
Epoch 28/40
 - 0s - loss: 0.0351 - acc: 0.9775
Epoch 29/40
 - 0s - loss: 0.0346 - acc: 0.9782
Epoch 30/40
 - 0s - loss: 0.0343 - acc: 0.9778
Epoch 31/40
 - 0s - loss: 0.0339 - acc: 0.9796
Epoch 32/40
 - 0s - loss: 0.0346 - acc: 0.9791
Epoch 33/40
 - 0s - loss: 0.0348 - acc: 0.9778
Epoch 34/40
 - 0s - loss: 0.0337 - acc: 0.9788
Epoch 35/40
 - 0s - loss: 0.0332 - acc: 0.9791
Epoch 36/40
 - 0s - loss: 0.0339 - acc: 0.9779
Epoch 37/40
 - 0s - loss: 0.0332 - acc: 0.9793
Epoch 38/40
 - 0s - loss: 0.0334 - acc: 0.9785
Epoch 39/40
 - 0s - loss: 0.0333 - acc: 0.9786
Epoch 40/40
 - 0s - loss: 0.0339 - acc: 0.9778
# Training time = 0:03:45.887425
# F-Score(Ordinary) = 0.475, Recall: 0.828, Precision: 0.333
# F-Score(ireflv) = 0.611, Recall: 0.831, Precision: 0.484
# F-Score(id) = 0.583, Recall: 0.807, Precision: 0.456
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_273 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_274 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_273 (Embedding)       (None, 4, 48)        705264      input_273[0][0]                  
__________________________________________________________________________________________________
embedding_274 (Embedding)       (None, 4, 24)        5640        input_274[0][0]                  
__________________________________________________________________________________________________
flatten_273 (Flatten)           (None, 192)          0           embedding_273[0][0]              
__________________________________________________________________________________________________
flatten_274 (Flatten)           (None, 96)           0           embedding_274[0][0]              
__________________________________________________________________________________________________
concatenate_137 (Concatenate)   (None, 288)          0           flatten_273[0][0]                
                                                                 flatten_274[0][0]                
__________________________________________________________________________________________________
dense_273 (Dense)               (None, 24)           6936        concatenate_137[0][0]            
__________________________________________________________________________________________________
dropout_137 (Dropout)           (None, 24)           0           dense_273[0][0]                  
__________________________________________________________________________________________________
dense_274 (Dense)               (None, 8)            200         dropout_137[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1119 - acc: 0.9598 - val_loss: 0.0720 - val_acc: 0.9685
Epoch 2/40
 - 5s - loss: 0.0649 - acc: 0.9698 - val_loss: 0.0696 - val_acc: 0.9682
Epoch 3/40
 - 5s - loss: 0.0564 - acc: 0.9720 - val_loss: 0.0699 - val_acc: 0.9676
Epoch 4/40
 - 5s - loss: 0.0518 - acc: 0.9736 - val_loss: 0.0709 - val_acc: 0.9684
Epoch 5/40
 - 5s - loss: 0.0488 - acc: 0.9744 - val_loss: 0.0745 - val_acc: 0.9678
Epoch 6/40
 - 5s - loss: 0.0463 - acc: 0.9749 - val_loss: 0.0773 - val_acc: 0.9679
Epoch 7/40
 - 5s - loss: 0.0445 - acc: 0.9752 - val_loss: 0.0763 - val_acc: 0.9681
Epoch 8/40
 - 5s - loss: 0.0431 - acc: 0.9757 - val_loss: 0.0792 - val_acc: 0.9679
Epoch 9/40
 - 5s - loss: 0.0421 - acc: 0.9758 - val_loss: 0.0811 - val_acc: 0.9677
Epoch 10/40
 - 5s - loss: 0.0413 - acc: 0.9761 - val_loss: 0.0829 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0403 - acc: 0.9768 - val_loss: 0.0889 - val_acc: 0.9674
Epoch 12/40
 - 5s - loss: 0.0400 - acc: 0.9769 - val_loss: 0.0912 - val_acc: 0.9671
Epoch 13/40
 - 5s - loss: 0.0395 - acc: 0.9768 - val_loss: 0.0869 - val_acc: 0.9679
Epoch 14/40
 - 5s - loss: 0.0387 - acc: 0.9765 - val_loss: 0.0928 - val_acc: 0.9680
Epoch 15/40
 - 5s - loss: 0.0382 - acc: 0.9772 - val_loss: 0.0862 - val_acc: 0.9681
Epoch 16/40
 - 5s - loss: 0.0378 - acc: 0.9771 - val_loss: 0.0904 - val_acc: 0.9673
Epoch 17/40
 - 5s - loss: 0.0373 - acc: 0.9770 - val_loss: 0.0902 - val_acc: 0.9658
Epoch 18/40
 - 5s - loss: 0.0370 - acc: 0.9776 - val_loss: 0.0915 - val_acc: 0.9675
Epoch 19/40
 - 5s - loss: 0.0367 - acc: 0.9774 - val_loss: 0.0930 - val_acc: 0.9665
Epoch 20/40
 - 5s - loss: 0.0366 - acc: 0.9774 - val_loss: 0.0945 - val_acc: 0.9660
Epoch 21/40
 - 5s - loss: 0.0361 - acc: 0.9773 - val_loss: 0.0948 - val_acc: 0.9660
Epoch 22/40
 - 5s - loss: 0.0358 - acc: 0.9777 - val_loss: 0.0978 - val_acc: 0.9648
Epoch 23/40
 - 5s - loss: 0.0359 - acc: 0.9776 - val_loss: 0.0956 - val_acc: 0.9658
Epoch 24/40
 - 5s - loss: 0.0353 - acc: 0.9780 - val_loss: 0.1018 - val_acc: 0.9650
Epoch 25/40
 - 5s - loss: 0.0352 - acc: 0.9779 - val_loss: 0.0998 - val_acc: 0.9654
Epoch 26/40
 - 5s - loss: 0.0350 - acc: 0.9777 - val_loss: 0.1001 - val_acc: 0.9658
Epoch 27/40
 - 5s - loss: 0.0349 - acc: 0.9780 - val_loss: 0.1025 - val_acc: 0.9636
Epoch 28/40
 - 5s - loss: 0.0347 - acc: 0.9777 - val_loss: 0.1051 - val_acc: 0.9590
Epoch 29/40
 - 5s - loss: 0.0346 - acc: 0.9777 - val_loss: 0.1049 - val_acc: 0.9651
Epoch 30/40
 - 5s - loss: 0.0343 - acc: 0.9784 - val_loss: 0.1007 - val_acc: 0.9628
Epoch 31/40
 - 5s - loss: 0.0340 - acc: 0.9783 - val_loss: 0.1055 - val_acc: 0.9641
Epoch 32/40
 - 5s - loss: 0.0343 - acc: 0.9784 - val_loss: 0.1076 - val_acc: 0.9647
Epoch 33/40
 - 5s - loss: 0.0340 - acc: 0.9784 - val_loss: 0.1083 - val_acc: 0.9642
Epoch 34/40
 - 5s - loss: 0.0339 - acc: 0.9780 - val_loss: 0.1074 - val_acc: 0.9620
Epoch 35/40
 - 5s - loss: 0.0339 - acc: 0.9779 - val_loss: 0.1067 - val_acc: 0.9631
Epoch 36/40
 - 5s - loss: 0.0336 - acc: 0.9786 - val_loss: 0.1084 - val_acc: 0.9624
Epoch 37/40
 - 5s - loss: 0.0335 - acc: 0.9785 - val_loss: 0.1058 - val_acc: 0.9602
Epoch 38/40
 - 5s - loss: 0.0334 - acc: 0.9784 - val_loss: 0.1101 - val_acc: 0.9633
Epoch 39/40
 - 5s - loss: 0.0331 - acc: 0.9787 - val_loss: 0.1129 - val_acc: 0.9618
Epoch 40/40
 - 5s - loss: 0.0333 - acc: 0.9784 - val_loss: 0.1110 - val_acc: 0.9623
Epoch 1/40
 - 0s - loss: 0.0962 - acc: 0.9649
Epoch 2/40
 - 0s - loss: 0.0765 - acc: 0.9675
Epoch 3/40
 - 0s - loss: 0.0672 - acc: 0.9687
Epoch 4/40
 - 0s - loss: 0.0625 - acc: 0.9696
Epoch 5/40
 - 0s - loss: 0.0583 - acc: 0.9709
Epoch 6/40
 - 0s - loss: 0.0549 - acc: 0.9716
Epoch 7/40
 - 0s - loss: 0.0524 - acc: 0.9720
Epoch 8/40
 - 0s - loss: 0.0495 - acc: 0.9747
Epoch 9/40
 - 0s - loss: 0.0461 - acc: 0.9737
Epoch 10/40
 - 0s - loss: 0.0460 - acc: 0.9748
Epoch 11/40
 - 0s - loss: 0.0447 - acc: 0.9757
Epoch 12/40
 - 0s - loss: 0.0432 - acc: 0.9757
Epoch 13/40
 - 0s - loss: 0.0413 - acc: 0.9755
Epoch 14/40
 - 0s - loss: 0.0408 - acc: 0.9761
Epoch 15/40
 - 0s - loss: 0.0401 - acc: 0.9765
Epoch 16/40
 - 0s - loss: 0.0388 - acc: 0.9763
Epoch 17/40
 - 0s - loss: 0.0393 - acc: 0.9774
Epoch 18/40
 - 0s - loss: 0.0377 - acc: 0.9771
Epoch 19/40
 - 0s - loss: 0.0378 - acc: 0.9776
Epoch 20/40
 - 0s - loss: 0.0375 - acc: 0.9774
Epoch 21/40
 - 1s - loss: 0.0378 - acc: 0.9777
Epoch 22/40
 - 0s - loss: 0.0368 - acc: 0.9775
Epoch 23/40
 - 0s - loss: 0.0366 - acc: 0.9781
Epoch 24/40
 - 0s - loss: 0.0366 - acc: 0.9784
Epoch 25/40
 - 1s - loss: 0.0357 - acc: 0.9789
Epoch 26/40
 - 0s - loss: 0.0348 - acc: 0.9793
Epoch 27/40
 - 0s - loss: 0.0351 - acc: 0.9784
Epoch 28/40
 - 0s - loss: 0.0355 - acc: 0.9793
Epoch 29/40
 - 0s - loss: 0.0349 - acc: 0.9779
Epoch 30/40
 - 0s - loss: 0.0352 - acc: 0.9787
Epoch 31/40
 - 0s - loss: 0.0342 - acc: 0.9786
Epoch 32/40
 - 0s - loss: 0.0349 - acc: 0.9781
Epoch 33/40
 - 0s - loss: 0.0345 - acc: 0.9791
Epoch 34/40
 - 0s - loss: 0.0348 - acc: 0.9777
Epoch 35/40
 - 0s - loss: 0.0339 - acc: 0.9776
Epoch 36/40
 - 0s - loss: 0.0342 - acc: 0.9791
Epoch 37/40
 - 0s - loss: 0.0343 - acc: 0.9782
Epoch 38/40
 - 0s - loss: 0.0336 - acc: 0.9792
Epoch 39/40
 - 0s - loss: 0.0338 - acc: 0.9781
Epoch 40/40
 - 0s - loss: 0.0342 - acc: 0.9782
# Training time = 0:03:47.489138
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_275 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_276 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_275 (Embedding)       (None, 4, 48)        705264      input_275[0][0]                  
__________________________________________________________________________________________________
embedding_276 (Embedding)       (None, 4, 24)        5640        input_276[0][0]                  
__________________________________________________________________________________________________
flatten_275 (Flatten)           (None, 192)          0           embedding_275[0][0]              
__________________________________________________________________________________________________
flatten_276 (Flatten)           (None, 96)           0           embedding_276[0][0]              
__________________________________________________________________________________________________
concatenate_138 (Concatenate)   (None, 288)          0           flatten_275[0][0]                
                                                                 flatten_276[0][0]                
__________________________________________________________________________________________________
dense_275 (Dense)               (None, 24)           6936        concatenate_138[0][0]            
__________________________________________________________________________________________________
dropout_138 (Dropout)           (None, 24)           0           dense_275[0][0]                  
__________________________________________________________________________________________________
dense_276 (Dense)               (None, 8)            200         dropout_138[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1086 - acc: 0.9612 - val_loss: 0.0742 - val_acc: 0.9674
Epoch 2/40
 - 5s - loss: 0.0648 - acc: 0.9693 - val_loss: 0.0683 - val_acc: 0.9681
Epoch 3/40
 - 5s - loss: 0.0565 - acc: 0.9724 - val_loss: 0.0718 - val_acc: 0.9674
Epoch 4/40
 - 5s - loss: 0.0515 - acc: 0.9733 - val_loss: 0.0708 - val_acc: 0.9683
Epoch 5/40
 - 5s - loss: 0.0483 - acc: 0.9743 - val_loss: 0.0740 - val_acc: 0.9677
Epoch 6/40
 - 5s - loss: 0.0461 - acc: 0.9746 - val_loss: 0.0740 - val_acc: 0.9683
Epoch 7/40
 - 5s - loss: 0.0443 - acc: 0.9758 - val_loss: 0.0770 - val_acc: 0.9677
Epoch 8/40
 - 5s - loss: 0.0429 - acc: 0.9757 - val_loss: 0.0806 - val_acc: 0.9675
Epoch 9/40
 - 5s - loss: 0.0416 - acc: 0.9757 - val_loss: 0.0824 - val_acc: 0.9661
Epoch 10/40
 - 5s - loss: 0.0407 - acc: 0.9762 - val_loss: 0.0808 - val_acc: 0.9669
Epoch 11/40
 - 5s - loss: 0.0399 - acc: 0.9763 - val_loss: 0.0844 - val_acc: 0.9666
Epoch 12/40
 - 5s - loss: 0.0390 - acc: 0.9764 - val_loss: 0.0890 - val_acc: 0.9673
Epoch 13/40
 - 5s - loss: 0.0385 - acc: 0.9769 - val_loss: 0.0880 - val_acc: 0.9659
Epoch 14/40
 - 5s - loss: 0.0379 - acc: 0.9772 - val_loss: 0.0932 - val_acc: 0.9664
Epoch 15/40
 - 5s - loss: 0.0376 - acc: 0.9769 - val_loss: 0.0896 - val_acc: 0.9644
Epoch 16/40
 - 5s - loss: 0.0373 - acc: 0.9776 - val_loss: 0.0884 - val_acc: 0.9662
Epoch 17/40
 - 5s - loss: 0.0372 - acc: 0.9772 - val_loss: 0.0899 - val_acc: 0.9650
Epoch 18/40
 - 5s - loss: 0.0367 - acc: 0.9770 - val_loss: 0.0912 - val_acc: 0.9664
Epoch 19/40
 - 5s - loss: 0.0365 - acc: 0.9777 - val_loss: 0.0927 - val_acc: 0.9654
Epoch 20/40
 - 5s - loss: 0.0359 - acc: 0.9774 - val_loss: 0.0951 - val_acc: 0.9649
Epoch 21/40
 - 5s - loss: 0.0359 - acc: 0.9779 - val_loss: 0.0943 - val_acc: 0.9646
Epoch 22/40
 - 5s - loss: 0.0352 - acc: 0.9777 - val_loss: 0.0947 - val_acc: 0.9645
Epoch 23/40
 - 5s - loss: 0.0353 - acc: 0.9780 - val_loss: 0.0987 - val_acc: 0.9642
Epoch 24/40
 - 5s - loss: 0.0351 - acc: 0.9779 - val_loss: 0.0970 - val_acc: 0.9640
Epoch 25/40
 - 5s - loss: 0.0350 - acc: 0.9779 - val_loss: 0.1023 - val_acc: 0.9633
Epoch 26/40
 - 5s - loss: 0.0347 - acc: 0.9780 - val_loss: 0.0968 - val_acc: 0.9638
Epoch 27/40
 - 5s - loss: 0.0345 - acc: 0.9780 - val_loss: 0.1014 - val_acc: 0.9642
Epoch 28/40
 - 5s - loss: 0.0345 - acc: 0.9782 - val_loss: 0.1019 - val_acc: 0.9578
Epoch 29/40
 - 5s - loss: 0.0345 - acc: 0.9782 - val_loss: 0.1028 - val_acc: 0.9614
Epoch 30/40
 - 5s - loss: 0.0344 - acc: 0.9784 - val_loss: 0.1021 - val_acc: 0.9631
Epoch 31/40
 - 5s - loss: 0.0338 - acc: 0.9778 - val_loss: 0.1053 - val_acc: 0.9595
Epoch 32/40
 - 5s - loss: 0.0337 - acc: 0.9789 - val_loss: 0.1037 - val_acc: 0.9629
Epoch 33/40
 - 5s - loss: 0.0339 - acc: 0.9784 - val_loss: 0.1090 - val_acc: 0.9607
Epoch 34/40
 - 5s - loss: 0.0337 - acc: 0.9788 - val_loss: 0.1120 - val_acc: 0.9566
Epoch 35/40
 - 5s - loss: 0.0334 - acc: 0.9785 - val_loss: 0.1087 - val_acc: 0.9592
Epoch 36/40
 - 5s - loss: 0.0333 - acc: 0.9781 - val_loss: 0.1132 - val_acc: 0.9593
Epoch 37/40
 - 5s - loss: 0.0333 - acc: 0.9785 - val_loss: 0.1084 - val_acc: 0.9586
Epoch 38/40
 - 5s - loss: 0.0333 - acc: 0.9784 - val_loss: 0.1135 - val_acc: 0.9590
Epoch 39/40
 - 5s - loss: 0.0334 - acc: 0.9782 - val_loss: 0.1158 - val_acc: 0.9564
Epoch 40/40
 - 5s - loss: 0.0330 - acc: 0.9788 - val_loss: 0.1113 - val_acc: 0.9590
Epoch 1/40
 - 0s - loss: 0.0975 - acc: 0.9640
Epoch 2/40
 - 0s - loss: 0.0785 - acc: 0.9672
Epoch 3/40
 - 0s - loss: 0.0682 - acc: 0.9702
Epoch 4/40
 - 0s - loss: 0.0664 - acc: 0.9700
Epoch 5/40
 - 0s - loss: 0.0612 - acc: 0.9704
Epoch 6/40
 - 0s - loss: 0.0576 - acc: 0.9706
Epoch 7/40
 - 0s - loss: 0.0552 - acc: 0.9723
Epoch 8/40
 - 0s - loss: 0.0524 - acc: 0.9728
Epoch 9/40
 - 0s - loss: 0.0514 - acc: 0.9733
Epoch 10/40
 - 0s - loss: 0.0482 - acc: 0.9735
Epoch 11/40
 - 0s - loss: 0.0476 - acc: 0.9742
Epoch 12/40
 - 0s - loss: 0.0459 - acc: 0.9737
Epoch 13/40
 - 0s - loss: 0.0434 - acc: 0.9752
Epoch 14/40
 - 0s - loss: 0.0429 - acc: 0.9760
Epoch 15/40
 - 0s - loss: 0.0419 - acc: 0.9763
Epoch 16/40
 - 0s - loss: 0.0412 - acc: 0.9761
Epoch 17/40
 - 0s - loss: 0.0397 - acc: 0.9765
Epoch 18/40
 - 0s - loss: 0.0388 - acc: 0.9769
Epoch 19/40
 - 0s - loss: 0.0383 - acc: 0.9774
Epoch 20/40
 - 0s - loss: 0.0377 - acc: 0.9763
Epoch 21/40
 - 0s - loss: 0.0379 - acc: 0.9779
Epoch 22/40
 - 0s - loss: 0.0370 - acc: 0.9790
Epoch 23/40
 - 0s - loss: 0.0369 - acc: 0.9771
Epoch 24/40
 - 0s - loss: 0.0366 - acc: 0.9777
Epoch 25/40
 - 0s - loss: 0.0364 - acc: 0.9772
Epoch 26/40
 - 0s - loss: 0.0360 - acc: 0.9779
Epoch 27/40
 - 0s - loss: 0.0352 - acc: 0.9785
Epoch 28/40
 - 0s - loss: 0.0357 - acc: 0.9784
Epoch 29/40
 - 0s - loss: 0.0358 - acc: 0.9764
Epoch 30/40
 - 0s - loss: 0.0359 - acc: 0.9782
Epoch 31/40
 - 0s - loss: 0.0349 - acc: 0.9778
Epoch 32/40
 - 0s - loss: 0.0353 - acc: 0.9777
Epoch 33/40
 - 0s - loss: 0.0344 - acc: 0.9791
Epoch 34/40
 - 0s - loss: 0.0341 - acc: 0.9784
Epoch 35/40
 - 0s - loss: 0.0345 - acc: 0.9777
Epoch 36/40
 - 0s - loss: 0.0344 - acc: 0.9784
Epoch 37/40
 - 0s - loss: 0.0339 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0343 - acc: 0.9794
Epoch 39/40
 - 0s - loss: 0.0344 - acc: 0.9788
Epoch 40/40
 - 0s - loss: 0.0337 - acc: 0.9790
# Training time = 0:03:46.517871
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_277 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_278 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_277 (Embedding)       (None, 4, 48)        705264      input_277[0][0]                  
__________________________________________________________________________________________________
embedding_278 (Embedding)       (None, 4, 24)        5640        input_278[0][0]                  
__________________________________________________________________________________________________
flatten_277 (Flatten)           (None, 192)          0           embedding_277[0][0]              
__________________________________________________________________________________________________
flatten_278 (Flatten)           (None, 96)           0           embedding_278[0][0]              
__________________________________________________________________________________________________
concatenate_139 (Concatenate)   (None, 288)          0           flatten_277[0][0]                
                                                                 flatten_278[0][0]                
__________________________________________________________________________________________________
dense_277 (Dense)               (None, 24)           6936        concatenate_139[0][0]            
__________________________________________________________________________________________________
dropout_139 (Dropout)           (None, 24)           0           dense_277[0][0]                  
__________________________________________________________________________________________________
dense_278 (Dense)               (None, 8)            200         dropout_139[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1152 - acc: 0.9587 - val_loss: 0.0737 - val_acc: 0.9674
Epoch 2/40
 - 5s - loss: 0.0662 - acc: 0.9690 - val_loss: 0.0703 - val_acc: 0.9679
Epoch 3/40
 - 5s - loss: 0.0574 - acc: 0.9717 - val_loss: 0.0685 - val_acc: 0.9681
Epoch 4/40
 - 5s - loss: 0.0522 - acc: 0.9731 - val_loss: 0.0716 - val_acc: 0.9677
Epoch 5/40
 - 5s - loss: 0.0484 - acc: 0.9746 - val_loss: 0.0738 - val_acc: 0.9680
Epoch 6/40
 - 5s - loss: 0.0461 - acc: 0.9748 - val_loss: 0.0743 - val_acc: 0.9678
Epoch 7/40
 - 5s - loss: 0.0443 - acc: 0.9752 - val_loss: 0.0764 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0428 - acc: 0.9759 - val_loss: 0.0824 - val_acc: 0.9671
Epoch 9/40
 - 5s - loss: 0.0417 - acc: 0.9758 - val_loss: 0.0800 - val_acc: 0.9669
Epoch 10/40
 - 5s - loss: 0.0408 - acc: 0.9765 - val_loss: 0.0819 - val_acc: 0.9670
Epoch 11/40
 - 5s - loss: 0.0404 - acc: 0.9764 - val_loss: 0.0832 - val_acc: 0.9672
Epoch 12/40
 - 5s - loss: 0.0395 - acc: 0.9766 - val_loss: 0.0829 - val_acc: 0.9674
Epoch 13/40
 - 5s - loss: 0.0391 - acc: 0.9770 - val_loss: 0.0878 - val_acc: 0.9673
Epoch 14/40
 - 5s - loss: 0.0387 - acc: 0.9768 - val_loss: 0.0838 - val_acc: 0.9673
Epoch 15/40
 - 5s - loss: 0.0384 - acc: 0.9768 - val_loss: 0.0895 - val_acc: 0.9671
Epoch 16/40
 - 5s - loss: 0.0377 - acc: 0.9775 - val_loss: 0.0916 - val_acc: 0.9659
Epoch 17/40
 - 5s - loss: 0.0374 - acc: 0.9769 - val_loss: 0.0886 - val_acc: 0.9672
Epoch 18/40
 - 5s - loss: 0.0367 - acc: 0.9775 - val_loss: 0.0920 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0367 - acc: 0.9772 - val_loss: 0.0988 - val_acc: 0.9672
Epoch 20/40
 - 5s - loss: 0.0365 - acc: 0.9774 - val_loss: 0.0918 - val_acc: 0.9669
Epoch 21/40
 - 5s - loss: 0.0359 - acc: 0.9777 - val_loss: 0.0980 - val_acc: 0.9663
Epoch 22/40
 - 5s - loss: 0.0357 - acc: 0.9777 - val_loss: 0.0978 - val_acc: 0.9673
Epoch 23/40
 - 5s - loss: 0.0355 - acc: 0.9777 - val_loss: 0.0984 - val_acc: 0.9657
Epoch 24/40
 - 5s - loss: 0.0354 - acc: 0.9780 - val_loss: 0.0947 - val_acc: 0.9657
Epoch 25/40
 - 5s - loss: 0.0352 - acc: 0.9774 - val_loss: 0.0994 - val_acc: 0.9672
Epoch 26/40
 - 5s - loss: 0.0349 - acc: 0.9779 - val_loss: 0.0996 - val_acc: 0.9660
Epoch 27/40
 - 5s - loss: 0.0348 - acc: 0.9777 - val_loss: 0.1011 - val_acc: 0.9665
Epoch 28/40
 - 5s - loss: 0.0346 - acc: 0.9775 - val_loss: 0.1027 - val_acc: 0.9673
Epoch 29/40
 - 5s - loss: 0.0346 - acc: 0.9778 - val_loss: 0.1040 - val_acc: 0.9671
Epoch 30/40
 - 5s - loss: 0.0343 - acc: 0.9778 - val_loss: 0.1008 - val_acc: 0.9654
Epoch 31/40
 - 5s - loss: 0.0340 - acc: 0.9780 - val_loss: 0.1029 - val_acc: 0.9655
Epoch 32/40
 - 5s - loss: 0.0342 - acc: 0.9776 - val_loss: 0.1026 - val_acc: 0.9657
Epoch 33/40
 - 5s - loss: 0.0338 - acc: 0.9782 - val_loss: 0.1084 - val_acc: 0.9667
Epoch 34/40
 - 5s - loss: 0.0338 - acc: 0.9782 - val_loss: 0.1051 - val_acc: 0.9658
Epoch 35/40
 - 5s - loss: 0.0335 - acc: 0.9784 - val_loss: 0.1056 - val_acc: 0.9665
Epoch 36/40
 - 5s - loss: 0.0334 - acc: 0.9783 - val_loss: 0.1101 - val_acc: 0.9657
Epoch 37/40
 - 5s - loss: 0.0332 - acc: 0.9787 - val_loss: 0.1075 - val_acc: 0.9658
Epoch 38/40
 - 5s - loss: 0.0333 - acc: 0.9779 - val_loss: 0.1088 - val_acc: 0.9643
Epoch 39/40
 - 5s - loss: 0.0334 - acc: 0.9782 - val_loss: 0.1042 - val_acc: 0.9680
Epoch 40/40
 - 5s - loss: 0.0332 - acc: 0.9784 - val_loss: 0.1088 - val_acc: 0.9675
Epoch 1/40
 - 0s - loss: 0.0927 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.0750 - acc: 0.9675
Epoch 3/40
 - 0s - loss: 0.0665 - acc: 0.9689
Epoch 4/40
 - 0s - loss: 0.0626 - acc: 0.9704
Epoch 5/40
 - 0s - loss: 0.0575 - acc: 0.9713
Epoch 6/40
 - 0s - loss: 0.0534 - acc: 0.9716
Epoch 7/40
 - 0s - loss: 0.0530 - acc: 0.9737
Epoch 8/40
 - 0s - loss: 0.0498 - acc: 0.9739
Epoch 9/40
 - 0s - loss: 0.0479 - acc: 0.9741
Epoch 10/40
 - 0s - loss: 0.0454 - acc: 0.9743
Epoch 11/40
 - 0s - loss: 0.0439 - acc: 0.9742
Epoch 12/40
 - 0s - loss: 0.0429 - acc: 0.9753
Epoch 13/40
 - 0s - loss: 0.0411 - acc: 0.9765
Epoch 14/40
 - 0s - loss: 0.0406 - acc: 0.9770
Epoch 15/40
 - 0s - loss: 0.0396 - acc: 0.9768
Epoch 16/40
 - 0s - loss: 0.0393 - acc: 0.9769
Epoch 17/40
 - 0s - loss: 0.0396 - acc: 0.9761
Epoch 18/40
 - 0s - loss: 0.0381 - acc: 0.9778
Epoch 19/40
 - 0s - loss: 0.0379 - acc: 0.9776
Epoch 20/40
 - 0s - loss: 0.0375 - acc: 0.9779
Epoch 21/40
 - 0s - loss: 0.0362 - acc: 0.9785
Epoch 22/40
 - 0s - loss: 0.0363 - acc: 0.9784
Epoch 23/40
 - 0s - loss: 0.0362 - acc: 0.9780
Epoch 24/40
 - 0s - loss: 0.0361 - acc: 0.9767
Epoch 25/40
 - 0s - loss: 0.0351 - acc: 0.9780
Epoch 26/40
 - 0s - loss: 0.0346 - acc: 0.9789
Epoch 27/40
 - 0s - loss: 0.0350 - acc: 0.9787
Epoch 28/40
 - 0s - loss: 0.0352 - acc: 0.9778
Epoch 29/40
 - 0s - loss: 0.0340 - acc: 0.9787
Epoch 30/40
 - 0s - loss: 0.0332 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0347 - acc: 0.9786
Epoch 32/40
 - 0s - loss: 0.0345 - acc: 0.9784
Epoch 33/40
 - 0s - loss: 0.0334 - acc: 0.9784
Epoch 34/40
 - 0s - loss: 0.0342 - acc: 0.9798
Epoch 35/40
 - 0s - loss: 0.0334 - acc: 0.9788
Epoch 36/40
 - 0s - loss: 0.0340 - acc: 0.9793
Epoch 37/40
 - 0s - loss: 0.0341 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0339 - acc: 0.9786
Epoch 39/40
 - 0s - loss: 0.0336 - acc: 0.9783
Epoch 40/40
 - 0s - loss: 0.0330 - acc: 0.9791
# Training time = 0:03:46.705889
# F-Score(Ordinary) = 0.009, Recall: 0.4, Precision: 0.004
# F-Score(lvc) = 0.029, Recall: 0.4, Precision: 0.015
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_279 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_280 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_279 (Embedding)       (None, 4, 48)        705264      input_279[0][0]                  
__________________________________________________________________________________________________
embedding_280 (Embedding)       (None, 4, 24)        5640        input_280[0][0]                  
__________________________________________________________________________________________________
flatten_279 (Flatten)           (None, 192)          0           embedding_279[0][0]              
__________________________________________________________________________________________________
flatten_280 (Flatten)           (None, 96)           0           embedding_280[0][0]              
__________________________________________________________________________________________________
concatenate_140 (Concatenate)   (None, 288)          0           flatten_279[0][0]                
                                                                 flatten_280[0][0]                
__________________________________________________________________________________________________
dense_279 (Dense)               (None, 24)           6936        concatenate_140[0][0]            
__________________________________________________________________________________________________
dropout_140 (Dropout)           (None, 24)           0           dense_279[0][0]                  
__________________________________________________________________________________________________
dense_280 (Dense)               (None, 8)            200         dropout_140[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1146 - acc: 0.9585 - val_loss: 0.0747 - val_acc: 0.9670
Epoch 2/40
 - 5s - loss: 0.0662 - acc: 0.9697 - val_loss: 0.0712 - val_acc: 0.9682
Epoch 3/40
 - 5s - loss: 0.0576 - acc: 0.9715 - val_loss: 0.0714 - val_acc: 0.9678
Epoch 4/40
 - 5s - loss: 0.0526 - acc: 0.9727 - val_loss: 0.0693 - val_acc: 0.9685
Epoch 5/40
 - 5s - loss: 0.0498 - acc: 0.9740 - val_loss: 0.0725 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0470 - acc: 0.9748 - val_loss: 0.0739 - val_acc: 0.9686
Epoch 7/40
 - 5s - loss: 0.0453 - acc: 0.9749 - val_loss: 0.0770 - val_acc: 0.9679
Epoch 8/40
 - 5s - loss: 0.0441 - acc: 0.9754 - val_loss: 0.0792 - val_acc: 0.9674
Epoch 9/40
 - 5s - loss: 0.0427 - acc: 0.9760 - val_loss: 0.0806 - val_acc: 0.9670
Epoch 10/40
 - 5s - loss: 0.0417 - acc: 0.9756 - val_loss: 0.0828 - val_acc: 0.9673
Epoch 11/40
 - 5s - loss: 0.0410 - acc: 0.9762 - val_loss: 0.0856 - val_acc: 0.9674
Epoch 12/40
 - 5s - loss: 0.0404 - acc: 0.9761 - val_loss: 0.0839 - val_acc: 0.9675
Epoch 13/40
 - 5s - loss: 0.0396 - acc: 0.9769 - val_loss: 0.0861 - val_acc: 0.9669
Epoch 14/40
 - 5s - loss: 0.0393 - acc: 0.9766 - val_loss: 0.0869 - val_acc: 0.9681
Epoch 15/40
 - 5s - loss: 0.0387 - acc: 0.9771 - val_loss: 0.0831 - val_acc: 0.9677
Epoch 16/40
 - 5s - loss: 0.0383 - acc: 0.9767 - val_loss: 0.0881 - val_acc: 0.9677
Epoch 17/40
 - 5s - loss: 0.0377 - acc: 0.9772 - val_loss: 0.0885 - val_acc: 0.9676
Epoch 18/40
 - 5s - loss: 0.0375 - acc: 0.9770 - val_loss: 0.0915 - val_acc: 0.9661
Epoch 19/40
 - 5s - loss: 0.0370 - acc: 0.9771 - val_loss: 0.0908 - val_acc: 0.9663
Epoch 20/40
 - 5s - loss: 0.0369 - acc: 0.9771 - val_loss: 0.0902 - val_acc: 0.9675
Epoch 21/40
 - 5s - loss: 0.0366 - acc: 0.9772 - val_loss: 0.0943 - val_acc: 0.9644
Epoch 22/40
 - 5s - loss: 0.0364 - acc: 0.9774 - val_loss: 0.0945 - val_acc: 0.9626
Epoch 23/40
 - 5s - loss: 0.0360 - acc: 0.9775 - val_loss: 0.0934 - val_acc: 0.9677
Epoch 24/40
 - 5s - loss: 0.0353 - acc: 0.9771 - val_loss: 0.0945 - val_acc: 0.9653
Epoch 25/40
 - 5s - loss: 0.0353 - acc: 0.9780 - val_loss: 0.0960 - val_acc: 0.9633
Epoch 26/40
 - 5s - loss: 0.0353 - acc: 0.9780 - val_loss: 0.1006 - val_acc: 0.9666
Epoch 27/40
 - 5s - loss: 0.0349 - acc: 0.9785 - val_loss: 0.0978 - val_acc: 0.9670
Epoch 28/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.0999 - val_acc: 0.9664
Epoch 29/40
 - 5s - loss: 0.0347 - acc: 0.9781 - val_loss: 0.1042 - val_acc: 0.9637
Epoch 30/40
 - 5s - loss: 0.0345 - acc: 0.9784 - val_loss: 0.1015 - val_acc: 0.9678
Epoch 31/40
 - 5s - loss: 0.0343 - acc: 0.9783 - val_loss: 0.1027 - val_acc: 0.9658
Epoch 32/40
 - 5s - loss: 0.0341 - acc: 0.9781 - val_loss: 0.1040 - val_acc: 0.9628
Epoch 33/40
 - 5s - loss: 0.0341 - acc: 0.9778 - val_loss: 0.1047 - val_acc: 0.9641
Epoch 34/40
 - 5s - loss: 0.0340 - acc: 0.9783 - val_loss: 0.1068 - val_acc: 0.9645
Epoch 35/40
 - 5s - loss: 0.0339 - acc: 0.9782 - val_loss: 0.1040 - val_acc: 0.9675
Epoch 36/40
 - 5s - loss: 0.0338 - acc: 0.9783 - val_loss: 0.1122 - val_acc: 0.9658
Epoch 37/40
 - 5s - loss: 0.0339 - acc: 0.9780 - val_loss: 0.1092 - val_acc: 0.9663
Epoch 38/40
 - 5s - loss: 0.0335 - acc: 0.9784 - val_loss: 0.1064 - val_acc: 0.9654
Epoch 39/40
 - 5s - loss: 0.0334 - acc: 0.9785 - val_loss: 0.1089 - val_acc: 0.9634
Epoch 40/40
 - 5s - loss: 0.0336 - acc: 0.9783 - val_loss: 0.1096 - val_acc: 0.9675
Epoch 1/40
 - 0s - loss: 0.0963 - acc: 0.9661
Epoch 2/40
 - 0s - loss: 0.0765 - acc: 0.9677
Epoch 3/40
 - 0s - loss: 0.0663 - acc: 0.9685
Epoch 4/40
 - 0s - loss: 0.0638 - acc: 0.9704
Epoch 5/40
 - 0s - loss: 0.0593 - acc: 0.9708
Epoch 6/40
 - 0s - loss: 0.0567 - acc: 0.9730
Epoch 7/40
 - 0s - loss: 0.0529 - acc: 0.9731
Epoch 8/40
 - 0s - loss: 0.0514 - acc: 0.9728
Epoch 9/40
 - 0s - loss: 0.0489 - acc: 0.9730
Epoch 10/40
 - 0s - loss: 0.0479 - acc: 0.9737
Epoch 11/40
 - 0s - loss: 0.0473 - acc: 0.9745
Epoch 12/40
 - 0s - loss: 0.0449 - acc: 0.9746
Epoch 13/40
 - 0s - loss: 0.0442 - acc: 0.9759
Epoch 14/40
 - 0s - loss: 0.0423 - acc: 0.9760
Epoch 15/40
 - 0s - loss: 0.0418 - acc: 0.9775
Epoch 16/40
 - 0s - loss: 0.0415 - acc: 0.9768
Epoch 17/40
 - 0s - loss: 0.0401 - acc: 0.9765
Epoch 18/40
 - 0s - loss: 0.0400 - acc: 0.9765
Epoch 19/40
 - 0s - loss: 0.0389 - acc: 0.9765
Epoch 20/40
 - 0s - loss: 0.0380 - acc: 0.9773
Epoch 21/40
 - 0s - loss: 0.0384 - acc: 0.9766
Epoch 22/40
 - 0s - loss: 0.0370 - acc: 0.9781
Epoch 23/40
 - 0s - loss: 0.0373 - acc: 0.9774
Epoch 24/40
 - 0s - loss: 0.0367 - acc: 0.9778
Epoch 25/40
 - 0s - loss: 0.0368 - acc: 0.9776
Epoch 26/40
 - 0s - loss: 0.0361 - acc: 0.9783
Epoch 27/40
 - 0s - loss: 0.0364 - acc: 0.9776
Epoch 28/40
 - 0s - loss: 0.0360 - acc: 0.9775
Epoch 29/40
 - 0s - loss: 0.0353 - acc: 0.9783
Epoch 30/40
 - 0s - loss: 0.0355 - acc: 0.9791
Epoch 31/40
 - 0s - loss: 0.0352 - acc: 0.9784
Epoch 32/40
 - 0s - loss: 0.0347 - acc: 0.9788
Epoch 33/40
 - 0s - loss: 0.0344 - acc: 0.9791
Epoch 34/40
 - 0s - loss: 0.0339 - acc: 0.9795
Epoch 35/40
 - 0s - loss: 0.0340 - acc: 0.9798
Epoch 36/40
 - 0s - loss: 0.0338 - acc: 0.9798
Epoch 37/40
 - 0s - loss: 0.0336 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0343 - acc: 0.9797
Epoch 39/40
 - 0s - loss: 0.0338 - acc: 0.9798
Epoch 40/40
 - 0s - loss: 0.0346 - acc: 0.9784
# Training time = 0:03:47.443142
# F-Score(Ordinary) = 0.554, Recall: 0.772, Precision: 0.432
# F-Score(lvc) = 0.483, Recall: 0.646, Precision: 0.386
# F-Score(ireflv) = 0.769, Recall: 0.76, Precision: 0.779
# F-Score(id) = 0.368, Recall: 0.957, Precision: 0.228
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_281 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_282 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_281 (Embedding)       (None, 4, 48)        705264      input_281[0][0]                  
__________________________________________________________________________________________________
embedding_282 (Embedding)       (None, 4, 24)        5640        input_282[0][0]                  
__________________________________________________________________________________________________
flatten_281 (Flatten)           (None, 192)          0           embedding_281[0][0]              
__________________________________________________________________________________________________
flatten_282 (Flatten)           (None, 96)           0           embedding_282[0][0]              
__________________________________________________________________________________________________
concatenate_141 (Concatenate)   (None, 288)          0           flatten_281[0][0]                
                                                                 flatten_282[0][0]                
__________________________________________________________________________________________________
dense_281 (Dense)               (None, 24)           6936        concatenate_141[0][0]            
__________________________________________________________________________________________________
dropout_141 (Dropout)           (None, 24)           0           dense_281[0][0]                  
__________________________________________________________________________________________________
dense_282 (Dense)               (None, 8)            200         dropout_141[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0963 - acc: 0.9626 - val_loss: 0.0746 - val_acc: 0.9675
Epoch 2/40
 - 5s - loss: 0.0625 - acc: 0.9701 - val_loss: 0.0696 - val_acc: 0.9685
Epoch 3/40
 - 5s - loss: 0.0533 - acc: 0.9725 - val_loss: 0.0715 - val_acc: 0.9671
Epoch 4/40
 - 5s - loss: 0.0488 - acc: 0.9741 - val_loss: 0.0745 - val_acc: 0.9673
Epoch 5/40
 - 5s - loss: 0.0458 - acc: 0.9742 - val_loss: 0.0805 - val_acc: 0.9677
Epoch 6/40
 - 5s - loss: 0.0439 - acc: 0.9751 - val_loss: 0.0795 - val_acc: 0.9672
Epoch 7/40
 - 5s - loss: 0.0425 - acc: 0.9757 - val_loss: 0.0834 - val_acc: 0.9666
Epoch 8/40
 - 5s - loss: 0.0410 - acc: 0.9760 - val_loss: 0.0853 - val_acc: 0.9686
Epoch 9/40
 - 5s - loss: 0.0400 - acc: 0.9770 - val_loss: 0.0879 - val_acc: 0.9671
Epoch 10/40
 - 5s - loss: 0.0395 - acc: 0.9763 - val_loss: 0.0901 - val_acc: 0.9673
Epoch 11/40
 - 5s - loss: 0.0385 - acc: 0.9770 - val_loss: 0.0977 - val_acc: 0.9648
Epoch 12/40
 - 5s - loss: 0.0382 - acc: 0.9770 - val_loss: 0.0905 - val_acc: 0.9673
Epoch 13/40
 - 5s - loss: 0.0377 - acc: 0.9769 - val_loss: 0.0948 - val_acc: 0.9657
Epoch 14/40
 - 5s - loss: 0.0371 - acc: 0.9773 - val_loss: 0.0968 - val_acc: 0.9668
Epoch 15/40
 - 5s - loss: 0.0366 - acc: 0.9776 - val_loss: 0.0988 - val_acc: 0.9667
Epoch 16/40
 - 5s - loss: 0.0363 - acc: 0.9773 - val_loss: 0.1012 - val_acc: 0.9646
Epoch 17/40
 - 5s - loss: 0.0364 - acc: 0.9776 - val_loss: 0.1009 - val_acc: 0.9670
Epoch 18/40
 - 5s - loss: 0.0359 - acc: 0.9776 - val_loss: 0.1017 - val_acc: 0.9641
Epoch 19/40
 - 5s - loss: 0.0357 - acc: 0.9775 - val_loss: 0.1028 - val_acc: 0.9666
Epoch 20/40
 - 5s - loss: 0.0354 - acc: 0.9776 - val_loss: 0.1050 - val_acc: 0.9668
Epoch 21/40
 - 5s - loss: 0.0351 - acc: 0.9781 - val_loss: 0.1044 - val_acc: 0.9664
Epoch 22/40
 - 5s - loss: 0.0352 - acc: 0.9780 - val_loss: 0.1064 - val_acc: 0.9649
Epoch 23/40
 - 5s - loss: 0.0347 - acc: 0.9781 - val_loss: 0.1021 - val_acc: 0.9661
Epoch 24/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1100 - val_acc: 0.9638
Epoch 25/40
 - 5s - loss: 0.0348 - acc: 0.9780 - val_loss: 0.1047 - val_acc: 0.9678
Epoch 26/40
 - 5s - loss: 0.0344 - acc: 0.9786 - val_loss: 0.1138 - val_acc: 0.9651
Epoch 27/40
 - 5s - loss: 0.0345 - acc: 0.9781 - val_loss: 0.1106 - val_acc: 0.9663
Epoch 28/40
 - 5s - loss: 0.0338 - acc: 0.9786 - val_loss: 0.1086 - val_acc: 0.9643
Epoch 29/40
 - 5s - loss: 0.0342 - acc: 0.9784 - val_loss: 0.1113 - val_acc: 0.9655
Epoch 30/40
 - 5s - loss: 0.0338 - acc: 0.9785 - val_loss: 0.1143 - val_acc: 0.9670
Epoch 31/40
 - 5s - loss: 0.0340 - acc: 0.9783 - val_loss: 0.1107 - val_acc: 0.9661
Epoch 32/40
 - 5s - loss: 0.0338 - acc: 0.9785 - val_loss: 0.1156 - val_acc: 0.9645
Epoch 33/40
 - 5s - loss: 0.0338 - acc: 0.9783 - val_loss: 0.1166 - val_acc: 0.9666
Epoch 34/40
 - 5s - loss: 0.0334 - acc: 0.9786 - val_loss: 0.1157 - val_acc: 0.9636
Epoch 35/40
 - 5s - loss: 0.0336 - acc: 0.9786 - val_loss: 0.1165 - val_acc: 0.9661
Epoch 36/40
 - 5s - loss: 0.0338 - acc: 0.9786 - val_loss: 0.1131 - val_acc: 0.9658
Epoch 37/40
 - 5s - loss: 0.0334 - acc: 0.9784 - val_loss: 0.1151 - val_acc: 0.9666
Epoch 38/40
 - 5s - loss: 0.0334 - acc: 0.9785 - val_loss: 0.1182 - val_acc: 0.9657
Epoch 39/40
 - 5s - loss: 0.0336 - acc: 0.9785 - val_loss: 0.1170 - val_acc: 0.9662
Epoch 40/40
 - 5s - loss: 0.0335 - acc: 0.9787 - val_loss: 0.1204 - val_acc: 0.9662
Epoch 1/40
 - 0s - loss: 0.1046 - acc: 0.9641
Epoch 2/40
 - 0s - loss: 0.0779 - acc: 0.9675
Epoch 3/40
 - 1s - loss: 0.0707 - acc: 0.9683
Epoch 4/40
 - 1s - loss: 0.0634 - acc: 0.9698
Epoch 5/40
 - 0s - loss: 0.0607 - acc: 0.9700
Epoch 6/40
 - 0s - loss: 0.0571 - acc: 0.9717
Epoch 7/40
 - 0s - loss: 0.0542 - acc: 0.9721
Epoch 8/40
 - 1s - loss: 0.0511 - acc: 0.9736
Epoch 9/40
 - 0s - loss: 0.0497 - acc: 0.9749
Epoch 10/40
 - 0s - loss: 0.0489 - acc: 0.9755
Epoch 11/40
 - 0s - loss: 0.0472 - acc: 0.9746
Epoch 12/40
 - 0s - loss: 0.0459 - acc: 0.9751
Epoch 13/40
 - 0s - loss: 0.0438 - acc: 0.9766
Epoch 14/40
 - 0s - loss: 0.0439 - acc: 0.9761
Epoch 15/40
 - 0s - loss: 0.0416 - acc: 0.9760
Epoch 16/40
 - 1s - loss: 0.0421 - acc: 0.9762
Epoch 17/40
 - 0s - loss: 0.0401 - acc: 0.9768
Epoch 18/40
 - 0s - loss: 0.0394 - acc: 0.9767
Epoch 19/40
 - 1s - loss: 0.0392 - acc: 0.9770
Epoch 20/40
 - 0s - loss: 0.0395 - acc: 0.9765
Epoch 21/40
 - 0s - loss: 0.0378 - acc: 0.9778
Epoch 22/40
 - 1s - loss: 0.0388 - acc: 0.9778
Epoch 23/40
 - 1s - loss: 0.0378 - acc: 0.9781
Epoch 24/40
 - 1s - loss: 0.0388 - acc: 0.9767
Epoch 25/40
 - 1s - loss: 0.0383 - acc: 0.9768
Epoch 26/40
 - 1s - loss: 0.0373 - acc: 0.9775
Epoch 27/40
 - 1s - loss: 0.0377 - acc: 0.9775
Epoch 28/40
 - 0s - loss: 0.0368 - acc: 0.9774
Epoch 29/40
 - 0s - loss: 0.0366 - acc: 0.9786
Epoch 30/40
 - 1s - loss: 0.0351 - acc: 0.9784
Epoch 31/40
 - 0s - loss: 0.0350 - acc: 0.9796
Epoch 32/40
 - 0s - loss: 0.0357 - acc: 0.9805
Epoch 33/40
 - 1s - loss: 0.0357 - acc: 0.9781
Epoch 34/40
 - 0s - loss: 0.0352 - acc: 0.9795
Epoch 35/40
 - 0s - loss: 0.0355 - acc: 0.9785
Epoch 36/40
 - 0s - loss: 0.0358 - acc: 0.9779
Epoch 37/40
 - 0s - loss: 0.0348 - acc: 0.9790
Epoch 38/40
 - 0s - loss: 0.0354 - acc: 0.9778
Epoch 39/40
 - 1s - loss: 0.0354 - acc: 0.9774
Epoch 40/40
 - 1s - loss: 0.0345 - acc: 0.9774
# Training time = 0:03:48.000454
# F-Score(Ordinary) = 0.193, Recall: 0.714, Precision: 0.112
# F-Score(ireflv) = 0.516, Recall: 0.721, Precision: 0.402
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_283 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_284 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_283 (Embedding)       (None, 4, 48)        705264      input_283[0][0]                  
__________________________________________________________________________________________________
embedding_284 (Embedding)       (None, 4, 24)        5640        input_284[0][0]                  
__________________________________________________________________________________________________
flatten_283 (Flatten)           (None, 192)          0           embedding_283[0][0]              
__________________________________________________________________________________________________
flatten_284 (Flatten)           (None, 96)           0           embedding_284[0][0]              
__________________________________________________________________________________________________
concatenate_142 (Concatenate)   (None, 288)          0           flatten_283[0][0]                
                                                                 flatten_284[0][0]                
__________________________________________________________________________________________________
dense_283 (Dense)               (None, 24)           6936        concatenate_142[0][0]            
__________________________________________________________________________________________________
dropout_142 (Dropout)           (None, 24)           0           dense_283[0][0]                  
__________________________________________________________________________________________________
dense_284 (Dense)               (None, 8)            200         dropout_142[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0968 - acc: 0.9627 - val_loss: 0.0704 - val_acc: 0.9684
Epoch 2/40
 - 5s - loss: 0.0614 - acc: 0.9702 - val_loss: 0.0730 - val_acc: 0.9673
Epoch 3/40
 - 5s - loss: 0.0540 - acc: 0.9728 - val_loss: 0.0733 - val_acc: 0.9672
Epoch 4/40
 - 5s - loss: 0.0500 - acc: 0.9742 - val_loss: 0.0739 - val_acc: 0.9685
Epoch 5/40
 - 5s - loss: 0.0470 - acc: 0.9750 - val_loss: 0.0779 - val_acc: 0.9681
Epoch 6/40
 - 5s - loss: 0.0448 - acc: 0.9755 - val_loss: 0.0839 - val_acc: 0.9669
Epoch 7/40
 - 5s - loss: 0.0430 - acc: 0.9756 - val_loss: 0.0806 - val_acc: 0.9678
Epoch 8/40
 - 5s - loss: 0.0415 - acc: 0.9763 - val_loss: 0.0837 - val_acc: 0.9676
Epoch 9/40
 - 5s - loss: 0.0407 - acc: 0.9761 - val_loss: 0.0852 - val_acc: 0.9676
Epoch 10/40
 - 5s - loss: 0.0401 - acc: 0.9762 - val_loss: 0.0860 - val_acc: 0.9687
Epoch 11/40
 - 5s - loss: 0.0393 - acc: 0.9767 - val_loss: 0.0917 - val_acc: 0.9676
Epoch 12/40
 - 5s - loss: 0.0387 - acc: 0.9769 - val_loss: 0.0917 - val_acc: 0.9674
Epoch 13/40
 - 5s - loss: 0.0384 - acc: 0.9771 - val_loss: 0.0904 - val_acc: 0.9671
Epoch 14/40
 - 5s - loss: 0.0378 - acc: 0.9767 - val_loss: 0.0978 - val_acc: 0.9676
Epoch 15/40
 - 5s - loss: 0.0374 - acc: 0.9772 - val_loss: 0.0899 - val_acc: 0.9687
Epoch 16/40
 - 5s - loss: 0.0371 - acc: 0.9772 - val_loss: 0.0970 - val_acc: 0.9661
Epoch 17/40
 - 5s - loss: 0.0365 - acc: 0.9771 - val_loss: 0.0954 - val_acc: 0.9649
Epoch 18/40
 - 5s - loss: 0.0365 - acc: 0.9779 - val_loss: 0.0985 - val_acc: 0.9656
Epoch 19/40
 - 5s - loss: 0.0363 - acc: 0.9775 - val_loss: 0.0947 - val_acc: 0.9676
Epoch 20/40
 - 5s - loss: 0.0361 - acc: 0.9773 - val_loss: 0.0964 - val_acc: 0.9673
Epoch 21/40
 - 5s - loss: 0.0355 - acc: 0.9777 - val_loss: 0.0982 - val_acc: 0.9667
Epoch 22/40
 - 5s - loss: 0.0356 - acc: 0.9781 - val_loss: 0.1018 - val_acc: 0.9663
Epoch 23/40
 - 5s - loss: 0.0355 - acc: 0.9776 - val_loss: 0.1005 - val_acc: 0.9675
Epoch 24/40
 - 5s - loss: 0.0351 - acc: 0.9781 - val_loss: 0.1049 - val_acc: 0.9666
Epoch 25/40
 - 5s - loss: 0.0352 - acc: 0.9778 - val_loss: 0.1024 - val_acc: 0.9675
Epoch 26/40
 - 5s - loss: 0.0353 - acc: 0.9778 - val_loss: 0.1048 - val_acc: 0.9685
Epoch 27/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1081 - val_acc: 0.9656
Epoch 28/40
 - 5s - loss: 0.0348 - acc: 0.9780 - val_loss: 0.1126 - val_acc: 0.9634
Epoch 29/40
 - 5s - loss: 0.0349 - acc: 0.9784 - val_loss: 0.1164 - val_acc: 0.9661
Epoch 30/40
 - 5s - loss: 0.0347 - acc: 0.9782 - val_loss: 0.1074 - val_acc: 0.9655
Epoch 31/40
 - 5s - loss: 0.0347 - acc: 0.9780 - val_loss: 0.1112 - val_acc: 0.9654
Epoch 32/40
 - 5s - loss: 0.0346 - acc: 0.9783 - val_loss: 0.1128 - val_acc: 0.9657
Epoch 33/40
 - 5s - loss: 0.0345 - acc: 0.9781 - val_loss: 0.1136 - val_acc: 0.9669
Epoch 34/40
 - 5s - loss: 0.0341 - acc: 0.9780 - val_loss: 0.1107 - val_acc: 0.9646
Epoch 35/40
 - 5s - loss: 0.0345 - acc: 0.9779 - val_loss: 0.1118 - val_acc: 0.9638
Epoch 36/40
 - 5s - loss: 0.0343 - acc: 0.9785 - val_loss: 0.1140 - val_acc: 0.9660
Epoch 37/40
 - 5s - loss: 0.0341 - acc: 0.9784 - val_loss: 0.1152 - val_acc: 0.9629
Epoch 38/40
 - 5s - loss: 0.0341 - acc: 0.9782 - val_loss: 0.1151 - val_acc: 0.9661
Epoch 39/40
 - 5s - loss: 0.0340 - acc: 0.9782 - val_loss: 0.1187 - val_acc: 0.9667
Epoch 40/40
 - 5s - loss: 0.0339 - acc: 0.9785 - val_loss: 0.1191 - val_acc: 0.9654
Epoch 1/40
 - 0s - loss: 0.1002 - acc: 0.9662
Epoch 2/40
 - 0s - loss: 0.0808 - acc: 0.9678
Epoch 3/40
 - 0s - loss: 0.0696 - acc: 0.9692
Epoch 4/40
 - 0s - loss: 0.0661 - acc: 0.9696
Epoch 5/40
 - 0s - loss: 0.0602 - acc: 0.9710
Epoch 6/40
 - 0s - loss: 0.0561 - acc: 0.9726
Epoch 7/40
 - 0s - loss: 0.0537 - acc: 0.9717
Epoch 8/40
 - 0s - loss: 0.0518 - acc: 0.9741
Epoch 9/40
 - 0s - loss: 0.0490 - acc: 0.9743
Epoch 10/40
 - 0s - loss: 0.0493 - acc: 0.9747
Epoch 11/40
 - 0s - loss: 0.0470 - acc: 0.9753
Epoch 12/40
 - 0s - loss: 0.0446 - acc: 0.9756
Epoch 13/40
 - 0s - loss: 0.0432 - acc: 0.9760
Epoch 14/40
 - 0s - loss: 0.0431 - acc: 0.9769
Epoch 15/40
 - 0s - loss: 0.0434 - acc: 0.9758
Epoch 16/40
 - 0s - loss: 0.0413 - acc: 0.9776
Epoch 17/40
 - 0s - loss: 0.0416 - acc: 0.9778
Epoch 18/40
 - 0s - loss: 0.0409 - acc: 0.9770
Epoch 19/40
 - 0s - loss: 0.0403 - acc: 0.9774
Epoch 20/40
 - 0s - loss: 0.0400 - acc: 0.9778
Epoch 21/40
 - 0s - loss: 0.0396 - acc: 0.9771
Epoch 22/40
 - 0s - loss: 0.0386 - acc: 0.9777
Epoch 23/40
 - 0s - loss: 0.0395 - acc: 0.9782
Epoch 24/40
 - 0s - loss: 0.0398 - acc: 0.9783
Epoch 25/40
 - 0s - loss: 0.0382 - acc: 0.9787
Epoch 26/40
 - 0s - loss: 0.0390 - acc: 0.9786
Epoch 27/40
 - 0s - loss: 0.0370 - acc: 0.9789
Epoch 28/40
 - 0s - loss: 0.0383 - acc: 0.9780
Epoch 29/40
 - 0s - loss: 0.0376 - acc: 0.9778
Epoch 30/40
 - 0s - loss: 0.0379 - acc: 0.9779
Epoch 31/40
 - 0s - loss: 0.0362 - acc: 0.9786
Epoch 32/40
 - 0s - loss: 0.0362 - acc: 0.9787
Epoch 33/40
 - 0s - loss: 0.0362 - acc: 0.9795
Epoch 34/40
 - 0s - loss: 0.0361 - acc: 0.9782
Epoch 35/40
 - 0s - loss: 0.0362 - acc: 0.9785
Epoch 36/40
 - 0s - loss: 0.0361 - acc: 0.9791
Epoch 37/40
 - 0s - loss: 0.0366 - acc: 0.9788
Epoch 38/40
 - 0s - loss: 0.0359 - acc: 0.9787
Epoch 39/40
 - 0s - loss: 0.0357 - acc: 0.9777
Epoch 40/40
 - 0s - loss: 0.0362 - acc: 0.9786
# Training time = 0:03:46.796360
# F-Score(Ordinary) = 0.004, Recall: 0.5, Precision: 0.002
# F-Score(lvc) = 0.015, Recall: 0.5, Precision: 0.008
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_285 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_286 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_285 (Embedding)       (None, 4, 48)        705264      input_285[0][0]                  
__________________________________________________________________________________________________
embedding_286 (Embedding)       (None, 4, 24)        5640        input_286[0][0]                  
__________________________________________________________________________________________________
flatten_285 (Flatten)           (None, 192)          0           embedding_285[0][0]              
__________________________________________________________________________________________________
flatten_286 (Flatten)           (None, 96)           0           embedding_286[0][0]              
__________________________________________________________________________________________________
concatenate_143 (Concatenate)   (None, 288)          0           flatten_285[0][0]                
                                                                 flatten_286[0][0]                
__________________________________________________________________________________________________
dense_285 (Dense)               (None, 24)           6936        concatenate_143[0][0]            
__________________________________________________________________________________________________
dropout_143 (Dropout)           (None, 24)           0           dense_285[0][0]                  
__________________________________________________________________________________________________
dense_286 (Dense)               (None, 8)            200         dropout_143[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0952 - acc: 0.9632 - val_loss: 0.0735 - val_acc: 0.9675
Epoch 2/40
 - 5s - loss: 0.0608 - acc: 0.9704 - val_loss: 0.0678 - val_acc: 0.9680
Epoch 3/40
 - 5s - loss: 0.0527 - acc: 0.9728 - val_loss: 0.0743 - val_acc: 0.9663
Epoch 4/40
 - 5s - loss: 0.0487 - acc: 0.9744 - val_loss: 0.0736 - val_acc: 0.9681
Epoch 5/40
 - 5s - loss: 0.0457 - acc: 0.9749 - val_loss: 0.0822 - val_acc: 0.9674
Epoch 6/40
 - 5s - loss: 0.0438 - acc: 0.9750 - val_loss: 0.0816 - val_acc: 0.9677
Epoch 7/40
 - 5s - loss: 0.0425 - acc: 0.9761 - val_loss: 0.0815 - val_acc: 0.9679
Epoch 8/40
 - 5s - loss: 0.0411 - acc: 0.9762 - val_loss: 0.0874 - val_acc: 0.9673
Epoch 9/40
 - 5s - loss: 0.0402 - acc: 0.9763 - val_loss: 0.0887 - val_acc: 0.9649
Epoch 10/40
 - 5s - loss: 0.0398 - acc: 0.9765 - val_loss: 0.0886 - val_acc: 0.9671
Epoch 11/40
 - 5s - loss: 0.0392 - acc: 0.9764 - val_loss: 0.0923 - val_acc: 0.9654
Epoch 12/40
 - 5s - loss: 0.0381 - acc: 0.9766 - val_loss: 0.0931 - val_acc: 0.9677
Epoch 13/40
 - 5s - loss: 0.0380 - acc: 0.9773 - val_loss: 0.0933 - val_acc: 0.9660
Epoch 14/40
 - 5s - loss: 0.0376 - acc: 0.9772 - val_loss: 0.1052 - val_acc: 0.9665
Epoch 15/40
 - 5s - loss: 0.0374 - acc: 0.9774 - val_loss: 0.0938 - val_acc: 0.9663
Epoch 16/40
 - 5s - loss: 0.0372 - acc: 0.9774 - val_loss: 0.0982 - val_acc: 0.9669
Epoch 17/40
 - 5s - loss: 0.0366 - acc: 0.9773 - val_loss: 0.0953 - val_acc: 0.9630
Epoch 18/40
 - 5s - loss: 0.0365 - acc: 0.9772 - val_loss: 0.0997 - val_acc: 0.9642
Epoch 19/40
 - 5s - loss: 0.0364 - acc: 0.9776 - val_loss: 0.1024 - val_acc: 0.9646
Epoch 20/40
 - 5s - loss: 0.0357 - acc: 0.9776 - val_loss: 0.1047 - val_acc: 0.9644
Epoch 21/40
 - 5s - loss: 0.0358 - acc: 0.9777 - val_loss: 0.1008 - val_acc: 0.9645
Epoch 22/40
 - 5s - loss: 0.0354 - acc: 0.9777 - val_loss: 0.1066 - val_acc: 0.9637
Epoch 23/40
 - 5s - loss: 0.0355 - acc: 0.9777 - val_loss: 0.1072 - val_acc: 0.9631
Epoch 24/40
 - 5s - loss: 0.0353 - acc: 0.9778 - val_loss: 0.1008 - val_acc: 0.9663
Epoch 25/40
 - 5s - loss: 0.0351 - acc: 0.9782 - val_loss: 0.1092 - val_acc: 0.9634
Epoch 26/40
 - 5s - loss: 0.0352 - acc: 0.9778 - val_loss: 0.1063 - val_acc: 0.9631
Epoch 27/40
 - 5s - loss: 0.0348 - acc: 0.9779 - val_loss: 0.1084 - val_acc: 0.9650
Epoch 28/40
 - 5s - loss: 0.0347 - acc: 0.9786 - val_loss: 0.1102 - val_acc: 0.9621
Epoch 29/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1109 - val_acc: 0.9606
Epoch 30/40
 - 5s - loss: 0.0346 - acc: 0.9780 - val_loss: 0.1093 - val_acc: 0.9619
Epoch 31/40
 - 5s - loss: 0.0344 - acc: 0.9783 - val_loss: 0.1155 - val_acc: 0.9639
Epoch 32/40
 - 5s - loss: 0.0343 - acc: 0.9786 - val_loss: 0.1134 - val_acc: 0.9626
Epoch 33/40
 - 5s - loss: 0.0345 - acc: 0.9782 - val_loss: 0.1150 - val_acc: 0.9651
Epoch 34/40
 - 5s - loss: 0.0341 - acc: 0.9784 - val_loss: 0.1208 - val_acc: 0.9602
Epoch 35/40
 - 5s - loss: 0.0340 - acc: 0.9786 - val_loss: 0.1158 - val_acc: 0.9638
Epoch 36/40
 - 5s - loss: 0.0340 - acc: 0.9784 - val_loss: 0.1171 - val_acc: 0.9622
Epoch 37/40
 - 5s - loss: 0.0339 - acc: 0.9784 - val_loss: 0.1160 - val_acc: 0.9613
Epoch 38/40
 - 5s - loss: 0.0341 - acc: 0.9782 - val_loss: 0.1172 - val_acc: 0.9635
Epoch 39/40
 - 5s - loss: 0.0340 - acc: 0.9782 - val_loss: 0.1197 - val_acc: 0.9593
Epoch 40/40
 - 5s - loss: 0.0342 - acc: 0.9782 - val_loss: 0.1232 - val_acc: 0.9639
Epoch 1/40
 - 1s - loss: 0.1029 - acc: 0.9656
Epoch 2/40
 - 1s - loss: 0.0779 - acc: 0.9675
Epoch 3/40
 - 1s - loss: 0.0677 - acc: 0.9695
Epoch 4/40
 - 1s - loss: 0.0647 - acc: 0.9701
Epoch 5/40
 - 1s - loss: 0.0603 - acc: 0.9708
Epoch 6/40
 - 1s - loss: 0.0560 - acc: 0.9712
Epoch 7/40
 - 1s - loss: 0.0531 - acc: 0.9728
Epoch 8/40
 - 0s - loss: 0.0510 - acc: 0.9744
Epoch 9/40
 - 0s - loss: 0.0503 - acc: 0.9741
Epoch 10/40
 - 0s - loss: 0.0478 - acc: 0.9749
Epoch 11/40
 - 0s - loss: 0.0458 - acc: 0.9752
Epoch 12/40
 - 1s - loss: 0.0459 - acc: 0.9753
Epoch 13/40
 - 1s - loss: 0.0439 - acc: 0.9759
Epoch 14/40
 - 1s - loss: 0.0443 - acc: 0.9764
Epoch 15/40
 - 1s - loss: 0.0430 - acc: 0.9759
Epoch 16/40
 - 1s - loss: 0.0426 - acc: 0.9770
Epoch 17/40
 - 1s - loss: 0.0408 - acc: 0.9767
Epoch 18/40
 - 1s - loss: 0.0405 - acc: 0.9776
Epoch 19/40
 - 1s - loss: 0.0407 - acc: 0.9770
Epoch 20/40
 - 1s - loss: 0.0399 - acc: 0.9757
Epoch 21/40
 - 1s - loss: 0.0409 - acc: 0.9781
Epoch 22/40
 - 1s - loss: 0.0397 - acc: 0.9792
Epoch 23/40
 - 1s - loss: 0.0395 - acc: 0.9774
Epoch 24/40
 - 1s - loss: 0.0392 - acc: 0.9774
Epoch 25/40
 - 1s - loss: 0.0387 - acc: 0.9774
Epoch 26/40
 - 1s - loss: 0.0380 - acc: 0.9781
Epoch 27/40
 - 1s - loss: 0.0379 - acc: 0.9778
Epoch 28/40
 - 1s - loss: 0.0393 - acc: 0.9789
Epoch 29/40
 - 1s - loss: 0.0390 - acc: 0.9773
Epoch 30/40
 - 1s - loss: 0.0373 - acc: 0.9787
Epoch 31/40
 - 1s - loss: 0.0381 - acc: 0.9776
Epoch 32/40
 - 1s - loss: 0.0378 - acc: 0.9778
Epoch 33/40
 - 1s - loss: 0.0369 - acc: 0.9786
Epoch 34/40
 - 1s - loss: 0.0378 - acc: 0.9779
Epoch 35/40
 - 1s - loss: 0.0371 - acc: 0.9778
Epoch 36/40
 - 1s - loss: 0.0366 - acc: 0.9780
Epoch 37/40
 - 1s - loss: 0.0365 - acc: 0.9786
Epoch 38/40
 - 1s - loss: 0.0376 - acc: 0.9780
Epoch 39/40
 - 1s - loss: 0.0362 - acc: 0.9777
Epoch 40/40
 - 1s - loss: 0.0359 - acc: 0.9786
# Training time = 0:03:48.258831
# F-Score(Ordinary) = 0.122, Recall: 0.5, Precision: 0.069
# F-Score(id) = 0.253, Recall: 0.596, Precision: 0.161
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_287 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_288 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_287 (Embedding)       (None, 4, 48)        705264      input_287[0][0]                  
__________________________________________________________________________________________________
embedding_288 (Embedding)       (None, 4, 24)        5640        input_288[0][0]                  
__________________________________________________________________________________________________
flatten_287 (Flatten)           (None, 192)          0           embedding_287[0][0]              
__________________________________________________________________________________________________
flatten_288 (Flatten)           (None, 96)           0           embedding_288[0][0]              
__________________________________________________________________________________________________
concatenate_144 (Concatenate)   (None, 288)          0           flatten_287[0][0]                
                                                                 flatten_288[0][0]                
__________________________________________________________________________________________________
dense_287 (Dense)               (None, 24)           6936        concatenate_144[0][0]            
__________________________________________________________________________________________________
dropout_144 (Dropout)           (None, 24)           0           dense_287[0][0]                  
__________________________________________________________________________________________________
dense_288 (Dense)               (None, 8)            200         dropout_144[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0984 - acc: 0.9619 - val_loss: 0.0710 - val_acc: 0.9671
Epoch 2/40
 - 5s - loss: 0.0612 - acc: 0.9705 - val_loss: 0.0711 - val_acc: 0.9676
Epoch 3/40
 - 5s - loss: 0.0527 - acc: 0.9726 - val_loss: 0.0695 - val_acc: 0.9685
Epoch 4/40
 - 5s - loss: 0.0481 - acc: 0.9740 - val_loss: 0.0768 - val_acc: 0.9673
Epoch 5/40
 - 5s - loss: 0.0453 - acc: 0.9755 - val_loss: 0.0801 - val_acc: 0.9677
Epoch 6/40
 - 5s - loss: 0.0434 - acc: 0.9757 - val_loss: 0.0780 - val_acc: 0.9689
Epoch 7/40
 - 5s - loss: 0.0421 - acc: 0.9761 - val_loss: 0.0802 - val_acc: 0.9677
Epoch 8/40
 - 5s - loss: 0.0408 - acc: 0.9760 - val_loss: 0.0872 - val_acc: 0.9672
Epoch 9/40
 - 5s - loss: 0.0400 - acc: 0.9762 - val_loss: 0.0875 - val_acc: 0.9673
Epoch 10/40
 - 5s - loss: 0.0396 - acc: 0.9768 - val_loss: 0.0876 - val_acc: 0.9670
Epoch 11/40
 - 5s - loss: 0.0391 - acc: 0.9769 - val_loss: 0.0949 - val_acc: 0.9671
Epoch 12/40
 - 5s - loss: 0.0383 - acc: 0.9772 - val_loss: 0.0921 - val_acc: 0.9674
Epoch 13/40
 - 5s - loss: 0.0380 - acc: 0.9767 - val_loss: 0.0957 - val_acc: 0.9670
Epoch 14/40
 - 5s - loss: 0.0375 - acc: 0.9767 - val_loss: 0.0928 - val_acc: 0.9669
Epoch 15/40
 - 5s - loss: 0.0371 - acc: 0.9771 - val_loss: 0.1002 - val_acc: 0.9667
Epoch 16/40
 - 5s - loss: 0.0368 - acc: 0.9774 - val_loss: 0.0979 - val_acc: 0.9668
Epoch 17/40
 - 5s - loss: 0.0363 - acc: 0.9775 - val_loss: 0.0999 - val_acc: 0.9678
Epoch 18/40
 - 5s - loss: 0.0360 - acc: 0.9775 - val_loss: 0.1045 - val_acc: 0.9671
Epoch 19/40
 - 5s - loss: 0.0359 - acc: 0.9777 - val_loss: 0.1040 - val_acc: 0.9674
Epoch 20/40
 - 5s - loss: 0.0360 - acc: 0.9775 - val_loss: 0.1015 - val_acc: 0.9683
Epoch 21/40
 - 5s - loss: 0.0355 - acc: 0.9780 - val_loss: 0.1092 - val_acc: 0.9666
Epoch 22/40
 - 5s - loss: 0.0353 - acc: 0.9777 - val_loss: 0.1099 - val_acc: 0.9678
Epoch 23/40
 - 5s - loss: 0.0352 - acc: 0.9777 - val_loss: 0.1097 - val_acc: 0.9676
Epoch 24/40
 - 5s - loss: 0.0350 - acc: 0.9779 - val_loss: 0.1058 - val_acc: 0.9638
Epoch 25/40
 - 5s - loss: 0.0352 - acc: 0.9776 - val_loss: 0.1121 - val_acc: 0.9669
Epoch 26/40
 - 5s - loss: 0.0347 - acc: 0.9781 - val_loss: 0.1108 - val_acc: 0.9630
Epoch 27/40
 - 5s - loss: 0.0343 - acc: 0.9783 - val_loss: 0.1095 - val_acc: 0.9682
Epoch 28/40
 - 5s - loss: 0.0349 - acc: 0.9774 - val_loss: 0.1116 - val_acc: 0.9677
Epoch 29/40
 - 5s - loss: 0.0348 - acc: 0.9774 - val_loss: 0.1154 - val_acc: 0.9676
Epoch 30/40
 - 5s - loss: 0.0342 - acc: 0.9783 - val_loss: 0.1122 - val_acc: 0.9649
Epoch 31/40
 - 5s - loss: 0.0343 - acc: 0.9785 - val_loss: 0.1138 - val_acc: 0.9671
Epoch 32/40
 - 5s - loss: 0.0345 - acc: 0.9776 - val_loss: 0.1129 - val_acc: 0.9653
Epoch 33/40
 - 5s - loss: 0.0340 - acc: 0.9781 - val_loss: 0.1165 - val_acc: 0.9674
Epoch 34/40
 - 5s - loss: 0.0342 - acc: 0.9784 - val_loss: 0.1160 - val_acc: 0.9652
Epoch 35/40
 - 5s - loss: 0.0342 - acc: 0.9781 - val_loss: 0.1141 - val_acc: 0.9668
Epoch 36/40
 - 5s - loss: 0.0339 - acc: 0.9784 - val_loss: 0.1167 - val_acc: 0.9672
Epoch 37/40
 - 5s - loss: 0.0338 - acc: 0.9785 - val_loss: 0.1149 - val_acc: 0.9664
Epoch 38/40
 - 5s - loss: 0.0342 - acc: 0.9783 - val_loss: 0.1179 - val_acc: 0.9662
Epoch 39/40
 - 5s - loss: 0.0339 - acc: 0.9782 - val_loss: 0.1193 - val_acc: 0.9687
Epoch 40/40
 - 5s - loss: 0.0340 - acc: 0.9781 - val_loss: 0.1218 - val_acc: 0.9683
Epoch 1/40
 - 1s - loss: 0.0997 - acc: 0.9661
Epoch 2/40
 - 1s - loss: 0.0790 - acc: 0.9672
Epoch 3/40
 - 1s - loss: 0.0729 - acc: 0.9680
Epoch 4/40
 - 1s - loss: 0.0669 - acc: 0.9702
Epoch 5/40
 - 1s - loss: 0.0623 - acc: 0.9697
Epoch 6/40
 - 1s - loss: 0.0594 - acc: 0.9719
Epoch 7/40
 - 0s - loss: 0.0575 - acc: 0.9717
Epoch 8/40
 - 1s - loss: 0.0538 - acc: 0.9728
Epoch 9/40
 - 0s - loss: 0.0527 - acc: 0.9740
Epoch 10/40
 - 1s - loss: 0.0515 - acc: 0.9733
Epoch 11/40
 - 1s - loss: 0.0494 - acc: 0.9746
Epoch 12/40
 - 0s - loss: 0.0482 - acc: 0.9746
Epoch 13/40
 - 0s - loss: 0.0462 - acc: 0.9761
Epoch 14/40
 - 1s - loss: 0.0458 - acc: 0.9759
Epoch 15/40
 - 1s - loss: 0.0447 - acc: 0.9756
Epoch 16/40
 - 1s - loss: 0.0458 - acc: 0.9765
Epoch 17/40
 - 0s - loss: 0.0443 - acc: 0.9757
Epoch 18/40
 - 1s - loss: 0.0427 - acc: 0.9771
Epoch 19/40
 - 0s - loss: 0.0416 - acc: 0.9784
Epoch 20/40
 - 0s - loss: 0.0414 - acc: 0.9770
Epoch 21/40
 - 0s - loss: 0.0393 - acc: 0.9784
Epoch 22/40
 - 0s - loss: 0.0400 - acc: 0.9777
Epoch 23/40
 - 0s - loss: 0.0404 - acc: 0.9778
Epoch 24/40
 - 1s - loss: 0.0393 - acc: 0.9777
Epoch 25/40
 - 0s - loss: 0.0401 - acc: 0.9781
Epoch 26/40
 - 1s - loss: 0.0392 - acc: 0.9780
Epoch 27/40
 - 0s - loss: 0.0385 - acc: 0.9779
Epoch 28/40
 - 0s - loss: 0.0385 - acc: 0.9774
Epoch 29/40
 - 0s - loss: 0.0380 - acc: 0.9783
Epoch 30/40
 - 1s - loss: 0.0357 - acc: 0.9783
Epoch 31/40
 - 1s - loss: 0.0380 - acc: 0.9783
Epoch 32/40
 - 1s - loss: 0.0376 - acc: 0.9789
Epoch 33/40
 - 0s - loss: 0.0389 - acc: 0.9775
Epoch 34/40
 - 1s - loss: 0.0369 - acc: 0.9792
Epoch 35/40
 - 0s - loss: 0.0357 - acc: 0.9786
Epoch 36/40
 - 1s - loss: 0.0363 - acc: 0.9793
Epoch 37/40
 - 1s - loss: 0.0368 - acc: 0.9779
Epoch 38/40
 - 0s - loss: 0.0369 - acc: 0.9774
Epoch 39/40
 - 0s - loss: 0.0366 - acc: 0.9774
Epoch 40/40
 - 0s - loss: 0.0363 - acc: 0.9782
# Training time = 0:03:47.826114
# F-Score(Ordinary) = 0.095, Recall: 0.265, Precision: 0.058
# F-Score(lvc) = 0.174, Recall: 0.204, Precision: 0.152
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_289 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_290 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_289 (Embedding)       (None, 4, 48)        705264      input_289[0][0]                  
__________________________________________________________________________________________________
embedding_290 (Embedding)       (None, 4, 24)        5640        input_290[0][0]                  
__________________________________________________________________________________________________
flatten_289 (Flatten)           (None, 192)          0           embedding_289[0][0]              
__________________________________________________________________________________________________
flatten_290 (Flatten)           (None, 96)           0           embedding_290[0][0]              
__________________________________________________________________________________________________
concatenate_145 (Concatenate)   (None, 288)          0           flatten_289[0][0]                
                                                                 flatten_290[0][0]                
__________________________________________________________________________________________________
dense_289 (Dense)               (None, 24)           6936        concatenate_145[0][0]            
__________________________________________________________________________________________________
dropout_145 (Dropout)           (None, 24)           0           dense_289[0][0]                  
__________________________________________________________________________________________________
dense_290 (Dense)               (None, 8)            200         dropout_145[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = adamax, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0987 - acc: 0.9617 - val_loss: 0.0733 - val_acc: 0.9671
Epoch 2/40
 - 5s - loss: 0.0614 - acc: 0.9706 - val_loss: 0.0732 - val_acc: 0.9686
Epoch 3/40
 - 5s - loss: 0.0538 - acc: 0.9729 - val_loss: 0.0731 - val_acc: 0.9678
Epoch 4/40
 - 5s - loss: 0.0494 - acc: 0.9732 - val_loss: 0.0711 - val_acc: 0.9674
Epoch 5/40
 - 5s - loss: 0.0471 - acc: 0.9743 - val_loss: 0.0765 - val_acc: 0.9670
Epoch 6/40
 - 5s - loss: 0.0449 - acc: 0.9749 - val_loss: 0.0753 - val_acc: 0.9684
Epoch 7/40
 - 5s - loss: 0.0433 - acc: 0.9753 - val_loss: 0.0813 - val_acc: 0.9676
Epoch 8/40
 - 5s - loss: 0.0422 - acc: 0.9756 - val_loss: 0.0840 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0412 - acc: 0.9761 - val_loss: 0.0860 - val_acc: 0.9665
Epoch 10/40
 - 5s - loss: 0.0405 - acc: 0.9763 - val_loss: 0.0859 - val_acc: 0.9674
Epoch 11/40
 - 5s - loss: 0.0399 - acc: 0.9768 - val_loss: 0.0900 - val_acc: 0.9673
Epoch 12/40
 - 5s - loss: 0.0394 - acc: 0.9767 - val_loss: 0.0862 - val_acc: 0.9662
Epoch 13/40
 - 5s - loss: 0.0386 - acc: 0.9772 - val_loss: 0.0908 - val_acc: 0.9671
Epoch 14/40
 - 5s - loss: 0.0384 - acc: 0.9771 - val_loss: 0.0913 - val_acc: 0.9683
Epoch 15/40
 - 5s - loss: 0.0381 - acc: 0.9769 - val_loss: 0.0909 - val_acc: 0.9675
Epoch 16/40
 - 5s - loss: 0.0375 - acc: 0.9773 - val_loss: 0.0970 - val_acc: 0.9657
Epoch 17/40
 - 5s - loss: 0.0372 - acc: 0.9771 - val_loss: 0.0973 - val_acc: 0.9657
Epoch 18/40
 - 5s - loss: 0.0371 - acc: 0.9771 - val_loss: 0.0988 - val_acc: 0.9663
Epoch 19/40
 - 5s - loss: 0.0367 - acc: 0.9774 - val_loss: 0.0994 - val_acc: 0.9658
Epoch 20/40
 - 5s - loss: 0.0368 - acc: 0.9770 - val_loss: 0.1012 - val_acc: 0.9651
Epoch 21/40
 - 5s - loss: 0.0362 - acc: 0.9776 - val_loss: 0.1034 - val_acc: 0.9630
Epoch 22/40
 - 5s - loss: 0.0362 - acc: 0.9778 - val_loss: 0.1016 - val_acc: 0.9642
Epoch 23/40
 - 5s - loss: 0.0359 - acc: 0.9776 - val_loss: 0.1029 - val_acc: 0.9666
Epoch 24/40
 - 5s - loss: 0.0353 - acc: 0.9774 - val_loss: 0.1061 - val_acc: 0.9640
Epoch 25/40
 - 5s - loss: 0.0356 - acc: 0.9782 - val_loss: 0.1038 - val_acc: 0.9635
Epoch 26/40
 - 5s - loss: 0.0356 - acc: 0.9779 - val_loss: 0.1051 - val_acc: 0.9675
Epoch 27/40
 - 5s - loss: 0.0353 - acc: 0.9780 - val_loss: 0.1051 - val_acc: 0.9670
Epoch 28/40
 - 5s - loss: 0.0350 - acc: 0.9782 - val_loss: 0.1059 - val_acc: 0.9661
Epoch 29/40
 - 5s - loss: 0.0351 - acc: 0.9780 - val_loss: 0.1113 - val_acc: 0.9625
Epoch 30/40
 - 5s - loss: 0.0350 - acc: 0.9778 - val_loss: 0.1093 - val_acc: 0.9668
Epoch 31/40
 - 5s - loss: 0.0349 - acc: 0.9782 - val_loss: 0.1110 - val_acc: 0.9638
Epoch 32/40
 - 5s - loss: 0.0346 - acc: 0.9785 - val_loss: 0.1129 - val_acc: 0.9616
Epoch 33/40
 - 5s - loss: 0.0350 - acc: 0.9778 - val_loss: 0.1143 - val_acc: 0.9616
Epoch 34/40
 - 5s - loss: 0.0346 - acc: 0.9784 - val_loss: 0.1171 - val_acc: 0.9666
Epoch 35/40
 - 5s - loss: 0.0347 - acc: 0.9781 - val_loss: 0.1222 - val_acc: 0.9658
Epoch 36/40
 - 5s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.1203 - val_acc: 0.9656
Epoch 37/40
 - 5s - loss: 0.0347 - acc: 0.9778 - val_loss: 0.1192 - val_acc: 0.9669
Epoch 38/40
 - 5s - loss: 0.0345 - acc: 0.9785 - val_loss: 0.1152 - val_acc: 0.9622
Epoch 39/40
 - 5s - loss: 0.0341 - acc: 0.9787 - val_loss: 0.1188 - val_acc: 0.9647
Epoch 40/40
 - 5s - loss: 0.0345 - acc: 0.9784 - val_loss: 0.1178 - val_acc: 0.9656
Epoch 1/40
 - 0s - loss: 0.1024 - acc: 0.9643
Epoch 2/40
 - 1s - loss: 0.0779 - acc: 0.9665
Epoch 3/40
 - 1s - loss: 0.0667 - acc: 0.9690
Epoch 4/40
 - 1s - loss: 0.0654 - acc: 0.9689
Epoch 5/40
 - 0s - loss: 0.0578 - acc: 0.9709
Epoch 6/40
 - 1s - loss: 0.0563 - acc: 0.9719
Epoch 7/40
 - 0s - loss: 0.0525 - acc: 0.9729
Epoch 8/40
 - 0s - loss: 0.0493 - acc: 0.9731
Epoch 9/40
 - 1s - loss: 0.0482 - acc: 0.9741
Epoch 10/40
 - 1s - loss: 0.0471 - acc: 0.9740
Epoch 11/40
 - 0s - loss: 0.0479 - acc: 0.9746
Epoch 12/40
 - 0s - loss: 0.0452 - acc: 0.9748
Epoch 13/40
 - 0s - loss: 0.0430 - acc: 0.9768
Epoch 14/40
 - 0s - loss: 0.0430 - acc: 0.9761
Epoch 15/40
 - 0s - loss: 0.0424 - acc: 0.9769
Epoch 16/40
 - 0s - loss: 0.0414 - acc: 0.9768
Epoch 17/40
 - 0s - loss: 0.0413 - acc: 0.9762
Epoch 18/40
 - 0s - loss: 0.0403 - acc: 0.9768
Epoch 19/40
 - 0s - loss: 0.0400 - acc: 0.9778
Epoch 20/40
 - 0s - loss: 0.0389 - acc: 0.9779
Epoch 21/40
 - 0s - loss: 0.0397 - acc: 0.9761
Epoch 22/40
 - 0s - loss: 0.0384 - acc: 0.9782
Epoch 23/40
 - 0s - loss: 0.0373 - acc: 0.9778
Epoch 24/40
 - 0s - loss: 0.0373 - acc: 0.9774
Epoch 25/40
 - 0s - loss: 0.0374 - acc: 0.9783
Epoch 26/40
 - 0s - loss: 0.0375 - acc: 0.9783
Epoch 27/40
 - 0s - loss: 0.0375 - acc: 0.9778
Epoch 28/40
 - 0s - loss: 0.0359 - acc: 0.9781
Epoch 29/40
 - 0s - loss: 0.0365 - acc: 0.9784
Epoch 30/40
 - 0s - loss: 0.0369 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0356 - acc: 0.9778
Epoch 32/40
 - 0s - loss: 0.0361 - acc: 0.9791
Epoch 33/40
 - 0s - loss: 0.0354 - acc: 0.9786
Epoch 34/40
 - 0s - loss: 0.0350 - acc: 0.9779
Epoch 35/40
 - 0s - loss: 0.0360 - acc: 0.9779
Epoch 36/40
 - 0s - loss: 0.0349 - acc: 0.9794
Epoch 37/40
 - 0s - loss: 0.0354 - acc: 0.9789
Epoch 38/40
 - 0s - loss: 0.0351 - acc: 0.9792
Epoch 39/40
 - 0s - loss: 0.0355 - acc: 0.9776
Epoch 40/40
 - 0s - loss: 0.0359 - acc: 0.9776
# Training time = 0:03:47.902149
# F-Score(Ordinary) = 0.596, Recall: 0.803, Precision: 0.474
# F-Score(lvc) = 0.526, Recall: 0.691, Precision: 0.424
# F-Score(ireflv) = 0.763, Recall: 0.821, Precision: 0.713
# F-Score(id) = 0.481, Recall: 0.844, Precision: 0.337
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_291 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_292 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_291 (Embedding)       (None, 4, 48)        705264      input_291[0][0]                  
__________________________________________________________________________________________________
embedding_292 (Embedding)       (None, 4, 24)        5640        input_292[0][0]                  
__________________________________________________________________________________________________
flatten_291 (Flatten)           (None, 192)          0           embedding_291[0][0]              
__________________________________________________________________________________________________
flatten_292 (Flatten)           (None, 96)           0           embedding_292[0][0]              
__________________________________________________________________________________________________
concatenate_146 (Concatenate)   (None, 288)          0           flatten_291[0][0]                
                                                                 flatten_292[0][0]                
__________________________________________________________________________________________________
dense_291 (Dense)               (None, 24)           6936        concatenate_146[0][0]            
__________________________________________________________________________________________________
dropout_146 (Dropout)           (None, 24)           0           dense_291[0][0]                  
__________________________________________________________________________________________________
dense_292 (Dense)               (None, 8)            200         dropout_146[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1288 - acc: 0.9573 - val_loss: 0.0749 - val_acc: 0.9673
Epoch 2/40
 - 5s - loss: 0.0599 - acc: 0.9706 - val_loss: 0.0706 - val_acc: 0.9673
Epoch 3/40
 - 5s - loss: 0.0494 - acc: 0.9733 - val_loss: 0.0740 - val_acc: 0.9673
Epoch 4/40
 - 5s - loss: 0.0450 - acc: 0.9754 - val_loss: 0.0792 - val_acc: 0.9658
Epoch 5/40
 - 5s - loss: 0.0427 - acc: 0.9751 - val_loss: 0.0857 - val_acc: 0.9671
Epoch 6/40
 - 5s - loss: 0.0412 - acc: 0.9755 - val_loss: 0.0822 - val_acc: 0.9673
Epoch 7/40
 - 5s - loss: 0.0399 - acc: 0.9762 - val_loss: 0.0856 - val_acc: 0.9674
Epoch 8/40
 - 5s - loss: 0.0387 - acc: 0.9763 - val_loss: 0.0894 - val_acc: 0.9678
Epoch 9/40
 - 5s - loss: 0.0377 - acc: 0.9772 - val_loss: 0.0894 - val_acc: 0.9677
Epoch 10/40
 - 5s - loss: 0.0372 - acc: 0.9768 - val_loss: 0.0917 - val_acc: 0.9669
Epoch 11/40
 - 5s - loss: 0.0364 - acc: 0.9771 - val_loss: 0.0966 - val_acc: 0.9662
Epoch 12/40
 - 5s - loss: 0.0362 - acc: 0.9775 - val_loss: 0.0897 - val_acc: 0.9686
Epoch 13/40
 - 5s - loss: 0.0357 - acc: 0.9768 - val_loss: 0.0967 - val_acc: 0.9677
Epoch 14/40
 - 5s - loss: 0.0353 - acc: 0.9774 - val_loss: 0.0980 - val_acc: 0.9676
Epoch 15/40
 - 5s - loss: 0.0349 - acc: 0.9778 - val_loss: 0.1027 - val_acc: 0.9680
Epoch 16/40
 - 5s - loss: 0.0346 - acc: 0.9775 - val_loss: 0.0996 - val_acc: 0.9656
Epoch 17/40
 - 5s - loss: 0.0346 - acc: 0.9778 - val_loss: 0.1015 - val_acc: 0.9670
Epoch 18/40
 - 5s - loss: 0.0341 - acc: 0.9780 - val_loss: 0.1044 - val_acc: 0.9654
Epoch 19/40
 - 5s - loss: 0.0339 - acc: 0.9777 - val_loss: 0.1029 - val_acc: 0.9674
Epoch 20/40
 - 5s - loss: 0.0342 - acc: 0.9778 - val_loss: 0.1036 - val_acc: 0.9674
Epoch 21/40
 - 5s - loss: 0.0337 - acc: 0.9781 - val_loss: 0.1052 - val_acc: 0.9675
Epoch 22/40
 - 5s - loss: 0.0336 - acc: 0.9781 - val_loss: 0.1115 - val_acc: 0.9657
Epoch 23/40
 - 5s - loss: 0.0332 - acc: 0.9782 - val_loss: 0.1095 - val_acc: 0.9680
Epoch 24/40
 - 5s - loss: 0.0336 - acc: 0.9781 - val_loss: 0.1032 - val_acc: 0.9656
Epoch 25/40
 - 5s - loss: 0.0331 - acc: 0.9781 - val_loss: 0.1107 - val_acc: 0.9680
Epoch 26/40
 - 5s - loss: 0.0329 - acc: 0.9788 - val_loss: 0.1121 - val_acc: 0.9677
Epoch 27/40
 - 5s - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1082 - val_acc: 0.9678
Epoch 28/40
 - 5s - loss: 0.0328 - acc: 0.9785 - val_loss: 0.1094 - val_acc: 0.9671
Epoch 29/40
 - 5s - loss: 0.0329 - acc: 0.9783 - val_loss: 0.1138 - val_acc: 0.9669
Epoch 30/40
 - 5s - loss: 0.0324 - acc: 0.9788 - val_loss: 0.1159 - val_acc: 0.9663
Epoch 31/40
 - 5s - loss: 0.0328 - acc: 0.9785 - val_loss: 0.1096 - val_acc: 0.9637
Epoch 32/40
 - 5s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1128 - val_acc: 0.9667
Epoch 33/40
 - 5s - loss: 0.0323 - acc: 0.9788 - val_loss: 0.1163 - val_acc: 0.9679
Epoch 34/40
 - 5s - loss: 0.0325 - acc: 0.9785 - val_loss: 0.1123 - val_acc: 0.9664
Epoch 35/40
 - 5s - loss: 0.0323 - acc: 0.9788 - val_loss: 0.1156 - val_acc: 0.9651
Epoch 36/40
 - 5s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1101 - val_acc: 0.9646
Epoch 37/40
 - 5s - loss: 0.0322 - acc: 0.9786 - val_loss: 0.1194 - val_acc: 0.9679
Epoch 38/40
 - 5s - loss: 0.0323 - acc: 0.9788 - val_loss: 0.1133 - val_acc: 0.9626
Epoch 39/40
 - 5s - loss: 0.0323 - acc: 0.9786 - val_loss: 0.1107 - val_acc: 0.9674
Epoch 40/40
 - 5s - loss: 0.0322 - acc: 0.9789 - val_loss: 0.1180 - val_acc: 0.9674
Epoch 1/40
 - 0s - loss: 0.0996 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.0713 - acc: 0.9679
Epoch 3/40
 - 0s - loss: 0.0639 - acc: 0.9693
Epoch 4/40
 - 0s - loss: 0.0568 - acc: 0.9706
Epoch 5/40
 - 0s - loss: 0.0536 - acc: 0.9735
Epoch 6/40
 - 0s - loss: 0.0515 - acc: 0.9732
Epoch 7/40
 - 0s - loss: 0.0496 - acc: 0.9728
Epoch 8/40
 - 0s - loss: 0.0468 - acc: 0.9739
Epoch 9/40
 - 0s - loss: 0.0468 - acc: 0.9750
Epoch 10/40
 - 0s - loss: 0.0447 - acc: 0.9759
Epoch 11/40
 - 0s - loss: 0.0428 - acc: 0.9768
Epoch 12/40
 - 0s - loss: 0.0419 - acc: 0.9774
Epoch 13/40
 - 0s - loss: 0.0411 - acc: 0.9768
Epoch 14/40
 - 0s - loss: 0.0398 - acc: 0.9770
Epoch 15/40
 - 0s - loss: 0.0394 - acc: 0.9752
Epoch 16/40
 - 0s - loss: 0.0387 - acc: 0.9786
Epoch 17/40
 - 0s - loss: 0.0383 - acc: 0.9770
Epoch 18/40
 - 0s - loss: 0.0380 - acc: 0.9779
Epoch 19/40
 - 0s - loss: 0.0368 - acc: 0.9781
Epoch 20/40
 - 0s - loss: 0.0378 - acc: 0.9778
Epoch 21/40
 - 0s - loss: 0.0379 - acc: 0.9767
Epoch 22/40
 - 0s - loss: 0.0356 - acc: 0.9781
Epoch 23/40
 - 0s - loss: 0.0371 - acc: 0.9766
Epoch 24/40
 - 0s - loss: 0.0356 - acc: 0.9783
Epoch 25/40
 - 0s - loss: 0.0367 - acc: 0.9769
Epoch 26/40
 - 0s - loss: 0.0363 - acc: 0.9774
Epoch 27/40
 - 0s - loss: 0.0354 - acc: 0.9783
Epoch 28/40
 - 0s - loss: 0.0339 - acc: 0.9772
Epoch 29/40
 - 0s - loss: 0.0341 - acc: 0.9786
Epoch 30/40
 - 0s - loss: 0.0343 - acc: 0.9780
Epoch 31/40
 - 0s - loss: 0.0339 - acc: 0.9793
Epoch 32/40
 - 0s - loss: 0.0328 - acc: 0.9791
Epoch 33/40
 - 0s - loss: 0.0341 - acc: 0.9779
Epoch 34/40
 - 0s - loss: 0.0334 - acc: 0.9791
Epoch 35/40
 - 0s - loss: 0.0324 - acc: 0.9784
Epoch 36/40
 - 0s - loss: 0.0331 - acc: 0.9793
Epoch 37/40
 - 0s - loss: 0.0324 - acc: 0.9789
Epoch 38/40
 - 0s - loss: 0.0321 - acc: 0.9793
Epoch 39/40
 - 0s - loss: 0.0317 - acc: 0.9778
Epoch 40/40
 - 0s - loss: 0.0314 - acc: 0.9780
# Training time = 0:03:48.915022
# F-Score(Ordinary) = 0.052, Recall: 0.632, Precision: 0.027
# F-Score(ireflv) = 0.016, Recall: 0.2, Precision: 0.008
# F-Score(id) = 0.097, Recall: 0.714, Precision: 0.052
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_293 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_294 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_293 (Embedding)       (None, 4, 48)        705264      input_293[0][0]                  
__________________________________________________________________________________________________
embedding_294 (Embedding)       (None, 4, 24)        5640        input_294[0][0]                  
__________________________________________________________________________________________________
flatten_293 (Flatten)           (None, 192)          0           embedding_293[0][0]              
__________________________________________________________________________________________________
flatten_294 (Flatten)           (None, 96)           0           embedding_294[0][0]              
__________________________________________________________________________________________________
concatenate_147 (Concatenate)   (None, 288)          0           flatten_293[0][0]                
                                                                 flatten_294[0][0]                
__________________________________________________________________________________________________
dense_293 (Dense)               (None, 24)           6936        concatenate_147[0][0]            
__________________________________________________________________________________________________
dropout_147 (Dropout)           (None, 24)           0           dense_293[0][0]                  
__________________________________________________________________________________________________
dense_294 (Dense)               (None, 8)            200         dropout_147[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.1301 - acc: 0.9583 - val_loss: 0.0696 - val_acc: 0.9682
Epoch 2/40
 - 4s - loss: 0.0585 - acc: 0.9714 - val_loss: 0.0708 - val_acc: 0.9675
Epoch 3/40
 - 4s - loss: 0.0499 - acc: 0.9737 - val_loss: 0.0744 - val_acc: 0.9672
Epoch 4/40
 - 4s - loss: 0.0459 - acc: 0.9752 - val_loss: 0.0772 - val_acc: 0.9677
Epoch 5/40
 - 4s - loss: 0.0433 - acc: 0.9759 - val_loss: 0.0819 - val_acc: 0.9675
Epoch 6/40
 - 4s - loss: 0.0416 - acc: 0.9761 - val_loss: 0.0852 - val_acc: 0.9676
Epoch 7/40
 - 4s - loss: 0.0400 - acc: 0.9764 - val_loss: 0.0863 - val_acc: 0.9677
Epoch 8/40
 - 4s - loss: 0.0389 - acc: 0.9769 - val_loss: 0.0895 - val_acc: 0.9683
Epoch 9/40
 - 5s - loss: 0.0382 - acc: 0.9768 - val_loss: 0.0891 - val_acc: 0.9678
Epoch 10/40
 - 5s - loss: 0.0376 - acc: 0.9766 - val_loss: 0.0951 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0371 - acc: 0.9775 - val_loss: 0.0952 - val_acc: 0.9675
Epoch 12/40
 - 5s - loss: 0.0365 - acc: 0.9776 - val_loss: 0.0983 - val_acc: 0.9677
Epoch 13/40
 - 5s - loss: 0.0362 - acc: 0.9775 - val_loss: 0.0979 - val_acc: 0.9666
Epoch 14/40
 - 5s - loss: 0.0355 - acc: 0.9774 - val_loss: 0.1043 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0354 - acc: 0.9780 - val_loss: 0.0983 - val_acc: 0.9682
Epoch 16/40
 - 5s - loss: 0.0351 - acc: 0.9778 - val_loss: 0.1027 - val_acc: 0.9669
Epoch 17/40
 - 5s - loss: 0.0348 - acc: 0.9773 - val_loss: 0.1009 - val_acc: 0.9668
Epoch 18/40
 - 5s - loss: 0.0345 - acc: 0.9781 - val_loss: 0.0982 - val_acc: 0.9674
Epoch 19/40
 - 5s - loss: 0.0344 - acc: 0.9778 - val_loss: 0.1073 - val_acc: 0.9666
Epoch 20/40
 - 5s - loss: 0.0343 - acc: 0.9778 - val_loss: 0.1020 - val_acc: 0.9668
Epoch 21/40
 - 5s - loss: 0.0339 - acc: 0.9781 - val_loss: 0.1018 - val_acc: 0.9663
Epoch 22/40
 - 5s - loss: 0.0336 - acc: 0.9782 - val_loss: 0.1060 - val_acc: 0.9657
Epoch 23/40
 - 5s - loss: 0.0335 - acc: 0.9780 - val_loss: 0.1068 - val_acc: 0.9672
Epoch 24/40
 - 5s - loss: 0.0336 - acc: 0.9785 - val_loss: 0.1108 - val_acc: 0.9659
Epoch 25/40
 - 5s - loss: 0.0336 - acc: 0.9784 - val_loss: 0.1108 - val_acc: 0.9660
Epoch 26/40
 - 5s - loss: 0.0332 - acc: 0.9785 - val_loss: 0.1134 - val_acc: 0.9679
Epoch 27/40
 - 5s - loss: 0.0332 - acc: 0.9786 - val_loss: 0.1059 - val_acc: 0.9653
Epoch 28/40
 - 5s - loss: 0.0329 - acc: 0.9787 - val_loss: 0.1087 - val_acc: 0.9626
Epoch 29/40
 - 5s - loss: 0.0334 - acc: 0.9785 - val_loss: 0.1103 - val_acc: 0.9655
Epoch 30/40
 - 5s - loss: 0.0327 - acc: 0.9787 - val_loss: 0.1067 - val_acc: 0.9634
Epoch 31/40
 - 5s - loss: 0.0326 - acc: 0.9789 - val_loss: 0.1130 - val_acc: 0.9656
Epoch 32/40
 - 5s - loss: 0.0326 - acc: 0.9785 - val_loss: 0.1114 - val_acc: 0.9651
Epoch 33/40
 - 5s - loss: 0.0326 - acc: 0.9783 - val_loss: 0.1144 - val_acc: 0.9671
Epoch 34/40
 - 5s - loss: 0.0330 - acc: 0.9784 - val_loss: 0.1138 - val_acc: 0.9638
Epoch 35/40
 - 5s - loss: 0.0329 - acc: 0.9787 - val_loss: 0.1118 - val_acc: 0.9640
Epoch 36/40
 - 5s - loss: 0.0326 - acc: 0.9787 - val_loss: 0.1126 - val_acc: 0.9648
Epoch 37/40
 - 5s - loss: 0.0322 - acc: 0.9795 - val_loss: 0.1119 - val_acc: 0.9668
Epoch 38/40
 - 5s - loss: 0.0325 - acc: 0.9786 - val_loss: 0.1197 - val_acc: 0.9655
Epoch 39/40
 - 5s - loss: 0.0322 - acc: 0.9786 - val_loss: 0.1159 - val_acc: 0.9668
Epoch 40/40
 - 5s - loss: 0.0323 - acc: 0.9787 - val_loss: 0.1165 - val_acc: 0.9680
Epoch 1/40
 - 0s - loss: 0.1026 - acc: 0.9647
Epoch 2/40
 - 0s - loss: 0.0751 - acc: 0.9685
Epoch 3/40
 - 0s - loss: 0.0623 - acc: 0.9707
Epoch 4/40
 - 0s - loss: 0.0549 - acc: 0.9726
Epoch 5/40
 - 0s - loss: 0.0523 - acc: 0.9731
Epoch 6/40
 - 0s - loss: 0.0499 - acc: 0.9736
Epoch 7/40
 - 0s - loss: 0.0473 - acc: 0.9749
Epoch 8/40
 - 0s - loss: 0.0454 - acc: 0.9758
Epoch 9/40
 - 0s - loss: 0.0441 - acc: 0.9746
Epoch 10/40
 - 0s - loss: 0.0437 - acc: 0.9769
Epoch 11/40
 - 0s - loss: 0.0424 - acc: 0.9761
Epoch 12/40
 - 0s - loss: 0.0410 - acc: 0.9779
Epoch 13/40
 - 0s - loss: 0.0393 - acc: 0.9770
Epoch 14/40
 - 0s - loss: 0.0391 - acc: 0.9772
Epoch 15/40
 - 0s - loss: 0.0382 - acc: 0.9772
Epoch 16/40
 - 0s - loss: 0.0385 - acc: 0.9782
Epoch 17/40
 - 0s - loss: 0.0381 - acc: 0.9778
Epoch 18/40
 - 0s - loss: 0.0381 - acc: 0.9778
Epoch 19/40
 - 0s - loss: 0.0379 - acc: 0.9774
Epoch 20/40
 - 0s - loss: 0.0380 - acc: 0.9767
Epoch 21/40
 - 0s - loss: 0.0376 - acc: 0.9773
Epoch 22/40
 - 0s - loss: 0.0367 - acc: 0.9785
Epoch 23/40
 - 0s - loss: 0.0362 - acc: 0.9786
Epoch 24/40
 - 0s - loss: 0.0368 - acc: 0.9785
Epoch 25/40
 - 0s - loss: 0.0363 - acc: 0.9792
Epoch 26/40
 - 0s - loss: 0.0357 - acc: 0.9789
Epoch 27/40
 - 0s - loss: 0.0350 - acc: 0.9792
Epoch 28/40
 - 0s - loss: 0.0358 - acc: 0.9786
Epoch 29/40
 - 0s - loss: 0.0356 - acc: 0.9778
Epoch 30/40
 - 0s - loss: 0.0348 - acc: 0.9791
Epoch 31/40
 - 0s - loss: 0.0349 - acc: 0.9791
Epoch 32/40
 - 0s - loss: 0.0358 - acc: 0.9781
Epoch 33/40
 - 0s - loss: 0.0348 - acc: 0.9796
Epoch 34/40
 - 0s - loss: 0.0355 - acc: 0.9782
Epoch 35/40
 - 0s - loss: 0.0349 - acc: 0.9780
Epoch 36/40
 - 0s - loss: 0.0352 - acc: 0.9790
Epoch 37/40
 - 0s - loss: 0.0351 - acc: 0.9785
Epoch 38/40
 - 0s - loss: 0.0341 - acc: 0.9793
Epoch 39/40
 - 0s - loss: 0.0341 - acc: 0.9777
Epoch 40/40
 - 0s - loss: 0.0348 - acc: 0.9784
# Training time = 0:03:40.017844
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_295 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_296 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_295 (Embedding)       (None, 4, 48)        705264      input_295[0][0]                  
__________________________________________________________________________________________________
embedding_296 (Embedding)       (None, 4, 24)        5640        input_296[0][0]                  
__________________________________________________________________________________________________
flatten_295 (Flatten)           (None, 192)          0           embedding_295[0][0]              
__________________________________________________________________________________________________
flatten_296 (Flatten)           (None, 96)           0           embedding_296[0][0]              
__________________________________________________________________________________________________
concatenate_148 (Concatenate)   (None, 288)          0           flatten_295[0][0]                
                                                                 flatten_296[0][0]                
__________________________________________________________________________________________________
dense_295 (Dense)               (None, 24)           6936        concatenate_148[0][0]            
__________________________________________________________________________________________________
dropout_148 (Dropout)           (None, 24)           0           dense_295[0][0]                  
__________________________________________________________________________________________________
dense_296 (Dense)               (None, 8)            200         dropout_148[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1283 - acc: 0.9587 - val_loss: 0.0727 - val_acc: 0.9672
Epoch 2/40
 - 5s - loss: 0.0586 - acc: 0.9715 - val_loss: 0.0704 - val_acc: 0.9676
Epoch 3/40
 - 5s - loss: 0.0497 - acc: 0.9738 - val_loss: 0.0735 - val_acc: 0.9672
Epoch 4/40
 - 5s - loss: 0.0454 - acc: 0.9747 - val_loss: 0.0772 - val_acc: 0.9680
Epoch 5/40
 - 5s - loss: 0.0427 - acc: 0.9754 - val_loss: 0.0829 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0410 - acc: 0.9757 - val_loss: 0.0843 - val_acc: 0.9675
Epoch 7/40
 - 5s - loss: 0.0398 - acc: 0.9767 - val_loss: 0.0853 - val_acc: 0.9679
Epoch 8/40
 - 5s - loss: 0.0387 - acc: 0.9768 - val_loss: 0.0892 - val_acc: 0.9686
Epoch 9/40
 - 5s - loss: 0.0379 - acc: 0.9768 - val_loss: 0.0944 - val_acc: 0.9661
Epoch 10/40
 - 5s - loss: 0.0372 - acc: 0.9773 - val_loss: 0.0920 - val_acc: 0.9667
Epoch 11/40
 - 5s - loss: 0.0366 - acc: 0.9770 - val_loss: 0.0938 - val_acc: 0.9675
Epoch 12/40
 - 5s - loss: 0.0358 - acc: 0.9775 - val_loss: 0.0972 - val_acc: 0.9681
Epoch 13/40
 - 5s - loss: 0.0353 - acc: 0.9777 - val_loss: 0.1005 - val_acc: 0.9672
Epoch 14/40
 - 5s - loss: 0.0351 - acc: 0.9777 - val_loss: 0.0997 - val_acc: 0.9673
Epoch 15/40
 - 5s - loss: 0.0349 - acc: 0.9776 - val_loss: 0.0974 - val_acc: 0.9672
Epoch 16/40
 - 5s - loss: 0.0346 - acc: 0.9780 - val_loss: 0.0996 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0339 - acc: 0.9778 - val_loss: 0.1011 - val_acc: 0.9660
Epoch 18/40
 - 5s - loss: 0.0343 - acc: 0.9778 - val_loss: 0.0957 - val_acc: 0.9674
Epoch 19/40
 - 5s - loss: 0.0339 - acc: 0.9780 - val_loss: 0.1010 - val_acc: 0.9676
Epoch 20/40
 - 5s - loss: 0.0335 - acc: 0.9784 - val_loss: 0.1030 - val_acc: 0.9661
Epoch 21/40
 - 5s - loss: 0.0337 - acc: 0.9784 - val_loss: 0.1024 - val_acc: 0.9656
Epoch 22/40
 - 5s - loss: 0.0334 - acc: 0.9784 - val_loss: 0.1056 - val_acc: 0.9663
Epoch 23/40
 - 5s - loss: 0.0331 - acc: 0.9785 - val_loss: 0.1109 - val_acc: 0.9654
Epoch 24/40
 - 5s - loss: 0.0332 - acc: 0.9784 - val_loss: 0.1082 - val_acc: 0.9661
Epoch 25/40
 - 5s - loss: 0.0330 - acc: 0.9788 - val_loss: 0.1072 - val_acc: 0.9646
Epoch 26/40
 - 5s - loss: 0.0330 - acc: 0.9782 - val_loss: 0.1070 - val_acc: 0.9662
Epoch 27/40
 - 5s - loss: 0.0328 - acc: 0.9785 - val_loss: 0.1070 - val_acc: 0.9655
Epoch 28/40
 - 5s - loss: 0.0328 - acc: 0.9789 - val_loss: 0.1084 - val_acc: 0.9635
Epoch 29/40
 - 5s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1111 - val_acc: 0.9647
Epoch 30/40
 - 5s - loss: 0.0325 - acc: 0.9791 - val_loss: 0.1104 - val_acc: 0.9665
Epoch 31/40
 - 5s - loss: 0.0323 - acc: 0.9785 - val_loss: 0.1170 - val_acc: 0.9643
Epoch 32/40
 - 5s - loss: 0.0324 - acc: 0.9790 - val_loss: 0.1161 - val_acc: 0.9660
Epoch 33/40
 - 5s - loss: 0.0326 - acc: 0.9784 - val_loss: 0.1156 - val_acc: 0.9653
Epoch 34/40
 - 5s - loss: 0.0324 - acc: 0.9791 - val_loss: 0.1135 - val_acc: 0.9644
Epoch 35/40
 - 5s - loss: 0.0322 - acc: 0.9791 - val_loss: 0.1140 - val_acc: 0.9659
Epoch 36/40
 - 5s - loss: 0.0324 - acc: 0.9789 - val_loss: 0.1167 - val_acc: 0.9648
Epoch 37/40
 - 5s - loss: 0.0321 - acc: 0.9790 - val_loss: 0.1109 - val_acc: 0.9645
Epoch 38/40
 - 5s - loss: 0.0323 - acc: 0.9788 - val_loss: 0.1145 - val_acc: 0.9644
Epoch 39/40
 - 5s - loss: 0.0321 - acc: 0.9788 - val_loss: 0.1155 - val_acc: 0.9635
Epoch 40/40
 - 5s - loss: 0.0322 - acc: 0.9789 - val_loss: 0.1155 - val_acc: 0.9661
Epoch 1/40
 - 0s - loss: 0.0949 - acc: 0.9639
Epoch 2/40
 - 0s - loss: 0.0691 - acc: 0.9672
Epoch 3/40
 - 0s - loss: 0.0614 - acc: 0.9712
Epoch 4/40
 - 0s - loss: 0.0545 - acc: 0.9718
Epoch 5/40
 - 0s - loss: 0.0517 - acc: 0.9731
Epoch 6/40
 - 0s - loss: 0.0490 - acc: 0.9726
Epoch 7/40
 - 0s - loss: 0.0466 - acc: 0.9741
Epoch 8/40
 - 0s - loss: 0.0436 - acc: 0.9751
Epoch 9/40
 - 0s - loss: 0.0437 - acc: 0.9761
Epoch 10/40
 - 0s - loss: 0.0422 - acc: 0.9765
Epoch 11/40
 - 0s - loss: 0.0417 - acc: 0.9756
Epoch 12/40
 - 0s - loss: 0.0405 - acc: 0.9759
Epoch 13/40
 - 0s - loss: 0.0410 - acc: 0.9770
Epoch 14/40
 - 0s - loss: 0.0406 - acc: 0.9766
Epoch 15/40
 - 0s - loss: 0.0393 - acc: 0.9777
Epoch 16/40
 - 0s - loss: 0.0382 - acc: 0.9776
Epoch 17/40
 - 0s - loss: 0.0373 - acc: 0.9776
Epoch 18/40
 - 0s - loss: 0.0383 - acc: 0.9781
Epoch 19/40
 - 0s - loss: 0.0375 - acc: 0.9771
Epoch 20/40
 - 0s - loss: 0.0377 - acc: 0.9765
Epoch 21/40
 - 0s - loss: 0.0377 - acc: 0.9773
Epoch 22/40
 - 0s - loss: 0.0365 - acc: 0.9795
Epoch 23/40
 - 0s - loss: 0.0366 - acc: 0.9784
Epoch 24/40
 - 0s - loss: 0.0361 - acc: 0.9777
Epoch 25/40
 - 0s - loss: 0.0358 - acc: 0.9773
Epoch 26/40
 - 0s - loss: 0.0355 - acc: 0.9790
Epoch 27/40
 - 0s - loss: 0.0340 - acc: 0.9790
Epoch 28/40
 - 0s - loss: 0.0356 - acc: 0.9796
Epoch 29/40
 - 0s - loss: 0.0343 - acc: 0.9776
Epoch 30/40
 - 0s - loss: 0.0345 - acc: 0.9789
Epoch 31/40
 - 0s - loss: 0.0340 - acc: 0.9781
Epoch 32/40
 - 0s - loss: 0.0346 - acc: 0.9776
Epoch 33/40
 - 0s - loss: 0.0342 - acc: 0.9785
Epoch 34/40
 - 0s - loss: 0.0344 - acc: 0.9784
Epoch 35/40
 - 0s - loss: 0.0333 - acc: 0.9790
Epoch 36/40
 - 0s - loss: 0.0327 - acc: 0.9786
Epoch 37/40
 - 0s - loss: 0.0331 - acc: 0.9793
Epoch 38/40
 - 0s - loss: 0.0342 - acc: 0.9796
Epoch 39/40
 - 0s - loss: 0.0334 - acc: 0.9782
Epoch 40/40
 - 0s - loss: 0.0334 - acc: 0.9782
# Training time = 0:03:47.382646
# F-Score(Ordinary) = 0.004, Recall: 0.2, Precision: 0.002
# F-Score(lvc) = 0.015, Recall: 0.5, Precision: 0.008
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_297 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_298 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_297 (Embedding)       (None, 4, 48)        705264      input_297[0][0]                  
__________________________________________________________________________________________________
embedding_298 (Embedding)       (None, 4, 24)        5640        input_298[0][0]                  
__________________________________________________________________________________________________
flatten_297 (Flatten)           (None, 192)          0           embedding_297[0][0]              
__________________________________________________________________________________________________
flatten_298 (Flatten)           (None, 96)           0           embedding_298[0][0]              
__________________________________________________________________________________________________
concatenate_149 (Concatenate)   (None, 288)          0           flatten_297[0][0]                
                                                                 flatten_298[0][0]                
__________________________________________________________________________________________________
dense_297 (Dense)               (None, 24)           6936        concatenate_149[0][0]            
__________________________________________________________________________________________________
dropout_149 (Dropout)           (None, 24)           0           dense_297[0][0]                  
__________________________________________________________________________________________________
dense_298 (Dense)               (None, 8)            200         dropout_149[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1351 - acc: 0.9549 - val_loss: 0.0724 - val_acc: 0.9675
Epoch 2/40
 - 5s - loss: 0.0598 - acc: 0.9707 - val_loss: 0.0715 - val_acc: 0.9683
Epoch 3/40
 - 5s - loss: 0.0501 - acc: 0.9734 - val_loss: 0.0736 - val_acc: 0.9677
Epoch 4/40
 - 5s - loss: 0.0456 - acc: 0.9748 - val_loss: 0.0794 - val_acc: 0.9677
Epoch 5/40
 - 5s - loss: 0.0429 - acc: 0.9759 - val_loss: 0.0828 - val_acc: 0.9678
Epoch 6/40
 - 5s - loss: 0.0414 - acc: 0.9758 - val_loss: 0.0841 - val_acc: 0.9685
Epoch 7/40
 - 5s - loss: 0.0401 - acc: 0.9762 - val_loss: 0.0858 - val_acc: 0.9690
Epoch 8/40
 - 5s - loss: 0.0387 - acc: 0.9764 - val_loss: 0.0917 - val_acc: 0.9675
Epoch 9/40
 - 5s - loss: 0.0381 - acc: 0.9767 - val_loss: 0.0915 - val_acc: 0.9679
Epoch 10/40
 - 5s - loss: 0.0373 - acc: 0.9772 - val_loss: 0.0911 - val_acc: 0.9675
Epoch 11/40
 - 5s - loss: 0.0369 - acc: 0.9771 - val_loss: 0.0976 - val_acc: 0.9674
Epoch 12/40
 - 5s - loss: 0.0362 - acc: 0.9776 - val_loss: 0.0978 - val_acc: 0.9668
Epoch 13/40
 - 5s - loss: 0.0363 - acc: 0.9774 - val_loss: 0.0983 - val_acc: 0.9677
Epoch 14/40
 - 5s - loss: 0.0356 - acc: 0.9774 - val_loss: 0.1006 - val_acc: 0.9671
Epoch 15/40
 - 5s - loss: 0.0354 - acc: 0.9773 - val_loss: 0.1048 - val_acc: 0.9685
Epoch 16/40
 - 5s - loss: 0.0350 - acc: 0.9774 - val_loss: 0.1020 - val_acc: 0.9678
Epoch 17/40
 - 5s - loss: 0.0350 - acc: 0.9776 - val_loss: 0.1065 - val_acc: 0.9674
Epoch 18/40
 - 5s - loss: 0.0343 - acc: 0.9782 - val_loss: 0.1092 - val_acc: 0.9669
Epoch 19/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.1136 - val_acc: 0.9680
Epoch 20/40
 - 5s - loss: 0.0341 - acc: 0.9781 - val_loss: 0.1079 - val_acc: 0.9671
Epoch 21/40
 - 5s - loss: 0.0337 - acc: 0.9788 - val_loss: 0.1196 - val_acc: 0.9661
Epoch 22/40
 - 5s - loss: 0.0338 - acc: 0.9781 - val_loss: 0.1135 - val_acc: 0.9686
Epoch 23/40
 - 5s - loss: 0.0339 - acc: 0.9780 - val_loss: 0.1164 - val_acc: 0.9659
Epoch 24/40
 - 5s - loss: 0.0339 - acc: 0.9785 - val_loss: 0.1125 - val_acc: 0.9654
Epoch 25/40
 - 5s - loss: 0.0338 - acc: 0.9781 - val_loss: 0.1155 - val_acc: 0.9672
Epoch 26/40
 - 5s - loss: 0.0334 - acc: 0.9784 - val_loss: 0.1181 - val_acc: 0.9654
Epoch 27/40
 - 5s - loss: 0.0332 - acc: 0.9782 - val_loss: 0.1166 - val_acc: 0.9673
Epoch 28/40
 - 5s - loss: 0.0334 - acc: 0.9778 - val_loss: 0.1180 - val_acc: 0.9673
Epoch 29/40
 - 5s - loss: 0.0333 - acc: 0.9778 - val_loss: 0.1188 - val_acc: 0.9666
Epoch 30/40
 - 5s - loss: 0.0329 - acc: 0.9787 - val_loss: 0.1157 - val_acc: 0.9654
Epoch 31/40
 - 5s - loss: 0.0329 - acc: 0.9790 - val_loss: 0.1109 - val_acc: 0.9677
Epoch 32/40
 - 5s - loss: 0.0332 - acc: 0.9784 - val_loss: 0.1196 - val_acc: 0.9665
Epoch 33/40
 - 5s - loss: 0.0330 - acc: 0.9787 - val_loss: 0.1156 - val_acc: 0.9682
Epoch 34/40
 - 5s - loss: 0.0330 - acc: 0.9786 - val_loss: 0.1174 - val_acc: 0.9673
Epoch 35/40
 - 5s - loss: 0.0326 - acc: 0.9785 - val_loss: 0.1220 - val_acc: 0.9665
Epoch 36/40
 - 5s - loss: 0.0326 - acc: 0.9786 - val_loss: 0.1273 - val_acc: 0.9663
Epoch 37/40
 - 5s - loss: 0.0327 - acc: 0.9790 - val_loss: 0.1187 - val_acc: 0.9660
Epoch 38/40
 - 5s - loss: 0.0328 - acc: 0.9783 - val_loss: 0.1225 - val_acc: 0.9668
Epoch 39/40
 - 5s - loss: 0.0323 - acc: 0.9788 - val_loss: 0.1233 - val_acc: 0.9679
Epoch 40/40
 - 5s - loss: 0.0326 - acc: 0.9786 - val_loss: 0.1165 - val_acc: 0.9682
Epoch 1/40
 - 0s - loss: 0.1011 - acc: 0.9658
Epoch 2/40
 - 0s - loss: 0.0746 - acc: 0.9684
Epoch 3/40
 - 0s - loss: 0.0634 - acc: 0.9707
Epoch 4/40
 - 0s - loss: 0.0569 - acc: 0.9708
Epoch 5/40
 - 0s - loss: 0.0528 - acc: 0.9714
Epoch 6/40
 - 0s - loss: 0.0499 - acc: 0.9735
Epoch 7/40
 - 0s - loss: 0.0489 - acc: 0.9744
Epoch 8/40
 - 0s - loss: 0.0470 - acc: 0.9741
Epoch 9/40
 - 0s - loss: 0.0443 - acc: 0.9749
Epoch 10/40
 - 0s - loss: 0.0436 - acc: 0.9757
Epoch 11/40
 - 0s - loss: 0.0418 - acc: 0.9765
Epoch 12/40
 - 0s - loss: 0.0417 - acc: 0.9762
Epoch 13/40
 - 0s - loss: 0.0405 - acc: 0.9770
Epoch 14/40
 - 0s - loss: 0.0401 - acc: 0.9773
Epoch 15/40
 - 0s - loss: 0.0389 - acc: 0.9779
Epoch 16/40
 - 0s - loss: 0.0392 - acc: 0.9774
Epoch 17/40
 - 0s - loss: 0.0392 - acc: 0.9778
Epoch 18/40
 - 0s - loss: 0.0378 - acc: 0.9773
Epoch 19/40
 - 0s - loss: 0.0376 - acc: 0.9783
Epoch 20/40
 - 0s - loss: 0.0378 - acc: 0.9791
Epoch 21/40
 - 0s - loss: 0.0367 - acc: 0.9788
Epoch 22/40
 - 0s - loss: 0.0363 - acc: 0.9786
Epoch 23/40
 - 0s - loss: 0.0375 - acc: 0.9769
Epoch 24/40
 - 0s - loss: 0.0362 - acc: 0.9778
Epoch 25/40
 - 0s - loss: 0.0358 - acc: 0.9788
Epoch 26/40
 - 0s - loss: 0.0359 - acc: 0.9786
Epoch 27/40
 - 0s - loss: 0.0352 - acc: 0.9788
Epoch 28/40
 - 0s - loss: 0.0357 - acc: 0.9782
Epoch 29/40
 - 0s - loss: 0.0353 - acc: 0.9782
Epoch 30/40
 - 0s - loss: 0.0356 - acc: 0.9777
Epoch 31/40
 - 0s - loss: 0.0347 - acc: 0.9798
Epoch 32/40
 - 0s - loss: 0.0360 - acc: 0.9788
Epoch 33/40
 - 0s - loss: 0.0355 - acc: 0.9790
Epoch 34/40
 - 0s - loss: 0.0347 - acc: 0.9797
Epoch 35/40
 - 0s - loss: 0.0347 - acc: 0.9789
Epoch 36/40
 - 0s - loss: 0.0342 - acc: 0.9793
Epoch 37/40
 - 0s - loss: 0.0346 - acc: 0.9789
Epoch 38/40
 - 0s - loss: 0.0348 - acc: 0.9780
Epoch 39/40
 - 0s - loss: 0.0343 - acc: 0.9786
Epoch 40/40
 - 0s - loss: 0.0339 - acc: 0.9786
# Training time = 0:03:41.144497
# F-Score(Ordinary) = 0.158, Recall: 0.488, Precision: 0.094
# F-Score(lvc) = 0.346, Recall: 0.451, Precision: 0.28
# F-Score(id) = 0.03, Recall: 0.75, Precision: 0.016
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_299 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_300 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_299 (Embedding)       (None, 4, 48)        705264      input_299[0][0]                  
__________________________________________________________________________________________________
embedding_300 (Embedding)       (None, 4, 24)        5640        input_300[0][0]                  
__________________________________________________________________________________________________
flatten_299 (Flatten)           (None, 192)          0           embedding_299[0][0]              
__________________________________________________________________________________________________
flatten_300 (Flatten)           (None, 96)           0           embedding_300[0][0]              
__________________________________________________________________________________________________
concatenate_150 (Concatenate)   (None, 288)          0           flatten_299[0][0]                
                                                                 flatten_300[0][0]                
__________________________________________________________________________________________________
dense_299 (Dense)               (None, 24)           6936        concatenate_150[0][0]            
__________________________________________________________________________________________________
dropout_150 (Dropout)           (None, 24)           0           dense_299[0][0]                  
__________________________________________________________________________________________________
dense_300 (Dense)               (None, 8)            200         dropout_150[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.001
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1320 - acc: 0.9550 - val_loss: 0.0735 - val_acc: 0.9677
Epoch 2/40
 - 5s - loss: 0.0600 - acc: 0.9709 - val_loss: 0.0755 - val_acc: 0.9675
Epoch 3/40
 - 5s - loss: 0.0509 - acc: 0.9729 - val_loss: 0.0767 - val_acc: 0.9675
Epoch 4/40
 - 5s - loss: 0.0462 - acc: 0.9742 - val_loss: 0.0759 - val_acc: 0.9682
Epoch 5/40
 - 5s - loss: 0.0435 - acc: 0.9749 - val_loss: 0.0801 - val_acc: 0.9680
Epoch 6/40
 - 5s - loss: 0.0412 - acc: 0.9753 - val_loss: 0.0818 - val_acc: 0.9680
Epoch 7/40
 - 5s - loss: 0.0399 - acc: 0.9759 - val_loss: 0.0849 - val_acc: 0.9682
Epoch 8/40
 - 5s - loss: 0.0388 - acc: 0.9762 - val_loss: 0.0881 - val_acc: 0.9676
Epoch 9/40
 - 5s - loss: 0.0380 - acc: 0.9768 - val_loss: 0.0899 - val_acc: 0.9670
Epoch 10/40
 - 5s - loss: 0.0374 - acc: 0.9768 - val_loss: 0.0909 - val_acc: 0.9683
Epoch 11/40
 - 5s - loss: 0.0370 - acc: 0.9772 - val_loss: 0.0936 - val_acc: 0.9685
Epoch 12/40
 - 5s - loss: 0.0365 - acc: 0.9770 - val_loss: 0.0891 - val_acc: 0.9675
Epoch 13/40
 - 5s - loss: 0.0358 - acc: 0.9771 - val_loss: 0.0909 - val_acc: 0.9667
Epoch 14/40
 - 5s - loss: 0.0355 - acc: 0.9777 - val_loss: 0.0987 - val_acc: 0.9685
Epoch 15/40
 - 5s - loss: 0.0350 - acc: 0.9773 - val_loss: 0.0945 - val_acc: 0.9682
Epoch 16/40
 - 5s - loss: 0.0347 - acc: 0.9776 - val_loss: 0.1026 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0344 - acc: 0.9781 - val_loss: 0.1039 - val_acc: 0.9671
Epoch 18/40
 - 5s - loss: 0.0344 - acc: 0.9778 - val_loss: 0.0992 - val_acc: 0.9665
Epoch 19/40
 - 5s - loss: 0.0343 - acc: 0.9778 - val_loss: 0.1013 - val_acc: 0.9666
Epoch 20/40
 - 5s - loss: 0.0338 - acc: 0.9779 - val_loss: 0.1042 - val_acc: 0.9657
Epoch 21/40
 - 5s - loss: 0.0336 - acc: 0.9782 - val_loss: 0.1032 - val_acc: 0.9659
Epoch 22/40
 - 5s - loss: 0.0335 - acc: 0.9784 - val_loss: 0.1056 - val_acc: 0.9643
Epoch 23/40
 - 5s - loss: 0.0333 - acc: 0.9782 - val_loss: 0.1019 - val_acc: 0.9678
Epoch 24/40
 - 5s - loss: 0.0331 - acc: 0.9783 - val_loss: 0.1015 - val_acc: 0.9676
Epoch 25/40
 - 5s - loss: 0.0331 - acc: 0.9786 - val_loss: 0.1002 - val_acc: 0.9671
Epoch 26/40
 - 5s - loss: 0.0332 - acc: 0.9782 - val_loss: 0.1077 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0328 - acc: 0.9785 - val_loss: 0.1085 - val_acc: 0.9660
Epoch 28/40
 - 5s - loss: 0.0322 - acc: 0.9793 - val_loss: 0.1057 - val_acc: 0.9673
Epoch 29/40
 - 5s - loss: 0.0328 - acc: 0.9789 - val_loss: 0.1121 - val_acc: 0.9660
Epoch 30/40
 - 5s - loss: 0.0328 - acc: 0.9784 - val_loss: 0.1072 - val_acc: 0.9681
Epoch 31/40
 - 5s - loss: 0.0325 - acc: 0.9790 - val_loss: 0.1089 - val_acc: 0.9662
Epoch 32/40
 - 5s - loss: 0.0325 - acc: 0.9789 - val_loss: 0.1069 - val_acc: 0.9630
Epoch 33/40
 - 5s - loss: 0.0323 - acc: 0.9785 - val_loss: 0.1109 - val_acc: 0.9657
Epoch 34/40
 - 5s - loss: 0.0322 - acc: 0.9790 - val_loss: 0.1136 - val_acc: 0.9649
Epoch 35/40
 - 5s - loss: 0.0324 - acc: 0.9787 - val_loss: 0.1140 - val_acc: 0.9682
Epoch 36/40
 - 5s - loss: 0.0326 - acc: 0.9790 - val_loss: 0.1149 - val_acc: 0.9656
Epoch 37/40
 - 5s - loss: 0.0321 - acc: 0.9784 - val_loss: 0.1171 - val_acc: 0.9650
Epoch 38/40
 - 5s - loss: 0.0322 - acc: 0.9790 - val_loss: 0.1143 - val_acc: 0.9670
Epoch 39/40
 - 5s - loss: 0.0322 - acc: 0.9789 - val_loss: 0.1171 - val_acc: 0.9662
Epoch 40/40
 - 5s - loss: 0.0322 - acc: 0.9790 - val_loss: 0.1175 - val_acc: 0.9667
Epoch 1/40
 - 0s - loss: 0.1012 - acc: 0.9655
Epoch 2/40
 - 0s - loss: 0.0699 - acc: 0.9704
Epoch 3/40
 - 0s - loss: 0.0606 - acc: 0.9704
Epoch 4/40
 - 0s - loss: 0.0590 - acc: 0.9717
Epoch 5/40
 - 0s - loss: 0.0543 - acc: 0.9733
Epoch 6/40
 - 0s - loss: 0.0506 - acc: 0.9746
Epoch 7/40
 - 0s - loss: 0.0485 - acc: 0.9751
Epoch 8/40
 - 0s - loss: 0.0475 - acc: 0.9743
Epoch 9/40
 - 0s - loss: 0.0448 - acc: 0.9744
Epoch 10/40
 - 0s - loss: 0.0444 - acc: 0.9749
Epoch 11/40
 - 0s - loss: 0.0433 - acc: 0.9757
Epoch 12/40
 - 0s - loss: 0.0424 - acc: 0.9758
Epoch 13/40
 - 0s - loss: 0.0423 - acc: 0.9761
Epoch 14/40
 - 0s - loss: 0.0415 - acc: 0.9760
Epoch 15/40
 - 0s - loss: 0.0416 - acc: 0.9771
Epoch 16/40
 - 0s - loss: 0.0406 - acc: 0.9762
Epoch 17/40
 - 0s - loss: 0.0393 - acc: 0.9767
Epoch 18/40
 - 0s - loss: 0.0389 - acc: 0.9770
Epoch 19/40
 - 0s - loss: 0.0390 - acc: 0.9771
Epoch 20/40
 - 0s - loss: 0.0377 - acc: 0.9775
Epoch 21/40
 - 0s - loss: 0.0369 - acc: 0.9778
Epoch 22/40
 - 0s - loss: 0.0364 - acc: 0.9782
Epoch 23/40
 - 0s - loss: 0.0361 - acc: 0.9770
Epoch 24/40
 - 0s - loss: 0.0354 - acc: 0.9777
Epoch 25/40
 - 0s - loss: 0.0360 - acc: 0.9775
Epoch 26/40
 - 0s - loss: 0.0353 - acc: 0.9779
Epoch 27/40
 - 0s - loss: 0.0352 - acc: 0.9775
Epoch 28/40
 - 0s - loss: 0.0351 - acc: 0.9786
Epoch 29/40
 - 0s - loss: 0.0347 - acc: 0.9787
Epoch 30/40
 - 0s - loss: 0.0346 - acc: 0.9789
Epoch 31/40
 - 0s - loss: 0.0337 - acc: 0.9779
Epoch 32/40
 - 0s - loss: 0.0341 - acc: 0.9790
Epoch 33/40
 - 0s - loss: 0.0341 - acc: 0.9782
Epoch 34/40
 - 0s - loss: 0.0338 - acc: 0.9789
Epoch 35/40
 - 0s - loss: 0.0339 - acc: 0.9782
Epoch 36/40
 - 0s - loss: 0.0340 - acc: 0.9788
Epoch 37/40
 - 0s - loss: 0.0337 - acc: 0.9781
Epoch 38/40
 - 0s - loss: 0.0336 - acc: 0.9791
Epoch 39/40
 - 0s - loss: 0.0341 - acc: 0.9791
Epoch 40/40
 - 0s - loss: 0.0323 - acc: 0.9783
# Training time = 0:03:47.435914
# F-Score(Ordinary) = 0.609, Recall: 0.629, Precision: 0.591
# F-Score(lvc) = 0.446, Recall: 0.413, Precision: 0.485
# F-Score(ireflv) = 0.72, Recall: 0.676, Precision: 0.77
# F-Score(id) = 0.646, Recall: 0.817, Precision: 0.534
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_301 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_302 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_301 (Embedding)       (None, 4, 48)        705264      input_301[0][0]                  
__________________________________________________________________________________________________
embedding_302 (Embedding)       (None, 4, 24)        5640        input_302[0][0]                  
__________________________________________________________________________________________________
flatten_301 (Flatten)           (None, 192)          0           embedding_301[0][0]              
__________________________________________________________________________________________________
flatten_302 (Flatten)           (None, 96)           0           embedding_302[0][0]              
__________________________________________________________________________________________________
concatenate_151 (Concatenate)   (None, 288)          0           flatten_301[0][0]                
                                                                 flatten_302[0][0]                
__________________________________________________________________________________________________
dense_301 (Dense)               (None, 24)           6936        concatenate_151[0][0]            
__________________________________________________________________________________________________
dropout_151 (Dropout)           (None, 24)           0           dense_301[0][0]                  
__________________________________________________________________________________________________
dense_302 (Dense)               (None, 8)            200         dropout_151[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1052 - acc: 0.9613 - val_loss: 0.0708 - val_acc: 0.9672
Epoch 2/40
 - 5s - loss: 0.0562 - acc: 0.9714 - val_loss: 0.0777 - val_acc: 0.9663
Epoch 3/40
 - 5s - loss: 0.0479 - acc: 0.9736 - val_loss: 0.0820 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0448 - acc: 0.9754 - val_loss: 0.0811 - val_acc: 0.9660
Epoch 5/40
 - 5s - loss: 0.0427 - acc: 0.9752 - val_loss: 0.0903 - val_acc: 0.9675
Epoch 6/40
 - 5s - loss: 0.0412 - acc: 0.9759 - val_loss: 0.0880 - val_acc: 0.9674
Epoch 7/40
 - 5s - loss: 0.0407 - acc: 0.9757 - val_loss: 0.0876 - val_acc: 0.9688
Epoch 8/40
 - 5s - loss: 0.0393 - acc: 0.9763 - val_loss: 0.0940 - val_acc: 0.9677
Epoch 9/40
 - 5s - loss: 0.0386 - acc: 0.9769 - val_loss: 0.0976 - val_acc: 0.9681
Epoch 10/40
 - 5s - loss: 0.0386 - acc: 0.9761 - val_loss: 0.0975 - val_acc: 0.9678
Epoch 11/40
 - 5s - loss: 0.0376 - acc: 0.9774 - val_loss: 0.1105 - val_acc: 0.9661
Epoch 12/40
 - 5s - loss: 0.0374 - acc: 0.9773 - val_loss: 0.1003 - val_acc: 0.9683
Epoch 13/40
 - 5s - loss: 0.0372 - acc: 0.9769 - val_loss: 0.0972 - val_acc: 0.9679
Epoch 14/40
 - 5s - loss: 0.0370 - acc: 0.9769 - val_loss: 0.1074 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0367 - acc: 0.9778 - val_loss: 0.1055 - val_acc: 0.9686
Epoch 16/40
 - 5s - loss: 0.0363 - acc: 0.9772 - val_loss: 0.1031 - val_acc: 0.9681
Epoch 17/40
 - 5s - loss: 0.0360 - acc: 0.9775 - val_loss: 0.1082 - val_acc: 0.9671
Epoch 18/40
 - 5s - loss: 0.0362 - acc: 0.9776 - val_loss: 0.1101 - val_acc: 0.9666
Epoch 19/40
 - 5s - loss: 0.0359 - acc: 0.9774 - val_loss: 0.1097 - val_acc: 0.9683
Epoch 20/40
 - 5s - loss: 0.0358 - acc: 0.9779 - val_loss: 0.1162 - val_acc: 0.9675
Epoch 21/40
 - 5s - loss: 0.0355 - acc: 0.9779 - val_loss: 0.1121 - val_acc: 0.9663
Epoch 22/40
 - 5s - loss: 0.0350 - acc: 0.9781 - val_loss: 0.1091 - val_acc: 0.9670
Epoch 23/40
 - 5s - loss: 0.0357 - acc: 0.9779 - val_loss: 0.1084 - val_acc: 0.9682
Epoch 24/40
 - 5s - loss: 0.0354 - acc: 0.9777 - val_loss: 0.1154 - val_acc: 0.9658
Epoch 25/40
 - 5s - loss: 0.0352 - acc: 0.9775 - val_loss: 0.1117 - val_acc: 0.9685
Epoch 26/40
 - 5s - loss: 0.0345 - acc: 0.9781 - val_loss: 0.1165 - val_acc: 0.9673
Epoch 27/40
 - 5s - loss: 0.0346 - acc: 0.9780 - val_loss: 0.1203 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0347 - acc: 0.9777 - val_loss: 0.1189 - val_acc: 0.9677
Epoch 29/40
 - 5s - loss: 0.0350 - acc: 0.9780 - val_loss: 0.1142 - val_acc: 0.9652
Epoch 30/40
 - 5s - loss: 0.0344 - acc: 0.9781 - val_loss: 0.1138 - val_acc: 0.9674
Epoch 31/40
 - 5s - loss: 0.0354 - acc: 0.9779 - val_loss: 0.1148 - val_acc: 0.9666
Epoch 32/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.1185 - val_acc: 0.9676
Epoch 33/40
 - 5s - loss: 0.0347 - acc: 0.9782 - val_loss: 0.1244 - val_acc: 0.9665
Epoch 34/40
 - 5s - loss: 0.0345 - acc: 0.9782 - val_loss: 0.1212 - val_acc: 0.9673
Epoch 35/40
 - 5s - loss: 0.0346 - acc: 0.9785 - val_loss: 0.1232 - val_acc: 0.9679
Epoch 36/40
 - 5s - loss: 0.0347 - acc: 0.9782 - val_loss: 0.1215 - val_acc: 0.9669
Epoch 37/40
 - 5s - loss: 0.0342 - acc: 0.9785 - val_loss: 0.1239 - val_acc: 0.9681
Epoch 38/40
 - 5s - loss: 0.0350 - acc: 0.9782 - val_loss: 0.1184 - val_acc: 0.9653
Epoch 39/40
 - 5s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.1263 - val_acc: 0.9677
Epoch 40/40
 - 5s - loss: 0.0348 - acc: 0.9785 - val_loss: 0.1302 - val_acc: 0.9672
Epoch 1/40
 - 0s - loss: 0.1100 - acc: 0.9643
Epoch 2/40
 - 1s - loss: 0.0769 - acc: 0.9687
Epoch 3/40
 - 0s - loss: 0.0659 - acc: 0.9701
Epoch 4/40
 - 0s - loss: 0.0618 - acc: 0.9709
Epoch 5/40
 - 0s - loss: 0.0575 - acc: 0.9721
Epoch 6/40
 - 0s - loss: 0.0546 - acc: 0.9721
Epoch 7/40
 - 0s - loss: 0.0521 - acc: 0.9732
Epoch 8/40
 - 0s - loss: 0.0498 - acc: 0.9746
Epoch 9/40
 - 0s - loss: 0.0496 - acc: 0.9733
Epoch 10/40
 - 0s - loss: 0.0475 - acc: 0.9753
Epoch 11/40
 - 0s - loss: 0.0466 - acc: 0.9757
Epoch 12/40
 - 0s - loss: 0.0453 - acc: 0.9761
Epoch 13/40
 - 0s - loss: 0.0428 - acc: 0.9763
Epoch 14/40
 - 0s - loss: 0.0433 - acc: 0.9772
Epoch 15/40
 - 0s - loss: 0.0426 - acc: 0.9753
Epoch 16/40
 - 0s - loss: 0.0429 - acc: 0.9770
Epoch 17/40
 - 0s - loss: 0.0418 - acc: 0.9777
Epoch 18/40
 - 1s - loss: 0.0402 - acc: 0.9766
Epoch 19/40
 - 0s - loss: 0.0408 - acc: 0.9773
Epoch 20/40
 - 1s - loss: 0.0404 - acc: 0.9775
Epoch 21/40
 - 1s - loss: 0.0399 - acc: 0.9774
Epoch 22/40
 - 1s - loss: 0.0393 - acc: 0.9776
Epoch 23/40
 - 1s - loss: 0.0402 - acc: 0.9787
Epoch 24/40
 - 0s - loss: 0.0378 - acc: 0.9778
Epoch 25/40
 - 1s - loss: 0.0380 - acc: 0.9777
Epoch 26/40
 - 0s - loss: 0.0391 - acc: 0.9775
Epoch 27/40
 - 1s - loss: 0.0382 - acc: 0.9788
Epoch 28/40
 - 1s - loss: 0.0375 - acc: 0.9774
Epoch 29/40
 - 1s - loss: 0.0377 - acc: 0.9777
Epoch 30/40
 - 1s - loss: 0.0385 - acc: 0.9770
Epoch 31/40
 - 1s - loss: 0.0381 - acc: 0.9787
Epoch 32/40
 - 1s - loss: 0.0377 - acc: 0.9796
Epoch 33/40
 - 1s - loss: 0.0371 - acc: 0.9781
Epoch 34/40
 - 0s - loss: 0.0357 - acc: 0.9804
Epoch 35/40
 - 0s - loss: 0.0375 - acc: 0.9781
Epoch 36/40
 - 1s - loss: 0.0377 - acc: 0.9788
Epoch 37/40
 - 0s - loss: 0.0355 - acc: 0.9782
Epoch 38/40
 - 1s - loss: 0.0389 - acc: 0.9783
Epoch 39/40
 - 0s - loss: 0.0375 - acc: 0.9782
Epoch 40/40
 - 0s - loss: 0.0365 - acc: 0.9784
# Training time = 0:03:48.805225
# F-Score(Ordinary) = 0.356, Recall: 0.734, Precision: 0.235
# F-Score(ireflv) = 0.603, Recall: 0.724, Precision: 0.516
# F-Score(id) = 0.329, Recall: 0.732, Precision: 0.212
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_303 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_304 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_303 (Embedding)       (None, 4, 48)        705264      input_303[0][0]                  
__________________________________________________________________________________________________
embedding_304 (Embedding)       (None, 4, 24)        5640        input_304[0][0]                  
__________________________________________________________________________________________________
flatten_303 (Flatten)           (None, 192)          0           embedding_303[0][0]              
__________________________________________________________________________________________________
flatten_304 (Flatten)           (None, 96)           0           embedding_304[0][0]              
__________________________________________________________________________________________________
concatenate_152 (Concatenate)   (None, 288)          0           flatten_303[0][0]                
                                                                 flatten_304[0][0]                
__________________________________________________________________________________________________
dense_303 (Dense)               (None, 24)           6936        concatenate_152[0][0]            
__________________________________________________________________________________________________
dropout_152 (Dropout)           (None, 24)           0           dense_303[0][0]                  
__________________________________________________________________________________________________
dense_304 (Dense)               (None, 8)            200         dropout_152[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1057 - acc: 0.9620 - val_loss: 0.0694 - val_acc: 0.9676
Epoch 2/40
 - 5s - loss: 0.0560 - acc: 0.9716 - val_loss: 0.0765 - val_acc: 0.9656
Epoch 3/40
 - 5s - loss: 0.0492 - acc: 0.9736 - val_loss: 0.0813 - val_acc: 0.9654
Epoch 4/40
 - 5s - loss: 0.0451 - acc: 0.9750 - val_loss: 0.0801 - val_acc: 0.9682
Epoch 5/40
 - 5s - loss: 0.0426 - acc: 0.9756 - val_loss: 0.0824 - val_acc: 0.9674
Epoch 6/40
 - 5s - loss: 0.0413 - acc: 0.9762 - val_loss: 0.0881 - val_acc: 0.9677
Epoch 7/40
 - 5s - loss: 0.0402 - acc: 0.9763 - val_loss: 0.0896 - val_acc: 0.9675
Epoch 8/40
 - 5s - loss: 0.0392 - acc: 0.9766 - val_loss: 0.0890 - val_acc: 0.9680
Epoch 9/40
 - 5s - loss: 0.0388 - acc: 0.9764 - val_loss: 0.0974 - val_acc: 0.9677
Epoch 10/40
 - 5s - loss: 0.0382 - acc: 0.9762 - val_loss: 0.1018 - val_acc: 0.9675
Epoch 11/40
 - 5s - loss: 0.0376 - acc: 0.9773 - val_loss: 0.0975 - val_acc: 0.9676
Epoch 12/40
 - 5s - loss: 0.0376 - acc: 0.9772 - val_loss: 0.0975 - val_acc: 0.9681
Epoch 13/40
 - 5s - loss: 0.0369 - acc: 0.9773 - val_loss: 0.1061 - val_acc: 0.9667
Epoch 14/40
 - 5s - loss: 0.0363 - acc: 0.9769 - val_loss: 0.1146 - val_acc: 0.9674
Epoch 15/40
 - 5s - loss: 0.0366 - acc: 0.9774 - val_loss: 0.1007 - val_acc: 0.9678
Epoch 16/40
 - 5s - loss: 0.0365 - acc: 0.9774 - val_loss: 0.1034 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0362 - acc: 0.9774 - val_loss: 0.1002 - val_acc: 0.9652
Epoch 18/40
 - 5s - loss: 0.0360 - acc: 0.9779 - val_loss: 0.1026 - val_acc: 0.9661
Epoch 19/40
 - 5s - loss: 0.0359 - acc: 0.9778 - val_loss: 0.0990 - val_acc: 0.9675
Epoch 20/40
 - 5s - loss: 0.0357 - acc: 0.9776 - val_loss: 0.1054 - val_acc: 0.9665
Epoch 21/40
 - 5s - loss: 0.0354 - acc: 0.9782 - val_loss: 0.1050 - val_acc: 0.9657
Epoch 22/40
 - 5s - loss: 0.0356 - acc: 0.9778 - val_loss: 0.1036 - val_acc: 0.9673
Epoch 23/40
 - 5s - loss: 0.0358 - acc: 0.9778 - val_loss: 0.1062 - val_acc: 0.9674
Epoch 24/40
 - 5s - loss: 0.0355 - acc: 0.9780 - val_loss: 0.1161 - val_acc: 0.9663
Epoch 25/40
 - 5s - loss: 0.0356 - acc: 0.9781 - val_loss: 0.1072 - val_acc: 0.9679
Epoch 26/40
 - 5s - loss: 0.0351 - acc: 0.9777 - val_loss: 0.1067 - val_acc: 0.9684
Epoch 27/40
 - 5s - loss: 0.0357 - acc: 0.9779 - val_loss: 0.1110 - val_acc: 0.9687
Epoch 28/40
 - 5s - loss: 0.0352 - acc: 0.9781 - val_loss: 0.1170 - val_acc: 0.9673
Epoch 29/40
 - 5s - loss: 0.0353 - acc: 0.9783 - val_loss: 0.1162 - val_acc: 0.9677
Epoch 30/40
 - 5s - loss: 0.0349 - acc: 0.9778 - val_loss: 0.1087 - val_acc: 0.9684
Epoch 31/40
 - 5s - loss: 0.0351 - acc: 0.9783 - val_loss: 0.1147 - val_acc: 0.9682
Epoch 32/40
 - 5s - loss: 0.0349 - acc: 0.9781 - val_loss: 0.1088 - val_acc: 0.9682
Epoch 33/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1130 - val_acc: 0.9679
Epoch 34/40
 - 5s - loss: 0.0349 - acc: 0.9780 - val_loss: 0.1173 - val_acc: 0.9680
Epoch 35/40
 - 5s - loss: 0.0351 - acc: 0.9781 - val_loss: 0.1098 - val_acc: 0.9655
Epoch 36/40
 - 5s - loss: 0.0345 - acc: 0.9786 - val_loss: 0.1193 - val_acc: 0.9670
Epoch 37/40
 - 5s - loss: 0.0344 - acc: 0.9787 - val_loss: 0.1143 - val_acc: 0.9670
Epoch 38/40
 - 5s - loss: 0.0343 - acc: 0.9782 - val_loss: 0.1148 - val_acc: 0.9671
Epoch 39/40
 - 5s - loss: 0.0343 - acc: 0.9782 - val_loss: 0.1181 - val_acc: 0.9673
Epoch 40/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.1196 - val_acc: 0.9674
Epoch 1/40
 - 0s - loss: 0.1018 - acc: 0.9659
Epoch 2/40
 - 0s - loss: 0.0788 - acc: 0.9679
Epoch 3/40
 - 0s - loss: 0.0662 - acc: 0.9706
Epoch 4/40
 - 0s - loss: 0.0591 - acc: 0.9726
Epoch 5/40
 - 0s - loss: 0.0558 - acc: 0.9738
Epoch 6/40
 - 0s - loss: 0.0529 - acc: 0.9737
Epoch 7/40
 - 0s - loss: 0.0489 - acc: 0.9740
Epoch 8/40
 - 0s - loss: 0.0489 - acc: 0.9755
Epoch 9/40
 - 0s - loss: 0.0459 - acc: 0.9761
Epoch 10/40
 - 0s - loss: 0.0459 - acc: 0.9770
Epoch 11/40
 - 0s - loss: 0.0447 - acc: 0.9757
Epoch 12/40
 - 0s - loss: 0.0436 - acc: 0.9772
Epoch 13/40
 - 0s - loss: 0.0431 - acc: 0.9776
Epoch 14/40
 - 0s - loss: 0.0421 - acc: 0.9769
Epoch 15/40
 - 0s - loss: 0.0431 - acc: 0.9760
Epoch 16/40
 - 0s - loss: 0.0407 - acc: 0.9777
Epoch 17/40
 - 0s - loss: 0.0399 - acc: 0.9778
Epoch 18/40
 - 0s - loss: 0.0400 - acc: 0.9771
Epoch 19/40
 - 0s - loss: 0.0414 - acc: 0.9779
Epoch 20/40
 - 0s - loss: 0.0411 - acc: 0.9770
Epoch 21/40
 - 0s - loss: 0.0419 - acc: 0.9766
Epoch 22/40
 - 0s - loss: 0.0397 - acc: 0.9778
Epoch 23/40
 - 0s - loss: 0.0391 - acc: 0.9781
Epoch 24/40
 - 0s - loss: 0.0405 - acc: 0.9783
Epoch 25/40
 - 0s - loss: 0.0382 - acc: 0.9786
Epoch 26/40
 - 0s - loss: 0.0377 - acc: 0.9780
Epoch 27/40
 - 0s - loss: 0.0384 - acc: 0.9780
Epoch 28/40
 - 0s - loss: 0.0380 - acc: 0.9784
Epoch 29/40
 - 0s - loss: 0.0370 - acc: 0.9786
Epoch 30/40
 - 0s - loss: 0.0390 - acc: 0.9781
Epoch 31/40
 - 0s - loss: 0.0377 - acc: 0.9786
Epoch 32/40
 - 0s - loss: 0.0360 - acc: 0.9779
Epoch 33/40
 - 0s - loss: 0.0385 - acc: 0.9794
Epoch 34/40
 - 0s - loss: 0.0373 - acc: 0.9781
Epoch 35/40
 - 0s - loss: 0.0375 - acc: 0.9776
Epoch 36/40
 - 0s - loss: 0.0366 - acc: 0.9788
Epoch 37/40
 - 0s - loss: 0.0367 - acc: 0.9783
Epoch 38/40
 - 0s - loss: 0.0364 - acc: 0.9778
Epoch 39/40
 - 0s - loss: 0.0361 - acc: 0.9778
Epoch 40/40
 - 0s - loss: 0.0361 - acc: 0.9781
# Training time = 0:03:48.179160
# F-Score(Ordinary) = 0.013, Recall: 0.3, Precision: 0.007
# F-Score(lvc) = 0.015, Recall: 1.0, Precision: 0.008
# F-Score(ireflv) = 0.016, Recall: 0.167, Precision: 0.008
# F-Score(id) = 0.01, Recall: 0.333, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_305 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_306 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_305 (Embedding)       (None, 4, 48)        705264      input_305[0][0]                  
__________________________________________________________________________________________________
embedding_306 (Embedding)       (None, 4, 24)        5640        input_306[0][0]                  
__________________________________________________________________________________________________
flatten_305 (Flatten)           (None, 192)          0           embedding_305[0][0]              
__________________________________________________________________________________________________
flatten_306 (Flatten)           (None, 96)           0           embedding_306[0][0]              
__________________________________________________________________________________________________
concatenate_153 (Concatenate)   (None, 288)          0           flatten_305[0][0]                
                                                                 flatten_306[0][0]                
__________________________________________________________________________________________________
dense_305 (Dense)               (None, 24)           6936        concatenate_153[0][0]            
__________________________________________________________________________________________________
dropout_153 (Dropout)           (None, 24)           0           dense_305[0][0]                  
__________________________________________________________________________________________________
dense_306 (Dense)               (None, 8)            200         dropout_153[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1043 - acc: 0.9623 - val_loss: 0.0722 - val_acc: 0.9671
Epoch 2/40
 - 5s - loss: 0.0562 - acc: 0.9715 - val_loss: 0.0710 - val_acc: 0.9673
Epoch 3/40
 - 5s - loss: 0.0483 - acc: 0.9742 - val_loss: 0.0764 - val_acc: 0.9671
Epoch 4/40
 - 5s - loss: 0.0451 - acc: 0.9749 - val_loss: 0.0800 - val_acc: 0.9679
Epoch 5/40
 - 5s - loss: 0.0425 - acc: 0.9756 - val_loss: 0.0877 - val_acc: 0.9676
Epoch 6/40
 - 5s - loss: 0.0414 - acc: 0.9760 - val_loss: 0.0886 - val_acc: 0.9680
Epoch 7/40
 - 5s - loss: 0.0400 - acc: 0.9769 - val_loss: 0.0838 - val_acc: 0.9682
Epoch 8/40
 - 5s - loss: 0.0399 - acc: 0.9765 - val_loss: 0.0940 - val_acc: 0.9692
Epoch 9/40
 - 5s - loss: 0.0386 - acc: 0.9770 - val_loss: 0.0927 - val_acc: 0.9672
Epoch 10/40
 - 5s - loss: 0.0379 - acc: 0.9772 - val_loss: 0.0907 - val_acc: 0.9662
Epoch 11/40
 - 5s - loss: 0.0375 - acc: 0.9772 - val_loss: 0.0944 - val_acc: 0.9669
Epoch 12/40
 - 5s - loss: 0.0368 - acc: 0.9771 - val_loss: 0.1000 - val_acc: 0.9674
Epoch 13/40
 - 5s - loss: 0.0364 - acc: 0.9774 - val_loss: 0.0984 - val_acc: 0.9671
Epoch 14/40
 - 5s - loss: 0.0364 - acc: 0.9775 - val_loss: 0.1034 - val_acc: 0.9673
Epoch 15/40
 - 5s - loss: 0.0362 - acc: 0.9775 - val_loss: 0.1074 - val_acc: 0.9657
Epoch 16/40
 - 5s - loss: 0.0363 - acc: 0.9772 - val_loss: 0.1048 - val_acc: 0.9663
Epoch 17/40
 - 5s - loss: 0.0358 - acc: 0.9777 - val_loss: 0.1089 - val_acc: 0.9654
Epoch 18/40
 - 5s - loss: 0.0358 - acc: 0.9776 - val_loss: 0.1061 - val_acc: 0.9674
Epoch 19/40
 - 5s - loss: 0.0356 - acc: 0.9777 - val_loss: 0.1079 - val_acc: 0.9677
Epoch 20/40
 - 5s - loss: 0.0357 - acc: 0.9775 - val_loss: 0.1066 - val_acc: 0.9679
Epoch 21/40
 - 5s - loss: 0.0358 - acc: 0.9780 - val_loss: 0.1050 - val_acc: 0.9662
Epoch 22/40
 - 5s - loss: 0.0355 - acc: 0.9777 - val_loss: 0.1092 - val_acc: 0.9678
Epoch 23/40
 - 5s - loss: 0.0350 - acc: 0.9781 - val_loss: 0.1036 - val_acc: 0.9668
Epoch 24/40
 - 5s - loss: 0.0350 - acc: 0.9777 - val_loss: 0.1047 - val_acc: 0.9688
Epoch 25/40
 - 5s - loss: 0.0357 - acc: 0.9778 - val_loss: 0.1132 - val_acc: 0.9667
Epoch 26/40
 - 5s - loss: 0.0350 - acc: 0.9774 - val_loss: 0.1106 - val_acc: 0.9671
Epoch 27/40
 - 5s - loss: 0.0353 - acc: 0.9778 - val_loss: 0.1179 - val_acc: 0.9671
Epoch 28/40
 - 5s - loss: 0.0346 - acc: 0.9784 - val_loss: 0.1072 - val_acc: 0.9668
Epoch 29/40
 - 5s - loss: 0.0347 - acc: 0.9780 - val_loss: 0.1173 - val_acc: 0.9671
Epoch 30/40
 - 5s - loss: 0.0347 - acc: 0.9782 - val_loss: 0.1174 - val_acc: 0.9673
Epoch 31/40
 - 5s - loss: 0.0352 - acc: 0.9781 - val_loss: 0.1170 - val_acc: 0.9679
Epoch 32/40
 - 5s - loss: 0.0352 - acc: 0.9785 - val_loss: 0.1179 - val_acc: 0.9669
Epoch 33/40
 - 5s - loss: 0.0349 - acc: 0.9782 - val_loss: 0.1201 - val_acc: 0.9681
Epoch 34/40
 - 5s - loss: 0.0347 - acc: 0.9782 - val_loss: 0.1184 - val_acc: 0.9666
Epoch 35/40
 - 5s - loss: 0.0345 - acc: 0.9785 - val_loss: 0.1185 - val_acc: 0.9676
Epoch 36/40
 - 5s - loss: 0.0344 - acc: 0.9780 - val_loss: 0.1171 - val_acc: 0.9674
Epoch 37/40
 - 5s - loss: 0.0339 - acc: 0.9785 - val_loss: 0.1178 - val_acc: 0.9668
Epoch 38/40
 - 5s - loss: 0.0346 - acc: 0.9783 - val_loss: 0.1160 - val_acc: 0.9670
Epoch 39/40
 - 5s - loss: 0.0354 - acc: 0.9786 - val_loss: 0.1184 - val_acc: 0.9669
Epoch 40/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.1213 - val_acc: 0.9678
Epoch 1/40
 - 0s - loss: 0.1011 - acc: 0.9641
Epoch 2/40
 - 0s - loss: 0.0683 - acc: 0.9687
Epoch 3/40
 - 0s - loss: 0.0604 - acc: 0.9713
Epoch 4/40
 - 0s - loss: 0.0545 - acc: 0.9720
Epoch 5/40
 - 0s - loss: 0.0535 - acc: 0.9743
Epoch 6/40
 - 0s - loss: 0.0502 - acc: 0.9737
Epoch 7/40
 - 0s - loss: 0.0464 - acc: 0.9752
Epoch 8/40
 - 0s - loss: 0.0438 - acc: 0.9761
Epoch 9/40
 - 0s - loss: 0.0451 - acc: 0.9768
Epoch 10/40
 - 0s - loss: 0.0421 - acc: 0.9771
Epoch 11/40
 - 0s - loss: 0.0405 - acc: 0.9772
Epoch 12/40
 - 0s - loss: 0.0402 - acc: 0.9774
Epoch 13/40
 - 0s - loss: 0.0409 - acc: 0.9778
Epoch 14/40
 - 0s - loss: 0.0389 - acc: 0.9767
Epoch 15/40
 - 0s - loss: 0.0389 - acc: 0.9779
Epoch 16/40
 - 0s - loss: 0.0383 - acc: 0.9776
Epoch 17/40
 - 0s - loss: 0.0391 - acc: 0.9776
Epoch 18/40
 - 0s - loss: 0.0375 - acc: 0.9787
Epoch 19/40
 - 0s - loss: 0.0382 - acc: 0.9773
Epoch 20/40
 - 0s - loss: 0.0386 - acc: 0.9767
Epoch 21/40
 - 0s - loss: 0.0378 - acc: 0.9777
Epoch 22/40
 - 0s - loss: 0.0375 - acc: 0.9777
Epoch 23/40
 - 0s - loss: 0.0377 - acc: 0.9778
Epoch 24/40
 - 0s - loss: 0.0373 - acc: 0.9779
Epoch 25/40
 - 0s - loss: 0.0367 - acc: 0.9781
Epoch 26/40
 - 0s - loss: 0.0359 - acc: 0.9793
Epoch 27/40
 - 0s - loss: 0.0356 - acc: 0.9786
Epoch 28/40
 - 0s - loss: 0.0367 - acc: 0.9777
Epoch 29/40
 - 0s - loss: 0.0354 - acc: 0.9778
Epoch 30/40
 - 0s - loss: 0.0359 - acc: 0.9787
Epoch 31/40
 - 0s - loss: 0.0362 - acc: 0.9782
Epoch 32/40
 - 0s - loss: 0.0357 - acc: 0.9772
Epoch 33/40
 - 0s - loss: 0.0362 - acc: 0.9779
Epoch 34/40
 - 0s - loss: 0.0346 - acc: 0.9783
Epoch 35/40
 - 0s - loss: 0.0367 - acc: 0.9778
Epoch 36/40
 - 0s - loss: 0.0361 - acc: 0.9773
Epoch 37/40
 - 0s - loss: 0.0360 - acc: 0.9792
Epoch 38/40
 - 0s - loss: 0.0357 - acc: 0.9789
Epoch 39/40
 - 0s - loss: 0.0362 - acc: 0.9792
Epoch 40/40
 - 0s - loss: 0.0382 - acc: 0.9778
# Training time = 0:03:48.343406
# F-Score(Ordinary) = 0.091, Recall: 0.629, Precision: 0.049
# F-Score(lvc) = 0.087, Recall: 1.0, Precision: 0.045
# F-Score(ireflv) = 0.015, Recall: 0.125, Precision: 0.008
# F-Score(id) = 0.14, Recall: 0.714, Precision: 0.078
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_307 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_308 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_307 (Embedding)       (None, 4, 48)        705264      input_307[0][0]                  
__________________________________________________________________________________________________
embedding_308 (Embedding)       (None, 4, 24)        5640        input_308[0][0]                  
__________________________________________________________________________________________________
flatten_307 (Flatten)           (None, 192)          0           embedding_307[0][0]              
__________________________________________________________________________________________________
flatten_308 (Flatten)           (None, 96)           0           embedding_308[0][0]              
__________________________________________________________________________________________________
concatenate_154 (Concatenate)   (None, 288)          0           flatten_307[0][0]                
                                                                 flatten_308[0][0]                
__________________________________________________________________________________________________
dense_307 (Dense)               (None, 24)           6936        concatenate_154[0][0]            
__________________________________________________________________________________________________
dropout_154 (Dropout)           (None, 24)           0           dense_307[0][0]                  
__________________________________________________________________________________________________
dense_308 (Dense)               (None, 8)            200         dropout_154[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1080 - acc: 0.9596 - val_loss: 0.0699 - val_acc: 0.9678
Epoch 2/40
 - 5s - loss: 0.0564 - acc: 0.9712 - val_loss: 0.0722 - val_acc: 0.9676
Epoch 3/40
 - 5s - loss: 0.0483 - acc: 0.9739 - val_loss: 0.0754 - val_acc: 0.9662
Epoch 4/40
 - 5s - loss: 0.0447 - acc: 0.9748 - val_loss: 0.0811 - val_acc: 0.9674
Epoch 5/40
 - 5s - loss: 0.0424 - acc: 0.9758 - val_loss: 0.0864 - val_acc: 0.9676
Epoch 6/40
 - 5s - loss: 0.0414 - acc: 0.9761 - val_loss: 0.0858 - val_acc: 0.9690
Epoch 7/40
 - 5s - loss: 0.0405 - acc: 0.9765 - val_loss: 0.0864 - val_acc: 0.9685
Epoch 8/40
 - 5s - loss: 0.0394 - acc: 0.9765 - val_loss: 0.0935 - val_acc: 0.9675
Epoch 9/40
 - 5s - loss: 0.0386 - acc: 0.9768 - val_loss: 0.0940 - val_acc: 0.9672
Epoch 10/40
 - 5s - loss: 0.0380 - acc: 0.9767 - val_loss: 0.0905 - val_acc: 0.9672
Epoch 11/40
 - 5s - loss: 0.0375 - acc: 0.9769 - val_loss: 0.0966 - val_acc: 0.9665
Epoch 12/40
 - 5s - loss: 0.0373 - acc: 0.9771 - val_loss: 0.0992 - val_acc: 0.9671
Epoch 13/40
 - 5s - loss: 0.0372 - acc: 0.9773 - val_loss: 0.0996 - val_acc: 0.9679
Epoch 14/40
 - 5s - loss: 0.0366 - acc: 0.9773 - val_loss: 0.0995 - val_acc: 0.9671
Epoch 15/40
 - 5s - loss: 0.0367 - acc: 0.9772 - val_loss: 0.1035 - val_acc: 0.9676
Epoch 16/40
 - 5s - loss: 0.0363 - acc: 0.9774 - val_loss: 0.1000 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0360 - acc: 0.9776 - val_loss: 0.1010 - val_acc: 0.9667
Epoch 18/40
 - 5s - loss: 0.0361 - acc: 0.9776 - val_loss: 0.1071 - val_acc: 0.9678
Epoch 19/40
 - 5s - loss: 0.0358 - acc: 0.9778 - val_loss: 0.1086 - val_acc: 0.9674
Epoch 20/40
 - 5s - loss: 0.0356 - acc: 0.9774 - val_loss: 0.1074 - val_acc: 0.9681
Epoch 21/40
 - 5s - loss: 0.0354 - acc: 0.9780 - val_loss: 0.1171 - val_acc: 0.9684
Epoch 22/40
 - 5s - loss: 0.0358 - acc: 0.9778 - val_loss: 0.1116 - val_acc: 0.9679
Epoch 23/40
 - 5s - loss: 0.0356 - acc: 0.9777 - val_loss: 0.1090 - val_acc: 0.9663
Epoch 24/40
 - 5s - loss: 0.0352 - acc: 0.9778 - val_loss: 0.1080 - val_acc: 0.9661
Epoch 25/40
 - 5s - loss: 0.0352 - acc: 0.9778 - val_loss: 0.1175 - val_acc: 0.9679
Epoch 26/40
 - 5s - loss: 0.0351 - acc: 0.9783 - val_loss: 0.1123 - val_acc: 0.9636
Epoch 27/40
 - 5s - loss: 0.0348 - acc: 0.9782 - val_loss: 0.1153 - val_acc: 0.9681
Epoch 28/40
 - 5s - loss: 0.0348 - acc: 0.9778 - val_loss: 0.1155 - val_acc: 0.9680
Epoch 29/40
 - 5s - loss: 0.0352 - acc: 0.9776 - val_loss: 0.1214 - val_acc: 0.9668
Epoch 30/40
 - 5s - loss: 0.0351 - acc: 0.9779 - val_loss: 0.1182 - val_acc: 0.9677
Epoch 31/40
 - 5s - loss: 0.0346 - acc: 0.9782 - val_loss: 0.1179 - val_acc: 0.9682
Epoch 32/40
 - 5s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.1257 - val_acc: 0.9652
Epoch 33/40
 - 5s - loss: 0.0342 - acc: 0.9784 - val_loss: 0.1232 - val_acc: 0.9682
Epoch 34/40
 - 5s - loss: 0.0350 - acc: 0.9781 - val_loss: 0.1216 - val_acc: 0.9675
Epoch 35/40
 - 5s - loss: 0.0345 - acc: 0.9785 - val_loss: 0.1201 - val_acc: 0.9680
Epoch 36/40
 - 5s - loss: 0.0346 - acc: 0.9781 - val_loss: 0.1152 - val_acc: 0.9659
Epoch 37/40
 - 5s - loss: 0.0358 - acc: 0.9784 - val_loss: 0.1179 - val_acc: 0.9683
Epoch 38/40
 - 5s - loss: 0.0348 - acc: 0.9783 - val_loss: 0.1249 - val_acc: 0.9671
Epoch 39/40
 - 5s - loss: 0.0346 - acc: 0.9784 - val_loss: 0.1156 - val_acc: 0.9690
Epoch 40/40
 - 5s - loss: 0.0349 - acc: 0.9780 - val_loss: 0.1192 - val_acc: 0.9685
Epoch 1/40
 - 0s - loss: 0.1038 - acc: 0.9629
Epoch 2/40
 - 0s - loss: 0.0731 - acc: 0.9684
Epoch 3/40
 - 0s - loss: 0.0627 - acc: 0.9710
Epoch 4/40
 - 0s - loss: 0.0582 - acc: 0.9727
Epoch 5/40
 - 0s - loss: 0.0538 - acc: 0.9726
Epoch 6/40
 - 0s - loss: 0.0502 - acc: 0.9741
Epoch 7/40
 - 0s - loss: 0.0491 - acc: 0.9745
Epoch 8/40
 - 0s - loss: 0.0475 - acc: 0.9747
Epoch 9/40
 - 0s - loss: 0.0466 - acc: 0.9746
Epoch 10/40
 - 0s - loss: 0.0459 - acc: 0.9755
Epoch 11/40
 - 0s - loss: 0.0447 - acc: 0.9760
Epoch 12/40
 - 0s - loss: 0.0449 - acc: 0.9753
Epoch 13/40
 - 0s - loss: 0.0438 - acc: 0.9769
Epoch 14/40
 - 0s - loss: 0.0435 - acc: 0.9774
Epoch 15/40
 - 0s - loss: 0.0432 - acc: 0.9770
Epoch 16/40
 - 0s - loss: 0.0426 - acc: 0.9768
Epoch 17/40
 - 0s - loss: 0.0420 - acc: 0.9763
Epoch 18/40
 - 0s - loss: 0.0415 - acc: 0.9775
Epoch 19/40
 - 0s - loss: 0.0417 - acc: 0.9774
Epoch 20/40
 - 0s - loss: 0.0437 - acc: 0.9778
Epoch 21/40
 - 0s - loss: 0.0408 - acc: 0.9778
Epoch 22/40
 - 0s - loss: 0.0417 - acc: 0.9763
Epoch 23/40
 - 0s - loss: 0.0411 - acc: 0.9772
Epoch 24/40
 - 0s - loss: 0.0396 - acc: 0.9774
Epoch 25/40
 - 0s - loss: 0.0406 - acc: 0.9772
Epoch 26/40
 - 0s - loss: 0.0396 - acc: 0.9793
Epoch 27/40
 - 0s - loss: 0.0386 - acc: 0.9788
Epoch 28/40
 - 0s - loss: 0.0376 - acc: 0.9778
Epoch 29/40
 - 0s - loss: 0.0381 - acc: 0.9786
Epoch 30/40
 - 0s - loss: 0.0378 - acc: 0.9794
Epoch 31/40
 - 0s - loss: 0.0385 - acc: 0.9797
Epoch 32/40
 - 0s - loss: 0.0375 - acc: 0.9794
Epoch 33/40
 - 0s - loss: 0.0382 - acc: 0.9775
Epoch 34/40
 - 0s - loss: 0.0380 - acc: 0.9793
Epoch 35/40
 - 0s - loss: 0.0374 - acc: 0.9783
Epoch 36/40
 - 0s - loss: 0.0371 - acc: 0.9782
Epoch 37/40
 - 0s - loss: 0.0381 - acc: 0.9786
Epoch 38/40
 - 0s - loss: 0.0366 - acc: 0.9793
Epoch 39/40
 - 0s - loss: 0.0369 - acc: 0.9785
Epoch 40/40
 - 0s - loss: 0.0363 - acc: 0.9797
# Training time = 0:03:41.130880
# F-Score(Ordinary) = 0.15, Recall: 0.465, Precision: 0.089
# F-Score(lvc) = 0.346, Recall: 0.451, Precision: 0.28
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_309 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_310 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_309 (Embedding)       (None, 4, 48)        705264      input_309[0][0]                  
__________________________________________________________________________________________________
embedding_310 (Embedding)       (None, 4, 24)        5640        input_310[0][0]                  
__________________________________________________________________________________________________
flatten_309 (Flatten)           (None, 192)          0           embedding_309[0][0]              
__________________________________________________________________________________________________
flatten_310 (Flatten)           (None, 96)           0           embedding_310[0][0]              
__________________________________________________________________________________________________
concatenate_155 (Concatenate)   (None, 288)          0           flatten_309[0][0]                
                                                                 flatten_310[0][0]                
__________________________________________________________________________________________________
dense_309 (Dense)               (None, 24)           6936        concatenate_155[0][0]            
__________________________________________________________________________________________________
dropout_155 (Dropout)           (None, 24)           0           dense_309[0][0]                  
__________________________________________________________________________________________________
dense_310 (Dense)               (None, 8)            200         dropout_155[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.002
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.1078 - acc: 0.9600 - val_loss: 0.0720 - val_acc: 0.9671
Epoch 2/40
 - 5s - loss: 0.0570 - acc: 0.9718 - val_loss: 0.0740 - val_acc: 0.9681
Epoch 3/40
 - 5s - loss: 0.0497 - acc: 0.9733 - val_loss: 0.0821 - val_acc: 0.9664
Epoch 4/40
 - 5s - loss: 0.0456 - acc: 0.9743 - val_loss: 0.0749 - val_acc: 0.9681
Epoch 5/40
 - 5s - loss: 0.0436 - acc: 0.9750 - val_loss: 0.0846 - val_acc: 0.9679
Epoch 6/40
 - 5s - loss: 0.0414 - acc: 0.9755 - val_loss: 0.0861 - val_acc: 0.9689
Epoch 7/40
 - 5s - loss: 0.0407 - acc: 0.9760 - val_loss: 0.0907 - val_acc: 0.9679
Epoch 8/40
 - 5s - loss: 0.0397 - acc: 0.9762 - val_loss: 0.0891 - val_acc: 0.9676
Epoch 9/40
 - 5s - loss: 0.0389 - acc: 0.9769 - val_loss: 0.0898 - val_acc: 0.9671
Epoch 10/40
 - 5s - loss: 0.0385 - acc: 0.9765 - val_loss: 0.0905 - val_acc: 0.9680
Epoch 11/40
 - 5s - loss: 0.0380 - acc: 0.9772 - val_loss: 0.0993 - val_acc: 0.9667
Epoch 12/40
 - 5s - loss: 0.0376 - acc: 0.9767 - val_loss: 0.0956 - val_acc: 0.9679
Epoch 13/40
 - 5s - loss: 0.0372 - acc: 0.9773 - val_loss: 0.0916 - val_acc: 0.9668
Epoch 14/40
 - 5s - loss: 0.0371 - acc: 0.9775 - val_loss: 0.0986 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0372 - acc: 0.9771 - val_loss: 0.0934 - val_acc: 0.9683
Epoch 16/40
 - 5s - loss: 0.0360 - acc: 0.9774 - val_loss: 0.1001 - val_acc: 0.9676
Epoch 17/40
 - 5s - loss: 0.0363 - acc: 0.9777 - val_loss: 0.0961 - val_acc: 0.9673
Epoch 18/40
 - 5s - loss: 0.0358 - acc: 0.9774 - val_loss: 0.0994 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0357 - acc: 0.9779 - val_loss: 0.1013 - val_acc: 0.9669
Epoch 20/40
 - 5s - loss: 0.0355 - acc: 0.9777 - val_loss: 0.1029 - val_acc: 0.9659
Epoch 21/40
 - 5s - loss: 0.0358 - acc: 0.9777 - val_loss: 0.1080 - val_acc: 0.9638
Epoch 22/40
 - 5s - loss: 0.0359 - acc: 0.9778 - val_loss: 0.1024 - val_acc: 0.9645
Epoch 23/40
 - 5s - loss: 0.0353 - acc: 0.9776 - val_loss: 0.1068 - val_acc: 0.9663
Epoch 24/40
 - 5s - loss: 0.0356 - acc: 0.9776 - val_loss: 0.1026 - val_acc: 0.9663
Epoch 25/40
 - 5s - loss: 0.0355 - acc: 0.9781 - val_loss: 0.1040 - val_acc: 0.9669
Epoch 26/40
 - 5s - loss: 0.0354 - acc: 0.9781 - val_loss: 0.1095 - val_acc: 0.9660
Epoch 27/40
 - 5s - loss: 0.0347 - acc: 0.9783 - val_loss: 0.1101 - val_acc: 0.9659
Epoch 28/40
 - 5s - loss: 0.0352 - acc: 0.9788 - val_loss: 0.1095 - val_acc: 0.9668
Epoch 29/40
 - 5s - loss: 0.0354 - acc: 0.9779 - val_loss: 0.1141 - val_acc: 0.9667
Epoch 30/40
 - 5s - loss: 0.0345 - acc: 0.9783 - val_loss: 0.1099 - val_acc: 0.9675
Epoch 31/40
 - 5s - loss: 0.0351 - acc: 0.9787 - val_loss: 0.1144 - val_acc: 0.9676
Epoch 32/40
 - 5s - loss: 0.0347 - acc: 0.9786 - val_loss: 0.1144 - val_acc: 0.9659
Epoch 33/40
 - 5s - loss: 0.0346 - acc: 0.9779 - val_loss: 0.1135 - val_acc: 0.9677
Epoch 34/40
 - 5s - loss: 0.0345 - acc: 0.9785 - val_loss: 0.1197 - val_acc: 0.9670
Epoch 35/40
 - 5s - loss: 0.0346 - acc: 0.9780 - val_loss: 0.1197 - val_acc: 0.9682
Epoch 36/40
 - 5s - loss: 0.0352 - acc: 0.9781 - val_loss: 0.1161 - val_acc: 0.9655
Epoch 37/40
 - 5s - loss: 0.0347 - acc: 0.9780 - val_loss: 0.1167 - val_acc: 0.9662
Epoch 38/40
 - 5s - loss: 0.0343 - acc: 0.9787 - val_loss: 0.1176 - val_acc: 0.9668
Epoch 39/40
 - 5s - loss: 0.0340 - acc: 0.9786 - val_loss: 0.1132 - val_acc: 0.9659
Epoch 40/40
 - 5s - loss: 0.0344 - acc: 0.9786 - val_loss: 0.1140 - val_acc: 0.9659
Epoch 1/40
 - 0s - loss: 0.1003 - acc: 0.9640
Epoch 2/40
 - 0s - loss: 0.0742 - acc: 0.9685
Epoch 3/40
 - 0s - loss: 0.0636 - acc: 0.9696
Epoch 4/40
 - 0s - loss: 0.0574 - acc: 0.9724
Epoch 5/40
 - 0s - loss: 0.0526 - acc: 0.9736
Epoch 6/40
 - 0s - loss: 0.0496 - acc: 0.9733
Epoch 7/40
 - 0s - loss: 0.0464 - acc: 0.9751
Epoch 8/40
 - 0s - loss: 0.0446 - acc: 0.9756
Epoch 9/40
 - 0s - loss: 0.0437 - acc: 0.9757
Epoch 10/40
 - 0s - loss: 0.0427 - acc: 0.9755
Epoch 11/40
 - 0s - loss: 0.0419 - acc: 0.9767
Epoch 12/40
 - 0s - loss: 0.0417 - acc: 0.9761
Epoch 13/40
 - 0s - loss: 0.0414 - acc: 0.9764
Epoch 14/40
 - 0s - loss: 0.0409 - acc: 0.9765
Epoch 15/40
 - 0s - loss: 0.0385 - acc: 0.9775
Epoch 16/40
 - 0s - loss: 0.0389 - acc: 0.9765
Epoch 17/40
 - 0s - loss: 0.0382 - acc: 0.9770
Epoch 18/40
 - 0s - loss: 0.0377 - acc: 0.9773
Epoch 19/40
 - 0s - loss: 0.0384 - acc: 0.9772
Epoch 20/40
 - 0s - loss: 0.0377 - acc: 0.9782
Epoch 21/40
 - 0s - loss: 0.0385 - acc: 0.9779
Epoch 22/40
 - 0s - loss: 0.0370 - acc: 0.9778
Epoch 23/40
 - 0s - loss: 0.0379 - acc: 0.9776
Epoch 24/40
 - 0s - loss: 0.0368 - acc: 0.9775
Epoch 25/40
 - 0s - loss: 0.0365 - acc: 0.9781
Epoch 26/40
 - 0s - loss: 0.0362 - acc: 0.9783
Epoch 27/40
 - 0s - loss: 0.0364 - acc: 0.9771
Epoch 28/40
 - 0s - loss: 0.0360 - acc: 0.9777
Epoch 29/40
 - 0s - loss: 0.0363 - acc: 0.9775
Epoch 30/40
 - 0s - loss: 0.0356 - acc: 0.9787
Epoch 31/40
 - 0s - loss: 0.0364 - acc: 0.9780
Epoch 32/40
 - 0s - loss: 0.0360 - acc: 0.9779
Epoch 33/40
 - 0s - loss: 0.0348 - acc: 0.9787
Epoch 34/40
 - 0s - loss: 0.0347 - acc: 0.9793
Epoch 35/40
 - 0s - loss: 0.0346 - acc: 0.9780
Epoch 36/40
 - 0s - loss: 0.0354 - acc: 0.9798
Epoch 37/40
 - 0s - loss: 0.0342 - acc: 0.9782
Epoch 38/40
 - 0s - loss: 0.0343 - acc: 0.9791
Epoch 39/40
 - 0s - loss: 0.0355 - acc: 0.9788
Epoch 40/40
 - 0s - loss: 0.0356 - acc: 0.9782
# Training time = 0:03:41.017930
# F-Score(Ordinary) = 0.526, Recall: 0.457, Precision: 0.62
# F-Score(lvc) = 0.558, Recall: 0.763, Precision: 0.439
# F-Score(ireflv) = 0.726, Recall: 0.788, Precision: 0.672
# F-Score(id) = 0.417, Recall: 0.303, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_311 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_312 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_311 (Embedding)       (None, 4, 48)        705264      input_311[0][0]                  
__________________________________________________________________________________________________
embedding_312 (Embedding)       (None, 4, 24)        5640        input_312[0][0]                  
__________________________________________________________________________________________________
flatten_311 (Flatten)           (None, 192)          0           embedding_311[0][0]              
__________________________________________________________________________________________________
flatten_312 (Flatten)           (None, 96)           0           embedding_312[0][0]              
__________________________________________________________________________________________________
concatenate_156 (Concatenate)   (None, 288)          0           flatten_311[0][0]                
                                                                 flatten_312[0][0]                
__________________________________________________________________________________________________
dense_311 (Dense)               (None, 24)           6936        concatenate_156[0][0]            
__________________________________________________________________________________________________
dropout_156 (Dropout)           (None, 24)           0           dense_311[0][0]                  
__________________________________________________________________________________________________
dense_312 (Dense)               (None, 8)            200         dropout_156[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0921 - acc: 0.9633 - val_loss: 0.0755 - val_acc: 0.9670
Epoch 2/40
 - 5s - loss: 0.0586 - acc: 0.9708 - val_loss: 0.0799 - val_acc: 0.9668
Epoch 3/40
 - 5s - loss: 0.0527 - acc: 0.9729 - val_loss: 0.0776 - val_acc: 0.9680
Epoch 4/40
 - 5s - loss: 0.0497 - acc: 0.9744 - val_loss: 0.0820 - val_acc: 0.9675
Epoch 5/40
 - 5s - loss: 0.0469 - acc: 0.9744 - val_loss: 0.1085 - val_acc: 0.9677
Epoch 6/40
 - 5s - loss: 0.0472 - acc: 0.9741 - val_loss: 0.0961 - val_acc: 0.9671
Epoch 7/40
 - 5s - loss: 0.0464 - acc: 0.9749 - val_loss: 0.0927 - val_acc: 0.9669
Epoch 8/40
 - 5s - loss: 0.0452 - acc: 0.9753 - val_loss: 0.0987 - val_acc: 0.9684
Epoch 9/40
 - 5s - loss: 0.0443 - acc: 0.9756 - val_loss: 0.1018 - val_acc: 0.9681
Epoch 10/40
 - 5s - loss: 0.0442 - acc: 0.9753 - val_loss: 0.1019 - val_acc: 0.9676
Epoch 11/40
 - 5s - loss: 0.0449 - acc: 0.9760 - val_loss: 0.1039 - val_acc: 0.9678
Epoch 12/40
 - 5s - loss: 0.0442 - acc: 0.9756 - val_loss: 0.1035 - val_acc: 0.9671
Epoch 13/40
 - 5s - loss: 0.0430 - acc: 0.9765 - val_loss: 0.1138 - val_acc: 0.9662
Epoch 14/40
 - 5s - loss: 0.0429 - acc: 0.9761 - val_loss: 0.1086 - val_acc: 0.9673
Epoch 15/40
 - 5s - loss: 0.0438 - acc: 0.9764 - val_loss: 0.1015 - val_acc: 0.9668
Epoch 16/40
 - 5s - loss: 0.0453 - acc: 0.9758 - val_loss: 0.0970 - val_acc: 0.9670
Epoch 17/40
 - 5s - loss: 0.0435 - acc: 0.9764 - val_loss: 0.1103 - val_acc: 0.9659
Epoch 18/40
 - 5s - loss: 0.0435 - acc: 0.9763 - val_loss: 0.1239 - val_acc: 0.9679
Epoch 19/40
 - 5s - loss: 0.0444 - acc: 0.9764 - val_loss: 0.1063 - val_acc: 0.9674
Epoch 20/40
 - 5s - loss: 0.0426 - acc: 0.9765 - val_loss: 0.1157 - val_acc: 0.9684
Epoch 21/40
 - 5s - loss: 0.0428 - acc: 0.9771 - val_loss: 0.1161 - val_acc: 0.9657
Epoch 22/40
 - 5s - loss: 0.0434 - acc: 0.9764 - val_loss: 0.1076 - val_acc: 0.9666
Epoch 23/40
 - 5s - loss: 0.0436 - acc: 0.9766 - val_loss: 0.1204 - val_acc: 0.9667
Epoch 24/40
 - 5s - loss: 0.0432 - acc: 0.9765 - val_loss: 0.1161 - val_acc: 0.9672
Epoch 25/40
 - 5s - loss: 0.0439 - acc: 0.9763 - val_loss: 0.1252 - val_acc: 0.9672
Epoch 26/40
 - 5s - loss: 0.0433 - acc: 0.9765 - val_loss: 0.1200 - val_acc: 0.9673
Epoch 27/40
 - 5s - loss: 0.0422 - acc: 0.9765 - val_loss: 0.1213 - val_acc: 0.9658
Epoch 28/40
 - 5s - loss: 0.0438 - acc: 0.9768 - val_loss: 0.1268 - val_acc: 0.9674
Epoch 29/40
 - 5s - loss: 0.0438 - acc: 0.9763 - val_loss: 0.1224 - val_acc: 0.9666
Epoch 30/40
 - 5s - loss: 0.0431 - acc: 0.9765 - val_loss: 0.1300 - val_acc: 0.9681
Epoch 31/40
 - 5s - loss: 0.0446 - acc: 0.9766 - val_loss: 0.1278 - val_acc: 0.9662
Epoch 32/40
 - 5s - loss: 0.0433 - acc: 0.9771 - val_loss: 0.1328 - val_acc: 0.9676
Epoch 33/40
 - 5s - loss: 0.0431 - acc: 0.9768 - val_loss: 0.1214 - val_acc: 0.9670
Epoch 34/40
 - 5s - loss: 0.0436 - acc: 0.9766 - val_loss: 0.1205 - val_acc: 0.9676
Epoch 35/40
 - 5s - loss: 0.0431 - acc: 0.9767 - val_loss: 0.1277 - val_acc: 0.9676
Epoch 36/40
 - 5s - loss: 0.0437 - acc: 0.9769 - val_loss: 0.1272 - val_acc: 0.9644
Epoch 37/40
 - 5s - loss: 0.0439 - acc: 0.9770 - val_loss: 0.1323 - val_acc: 0.9675
Epoch 38/40
 - 5s - loss: 0.0454 - acc: 0.9768 - val_loss: 0.1449 - val_acc: 0.9656
Epoch 39/40
 - 5s - loss: 0.0446 - acc: 0.9767 - val_loss: 0.1291 - val_acc: 0.9675
Epoch 40/40
 - 5s - loss: 0.0432 - acc: 0.9769 - val_loss: 0.1346 - val_acc: 0.9679
Epoch 1/40
 - 0s - loss: 0.1225 - acc: 0.9639
Epoch 2/40
 - 1s - loss: 0.0933 - acc: 0.9668
Epoch 3/40
 - 1s - loss: 0.0792 - acc: 0.9685
Epoch 4/40
 - 0s - loss: 0.0702 - acc: 0.9713
Epoch 5/40
 - 0s - loss: 0.0732 - acc: 0.9713
Epoch 6/40
 - 0s - loss: 0.0615 - acc: 0.9721
Epoch 7/40
 - 0s - loss: 0.0597 - acc: 0.9739
Epoch 8/40
 - 0s - loss: 0.0567 - acc: 0.9740
Epoch 9/40
 - 0s - loss: 0.0542 - acc: 0.9740
Epoch 10/40
 - 0s - loss: 0.0553 - acc: 0.9756
Epoch 11/40
 - 0s - loss: 0.0535 - acc: 0.9751
Epoch 12/40
 - 0s - loss: 0.0545 - acc: 0.9752
Epoch 13/40
 - 0s - loss: 0.0573 - acc: 0.9766
Epoch 14/40
 - 0s - loss: 0.0528 - acc: 0.9765
Epoch 15/40
 - 0s - loss: 0.0515 - acc: 0.9761
Epoch 16/40
 - 0s - loss: 0.0491 - acc: 0.9762
Epoch 17/40
 - 0s - loss: 0.0482 - acc: 0.9771
Epoch 18/40
 - 0s - loss: 0.0473 - acc: 0.9759
Epoch 19/40
 - 0s - loss: 0.0442 - acc: 0.9761
Epoch 20/40
 - 0s - loss: 0.0456 - acc: 0.9777
Epoch 21/40
 - 0s - loss: 0.0454 - acc: 0.9767
Epoch 22/40
 - 0s - loss: 0.0452 - acc: 0.9766
Epoch 23/40
 - 0s - loss: 0.0440 - acc: 0.9771
Epoch 24/40
 - 0s - loss: 0.0445 - acc: 0.9762
Epoch 25/40
 - 0s - loss: 0.0424 - acc: 0.9771
Epoch 26/40
 - 0s - loss: 0.0469 - acc: 0.9761
Epoch 27/40
 - 0s - loss: 0.0460 - acc: 0.9770
Epoch 28/40
 - 0s - loss: 0.0422 - acc: 0.9768
Epoch 29/40
 - 1s - loss: 0.0429 - acc: 0.9772
Epoch 30/40
 - 1s - loss: 0.0442 - acc: 0.9775
Epoch 31/40
 - 0s - loss: 0.0487 - acc: 0.9773
Epoch 32/40
 - 0s - loss: 0.0470 - acc: 0.9784
Epoch 33/40
 - 1s - loss: 0.0454 - acc: 0.9773
Epoch 34/40
 - 1s - loss: 0.0441 - acc: 0.9780
Epoch 35/40
 - 1s - loss: 0.0473 - acc: 0.9760
Epoch 36/40
 - 1s - loss: 0.0431 - acc: 0.9779
Epoch 37/40
 - 0s - loss: 0.0418 - acc: 0.9774
Epoch 38/40
 - 1s - loss: 0.0434 - acc: 0.9770
Epoch 39/40
 - 1s - loss: 0.0410 - acc: 0.9776
Epoch 40/40
 - 1s - loss: 0.0414 - acc: 0.9777
# Training time = 0:03:48.490723
# F-Score(Ordinary) = 0.044, Recall: 1.0, Precision: 0.022
# F-Score(ireflv) = 0.152, Recall: 1.0, Precision: 0.082
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_313 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_314 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_313 (Embedding)       (None, 4, 48)        705264      input_313[0][0]                  
__________________________________________________________________________________________________
embedding_314 (Embedding)       (None, 4, 24)        5640        input_314[0][0]                  
__________________________________________________________________________________________________
flatten_313 (Flatten)           (None, 192)          0           embedding_313[0][0]              
__________________________________________________________________________________________________
flatten_314 (Flatten)           (None, 96)           0           embedding_314[0][0]              
__________________________________________________________________________________________________
concatenate_157 (Concatenate)   (None, 288)          0           flatten_313[0][0]                
                                                                 flatten_314[0][0]                
__________________________________________________________________________________________________
dense_313 (Dense)               (None, 24)           6936        concatenate_157[0][0]            
__________________________________________________________________________________________________
dropout_157 (Dropout)           (None, 24)           0           dense_313[0][0]                  
__________________________________________________________________________________________________
dense_314 (Dense)               (None, 8)            200         dropout_157[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0930 - acc: 0.9631 - val_loss: 0.0718 - val_acc: 0.9676
Epoch 2/40
 - 5s - loss: 0.0591 - acc: 0.9711 - val_loss: 0.0813 - val_acc: 0.9655
Epoch 3/40
 - 5s - loss: 0.0533 - acc: 0.9724 - val_loss: 0.0842 - val_acc: 0.9675
Epoch 4/40
 - 5s - loss: 0.0500 - acc: 0.9740 - val_loss: 0.0832 - val_acc: 0.9687
Epoch 5/40
 - 5s - loss: 0.0484 - acc: 0.9744 - val_loss: 0.0915 - val_acc: 0.9668
Epoch 6/40
 - 5s - loss: 0.0475 - acc: 0.9751 - val_loss: 0.0983 - val_acc: 0.9661
Epoch 7/40
 - 5s - loss: 0.0460 - acc: 0.9748 - val_loss: 0.0983 - val_acc: 0.9665
Epoch 8/40
 - 5s - loss: 0.0448 - acc: 0.9751 - val_loss: 0.1019 - val_acc: 0.9668
Epoch 9/40
 - 5s - loss: 0.0454 - acc: 0.9753 - val_loss: 0.1013 - val_acc: 0.9671
Epoch 10/40
 - 5s - loss: 0.0451 - acc: 0.9752 - val_loss: 0.1074 - val_acc: 0.9672
Epoch 11/40
 - 5s - loss: 0.0440 - acc: 0.9760 - val_loss: 0.1095 - val_acc: 0.9650
Epoch 12/40
 - 5s - loss: 0.0444 - acc: 0.9759 - val_loss: 0.1098 - val_acc: 0.9669
Epoch 13/40
 - 5s - loss: 0.0447 - acc: 0.9761 - val_loss: 0.1011 - val_acc: 0.9661
Epoch 14/40
 - 5s - loss: 0.0436 - acc: 0.9760 - val_loss: 0.1131 - val_acc: 0.9673
Epoch 15/40
 - 5s - loss: 0.0441 - acc: 0.9758 - val_loss: 0.1117 - val_acc: 0.9663
Epoch 16/40
 - 5s - loss: 0.0445 - acc: 0.9760 - val_loss: 0.1114 - val_acc: 0.9663
Epoch 17/40
 - 5s - loss: 0.0439 - acc: 0.9762 - val_loss: 0.1260 - val_acc: 0.9657
Epoch 18/40
 - 5s - loss: 0.0438 - acc: 0.9764 - val_loss: 0.1137 - val_acc: 0.9671
Epoch 19/40
 - 5s - loss: 0.0456 - acc: 0.9762 - val_loss: 0.1143 - val_acc: 0.9664
Epoch 20/40
 - 5s - loss: 0.0437 - acc: 0.9762 - val_loss: 0.1153 - val_acc: 0.9664
Epoch 21/40
 - 5s - loss: 0.0430 - acc: 0.9766 - val_loss: 0.1250 - val_acc: 0.9666
Epoch 22/40
 - 5s - loss: 0.0430 - acc: 0.9770 - val_loss: 0.1298 - val_acc: 0.9659
Epoch 23/40
 - 5s - loss: 0.0458 - acc: 0.9759 - val_loss: 0.1240 - val_acc: 0.9671
Epoch 24/40
 - 5s - loss: 0.0433 - acc: 0.9767 - val_loss: 0.1274 - val_acc: 0.9663
Epoch 25/40
 - 5s - loss: 0.0432 - acc: 0.9765 - val_loss: 0.1259 - val_acc: 0.9662
Epoch 26/40
 - 5s - loss: 0.0438 - acc: 0.9764 - val_loss: 0.1211 - val_acc: 0.9668
Epoch 27/40
 - 5s - loss: 0.0452 - acc: 0.9766 - val_loss: 0.1179 - val_acc: 0.9666
Epoch 28/40
 - 5s - loss: 0.0438 - acc: 0.9767 - val_loss: 0.1375 - val_acc: 0.9621
Epoch 29/40
 - 5s - loss: 0.0451 - acc: 0.9764 - val_loss: 0.1310 - val_acc: 0.9666
Epoch 30/40
 - 5s - loss: 0.0443 - acc: 0.9767 - val_loss: 0.1258 - val_acc: 0.9677
Epoch 31/40
 - 5s - loss: 0.0429 - acc: 0.9771 - val_loss: 0.1299 - val_acc: 0.9665
Epoch 32/40
 - 5s - loss: 0.0444 - acc: 0.9766 - val_loss: 0.1396 - val_acc: 0.9663
Epoch 33/40
 - 5s - loss: 0.0444 - acc: 0.9771 - val_loss: 0.1395 - val_acc: 0.9671
Epoch 34/40
 - 5s - loss: 0.0440 - acc: 0.9768 - val_loss: 0.1294 - val_acc: 0.9679
Epoch 35/40
 - 5s - loss: 0.0461 - acc: 0.9768 - val_loss: 0.1366 - val_acc: 0.9635
Epoch 36/40
 - 5s - loss: 0.0449 - acc: 0.9773 - val_loss: 0.1407 - val_acc: 0.9662
Epoch 37/40
 - 5s - loss: 0.0464 - acc: 0.9769 - val_loss: 0.1356 - val_acc: 0.9668
Epoch 38/40
 - 5s - loss: 0.0444 - acc: 0.9772 - val_loss: 0.1308 - val_acc: 0.9668
Epoch 39/40
 - 5s - loss: 0.0438 - acc: 0.9775 - val_loss: 0.1335 - val_acc: 0.9672
Epoch 40/40
 - 5s - loss: 0.0444 - acc: 0.9771 - val_loss: 0.1237 - val_acc: 0.9675
Epoch 1/40
 - 0s - loss: 0.1104 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.0902 - acc: 0.9678
Epoch 3/40
 - 0s - loss: 0.0740 - acc: 0.9705
Epoch 4/40
 - 0s - loss: 0.0731 - acc: 0.9712
Epoch 5/40
 - 0s - loss: 0.0675 - acc: 0.9729
Epoch 6/40
 - 0s - loss: 0.0638 - acc: 0.9734
Epoch 7/40
 - 0s - loss: 0.0608 - acc: 0.9741
Epoch 8/40
 - 0s - loss: 0.0604 - acc: 0.9742
Epoch 9/40
 - 0s - loss: 0.0550 - acc: 0.9758
Epoch 10/40
 - 0s - loss: 0.0547 - acc: 0.9757
Epoch 11/40
 - 0s - loss: 0.0544 - acc: 0.9758
Epoch 12/40
 - 0s - loss: 0.0555 - acc: 0.9760
Epoch 13/40
 - 0s - loss: 0.0500 - acc: 0.9770
Epoch 14/40
 - 0s - loss: 0.0523 - acc: 0.9746
Epoch 15/40
 - 0s - loss: 0.0518 - acc: 0.9762
Epoch 16/40
 - 0s - loss: 0.0534 - acc: 0.9761
Epoch 17/40
 - 0s - loss: 0.0511 - acc: 0.9773
Epoch 18/40
 - 0s - loss: 0.0507 - acc: 0.9768
Epoch 19/40
 - 0s - loss: 0.0489 - acc: 0.9774
Epoch 20/40
 - 0s - loss: 0.0507 - acc: 0.9763
Epoch 21/40
 - 0s - loss: 0.0479 - acc: 0.9771
Epoch 22/40
 - 0s - loss: 0.0474 - acc: 0.9772
Epoch 23/40
 - 0s - loss: 0.0471 - acc: 0.9771
Epoch 24/40
 - 0s - loss: 0.0479 - acc: 0.9770
Epoch 25/40
 - 0s - loss: 0.0455 - acc: 0.9788
Epoch 26/40
 - 0s - loss: 0.0475 - acc: 0.9775
Epoch 27/40
 - 0s - loss: 0.0496 - acc: 0.9775
Epoch 28/40
 - 0s - loss: 0.0472 - acc: 0.9782
Epoch 29/40
 - 0s - loss: 0.0464 - acc: 0.9776
Epoch 30/40
 - 0s - loss: 0.0440 - acc: 0.9786
Epoch 31/40
 - 0s - loss: 0.0499 - acc: 0.9776
Epoch 32/40
 - 0s - loss: 0.0430 - acc: 0.9784
Epoch 33/40
 - 0s - loss: 0.0428 - acc: 0.9795
Epoch 34/40
 - 0s - loss: 0.0431 - acc: 0.9778
Epoch 35/40
 - 0s - loss: 0.0495 - acc: 0.9780
Epoch 36/40
 - 0s - loss: 0.0500 - acc: 0.9779
Epoch 37/40
 - 0s - loss: 0.0505 - acc: 0.9768
Epoch 38/40
 - 0s - loss: 0.0481 - acc: 0.9770
Epoch 39/40
 - 0s - loss: 0.0440 - acc: 0.9787
Epoch 40/40
 - 0s - loss: 0.0461 - acc: 0.9780
# Training time = 0:03:40.844475
# F-Score(Ordinary) = 0.061, Recall: 0.933, Precision: 0.031
# F-Score(ireflv) = 0.204, Recall: 0.933, Precision: 0.115
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_315 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_316 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_315 (Embedding)       (None, 4, 48)        705264      input_315[0][0]                  
__________________________________________________________________________________________________
embedding_316 (Embedding)       (None, 4, 24)        5640        input_316[0][0]                  
__________________________________________________________________________________________________
flatten_315 (Flatten)           (None, 192)          0           embedding_315[0][0]              
__________________________________________________________________________________________________
flatten_316 (Flatten)           (None, 96)           0           embedding_316[0][0]              
__________________________________________________________________________________________________
concatenate_158 (Concatenate)   (None, 288)          0           flatten_315[0][0]                
                                                                 flatten_316[0][0]                
__________________________________________________________________________________________________
dense_315 (Dense)               (None, 24)           6936        concatenate_158[0][0]            
__________________________________________________________________________________________________
dropout_158 (Dropout)           (None, 24)           0           dense_315[0][0]                  
__________________________________________________________________________________________________
dense_316 (Dense)               (None, 8)            200         dropout_158[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0923 - acc: 0.9636 - val_loss: 0.0784 - val_acc: 0.9659
Epoch 2/40
 - 5s - loss: 0.0586 - acc: 0.9708 - val_loss: 0.0775 - val_acc: 0.9646
Epoch 3/40
 - 5s - loss: 0.0527 - acc: 0.9732 - val_loss: 0.0838 - val_acc: 0.9658
Epoch 4/40
 - 5s - loss: 0.0500 - acc: 0.9739 - val_loss: 0.0867 - val_acc: 0.9674
Epoch 5/40
 - 5s - loss: 0.0477 - acc: 0.9747 - val_loss: 0.0936 - val_acc: 0.9678
Epoch 6/40
 - 5s - loss: 0.0474 - acc: 0.9745 - val_loss: 0.0902 - val_acc: 0.9682
Epoch 7/40
 - 5s - loss: 0.0468 - acc: 0.9747 - val_loss: 0.0918 - val_acc: 0.9683
Epoch 8/40
 - 5s - loss: 0.0453 - acc: 0.9755 - val_loss: 0.0985 - val_acc: 0.9691
Epoch 9/40
 - 5s - loss: 0.0444 - acc: 0.9753 - val_loss: 0.1007 - val_acc: 0.9669
Epoch 10/40
 - 5s - loss: 0.0445 - acc: 0.9755 - val_loss: 0.0979 - val_acc: 0.9679
Epoch 11/40
 - 5s - loss: 0.0447 - acc: 0.9760 - val_loss: 0.1032 - val_acc: 0.9670
Epoch 12/40
 - 5s - loss: 0.0440 - acc: 0.9757 - val_loss: 0.1022 - val_acc: 0.9661
Epoch 13/40
 - 5s - loss: 0.0434 - acc: 0.9760 - val_loss: 0.1167 - val_acc: 0.9669
Epoch 14/40
 - 5s - loss: 0.0442 - acc: 0.9765 - val_loss: 0.1071 - val_acc: 0.9663
Epoch 15/40
 - 5s - loss: 0.0453 - acc: 0.9757 - val_loss: 0.1029 - val_acc: 0.9661
Epoch 16/40
 - 5s - loss: 0.0438 - acc: 0.9763 - val_loss: 0.1080 - val_acc: 0.9680
Epoch 17/40
 - 5s - loss: 0.0443 - acc: 0.9765 - val_loss: 0.1209 - val_acc: 0.9655
Epoch 18/40
 - 5s - loss: 0.0456 - acc: 0.9762 - val_loss: 0.1050 - val_acc: 0.9672
Epoch 19/40
 - 5s - loss: 0.0439 - acc: 0.9762 - val_loss: 0.1159 - val_acc: 0.9684
Epoch 20/40
 - 5s - loss: 0.0435 - acc: 0.9759 - val_loss: 0.1148 - val_acc: 0.9672
Epoch 21/40
 - 5s - loss: 0.0442 - acc: 0.9759 - val_loss: 0.1053 - val_acc: 0.9664
Epoch 22/40
 - 5s - loss: 0.0437 - acc: 0.9763 - val_loss: 0.1207 - val_acc: 0.9676
Epoch 23/40
 - 5s - loss: 0.0450 - acc: 0.9766 - val_loss: 0.1157 - val_acc: 0.9652
Epoch 24/40
 - 5s - loss: 0.0434 - acc: 0.9764 - val_loss: 0.1247 - val_acc: 0.9672
Epoch 25/40
 - 5s - loss: 0.0437 - acc: 0.9764 - val_loss: 0.1157 - val_acc: 0.9660
Epoch 26/40
 - 5s - loss: 0.0434 - acc: 0.9760 - val_loss: 0.1285 - val_acc: 0.9654
Epoch 27/40
 - 5s - loss: 0.0439 - acc: 0.9764 - val_loss: 0.1303 - val_acc: 0.9679
Epoch 28/40
 - 5s - loss: 0.0439 - acc: 0.9763 - val_loss: 0.1382 - val_acc: 0.9639
Epoch 29/40
 - 5s - loss: 0.0430 - acc: 0.9764 - val_loss: 0.1356 - val_acc: 0.9669
Epoch 30/40
 - 5s - loss: 0.0435 - acc: 0.9767 - val_loss: 0.1244 - val_acc: 0.9673
Epoch 31/40
 - 5s - loss: 0.0429 - acc: 0.9766 - val_loss: 0.1333 - val_acc: 0.9667
Epoch 32/40
 - 5s - loss: 0.0435 - acc: 0.9770 - val_loss: 0.1210 - val_acc: 0.9661
Epoch 33/40
 - 5s - loss: 0.0442 - acc: 0.9766 - val_loss: 0.1259 - val_acc: 0.9668
Epoch 34/40
 - 5s - loss: 0.0431 - acc: 0.9770 - val_loss: 0.1268 - val_acc: 0.9660
Epoch 35/40
 - 5s - loss: 0.0438 - acc: 0.9770 - val_loss: 0.1319 - val_acc: 0.9632
Epoch 36/40
 - 5s - loss: 0.0456 - acc: 0.9765 - val_loss: 0.1323 - val_acc: 0.9665
Epoch 37/40
 - 5s - loss: 0.0437 - acc: 0.9775 - val_loss: 0.1316 - val_acc: 0.9650
Epoch 38/40
 - 5s - loss: 0.0428 - acc: 0.9771 - val_loss: 0.1364 - val_acc: 0.9665
Epoch 39/40
 - 5s - loss: 0.0448 - acc: 0.9767 - val_loss: 0.1300 - val_acc: 0.9677
Epoch 40/40
 - 5s - loss: 0.0449 - acc: 0.9770 - val_loss: 0.1340 - val_acc: 0.9670
Epoch 1/40
 - 0s - loss: 0.1199 - acc: 0.9634
Epoch 2/40
 - 1s - loss: 0.0930 - acc: 0.9672
Epoch 3/40
 - 1s - loss: 0.0796 - acc: 0.9694
Epoch 4/40
 - 1s - loss: 0.0763 - acc: 0.9705
Epoch 5/40
 - 1s - loss: 0.0693 - acc: 0.9702
Epoch 6/40
 - 1s - loss: 0.0676 - acc: 0.9712
Epoch 7/40
 - 0s - loss: 0.0628 - acc: 0.9720
Epoch 8/40
 - 0s - loss: 0.0631 - acc: 0.9735
Epoch 9/40
 - 0s - loss: 0.0589 - acc: 0.9735
Epoch 10/40
 - 0s - loss: 0.0553 - acc: 0.9740
Epoch 11/40
 - 1s - loss: 0.0547 - acc: 0.9749
Epoch 12/40
 - 1s - loss: 0.0561 - acc: 0.9749
Epoch 13/40
 - 0s - loss: 0.0578 - acc: 0.9744
Epoch 14/40
 - 1s - loss: 0.0573 - acc: 0.9744
Epoch 15/40
 - 0s - loss: 0.0563 - acc: 0.9747
Epoch 16/40
 - 1s - loss: 0.0594 - acc: 0.9753
Epoch 17/40
 - 0s - loss: 0.0552 - acc: 0.9754
Epoch 18/40
 - 1s - loss: 0.0517 - acc: 0.9763
Epoch 19/40
 - 1s - loss: 0.0528 - acc: 0.9753
Epoch 20/40
 - 1s - loss: 0.0543 - acc: 0.9761
Epoch 21/40
 - 1s - loss: 0.0537 - acc: 0.9757
Epoch 22/40
 - 0s - loss: 0.0499 - acc: 0.9767
Epoch 23/40
 - 1s - loss: 0.0479 - acc: 0.9773
Epoch 24/40
 - 1s - loss: 0.0497 - acc: 0.9775
Epoch 25/40
 - 1s - loss: 0.0475 - acc: 0.9764
Epoch 26/40
 - 1s - loss: 0.0473 - acc: 0.9783
Epoch 27/40
 - 1s - loss: 0.0482 - acc: 0.9764
Epoch 28/40
 - 1s - loss: 0.0461 - acc: 0.9784
Epoch 29/40
 - 1s - loss: 0.0461 - acc: 0.9766
Epoch 30/40
 - 1s - loss: 0.0481 - acc: 0.9773
Epoch 31/40
 - 1s - loss: 0.0457 - acc: 0.9780
Epoch 32/40
 - 1s - loss: 0.0463 - acc: 0.9768
Epoch 33/40
 - 1s - loss: 0.0493 - acc: 0.9775
Epoch 34/40
 - 1s - loss: 0.0468 - acc: 0.9766
Epoch 35/40
 - 1s - loss: 0.0475 - acc: 0.9778
Epoch 36/40
 - 1s - loss: 0.0501 - acc: 0.9765
Epoch 37/40
 - 0s - loss: 0.0508 - acc: 0.9774
Epoch 38/40
 - 1s - loss: 0.0541 - acc: 0.9784
Epoch 39/40
 - 1s - loss: 0.0512 - acc: 0.9778
Epoch 40/40
 - 1s - loss: 0.0485 - acc: 0.9770
# Training time = 0:03:48.599542
# F-Score(Ordinary) = 0.359, Recall: 0.871, Precision: 0.226
# F-Score(lvc) = 0.03, Recall: 0.667, Precision: 0.015
# F-Score(id) = 0.647, Recall: 0.876, Precision: 0.513
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_317 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_318 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_317 (Embedding)       (None, 4, 48)        705264      input_317[0][0]                  
__________________________________________________________________________________________________
embedding_318 (Embedding)       (None, 4, 24)        5640        input_318[0][0]                  
__________________________________________________________________________________________________
flatten_317 (Flatten)           (None, 192)          0           embedding_317[0][0]              
__________________________________________________________________________________________________
flatten_318 (Flatten)           (None, 96)           0           embedding_318[0][0]              
__________________________________________________________________________________________________
concatenate_159 (Concatenate)   (None, 288)          0           flatten_317[0][0]                
                                                                 flatten_318[0][0]                
__________________________________________________________________________________________________
dense_317 (Dense)               (None, 24)           6936        concatenate_159[0][0]            
__________________________________________________________________________________________________
dropout_159 (Dropout)           (None, 24)           0           dense_317[0][0]                  
__________________________________________________________________________________________________
dense_318 (Dense)               (None, 8)            200         dropout_159[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0934 - acc: 0.9625 - val_loss: 0.0756 - val_acc: 0.9657
Epoch 2/40
 - 5s - loss: 0.0581 - acc: 0.9714 - val_loss: 0.0787 - val_acc: 0.9664
Epoch 3/40
 - 5s - loss: 0.0525 - acc: 0.9730 - val_loss: 0.0800 - val_acc: 0.9674
Epoch 4/40
 - 5s - loss: 0.0499 - acc: 0.9737 - val_loss: 0.0830 - val_acc: 0.9684
Epoch 5/40
 - 5s - loss: 0.0480 - acc: 0.9748 - val_loss: 0.0902 - val_acc: 0.9676
Epoch 6/40
 - 5s - loss: 0.0467 - acc: 0.9750 - val_loss: 0.0941 - val_acc: 0.9686
Epoch 7/40
 - 5s - loss: 0.0465 - acc: 0.9750 - val_loss: 0.0971 - val_acc: 0.9669
Epoch 8/40
 - 5s - loss: 0.0458 - acc: 0.9751 - val_loss: 0.0986 - val_acc: 0.9662
Epoch 9/40
 - 5s - loss: 0.0451 - acc: 0.9752 - val_loss: 0.1033 - val_acc: 0.9673
Epoch 10/40
 - 5s - loss: 0.0449 - acc: 0.9757 - val_loss: 0.0946 - val_acc: 0.9671
Epoch 11/40
 - 5s - loss: 0.0443 - acc: 0.9758 - val_loss: 0.1083 - val_acc: 0.9677
Epoch 12/40
 - 5s - loss: 0.0450 - acc: 0.9760 - val_loss: 0.0932 - val_acc: 0.9684
Epoch 13/40
 - 5s - loss: 0.0441 - acc: 0.9761 - val_loss: 0.1062 - val_acc: 0.9679
Epoch 14/40
 - 5s - loss: 0.0438 - acc: 0.9761 - val_loss: 0.1046 - val_acc: 0.9677
Epoch 15/40
 - 5s - loss: 0.0440 - acc: 0.9758 - val_loss: 0.1111 - val_acc: 0.9674
Epoch 16/40
 - 5s - loss: 0.0436 - acc: 0.9761 - val_loss: 0.1243 - val_acc: 0.9679
Epoch 17/40
 - 5s - loss: 0.0434 - acc: 0.9760 - val_loss: 0.1038 - val_acc: 0.9690
Epoch 18/40
 - 5s - loss: 0.0435 - acc: 0.9760 - val_loss: 0.1065 - val_acc: 0.9683
Epoch 19/40
 - 5s - loss: 0.0437 - acc: 0.9766 - val_loss: 0.1233 - val_acc: 0.9660
Epoch 20/40
 - 5s - loss: 0.0432 - acc: 0.9762 - val_loss: 0.1110 - val_acc: 0.9669
Epoch 21/40
 - 5s - loss: 0.0432 - acc: 0.9766 - val_loss: 0.1266 - val_acc: 0.9678
Epoch 22/40
 - 5s - loss: 0.0429 - acc: 0.9764 - val_loss: 0.1216 - val_acc: 0.9676
Epoch 23/40
 - 5s - loss: 0.0424 - acc: 0.9763 - val_loss: 0.1179 - val_acc: 0.9664
Epoch 24/40
 - 5s - loss: 0.0436 - acc: 0.9767 - val_loss: 0.1198 - val_acc: 0.9681
Epoch 25/40
 - 5s - loss: 0.0435 - acc: 0.9766 - val_loss: 0.1189 - val_acc: 0.9675
Epoch 26/40
 - 5s - loss: 0.0430 - acc: 0.9771 - val_loss: 0.1204 - val_acc: 0.9678
Epoch 27/40
 - 5s - loss: 0.0426 - acc: 0.9766 - val_loss: 0.1159 - val_acc: 0.9659
Epoch 28/40
 - 5s - loss: 0.0431 - acc: 0.9771 - val_loss: 0.1238 - val_acc: 0.9683
Epoch 29/40
 - 5s - loss: 0.0427 - acc: 0.9767 - val_loss: 0.1222 - val_acc: 0.9671
Epoch 30/40
 - 5s - loss: 0.0429 - acc: 0.9768 - val_loss: 0.1314 - val_acc: 0.9676
Epoch 31/40
 - 5s - loss: 0.0427 - acc: 0.9770 - val_loss: 0.1233 - val_acc: 0.9669
Epoch 32/40
 - 5s - loss: 0.0443 - acc: 0.9767 - val_loss: 0.1352 - val_acc: 0.9662
Epoch 33/40
 - 5s - loss: 0.0435 - acc: 0.9770 - val_loss: 0.1260 - val_acc: 0.9670
Epoch 34/40
 - 5s - loss: 0.0448 - acc: 0.9772 - val_loss: 0.1340 - val_acc: 0.9666
Epoch 35/40
 - 5s - loss: 0.0439 - acc: 0.9771 - val_loss: 0.1277 - val_acc: 0.9673
Epoch 36/40
 - 5s - loss: 0.0433 - acc: 0.9773 - val_loss: 0.1384 - val_acc: 0.9667
Epoch 37/40
 - 5s - loss: 0.0430 - acc: 0.9771 - val_loss: 0.1430 - val_acc: 0.9674
Epoch 38/40
 - 5s - loss: 0.0446 - acc: 0.9772 - val_loss: 0.1296 - val_acc: 0.9670
Epoch 39/40
 - 5s - loss: 0.0440 - acc: 0.9771 - val_loss: 0.1289 - val_acc: 0.9673
Epoch 40/40
 - 5s - loss: 0.0431 - acc: 0.9771 - val_loss: 0.1255 - val_acc: 0.9677
Epoch 1/40
 - 0s - loss: 0.1164 - acc: 0.9640
Epoch 2/40
 - 1s - loss: 0.0867 - acc: 0.9683
Epoch 3/40
 - 1s - loss: 0.0800 - acc: 0.9697
Epoch 4/40
 - 1s - loss: 0.0715 - acc: 0.9712
Epoch 5/40
 - 1s - loss: 0.0622 - acc: 0.9719
Epoch 6/40
 - 0s - loss: 0.0608 - acc: 0.9722
Epoch 7/40
 - 0s - loss: 0.0602 - acc: 0.9729
Epoch 8/40
 - 0s - loss: 0.0554 - acc: 0.9741
Epoch 9/40
 - 0s - loss: 0.0520 - acc: 0.9747
Epoch 10/40
 - 0s - loss: 0.0510 - acc: 0.9755
Epoch 11/40
 - 0s - loss: 0.0539 - acc: 0.9756
Epoch 12/40
 - 0s - loss: 0.0521 - acc: 0.9758
Epoch 13/40
 - 0s - loss: 0.0527 - acc: 0.9755
Epoch 14/40
 - 0s - loss: 0.0507 - acc: 0.9759
Epoch 15/40
 - 0s - loss: 0.0489 - acc: 0.9763
Epoch 16/40
 - 1s - loss: 0.0474 - acc: 0.9769
Epoch 17/40
 - 0s - loss: 0.0457 - acc: 0.9761
Epoch 18/40
 - 1s - loss: 0.0470 - acc: 0.9768
Epoch 19/40
 - 0s - loss: 0.0466 - acc: 0.9771
Epoch 20/40
 - 0s - loss: 0.0444 - acc: 0.9766
Epoch 21/40
 - 0s - loss: 0.0448 - acc: 0.9767
Epoch 22/40
 - 0s - loss: 0.0466 - acc: 0.9783
Epoch 23/40
 - 0s - loss: 0.0450 - acc: 0.9771
Epoch 24/40
 - 0s - loss: 0.0442 - acc: 0.9779
Epoch 25/40
 - 0s - loss: 0.0438 - acc: 0.9781
Epoch 26/40
 - 0s - loss: 0.0434 - acc: 0.9780
Epoch 27/40
 - 0s - loss: 0.0448 - acc: 0.9770
Epoch 28/40
 - 0s - loss: 0.0506 - acc: 0.9772
Epoch 29/40
 - 0s - loss: 0.0498 - acc: 0.9770
Epoch 30/40
 - 0s - loss: 0.0455 - acc: 0.9775
Epoch 31/40
 - 0s - loss: 0.0447 - acc: 0.9787
Epoch 32/40
 - 1s - loss: 0.0466 - acc: 0.9778
Epoch 33/40
 - 0s - loss: 0.0435 - acc: 0.9779
Epoch 34/40
 - 0s - loss: 0.0453 - acc: 0.9780
Epoch 35/40
 - 0s - loss: 0.0449 - acc: 0.9774
Epoch 36/40
 - 0s - loss: 0.0445 - acc: 0.9784
Epoch 37/40
 - 1s - loss: 0.0452 - acc: 0.9785
Epoch 38/40
 - 0s - loss: 0.0435 - acc: 0.9778
Epoch 39/40
 - 1s - loss: 0.0429 - acc: 0.9788
Epoch 40/40
 - 1s - loss: 0.0451 - acc: 0.9781
# Training time = 0:03:48.946574
# F-Score(Ordinary) = 0.136, Recall: 0.507, Precision: 0.078
# F-Score(lvc) = 0.348, Recall: 0.507, Precision: 0.265
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_319 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_320 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_319 (Embedding)       (None, 4, 48)        705264      input_319[0][0]                  
__________________________________________________________________________________________________
embedding_320 (Embedding)       (None, 4, 24)        5640        input_320[0][0]                  
__________________________________________________________________________________________________
flatten_319 (Flatten)           (None, 192)          0           embedding_319[0][0]              
__________________________________________________________________________________________________
flatten_320 (Flatten)           (None, 96)           0           embedding_320[0][0]              
__________________________________________________________________________________________________
concatenate_160 (Concatenate)   (None, 288)          0           flatten_319[0][0]                
                                                                 flatten_320[0][0]                
__________________________________________________________________________________________________
dense_319 (Dense)               (None, 24)           6936        concatenate_160[0][0]            
__________________________________________________________________________________________________
dropout_160 (Dropout)           (None, 24)           0           dense_319[0][0]                  
__________________________________________________________________________________________________
dense_320 (Dense)               (None, 8)            200         dropout_160[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.005
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0947 - acc: 0.9624 - val_loss: 0.0801 - val_acc: 0.9648
Epoch 2/40
 - 4s - loss: 0.0596 - acc: 0.9706 - val_loss: 0.0790 - val_acc: 0.9675
Epoch 3/40
 - 4s - loss: 0.0533 - acc: 0.9723 - val_loss: 0.0838 - val_acc: 0.9675
Epoch 4/40
 - 4s - loss: 0.0503 - acc: 0.9731 - val_loss: 0.0775 - val_acc: 0.9683
Epoch 5/40
 - 5s - loss: 0.0485 - acc: 0.9744 - val_loss: 0.0889 - val_acc: 0.9668
Epoch 6/40
 - 5s - loss: 0.0464 - acc: 0.9743 - val_loss: 0.0892 - val_acc: 0.9681
Epoch 7/40
 - 5s - loss: 0.0472 - acc: 0.9746 - val_loss: 0.0980 - val_acc: 0.9673
Epoch 8/40
 - 5s - loss: 0.0456 - acc: 0.9753 - val_loss: 0.0925 - val_acc: 0.9677
Epoch 9/40
 - 5s - loss: 0.0451 - acc: 0.9754 - val_loss: 0.0987 - val_acc: 0.9659
Epoch 10/40
 - 5s - loss: 0.0441 - acc: 0.9755 - val_loss: 0.1018 - val_acc: 0.9666
Epoch 11/40
 - 5s - loss: 0.0450 - acc: 0.9755 - val_loss: 0.1023 - val_acc: 0.9681
Epoch 12/40
 - 5s - loss: 0.0446 - acc: 0.9755 - val_loss: 0.1011 - val_acc: 0.9674
Epoch 13/40
 - 5s - loss: 0.0446 - acc: 0.9762 - val_loss: 0.1011 - val_acc: 0.9675
Epoch 14/40
 - 5s - loss: 0.0447 - acc: 0.9763 - val_loss: 0.0984 - val_acc: 0.9680
Epoch 15/40
 - 5s - loss: 0.0439 - acc: 0.9759 - val_loss: 0.1123 - val_acc: 0.9672
Epoch 16/40
 - 5s - loss: 0.0445 - acc: 0.9757 - val_loss: 0.1185 - val_acc: 0.9667
Epoch 17/40
 - 5s - loss: 0.0442 - acc: 0.9755 - val_loss: 0.1114 - val_acc: 0.9653
Epoch 18/40
 - 4s - loss: 0.0440 - acc: 0.9758 - val_loss: 0.1113 - val_acc: 0.9670
Epoch 19/40
 - 5s - loss: 0.0440 - acc: 0.9763 - val_loss: 0.1107 - val_acc: 0.9672
Epoch 20/40
 - 4s - loss: 0.0438 - acc: 0.9763 - val_loss: 0.1201 - val_acc: 0.9654
Epoch 21/40
 - 4s - loss: 0.0431 - acc: 0.9765 - val_loss: 0.1156 - val_acc: 0.9663
Epoch 22/40
 - 4s - loss: 0.0433 - acc: 0.9761 - val_loss: 0.1126 - val_acc: 0.9667
Epoch 23/40
 - 4s - loss: 0.0426 - acc: 0.9764 - val_loss: 0.1095 - val_acc: 0.9682
Epoch 24/40
 - 4s - loss: 0.0439 - acc: 0.9763 - val_loss: 0.1125 - val_acc: 0.9680
Epoch 25/40
 - 5s - loss: 0.0444 - acc: 0.9762 - val_loss: 0.1136 - val_acc: 0.9658
Epoch 26/40
 - 5s - loss: 0.0429 - acc: 0.9767 - val_loss: 0.1300 - val_acc: 0.9676
Epoch 27/40
 - 5s - loss: 0.0442 - acc: 0.9767 - val_loss: 0.1232 - val_acc: 0.9673
Epoch 28/40
 - 4s - loss: 0.0446 - acc: 0.9772 - val_loss: 0.1211 - val_acc: 0.9684
Epoch 29/40
 - 4s - loss: 0.0444 - acc: 0.9766 - val_loss: 0.1351 - val_acc: 0.9675
Epoch 30/40
 - 4s - loss: 0.0427 - acc: 0.9767 - val_loss: 0.1307 - val_acc: 0.9671
Epoch 31/40
 - 5s - loss: 0.0429 - acc: 0.9767 - val_loss: 0.1271 - val_acc: 0.9670
Epoch 32/40
 - 5s - loss: 0.0428 - acc: 0.9768 - val_loss: 0.1223 - val_acc: 0.9678
Epoch 33/40
 - 4s - loss: 0.0437 - acc: 0.9763 - val_loss: 0.1217 - val_acc: 0.9678
Epoch 34/40
 - 4s - loss: 0.0427 - acc: 0.9770 - val_loss: 0.1299 - val_acc: 0.9675
Epoch 35/40
 - 4s - loss: 0.0429 - acc: 0.9766 - val_loss: 0.1266 - val_acc: 0.9676
Epoch 36/40
 - 5s - loss: 0.0436 - acc: 0.9766 - val_loss: 0.1332 - val_acc: 0.9662
Epoch 37/40
 - 5s - loss: 0.0447 - acc: 0.9765 - val_loss: 0.1264 - val_acc: 0.9679
Epoch 38/40
 - 5s - loss: 0.0445 - acc: 0.9770 - val_loss: 0.1299 - val_acc: 0.9674
Epoch 39/40
 - 4s - loss: 0.0443 - acc: 0.9770 - val_loss: 0.1374 - val_acc: 0.9678
Epoch 40/40
 - 4s - loss: 0.0457 - acc: 0.9770 - val_loss: 0.1308 - val_acc: 0.9687
Epoch 1/40
 - 0s - loss: 0.1124 - acc: 0.9654
Epoch 2/40
 - 0s - loss: 0.0885 - acc: 0.9666
Epoch 3/40
 - 0s - loss: 0.0728 - acc: 0.9698
Epoch 4/40
 - 0s - loss: 0.0680 - acc: 0.9722
Epoch 5/40
 - 0s - loss: 0.0665 - acc: 0.9719
Epoch 6/40
 - 0s - loss: 0.0609 - acc: 0.9720
Epoch 7/40
 - 0s - loss: 0.0599 - acc: 0.9737
Epoch 8/40
 - 0s - loss: 0.0556 - acc: 0.9733
Epoch 9/40
 - 0s - loss: 0.0575 - acc: 0.9732
Epoch 10/40
 - 0s - loss: 0.0516 - acc: 0.9746
Epoch 11/40
 - 0s - loss: 0.0520 - acc: 0.9755
Epoch 12/40
 - 0s - loss: 0.0514 - acc: 0.9750
Epoch 13/40
 - 0s - loss: 0.0506 - acc: 0.9750
Epoch 14/40
 - 0s - loss: 0.0521 - acc: 0.9769
Epoch 15/40
 - 0s - loss: 0.0507 - acc: 0.9756
Epoch 16/40
 - 0s - loss: 0.0492 - acc: 0.9769
Epoch 17/40
 - 0s - loss: 0.0514 - acc: 0.9762
Epoch 18/40
 - 0s - loss: 0.0502 - acc: 0.9766
Epoch 19/40
 - 0s - loss: 0.0512 - acc: 0.9757
Epoch 20/40
 - 0s - loss: 0.0492 - acc: 0.9762
Epoch 21/40
 - 0s - loss: 0.0487 - acc: 0.9758
Epoch 22/40
 - 0s - loss: 0.0476 - acc: 0.9768
Epoch 23/40
 - 0s - loss: 0.0484 - acc: 0.9767
Epoch 24/40
 - 0s - loss: 0.0489 - acc: 0.9778
Epoch 25/40
 - 0s - loss: 0.0492 - acc: 0.9768
Epoch 26/40
 - 0s - loss: 0.0476 - acc: 0.9774
Epoch 27/40
 - 0s - loss: 0.0484 - acc: 0.9764
Epoch 28/40
 - 0s - loss: 0.0512 - acc: 0.9764
Epoch 29/40
 - 0s - loss: 0.0458 - acc: 0.9767
Epoch 30/40
 - 0s - loss: 0.0457 - acc: 0.9781
Epoch 31/40
 - 0s - loss: 0.0461 - acc: 0.9771
Epoch 32/40
 - 0s - loss: 0.0470 - acc: 0.9783
Epoch 33/40
 - 0s - loss: 0.0490 - acc: 0.9761
Epoch 34/40
 - 0s - loss: 0.0479 - acc: 0.9770
Epoch 35/40
 - 0s - loss: 0.0471 - acc: 0.9786
Epoch 36/40
 - 0s - loss: 0.0483 - acc: 0.9773
Epoch 37/40
 - 0s - loss: 0.0528 - acc: 0.9764
Epoch 38/40
 - 0s - loss: 0.0460 - acc: 0.9769
Epoch 39/40
 - 0s - loss: 0.0487 - acc: 0.9767
Epoch 40/40
 - 0s - loss: 0.0459 - acc: 0.9778
# Training time = 0:03:39.954210
# F-Score(Ordinary) = 0.378, Recall: 0.359, Precision: 0.398
# F-Score(lvc) = 0.399, Recall: 0.37, Precision: 0.432
# F-Score(id) = 0.441, Recall: 0.345, Precision: 0.611
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_321 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_322 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_321 (Embedding)       (None, 4, 48)        705264      input_321[0][0]                  
__________________________________________________________________________________________________
embedding_322 (Embedding)       (None, 4, 24)        5640        input_322[0][0]                  
__________________________________________________________________________________________________
flatten_321 (Flatten)           (None, 192)          0           embedding_321[0][0]              
__________________________________________________________________________________________________
flatten_322 (Flatten)           (None, 96)           0           embedding_322[0][0]              
__________________________________________________________________________________________________
concatenate_161 (Concatenate)   (None, 288)          0           flatten_321[0][0]                
                                                                 flatten_322[0][0]                
__________________________________________________________________________________________________
dense_321 (Dense)               (None, 24)           6936        concatenate_161[0][0]            
__________________________________________________________________________________________________
dropout_161 (Dropout)           (None, 24)           0           dense_321[0][0]                  
__________________________________________________________________________________________________
dense_322 (Dense)               (None, 8)            200         dropout_161[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 4s - loss: 0.0962 - acc: 0.9617 - val_loss: 0.0910 - val_acc: 0.9640
Epoch 2/40
 - 5s - loss: 0.0694 - acc: 0.9689 - val_loss: 0.0868 - val_acc: 0.9666
Epoch 3/40
 - 5s - loss: 0.0656 - acc: 0.9706 - val_loss: 0.0889 - val_acc: 0.9668
Epoch 4/40
 - 5s - loss: 0.0647 - acc: 0.9716 - val_loss: 0.1111 - val_acc: 0.9647
Epoch 5/40
 - 5s - loss: 0.0644 - acc: 0.9719 - val_loss: 0.0991 - val_acc: 0.9676
Epoch 6/40
 - 5s - loss: 0.0632 - acc: 0.9726 - val_loss: 0.1030 - val_acc: 0.9664
Epoch 7/40
 - 5s - loss: 0.0625 - acc: 0.9729 - val_loss: 0.1056 - val_acc: 0.9652
Epoch 8/40
 - 5s - loss: 0.0635 - acc: 0.9729 - val_loss: 0.1073 - val_acc: 0.9663
Epoch 9/40
 - 5s - loss: 0.0643 - acc: 0.9727 - val_loss: 0.1195 - val_acc: 0.9674
Epoch 10/40
 - 5s - loss: 0.0621 - acc: 0.9732 - val_loss: 0.1189 - val_acc: 0.9671
Epoch 11/40
 - 4s - loss: 0.0630 - acc: 0.9734 - val_loss: 0.1217 - val_acc: 0.9671
Epoch 12/40
 - 5s - loss: 0.0650 - acc: 0.9736 - val_loss: 0.1200 - val_acc: 0.9672
Epoch 13/40
 - 5s - loss: 0.0659 - acc: 0.9732 - val_loss: 0.1171 - val_acc: 0.9669
Epoch 14/40
 - 5s - loss: 0.0656 - acc: 0.9732 - val_loss: 0.1464 - val_acc: 0.9661
Epoch 15/40
 - 5s - loss: 0.0649 - acc: 0.9737 - val_loss: 0.1290 - val_acc: 0.9677
Epoch 16/40
 - 4s - loss: 0.0640 - acc: 0.9737 - val_loss: 0.1281 - val_acc: 0.9659
Epoch 17/40
 - 5s - loss: 0.0697 - acc: 0.9741 - val_loss: 0.1513 - val_acc: 0.9663
Epoch 18/40
 - 5s - loss: 0.0673 - acc: 0.9738 - val_loss: 0.1329 - val_acc: 0.9674
Epoch 19/40
 - 4s - loss: 0.0681 - acc: 0.9734 - val_loss: 0.1417 - val_acc: 0.9657
Epoch 20/40
 - 5s - loss: 0.0739 - acc: 0.9727 - val_loss: 0.1532 - val_acc: 0.9654
Epoch 21/40
 - 5s - loss: 0.0705 - acc: 0.9736 - val_loss: 0.1477 - val_acc: 0.9646
Epoch 22/40
 - 5s - loss: 0.0691 - acc: 0.9740 - val_loss: 0.1595 - val_acc: 0.9644
Epoch 23/40
 - 5s - loss: 0.0784 - acc: 0.9735 - val_loss: 0.1495 - val_acc: 0.9654
Epoch 24/40
 - 5s - loss: 0.0771 - acc: 0.9732 - val_loss: 0.1536 - val_acc: 0.9669
Epoch 25/40
 - 5s - loss: 0.0774 - acc: 0.9734 - val_loss: 0.1609 - val_acc: 0.9656
Epoch 26/40
 - 5s - loss: 0.0742 - acc: 0.9734 - val_loss: 0.1546 - val_acc: 0.9665
Epoch 27/40
 - 5s - loss: 0.0782 - acc: 0.9731 - val_loss: 0.1644 - val_acc: 0.9663
Epoch 28/40
 - 5s - loss: 0.0824 - acc: 0.9736 - val_loss: 0.1453 - val_acc: 0.9640
Epoch 29/40
 - 5s - loss: 0.0786 - acc: 0.9737 - val_loss: 0.1646 - val_acc: 0.9657
Epoch 30/40
 - 5s - loss: 0.0810 - acc: 0.9737 - val_loss: 0.1630 - val_acc: 0.9662
Epoch 31/40
 - 5s - loss: 0.0798 - acc: 0.9742 - val_loss: 0.1525 - val_acc: 0.9655
Epoch 32/40
 - 5s - loss: 0.0799 - acc: 0.9742 - val_loss: 0.1569 - val_acc: 0.9667
Epoch 33/40
 - 5s - loss: 0.0816 - acc: 0.9742 - val_loss: 0.1669 - val_acc: 0.9671
Epoch 34/40
 - 5s - loss: 0.0802 - acc: 0.9733 - val_loss: 0.1627 - val_acc: 0.9654
Epoch 35/40
 - 5s - loss: 0.0773 - acc: 0.9737 - val_loss: 0.1745 - val_acc: 0.9663
Epoch 36/40
 - 5s - loss: 0.0823 - acc: 0.9742 - val_loss: 0.1569 - val_acc: 0.9673
Epoch 37/40
 - 5s - loss: 0.0837 - acc: 0.9740 - val_loss: 0.1707 - val_acc: 0.9657
Epoch 38/40
 - 4s - loss: 0.0899 - acc: 0.9733 - val_loss: 0.1793 - val_acc: 0.9658
Epoch 39/40
 - 5s - loss: 0.0926 - acc: 0.9733 - val_loss: 0.1652 - val_acc: 0.9652
Epoch 40/40
 - 4s - loss: 0.0904 - acc: 0.9727 - val_loss: 0.1905 - val_acc: 0.9662
Epoch 1/40
 - 0s - loss: 0.1697 - acc: 0.9648
Epoch 2/40
 - 0s - loss: 0.1463 - acc: 0.9656
Epoch 3/40
 - 0s - loss: 0.1440 - acc: 0.9654
Epoch 4/40
 - 0s - loss: 0.1354 - acc: 0.9671
Epoch 5/40
 - 0s - loss: 0.1319 - acc: 0.9679
Epoch 6/40
 - 0s - loss: 0.1282 - acc: 0.9688
Epoch 7/40
 - 0s - loss: 0.1176 - acc: 0.9668
Epoch 8/40
 - 0s - loss: 0.1425 - acc: 0.9681
Epoch 9/40
 - 0s - loss: 0.1480 - acc: 0.9685
Epoch 10/40
 - 0s - loss: 0.1340 - acc: 0.9703
Epoch 11/40
 - 0s - loss: 0.1363 - acc: 0.9708
Epoch 12/40
 - 0s - loss: 0.1361 - acc: 0.9705
Epoch 13/40
 - 0s - loss: 0.1237 - acc: 0.9709
Epoch 14/40
 - 0s - loss: 0.1131 - acc: 0.9729
Epoch 15/40
 - 0s - loss: 0.1199 - acc: 0.9733
Epoch 16/40
 - 0s - loss: 0.1155 - acc: 0.9730
Epoch 17/40
 - 0s - loss: 0.1130 - acc: 0.9731
Epoch 18/40
 - 0s - loss: 0.1115 - acc: 0.9718
Epoch 19/40
 - 0s - loss: 0.1079 - acc: 0.9731
Epoch 20/40
 - 0s - loss: 0.1081 - acc: 0.9733
Epoch 21/40
 - 0s - loss: 0.1045 - acc: 0.9738
Epoch 22/40
 - 0s - loss: 0.1188 - acc: 0.9715
Epoch 23/40
 - 0s - loss: 0.1118 - acc: 0.9737
Epoch 24/40
 - 0s - loss: 0.1120 - acc: 0.9739
Epoch 25/40
 - 0s - loss: 0.1154 - acc: 0.9732
Epoch 26/40
 - 0s - loss: 0.1126 - acc: 0.9739
Epoch 27/40
 - 0s - loss: 0.1080 - acc: 0.9742
Epoch 28/40
 - 0s - loss: 0.1076 - acc: 0.9746
Epoch 29/40
 - 0s - loss: 0.1152 - acc: 0.9731
Epoch 30/40
 - 0s - loss: 0.1083 - acc: 0.9745
Epoch 31/40
 - 0s - loss: 0.1088 - acc: 0.9748
Epoch 32/40
 - 0s - loss: 0.1095 - acc: 0.9749
Epoch 33/40
 - 0s - loss: 0.1038 - acc: 0.9752
Epoch 34/40
 - 0s - loss: 0.1086 - acc: 0.9743
Epoch 35/40
 - 0s - loss: 0.1008 - acc: 0.9761
Epoch 36/40
 - 0s - loss: 0.0993 - acc: 0.9755
Epoch 37/40
 - 0s - loss: 0.1084 - acc: 0.9742
Epoch 38/40
 - 0s - loss: 0.1087 - acc: 0.9752
Epoch 39/40
 - 0s - loss: 0.1151 - acc: 0.9749
Epoch 40/40
 - 0s - loss: 0.1108 - acc: 0.9737
# Training time = 0:03:39.988920
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_323 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_324 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_323 (Embedding)       (None, 4, 48)        705264      input_323[0][0]                  
__________________________________________________________________________________________________
embedding_324 (Embedding)       (None, 4, 24)        5640        input_324[0][0]                  
__________________________________________________________________________________________________
flatten_323 (Flatten)           (None, 192)          0           embedding_323[0][0]              
__________________________________________________________________________________________________
flatten_324 (Flatten)           (None, 96)           0           embedding_324[0][0]              
__________________________________________________________________________________________________
concatenate_162 (Concatenate)   (None, 288)          0           flatten_323[0][0]                
                                                                 flatten_324[0][0]                
__________________________________________________________________________________________________
dense_323 (Dense)               (None, 24)           6936        concatenate_162[0][0]            
__________________________________________________________________________________________________
dropout_162 (Dropout)           (None, 24)           0           dense_323[0][0]                  
__________________________________________________________________________________________________
dense_324 (Dense)               (None, 8)            200         dropout_162[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0965 - acc: 0.9618 - val_loss: 0.0841 - val_acc: 0.9667
Epoch 2/40
 - 5s - loss: 0.0697 - acc: 0.9689 - val_loss: 0.1062 - val_acc: 0.9629
Epoch 3/40
 - 5s - loss: 0.0665 - acc: 0.9703 - val_loss: 0.0900 - val_acc: 0.9658
Epoch 4/40
 - 5s - loss: 0.0635 - acc: 0.9714 - val_loss: 0.0991 - val_acc: 0.9664
Epoch 5/40
 - 5s - loss: 0.0622 - acc: 0.9726 - val_loss: 0.1048 - val_acc: 0.9664
Epoch 6/40
 - 5s - loss: 0.0637 - acc: 0.9726 - val_loss: 0.1030 - val_acc: 0.9653
Epoch 7/40
 - 5s - loss: 0.0616 - acc: 0.9726 - val_loss: 0.1130 - val_acc: 0.9653
Epoch 8/40
 - 5s - loss: 0.0655 - acc: 0.9729 - val_loss: 0.1346 - val_acc: 0.9672
Epoch 9/40
 - 5s - loss: 0.0677 - acc: 0.9734 - val_loss: 0.1205 - val_acc: 0.9668
Epoch 10/40
 - 5s - loss: 0.0686 - acc: 0.9730 - val_loss: 0.1201 - val_acc: 0.9677
Epoch 11/40
 - 5s - loss: 0.0647 - acc: 0.9734 - val_loss: 0.1223 - val_acc: 0.9661
Epoch 12/40
 - 5s - loss: 0.0640 - acc: 0.9734 - val_loss: 0.1437 - val_acc: 0.9669
Epoch 13/40
 - 5s - loss: 0.0658 - acc: 0.9737 - val_loss: 0.1208 - val_acc: 0.9665
Epoch 14/40
 - 5s - loss: 0.0653 - acc: 0.9734 - val_loss: 0.1372 - val_acc: 0.9679
Epoch 15/40
 - 5s - loss: 0.0649 - acc: 0.9734 - val_loss: 0.1300 - val_acc: 0.9673
Epoch 16/40
 - 5s - loss: 0.0705 - acc: 0.9736 - val_loss: 0.1463 - val_acc: 0.9660
Epoch 17/40
 - 5s - loss: 0.0669 - acc: 0.9737 - val_loss: 0.1271 - val_acc: 0.9672
Epoch 18/40
 - 5s - loss: 0.0644 - acc: 0.9745 - val_loss: 0.1395 - val_acc: 0.9666
Epoch 19/40
 - 5s - loss: 0.0678 - acc: 0.9739 - val_loss: 0.1360 - val_acc: 0.9663
Epoch 20/40
 - 5s - loss: 0.0681 - acc: 0.9736 - val_loss: 0.1212 - val_acc: 0.9624
Epoch 21/40
 - 5s - loss: 0.0693 - acc: 0.9737 - val_loss: 0.1502 - val_acc: 0.9605
Epoch 22/40
 - 5s - loss: 0.0726 - acc: 0.9739 - val_loss: 0.1591 - val_acc: 0.9665
Epoch 23/40
 - 5s - loss: 0.0725 - acc: 0.9739 - val_loss: 0.1539 - val_acc: 0.9677
Epoch 24/40
 - 5s - loss: 0.0731 - acc: 0.9736 - val_loss: 0.1636 - val_acc: 0.9671
Epoch 25/40
 - 5s - loss: 0.0739 - acc: 0.9735 - val_loss: 0.1425 - val_acc: 0.9658
Epoch 26/40
 - 5s - loss: 0.0746 - acc: 0.9732 - val_loss: 0.1831 - val_acc: 0.9646
Epoch 27/40
 - 5s - loss: 0.0768 - acc: 0.9737 - val_loss: 0.1607 - val_acc: 0.9673
Epoch 28/40
 - 5s - loss: 0.0715 - acc: 0.9736 - val_loss: 0.1504 - val_acc: 0.9664
Epoch 29/40
 - 5s - loss: 0.0815 - acc: 0.9729 - val_loss: 0.2361 - val_acc: 0.9662
Epoch 30/40
 - 5s - loss: 0.0930 - acc: 0.9735 - val_loss: 0.1639 - val_acc: 0.9666
Epoch 31/40
 - 5s - loss: 0.0858 - acc: 0.9736 - val_loss: 0.1821 - val_acc: 0.9658
Epoch 32/40
 - 5s - loss: 0.0860 - acc: 0.9740 - val_loss: 0.1834 - val_acc: 0.9647
Epoch 33/40
 - 5s - loss: 0.0874 - acc: 0.9739 - val_loss: 0.1742 - val_acc: 0.9650
Epoch 34/40
 - 5s - loss: 0.0902 - acc: 0.9738 - val_loss: 0.1850 - val_acc: 0.9661
Epoch 35/40
 - 5s - loss: 0.0894 - acc: 0.9740 - val_loss: 0.1774 - val_acc: 0.9679
Epoch 36/40
 - 5s - loss: 0.0906 - acc: 0.9741 - val_loss: 0.1941 - val_acc: 0.9665
Epoch 37/40
 - 5s - loss: 0.0884 - acc: 0.9746 - val_loss: 0.1786 - val_acc: 0.9657
Epoch 38/40
 - 5s - loss: 0.0915 - acc: 0.9741 - val_loss: 0.1766 - val_acc: 0.9657
Epoch 39/40
 - 5s - loss: 0.0886 - acc: 0.9738 - val_loss: 0.1683 - val_acc: 0.9667
Epoch 40/40
 - 5s - loss: 0.0904 - acc: 0.9745 - val_loss: 0.1859 - val_acc: 0.9627
Epoch 1/40
 - 0s - loss: 0.1723 - acc: 0.9621
Epoch 2/40
 - 0s - loss: 0.1470 - acc: 0.9647
Epoch 3/40
 - 0s - loss: 0.1357 - acc: 0.9671
Epoch 4/40
 - 0s - loss: 0.1354 - acc: 0.9694
Epoch 5/40
 - 0s - loss: 0.1322 - acc: 0.9703
Epoch 6/40
 - 0s - loss: 0.1317 - acc: 0.9684
Epoch 7/40
 - 0s - loss: 0.1293 - acc: 0.9704
Epoch 8/40
 - 0s - loss: 0.1316 - acc: 0.9710
Epoch 9/40
 - 0s - loss: 0.1362 - acc: 0.9694
Epoch 10/40
 - 0s - loss: 0.1291 - acc: 0.9710
Epoch 11/40
 - 0s - loss: 0.1286 - acc: 0.9707
Epoch 12/40
 - 0s - loss: 0.1296 - acc: 0.9707
Epoch 13/40
 - 0s - loss: 0.1292 - acc: 0.9701
Epoch 14/40
 - 0s - loss: 0.1295 - acc: 0.9710
Epoch 15/40
 - 0s - loss: 0.1308 - acc: 0.9712
Epoch 16/40
 - 0s - loss: 0.1329 - acc: 0.9708
Epoch 17/40
 - 0s - loss: 0.1272 - acc: 0.9714
Epoch 18/40
 - 0s - loss: 0.1230 - acc: 0.9711
Epoch 19/40
 - 0s - loss: 0.1322 - acc: 0.9712
Epoch 20/40
 - 0s - loss: 0.1241 - acc: 0.9711
Epoch 21/40
 - 0s - loss: 0.1272 - acc: 0.9712
Epoch 22/40
 - 0s - loss: 0.1277 - acc: 0.9720
Epoch 23/40
 - 0s - loss: 0.1200 - acc: 0.9719
Epoch 24/40
 - 0s - loss: 0.1089 - acc: 0.9737
Epoch 25/40
 - 0s - loss: 0.1128 - acc: 0.9728
Epoch 26/40
 - 0s - loss: 0.1123 - acc: 0.9719
Epoch 27/40
 - 0s - loss: 0.1221 - acc: 0.9722
Epoch 28/40
 - 0s - loss: 0.1218 - acc: 0.9720
Epoch 29/40
 - 0s - loss: 0.1189 - acc: 0.9734
Epoch 30/40
 - 0s - loss: 0.1173 - acc: 0.9737
Epoch 31/40
 - 0s - loss: 0.1203 - acc: 0.9723
Epoch 32/40
 - 0s - loss: 0.1190 - acc: 0.9728
Epoch 33/40
 - 0s - loss: 0.1179 - acc: 0.9740
Epoch 34/40
 - 0s - loss: 0.1160 - acc: 0.9748
Epoch 35/40
 - 0s - loss: 0.1158 - acc: 0.9733
Epoch 36/40
 - 0s - loss: 0.1046 - acc: 0.9744
Epoch 37/40
 - 0s - loss: 0.1055 - acc: 0.9748
Epoch 38/40
 - 0s - loss: 0.1124 - acc: 0.9743
Epoch 39/40
 - 0s - loss: 0.1241 - acc: 0.9731
Epoch 40/40
 - 0s - loss: 0.1029 - acc: 0.9744
# Training time = 0:03:40.702515
# F-Score(Ordinary) = 0.309, Recall: 0.922, Precision: 0.186
# F-Score(ireflv) = 0.783, Recall: 0.922, Precision: 0.68
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_325 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_326 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_325 (Embedding)       (None, 4, 48)        705264      input_325[0][0]                  
__________________________________________________________________________________________________
embedding_326 (Embedding)       (None, 4, 24)        5640        input_326[0][0]                  
__________________________________________________________________________________________________
flatten_325 (Flatten)           (None, 192)          0           embedding_325[0][0]              
__________________________________________________________________________________________________
flatten_326 (Flatten)           (None, 96)           0           embedding_326[0][0]              
__________________________________________________________________________________________________
concatenate_163 (Concatenate)   (None, 288)          0           flatten_325[0][0]                
                                                                 flatten_326[0][0]                
__________________________________________________________________________________________________
dense_325 (Dense)               (None, 24)           6936        concatenate_163[0][0]            
__________________________________________________________________________________________________
dropout_163 (Dropout)           (None, 24)           0           dense_325[0][0]                  
__________________________________________________________________________________________________
dense_326 (Dense)               (None, 8)            200         dropout_163[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0961 - acc: 0.9623 - val_loss: 0.0846 - val_acc: 0.9672
Epoch 2/40
 - 5s - loss: 0.0694 - acc: 0.9691 - val_loss: 0.0989 - val_acc: 0.9611
Epoch 3/40
 - 5s - loss: 0.0660 - acc: 0.9705 - val_loss: 0.0895 - val_acc: 0.9669
Epoch 4/40
 - 5s - loss: 0.0647 - acc: 0.9712 - val_loss: 0.1046 - val_acc: 0.9656
Epoch 5/40
 - 5s - loss: 0.0638 - acc: 0.9719 - val_loss: 0.1211 - val_acc: 0.9660
Epoch 6/40
 - 5s - loss: 0.0635 - acc: 0.9719 - val_loss: 0.1034 - val_acc: 0.9678
Epoch 7/40
 - 5s - loss: 0.0635 - acc: 0.9726 - val_loss: 0.1121 - val_acc: 0.9672
Epoch 8/40
 - 5s - loss: 0.0646 - acc: 0.9731 - val_loss: 0.1156 - val_acc: 0.9679
Epoch 9/40
 - 5s - loss: 0.0638 - acc: 0.9731 - val_loss: 0.1178 - val_acc: 0.9630
Epoch 10/40
 - 5s - loss: 0.0639 - acc: 0.9730 - val_loss: 0.1161 - val_acc: 0.9678
Epoch 11/40
 - 5s - loss: 0.0638 - acc: 0.9737 - val_loss: 0.1341 - val_acc: 0.9659
Epoch 12/40
 - 5s - loss: 0.0636 - acc: 0.9734 - val_loss: 0.1281 - val_acc: 0.9668
Epoch 13/40
 - 5s - loss: 0.0651 - acc: 0.9732 - val_loss: 0.1152 - val_acc: 0.9665
Epoch 14/40
 - 5s - loss: 0.0648 - acc: 0.9734 - val_loss: 0.1343 - val_acc: 0.9669
Epoch 15/40
 - 5s - loss: 0.0686 - acc: 0.9735 - val_loss: 0.1319 - val_acc: 0.9658
Epoch 16/40
 - 5s - loss: 0.0661 - acc: 0.9743 - val_loss: 0.1230 - val_acc: 0.9680
Epoch 17/40
 - 5s - loss: 0.0684 - acc: 0.9737 - val_loss: 0.1382 - val_acc: 0.9667
Epoch 18/40
 - 5s - loss: 0.0728 - acc: 0.9735 - val_loss: 0.1218 - val_acc: 0.9673
Epoch 19/40
 - 5s - loss: 0.0689 - acc: 0.9736 - val_loss: 0.1651 - val_acc: 0.9662
Epoch 20/40
 - 5s - loss: 0.0691 - acc: 0.9744 - val_loss: 0.1619 - val_acc: 0.9669
Epoch 21/40
 - 5s - loss: 0.0724 - acc: 0.9736 - val_loss: 0.1651 - val_acc: 0.9659
Epoch 22/40
 - 5s - loss: 0.0721 - acc: 0.9737 - val_loss: 0.1735 - val_acc: 0.9658
Epoch 23/40
 - 5s - loss: 0.0743 - acc: 0.9741 - val_loss: 0.1378 - val_acc: 0.9678
Epoch 24/40
 - 5s - loss: 0.0820 - acc: 0.9733 - val_loss: 0.1507 - val_acc: 0.9663
Epoch 25/40
 - 5s - loss: 0.0765 - acc: 0.9738 - val_loss: 0.1636 - val_acc: 0.9667
Epoch 26/40
 - 5s - loss: 0.0797 - acc: 0.9733 - val_loss: 0.2004 - val_acc: 0.9667
Epoch 27/40
 - 5s - loss: 0.0750 - acc: 0.9740 - val_loss: 0.1489 - val_acc: 0.9655
Epoch 28/40
 - 5s - loss: 0.0750 - acc: 0.9740 - val_loss: 0.1636 - val_acc: 0.9664
Epoch 29/40
 - 5s - loss: 0.0805 - acc: 0.9733 - val_loss: 0.1593 - val_acc: 0.9664
Epoch 30/40
 - 5s - loss: 0.0822 - acc: 0.9737 - val_loss: 0.1456 - val_acc: 0.9654
Epoch 31/40
 - 5s - loss: 0.0784 - acc: 0.9742 - val_loss: 0.1714 - val_acc: 0.9652
Epoch 32/40
 - 5s - loss: 0.0795 - acc: 0.9737 - val_loss: 0.1608 - val_acc: 0.9667
Epoch 33/40
 - 5s - loss: 0.0809 - acc: 0.9741 - val_loss: 0.1609 - val_acc: 0.9662
Epoch 34/40
 - 5s - loss: 0.0823 - acc: 0.9732 - val_loss: 0.1576 - val_acc: 0.9645
Epoch 35/40
 - 5s - loss: 0.0857 - acc: 0.9735 - val_loss: 0.1789 - val_acc: 0.9656
Epoch 36/40
 - 5s - loss: 0.0898 - acc: 0.9737 - val_loss: 0.1682 - val_acc: 0.9657
Epoch 37/40
 - 5s - loss: 0.0858 - acc: 0.9735 - val_loss: 0.1721 - val_acc: 0.9662
Epoch 38/40
 - 5s - loss: 0.0847 - acc: 0.9736 - val_loss: 0.1714 - val_acc: 0.9661
Epoch 39/40
 - 5s - loss: 0.0887 - acc: 0.9733 - val_loss: 0.1543 - val_acc: 0.9663
Epoch 40/40
 - 5s - loss: 0.0888 - acc: 0.9729 - val_loss: 0.1653 - val_acc: 0.9655
Epoch 1/40
 - 0s - loss: 0.1628 - acc: 0.9628
Epoch 2/40
 - 0s - loss: 0.1437 - acc: 0.9640
Epoch 3/40
 - 0s - loss: 0.1389 - acc: 0.9655
Epoch 4/40
 - 0s - loss: 0.1293 - acc: 0.9666
Epoch 5/40
 - 0s - loss: 0.1302 - acc: 0.9673
Epoch 6/40
 - 0s - loss: 0.1288 - acc: 0.9699
Epoch 7/40
 - 0s - loss: 0.1300 - acc: 0.9702
Epoch 8/40
 - 0s - loss: 0.1175 - acc: 0.9706
Epoch 9/40
 - 0s - loss: 0.1232 - acc: 0.9701
Epoch 10/40
 - 0s - loss: 0.1168 - acc: 0.9717
Epoch 11/40
 - 0s - loss: 0.1206 - acc: 0.9714
Epoch 12/40
 - 0s - loss: 0.1140 - acc: 0.9724
Epoch 13/40
 - 0s - loss: 0.1244 - acc: 0.9723
Epoch 14/40
 - 0s - loss: 0.1251 - acc: 0.9723
Epoch 15/40
 - 0s - loss: 0.1202 - acc: 0.9738
Epoch 16/40
 - 0s - loss: 0.1286 - acc: 0.9733
Epoch 17/40
 - 0s - loss: 0.1323 - acc: 0.9724
Epoch 18/40
 - 0s - loss: 0.1268 - acc: 0.9722
Epoch 19/40
 - 0s - loss: 0.1263 - acc: 0.9729
Epoch 20/40
 - 0s - loss: 0.1158 - acc: 0.9731
Epoch 21/40
 - 0s - loss: 0.1379 - acc: 0.9708
Epoch 22/40
 - 0s - loss: 0.1388 - acc: 0.9724
Epoch 23/40
 - 0s - loss: 0.1394 - acc: 0.9709
Epoch 24/40
 - 0s - loss: 0.1334 - acc: 0.9730
Epoch 25/40
 - 0s - loss: 0.1144 - acc: 0.9721
Epoch 26/40
 - 0s - loss: 0.1199 - acc: 0.9726
Epoch 27/40
 - 0s - loss: 0.1181 - acc: 0.9738
Epoch 28/40
 - 0s - loss: 0.1139 - acc: 0.9744
Epoch 29/40
 - 0s - loss: 0.1137 - acc: 0.9737
Epoch 30/40
 - 0s - loss: 0.1142 - acc: 0.9739
Epoch 31/40
 - 0s - loss: 0.1213 - acc: 0.9734
Epoch 32/40
 - 0s - loss: 0.1281 - acc: 0.9733
Epoch 33/40
 - 0s - loss: 0.1119 - acc: 0.9744
Epoch 34/40
 - 0s - loss: 0.1086 - acc: 0.9741
Epoch 35/40
 - 0s - loss: 0.1143 - acc: 0.9739
Epoch 36/40
 - 0s - loss: 0.1156 - acc: 0.9738
Epoch 37/40
 - 0s - loss: 0.1166 - acc: 0.9741
Epoch 38/40
 - 0s - loss: 0.1235 - acc: 0.9741
Epoch 39/40
 - 0s - loss: 0.1307 - acc: 0.9735
Epoch 40/40
 - 0s - loss: 0.1298 - acc: 0.9738
# Training time = 0:03:46.643613
# F-Score(Ordinary) = 0.022, Recall: 0.625, Precision: 0.011
# F-Score(id) = 0.05, Recall: 0.714, Precision: 0.026
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_327 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_328 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_327 (Embedding)       (None, 4, 48)        705264      input_327[0][0]                  
__________________________________________________________________________________________________
embedding_328 (Embedding)       (None, 4, 24)        5640        input_328[0][0]                  
__________________________________________________________________________________________________
flatten_327 (Flatten)           (None, 192)          0           embedding_327[0][0]              
__________________________________________________________________________________________________
flatten_328 (Flatten)           (None, 96)           0           embedding_328[0][0]              
__________________________________________________________________________________________________
concatenate_164 (Concatenate)   (None, 288)          0           flatten_327[0][0]                
                                                                 flatten_328[0][0]                
__________________________________________________________________________________________________
dense_327 (Dense)               (None, 24)           6936        concatenate_164[0][0]            
__________________________________________________________________________________________________
dropout_164 (Dropout)           (None, 24)           0           dense_327[0][0]                  
__________________________________________________________________________________________________
dense_328 (Dense)               (None, 8)            200         dropout_164[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0966 - acc: 0.9622 - val_loss: 0.0783 - val_acc: 0.9656
Epoch 2/40
 - 5s - loss: 0.0705 - acc: 0.9689 - val_loss: 0.0849 - val_acc: 0.9643
Epoch 3/40
 - 5s - loss: 0.0655 - acc: 0.9704 - val_loss: 0.0898 - val_acc: 0.9662
Epoch 4/40
 - 5s - loss: 0.0637 - acc: 0.9716 - val_loss: 0.0893 - val_acc: 0.9674
Epoch 5/40
 - 5s - loss: 0.0635 - acc: 0.9722 - val_loss: 0.1216 - val_acc: 0.9663
Epoch 6/40
 - 5s - loss: 0.0637 - acc: 0.9723 - val_loss: 0.1171 - val_acc: 0.9667
Epoch 7/40
 - 5s - loss: 0.0618 - acc: 0.9726 - val_loss: 0.1160 - val_acc: 0.9675
Epoch 8/40
 - 5s - loss: 0.0617 - acc: 0.9730 - val_loss: 0.1492 - val_acc: 0.9653
Epoch 9/40
 - 5s - loss: 0.0656 - acc: 0.9731 - val_loss: 0.1320 - val_acc: 0.9659
Epoch 10/40
 - 5s - loss: 0.0639 - acc: 0.9730 - val_loss: 0.1219 - val_acc: 0.9667
Epoch 11/40
 - 5s - loss: 0.0638 - acc: 0.9737 - val_loss: 0.1256 - val_acc: 0.9663
Epoch 12/40
 - 5s - loss: 0.0651 - acc: 0.9731 - val_loss: 0.1249 - val_acc: 0.9664
Epoch 13/40
 - 5s - loss: 0.0645 - acc: 0.9733 - val_loss: 0.1382 - val_acc: 0.9660
Epoch 14/40
 - 5s - loss: 0.0644 - acc: 0.9737 - val_loss: 0.1157 - val_acc: 0.9671
Epoch 15/40
 - 5s - loss: 0.0635 - acc: 0.9734 - val_loss: 0.1210 - val_acc: 0.9665
Epoch 16/40
 - 5s - loss: 0.0680 - acc: 0.9736 - val_loss: 0.1406 - val_acc: 0.9663
Epoch 17/40
 - 5s - loss: 0.0661 - acc: 0.9738 - val_loss: 0.1417 - val_acc: 0.9662
Epoch 18/40
 - 5s - loss: 0.0663 - acc: 0.9742 - val_loss: 0.1376 - val_acc: 0.9666
Epoch 19/40
 - 5s - loss: 0.0675 - acc: 0.9740 - val_loss: 0.1663 - val_acc: 0.9663
Epoch 20/40
 - 5s - loss: 0.0679 - acc: 0.9734 - val_loss: 0.1425 - val_acc: 0.9661
Epoch 21/40
 - 5s - loss: 0.0683 - acc: 0.9740 - val_loss: 0.1758 - val_acc: 0.9663
Epoch 22/40
 - 5s - loss: 0.0683 - acc: 0.9740 - val_loss: 0.1502 - val_acc: 0.9650
Epoch 23/40
 - 5s - loss: 0.0740 - acc: 0.9735 - val_loss: 0.1473 - val_acc: 0.9661
Epoch 24/40
 - 5s - loss: 0.0715 - acc: 0.9735 - val_loss: 0.1501 - val_acc: 0.9659
Epoch 25/40
 - 5s - loss: 0.0750 - acc: 0.9738 - val_loss: 0.1764 - val_acc: 0.9660
Epoch 26/40
 - 5s - loss: 0.0784 - acc: 0.9738 - val_loss: 0.1701 - val_acc: 0.9662
Epoch 27/40
 - 5s - loss: 0.0889 - acc: 0.9733 - val_loss: 0.1652 - val_acc: 0.9658
Epoch 28/40
 - 5s - loss: 0.0815 - acc: 0.9731 - val_loss: 0.1497 - val_acc: 0.9656
Epoch 29/40
 - 5s - loss: 0.0755 - acc: 0.9731 - val_loss: 0.1661 - val_acc: 0.9636
Epoch 30/40
 - 5s - loss: 0.0767 - acc: 0.9736 - val_loss: 0.1689 - val_acc: 0.9623
Epoch 31/40
 - 5s - loss: 0.0787 - acc: 0.9741 - val_loss: 0.1655 - val_acc: 0.9658
Epoch 32/40
 - 5s - loss: 0.0835 - acc: 0.9735 - val_loss: 0.1536 - val_acc: 0.9655
Epoch 33/40
 - 5s - loss: 0.0792 - acc: 0.9731 - val_loss: 0.1674 - val_acc: 0.9667
Epoch 34/40
 - 5s - loss: 0.0780 - acc: 0.9732 - val_loss: 0.1423 - val_acc: 0.9661
Epoch 35/40
 - 5s - loss: 0.0774 - acc: 0.9737 - val_loss: 0.1591 - val_acc: 0.9650
Epoch 36/40
 - 5s - loss: 0.0850 - acc: 0.9733 - val_loss: 0.1782 - val_acc: 0.9657
Epoch 37/40
 - 5s - loss: 0.0858 - acc: 0.9739 - val_loss: 0.1540 - val_acc: 0.9654
Epoch 38/40
 - 5s - loss: 0.0825 - acc: 0.9738 - val_loss: 0.1587 - val_acc: 0.9648
Epoch 39/40
 - 5s - loss: 0.0865 - acc: 0.9737 - val_loss: 0.1827 - val_acc: 0.9671
Epoch 40/40
 - 5s - loss: 0.0868 - acc: 0.9741 - val_loss: 0.1896 - val_acc: 0.9660
Epoch 1/40
 - 0s - loss: 0.1647 - acc: 0.9627
Epoch 2/40
 - 0s - loss: 0.1484 - acc: 0.9633
Epoch 3/40
 - 0s - loss: 0.1541 - acc: 0.9647
Epoch 4/40
 - 0s - loss: 0.1492 - acc: 0.9663
Epoch 5/40
 - 0s - loss: 0.1301 - acc: 0.9686
Epoch 6/40
 - 0s - loss: 0.1172 - acc: 0.9706
Epoch 7/40
 - 0s - loss: 0.1227 - acc: 0.9690
Epoch 8/40
 - 0s - loss: 0.1219 - acc: 0.9695
Epoch 9/40
 - 0s - loss: 0.1139 - acc: 0.9706
Epoch 10/40
 - 0s - loss: 0.1109 - acc: 0.9709
Epoch 11/40
 - 0s - loss: 0.1129 - acc: 0.9704
Epoch 12/40
 - 0s - loss: 0.1001 - acc: 0.9730
Epoch 13/40
 - 0s - loss: 0.1034 - acc: 0.9722
Epoch 14/40
 - 0s - loss: 0.1081 - acc: 0.9715
Epoch 15/40
 - 0s - loss: 0.1126 - acc: 0.9716
Epoch 16/40
 - 0s - loss: 0.1221 - acc: 0.9725
Epoch 17/40
 - 0s - loss: 0.1203 - acc: 0.9725
Epoch 18/40
 - 0s - loss: 0.1172 - acc: 0.9734
Epoch 19/40
 - 0s - loss: 0.1182 - acc: 0.9730
Epoch 20/40
 - 0s - loss: 0.1120 - acc: 0.9739
Epoch 21/40
 - 0s - loss: 0.1521 - acc: 0.9738
Epoch 22/40
 - 0s - loss: 0.1156 - acc: 0.9744
Epoch 23/40
 - 0s - loss: 0.1099 - acc: 0.9745
Epoch 24/40
 - 0s - loss: 0.1150 - acc: 0.9745
Epoch 25/40
 - 0s - loss: 0.1126 - acc: 0.9738
Epoch 26/40
 - 0s - loss: 0.1198 - acc: 0.9739
Epoch 27/40
 - 0s - loss: 0.1149 - acc: 0.9741
Epoch 28/40
 - 0s - loss: 0.1172 - acc: 0.9742
Epoch 29/40
 - 0s - loss: 0.1147 - acc: 0.9742
Epoch 30/40
 - 0s - loss: 0.1146 - acc: 0.9742
Epoch 31/40
 - 0s - loss: 0.1109 - acc: 0.9745
Epoch 32/40
 - 0s - loss: 0.1063 - acc: 0.9750
Epoch 33/40
 - 0s - loss: 0.1107 - acc: 0.9736
Epoch 34/40
 - 0s - loss: 0.1109 - acc: 0.9738
Epoch 35/40
 - 0s - loss: 0.1196 - acc: 0.9738
Epoch 36/40
 - 0s - loss: 0.1119 - acc: 0.9745
Epoch 37/40
 - 0s - loss: 0.1085 - acc: 0.9750
Epoch 38/40
 - 0s - loss: 0.1052 - acc: 0.9757
Epoch 39/40
 - 0s - loss: 0.1174 - acc: 0.9740
Epoch 40/40
 - 0s - loss: 0.1139 - acc: 0.9744
# Training time = 0:03:48.218568
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_329 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_330 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_329 (Embedding)       (None, 4, 48)        705264      input_329[0][0]                  
__________________________________________________________________________________________________
embedding_330 (Embedding)       (None, 4, 24)        5640        input_330[0][0]                  
__________________________________________________________________________________________________
flatten_329 (Flatten)           (None, 192)          0           embedding_329[0][0]              
__________________________________________________________________________________________________
flatten_330 (Flatten)           (None, 96)           0           embedding_330[0][0]              
__________________________________________________________________________________________________
concatenate_165 (Concatenate)   (None, 288)          0           flatten_329[0][0]                
                                                                 flatten_330[0][0]                
__________________________________________________________________________________________________
dense_329 (Dense)               (None, 24)           6936        concatenate_165[0][0]            
__________________________________________________________________________________________________
dropout_165 (Dropout)           (None, 24)           0           dense_329[0][0]                  
__________________________________________________________________________________________________
dense_330 (Dense)               (None, 8)            200         dropout_165[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.2911783840762915, 1.0: 0.3061502805421506, 2.0: 59.54111703620544, 4.0: 254.46203670385032, 5.0: 190.2475114339521, 6.0: 253.39823220258, 7.0: 303064.28571428574}
# Network optimizer = nadam, learning rate = 0.01
Train on 190930 samples, validate on 21215 samples
Epoch 1/40
 - 5s - loss: 0.0975 - acc: 0.9615 - val_loss: 0.0796 - val_acc: 0.9657
Epoch 2/40
 - 5s - loss: 0.0698 - acc: 0.9690 - val_loss: 0.0947 - val_acc: 0.9653
Epoch 3/40
 - 5s - loss: 0.0655 - acc: 0.9699 - val_loss: 0.1019 - val_acc: 0.9676
Epoch 4/40
 - 5s - loss: 0.0627 - acc: 0.9713 - val_loss: 0.0959 - val_acc: 0.9667
Epoch 5/40
 - 5s - loss: 0.0636 - acc: 0.9721 - val_loss: 0.1022 - val_acc: 0.9663
Epoch 6/40
 - 5s - loss: 0.0629 - acc: 0.9724 - val_loss: 0.0952 - val_acc: 0.9677
Epoch 7/40
 - 5s - loss: 0.0640 - acc: 0.9726 - val_loss: 0.1187 - val_acc: 0.9661
Epoch 8/40
 - 5s - loss: 0.0645 - acc: 0.9729 - val_loss: 0.1362 - val_acc: 0.9659
Epoch 9/40
 - 5s - loss: 0.0633 - acc: 0.9731 - val_loss: 0.1113 - val_acc: 0.9664
Epoch 10/40
 - 5s - loss: 0.0656 - acc: 0.9732 - val_loss: 0.1271 - val_acc: 0.9657
Epoch 11/40
 - 5s - loss: 0.0622 - acc: 0.9735 - val_loss: 0.1311 - val_acc: 0.9667
Epoch 12/40
 - 5s - loss: 0.0632 - acc: 0.9737 - val_loss: 0.1593 - val_acc: 0.9662
Epoch 13/40
 - 5s - loss: 0.0686 - acc: 0.9738 - val_loss: 0.1258 - val_acc: 0.9669
Epoch 14/40
 - 5s - loss: 0.0675 - acc: 0.9741 - val_loss: 0.1497 - val_acc: 0.9666
Epoch 15/40
 - 5s - loss: 0.0667 - acc: 0.9734 - val_loss: 0.1312 - val_acc: 0.9665
Epoch 16/40
 - 5s - loss: 0.0681 - acc: 0.9739 - val_loss: 0.1511 - val_acc: 0.9660
Epoch 17/40
 - 5s - loss: 0.0687 - acc: 0.9740 - val_loss: 0.1386 - val_acc: 0.9673
Epoch 18/40
 - 5s - loss: 0.0692 - acc: 0.9739 - val_loss: 0.1567 - val_acc: 0.9664
Epoch 19/40
 - 5s - loss: 0.0708 - acc: 0.9736 - val_loss: 0.1338 - val_acc: 0.9646
Epoch 20/40
 - 5s - loss: 0.0791 - acc: 0.9729 - val_loss: 0.1513 - val_acc: 0.9648
Epoch 21/40
 - 5s - loss: 0.0787 - acc: 0.9738 - val_loss: 0.1500 - val_acc: 0.9674
Epoch 22/40
 - 5s - loss: 0.0768 - acc: 0.9739 - val_loss: 0.1822 - val_acc: 0.9635
Epoch 23/40
 - 5s - loss: 0.0844 - acc: 0.9739 - val_loss: 0.1299 - val_acc: 0.9673
Epoch 24/40
 - 5s - loss: 0.0788 - acc: 0.9740 - val_loss: 0.1532 - val_acc: 0.9669
Epoch 25/40
 - 5s - loss: 0.0864 - acc: 0.9737 - val_loss: 0.1688 - val_acc: 0.9656
Epoch 26/40
 - 5s - loss: 0.0818 - acc: 0.9737 - val_loss: 0.1590 - val_acc: 0.9674
Epoch 27/40
 - 5s - loss: 0.0919 - acc: 0.9728 - val_loss: 0.1693 - val_acc: 0.9662
Epoch 28/40
 - 5s - loss: 0.0849 - acc: 0.9734 - val_loss: 0.1506 - val_acc: 0.9668
Epoch 29/40
 - 5s - loss: 0.0865 - acc: 0.9732 - val_loss: 0.1500 - val_acc: 0.9667
Epoch 30/40
 - 5s - loss: 0.0924 - acc: 0.9726 - val_loss: 0.1495 - val_acc: 0.9662
Epoch 31/40
 - 5s - loss: 0.0971 - acc: 0.9733 - val_loss: 0.2165 - val_acc: 0.9639
Epoch 32/40
 - 5s - loss: 0.0996 - acc: 0.9736 - val_loss: 0.1844 - val_acc: 0.9664
Epoch 33/40
 - 5s - loss: 0.0960 - acc: 0.9737 - val_loss: 0.1612 - val_acc: 0.9634
Epoch 34/40
 - 5s - loss: 0.0954 - acc: 0.9735 - val_loss: 0.1668 - val_acc: 0.9658
Epoch 35/40
 - 5s - loss: 0.0960 - acc: 0.9732 - val_loss: 0.1849 - val_acc: 0.9642
Epoch 36/40
 - 5s - loss: 0.0921 - acc: 0.9736 - val_loss: 0.1740 - val_acc: 0.9666
Epoch 37/40
 - 5s - loss: 0.0924 - acc: 0.9741 - val_loss: 0.1558 - val_acc: 0.9667
Epoch 38/40
 - 5s - loss: 0.0982 - acc: 0.9730 - val_loss: 0.1585 - val_acc: 0.9664
Epoch 39/40
 - 5s - loss: 0.1022 - acc: 0.9734 - val_loss: 0.1777 - val_acc: 0.9642
Epoch 40/40
 - 5s - loss: 0.0963 - acc: 0.9735 - val_loss: 0.1907 - val_acc: 0.9662
Epoch 1/40
 - 0s - loss: 0.1782 - acc: 0.9640
Epoch 2/40
 - 0s - loss: 0.1597 - acc: 0.9633
Epoch 3/40
 - 0s - loss: 0.1380 - acc: 0.9656
Epoch 4/40
 - 0s - loss: 0.1893 - acc: 0.9645
Epoch 5/40
 - 0s - loss: 0.1723 - acc: 0.9657
Epoch 6/40
 - 0s - loss: 0.1695 - acc: 0.9663
Epoch 7/40
 - 0s - loss: 0.1805 - acc: 0.9672
Epoch 8/40
 - 0s - loss: 0.1798 - acc: 0.9690
Epoch 9/40
 - 0s - loss: 0.1811 - acc: 0.9700
Epoch 10/40
 - 0s - loss: 0.1689 - acc: 0.9699
Epoch 11/40
 - 0s - loss: 0.1608 - acc: 0.9704
Epoch 12/40
 - 0s - loss: 0.1549 - acc: 0.9693
Epoch 13/40
 - 0s - loss: 0.1493 - acc: 0.9704
Epoch 14/40
 - 0s - loss: 0.1350 - acc: 0.9719
Epoch 15/40
 - 0s - loss: 0.1177 - acc: 0.9730
Epoch 16/40
 - 0s - loss: 0.1400 - acc: 0.9721
Epoch 17/40
 - 0s - loss: 0.1400 - acc: 0.9726
Epoch 18/40
 - 0s - loss: 0.1310 - acc: 0.9719
Epoch 19/40
 - 0s - loss: 0.1227 - acc: 0.9724
Epoch 20/40
 - 0s - loss: 0.1353 - acc: 0.9730
Epoch 21/40
 - 0s - loss: 0.1511 - acc: 0.9737
Epoch 22/40
 - 0s - loss: 0.1364 - acc: 0.9737
Epoch 23/40
 - 0s - loss: 0.1275 - acc: 0.9729
Epoch 24/40
 - 0s - loss: 0.1260 - acc: 0.9732
Epoch 25/40
 - 0s - loss: 0.1230 - acc: 0.9730
Epoch 26/40
 - 0s - loss: 0.1273 - acc: 0.9734
Epoch 27/40
 - 0s - loss: 0.1248 - acc: 0.9735
Epoch 28/40
 - 0s - loss: 0.1271 - acc: 0.9736
Epoch 29/40
 - 0s - loss: 0.1213 - acc: 0.9742
Epoch 30/40
 - 0s - loss: 0.1267 - acc: 0.9736
Epoch 31/40
 - 0s - loss: 0.1241 - acc: 0.9742
Epoch 32/40
 - 0s - loss: 0.1226 - acc: 0.9742
Epoch 33/40
 - 0s - loss: 0.1230 - acc: 0.9737
Epoch 34/40
 - 0s - loss: 0.1192 - acc: 0.9746
Epoch 35/40
 - 0s - loss: 0.1286 - acc: 0.9744
Epoch 36/40
 - 0s - loss: 0.1296 - acc: 0.9724
Epoch 37/40
 - 0s - loss: 0.1320 - acc: 0.9729
Epoch 38/40
 - 0s - loss: 0.1425 - acc: 0.9739
Epoch 39/40
 - 0s - loss: 0.1347 - acc: 0.9739
Epoch 40/40
 - 0s - loss: 0.1342 - acc: 0.9734
# Training time = 0:03:47.575731
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
