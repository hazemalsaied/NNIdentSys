INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 48)        705264      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 24)        5640        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 192)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 96)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 288)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 24)           6936        concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            200         dropout_1[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0248 - acc: 0.9950 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0154 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0149 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0147 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0145 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0156 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0154 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0151 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0150 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0150 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0151 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0155 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0153 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0157 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0157 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0157 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0157 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 7.9684e-04 - acc: 0.9999
Epoch 2/40
 - 7s - loss: 4.2407e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 5.9130e-04 - acc: 0.9999
Epoch 4/40
 - 7s - loss: 8.3843e-04 - acc: 0.9999
Epoch 5/40
 - 7s - loss: 5.2771e-04 - acc: 0.9999
Epoch 6/40
 - 7s - loss: 5.9016e-04 - acc: 0.9999
Epoch 7/40
 - 7s - loss: 2.6522e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 7.9993e-04 - acc: 0.9999
Epoch 9/40
 - 7s - loss: 4.3293e-04 - acc: 0.9999
Epoch 10/40
 - 7s - loss: 4.0059e-04 - acc: 0.9999
Epoch 11/40
 - 7s - loss: 3.7075e-04 - acc: 0.9999
Epoch 12/40
 - 7s - loss: 1.6351e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 3.3926e-04 - acc: 0.9999
Epoch 14/40
 - 7s - loss: 3.3715e-04 - acc: 0.9999
Epoch 15/40
 - 7s - loss: 1.6717e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 2.9429e-04 - acc: 0.9999
Epoch 17/40
 - 7s - loss: 2.2720e-04 - acc: 0.9999
Epoch 18/40
 - 7s - loss: 1.3590e-04 - acc: 0.9999
Epoch 19/40
 - 7s - loss: 1.5462e-04 - acc: 0.9999
Epoch 20/40
 - 7s - loss: 1.3362e-04 - acc: 0.9999
Epoch 21/40
 - 7s - loss: 7.3657e-05 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 6.2876e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 5.0439e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 4.4403e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.7130e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 8.0658e-06 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 8.4650e-06 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 5.6580e-06 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 4.3757e-06 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.6475e-06 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.2859e-06 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 8.7197e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 5.9569e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 4.8483e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 4.0739e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 2.7335e-07 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 2.1176e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.7138e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.8580e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 1.4872e-07 - acc: 1.0000
# Training time = 0:47:23.845080
# F-Score(Ordinary) = 0.72, Recall: 0.846, Precision: 0.626
# F-Score(lvc) = 0.592, Recall: 0.906, Precision: 0.439
# F-Score(ireflv) = 0.847, Recall: 0.877, Precision: 0.82
# F-Score(id) = 0.701, Recall: 0.796, Precision: 0.627
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 48)        705264      input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 24)        5640        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 192)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 96)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 288)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 24)           6936        concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 24)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            200         dropout_2[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0250 - acc: 0.9950 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0155 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0149 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0146 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0145 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0143 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0143 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0148 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0148 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0150 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0152 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0150 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0150 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0150 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0152 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0153 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0155 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0154 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0164 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0156 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0158 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0162 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 1.9375e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 1.7895e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 1.7634e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 4.3687e-05 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 1.2951e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 8.5365e-05 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.5131e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 8.2538e-05 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 4.1068e-05 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 4.0843e-05 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 8.0883e-05 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 3.5490e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 7.7412e-05 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 1.9038e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 2.2259e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.0903e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.4284e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.3986e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 6.8895e-05 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 6.8036e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.9932e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 9.7527e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 9.5835e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 2.4802e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 9.0434e-05 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 1.4699e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.4249e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.2028e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 8.0007e-05 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.2923e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 1.2455e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 4.8677e-05 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.4145e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 4.6011e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 8.9763e-05 - acc: 1.0000
# Training time = 0:47:16.815198
# F-Score(Ordinary) = 0.698, Recall: 0.835, Precision: 0.6
# F-Score(lvc) = 0.595, Recall: 0.921, Precision: 0.439
# F-Score(ireflv) = 0.805, Recall: 0.853, Precision: 0.762
# F-Score(id) = 0.669, Recall: 0.77, Precision: 0.591
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 48)        705264      input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 24)        5640        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 192)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 96)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 288)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 24)           6936        concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 24)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            200         dropout_3[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0253 - acc: 0.9949 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0157 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0150 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0146 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0144 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0147 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0149 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0150 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0152 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0150 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0150 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0150 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0153 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0153 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 9.1020e-05 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 3.7440e-07 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 9.7373e-05 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 4.8407e-05 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 1.1995e-07 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 4.8174e-05 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 4.7944e-05 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 4.7722e-05 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 4.7491e-05 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 4.7263e-05 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 4.7033e-05 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 4.6806e-05 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 4.6576e-05 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 9.2352e-05 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 4.5934e-05 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 4.5704e-05 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 9.0598e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.3426e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 4.4436e-05 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 4.4207e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 8.7607e-05 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 8.6700e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 4.3119e-05 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.2780e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 4.2301e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:47:16.345096
# F-Score(Ordinary) = 0.694, Recall: 0.829, Precision: 0.597
# F-Score(lvc) = 0.6, Recall: 0.983, Precision: 0.432
# F-Score(ireflv) = 0.809, Recall: 0.908, Precision: 0.73
# F-Score(id) = 0.697, Recall: 0.829, Precision: 0.601
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 48)        705264      input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 24)        5640        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 192)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 96)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 288)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 24)           6936        concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            200         dropout_4[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0254 - acc: 0.9949 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0155 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0149 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0145 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 64s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 65s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 66s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 65s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 66s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0147 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0150 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0151 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0154 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0161 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0156 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0158 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0162 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0163 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0162 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0162 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0160 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 1.2497e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 1.8726e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 1.2147e-07 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 1.3851e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 2.2719e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 1.7851e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.1922e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 1.2951e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 2.5417e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 8.3561e-05 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 2.8652e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 2.0001e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 3.9625e-05 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 2.3310e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 1.5199e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.1217e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.1046e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 7.2643e-05 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.0733e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.0544e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 3.4852e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 6.8899e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 1.6863e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 3.3372e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.6334e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 9.5929e-05 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 1.8712e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 6.1065e-05 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 9.0113e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.6160e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 5.6635e-05 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.3818e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.0742e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.6503e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.5449e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 5.0160e-05 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 7.3682e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 4.8161e-05 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.3849e-05 - acc: 1.0000
# Training time = 0:47:23.184061
# F-Score(Ordinary) = 0.721, Recall: 0.863, Precision: 0.62
# F-Score(lvc) = 0.604, Recall: 0.967, Precision: 0.439
# F-Score(ireflv) = 0.825, Recall: 0.887, Precision: 0.77
# F-Score(id) = 0.678, Recall: 0.761, Precision: 0.611
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 48)        705264      input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 24)        5640        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 192)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 96)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 288)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 24)           6936        concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 24)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            200         dropout_5[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0253 - acc: 0.9950 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 64s - loss: 0.0157 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 64s - loss: 0.0150 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 64s - loss: 0.0147 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 64s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 64s - loss: 0.0144 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 64s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 64s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 64s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 64s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 64s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 64s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0151 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0154 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0153 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0159 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0170 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0169 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0156 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0154 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0158 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 66s - loss: 0.0159 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 64s - loss: 0.0157 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0158 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0161 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0164 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0163 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 6.5356e-04 - acc: 0.9999
Epoch 2/40
 - 7s - loss: 5.1047e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 5.2759e-04 - acc: 0.9999
Epoch 4/40
 - 7s - loss: 5.4008e-04 - acc: 0.9999
Epoch 5/40
 - 7s - loss: 6.8253e-04 - acc: 0.9999
Epoch 6/40
 - 7s - loss: 4.5166e-04 - acc: 0.9999
Epoch 7/40
 - 7s - loss: 7.5580e-04 - acc: 0.9999
Epoch 8/40
 - 7s - loss: 3.9655e-04 - acc: 0.9999
Epoch 9/40
 - 7s - loss: 4.0117e-04 - acc: 0.9999
Epoch 10/40
 - 7s - loss: 6.1593e-04 - acc: 0.9999
Epoch 11/40
 - 7s - loss: 3.1866e-04 - acc: 0.9999
Epoch 12/40
 - 7s - loss: 4.2018e-04 - acc: 0.9999
Epoch 13/40
 - 7s - loss: 1.5589e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 3.5629e-04 - acc: 0.9999
Epoch 15/40
 - 7s - loss: 3.0075e-04 - acc: 0.9999
Epoch 16/40
 - 7s - loss: 2.6622e-04 - acc: 0.9999
Epoch 17/40
 - 7s - loss: 2.0785e-04 - acc: 0.9999
Epoch 18/40
 - 7s - loss: 2.1491e-04 - acc: 0.9999
Epoch 19/40
 - 7s - loss: 1.6129e-04 - acc: 0.9999
Epoch 20/40
 - 7s - loss: 1.0907e-04 - acc: 0.9999
Epoch 21/40
 - 7s - loss: 6.7284e-05 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 5.0918e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 4.9812e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 3.9532e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.8409e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 3.4056e-05 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.7109e-05 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 8.5563e-06 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 5.8920e-06 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 3.6633e-06 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.0527e-06 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.6601e-06 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 8.5440e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 6.0301e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 4.5667e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 3.8387e-07 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 2.0966e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 2.4476e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.0270e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 1.6938e-07 - acc: 1.0000
# Training time = 0:48:03.375103
# F-Score(Ordinary) = 0.647, Recall: 0.672, Precision: 0.624
# F-Score(lvc) = 0.594, Recall: 0.857, Precision: 0.455
# F-Score(ireflv) = 0.822, Recall: 0.832, Precision: 0.811
# F-Score(id) = 0.556, Recall: 0.518, Precision: 0.601
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 48)        705264      input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 24)        5640        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 192)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 96)           0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 288)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 24)           6936        concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 24)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            200         dropout_6[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0252 - acc: 0.9949 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0156 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0150 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0147 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0145 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0143 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0144 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0144 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0144 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0146 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0145 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0147 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0147 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0145 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0146 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0149 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0153 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0151 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0153 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0154 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0156 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0158 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0157 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0159 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 3.9655e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 1.8060e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 8.9278e-05 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 1.3247e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 4.3897e-05 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 2.1601e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 8.5215e-05 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 8.4461e-05 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 1.6638e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 8.2172e-05 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 8.1330e-05 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 4.0395e-05 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 1.9855e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 1.9416e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 1.1456e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 7.5488e-05 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 2.2157e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.4457e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 7.1248e-05 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.7455e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 1.0263e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.0044e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 9.8032e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 6.4440e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 3.2001e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.2556e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 9.2237e-05 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 1.4994e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 8.7930e-05 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 5.7728e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 8.4962e-05 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.3769e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 8.0637e-05 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.8193e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 5.0631e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 9.8690e-05 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 9.5889e-05 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 4.6948e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.5844e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 6.5467e-05 - acc: 1.0000
# Training time = 0:47:06.045876
# F-Score(Ordinary) = 0.663, Recall: 0.784, Precision: 0.575
# F-Score(lvc) = 0.474, Recall: 0.563, Precision: 0.409
# F-Score(ireflv) = 0.811, Recall: 0.926, Precision: 0.721
# F-Score(id) = 0.667, Recall: 0.803, Precision: 0.57
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 48)        705264      input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 24)        5640        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 192)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 96)           0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 288)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 24)           6936        concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 24)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            200         dropout_7[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0252 - acc: 0.9950 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 64s - loss: 0.0155 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 66s - loss: 0.0148 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0145 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 64s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 65s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0147 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0152 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0150 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0152 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0150 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0150 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0149 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 66s - loss: 0.0149 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0156 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0156 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0163 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 65s - loss: 0.0165 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0169 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0164 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0168 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0172 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0170 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 1.0476e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 1.1902e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 1.2822e-07 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 1.0959e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 5.4568e-05 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 1.0834e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 5.3939e-05 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 1.0708e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 5.3287e-05 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 5.3056e-05 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 5.2834e-05 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.5706e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.5517e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 5.1414e-05 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 5.1210e-05 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 5.0981e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 5.0755e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 5.0525e-05 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 5.0295e-05 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 5.0064e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.4858e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 4.9193e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 9.7614e-05 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 9.6737e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 4.8086e-05 - acc: 1.0000
# Training time = 0:47:27.356706
# F-Score(Ordinary) = 0.672, Recall: 0.787, Precision: 0.586
# F-Score(lvc) = 0.541, Recall: 0.943, Precision: 0.379
# F-Score(ireflv) = 0.817, Recall: 0.87, Precision: 0.77
# F-Score(id) = 0.625, Recall: 0.663, Precision: 0.591
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 48)        705264      input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 24)        5640        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 192)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 96)           0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 288)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 24)           6936        concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 24)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            200         dropout_8[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0253 - acc: 0.9949 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0155 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0149 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0146 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0144 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0143 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0141 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 65s - loss: 0.0140 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0140 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0143 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0144 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0152 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0155 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0146 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0148 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0149 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0150 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0151 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0156 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0151 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0151 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0160 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0167 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0169 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0166 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0167 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 65s - loss: 0.0168 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 66s - loss: 0.0169 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0167 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0170 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 66s - loss: 0.0169 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 1.4935e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 8.7090e-05 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 4.3313e-05 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 8.5879e-05 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 1.2717e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 1.6696e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.0460e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 1.2077e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 1.1902e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.1724e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 1.5359e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 1.1322e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 1.8513e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 1.0906e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 3.6121e-05 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.7721e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.7281e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.3532e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 3.3477e-05 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 6.6164e-05 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 6.5350e-05 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 9.6401e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 9.4542e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.4427e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 8.9178e-05 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.1641e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 8.5475e-05 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.1134e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 8.1639e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.5851e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.0242e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.9672e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.6417e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 9.0479e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.3045e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.0452e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 6.0675e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 3.9537e-05 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 7.6584e-05 - acc: 1.0000
# Training time = 0:47:36.433693
# F-Score(Ordinary) = 0.63, Recall: 0.64, Precision: 0.62
# F-Score(lvc) = 0.539, Recall: 0.852, Precision: 0.394
# F-Score(ireflv) = 0.802, Recall: 0.867, Precision: 0.746
# F-Score(id) = 0.535, Recall: 0.461, Precision: 0.637
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 48)        705264      input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 24)        5640        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 192)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 96)           0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 288)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 24)           6936        concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 24)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            200         dropout_9[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0258 - acc: 0.9949 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0158 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0151 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0149 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0146 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 64s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0143 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 66s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0155 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0150 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0151 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0155 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0161 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0154 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0153 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0168 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0173 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0168 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0171 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0169 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0170 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0172 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0170 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0172 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0175 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0172 - acc: 0.9969 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 8.3032e-05 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 3.6396e-07 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 5.0543e-05 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 1.2294e-07 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 1.2002e-07 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 5.0274e-05 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 1.1922e-07 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 1.9863e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 4.9279e-05 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 4.9049e-05 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 4.8819e-05 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 9.6846e-05 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 4.8157e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 4.7927e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 4.7715e-05 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 4.7485e-05 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 9.4160e-05 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 4.6846e-05 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 4.6618e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 4.6388e-05 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 4.6189e-05 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 9.1581e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 4.5526e-05 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 4.5332e-05 - acc: 1.0000
# Training time = 0:47:12.613419
# F-Score(Ordinary) = 0.678, Recall: 0.787, Precision: 0.595
# F-Score(lvc) = 0.537, Recall: 0.69, Precision: 0.439
# F-Score(ireflv) = 0.78, Recall: 0.807, Precision: 0.754
# F-Score(id) = 0.673, Recall: 0.8, Precision: 0.58
********************
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 48)        705264      input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 24)        5640        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 192)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 96)           0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 288)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 24)           6936        concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 24)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            200         dropout_10[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0, 7.0: 0.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0252 - acc: 0.9950 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 62s - loss: 0.0156 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0151 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0147 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 65s - loss: 0.0146 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 66s - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 64s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0141 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0143 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0143 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 64s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 66s - loss: 0.0143 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0142 - acc: 0.9975 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 65s - loss: 0.0144 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 66s - loss: 0.0145 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0148 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0146 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0148 - acc: 0.9974 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0154 - acc: 0.9973 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0158 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0164 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0156 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0160 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0161 - acc: 0.9972 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0167 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0168 - acc: 0.9971 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0166 - acc: 0.9970 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 1.8159e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 2.7804e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 1.6501e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 1.0916e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 1.6215e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 1.6042e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 3.6787e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 2.0660e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 2.5432e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 3.4868e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 1.9591e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 4.8690e-05 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 4.8482e-05 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 1.9148e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 4.7482e-05 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.8753e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 4.6570e-05 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.3816e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 9.1188e-05 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.6919e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 1.7631e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 2.1649e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 1.2789e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.6815e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.2417e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.4397e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 3.1668e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.5464e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 1.8939e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.8493e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.0883e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 3.6017e-05 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 7.1250e-05 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.4360e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.0195e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.9919e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 9.7418e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 6.3986e-05 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 6.3209e-05 - acc: 1.0000
# Training time = 0:47:27.790766
# F-Score(Ordinary) = 0.69, Recall: 0.794, Precision: 0.611
# F-Score(lvc) = 0.586, Recall: 0.949, Precision: 0.424
# F-Score(ireflv) = 0.826, Recall: 0.938, Precision: 0.738
# F-Score(id) = 0.628, Recall: 0.635, Precision: 0.622
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 48)        705264      input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 24)        5640        input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 192)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 96)           0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 288)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 24)           6936        concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 24)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            200         dropout_11[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0313 - acc: 0.9949 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 62s - loss: 0.0171 - acc: 0.9970 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 62s - loss: 0.0164 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0161 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 66s - loss: 0.0159 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0158 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0162 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 66s - loss: 0.0161 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0163 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0162 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0159 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0161 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0163 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0164 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0167 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0179 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0169 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0167 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0166 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0172 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0168 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 64s - loss: 0.0169 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 66s - loss: 0.0167 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0171 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0176 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 65s - loss: 0.0176 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0014 - acc: 0.9999
Epoch 2/40
 - 7s - loss: 7.1684e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 9.9120e-04 - acc: 0.9999
Epoch 4/40
 - 7s - loss: 0.0014 - acc: 0.9999
Epoch 5/40
 - 7s - loss: 8.6399e-04 - acc: 0.9999
Epoch 6/40
 - 7s - loss: 9.5304e-04 - acc: 0.9999
Epoch 7/40
 - 7s - loss: 4.2281e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0013 - acc: 0.9999
Epoch 9/40
 - 7s - loss: 6.6287e-04 - acc: 0.9999
Epoch 10/40
 - 7s - loss: 5.9853e-04 - acc: 0.9999
Epoch 11/40
 - 7s - loss: 5.3937e-04 - acc: 0.9999
Epoch 12/40
 - 7s - loss: 2.3219e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 4.6631e-04 - acc: 0.9999
Epoch 14/40
 - 7s - loss: 4.4114e-04 - acc: 0.9999
Epoch 15/40
 - 7s - loss: 2.0800e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 3.4260e-04 - acc: 0.9999
Epoch 17/40
 - 7s - loss: 2.4144e-04 - acc: 0.9999
Epoch 18/40
 - 7s - loss: 1.3192e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.3591e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.0343e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 5.0808e-05 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 3.9191e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 2.8812e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 2.3621e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.3735e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 4.0928e-06 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 4.2107e-06 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 2.8401e-06 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 2.2207e-06 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 1.4105e-06 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 7.7893e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 5.8815e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 4.6081e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 4.1042e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 3.7591e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 3.1350e-07 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 2.8457e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 2.6526e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.7418e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.5501e-07 - acc: 1.0000
# Training time = 0:47:23.314201
# F-Score(Ordinary) = 0.725, Recall: 0.828, Precision: 0.644
# F-Score(lvc) = 0.621, Recall: 0.887, Precision: 0.477
# F-Score(ireflv) = 0.827, Recall: 0.852, Precision: 0.803
# F-Score(id) = 0.691, Recall: 0.762, Precision: 0.632
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 48)        705264      input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 24)        5640        input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 192)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 96)           0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 288)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 24)           6936        concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 24)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            200         dropout_12[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0315 - acc: 0.9947 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0171 - acc: 0.9970 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 64s - loss: 0.0163 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0159 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0158 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0158 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0155 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0164 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0159 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 65s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 66s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0161 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0166 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0170 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0182 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0177 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0182 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0176 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0177 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0182 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0186 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0185 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0181 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0191 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0183 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0188 - acc: 0.9970 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 2.7015e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 4.4111e-07 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 1.7887e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 2.4255e-07 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 2.3894e-07 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 8.8854e-05 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 1.7614e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 8.7572e-05 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 8.7112e-05 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 8.6651e-05 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 8.6190e-05 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 8.5730e-05 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 8.5269e-05 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 8.4809e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 8.4348e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 8.3894e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.6617e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 8.2548e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 8.2144e-05 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 8.1683e-05 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.6178e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
# Training time = 0:47:47.610157
# F-Score(Ordinary) = 0.654, Recall: 0.718, Precision: 0.6
# F-Score(lvc) = 0.589, Recall: 0.892, Precision: 0.439
# F-Score(ireflv) = 0.818, Recall: 0.825, Precision: 0.811
# F-Score(id) = 0.583, Recall: 0.59, Precision: 0.575
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 48)        705264      input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 24)        5640        input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 192)          0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 96)           0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 288)          0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 24)           6936        concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 24)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            200         dropout_13[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0318 - acc: 0.9949 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 62s - loss: 0.0171 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 62s - loss: 0.0164 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0160 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0155 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0155 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0154 - acc: 0.9976 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0160 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0160 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0161 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0172 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0171 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0183 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0182 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0181 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0184 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0184 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 61s - loss: 0.0185 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 61s - loss: 0.0186 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 2.2919e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 6.4556e-07 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 9.1930e-05 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 2.4332e-07 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 9.1328e-05 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 2.3851e-07 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.3843e-07 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 9.0865e-05 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 1.8013e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 1.7853e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 8.8739e-05 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 8.8288e-05 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.7496e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.7323e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.7160e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 8.5354e-05 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 8.4893e-05 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 8.4433e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.6725e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 8.3074e-05 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 8.2625e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 8.2170e-05 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
# Training time = 0:46:41.162226
# F-Score(Ordinary) = 0.701, Recall: 0.838, Precision: 0.602
# F-Score(lvc) = 0.602, Recall: 0.922, Precision: 0.447
# F-Score(ireflv) = 0.8, Recall: 0.852, Precision: 0.754
# F-Score(id) = 0.673, Recall: 0.772, Precision: 0.596
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 48)        705264      input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 24)        5640        input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 192)          0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 96)           0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 288)          0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 24)           6936        concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 24)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            200         dropout_14[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0322 - acc: 0.9946 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0171 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0163 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0159 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0154 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0152 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0152 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0159 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0161 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0159 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0161 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0161 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0163 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0167 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0164 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0178 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0166 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0171 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0168 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0166 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0167 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0167 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0170 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0170 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 7.9298e-05 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 3.7400e-05 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 2.9448e-07 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 9.7601e-05 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 2.4033e-07 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 2.3849e-07 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.3843e-07 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 9.7136e-05 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 9.6675e-05 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 9.6214e-05 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 9.5754e-05 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 9.5294e-05 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.8898e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 9.3925e-05 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 9.3464e-05 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 1.8531e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 9.2107e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 9.1647e-05 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 9.1186e-05 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 9.0725e-05 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 9.0266e-05 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 8.9805e-05 - acc: 1.0000
# Training time = 0:47:28.214436
# F-Score(Ordinary) = 0.686, Recall: 0.794, Precision: 0.604
# F-Score(lvc) = 0.494, Recall: 0.563, Precision: 0.439
# F-Score(ireflv) = 0.814, Recall: 0.885, Precision: 0.754
# F-Score(id) = 0.73, Recall: 0.895, Precision: 0.617
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 48)        705264      input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 24)        5640        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 192)          0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 96)           0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 288)          0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 24)           6936        concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 24)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            200         dropout_15[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0319 - acc: 0.9948 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0172 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 64s - loss: 0.0165 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0161 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0158 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0158 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 65s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 64s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0159 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0160 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0161 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0164 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0175 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0170 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0181 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0177 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0179 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0182 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0183 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0183 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0185 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0182 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0182 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0182 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0013 - acc: 0.9999
Epoch 2/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0010 - acc: 0.9999
Epoch 4/40
 - 7s - loss: 0.0011 - acc: 0.9999
Epoch 5/40
 - 7s - loss: 0.0013 - acc: 0.9999
Epoch 6/40
 - 7s - loss: 8.8425e-04 - acc: 0.9999
Epoch 7/40
 - 7s - loss: 0.0015 - acc: 0.9999
Epoch 8/40
 - 7s - loss: 7.7402e-04 - acc: 0.9999
Epoch 9/40
 - 7s - loss: 7.8190e-04 - acc: 0.9999
Epoch 10/40
 - 7s - loss: 0.0012 - acc: 0.9999
Epoch 11/40
 - 7s - loss: 6.1824e-04 - acc: 0.9999
Epoch 12/40
 - 7s - loss: 8.1315e-04 - acc: 0.9999
Epoch 13/40
 - 7s - loss: 3.0091e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 6.8548e-04 - acc: 0.9999
Epoch 15/40
 - 7s - loss: 5.7592e-04 - acc: 0.9999
Epoch 16/40
 - 7s - loss: 5.0711e-04 - acc: 0.9999
Epoch 17/40
 - 7s - loss: 3.9339e-04 - acc: 0.9999
Epoch 18/40
 - 7s - loss: 4.0311e-04 - acc: 0.9999
Epoch 19/40
 - 7s - loss: 2.9955e-04 - acc: 0.9999
Epoch 20/40
 - 7s - loss: 2.0015e-04 - acc: 0.9999
Epoch 21/40
 - 7s - loss: 1.2217e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 9.1549e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 8.8457e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 6.9362e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 4.9325e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 5.8543e-05 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.9214e-05 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 1.4564e-05 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.0007e-05 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 6.2219e-06 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 3.4962e-06 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 2.8292e-06 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.4712e-06 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.0469e-06 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 7.9956e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 6.7523e-07 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 3.8643e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 4.4091e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 3.7064e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 3.1609e-07 - acc: 1.0000
# Training time = 0:47:11.179836
# F-Score(Ordinary) = 0.685, Recall: 0.747, Precision: 0.633
# F-Score(lvc) = 0.582, Recall: 0.765, Precision: 0.47
# F-Score(ireflv) = 0.791, Recall: 0.75, Precision: 0.836
# F-Score(id) = 0.655, Recall: 0.752, Precision: 0.58
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 48)        705264      input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 24)        5640        input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 192)          0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 96)           0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 288)          0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 24)           6936        concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 24)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            200         dropout_16[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0318 - acc: 0.9949 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 62s - loss: 0.0171 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0164 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0161 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0159 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0161 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0159 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0158 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0159 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 64s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 66s - loss: 0.0160 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 64s - loss: 0.0160 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0162 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0164 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 65s - loss: 0.0166 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0168 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0169 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0180 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0176 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0176 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0178 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0178 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0186 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0183 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0183 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0186 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0186 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0191 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0186 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 7.7455e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 3.7155e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 1.8371e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 2.7267e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 9.0371e-05 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 4.4490e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 1.7558e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 1.7408e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 3.4306e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.6950e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 1.6781e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 8.3364e-05 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 4.0997e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 4.0119e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 2.3683e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.5612e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 4.5859e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 2.9942e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.4764e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 3.6195e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.1298e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 4.1631e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 2.0378e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 1.3402e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 6.6573e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 2.6140e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.9219e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 3.1272e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.8357e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 1.2059e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.7763e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 2.8823e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.6898e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 3.8183e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 1.0639e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 2.0764e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 2.0204e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 9.9024e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 3.3482e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 1.3862e-04 - acc: 1.0000
# Training time = 0:47:14.272917
# F-Score(Ordinary) = 0.629, Recall: 0.648, Precision: 0.611
# F-Score(lvc) = 0.539, Recall: 0.764, Precision: 0.417
# F-Score(ireflv) = 0.819, Recall: 0.886, Precision: 0.762
# F-Score(id) = 0.54, Recall: 0.484, Precision: 0.611
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 48)        705264      input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 24)        5640        input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 192)          0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 96)           0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 288)          0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 24)           6936        concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 24)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            200         dropout_17[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0322 - acc: 0.9947 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 62s - loss: 0.0171 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 62s - loss: 0.0163 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0160 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0158 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 64s - loss: 0.0155 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 66s - loss: 0.0155 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0155 - acc: 0.9976 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0161 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0163 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0166 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0160 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0160 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0162 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 65s - loss: 0.0177 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0166 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0169 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0168 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0169 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0169 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0169 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0180 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0179 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 64s - loss: 0.0183 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 65s - loss: 0.0188 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0183 - acc: 0.9970 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 1.9481e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 6.4713e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 9.1488e-05 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 6.2889e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 3.5259e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 5.1828e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 8.5435e-05 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 2.5341e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 4.1419e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.6330e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 1.6179e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 3.9691e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 2.3406e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 3.8307e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 7.5854e-05 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.5013e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 2.9493e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 5.0340e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 6.2471e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 5.3638e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 1.9693e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 3.8447e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.2545e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 1.8503e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 6.1063e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.2054e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.1884e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 1.7497e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 2.2788e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.7677e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 5.4563e-05 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.6080e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 2.6004e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 3.0134e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 9.8158e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.9147e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.3959e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.8106e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.3206e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 8.6355e-05 - acc: 1.0000
# Training time = 0:47:09.778207
# F-Score(Ordinary) = 0.687, Recall: 0.844, Precision: 0.579
# F-Score(lvc) = 0.464, Recall: 0.565, Precision: 0.394
# F-Score(ireflv) = 0.823, Recall: 0.894, Precision: 0.762
# F-Score(id) = 0.704, Recall: 0.964, Precision: 0.554
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 48)        705264      input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 24)        5640        input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 192)          0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 96)           0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 288)          0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 24)           6936        concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 24)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            200         dropout_18[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0321 - acc: 0.9947 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 62s - loss: 0.0171 - acc: 0.9970 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 62s - loss: 0.0164 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0161 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0159 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0158 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0155 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0164 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0163 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0164 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0165 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0169 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0170 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 66s - loss: 0.0169 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0173 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0171 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0171 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0174 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0185 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0173 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0182 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0171 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0172 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0173 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0175 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 1.2509e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 5.2694e-06 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 2.4248e-07 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 2.3936e-07 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 1.1512e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 1.5277e-05 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.3865e-07 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 2.3850e-07 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 2.3849e-07 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 2.3848e-07 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.1213e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.1167e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
# Training time = 0:46:55.552390
# F-Score(Ordinary) = 0.661, Recall: 0.769, Precision: 0.579
# F-Score(lvc) = 0.55, Recall: 0.912, Precision: 0.394
# F-Score(ireflv) = 0.786, Recall: 0.841, Precision: 0.738
# F-Score(id) = 0.628, Recall: 0.665, Precision: 0.596
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 48)        705264      input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 24)        5640        input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 192)          0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 96)           0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 288)          0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 24)           6936        concatenate_19[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 24)           0           dense_37[0][0]                   
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            200         dropout_19[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0318 - acc: 0.9946 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0173 - acc: 0.9970 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0166 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0162 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0159 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0158 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0158 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0160 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0159 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0160 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0162 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0162 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0167 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0177 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0163 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0167 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0173 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0174 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0174 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 66s - loss: 0.0174 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 67s - loss: 0.0180 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0184 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 2.2996e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 5.5611e-07 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 9.3388e-05 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 2.7687e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 2.3889e-07 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 9.1704e-05 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 1.8179e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 1.8003e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 8.9470e-05 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 8.9029e-05 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 8.8574e-05 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.7553e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 8.7210e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 8.6749e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 8.6315e-05 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.7104e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 8.5077e-05 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 8.4629e-05 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 8.4171e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 8.3711e-05 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.6584e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 8.2391e-05 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.6319e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 8.1102e-05 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 8.0683e-05 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 8.0222e-05 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 7.9763e-05 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
# Training time = 0:47:50.042323
# F-Score(Ordinary) = 0.675, Recall: 0.828, Precision: 0.57
# F-Score(lvc) = 0.564, Recall: 0.946, Precision: 0.402
# F-Score(ireflv) = 0.782, Recall: 0.854, Precision: 0.721
# F-Score(id) = 0.643, Recall: 0.738, Precision: 0.57
********************
********************
# XP = FR: overSampling favorisationCoeff = 2
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_39 (Embedding)        (None, 4, 48)        705264      input_39[0][0]                   
__________________________________________________________________________________________________
embedding_40 (Embedding)        (None, 4, 24)        5640        input_40[0][0]                   
__________________________________________________________________________________________________
flatten_39 (Flatten)            (None, 192)          0           embedding_39[0][0]               
__________________________________________________________________________________________________
flatten_40 (Flatten)            (None, 96)           0           embedding_40[0][0]               
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 288)          0           flatten_39[0][0]                 
                                                                 flatten_40[0][0]                 
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 24)           6936        concatenate_20[0][0]             
__________________________________________________________________________________________________
dropout_20 (Dropout)            (None, 24)           0           dense_39[0][0]                   
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 8)            200         dropout_20[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 2.0, 4.0: 2.0, 5.0: 2.0, 6.0: 2.0, 7.0: 2.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0317 - acc: 0.9947 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0172 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0166 - acc: 0.9972 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0162 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0159 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0158 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0157 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0156 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0153 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0154 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0155 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0184 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0169 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0156 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0159 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0157 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0160 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0160 - acc: 0.9975 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 28/40
 - 65s - loss: 0.0163 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 66s - loss: 0.0164 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0160 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 31/40
 - 66s - loss: 0.0163 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0166 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0170 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0172 - acc: 0.9974 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0172 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0185 - acc: 0.9973 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0195 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0191 - acc: 0.9970 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0192 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0186 - acc: 0.9971 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 7.3442e-07 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 9.3277e-05 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 1.1498e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 2.3865e-07 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 9.1820e-05 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 2.4028e-07 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.3906e-07 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 4.5385e-05 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.1848e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 2.3875e-07 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 2.3874e-07 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 2.3871e-07 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 2.3844e-07 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.1305e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.1259e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.3842e-07 - acc: 1.0000
# Training time = 0:47:37.270106
# F-Score(Ordinary) = 0.704, Recall: 0.902, Precision: 0.577
# F-Score(lvc) = 0.541, Recall: 0.943, Precision: 0.379
# F-Score(ireflv) = 0.809, Recall: 0.883, Precision: 0.746
# F-Score(id) = 0.724, Recall: 0.9, Precision: 0.606
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_41 (Embedding)        (None, 4, 48)        705264      input_41[0][0]                   
__________________________________________________________________________________________________
embedding_42 (Embedding)        (None, 4, 24)        5640        input_42[0][0]                   
__________________________________________________________________________________________________
flatten_41 (Flatten)            (None, 192)          0           embedding_41[0][0]               
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 96)           0           embedding_42[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 288)          0           flatten_41[0][0]                 
                                                                 flatten_42[0][0]                 
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 24)           6936        concatenate_21[0][0]             
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 24)           0           dense_41[0][0]                   
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 8)            200         dropout_21[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0478 - acc: 0.9939 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0191 - acc: 0.9970 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0184 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0181 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0179 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0189 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0190 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0177 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0177 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0185 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0184 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0185 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0186 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0189 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0192 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0196 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0196 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0198 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0198 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0204 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0194 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0194 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0196 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0202 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0204 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0201 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0201 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0205 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0213 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0202 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0227 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0205 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0082 - acc: 0.9996
Epoch 2/40
 - 7s - loss: 0.0044 - acc: 0.9997
Epoch 3/40
 - 7s - loss: 0.0037 - acc: 0.9997
Epoch 4/40
 - 7s - loss: 0.0022 - acc: 0.9998
Epoch 5/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 5.2931e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 1.4979e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 8.4460e-05 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 3.2694e-05 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.2541e-05 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 5.1262e-06 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 2.4824e-06 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 1.4989e-06 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 9.8378e-07 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 7.4064e-07 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 6.6735e-07 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 6.3734e-07 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 6.1315e-07 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 6.1115e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 6.0397e-07 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 6.0235e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 6.0039e-07 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 6.0030e-07 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 5.9920e-07 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 5.9869e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 5.9823e-07 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 5.9794e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 5.9774e-07 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 5.9770e-07 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 5.9758e-07 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 5.9712e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 5.9733e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 5.9729e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 5.9724e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 5.9716e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 5.9696e-07 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 5.9694e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 5.9677e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 5.9690e-07 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 5.9670e-07 - acc: 1.0000
# Training time = 0:47:11.019950
# F-Score(Ordinary) = 0.706, Recall: 0.815, Precision: 0.622
# F-Score(lvc) = 0.56, Recall: 0.709, Precision: 0.462
# F-Score(ireflv) = 0.834, Recall: 0.867, Precision: 0.803
# F-Score(id) = 0.704, Recall: 0.831, Precision: 0.611
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 4, 48)        705264      input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 4, 24)        5640        input_44[0][0]                   
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 192)          0           embedding_43[0][0]               
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 96)           0           embedding_44[0][0]               
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 288)          0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 24)           6936        concatenate_22[0][0]             
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 24)           0           dense_43[0][0]                   
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 8)            200         dropout_22[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0483 - acc: 0.9938 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0193 - acc: 0.9970 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0184 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0181 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0178 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0173 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0173 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0179 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0179 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0186 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0188 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0191 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0193 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0194 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0199 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0196 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0197 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0194 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0199 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0202 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0201 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0197 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0225 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0223 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0204 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0200 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 66s - loss: 0.0219 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0204 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0205 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0217 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 8.0785e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 1.9480e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 1.9388e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 7.6289e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 7.4868e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 3.6934e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 7.2589e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 5.1847e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 6.7782e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 8.2652e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 6.4556e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 3.1792e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 4.5210e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.4930e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 4.4014e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 2.8866e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 8.4192e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 6.8055e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 5.2981e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 3.8766e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.5360e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 6.1635e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.2108e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 7.0283e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 5.6366e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 4.3582e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 3.1680e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 5.0887e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 3.9122e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 4.6919e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 2.7108e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 8.9029e-05 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 4.2771e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.6521e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.4012e-04 - acc: 1.0000
# Training time = 0:47:41.650587
# F-Score(Ordinary) = 0.716, Recall: 0.847, Precision: 0.62
# F-Score(lvc) = 0.599, Recall: 0.908, Precision: 0.447
# F-Score(ireflv) = 0.808, Recall: 0.822, Precision: 0.795
# F-Score(id) = 0.712, Recall: 0.833, Precision: 0.622
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_45 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 4, 48)        705264      input_45[0][0]                   
__________________________________________________________________________________________________
embedding_46 (Embedding)        (None, 4, 24)        5640        input_46[0][0]                   
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 192)          0           embedding_45[0][0]               
__________________________________________________________________________________________________
flatten_46 (Flatten)            (None, 96)           0           embedding_46[0][0]               
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 288)          0           flatten_45[0][0]                 
                                                                 flatten_46[0][0]                 
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 24)           6936        concatenate_23[0][0]             
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 24)           0           dense_45[0][0]                   
__________________________________________________________________________________________________
dense_46 (Dense)                (None, 8)            200         dropout_23[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0477 - acc: 0.9942 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0193 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0185 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0180 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0173 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0172 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0171 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0171 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0171 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0170 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0171 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0172 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0172 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0175 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 66s - loss: 0.0173 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0174 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0176 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0179 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0185 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0184 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0189 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0193 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0199 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0206 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0222 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0209 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0211 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0210 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0213 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0218 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0219 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0227 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0222 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0222 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0223 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0227 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0223 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0021 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 8.7751e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 8.7750e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 8.7750e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 8.5579e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 2.9290e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 8.7750e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 8.5032e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 8.7750e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 2.9290e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 0.0029 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 8.4297e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 8.3977e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 8.7750e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 5.8520e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.9290e-04 - acc: 1.0000
# Training time = 0:47:33.317090
# F-Score(Ordinary) = 0.647, Recall: 0.693, Precision: 0.606
# F-Score(lvc) = 0.368, Recall: 0.34, Precision: 0.402
# F-Score(ireflv) = 0.809, Recall: 0.861, Precision: 0.762
# F-Score(id) = 0.737, Recall: 0.929, Precision: 0.611
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_47 (Embedding)        (None, 4, 48)        705264      input_47[0][0]                   
__________________________________________________________________________________________________
embedding_48 (Embedding)        (None, 4, 24)        5640        input_48[0][0]                   
__________________________________________________________________________________________________
flatten_47 (Flatten)            (None, 192)          0           embedding_47[0][0]               
__________________________________________________________________________________________________
flatten_48 (Flatten)            (None, 96)           0           embedding_48[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 288)          0           flatten_47[0][0]                 
                                                                 flatten_48[0][0]                 
__________________________________________________________________________________________________
dense_47 (Dense)                (None, 24)           6936        concatenate_24[0][0]             
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 24)           0           dense_47[0][0]                   
__________________________________________________________________________________________________
dense_48 (Dense)                (None, 8)            200         dropout_24[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0498 - acc: 0.9939 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0192 - acc: 0.9970 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0185 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0179 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0177 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0182 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0182 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0184 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0183 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0183 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0184 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0240 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0190 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0193 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0203 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0200 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0197 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0192 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0208 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0202 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0213 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0200 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0209 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0206 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0212 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0212 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0221 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0214 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0222 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0214 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0214 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 2.4339e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 6.2866e-07 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 4.8190e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 5.9617e-07 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 5.9606e-07 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 2.3955e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 4.7510e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 2.3626e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 4.6855e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 2.3304e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 2.3189e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 2.3074e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 4.5763e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 2.2760e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.2646e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 2.2531e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 2.2416e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 2.2301e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 4.4198e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 2.1986e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.1871e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
# Training time = 0:47:24.164514
# F-Score(Ordinary) = 0.654, Recall: 0.744, Precision: 0.584
# F-Score(lvc) = 0.579, Recall: 0.877, Precision: 0.432
# F-Score(ireflv) = 0.758, Recall: 0.856, Precision: 0.68
# F-Score(id) = 0.586, Recall: 0.593, Precision: 0.58
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_49 (Embedding)        (None, 4, 48)        705264      input_49[0][0]                   
__________________________________________________________________________________________________
embedding_50 (Embedding)        (None, 4, 24)        5640        input_50[0][0]                   
__________________________________________________________________________________________________
flatten_49 (Flatten)            (None, 192)          0           embedding_49[0][0]               
__________________________________________________________________________________________________
flatten_50 (Flatten)            (None, 96)           0           embedding_50[0][0]               
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 288)          0           flatten_49[0][0]                 
                                                                 flatten_50[0][0]                 
__________________________________________________________________________________________________
dense_49 (Dense)                (None, 24)           6936        concatenate_25[0][0]             
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 24)           0           dense_49[0][0]                   
__________________________________________________________________________________________________
dense_50 (Dense)                (None, 8)            200         dropout_25[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0484 - acc: 0.9941 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0195 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0187 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0183 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0178 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0179 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0180 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0186 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0187 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0189 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0193 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0202 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0194 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0200 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0203 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0205 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0207 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0212 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0209 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0212 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0211 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0217 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0220 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0220 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0226 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0224 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0231 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0033 - acc: 0.9999
Epoch 2/40
 - 7s - loss: 0.0026 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0027 - acc: 0.9999
Epoch 4/40
 - 7s - loss: 0.0027 - acc: 0.9999
Epoch 5/40
 - 7s - loss: 0.0035 - acc: 0.9999
Epoch 6/40
 - 7s - loss: 0.0023 - acc: 0.9999
Epoch 7/40
 - 7s - loss: 0.0038 - acc: 0.9999
Epoch 8/40
 - 7s - loss: 0.0020 - acc: 0.9999
Epoch 9/40
 - 7s - loss: 0.0020 - acc: 0.9999
Epoch 10/40
 - 7s - loss: 0.0031 - acc: 0.9999
Epoch 11/40
 - 7s - loss: 0.0016 - acc: 0.9999
Epoch 12/40
 - 7s - loss: 0.0022 - acc: 0.9999
Epoch 13/40
 - 7s - loss: 8.0006e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0018 - acc: 0.9999
Epoch 15/40
 - 7s - loss: 0.0016 - acc: 0.9999
Epoch 16/40
 - 7s - loss: 0.0014 - acc: 0.9999
Epoch 17/40
 - 7s - loss: 0.0011 - acc: 0.9999
Epoch 18/40
 - 7s - loss: 0.0011 - acc: 0.9999
Epoch 19/40
 - 7s - loss: 8.5049e-04 - acc: 0.9999
Epoch 20/40
 - 7s - loss: 5.8002e-04 - acc: 0.9999
Epoch 21/40
 - 7s - loss: 3.6051e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.7478e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 2.7126e-04 - acc: 0.9999
Epoch 24/40
 - 7s - loss: 2.1724e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.5737e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.9011e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 9.5997e-05 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 4.8109e-05 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 3.3150e-05 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.0586e-05 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.1491e-05 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 9.2593e-06 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 4.7132e-06 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 3.2886e-06 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.4567e-06 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 2.0337e-06 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.0789e-06 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.2459e-06 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.0115e-06 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 8.3331e-07 - acc: 1.0000
# Training time = 0:47:12.522577
# F-Score(Ordinary) = 0.696, Recall: 0.79, Precision: 0.622
# F-Score(lvc) = 0.557, Recall: 0.738, Precision: 0.447
# F-Score(ireflv) = 0.815, Recall: 0.856, Precision: 0.779
# F-Score(id) = 0.686, Recall: 0.756, Precision: 0.627
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_51 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_51 (Embedding)        (None, 4, 48)        705264      input_51[0][0]                   
__________________________________________________________________________________________________
embedding_52 (Embedding)        (None, 4, 24)        5640        input_52[0][0]                   
__________________________________________________________________________________________________
flatten_51 (Flatten)            (None, 192)          0           embedding_51[0][0]               
__________________________________________________________________________________________________
flatten_52 (Flatten)            (None, 96)           0           embedding_52[0][0]               
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 288)          0           flatten_51[0][0]                 
                                                                 flatten_52[0][0]                 
__________________________________________________________________________________________________
dense_51 (Dense)                (None, 24)           6936        concatenate_26[0][0]             
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 24)           0           dense_51[0][0]                   
__________________________________________________________________________________________________
dense_52 (Dense)                (None, 8)            200         dropout_26[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 64s - loss: 0.0482 - acc: 0.9940 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 64s - loss: 0.0193 - acc: 0.9970 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 64s - loss: 0.0185 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 64s - loss: 0.0182 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 64s - loss: 0.0179 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 64s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 64s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 64s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 64s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 64s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 64s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 64s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 64s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 64s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 64s - loss: 0.0180 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 64s - loss: 0.0177 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 64s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 64s - loss: 0.0179 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 64s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 64s - loss: 0.0180 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 64s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 64s - loss: 0.0182 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 64s - loss: 0.0183 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 64s - loss: 0.0192 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 64s - loss: 0.0196 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 64s - loss: 0.0199 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 64s - loss: 0.0201 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 64s - loss: 0.0208 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 64s - loss: 0.0211 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 64s - loss: 0.0215 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 64s - loss: 0.0227 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 64s - loss: 0.0221 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 64s - loss: 0.0232 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 64s - loss: 0.0241 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 64s - loss: 0.0237 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 64s - loss: 0.0242 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 64s - loss: 0.0242 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 64s - loss: 0.0244 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 64s - loss: 0.0260 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 64s - loss: 0.0250 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0060 - acc: 0.9999
Epoch 2/40
 - 7s - loss: 0.0040 - acc: 0.9999
Epoch 3/40
 - 7s - loss: 0.0026 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.0050 - acc: 0.9999
Epoch 5/40
 - 7s - loss: 0.0047 - acc: 0.9999
Epoch 6/40
 - 7s - loss: 0.0032 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 0.0050 - acc: 0.9999
Epoch 8/40
 - 7s - loss: 0.0026 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.0034 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.0041 - acc: 0.9999
Epoch 11/40
 - 7s - loss: 0.0032 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.0029 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0038 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0043 - acc: 0.9999
Epoch 15/40
 - 7s - loss: 0.0044 - acc: 0.9999
Epoch 16/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 0.0063 - acc: 0.9999
Epoch 18/40
 - 7s - loss: 0.0050 - acc: 0.9999
Epoch 19/40
 - 7s - loss: 0.0034 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 0.0041 - acc: 0.9999
Epoch 21/40
 - 7s - loss: 0.0046 - acc: 0.9999
Epoch 22/40
 - 7s - loss: 0.0033 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 0.0031 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 0.0046 - acc: 0.9999
Epoch 25/40
 - 7s - loss: 0.0034 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 0.0056 - acc: 0.9999
Epoch 27/40
 - 7s - loss: 0.0035 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 0.0058 - acc: 0.9999
Epoch 29/40
 - 7s - loss: 0.0041 - acc: 0.9999
Epoch 30/40
 - 7s - loss: 0.0037 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 0.0034 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 0.0041 - acc: 0.9999
Epoch 33/40
 - 7s - loss: 0.0030 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 0.0049 - acc: 0.9999
Epoch 35/40
 - 7s - loss: 0.0038 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 0.0029 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 0.0049 - acc: 0.9999
Epoch 38/40
 - 7s - loss: 0.0022 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 0.0061 - acc: 0.9999
Epoch 40/40
 - 7s - loss: 0.0058 - acc: 0.9999
# Training time = 0:48:22.777100
# F-Score(Ordinary) = 0.698, Recall: 0.844, Precision: 0.595
# F-Score(lvc) = 0.51, Recall: 0.697, Precision: 0.402
# F-Score(ireflv) = 0.811, Recall: 0.9, Precision: 0.738
# F-Score(id) = 0.711, Recall: 0.849, Precision: 0.611
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_53 (Embedding)        (None, 4, 48)        705264      input_53[0][0]                   
__________________________________________________________________________________________________
embedding_54 (Embedding)        (None, 4, 24)        5640        input_54[0][0]                   
__________________________________________________________________________________________________
flatten_53 (Flatten)            (None, 192)          0           embedding_53[0][0]               
__________________________________________________________________________________________________
flatten_54 (Flatten)            (None, 96)           0           embedding_54[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 288)          0           flatten_53[0][0]                 
                                                                 flatten_54[0][0]                 
__________________________________________________________________________________________________
dense_53 (Dense)                (None, 24)           6936        concatenate_27[0][0]             
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 24)           0           dense_53[0][0]                   
__________________________________________________________________________________________________
dense_54 (Dense)                (None, 8)            200         dropout_27[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0493 - acc: 0.9939 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0195 - acc: 0.9970 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0187 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0181 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0178 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0177 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0177 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0183 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0187 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0196 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0201 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0201 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0206 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0204 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0210 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0212 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0237 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0223 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0223 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0223 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0234 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0225 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0226 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0229 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0231 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0232 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 3.9429e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 7.7545e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 3.8227e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 5.6564e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 9.2408e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 9.0218e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.7827e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 7.0136e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 8.5639e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 6.4446e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 4.7402e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 4.4696e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 2.9349e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 7.1545e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 9.6696e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 4.0317e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 1.3280e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 3.9120e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 3.8267e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.2627e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 3.7111e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 7.1530e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 4.6003e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 7.7268e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 4.2316e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 4.0914e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.9993e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 2.9252e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 2.8387e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.7503e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 4.3994e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 2.5379e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 5.6098e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 3.7827e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.8791e-04 - acc: 1.0000
# Training time = 0:47:16.368722
# F-Score(Ordinary) = 0.675, Recall: 0.784, Precision: 0.593
# F-Score(lvc) = 0.573, Recall: 0.797, Precision: 0.447
# F-Score(ireflv) = 0.815, Recall: 0.856, Precision: 0.779
# F-Score(id) = 0.624, Recall: 0.706, Precision: 0.56
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_55 (Embedding)        (None, 4, 48)        705264      input_55[0][0]                   
__________________________________________________________________________________________________
embedding_56 (Embedding)        (None, 4, 24)        5640        input_56[0][0]                   
__________________________________________________________________________________________________
flatten_55 (Flatten)            (None, 192)          0           embedding_55[0][0]               
__________________________________________________________________________________________________
flatten_56 (Flatten)            (None, 96)           0           embedding_56[0][0]               
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 288)          0           flatten_55[0][0]                 
                                                                 flatten_56[0][0]                 
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 24)           6936        concatenate_28[0][0]             
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 24)           0           dense_55[0][0]                   
__________________________________________________________________________________________________
dense_56 (Dense)                (None, 8)            200         dropout_28[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0498 - acc: 0.9939 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0194 - acc: 0.9970 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0185 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0180 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0178 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0172 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0187 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0197 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0179 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0181 - acc: 0.9976 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0182 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0186 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0188 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0198 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0203 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0210 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0204 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0205 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0202 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0202 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0202 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0208 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0208 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0208 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0217 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0243 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0260 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0230 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 8.5108e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 6.0092e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 3.9623e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 5.8614e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 3.8584e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 7.5929e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 5.6003e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 3.4566e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 5.1079e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 6.6737e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 3.2830e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 3.2420e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 6.3611e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 3.1309e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 3.0891e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 3.0502e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 4.4991e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 4.4108e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 1.4547e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 4.2878e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 2.8089e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 5.4925e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.5697e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 9.8994e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 3.5835e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.3438e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 3.4412e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 4.4461e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 2.1731e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 4.2246e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 6.0598e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 4.8246e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 4.6222e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.7933e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.7514e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.5471e-04 - acc: 1.0000
# Training time = 0:47:17.301678
# F-Score(Ordinary) = 0.717, Recall: 0.801, Precision: 0.649
# F-Score(lvc) = 0.644, Recall: 0.882, Precision: 0.508
# F-Score(ireflv) = 0.841, Recall: 0.883, Precision: 0.803
# F-Score(id) = 0.674, Recall: 0.709, Precision: 0.642
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_57 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_58 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_57 (Embedding)        (None, 4, 48)        705264      input_57[0][0]                   
__________________________________________________________________________________________________
embedding_58 (Embedding)        (None, 4, 24)        5640        input_58[0][0]                   
__________________________________________________________________________________________________
flatten_57 (Flatten)            (None, 192)          0           embedding_57[0][0]               
__________________________________________________________________________________________________
flatten_58 (Flatten)            (None, 96)           0           embedding_58[0][0]               
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 288)          0           flatten_57[0][0]                 
                                                                 flatten_58[0][0]                 
__________________________________________________________________________________________________
dense_57 (Dense)                (None, 24)           6936        concatenate_29[0][0]             
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 24)           0           dense_57[0][0]                   
__________________________________________________________________________________________________
dense_58 (Dense)                (None, 8)            200         dropout_29[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0472 - acc: 0.9940 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0193 - acc: 0.9970 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0186 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0181 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0178 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0177 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 64s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 64s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0174 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 64s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 64s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 64s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 64s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 64s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0177 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0179 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0179 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0184 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0186 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0192 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0188 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0194 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0196 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0199 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0199 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0204 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0210 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0211 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0212 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0220 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0232 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0214 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0215 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0228 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0233 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0242 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 3.9277e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 5.8094e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 7.3316e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 3.6182e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 7.1215e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 1.7645e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 1.7531e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 5.1812e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 3.4126e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 6.6949e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 9.7522e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 4.7629e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 4.6756e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 3.0688e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.5214e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 7.4295e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 7.1933e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.4166e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 4.1755e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 8.0797e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 6.5155e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 3.8156e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 3.7318e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.4390e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.2073e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 5.8592e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 3.4108e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 4.4172e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 2.1590e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.0667e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 5.1531e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 2.9857e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 3.8654e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 4.6276e-04 - acc: 1.0000
# Training time = 0:47:39.561594
# F-Score(Ordinary) = 0.641, Recall: 0.755, Precision: 0.557
# F-Score(lvc) = 0.446, Recall: 0.543, Precision: 0.379
# F-Score(ireflv) = 0.805, Recall: 0.899, Precision: 0.73
# F-Score(id) = 0.627, Recall: 0.748, Precision: 0.539
********************
********************
# XP = FR: overSampling favorisationCoeff = 5
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_59 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_60 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_59 (Embedding)        (None, 4, 48)        705264      input_59[0][0]                   
__________________________________________________________________________________________________
embedding_60 (Embedding)        (None, 4, 24)        5640        input_60[0][0]                   
__________________________________________________________________________________________________
flatten_59 (Flatten)            (None, 192)          0           embedding_59[0][0]               
__________________________________________________________________________________________________
flatten_60 (Flatten)            (None, 96)           0           embedding_60[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 288)          0           flatten_59[0][0]                 
                                                                 flatten_60[0][0]                 
__________________________________________________________________________________________________
dense_59 (Dense)                (None, 24)           6936        concatenate_30[0][0]             
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 24)           0           dense_59[0][0]                   
__________________________________________________________________________________________________
dense_60 (Dense)                (None, 8)            200         dropout_30[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 5.0, 4.0: 5.0, 5.0: 5.0, 6.0: 5.0, 7.0: 5.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0476 - acc: 0.9939 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0193 - acc: 0.9970 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0186 - acc: 0.9971 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0180 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0178 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0176 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0175 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0174 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0173 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0173 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0173 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0175 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0176 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0178 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0177 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0181 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0189 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0184 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0184 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0189 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0190 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0191 - acc: 0.9975 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0197 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0196 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0204 - acc: 0.9974 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0217 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0211 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0219 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0227 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 35/40
 - 66s - loss: 0.0227 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0238 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0231 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0230 - acc: 0.9973 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0254 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0242 - acc: 0.9972 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 7.2679e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 8.5692e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 6.3275e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 6.2431e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 8.1835e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 2.0276e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 2.0188e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 5.9793e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 5.9002e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 3.8905e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 1.9322e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 3.8243e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 7.3234e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.7660e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 5.0770e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 6.6391e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 6.4780e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 3.1876e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 3.1482e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 4.6435e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 7.5472e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 7.3301e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 2.8755e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 5.6430e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 6.8508e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 4.0063e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 5.2032e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 5.0527e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 3.7003e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.2191e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 5.9605e-07 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 9.3824e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.2721e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 5.5079e-04 - acc: 1.0000
# Training time = 0:47:10.731345
# F-Score(Ordinary) = 0.705, Recall: 0.791, Precision: 0.635
# F-Score(lvc) = 0.624, Recall: 0.9, Precision: 0.477
# F-Score(ireflv) = 0.798, Recall: 0.802, Precision: 0.795
# F-Score(id) = 0.661, Recall: 0.713, Precision: 0.617
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_61 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_62 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_61 (Embedding)        (None, 4, 48)        705264      input_61[0][0]                   
__________________________________________________________________________________________________
embedding_62 (Embedding)        (None, 4, 24)        5640        input_62[0][0]                   
__________________________________________________________________________________________________
flatten_61 (Flatten)            (None, 192)          0           embedding_61[0][0]               
__________________________________________________________________________________________________
flatten_62 (Flatten)            (None, 96)           0           embedding_62[0][0]               
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 288)          0           flatten_61[0][0]                 
                                                                 flatten_62[0][0]                 
__________________________________________________________________________________________________
dense_61 (Dense)                (None, 24)           6936        concatenate_31[0][0]             
__________________________________________________________________________________________________
dropout_31 (Dropout)            (None, 24)           0           dense_61[0][0]                   
__________________________________________________________________________________________________
dense_62 (Dense)                (None, 8)            200         dropout_31[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0722 - acc: 0.9930 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 62s - loss: 0.0211 - acc: 0.9970 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 62s - loss: 0.0204 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0202 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0196 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0193 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0197 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0197 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0199 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0204 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0210 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0210 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0215 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0219 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0220 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0224 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0225 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0239 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0236 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0242 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0247 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0246 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0962 - acc: 0.9968 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0261 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0272 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0273 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0275 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0272 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0296 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0289 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0295 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0291 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0291 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0292 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0304 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0308 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0309 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0182 - acc: 0.9996
Epoch 2/40
 - 7s - loss: 0.0101 - acc: 0.9997
Epoch 3/40
 - 7s - loss: 0.0089 - acc: 0.9997
Epoch 4/40
 - 7s - loss: 0.0057 - acc: 0.9997
Epoch 5/40
 - 7s - loss: 0.0032 - acc: 0.9999
Epoch 6/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 4.3415e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 2.4603e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 9.5091e-05 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 3.6137e-05 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 1.4429e-05 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 6.6847e-06 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 3.7991e-06 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 2.2907e-06 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 1.5866e-06 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.3727e-06 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.2840e-06 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.2252e-06 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 1.2174e-06 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.2039e-06 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 1.2007e-06 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 1.1977e-06 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.1972e-06 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 1.1957e-06 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.1950e-06 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 1.1944e-06 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.1942e-06 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 1.1938e-06 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.1938e-06 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 1.1936e-06 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.1932e-06 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 1.1933e-06 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.1933e-06 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.1932e-06 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 1.1931e-06 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.1928e-06 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.1928e-06 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.1926e-06 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.1928e-06 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 1.1925e-06 - acc: 1.0000
# Training time = 0:46:53.833722
# F-Score(Ordinary) = 0.717, Recall: 0.913, Precision: 0.591
# F-Score(lvc) = 0.551, Recall: 0.844, Precision: 0.409
# F-Score(ireflv) = 0.825, Recall: 0.887, Precision: 0.77
# F-Score(id) = 0.733, Recall: 0.966, Precision: 0.591
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_63 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_64 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_63 (Embedding)        (None, 4, 48)        705264      input_63[0][0]                   
__________________________________________________________________________________________________
embedding_64 (Embedding)        (None, 4, 24)        5640        input_64[0][0]                   
__________________________________________________________________________________________________
flatten_63 (Flatten)            (None, 192)          0           embedding_63[0][0]               
__________________________________________________________________________________________________
flatten_64 (Flatten)            (None, 96)           0           embedding_64[0][0]               
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 288)          0           flatten_63[0][0]                 
                                                                 flatten_64[0][0]                 
__________________________________________________________________________________________________
dense_63 (Dense)                (None, 24)           6936        concatenate_32[0][0]             
__________________________________________________________________________________________________
dropout_32 (Dropout)            (None, 24)           0           dense_63[0][0]                   
__________________________________________________________________________________________________
dense_64 (Dense)                (None, 8)            200         dropout_32[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0722 - acc: 0.9931 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0210 - acc: 0.9970 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0201 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0196 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0194 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0190 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0189 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0189 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0190 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0190 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0189 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0188 - acc: 0.9976 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0192 - acc: 0.9976 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0196 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0199 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0201 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0202 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0202 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0206 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0211 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0211 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0213 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0219 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0219 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0223 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0223 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0222 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0311 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0239 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0233 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0225 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0233 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0237 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0233 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0241 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0287 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0388 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0419 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0424 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0081 - acc: 0.9999
Epoch 2/40
 - 7s - loss: 0.0018 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 1.1948e-06 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 1.3053e-06 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 4.9043e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 1.1922e-06 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 4.8812e-04 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 4.8582e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 9.6406e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 4.7951e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 4.6984e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 9.1901e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 9.1106e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 4.5299e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 4.4440e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 4.4210e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 8.7626e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 4.3571e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 4.3342e-04 - acc: 1.0000
# Training time = 0:47:14.501865
# F-Score(Ordinary) = 0.722, Recall: 0.917, Precision: 0.595
# F-Score(lvc) = 0.531, Recall: 0.85, Precision: 0.386
# F-Score(ireflv) = 0.839, Recall: 0.868, Precision: 0.811
# F-Score(id) = 0.744, Recall: 0.991, Precision: 0.596
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_65 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_66 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_65 (Embedding)        (None, 4, 48)        705264      input_65[0][0]                   
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 4, 24)        5640        input_66[0][0]                   
__________________________________________________________________________________________________
flatten_65 (Flatten)            (None, 192)          0           embedding_65[0][0]               
__________________________________________________________________________________________________
flatten_66 (Flatten)            (None, 96)           0           embedding_66[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 288)          0           flatten_65[0][0]                 
                                                                 flatten_66[0][0]                 
__________________________________________________________________________________________________
dense_65 (Dense)                (None, 24)           6936        concatenate_33[0][0]             
__________________________________________________________________________________________________
dropout_33 (Dropout)            (None, 24)           0           dense_65[0][0]                   
__________________________________________________________________________________________________
dense_66 (Dense)                (None, 8)            200         dropout_33[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0715 - acc: 0.9933 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0210 - acc: 0.9970 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 62s - loss: 0.0203 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0199 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0190 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0191 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0193 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0191 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0196 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0198 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0200 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0205 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0202 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0206 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0209 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0216 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0217 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0230 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 65s - loss: 0.0226 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 66s - loss: 0.0234 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0250 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0224 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0230 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0225 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0225 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0236 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0236 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0249 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0232 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0244 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0248 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0245 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0253 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0246 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0265 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0255 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0262 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0022 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0028 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.0024 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 3.8256e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 7.5732e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 7.3804e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 7.3020e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 6.8157e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 6.7318e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 9.9384e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 9.7596e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 6.2610e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 0.0018 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 5.6631e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 2.5180e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 7.4007e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 9.2774e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 4.3633e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 4.2923e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 5.9723e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 7.7071e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 3.7460e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.8478e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 5.3906e-04 - acc: 1.0000
# Training time = 0:47:02.298416
# F-Score(Ordinary) = 0.703, Recall: 0.933, Precision: 0.564
# F-Score(lvc) = 0.532, Recall: 0.893, Precision: 0.379
# F-Score(ireflv) = 0.811, Recall: 0.9, Precision: 0.738
# F-Score(id) = 0.73, Recall: 0.982, Precision: 0.58
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_67 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_68 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 4, 48)        705264      input_67[0][0]                   
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 4, 24)        5640        input_68[0][0]                   
__________________________________________________________________________________________________
flatten_67 (Flatten)            (None, 192)          0           embedding_67[0][0]               
__________________________________________________________________________________________________
flatten_68 (Flatten)            (None, 96)           0           embedding_68[0][0]               
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 288)          0           flatten_67[0][0]                 
                                                                 flatten_68[0][0]                 
__________________________________________________________________________________________________
dense_67 (Dense)                (None, 24)           6936        concatenate_34[0][0]             
__________________________________________________________________________________________________
dropout_34 (Dropout)            (None, 24)           0           dense_67[0][0]                   
__________________________________________________________________________________________________
dense_68 (Dense)                (None, 8)            200         dropout_34[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0750 - acc: 0.9932 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 64s - loss: 0.0214 - acc: 0.9970 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 64s - loss: 0.0205 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0199 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0196 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0197 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0194 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0191 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0193 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 63s - loss: 0.0191 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0193 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0190 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0193 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0192 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 64s - loss: 0.0197 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0202 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0210 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0205 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0210 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0206 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0211 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0220 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0218 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 63s - loss: 0.0257 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0231 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 63s - loss: 0.0240 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0243 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0224 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0224 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0229 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0238 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0237 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0238 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0242 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0239 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 63s - loss: 0.0247 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0242 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0270 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0257 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0018 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0021 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 0.0023 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 6.5318e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 3.2445e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 8.9359e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 5.3657e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 7.8905e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 9.8514e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 9.5780e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 4.6996e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 2.3271e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 6.8319e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.1390e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 6.2726e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 6.0993e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 7.8726e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 5.4372e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 5.2888e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 5.1299e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.6817e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 1.6589e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 4.8331e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 6.1982e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.5145e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 2.9525e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 1.4549e-04 - acc: 1.0000
# Training time = 0:47:49.115976
# F-Score(Ordinary) = 0.65, Recall: 0.703, Precision: 0.604
# F-Score(lvc) = 0.533, Recall: 0.791, Precision: 0.402
# F-Score(ireflv) = 0.812, Recall: 0.848, Precision: 0.779
# F-Score(id) = 0.605, Recall: 0.588, Precision: 0.622
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_69 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_70 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_69 (Embedding)        (None, 4, 48)        705264      input_69[0][0]                   
__________________________________________________________________________________________________
embedding_70 (Embedding)        (None, 4, 24)        5640        input_70[0][0]                   
__________________________________________________________________________________________________
flatten_69 (Flatten)            (None, 192)          0           embedding_69[0][0]               
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 96)           0           embedding_70[0][0]               
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 288)          0           flatten_69[0][0]                 
                                                                 flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_69 (Dense)                (None, 24)           6936        concatenate_35[0][0]             
__________________________________________________________________________________________________
dropout_35 (Dropout)            (None, 24)           0           dense_69[0][0]                   
__________________________________________________________________________________________________
dense_70 (Dense)                (None, 8)            200         dropout_35[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0738 - acc: 0.9932 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 62s - loss: 0.0217 - acc: 0.9970 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 62s - loss: 0.0206 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0202 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0196 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0197 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0197 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0197 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0196 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0196 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0196 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0197 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0196 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0202 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0203 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0207 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0212 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0211 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0218 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0219 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0228 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0227 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0226 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0234 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0233 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0226 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0233 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0228 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 64s - loss: 0.0232 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 64s - loss: 0.0244 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 64s - loss: 0.0239 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 64s - loss: 0.0236 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0248 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0259 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0268 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0258 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0256 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0257 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0260 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0256 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0047 - acc: 0.9999
Epoch 2/40
 - 7s - loss: 0.0036 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0037 - acc: 0.9999
Epoch 4/40
 - 7s - loss: 0.0037 - acc: 0.9999
Epoch 5/40
 - 7s - loss: 0.0045 - acc: 0.9999
Epoch 6/40
 - 7s - loss: 0.0029 - acc: 0.9999
Epoch 7/40
 - 7s - loss: 0.0047 - acc: 0.9999
Epoch 8/40
 - 7s - loss: 0.0023 - acc: 0.9999
Epoch 9/40
 - 7s - loss: 0.0023 - acc: 0.9999
Epoch 10/40
 - 7s - loss: 0.0033 - acc: 0.9999
Epoch 11/40
 - 7s - loss: 0.0016 - acc: 0.9999
Epoch 12/40
 - 7s - loss: 0.0019 - acc: 0.9999
Epoch 13/40
 - 7s - loss: 6.6241e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0014 - acc: 0.9999
Epoch 15/40
 - 7s - loss: 9.9513e-04 - acc: 0.9999
Epoch 16/40
 - 7s - loss: 7.5112e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 4.9019e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 4.0535e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 2.4734e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 1.3610e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 7.2465e-05 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 4.8815e-05 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 4.1964e-05 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 3.0092e-05 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.0224e-05 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 2.2469e-05 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 1.1350e-05 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 6.1061e-06 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 4.4963e-06 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 3.2014e-06 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.2824e-06 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 2.0588e-06 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 1.6056e-06 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 1.4650e-06 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 1.3835e-06 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 1.3436e-06 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.2445e-06 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.2663e-06 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.2426e-06 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 1.2233e-06 - acc: 1.0000
# Training time = 0:47:08.210408
# F-Score(Ordinary) = 0.708, Recall: 0.843, Precision: 0.611
# F-Score(lvc) = 0.567, Recall: 0.887, Precision: 0.417
# F-Score(ireflv) = 0.835, Recall: 0.861, Precision: 0.811
# F-Score(id) = 0.686, Recall: 0.8, Precision: 0.601
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_71 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_72 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_71 (Embedding)        (None, 4, 48)        705264      input_71[0][0]                   
__________________________________________________________________________________________________
embedding_72 (Embedding)        (None, 4, 24)        5640        input_72[0][0]                   
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 192)          0           embedding_71[0][0]               
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 96)           0           embedding_72[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 288)          0           flatten_71[0][0]                 
                                                                 flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_71 (Dense)                (None, 24)           6936        concatenate_36[0][0]             
__________________________________________________________________________________________________
dropout_36 (Dropout)            (None, 24)           0           dense_71[0][0]                   
__________________________________________________________________________________________________
dense_72 (Dense)                (None, 8)            200         dropout_36[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0738 - acc: 0.9929 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0215 - acc: 0.9969 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0205 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0200 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0197 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0196 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0196 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0197 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0200 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0198 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0202 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0202 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0205 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0218 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0223 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0226 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0231 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0230 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0233 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0247 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0241 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0251 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0261 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0256 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0263 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0268 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0266 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0269 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0273 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0278 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0282 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0284 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0282 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0296 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0290 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0303 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0300 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 7.1411e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 7.0447e-04 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 6.9594e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 0.0016 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 6.2506e-04 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 2.9538e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 2.8514e-04 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 5.6265e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 8.2798e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 7.8318e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 7.6612e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 7.4937e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 4.8999e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 4.8141e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 9.3850e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 8.7454e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 4.1222e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 6.0168e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 5.8427e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 7.5211e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 3.6598e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 7.0746e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 3.4411e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 3.3561e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 6.4805e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 6.1970e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 7.3521e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 4.2146e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 5.3619e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 2.5866e-04 - acc: 1.0000
# Training time = 0:47:09.342459
# F-Score(Ordinary) = 0.671, Recall: 0.761, Precision: 0.6
# F-Score(lvc) = 0.525, Recall: 0.757, Precision: 0.402
# F-Score(ireflv) = 0.837, Recall: 0.905, Precision: 0.779
# F-Score(id) = 0.616, Recall: 0.649, Precision: 0.585
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_73 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_74 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_73 (Embedding)        (None, 4, 48)        705264      input_73[0][0]                   
__________________________________________________________________________________________________
embedding_74 (Embedding)        (None, 4, 24)        5640        input_74[0][0]                   
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 192)          0           embedding_73[0][0]               
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 96)           0           embedding_74[0][0]               
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 288)          0           flatten_73[0][0]                 
                                                                 flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_73 (Dense)                (None, 24)           6936        concatenate_37[0][0]             
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 24)           0           dense_73[0][0]                   
__________________________________________________________________________________________________
dense_74 (Dense)                (None, 8)            200         dropout_37[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0736 - acc: 0.9931 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0214 - acc: 0.9970 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0205 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0198 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0195 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0191 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0191 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0192 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 65s - loss: 0.0192 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 66s - loss: 0.0190 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0193 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0194 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0198 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0204 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0205 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0210 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0216 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0222 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0229 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0236 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0233 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0254 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0263 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0260 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0257 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0263 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0265 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0259 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0261 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0255 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0265 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0264 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0262 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0260 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0268 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0323 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0271 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0020 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0023 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 7.4894e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 7.2491e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 3.3674e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.0016 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0028 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 8.8862e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 0.0023 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 8.3450e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 5.4738e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 0.0018 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 7.4693e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.4581e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 7.2302e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 7.0595e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 2.3276e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 6.8286e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 8.4099e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 7.6737e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 7.3938e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 3.6044e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 5.2595e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 5.0872e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 4.9111e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 7.8184e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 4.4889e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 9.8550e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 6.5953e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 4.9860e-04 - acc: 1.0000
# Training time = 0:47:16.474375
# F-Score(Ordinary) = 0.61, Recall: 0.675, Precision: 0.557
# F-Score(lvc) = 0.425, Recall: 0.587, Precision: 0.333
# F-Score(ireflv) = 0.809, Recall: 0.883, Precision: 0.746
# F-Score(id) = 0.589, Recall: 0.592, Precision: 0.585
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_75 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_76 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_75 (Embedding)        (None, 4, 48)        705264      input_75[0][0]                   
__________________________________________________________________________________________________
embedding_76 (Embedding)        (None, 4, 24)        5640        input_76[0][0]                   
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 192)          0           embedding_75[0][0]               
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 96)           0           embedding_76[0][0]               
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 288)          0           flatten_75[0][0]                 
                                                                 flatten_76[0][0]                 
__________________________________________________________________________________________________
dense_75 (Dense)                (None, 24)           6936        concatenate_38[0][0]             
__________________________________________________________________________________________________
dropout_38 (Dropout)            (None, 24)           0           dense_75[0][0]                   
__________________________________________________________________________________________________
dense_76 (Dense)                (None, 8)            200         dropout_38[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 63s - loss: 0.0753 - acc: 0.9931 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0212 - acc: 0.9970 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0203 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0200 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 63s - loss: 0.0194 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 63s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0189 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 63s - loss: 0.0190 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 63s - loss: 0.0187 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 66s - loss: 0.0190 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0192 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 63s - loss: 0.0194 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 63s - loss: 0.0193 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 63s - loss: 0.0198 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 63s - loss: 0.0202 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 63s - loss: 0.0207 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 63s - loss: 0.0206 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 63s - loss: 0.0209 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 63s - loss: 0.0214 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 63s - loss: 0.0214 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 63s - loss: 0.0216 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 63s - loss: 0.0218 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 63s - loss: 0.0221 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0221 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 63s - loss: 0.0218 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0224 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 63s - loss: 0.0229 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 63s - loss: 0.0218 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 63s - loss: 0.0235 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 63s - loss: 0.0230 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 63s - loss: 0.0226 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 63s - loss: 0.0223 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 63s - loss: 0.0227 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 63s - loss: 0.0223 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 63s - loss: 0.0227 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0228 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 63s - loss: 0.0223 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 63s - loss: 0.0232 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 63s - loss: 0.0243 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 8.0369e-04 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 1.5274e-04 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 1.3663e-06 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 1.2808e-06 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 9.3483e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 9.2631e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 1.1922e-06 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 8.9309e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 4.4397e-04 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 4.4166e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 4.3936e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 4.3707e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 4.3479e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 4.3249e-04 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 8.5701e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 4.2630e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 4.2401e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 4.2175e-04 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 4.1951e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 4.1721e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 8.2634e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 8.1852e-04 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 8.0980e-04 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 8.0112e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 7.9363e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
# Training time = 0:47:25.958016
# F-Score(Ordinary) = 0.729, Recall: 0.916, Precision: 0.606
# F-Score(lvc) = 0.557, Recall: 0.812, Precision: 0.424
# F-Score(ireflv) = 0.843, Recall: 0.898, Precision: 0.795
# F-Score(id) = 0.75, Recall: 0.983, Precision: 0.606
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_77 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_78 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_77 (Embedding)        (None, 4, 48)        705264      input_77[0][0]                   
__________________________________________________________________________________________________
embedding_78 (Embedding)        (None, 4, 24)        5640        input_78[0][0]                   
__________________________________________________________________________________________________
flatten_77 (Flatten)            (None, 192)          0           embedding_77[0][0]               
__________________________________________________________________________________________________
flatten_78 (Flatten)            (None, 96)           0           embedding_78[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 288)          0           flatten_77[0][0]                 
                                                                 flatten_78[0][0]                 
__________________________________________________________________________________________________
dense_77 (Dense)                (None, 24)           6936        concatenate_39[0][0]             
__________________________________________________________________________________________________
dropout_39 (Dropout)            (None, 24)           0           dense_77[0][0]                   
__________________________________________________________________________________________________
dense_78 (Dense)                (None, 8)            200         dropout_39[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0700 - acc: 0.9933 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0214 - acc: 0.9969 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0206 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 62s - loss: 0.0199 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0197 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 63s - loss: 0.0195 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 63s - loss: 0.0194 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0195 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 63s - loss: 0.0196 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0199 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0206 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0219 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0222 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0223 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0229 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0230 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0239 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0251 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0247 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0248 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0244 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0250 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0250 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0259 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 64s - loss: 0.0270 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 66s - loss: 0.0260 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 65s - loss: 0.0264 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 65s - loss: 0.0270 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0264 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0270 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0283 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0287 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0306 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0291 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0286 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0294 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0301 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.0032 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 8.9490e-04 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 0.0022 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0030 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 4.2044e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 8.3283e-04 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 8.2402e-04 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0024 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 7.7305e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 1.1921e-06 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 7.6436e-04 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 3.7988e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 0.0019 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 3.6863e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 3.6699e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 0.0018 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 0.0021 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 6.8982e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 6.8214e-04 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 6.7458e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 0.0023 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 6.4296e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 0.0013 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 9.2585e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 0.0018 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 0.0015 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 5.0855e-04 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 5.0047e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 4.9321e-04 - acc: 1.0000
# Training time = 0:47:19.832916
# F-Score(Ordinary) = 0.568, Recall: 0.592, Precision: 0.546
# F-Score(lvc) = 0.289, Recall: 0.251, Precision: 0.341
# F-Score(ireflv) = 0.807, Recall: 0.868, Precision: 0.754
# F-Score(id) = 0.656, Recall: 0.827, Precision: 0.544
********************
********************
# XP = FR: overSampling favorisationCoeff = 10
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_79 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_80 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_79 (Embedding)        (None, 4, 48)        705264      input_79[0][0]                   
__________________________________________________________________________________________________
embedding_80 (Embedding)        (None, 4, 24)        5640        input_80[0][0]                   
__________________________________________________________________________________________________
flatten_79 (Flatten)            (None, 192)          0           embedding_79[0][0]               
__________________________________________________________________________________________________
flatten_80 (Flatten)            (None, 96)           0           embedding_80[0][0]               
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 288)          0           flatten_79[0][0]                 
                                                                 flatten_80[0][0]                 
__________________________________________________________________________________________________
dense_79 (Dense)                (None, 24)           6936        concatenate_40[0][0]             
__________________________________________________________________________________________________
dropout_40 (Dropout)            (None, 24)           0           dense_79[0][0]                   
__________________________________________________________________________________________________
dense_80 (Dense)                (None, 8)            200         dropout_40[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 791721
data size after sampling = 2757090
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
Train on 2481381 samples, validate on 275709 samples
Epoch 1/40
 - 62s - loss: 0.0710 - acc: 0.9932 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 63s - loss: 0.0211 - acc: 0.9970 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 63s - loss: 0.0203 - acc: 0.9971 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 63s - loss: 0.0198 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 62s - loss: 0.0196 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 62s - loss: 0.0194 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 62s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 62s - loss: 0.0192 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 62s - loss: 0.0191 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 62s - loss: 0.0196 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 62s - loss: 0.0201 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 62s - loss: 0.0191 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 62s - loss: 0.0193 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 62s - loss: 0.0194 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 62s - loss: 0.0204 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 62s - loss: 0.0200 - acc: 0.9975 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 62s - loss: 0.0206 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 62s - loss: 0.0211 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 62s - loss: 0.0220 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 62s - loss: 0.0225 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 62s - loss: 0.0230 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 62s - loss: 0.0224 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 62s - loss: 0.0230 - acc: 0.9974 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 62s - loss: 0.0235 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 62s - loss: 0.0402 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 62s - loss: 0.0351 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 62s - loss: 0.0245 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 62s - loss: 0.0265 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 62s - loss: 0.0245 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 62s - loss: 0.0247 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 62s - loss: 0.0244 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 62s - loss: 0.0249 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 62s - loss: 0.0258 - acc: 0.9972 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 62s - loss: 0.0256 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 62s - loss: 0.0252 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 62s - loss: 0.0256 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 62s - loss: 0.0256 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 62s - loss: 0.0252 - acc: 0.9973 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 1/40
 - 7s - loss: 0.0014 - acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 4/40
 - 7s - loss: 7.6591e-04 - acc: 1.0000
Epoch 5/40
 - 7s - loss: 0.0026 - acc: 1.0000
Epoch 6/40
 - 7s - loss: 3.6912e-04 - acc: 1.0000
Epoch 7/40
 - 7s - loss: 7.3141e-04 - acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.0018 - acc: 1.0000
Epoch 10/40
 - 7s - loss: 3.4878e-04 - acc: 1.0000
Epoch 11/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.0017 - acc: 1.0000
Epoch 13/40
 - 7s - loss: 6.6167e-04 - acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0016 - acc: 1.0000
Epoch 15/40
 - 7s - loss: 6.3555e-04 - acc: 1.0000
Epoch 16/40
 - 7s - loss: 3.1553e-04 - acc: 1.0000
Epoch 17/40
 - 7s - loss: 0.0021 - acc: 1.0000
Epoch 18/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 19/40
 - 7s - loss: 8.7703e-04 - acc: 1.0000
Epoch 20/40
 - 7s - loss: 5.7521e-04 - acc: 1.0000
Epoch 21/40
 - 7s - loss: 8.4748e-04 - acc: 1.0000
Epoch 22/40
 - 7s - loss: 2.7941e-04 - acc: 1.0000
Epoch 23/40
 - 7s - loss: 8.2477e-04 - acc: 1.0000
Epoch 24/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 25/40
 - 7s - loss: 7.8269e-04 - acc: 1.0000
Epoch 26/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 27/40
 - 7s - loss: 9.8693e-04 - acc: 1.0000
Epoch 28/40
 - 7s - loss: 0.0012 - acc: 1.0000
Epoch 29/40
 - 7s - loss: 6.9529e-04 - acc: 1.0000
Epoch 30/40
 - 7s - loss: 2.2859e-04 - acc: 1.0000
Epoch 31/40
 - 7s - loss: 2.2640e-04 - acc: 1.0000
Epoch 32/40
 - 7s - loss: 6.6341e-04 - acc: 1.0000
Epoch 33/40
 - 7s - loss: 4.3269e-04 - acc: 1.0000
Epoch 34/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 35/40
 - 7s - loss: 0.0010 - acc: 1.0000
Epoch 36/40
 - 7s - loss: 9.6107e-04 - acc: 1.0000
Epoch 37/40
 - 7s - loss: 1.8824e-04 - acc: 1.0000
Epoch 38/40
 - 7s - loss: 0.0011 - acc: 1.0000
Epoch 39/40
 - 7s - loss: 3.4859e-04 - acc: 1.0000
Epoch 40/40
 - 7s - loss: 8.3788e-04 - acc: 1.0000
# Training time = 0:47:07.865266
# F-Score(Ordinary) = 0.71, Recall: 0.941, Precision: 0.57
# F-Score(lvc) = 0.5, Recall: 0.938, Precision: 0.341
# F-Score(ireflv) = 0.847, Recall: 0.907, Precision: 0.795
# F-Score(id) = 0.731, Recall: 0.974, Precision: 0.585
********************
