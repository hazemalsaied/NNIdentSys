INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_q3eYtR.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 10619/11178 Mb (0.950000) on cuda
Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:03:00.0)
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--2.7.14-64/tmpb7eScr and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--2.7.14-64/tmplz92BL). This is not supposed to happen! You may need to manually delete your cache directory to fix this.

==================================================================================================
	Corpus Mode
==================================================================================================

==================================================================================================
	Numerical expressions:
	Train: 0.11 Test: 0.15
==================================================================================================
	Language : FR
==================================================================================================
	Dataset : FTB
	Training (Important) : 11514, Test : 2541
	MWEs in tain : 5868, occurrences : 22772
	Impotant words in tain : 4155
	MWE length mean : 2.65
	Seen MWEs : 2836 (83 %)
	New MWEs : 580 (16 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19174 * POS : 969
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 200)       3834800     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 15)        14535       input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 800)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 60)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 860)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 25)           21525       concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 25)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            208         dropout_1[0][0]                  
==================================================================================================
Total params: 3,871,068
Trainable params: 3,871,068
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	11098 importatnt sents of 11514
	data size before focused sampling = 802565
	data size after focused sampling = 1672284
	data size before sampling = 1672284
	data size after sampling = 2929136
{0.0: 732284, 1.0: 732284, 2.0: 732284, 7.0: 732284}
	Class weights : {0: 1, 1: 1, 2: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 2636222 samples, validate on 292914 samples
Epoch 1/40
 - 75s - loss: 0.1603 - acc: 0.9601 - val_loss: 0.1049 - val_acc: 0.9997
Epoch 2/40
 - 76s - loss: 0.1257 - acc: 0.9681 - val_loss: 0.0982 - val_acc: 0.9998
Epoch 3/40
 - 72s - loss: 0.1193 - acc: 0.9698 - val_loss: 0.1049 - val_acc: 0.9998
Epoch 4/40
 - 71s - loss: 0.1153 - acc: 0.9710 - val_loss: 0.1045 - val_acc: 0.9998
Epoch 5/40
 - 71s - loss: 0.1124 - acc: 0.9718 - val_loss: 0.1028 - val_acc: 0.9999
Epoch 6/40
 - 71s - loss: 0.1102 - acc: 0.9725 - val_loss: 0.0991 - val_acc: 0.9998
Epoch 7/40
 - 71s - loss: 0.1085 - acc: 0.9731 - val_loss: 0.0976 - val_acc: 0.9998
Epoch 8/40
 - 71s - loss: 0.1074 - acc: 0.9734 - val_loss: 0.1092 - val_acc: 0.9998
Epoch 9/40
 - 71s - loss: 0.1063 - acc: 0.9737 - val_loss: 0.1041 - val_acc: 0.9998
Epoch 10/40
 - 71s - loss: 0.1054 - acc: 0.9739 - val_loss: 0.1025 - val_acc: 0.9998
Epoch 11/40
 - 71s - loss: 0.1046 - acc: 0.9741 - val_loss: 0.1031 - val_acc: 0.9998
Epoch 12/40
 - 71s - loss: 0.1040 - acc: 0.9743 - val_loss: 0.1054 - val_acc: 0.9998
Epoch 13/40
 - 71s - loss: 0.1034 - acc: 0.9745 - val_loss: 0.0952 - val_acc: 0.9998
Epoch 14/40
 - 71s - loss: 0.1030 - acc: 0.9746 - val_loss: 0.0995 - val_acc: 0.9998
Epoch 15/40
 - 71s - loss: 0.1026 - acc: 0.9747 - val_loss: 0.1033 - val_acc: 0.9998
Epoch 16/40
 - 72s - loss: 0.1021 - acc: 0.9749 - val_loss: 0.1060 - val_acc: 0.9998
Epoch 17/40
 - 71s - loss: 0.1016 - acc: 0.9750 - val_loss: 0.0935 - val_acc: 0.9999
Epoch 18/40
 - 71s - loss: 0.1014 - acc: 0.9751 - val_loss: 0.1012 - val_acc: 0.9999
Epoch 19/40
 - 71s - loss: 0.1009 - acc: 0.9753 - val_loss: 0.1010 - val_acc: 0.9999
Epoch 20/40
 - 71s - loss: 0.1008 - acc: 0.9753 - val_loss: 0.1063 - val_acc: 0.9999
Epoch 21/40
 - 71s - loss: 0.1005 - acc: 0.9754 - val_loss: 0.0983 - val_acc: 0.9998
Epoch 22/40
 - 70s - loss: 0.1001 - acc: 0.9755 - val_loss: 0.1034 - val_acc: 0.9998
Epoch 23/40
 - 71s - loss: 0.0998 - acc: 0.9755 - val_loss: 0.1027 - val_acc: 0.9998
Epoch 24/40
 - 71s - loss: 0.0997 - acc: 0.9755 - val_loss: 0.1116 - val_acc: 0.9998
Epoch 25/40
 - 71s - loss: 0.0995 - acc: 0.9757 - val_loss: 0.1024 - val_acc: 0.9998
Epoch 26/40
 - 71s - loss: 0.0993 - acc: 0.9757 - val_loss: 0.1038 - val_acc: 0.9999
Epoch 27/40
 - 71s - loss: 0.0991 - acc: 0.9758 - val_loss: 0.1021 - val_acc: 0.9998
Epoch 28/40
 - 71s - loss: 0.0988 - acc: 0.9758 - val_loss: 0.1047 - val_acc: 0.9998
Epoch 29/40
 - 71s - loss: 0.0987 - acc: 0.9759 - val_loss: 0.1051 - val_acc: 0.9998
Epoch 30/40
 - 71s - loss: 0.0985 - acc: 0.9759 - val_loss: 0.1004 - val_acc: 0.9999
Epoch 31/40
 - 71s - loss: 0.0984 - acc: 0.9759 - val_loss: 0.1016 - val_acc: 0.9999
Epoch 32/40
 - 71s - loss: 0.0982 - acc: 0.9760 - val_loss: 0.1014 - val_acc: 0.9999
Epoch 33/40
 - 71s - loss: 0.0981 - acc: 0.9760 - val_loss: 0.1012 - val_acc: 0.9998
Epoch 34/40
 - 71s - loss: 0.0979 - acc: 0.9760 - val_loss: 0.0975 - val_acc: 0.9999
Epoch 35/40
 - 71s - loss: 0.0979 - acc: 0.9761 - val_loss: 0.1019 - val_acc: 0.9998
Epoch 36/40
 - 71s - loss: 0.0978 - acc: 0.9761 - val_loss: 0.1024 - val_acc: 0.9999
Epoch 37/40
 - 71s - loss: 0.0975 - acc: 0.9762 - val_loss: 0.1063 - val_acc: 0.9998
Epoch 38/40
 - 71s - loss: 0.0975 - acc: 0.9762 - val_loss: 0.0962 - val_acc: 0.9999
Epoch 39/40
 - 71s - loss: 0.0974 - acc: 0.9762 - val_loss: 0.1085 - val_acc: 0.9999
Epoch 40/40
 - 71s - loss: 0.0972 - acc: 0.9763 - val_loss: 0.1069 - val_acc: 0.9998

==================================================================================================
	Training time : 0:53:51.353615
==================================================================================================
	Identification : 0.409
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 327
	100 : 12
	5 : 229
	200 : 3
	50 : 28
	25 : 56

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 618
	100 : 14
	5 : 240
	200 : 3
	50 : 23
	25 : 58

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
