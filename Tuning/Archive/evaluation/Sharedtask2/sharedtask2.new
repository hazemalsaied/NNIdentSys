INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_2Xvgy5.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 3841/4043 Mb (0.950000) on cuda
Mapped name None to device cuda: GeForce GTX 980 (0000:03:00.0)

==================================================================================================
	Corpus Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 5248, Test : 1832
	MWEs in tain : 2021, occurrences : 6034
	Impotant words in tain : 1589
	MWE length mean : 2.12
	Seen MWEs : 439 (65 %)
	New MWEs : 231 (34 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8017 * POS : 178
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 200)       1603400     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 15)        2670        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 800)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 60)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 860)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 25)           21525       concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 25)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            208         dropout_1[0][0]                  
==================================================================================================
Total params: 1,627,803
Trainable params: 1,627,803
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	5248 importatnt sents of 5248
	data size before sampling = 303816
	data size after sampling = 1042237
{0.0: 148891, 1.0: 148891, 2.0: 148891, 3.0: 148891, 4.0: 148891, 5.0: 148891, 6.0: 148891}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 938013 samples, validate on 104224 samples
Epoch 1/40
 - 24s - loss: 0.1460 - acc: 0.9837 - val_loss: 0.0739 - val_acc: 0.9988
Epoch 2/40
 - 24s - loss: 0.0818 - acc: 0.9891 - val_loss: 0.0752 - val_acc: 0.9988
Epoch 3/40
 - 24s - loss: 0.0761 - acc: 0.9898 - val_loss: 0.0686 - val_acc: 0.9988
Epoch 4/40
 - 24s - loss: 0.0728 - acc: 0.9903 - val_loss: 0.0649 - val_acc: 0.9988
Epoch 5/40
 - 24s - loss: 0.0711 - acc: 0.9905 - val_loss: 0.0665 - val_acc: 0.9988
Epoch 6/40
 - 24s - loss: 0.0702 - acc: 0.9907 - val_loss: 0.0649 - val_acc: 0.9988
Epoch 7/40
 - 24s - loss: 0.0689 - acc: 0.9908 - val_loss: 0.0576 - val_acc: 0.9988
Epoch 8/40
 - 24s - loss: 0.0681 - acc: 0.9909 - val_loss: 0.0602 - val_acc: 0.9988
Epoch 9/40
 - 24s - loss: 0.0674 - acc: 0.9910 - val_loss: 0.0501 - val_acc: 0.9988
Epoch 10/40
 - 24s - loss: 0.0670 - acc: 0.9911 - val_loss: 0.0640 - val_acc: 0.9988
Epoch 11/40
 - 24s - loss: 0.0661 - acc: 0.9912 - val_loss: 0.0671 - val_acc: 0.9988
Epoch 12/40
 - 24s - loss: 0.0656 - acc: 0.9912 - val_loss: 0.0659 - val_acc: 0.9988
Epoch 13/40
 - 24s - loss: 0.0654 - acc: 0.9913 - val_loss: 0.0664 - val_acc: 0.9988
Epoch 14/40
 - 24s - loss: 0.0650 - acc: 0.9914 - val_loss: 0.0668 - val_acc: 0.9988
Epoch 15/40
 - 24s - loss: 0.0647 - acc: 0.9914 - val_loss: 0.0684 - val_acc: 0.9988
Epoch 16/40
 - 24s - loss: 0.0646 - acc: 0.9914 - val_loss: 0.0706 - val_acc: 0.9988
Epoch 17/40
 - 24s - loss: 0.0641 - acc: 0.9915 - val_loss: 0.0628 - val_acc: 0.9988
Epoch 18/40
 - 24s - loss: 0.0639 - acc: 0.9914 - val_loss: 0.0640 - val_acc: 0.9988
Epoch 19/40
 - 24s - loss: 0.0636 - acc: 0.9915 - val_loss: 0.0686 - val_acc: 0.9988
Epoch 20/40
 - 24s - loss: 0.0635 - acc: 0.9916 - val_loss: 0.0634 - val_acc: 0.9988
Epoch 21/40
 - 24s - loss: 0.0631 - acc: 0.9916 - val_loss: 0.0697 - val_acc: 0.9988
Epoch 22/40
 - 24s - loss: 0.0631 - acc: 0.9916 - val_loss: 0.0609 - val_acc: 0.9988
Epoch 23/40
 - 24s - loss: 0.0628 - acc: 0.9917 - val_loss: 0.0618 - val_acc: 0.9988
Epoch 24/40
 - 24s - loss: 0.0628 - acc: 0.9916 - val_loss: 0.0662 - val_acc: 0.9988
Epoch 25/40
 - 24s - loss: 0.0627 - acc: 0.9916 - val_loss: 0.0722 - val_acc: 0.9988
Epoch 26/40
 - 24s - loss: 0.0627 - acc: 0.9917 - val_loss: 0.0644 - val_acc: 0.9988
Epoch 27/40
 - 24s - loss: 0.0627 - acc: 0.9917 - val_loss: 0.0611 - val_acc: 0.9988
Epoch 28/40
 - 24s - loss: 0.0625 - acc: 0.9917 - val_loss: 0.0696 - val_acc: 0.9988
Epoch 29/40
 - 24s - loss: 0.0624 - acc: 0.9917 - val_loss: 0.0696 - val_acc: 0.9988
Epoch 30/40
 - 24s - loss: 0.0621 - acc: 0.9917 - val_loss: 0.0647 - val_acc: 0.9988
Epoch 31/40
 - 24s - loss: 0.0621 - acc: 0.9918 - val_loss: 0.0625 - val_acc: 0.9988
Epoch 32/40
 - 24s - loss: 0.0620 - acc: 0.9917 - val_loss: 0.0627 - val_acc: 0.9988
Epoch 33/40
 - 24s - loss: 0.0618 - acc: 0.9918 - val_loss: 0.0681 - val_acc: 0.9988
Epoch 34/40
 - 24s - loss: 0.0619 - acc: 0.9918 - val_loss: 0.0656 - val_acc: 0.9988
Epoch 35/40
 - 24s - loss: 0.0617 - acc: 0.9918 - val_loss: 0.0694 - val_acc: 0.9988
Epoch 36/40
 - 24s - loss: 0.0616 - acc: 0.9918 - val_loss: 0.0666 - val_acc: 0.9988
Epoch 37/40
 - 24s - loss: 0.0615 - acc: 0.9918 - val_loss: 0.0653 - val_acc: 0.9988
Epoch 38/40
 - 24s - loss: 0.0616 - acc: 0.9918 - val_loss: 0.0686 - val_acc: 0.9988
Epoch 39/40
 - 24s - loss: 0.0613 - acc: 0.9918 - val_loss: 0.0697 - val_acc: 0.9988
Epoch 40/40
 - 24s - loss: 0.0615 - acc: 0.9918 - val_loss: 0.0642 - val_acc: 0.9988

==================================================================================================
	Training time : 0:18:11.173148
==================================================================================================
	Identification : 0.628
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 11
	100 : 5
	5 : 75
	10 : 28
	50 : 20
	500 : 2
	25 : 35

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 206
	100 : 2
	5 : 51
	10 : 6
	50 : 4
	500 : 1
	25 : 4

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : DE
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2799, Test : 1078
	MWEs in tain : 1944, occurrences : 3323
	Impotant words in tain : 1661
	MWE length mean : 1.96
	Seen MWEs : 245 (49 %)
	New MWEs : 255 (51 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 6423 * POS : 291
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 200)       1284600     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 15)        4365        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 800)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 60)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 860)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 25)           21525       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 25)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            208         dropout_2[0][0]                  
==================================================================================================
Total params: 1,310,698
Trainable params: 1,310,698
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	2799 importatnt sents of 2799
	data size before sampling = 127179
	data size after sampling = 433811
{0.0: 61973, 1.0: 61973, 2.0: 61973, 3.0: 61973, 4.0: 61973, 5.0: 61973, 6.0: 61973}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 390429 samples, validate on 43382 samples
Epoch 1/40
 - 10s - loss: 0.3574 - acc: 0.9606 - val_loss: 0.0138 - val_acc: 1.0000
Epoch 2/40
 - 10s - loss: 0.1806 - acc: 0.9789 - val_loss: 0.0083 - val_acc: 1.0000
Epoch 3/40
 - 10s - loss: 0.1638 - acc: 0.9810 - val_loss: 0.0067 - val_acc: 1.0000
Epoch 4/40
 - 10s - loss: 0.1549 - acc: 0.9823 - val_loss: 0.0062 - val_acc: 1.0000
Epoch 5/40
 - 10s - loss: 0.1494 - acc: 0.9830 - val_loss: 0.0055 - val_acc: 1.0000
Epoch 6/40
 - 10s - loss: 0.1464 - acc: 0.9834 - val_loss: 0.0056 - val_acc: 1.0000
Epoch 7/40
 - 10s - loss: 0.1438 - acc: 0.9838 - val_loss: 0.0058 - val_acc: 1.0000
Epoch 8/40
 - 10s - loss: 0.1415 - acc: 0.9840 - val_loss: 0.0051 - val_acc: 1.0000
Epoch 9/40
 - 10s - loss: 0.1399 - acc: 0.9844 - val_loss: 0.0058 - val_acc: 1.0000
Epoch 10/40
 - 10s - loss: 0.1381 - acc: 0.9845 - val_loss: 0.0075 - val_acc: 1.0000
Epoch 11/40
 - 10s - loss: 0.1365 - acc: 0.9847 - val_loss: 0.0051 - val_acc: 1.0000
Epoch 12/40
 - 10s - loss: 0.1358 - acc: 0.9849 - val_loss: 0.0058 - val_acc: 1.0000
Epoch 13/40
 - 10s - loss: 0.1349 - acc: 0.9850 - val_loss: 0.0064 - val_acc: 1.0000
Epoch 14/40
 - 10s - loss: 0.1340 - acc: 0.9851 - val_loss: 0.0064 - val_acc: 1.0000
Epoch 15/40
 - 10s - loss: 0.1339 - acc: 0.9853 - val_loss: 0.0069 - val_acc: 1.0000
Epoch 16/40
 - 10s - loss: 0.1330 - acc: 0.9853 - val_loss: 0.0068 - val_acc: 1.0000
Epoch 17/40
 - 10s - loss: 0.1330 - acc: 0.9854 - val_loss: 0.0069 - val_acc: 1.0000
Epoch 18/40
 - 10s - loss: 0.1321 - acc: 0.9854 - val_loss: 0.0066 - val_acc: 1.0000
Epoch 19/40
 - 10s - loss: 0.1315 - acc: 0.9855 - val_loss: 0.0076 - val_acc: 1.0000
Epoch 20/40
 - 10s - loss: 0.1313 - acc: 0.9856 - val_loss: 0.0065 - val_acc: 1.0000
Epoch 21/40
 - 10s - loss: 0.1313 - acc: 0.9856 - val_loss: 0.0082 - val_acc: 1.0000
Epoch 22/40
 - 10s - loss: 0.1304 - acc: 0.9856 - val_loss: 0.0078 - val_acc: 1.0000
Epoch 23/40
 - 10s - loss: 0.1304 - acc: 0.9857 - val_loss: 0.0082 - val_acc: 1.0000
Epoch 24/40
 - 10s - loss: 0.1299 - acc: 0.9858 - val_loss: 0.0069 - val_acc: 1.0000
Epoch 25/40
 - 10s - loss: 0.1291 - acc: 0.9858 - val_loss: 0.0072 - val_acc: 1.0000
Epoch 26/40
 - 10s - loss: 0.1287 - acc: 0.9859 - val_loss: 0.0080 - val_acc: 1.0000
Epoch 27/40
 - 10s - loss: 0.1291 - acc: 0.9859 - val_loss: 0.0079 - val_acc: 1.0000
Epoch 28/40
 - 10s - loss: 0.1285 - acc: 0.9860 - val_loss: 0.0080 - val_acc: 1.0000
Epoch 29/40
 - 10s - loss: 0.1283 - acc: 0.9859 - val_loss: 0.0080 - val_acc: 1.0000
Epoch 30/40
 - 10s - loss: 0.1280 - acc: 0.9860 - val_loss: 0.0079 - val_acc: 1.0000
Epoch 31/40
 - 10s - loss: 0.1275 - acc: 0.9859 - val_loss: 0.0084 - val_acc: 1.0000
Epoch 32/40
 - 10s - loss: 0.1277 - acc: 0.9861 - val_loss: 0.0078 - val_acc: 1.0000
Epoch 33/40
 - 10s - loss: 0.1280 - acc: 0.9861 - val_loss: 0.0084 - val_acc: 1.0000
Epoch 34/40
 - 10s - loss: 0.1273 - acc: 0.9862 - val_loss: 0.0086 - val_acc: 1.0000
Epoch 35/40
 - 10s - loss: 0.1271 - acc: 0.9862 - val_loss: 0.0087 - val_acc: 1.0000
Epoch 36/40
 - 10s - loss: 0.1271 - acc: 0.9861 - val_loss: 0.0082 - val_acc: 1.0000
Epoch 37/40
 - 10s - loss: 0.1267 - acc: 0.9862 - val_loss: 0.0088 - val_acc: 1.0000
Epoch 38/40
 - 10s - loss: 0.1265 - acc: 0.9863 - val_loss: 0.0086 - val_acc: 1.0000
Epoch 39/40
 - 10s - loss: 0.1270 - acc: 0.9862 - val_loss: 0.0085 - val_acc: 1.0000
Epoch 40/40
 - 10s - loss: 0.1267 - acc: 0.9861 - val_loss: 0.0087 - val_acc: 1.0000

==================================================================================================
	Training time : 0:07:12.298068
==================================================================================================
	Identification : 0.477
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 9
	10 : 16
	50 : 1
	100 : 2
	5 : 94

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 248
	100 : 1
	5 : 50
	10 : 8
	50 : 1
	25 : 2

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EL
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 1657, Test : 1261
	MWEs in tain : 1149, occurrences : 1904
	Impotant words in tain : 961
	MWE length mean : 2.37
	Seen MWEs : 292 (58 %)
	New MWEs : 209 (41 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5019 * POS : 198
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 200)       1003800     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 15)        2970        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 800)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 60)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 860)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 25)           21525       concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 25)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            208         dropout_3[0][0]                  
==================================================================================================
Total params: 1,028,503
Trainable params: 1,028,503
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1657 importatnt sents of 1657
	data size before sampling = 109355
	data size after sampling = 322572
{0.0: 53762, 1.0: 53762, 2.0: 53762, 3.0: 53762, 5.0: 53762, 6.0: 53762}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 290314 samples, validate on 32258 samples
Epoch 1/40
 - 7s - loss: 0.2054 - acc: 0.9756 - val_loss: 0.1428 - val_acc: 0.9956
Epoch 2/40
 - 7s - loss: 0.0907 - acc: 0.9870 - val_loss: 0.0924 - val_acc: 0.9970
Epoch 3/40
 - 7s - loss: 0.0795 - acc: 0.9887 - val_loss: 0.0922 - val_acc: 0.9970
Epoch 4/40
 - 7s - loss: 0.0751 - acc: 0.9895 - val_loss: 0.0946 - val_acc: 0.9983
Epoch 5/40
 - 7s - loss: 0.0722 - acc: 0.9898 - val_loss: 0.1252 - val_acc: 0.9983
Epoch 6/40
 - 7s - loss: 0.0702 - acc: 0.9900 - val_loss: 0.0783 - val_acc: 0.9983
Epoch 7/40
 - 7s - loss: 0.0685 - acc: 0.9904 - val_loss: 0.0963 - val_acc: 0.9983
Epoch 8/40
 - 7s - loss: 0.0664 - acc: 0.9906 - val_loss: 0.1069 - val_acc: 0.9983
Epoch 9/40
 - 7s - loss: 0.0665 - acc: 0.9907 - val_loss: 0.1073 - val_acc: 0.9983
Epoch 10/40
 - 7s - loss: 0.0645 - acc: 0.9910 - val_loss: 0.0943 - val_acc: 0.9983
Epoch 11/40
 - 7s - loss: 0.0639 - acc: 0.9911 - val_loss: 0.0818 - val_acc: 0.9983
Epoch 12/40
 - 7s - loss: 0.0635 - acc: 0.9911 - val_loss: 0.0950 - val_acc: 0.9983
Epoch 13/40
 - 7s - loss: 0.0627 - acc: 0.9913 - val_loss: 0.0970 - val_acc: 0.9983
Epoch 14/40
 - 7s - loss: 0.0623 - acc: 0.9912 - val_loss: 0.1022 - val_acc: 0.9983
Epoch 15/40
 - 7s - loss: 0.0621 - acc: 0.9913 - val_loss: 0.1041 - val_acc: 0.9983
Epoch 16/40
 - 7s - loss: 0.0621 - acc: 0.9914 - val_loss: 0.0948 - val_acc: 0.9983
Epoch 17/40
 - 7s - loss: 0.0624 - acc: 0.9914 - val_loss: 0.1003 - val_acc: 0.9983
Epoch 18/40
 - 7s - loss: 0.0614 - acc: 0.9916 - val_loss: 0.1040 - val_acc: 0.9983
Epoch 19/40
 - 7s - loss: 0.0613 - acc: 0.9915 - val_loss: 0.1012 - val_acc: 0.9983
Epoch 20/40
 - 7s - loss: 0.0611 - acc: 0.9915 - val_loss: 0.0902 - val_acc: 0.9983
Epoch 21/40
 - 7s - loss: 0.0609 - acc: 0.9916 - val_loss: 0.0997 - val_acc: 0.9983
Epoch 22/40
 - 7s - loss: 0.0602 - acc: 0.9916 - val_loss: 0.1004 - val_acc: 0.9983
Epoch 23/40
 - 7s - loss: 0.0597 - acc: 0.9917 - val_loss: 0.1000 - val_acc: 0.9983
Epoch 24/40
 - 7s - loss: 0.0598 - acc: 0.9918 - val_loss: 0.0942 - val_acc: 0.9983
Epoch 25/40
 - 7s - loss: 0.0598 - acc: 0.9917 - val_loss: 0.0975 - val_acc: 0.9983
Epoch 26/40
 - 7s - loss: 0.0596 - acc: 0.9916 - val_loss: 0.1022 - val_acc: 0.9983
Epoch 27/40
 - 7s - loss: 0.0599 - acc: 0.9917 - val_loss: 0.1009 - val_acc: 0.9983
Epoch 28/40
 - 7s - loss: 0.0598 - acc: 0.9918 - val_loss: 0.1058 - val_acc: 0.9983
Epoch 29/40
 - 7s - loss: 0.0594 - acc: 0.9917 - val_loss: 0.1024 - val_acc: 0.9983
Epoch 30/40
 - 7s - loss: 0.0590 - acc: 0.9919 - val_loss: 0.1040 - val_acc: 0.9983
Epoch 31/40
 - 7s - loss: 0.0591 - acc: 0.9918 - val_loss: 0.1045 - val_acc: 0.9983
Epoch 32/40
 - 7s - loss: 0.0589 - acc: 0.9919 - val_loss: 0.0931 - val_acc: 0.9983
Epoch 33/40
 - 7s - loss: 0.0586 - acc: 0.9920 - val_loss: 0.0929 - val_acc: 0.9983
Epoch 34/40
 - 7s - loss: 0.0586 - acc: 0.9919 - val_loss: 0.1022 - val_acc: 0.9983
Epoch 35/40
 - 7s - loss: 0.0584 - acc: 0.9919 - val_loss: 0.1046 - val_acc: 0.9983
Epoch 36/40
 - 7s - loss: 0.0584 - acc: 0.9919 - val_loss: 0.0971 - val_acc: 0.9983
Epoch 37/40
 - 7s - loss: 0.0585 - acc: 0.9919 - val_loss: 0.0912 - val_acc: 0.9983
Epoch 38/40
 - 7s - loss: 0.0582 - acc: 0.9919 - val_loss: 0.0974 - val_acc: 0.9983
Epoch 39/40
 - 7s - loss: 0.0586 - acc: 0.9919 - val_loss: 0.0953 - val_acc: 0.9983
Epoch 40/40
 - 7s - loss: 0.0585 - acc: 0.9919 - val_loss: 0.1021 - val_acc: 0.9983

==================================================================================================
	Training time : 0:05:11.029085
==================================================================================================
	Identification : 0.574
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 14
	25 : 8
	10 : 16
	50 : 2
	5 : 77

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 188
	25 : 5
	10 : 10
	50 : 2
	5 : 40

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EN
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 300, Test : 3965
	MWEs in tain : 233, occurrences : 331
	Impotant words in tain : 241
	MWE length mean : 2.16
	Seen MWEs : 139 (27 %)
	New MWEs : 362 (72 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 974 * POS : 63
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 200)       194800      input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 15)        945         input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 800)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 60)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 860)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 25)           21525       concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 25)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            208         dropout_4[0][0]                  
==================================================================================================
Total params: 217,478
Trainable params: 217,478
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	300 importatnt sents of 300
	data size before sampling = 14463
	data size after sampling = 42396
{0.0: 7066, 1.0: 7066, 2.0: 7066, 3.0: 7066, 5.0: 7066, 6.0: 7066}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 38156 samples, validate on 4240 samples
Epoch 1/40
 - 1s - loss: 0.4074 - acc: 0.9514 - val_loss: 0.0234 - val_acc: 1.0000
Epoch 2/40
 - 1s - loss: 0.1164 - acc: 0.9827 - val_loss: 0.0159 - val_acc: 1.0000
Epoch 3/40
 - 1s - loss: 0.1024 - acc: 0.9854 - val_loss: 0.0140 - val_acc: 1.0000
Epoch 4/40
 - 1s - loss: 0.0942 - acc: 0.9871 - val_loss: 0.0180 - val_acc: 1.0000
Epoch 5/40
 - 1s - loss: 0.0897 - acc: 0.9878 - val_loss: 0.0154 - val_acc: 1.0000
Epoch 6/40
 - 1s - loss: 0.0873 - acc: 0.9879 - val_loss: 0.0174 - val_acc: 1.0000
Epoch 7/40
 - 1s - loss: 0.0854 - acc: 0.9882 - val_loss: 0.0126 - val_acc: 1.0000
Epoch 8/40
 - 1s - loss: 0.0824 - acc: 0.9887 - val_loss: 0.0186 - val_acc: 1.0000
Epoch 9/40
 - 1s - loss: 0.0826 - acc: 0.9889 - val_loss: 0.0223 - val_acc: 1.0000
Epoch 10/40
 - 1s - loss: 0.0807 - acc: 0.9889 - val_loss: 0.0220 - val_acc: 1.0000
Epoch 11/40
 - 1s - loss: 0.0799 - acc: 0.9892 - val_loss: 0.0220 - val_acc: 1.0000
Epoch 12/40
 - 1s - loss: 0.0784 - acc: 0.9895 - val_loss: 0.0164 - val_acc: 1.0000
Epoch 13/40
 - 1s - loss: 0.0789 - acc: 0.9893 - val_loss: 0.0177 - val_acc: 1.0000
Epoch 14/40
 - 1s - loss: 0.0777 - acc: 0.9895 - val_loss: 0.0196 - val_acc: 1.0000
Epoch 15/40
 - 1s - loss: 0.0781 - acc: 0.9894 - val_loss: 0.0205 - val_acc: 1.0000
Epoch 16/40
 - 1s - loss: 0.0761 - acc: 0.9897 - val_loss: 0.0253 - val_acc: 1.0000
Epoch 17/40
 - 1s - loss: 0.0760 - acc: 0.9897 - val_loss: 0.0212 - val_acc: 1.0000
Epoch 18/40
 - 1s - loss: 0.0757 - acc: 0.9894 - val_loss: 0.0265 - val_acc: 1.0000
Epoch 19/40
 - 1s - loss: 0.0759 - acc: 0.9898 - val_loss: 0.0181 - val_acc: 1.0000
Epoch 20/40
 - 1s - loss: 0.0760 - acc: 0.9895 - val_loss: 0.0276 - val_acc: 1.0000
Epoch 21/40
 - 1s - loss: 0.0750 - acc: 0.9898 - val_loss: 0.0218 - val_acc: 1.0000
Epoch 22/40
 - 1s - loss: 0.0745 - acc: 0.9898 - val_loss: 0.0242 - val_acc: 1.0000
Epoch 23/40
 - 1s - loss: 0.0742 - acc: 0.9896 - val_loss: 0.0208 - val_acc: 1.0000
Epoch 24/40
 - 1s - loss: 0.0748 - acc: 0.9899 - val_loss: 0.0290 - val_acc: 1.0000
Epoch 25/40
 - 1s - loss: 0.0744 - acc: 0.9899 - val_loss: 0.0212 - val_acc: 1.0000
Epoch 26/40
 - 1s - loss: 0.0748 - acc: 0.9897 - val_loss: 0.0221 - val_acc: 1.0000
Epoch 27/40
 - 1s - loss: 0.0742 - acc: 0.9897 - val_loss: 0.0284 - val_acc: 1.0000
Epoch 28/40
 - 1s - loss: 0.0717 - acc: 0.9900 - val_loss: 0.0236 - val_acc: 1.0000
Epoch 29/40
 - 1s - loss: 0.0742 - acc: 0.9896 - val_loss: 0.0237 - val_acc: 1.0000
Epoch 30/40
 - 1s - loss: 0.0726 - acc: 0.9899 - val_loss: 0.0250 - val_acc: 1.0000
Epoch 31/40
 - 1s - loss: 0.0734 - acc: 0.9900 - val_loss: 0.0247 - val_acc: 1.0000
Epoch 32/40
 - 1s - loss: 0.0740 - acc: 0.9898 - val_loss: 0.0224 - val_acc: 1.0000
Epoch 33/40
 - 1s - loss: 0.0746 - acc: 0.9897 - val_loss: 0.0271 - val_acc: 1.0000
Epoch 34/40
 - 1s - loss: 0.0721 - acc: 0.9899 - val_loss: 0.0229 - val_acc: 1.0000
Epoch 35/40
 - 1s - loss: 0.0728 - acc: 0.9900 - val_loss: 0.0257 - val_acc: 1.0000
Epoch 36/40
 - 1s - loss: 0.0722 - acc: 0.9899 - val_loss: 0.0264 - val_acc: 1.0000
Epoch 37/40
 - 1s - loss: 0.0724 - acc: 0.9899 - val_loss: 0.0195 - val_acc: 1.0000
Epoch 38/40
 - 1s - loss: 0.0724 - acc: 0.9900 - val_loss: 0.0227 - val_acc: 1.0000
Epoch 39/40
 - 1s - loss: 0.0716 - acc: 0.9901 - val_loss: 0.0281 - val_acc: 1.0000
Epoch 40/40
 - 1s - loss: 0.0723 - acc: 0.9901 - val_loss: 0.0282 - val_acc: 1.0000

==================================================================================================
	Training time : 0:00:37.762963
==================================================================================================
	Identification : 0.303
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	10 : 2
	5 : 51

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 291
	10 : 1
	5 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : ES
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 1450, Test : 2046
	MWEs in tain : 1184, occurrences : 2239
	Impotant words in tain : 827
	MWE length mean : 2.27
	Seen MWEs : 277 (55 %)
	New MWEs : 223 (44 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4819 * POS : 126
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 200)       963800      input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 15)        1890        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 800)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 60)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 860)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 25)           21525       concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 25)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            208         dropout_5[0][0]                  
==================================================================================================
Total params: 987,423
Trainable params: 987,423
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1450 importatnt sents of 1450
	data size before sampling = 122431
	data size after sampling = 421414
{0.0: 60202, 1.0: 60202, 2.0: 60202, 3.0: 60202, 4.0: 60202, 5.0: 60202, 6.0: 60202}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 379272 samples, validate on 42142 samples
Epoch 1/40
 - 9s - loss: 0.2113 - acc: 0.9780 - val_loss: 0.1257 - val_acc: 0.9963
Epoch 2/40
 - 9s - loss: 0.1171 - acc: 0.9866 - val_loss: 0.1333 - val_acc: 0.9963
Epoch 3/40
 - 9s - loss: 0.1072 - acc: 0.9881 - val_loss: 0.1192 - val_acc: 0.9976
Epoch 4/40
 - 9s - loss: 0.1027 - acc: 0.9888 - val_loss: 0.1083 - val_acc: 0.9963
Epoch 5/40
 - 9s - loss: 0.0990 - acc: 0.9893 - val_loss: 0.1212 - val_acc: 0.9963
Epoch 6/40
 - 9s - loss: 0.0969 - acc: 0.9898 - val_loss: 0.0999 - val_acc: 0.9963
Epoch 7/40
 - 9s - loss: 0.0951 - acc: 0.9900 - val_loss: 0.1205 - val_acc: 0.9963
Epoch 8/40
 - 9s - loss: 0.0933 - acc: 0.9902 - val_loss: 0.1359 - val_acc: 0.9953
Epoch 9/40
 - 9s - loss: 0.0918 - acc: 0.9903 - val_loss: 0.1209 - val_acc: 0.9963
Epoch 10/40
 - 9s - loss: 0.0907 - acc: 0.9906 - val_loss: 0.1242 - val_acc: 0.9963
Epoch 11/40
 - 9s - loss: 0.0901 - acc: 0.9906 - val_loss: 0.1169 - val_acc: 0.9976
Epoch 12/40
 - 9s - loss: 0.0897 - acc: 0.9906 - val_loss: 0.1084 - val_acc: 0.9963
Epoch 13/40
 - 9s - loss: 0.0881 - acc: 0.9907 - val_loss: 0.1187 - val_acc: 0.9963
Epoch 14/40
 - 9s - loss: 0.0883 - acc: 0.9907 - val_loss: 0.1140 - val_acc: 0.9953
Epoch 15/40
 - 9s - loss: 0.0872 - acc: 0.9909 - val_loss: 0.1260 - val_acc: 0.9963
Epoch 16/40
 - 9s - loss: 0.0867 - acc: 0.9910 - val_loss: 0.1261 - val_acc: 0.9963
Epoch 17/40
 - 9s - loss: 0.0862 - acc: 0.9910 - val_loss: 0.1167 - val_acc: 0.9963
Epoch 18/40
 - 9s - loss: 0.0860 - acc: 0.9910 - val_loss: 0.1180 - val_acc: 0.9963
Epoch 19/40
 - 10s - loss: 0.0858 - acc: 0.9910 - val_loss: 0.1230 - val_acc: 0.9963
Epoch 20/40
 - 9s - loss: 0.0857 - acc: 0.9911 - val_loss: 0.1191 - val_acc: 0.9963
Epoch 21/40
 - 10s - loss: 0.0859 - acc: 0.9911 - val_loss: 0.1168 - val_acc: 0.9976
Epoch 22/40
 - 9s - loss: 0.0847 - acc: 0.9912 - val_loss: 0.1041 - val_acc: 0.9963
Epoch 23/40
 - 9s - loss: 0.0846 - acc: 0.9912 - val_loss: 0.1105 - val_acc: 0.9976
Epoch 24/40
 - 9s - loss: 0.0852 - acc: 0.9912 - val_loss: 0.1095 - val_acc: 0.9963
Epoch 25/40
 - 9s - loss: 0.0842 - acc: 0.9913 - val_loss: 0.1321 - val_acc: 0.9963
Epoch 26/40
 - 9s - loss: 0.0839 - acc: 0.9913 - val_loss: 0.1221 - val_acc: 0.9963
Epoch 27/40
 - 9s - loss: 0.0836 - acc: 0.9913 - val_loss: 0.1209 - val_acc: 0.9976
Epoch 28/40
 - 9s - loss: 0.0839 - acc: 0.9913 - val_loss: 0.1231 - val_acc: 0.9963
Epoch 29/40
 - 9s - loss: 0.0841 - acc: 0.9913 - val_loss: 0.1258 - val_acc: 0.9963
Epoch 30/40
 - 10s - loss: 0.0839 - acc: 0.9913 - val_loss: 0.1211 - val_acc: 0.9963
Epoch 31/40
 - 9s - loss: 0.0835 - acc: 0.9914 - val_loss: 0.1350 - val_acc: 0.9963
Epoch 32/40
 - 9s - loss: 0.0834 - acc: 0.9914 - val_loss: 0.1145 - val_acc: 0.9963
Epoch 33/40
 - 9s - loss: 0.0832 - acc: 0.9915 - val_loss: 0.1293 - val_acc: 0.9963
Epoch 34/40
 - 9s - loss: 0.0830 - acc: 0.9915 - val_loss: 0.1189 - val_acc: 0.9963
Epoch 35/40
 - 9s - loss: 0.0829 - acc: 0.9915 - val_loss: 0.1242 - val_acc: 0.9963
Epoch 36/40
 - 9s - loss: 0.0825 - acc: 0.9914 - val_loss: 0.1264 - val_acc: 0.9963
Epoch 37/40
 - 9s - loss: 0.0827 - acc: 0.9915 - val_loss: 0.1182 - val_acc: 0.9963
Epoch 38/40
 - 9s - loss: 0.0827 - acc: 0.9915 - val_loss: 0.1195 - val_acc: 0.9963
Epoch 39/40
 - 9s - loss: 0.0826 - acc: 0.9915 - val_loss: 0.1137 - val_acc: 0.9963
Epoch 40/40
 - 9s - loss: 0.0822 - acc: 0.9915 - val_loss: 0.1327 - val_acc: 0.9963

==================================================================================================
	Training time : 0:06:52.425203
==================================================================================================
	Identification : 0.346
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 10
	50 : 3
	10 : 18
	5 : 73

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 208
	25 : 6
	10 : 11
	50 : 3
	5 : 66

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EU
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2703, Test : 1404
	MWEs in tain : 945, occurrences : 3323
	Impotant words in tain : 628
	MWE length mean : 2.02
	Seen MWEs : 434 (86 %)
	New MWEs : 66 (13 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4019 * POS : 123
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 200)       803800      input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 15)        1845        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 800)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 60)           0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 860)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 25)           21525       concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 25)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            208         dropout_6[0][0]                  
==================================================================================================
Total params: 827,378
Trainable params: 827,378
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	2703 importatnt sents of 2703
	data size before sampling = 95390
	data size after sampling = 230300
{0.0: 46060, 1.0: 46060, 2.0: 46060, 5.0: 46060, 6.0: 46060}
	Class weights : {0: 1, 1: 1, 2: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 207270 samples, validate on 23030 samples
Epoch 1/40
 - 5s - loss: 0.2934 - acc: 0.9604 - val_loss: 0.2086 - val_acc: 0.9925
Epoch 2/40
 - 5s - loss: 0.1712 - acc: 0.9731 - val_loss: 0.2183 - val_acc: 0.9960
Epoch 3/40
 - 5s - loss: 0.1566 - acc: 0.9746 - val_loss: 0.2145 - val_acc: 0.9952
Epoch 4/40
 - 5s - loss: 0.1499 - acc: 0.9755 - val_loss: 0.1543 - val_acc: 0.9983
Epoch 5/40
 - 5s - loss: 0.1448 - acc: 0.9762 - val_loss: 0.1901 - val_acc: 0.9970
Epoch 6/40
 - 5s - loss: 0.1416 - acc: 0.9766 - val_loss: 0.1751 - val_acc: 0.9959
Epoch 7/40
 - 5s - loss: 0.1393 - acc: 0.9770 - val_loss: 0.2207 - val_acc: 0.9965
Epoch 8/40
 - 5s - loss: 0.1380 - acc: 0.9769 - val_loss: 0.1878 - val_acc: 0.9963
Epoch 9/40
 - 5s - loss: 0.1349 - acc: 0.9773 - val_loss: 0.1965 - val_acc: 0.9963
Epoch 10/40
 - 5s - loss: 0.1333 - acc: 0.9774 - val_loss: 0.1905 - val_acc: 0.9960
Epoch 11/40
 - 5s - loss: 0.1324 - acc: 0.9777 - val_loss: 0.1891 - val_acc: 0.9954
Epoch 12/40
 - 5s - loss: 0.1318 - acc: 0.9778 - val_loss: 0.1627 - val_acc: 0.9970
Epoch 13/40
 - 5s - loss: 0.1307 - acc: 0.9780 - val_loss: 0.1525 - val_acc: 0.9959
Epoch 14/40
 - 5s - loss: 0.1301 - acc: 0.9783 - val_loss: 0.1842 - val_acc: 0.9954
Epoch 15/40
 - 5s - loss: 0.1282 - acc: 0.9783 - val_loss: 0.1890 - val_acc: 0.9960
Epoch 16/40
 - 5s - loss: 0.1275 - acc: 0.9785 - val_loss: 0.1743 - val_acc: 0.9960
Epoch 17/40
 - 5s - loss: 0.1273 - acc: 0.9785 - val_loss: 0.1620 - val_acc: 0.9958
Epoch 18/40
 - 5s - loss: 0.1268 - acc: 0.9786 - val_loss: 0.1748 - val_acc: 0.9964
Epoch 19/40
 - 5s - loss: 0.1260 - acc: 0.9787 - val_loss: 0.1763 - val_acc: 0.9954
Epoch 20/40
 - 5s - loss: 0.1259 - acc: 0.9788 - val_loss: 0.1605 - val_acc: 0.9958
Epoch 21/40
 - 5s - loss: 0.1253 - acc: 0.9788 - val_loss: 0.1647 - val_acc: 0.9969
Epoch 22/40
 - 5s - loss: 0.1246 - acc: 0.9789 - val_loss: 0.1612 - val_acc: 0.9964
Epoch 23/40
 - 5s - loss: 0.1243 - acc: 0.9789 - val_loss: 0.1570 - val_acc: 0.9964
Epoch 24/40
 - 5s - loss: 0.1234 - acc: 0.9792 - val_loss: 0.1846 - val_acc: 0.9958
Epoch 25/40
 - 5s - loss: 0.1238 - acc: 0.9790 - val_loss: 0.1688 - val_acc: 0.9969
Epoch 26/40
 - 5s - loss: 0.1230 - acc: 0.9793 - val_loss: 0.1714 - val_acc: 0.9958
Epoch 27/40
 - 5s - loss: 0.1227 - acc: 0.9793 - val_loss: 0.1684 - val_acc: 0.9958
Epoch 28/40
 - 5s - loss: 0.1225 - acc: 0.9791 - val_loss: 0.1686 - val_acc: 0.9958
Epoch 29/40
 - 5s - loss: 0.1220 - acc: 0.9795 - val_loss: 0.1743 - val_acc: 0.9964
Epoch 30/40
 - 5s - loss: 0.1219 - acc: 0.9794 - val_loss: 0.1678 - val_acc: 0.9958
Epoch 31/40
 - 5s - loss: 0.1218 - acc: 0.9795 - val_loss: 0.1692 - val_acc: 0.9964
Epoch 32/40
 - 5s - loss: 0.1217 - acc: 0.9795 - val_loss: 0.1689 - val_acc: 0.9958
Epoch 33/40
 - 5s - loss: 0.1207 - acc: 0.9795 - val_loss: 0.1623 - val_acc: 0.9964
Epoch 34/40
 - 5s - loss: 0.1207 - acc: 0.9797 - val_loss: 0.1757 - val_acc: 0.9964
Epoch 35/40
 - 5s - loss: 0.1212 - acc: 0.9795 - val_loss: 0.1764 - val_acc: 0.9964
Epoch 36/40
 - 5s - loss: 0.1204 - acc: 0.9796 - val_loss: 0.1676 - val_acc: 0.9964
Epoch 37/40
 - 5s - loss: 0.1203 - acc: 0.9797 - val_loss: 0.1662 - val_acc: 0.9964
Epoch 38/40
 - 5s - loss: 0.1203 - acc: 0.9797 - val_loss: 0.1777 - val_acc: 0.9964
Epoch 39/40
 - 5s - loss: 0.1205 - acc: 0.9796 - val_loss: 0.1623 - val_acc: 0.9964
Epoch 40/40
 - 5s - loss: 0.1193 - acc: 0.9798 - val_loss: 0.1599 - val_acc: 0.9958

==================================================================================================
	Training time : 0:03:41.039270
==================================================================================================
	Identification : 0.709
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 4
	100 : 6
	5 : 39
	10 : 6
	50 : 4
	500 : 3
	25 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 57
	100 : 2
	5 : 65
	10 : 12
	50 : 1
	500 : 2
	25 : 6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FA
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 1965, Test : 359
	MWEs in tain : 1295, occurrences : 2952
	Impotant words in tain : 814
	MWE length mean : 2.14
	Seen MWEs : 330 (65 %)
	New MWEs : 171 (34 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3793 * POS : 77
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 200)       758600      input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 15)        1155        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 800)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 60)           0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 860)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 25)           21525       concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 25)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            208         dropout_7[0][0]                  
==================================================================================================
Total params: 781,488
Trainable params: 781,488
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1965 importatnt sents of 1965
	data size before sampling = 80942
	data size after sampling = 233994
{0.0: 38999, 1.0: 38999, 2.0: 38999, 4.0: 38999, 5.0: 38999, 6.0: 38999}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 210594 samples, validate on 23400 samples
Epoch 1/40
 - 5s - loss: 0.1870 - acc: 0.9697 - val_loss: 0.2744 - val_acc: 0.9985
Epoch 2/40
 - 5s - loss: 0.1080 - acc: 0.9796 - val_loss: 0.2392 - val_acc: 0.9982
Epoch 3/40
 - 5s - loss: 0.1014 - acc: 0.9807 - val_loss: 0.2261 - val_acc: 0.9985
Epoch 4/40
 - 5s - loss: 0.0986 - acc: 0.9810 - val_loss: 0.1946 - val_acc: 0.9989
Epoch 5/40
 - 5s - loss: 0.0966 - acc: 0.9812 - val_loss: 0.2057 - val_acc: 0.9985
Epoch 6/40
 - 5s - loss: 0.0949 - acc: 0.9814 - val_loss: 0.2237 - val_acc: 0.9985
Epoch 7/40
 - 5s - loss: 0.0939 - acc: 0.9816 - val_loss: 0.2390 - val_acc: 0.9988
Epoch 8/40
 - 5s - loss: 0.0925 - acc: 0.9817 - val_loss: 0.2287 - val_acc: 0.9985
Epoch 9/40
 - 5s - loss: 0.0923 - acc: 0.9818 - val_loss: 0.2052 - val_acc: 0.9985
Epoch 10/40
 - 5s - loss: 0.0906 - acc: 0.9821 - val_loss: 0.2246 - val_acc: 0.9985
Epoch 11/40
 - 5s - loss: 0.0904 - acc: 0.9823 - val_loss: 0.2311 - val_acc: 0.9985
Epoch 12/40
 - 5s - loss: 0.0897 - acc: 0.9822 - val_loss: 0.2042 - val_acc: 0.9985
Epoch 13/40
 - 5s - loss: 0.0888 - acc: 0.9824 - val_loss: 0.2195 - val_acc: 0.9985
Epoch 14/40
 - 5s - loss: 0.0888 - acc: 0.9823 - val_loss: 0.2537 - val_acc: 0.9985
Epoch 15/40
 - 5s - loss: 0.0886 - acc: 0.9823 - val_loss: 0.2381 - val_acc: 0.9985
Epoch 16/40
 - 5s - loss: 0.0889 - acc: 0.9824 - val_loss: 0.2288 - val_acc: 0.9985
Epoch 17/40
 - 5s - loss: 0.0874 - acc: 0.9825 - val_loss: 0.2061 - val_acc: 0.9985
Epoch 18/40
 - 5s - loss: 0.0884 - acc: 0.9824 - val_loss: 0.2185 - val_acc: 0.9985
Epoch 19/40
 - 5s - loss: 0.0880 - acc: 0.9825 - val_loss: 0.2097 - val_acc: 0.9985
Epoch 20/40
 - 5s - loss: 0.0877 - acc: 0.9826 - val_loss: 0.2294 - val_acc: 0.9985
Epoch 21/40
 - 5s - loss: 0.0873 - acc: 0.9825 - val_loss: 0.2395 - val_acc: 0.9985
Epoch 22/40
 - 5s - loss: 0.0864 - acc: 0.9827 - val_loss: 0.2237 - val_acc: 0.9985
Epoch 23/40
 - 5s - loss: 0.0871 - acc: 0.9825 - val_loss: 0.2127 - val_acc: 0.9985
Epoch 24/40
 - 5s - loss: 0.0868 - acc: 0.9825 - val_loss: 0.2008 - val_acc: 0.9985
Epoch 25/40
 - 5s - loss: 0.0864 - acc: 0.9826 - val_loss: 0.2163 - val_acc: 0.9985
Epoch 26/40
 - 5s - loss: 0.0861 - acc: 0.9828 - val_loss: 0.2136 - val_acc: 0.9985
Epoch 27/40
 - 5s - loss: 0.0862 - acc: 0.9828 - val_loss: 0.2095 - val_acc: 0.9985
Epoch 28/40
 - 5s - loss: 0.0862 - acc: 0.9827 - val_loss: 0.2213 - val_acc: 0.9985
Epoch 29/40
 - 5s - loss: 0.0862 - acc: 0.9827 - val_loss: 0.2406 - val_acc: 0.9985
Epoch 30/40
 - 5s - loss: 0.0861 - acc: 0.9827 - val_loss: 0.2209 - val_acc: 0.9985
Epoch 31/40
 - 5s - loss: 0.0855 - acc: 0.9828 - val_loss: 0.2183 - val_acc: 0.9985
Epoch 32/40
 - 5s - loss: 0.0858 - acc: 0.9829 - val_loss: 0.2379 - val_acc: 0.9985
Epoch 33/40
 - 5s - loss: 0.0853 - acc: 0.9828 - val_loss: 0.2367 - val_acc: 0.9985
Epoch 34/40
 - 5s - loss: 0.0854 - acc: 0.9828 - val_loss: 0.2426 - val_acc: 0.9985
Epoch 35/40
 - 5s - loss: 0.0854 - acc: 0.9828 - val_loss: 0.2263 - val_acc: 0.9985
Epoch 36/40
 - 5s - loss: 0.0854 - acc: 0.9828 - val_loss: 0.2391 - val_acc: 0.9985
Epoch 37/40
 - 5s - loss: 0.0850 - acc: 0.9829 - val_loss: 0.2293 - val_acc: 0.9985
Epoch 38/40
 - 5s - loss: 0.0849 - acc: 0.9829 - val_loss: 0.2224 - val_acc: 0.9985
Epoch 39/40
 - 5s - loss: 0.0845 - acc: 0.9829 - val_loss: 0.2305 - val_acc: 0.9985
Epoch 40/40
 - 5s - loss: 0.0850 - acc: 0.9828 - val_loss: 0.2338 - val_acc: 0.9985

==================================================================================================
	Training time : 0:03:41.190825
==================================================================================================
	Identification : 0.621
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 29
	100 : 1
	5 : 97
	10 : 27
	50 : 4
	25 : 18

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 136
	100 : 1
	5 : 62
	10 : 11
	50 : 2
	25 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 4426, Test : 1606
	MWEs in tain : 1773, occurrences : 5179
	Impotant words in tain : 1295
	MWE length mean : 2.29
	Seen MWEs : 251 (50 %)
	New MWEs : 247 (49 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8159 * POS : 224
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 200)       1631800     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 15)        3360        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 800)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 60)           0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 860)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 25)           21525       concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 25)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            208         dropout_8[0][0]                  
==================================================================================================
Total params: 1,656,893
Trainable params: 1,656,893
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	4426 importatnt sents of 4426
	data size before sampling = 271347
	data size after sampling = 798696
{0.0: 133116, 1.0: 133116, 2.0: 133116, 4.0: 133116, 5.0: 133116, 6.0: 133116}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 718826 samples, validate on 79870 samples
Epoch 1/40
 - 18s - loss: 0.1594 - acc: 0.9798 - val_loss: 0.0698 - val_acc: 0.9975
Epoch 2/40
 - 19s - loss: 0.0902 - acc: 0.9876 - val_loss: 0.0496 - val_acc: 0.9987
Epoch 3/40
 - 19s - loss: 0.0830 - acc: 0.9886 - val_loss: 0.0532 - val_acc: 0.9994
Epoch 4/40
 - 19s - loss: 0.0798 - acc: 0.9890 - val_loss: 0.0461 - val_acc: 0.9988
Epoch 5/40
 - 19s - loss: 0.0774 - acc: 0.9894 - val_loss: 0.0393 - val_acc: 0.9994
Epoch 6/40
 - 18s - loss: 0.0756 - acc: 0.9896 - val_loss: 0.0519 - val_acc: 0.9982
Epoch 7/40
 - 18s - loss: 0.0748 - acc: 0.9898 - val_loss: 0.0628 - val_acc: 0.9982
Epoch 8/40
 - 18s - loss: 0.0737 - acc: 0.9899 - val_loss: 0.0528 - val_acc: 0.9982
Epoch 9/40
 - 18s - loss: 0.0729 - acc: 0.9900 - val_loss: 0.0536 - val_acc: 0.9982
Epoch 10/40
 - 18s - loss: 0.0726 - acc: 0.9901 - val_loss: 0.0575 - val_acc: 0.9982
Epoch 11/40
 - 18s - loss: 0.0720 - acc: 0.9902 - val_loss: 0.0503 - val_acc: 0.9988
Epoch 12/40
 - 18s - loss: 0.0712 - acc: 0.9903 - val_loss: 0.0561 - val_acc: 0.9982
Epoch 13/40
 - 18s - loss: 0.0707 - acc: 0.9903 - val_loss: 0.0576 - val_acc: 0.9982
Epoch 14/40
 - 18s - loss: 0.0705 - acc: 0.9904 - val_loss: 0.0572 - val_acc: 0.9982
Epoch 15/40
 - 18s - loss: 0.0702 - acc: 0.9904 - val_loss: 0.0487 - val_acc: 0.9982
Epoch 16/40
 - 18s - loss: 0.0701 - acc: 0.9904 - val_loss: 0.0529 - val_acc: 0.9982
Epoch 17/40
 - 18s - loss: 0.0696 - acc: 0.9905 - val_loss: 0.0576 - val_acc: 0.9982
Epoch 18/40
 - 18s - loss: 0.0695 - acc: 0.9905 - val_loss: 0.0524 - val_acc: 0.9982
Epoch 19/40
 - 18s - loss: 0.0695 - acc: 0.9905 - val_loss: 0.0506 - val_acc: 0.9982
Epoch 20/40
 - 18s - loss: 0.0690 - acc: 0.9905 - val_loss: 0.0491 - val_acc: 0.9982
Epoch 21/40
 - 18s - loss: 0.0690 - acc: 0.9905 - val_loss: 0.0595 - val_acc: 0.9982
Epoch 22/40
 - 18s - loss: 0.0688 - acc: 0.9905 - val_loss: 0.0511 - val_acc: 0.9988
Epoch 23/40
 - 18s - loss: 0.0685 - acc: 0.9906 - val_loss: 0.0486 - val_acc: 0.9982
Epoch 24/40
 - 18s - loss: 0.0684 - acc: 0.9906 - val_loss: 0.0539 - val_acc: 0.9982
Epoch 25/40
 - 18s - loss: 0.0683 - acc: 0.9907 - val_loss: 0.0566 - val_acc: 0.9982
Epoch 26/40
 - 18s - loss: 0.0681 - acc: 0.9907 - val_loss: 0.0525 - val_acc: 0.9982
Epoch 27/40
 - 18s - loss: 0.0681 - acc: 0.9907 - val_loss: 0.0548 - val_acc: 0.9982
Epoch 28/40
 - 18s - loss: 0.0680 - acc: 0.9907 - val_loss: 0.0558 - val_acc: 0.9982
Epoch 29/40
 - 18s - loss: 0.0676 - acc: 0.9907 - val_loss: 0.0561 - val_acc: 0.9982
Epoch 30/40
 - 18s - loss: 0.0675 - acc: 0.9908 - val_loss: 0.0556 - val_acc: 0.9982
Epoch 31/40
 - 18s - loss: 0.0674 - acc: 0.9908 - val_loss: 0.0569 - val_acc: 0.9982
Epoch 32/40
 - 18s - loss: 0.0674 - acc: 0.9907 - val_loss: 0.0573 - val_acc: 0.9982
Epoch 33/40
 - 18s - loss: 0.0673 - acc: 0.9908 - val_loss: 0.0595 - val_acc: 0.9982
Epoch 34/40
 - 18s - loss: 0.0673 - acc: 0.9908 - val_loss: 0.0582 - val_acc: 0.9982
Epoch 35/40
 - 18s - loss: 0.0672 - acc: 0.9908 - val_loss: 0.0557 - val_acc: 0.9982
Epoch 36/40
 - 18s - loss: 0.0671 - acc: 0.9908 - val_loss: 0.0521 - val_acc: 0.9982
Epoch 37/40
 - 18s - loss: 0.0667 - acc: 0.9909 - val_loss: 0.0579 - val_acc: 0.9982
Epoch 38/40
 - 18s - loss: 0.0670 - acc: 0.9908 - val_loss: 0.0598 - val_acc: 0.9982
Epoch 39/40
 - 18s - loss: 0.0667 - acc: 0.9909 - val_loss: 0.0542 - val_acc: 0.9982
Epoch 40/40
 - 18s - loss: 0.0666 - acc: 0.9908 - val_loss: 0.0584 - val_acc: 0.9982

==================================================================================================
	Training time : 0:14:05.755427
==================================================================================================
	Identification : 0.444
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 14
	100 : 3
	5 : 51
	10 : 20
	50 : 8
	500 : 2
	25 : 13

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 103
	100 : 1
	5 : 48
	10 : 12
	50 : 3
	500 : 2
	25 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HE
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 1593, Test : 3209
	MWEs in tain : 1235, occurrences : 1737
	Impotant words in tain : 1666
	MWE length mean : 2.39
	Seen MWEs : 204 (40 %)
	New MWEs : 298 (59 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 6500 * POS : 321
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 200)       1300000     input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 15)        4815        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 800)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 60)           0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 860)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 25)           21525       concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 25)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            208         dropout_9[0][0]                  
==================================================================================================
Total params: 1,326,548
Trainable params: 1,326,548
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1593 importatnt sents of 1593
	data size before sampling = 77521
	data size after sampling = 227418
{0.0: 37903, 1.0: 37903, 2.0: 37903, 3.0: 37903, 5.0: 37903, 6.0: 37903}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 204676 samples, validate on 22742 samples
Epoch 1/40
 - 5s - loss: 0.3398 - acc: 0.9643 - val_loss: 0.1420 - val_acc: 0.9967
Epoch 2/40
 - 5s - loss: 0.1315 - acc: 0.9821 - val_loss: 0.0741 - val_acc: 0.9980
Epoch 3/40
 - 5s - loss: 0.1102 - acc: 0.9853 - val_loss: 0.0927 - val_acc: 0.9980
Epoch 4/40
 - 5s - loss: 0.1022 - acc: 0.9866 - val_loss: 0.0962 - val_acc: 0.9980
Epoch 5/40
 - 5s - loss: 0.0968 - acc: 0.9876 - val_loss: 0.0866 - val_acc: 0.9980
Epoch 6/40
 - 5s - loss: 0.0943 - acc: 0.9880 - val_loss: 0.0933 - val_acc: 0.9980
Epoch 7/40
 - 5s - loss: 0.0920 - acc: 0.9883 - val_loss: 0.0794 - val_acc: 0.9980
Epoch 8/40
 - 5s - loss: 0.0892 - acc: 0.9887 - val_loss: 0.1047 - val_acc: 0.9980
Epoch 9/40
 - 5s - loss: 0.0884 - acc: 0.9888 - val_loss: 0.0761 - val_acc: 0.9980
Epoch 10/40
 - 5s - loss: 0.0880 - acc: 0.9889 - val_loss: 0.0970 - val_acc: 0.9980
Epoch 11/40
 - 5s - loss: 0.0865 - acc: 0.9890 - val_loss: 0.0981 - val_acc: 0.9980
Epoch 12/40
 - 5s - loss: 0.0856 - acc: 0.9891 - val_loss: 0.0767 - val_acc: 0.9980
Epoch 13/40
 - 5s - loss: 0.0848 - acc: 0.9891 - val_loss: 0.0731 - val_acc: 0.9980
Epoch 14/40
 - 5s - loss: 0.0843 - acc: 0.9893 - val_loss: 0.0795 - val_acc: 0.9980
Epoch 15/40
 - 5s - loss: 0.0828 - acc: 0.9895 - val_loss: 0.0871 - val_acc: 0.9980
Epoch 16/40
 - 5s - loss: 0.0829 - acc: 0.9895 - val_loss: 0.0872 - val_acc: 0.9980
Epoch 17/40
 - 5s - loss: 0.0823 - acc: 0.9895 - val_loss: 0.0867 - val_acc: 0.9980
Epoch 18/40
 - 5s - loss: 0.0819 - acc: 0.9895 - val_loss: 0.0876 - val_acc: 0.9980
Epoch 19/40
 - 5s - loss: 0.0812 - acc: 0.9896 - val_loss: 0.0798 - val_acc: 0.9980
Epoch 20/40
 - 5s - loss: 0.0806 - acc: 0.9896 - val_loss: 0.0930 - val_acc: 0.9980
Epoch 21/40
 - 5s - loss: 0.0809 - acc: 0.9896 - val_loss: 0.0814 - val_acc: 0.9980
Epoch 22/40
 - 5s - loss: 0.0806 - acc: 0.9898 - val_loss: 0.0914 - val_acc: 0.9980
Epoch 23/40
 - 5s - loss: 0.0800 - acc: 0.9897 - val_loss: 0.0789 - val_acc: 0.9980
Epoch 24/40
 - 5s - loss: 0.0801 - acc: 0.9896 - val_loss: 0.0841 - val_acc: 0.9980
Epoch 25/40
 - 5s - loss: 0.0791 - acc: 0.9898 - val_loss: 0.0905 - val_acc: 0.9980
Epoch 26/40
 - 5s - loss: 0.0789 - acc: 0.9897 - val_loss: 0.0910 - val_acc: 0.9980
Epoch 27/40
 - 5s - loss: 0.0788 - acc: 0.9898 - val_loss: 0.0988 - val_acc: 0.9980
Epoch 28/40
 - 5s - loss: 0.0783 - acc: 0.9899 - val_loss: 0.0905 - val_acc: 0.9980
Epoch 29/40
 - 5s - loss: 0.0777 - acc: 0.9899 - val_loss: 0.0928 - val_acc: 0.9980
Epoch 30/40
 - 5s - loss: 0.0782 - acc: 0.9899 - val_loss: 0.0876 - val_acc: 0.9980
Epoch 31/40
 - 5s - loss: 0.0776 - acc: 0.9900 - val_loss: 0.0869 - val_acc: 0.9980
Epoch 32/40
 - 5s - loss: 0.0771 - acc: 0.9901 - val_loss: 0.0769 - val_acc: 0.9980
Epoch 33/40
 - 5s - loss: 0.0774 - acc: 0.9900 - val_loss: 0.0709 - val_acc: 0.9980
Epoch 34/40
 - 5s - loss: 0.0770 - acc: 0.9900 - val_loss: 0.0845 - val_acc: 0.9980
Epoch 35/40
 - 5s - loss: 0.0771 - acc: 0.9900 - val_loss: 0.0944 - val_acc: 0.9980
Epoch 36/40
 - 5s - loss: 0.0772 - acc: 0.9900 - val_loss: 0.0772 - val_acc: 0.9980
Epoch 37/40
 - 5s - loss: 0.0770 - acc: 0.9900 - val_loss: 0.0762 - val_acc: 0.9980
Epoch 38/40
 - 5s - loss: 0.0770 - acc: 0.9899 - val_loss: 0.0834 - val_acc: 0.9980
Epoch 39/40
 - 5s - loss: 0.0767 - acc: 0.9900 - val_loss: 0.0872 - val_acc: 0.9980
Epoch 40/40
 - 5s - loss: 0.0765 - acc: 0.9901 - val_loss: 0.0861 - val_acc: 0.9980

==================================================================================================
	Training time : 0:03:49.510317
==================================================================================================
	Identification : 0.402
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 4
	25 : 5
	10 : 16
	5 : 65

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 277
	25 : 1
	10 : 5
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HI
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 418, Test : 828
	MWEs in tain : 265, occurrences : 534
	Impotant words in tain : 247
	MWE length mean : 2.14
	Seen MWEs : 282 (56 %)
	New MWEs : 218 (43 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 1268 * POS : 46
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 200)       253600      input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 15)        690         input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 800)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 60)           0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 860)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 25)           21525       concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 25)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            208         dropout_10[0][0]                 
==================================================================================================
Total params: 276,023
Trainable params: 276,023
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	418 importatnt sents of 418
	data size before sampling = 19978
	data size after sampling = 48780
{0.0: 9756, 1.0: 9756, 2.0: 9756, 5.0: 9756, 6.0: 9756}
	Class weights : {0: 1, 1: 1, 2: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 43902 samples, validate on 4878 samples
Epoch 1/40
 - 1s - loss: 0.2590 - acc: 0.9600 - val_loss: 0.0859 - val_acc: 1.0000
Epoch 2/40
 - 1s - loss: 0.0952 - acc: 0.9846 - val_loss: 0.0987 - val_acc: 1.0000
Epoch 3/40
 - 1s - loss: 0.0852 - acc: 0.9857 - val_loss: 0.0692 - val_acc: 1.0000
Epoch 4/40
 - 1s - loss: 0.0818 - acc: 0.9862 - val_loss: 0.0765 - val_acc: 1.0000
Epoch 5/40
 - 1s - loss: 0.0806 - acc: 0.9864 - val_loss: 0.0802 - val_acc: 1.0000
Epoch 6/40
 - 1s - loss: 0.0788 - acc: 0.9864 - val_loss: 0.0996 - val_acc: 1.0000
Epoch 7/40
 - 1s - loss: 0.0772 - acc: 0.9868 - val_loss: 0.0663 - val_acc: 1.0000
Epoch 8/40
 - 1s - loss: 0.0762 - acc: 0.9869 - val_loss: 0.0750 - val_acc: 1.0000
Epoch 9/40
 - 1s - loss: 0.0757 - acc: 0.9872 - val_loss: 0.0768 - val_acc: 1.0000
Epoch 10/40
 - 1s - loss: 0.0748 - acc: 0.9872 - val_loss: 0.0943 - val_acc: 1.0000
Epoch 11/40
 - 1s - loss: 0.0752 - acc: 0.9871 - val_loss: 0.0725 - val_acc: 1.0000
Epoch 12/40
 - 1s - loss: 0.0748 - acc: 0.9874 - val_loss: 0.0956 - val_acc: 1.0000
Epoch 13/40
 - 1s - loss: 0.0738 - acc: 0.9872 - val_loss: 0.0913 - val_acc: 1.0000
Epoch 14/40
 - 1s - loss: 0.0735 - acc: 0.9872 - val_loss: 0.0903 - val_acc: 1.0000
Epoch 15/40
 - 1s - loss: 0.0729 - acc: 0.9874 - val_loss: 0.0887 - val_acc: 1.0000
Epoch 16/40
 - 1s - loss: 0.0729 - acc: 0.9871 - val_loss: 0.0899 - val_acc: 1.0000
Epoch 17/40
 - 1s - loss: 0.0733 - acc: 0.9874 - val_loss: 0.0907 - val_acc: 1.0000
Epoch 18/40
 - 1s - loss: 0.0721 - acc: 0.9876 - val_loss: 0.0901 - val_acc: 1.0000
Epoch 19/40
 - 1s - loss: 0.0718 - acc: 0.9875 - val_loss: 0.0833 - val_acc: 1.0000
Epoch 20/40
 - 1s - loss: 0.0726 - acc: 0.9876 - val_loss: 0.0766 - val_acc: 1.0000
Epoch 21/40
 - 1s - loss: 0.0719 - acc: 0.9877 - val_loss: 0.0780 - val_acc: 1.0000
Epoch 22/40
 - 1s - loss: 0.0717 - acc: 0.9877 - val_loss: 0.0775 - val_acc: 1.0000
Epoch 23/40
 - 1s - loss: 0.0716 - acc: 0.9877 - val_loss: 0.0836 - val_acc: 1.0000
Epoch 24/40
 - 1s - loss: 0.0711 - acc: 0.9877 - val_loss: 0.0985 - val_acc: 1.0000
Epoch 25/40
 - 1s - loss: 0.0716 - acc: 0.9876 - val_loss: 0.0814 - val_acc: 1.0000
Epoch 26/40
 - 1s - loss: 0.0719 - acc: 0.9877 - val_loss: 0.1021 - val_acc: 1.0000
Epoch 27/40
 - 1s - loss: 0.0706 - acc: 0.9878 - val_loss: 0.0898 - val_acc: 1.0000
Epoch 28/40
 - 1s - loss: 0.0710 - acc: 0.9877 - val_loss: 0.0844 - val_acc: 1.0000
Epoch 29/40
 - 1s - loss: 0.0712 - acc: 0.9878 - val_loss: 0.1026 - val_acc: 1.0000
Epoch 30/40
 - 1s - loss: 0.0716 - acc: 0.9878 - val_loss: 0.0746 - val_acc: 1.0000
Epoch 31/40
 - 1s - loss: 0.0716 - acc: 0.9878 - val_loss: 0.0832 - val_acc: 1.0000
Epoch 32/40
 - 1s - loss: 0.0706 - acc: 0.9877 - val_loss: 0.0802 - val_acc: 1.0000
Epoch 33/40
 - 1s - loss: 0.0703 - acc: 0.9878 - val_loss: 0.1028 - val_acc: 1.0000
Epoch 34/40
 - 1s - loss: 0.0707 - acc: 0.9878 - val_loss: 0.1063 - val_acc: 1.0000
Epoch 35/40
 - 1s - loss: 0.0705 - acc: 0.9879 - val_loss: 0.1016 - val_acc: 1.0000
Epoch 36/40
 - 1s - loss: 0.0704 - acc: 0.9878 - val_loss: 0.0956 - val_acc: 1.0000
Epoch 37/40
 - 1s - loss: 0.0708 - acc: 0.9878 - val_loss: 0.1080 - val_acc: 1.0000
Epoch 38/40
 - 1s - loss: 0.0705 - acc: 0.9879 - val_loss: 0.0859 - val_acc: 1.0000
Epoch 39/40
 - 1s - loss: 0.0705 - acc: 0.9880 - val_loss: 0.0827 - val_acc: 1.0000
Epoch 40/40
 - 1s - loss: 0.0694 - acc: 0.9881 - val_loss: 0.0936 - val_acc: 1.0000

==================================================================================================
	Training time : 0:00:44.021990
==================================================================================================
	Identification : 0.573
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 5
	25 : 1
	50 : 2
	10 : 9
	5 : 74

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 176
	25 : 1
	50 : 2
	10 : 1
	5 : 29

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 1363, Test : 708
	MWEs in tain : 1133, occurrences : 1950
	Impotant words in tain : 901
	MWE length mean : 2.2
	Seen MWEs : 289 (57 %)
	New MWEs : 212 (42 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4338 * POS : 121
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 200)       867600      input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 15)        1815        input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 800)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 60)           0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 860)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 25)           21525       concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 25)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            208         dropout_11[0][0]                 
==================================================================================================
Total params: 891,148
Trainable params: 891,148
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1363 importatnt sents of 1363
	data size before sampling = 72048
	data size after sampling = 245840
{0.0: 35120, 1.0: 35120, 2.0: 35120, 3.0: 35120, 4.0: 35120, 5.0: 35120, 6.0: 35120}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 221256 samples, validate on 24584 samples
Epoch 1/40
 - 5s - loss: 0.3334 - acc: 0.9631 - val_loss: 0.1813 - val_acc: 0.9914
Epoch 2/40
 - 5s - loss: 0.1644 - acc: 0.9803 - val_loss: 0.1618 - val_acc: 0.9935
Epoch 3/40
 - 5s - loss: 0.1490 - acc: 0.9827 - val_loss: 0.1365 - val_acc: 0.9907
Epoch 4/40
 - 5s - loss: 0.1423 - acc: 0.9834 - val_loss: 0.1266 - val_acc: 0.9887
Epoch 5/40
 - 5s - loss: 0.1365 - acc: 0.9842 - val_loss: 0.1128 - val_acc: 0.9935
Epoch 6/40
 - 5s - loss: 0.1335 - acc: 0.9850 - val_loss: 0.1482 - val_acc: 0.9908
Epoch 7/40
 - 5s - loss: 0.1301 - acc: 0.9855 - val_loss: 0.1254 - val_acc: 0.9931
Epoch 8/40
 - 5s - loss: 0.1272 - acc: 0.9857 - val_loss: 0.1177 - val_acc: 0.9977
Epoch 9/40
 - 5s - loss: 0.1260 - acc: 0.9859 - val_loss: 0.1245 - val_acc: 0.9954
Epoch 10/40
 - 5s - loss: 0.1245 - acc: 0.9862 - val_loss: 0.1113 - val_acc: 0.9977
Epoch 11/40
 - 5s - loss: 0.1234 - acc: 0.9864 - val_loss: 0.1155 - val_acc: 0.9977
Epoch 12/40
 - 5s - loss: 0.1225 - acc: 0.9864 - val_loss: 0.1078 - val_acc: 0.9974
Epoch 13/40
 - 5s - loss: 0.1217 - acc: 0.9868 - val_loss: 0.1206 - val_acc: 0.9954
Epoch 14/40
 - 5s - loss: 0.1213 - acc: 0.9868 - val_loss: 0.1173 - val_acc: 0.9935
Epoch 15/40
 - 5s - loss: 0.1185 - acc: 0.9869 - val_loss: 0.1350 - val_acc: 0.9908
Epoch 16/40
 - 5s - loss: 0.1186 - acc: 0.9871 - val_loss: 0.1286 - val_acc: 0.9908
Epoch 17/40
 - 5s - loss: 0.1180 - acc: 0.9872 - val_loss: 0.1016 - val_acc: 0.9957
Epoch 18/40
 - 5s - loss: 0.1181 - acc: 0.9871 - val_loss: 0.1225 - val_acc: 0.9914
Epoch 19/40
 - 5s - loss: 0.1170 - acc: 0.9873 - val_loss: 0.1125 - val_acc: 0.9954
Epoch 20/40
 - 5s - loss: 0.1171 - acc: 0.9872 - val_loss: 0.1124 - val_acc: 0.9954
Epoch 21/40
 - 5s - loss: 0.1168 - acc: 0.9873 - val_loss: 0.1281 - val_acc: 0.9954
Epoch 22/40
 - 5s - loss: 0.1160 - acc: 0.9875 - val_loss: 0.1329 - val_acc: 0.9935
Epoch 23/40
 - 5s - loss: 0.1157 - acc: 0.9874 - val_loss: 0.1369 - val_acc: 0.9914
Epoch 24/40
 - 5s - loss: 0.1155 - acc: 0.9875 - val_loss: 0.1283 - val_acc: 0.9935
Epoch 25/40
 - 5s - loss: 0.1142 - acc: 0.9876 - val_loss: 0.1385 - val_acc: 0.9908
Epoch 26/40
 - 5s - loss: 0.1145 - acc: 0.9875 - val_loss: 0.1261 - val_acc: 0.9954
Epoch 27/40
 - 5s - loss: 0.1144 - acc: 0.9877 - val_loss: 0.1257 - val_acc: 0.9954
Epoch 28/40
 - 5s - loss: 0.1141 - acc: 0.9877 - val_loss: 0.1244 - val_acc: 0.9954
Epoch 29/40
 - 5s - loss: 0.1135 - acc: 0.9879 - val_loss: 0.1185 - val_acc: 0.9954
Epoch 30/40
 - 5s - loss: 0.1131 - acc: 0.9878 - val_loss: 0.1140 - val_acc: 0.9954
Epoch 31/40
 - 5s - loss: 0.1132 - acc: 0.9878 - val_loss: 0.1349 - val_acc: 0.9935
Epoch 32/40
 - 5s - loss: 0.1132 - acc: 0.9880 - val_loss: 0.1227 - val_acc: 0.9954
Epoch 33/40
 - 5s - loss: 0.1122 - acc: 0.9878 - val_loss: 0.1314 - val_acc: 0.9954
Epoch 34/40
 - 5s - loss: 0.1121 - acc: 0.9879 - val_loss: 0.1459 - val_acc: 0.9887
Epoch 35/40
 - 5s - loss: 0.1125 - acc: 0.9878 - val_loss: 0.1348 - val_acc: 0.9934
Epoch 36/40
 - 5s - loss: 0.1118 - acc: 0.9880 - val_loss: 0.1280 - val_acc: 0.9935
Epoch 37/40
 - 5s - loss: 0.1117 - acc: 0.9880 - val_loss: 0.1256 - val_acc: 0.9954
Epoch 38/40
 - 5s - loss: 0.1113 - acc: 0.9880 - val_loss: 0.1139 - val_acc: 0.9954
Epoch 39/40
 - 5s - loss: 0.1118 - acc: 0.9880 - val_loss: 0.1221 - val_acc: 0.9954
Epoch 40/40
 - 5s - loss: 0.1117 - acc: 0.9880 - val_loss: 0.1179 - val_acc: 0.9954

==================================================================================================
	Training time : 0:03:52.885794
==================================================================================================
	Identification : 0.456
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1
	25 : 10
	10 : 19
	50 : 1
	5 : 62

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 189
	25 : 8
	10 : 12
	50 : 1
	5 : 69

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HU
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3595, Test : 755
	MWEs in tain : 745, occurrences : 6984
	Impotant words in tain : 602
	MWE length mean : 1.26
	Seen MWEs : 707 (91 %)
	New MWEs : 69 (8 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4298 * POS : 50
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 200)       859600      input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 15)        750         input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 800)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 60)           0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 860)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 25)           21525       concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 25)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            208         dropout_12[0][0]                 
==================================================================================================
Total params: 882,083
Trainable params: 882,083
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	3595 importatnt sents of 3595
	data size before sampling = 210093
	data size after sampling = 609606
{0.0: 101601, 1.0: 101601, 2.0: 101601, 3.0: 101601, 5.0: 101601, 6.0: 101601}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 548645 samples, validate on 60961 samples
Epoch 1/40
 - 13s - loss: 0.2021 - acc: 0.9759 - val_loss: 0.0528 - val_acc: 0.9969
Epoch 2/40
 - 13s - loss: 0.1447 - acc: 0.9826 - val_loss: 0.0555 - val_acc: 0.9969
Epoch 3/40
 - 13s - loss: 0.1390 - acc: 0.9833 - val_loss: 0.0637 - val_acc: 0.9969
Epoch 4/40
 - 13s - loss: 0.1365 - acc: 0.9836 - val_loss: 0.0547 - val_acc: 0.9969
Epoch 5/40
 - 13s - loss: 0.1351 - acc: 0.9838 - val_loss: 0.0650 - val_acc: 0.9969
Epoch 6/40
 - 13s - loss: 0.1333 - acc: 0.9839 - val_loss: 0.0782 - val_acc: 0.9969
Epoch 7/40
 - 13s - loss: 0.1326 - acc: 0.9840 - val_loss: 0.0651 - val_acc: 0.9969
Epoch 8/40
 - 13s - loss: 0.1319 - acc: 0.9841 - val_loss: 0.0600 - val_acc: 0.9969
Epoch 9/40
 - 13s - loss: 0.1311 - acc: 0.9842 - val_loss: 0.0738 - val_acc: 0.9969
Epoch 10/40
 - 13s - loss: 0.1307 - acc: 0.9843 - val_loss: 0.0591 - val_acc: 0.9969
Epoch 11/40
 - 13s - loss: 0.1297 - acc: 0.9843 - val_loss: 0.0623 - val_acc: 0.9969
Epoch 12/40
 - 13s - loss: 0.1299 - acc: 0.9844 - val_loss: 0.0736 - val_acc: 0.9969
Epoch 13/40
 - 13s - loss: 0.1293 - acc: 0.9844 - val_loss: 0.0670 - val_acc: 0.9969
Epoch 14/40
 - 13s - loss: 0.1292 - acc: 0.9844 - val_loss: 0.0698 - val_acc: 0.9969
Epoch 15/40
 - 13s - loss: 0.1287 - acc: 0.9845 - val_loss: 0.0563 - val_acc: 0.9969
Epoch 16/40
 - 13s - loss: 0.1284 - acc: 0.9845 - val_loss: 0.0587 - val_acc: 0.9969
Epoch 17/40
 - 13s - loss: 0.1281 - acc: 0.9845 - val_loss: 0.0615 - val_acc: 0.9969
Epoch 18/40
 - 13s - loss: 0.1282 - acc: 0.9845 - val_loss: 0.0678 - val_acc: 0.9969
Epoch 19/40
 - 13s - loss: 0.1277 - acc: 0.9846 - val_loss: 0.0578 - val_acc: 0.9969
Epoch 20/40
 - 13s - loss: 0.1273 - acc: 0.9846 - val_loss: 0.0629 - val_acc: 0.9969
Epoch 21/40
 - 13s - loss: 0.1269 - acc: 0.9846 - val_loss: 0.0694 - val_acc: 0.9969
Epoch 22/40
 - 13s - loss: 0.1272 - acc: 0.9846 - val_loss: 0.0608 - val_acc: 0.9969
Epoch 23/40
 - 13s - loss: 0.1269 - acc: 0.9847 - val_loss: 0.0608 - val_acc: 0.9969
Epoch 24/40
 - 13s - loss: 0.1265 - acc: 0.9847 - val_loss: 0.0626 - val_acc: 0.9969
Epoch 25/40
 - 13s - loss: 0.1263 - acc: 0.9847 - val_loss: 0.0716 - val_acc: 0.9969
Epoch 26/40
 - 13s - loss: 0.1265 - acc: 0.9847 - val_loss: 0.0677 - val_acc: 0.9969
Epoch 27/40
 - 13s - loss: 0.1264 - acc: 0.9847 - val_loss: 0.0700 - val_acc: 0.9969
Epoch 28/40
 - 13s - loss: 0.1261 - acc: 0.9847 - val_loss: 0.0703 - val_acc: 0.9969
Epoch 29/40
 - 13s - loss: 0.1264 - acc: 0.9847 - val_loss: 0.0655 - val_acc: 0.9969
Epoch 30/40
 - 13s - loss: 0.1257 - acc: 0.9847 - val_loss: 0.0652 - val_acc: 0.9969
Epoch 31/40
 - 13s - loss: 0.1257 - acc: 0.9847 - val_loss: 0.0638 - val_acc: 0.9969
Epoch 32/40
 - 13s - loss: 0.1256 - acc: 0.9848 - val_loss: 0.0651 - val_acc: 0.9969
Epoch 33/40
 - 13s - loss: 0.1256 - acc: 0.9848 - val_loss: 0.0685 - val_acc: 0.9969
Epoch 34/40
 - 13s - loss: 0.1252 - acc: 0.9848 - val_loss: 0.0629 - val_acc: 0.9969
Epoch 35/40
 - 13s - loss: 0.1253 - acc: 0.9848 - val_loss: 0.0635 - val_acc: 0.9969
Epoch 36/40
 - 13s - loss: 0.1253 - acc: 0.9848 - val_loss: 0.0675 - val_acc: 0.9969
Epoch 37/40
 - 13s - loss: 0.1254 - acc: 0.9848 - val_loss: 0.0701 - val_acc: 0.9969
Epoch 38/40
 - 13s - loss: 0.1251 - acc: 0.9848 - val_loss: 0.0662 - val_acc: 0.9969
Epoch 39/40
 - 13s - loss: 0.1248 - acc: 0.9849 - val_loss: 0.0620 - val_acc: 0.9969
Epoch 40/40
 - 13s - loss: 0.1249 - acc: 0.9848 - val_loss: 0.0653 - val_acc: 0.9969

==================================================================================================
	Training time : 0:09:49.917537
==================================================================================================
	Identification : 0.814
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	100 : 7
	5 : 59
	10 : 19
	50 : 16
	500 : 6
	25 : 39
	None : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 57
	100 : 3
	5 : 45
	10 : 15
	50 : 11
	500 : 5
	25 : 20
	None : 1

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : IT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2824, Test : 1256
	MWEs in tain : 1647, occurrences : 3754
	Impotant words in tain : 1232
	MWE length mean : 2.48
	Seen MWEs : 302 (60 %)
	New MWEs : 201 (39 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7968 * POS : 258
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 200)       1593600     input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 15)        3870        input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 800)          0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 60)           0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 860)          0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 25)           21525       concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 25)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            208         dropout_13[0][0]                 
==================================================================================================
Total params: 1,619,203
Trainable params: 1,619,203
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	2824 importatnt sents of 2824
	data size before sampling = 235420
	data size after sampling = 811720
{0.0: 115960, 1.0: 115960, 2.0: 115960, 3.0: 115960, 4.0: 115960, 5.0: 115960, 6.0: 115960}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 730548 samples, validate on 81172 samples
Epoch 1/40
 - 18s - loss: 0.2004 - acc: 0.9814 - val_loss: 0.2592 - val_acc: 0.9929
Epoch 2/40
 - 18s - loss: 0.1207 - acc: 0.9879 - val_loss: 0.2196 - val_acc: 0.9916
Epoch 3/40
 - 18s - loss: 0.1119 - acc: 0.9891 - val_loss: 0.2289 - val_acc: 0.9929
Epoch 4/40
 - 18s - loss: 0.1073 - acc: 0.9897 - val_loss: 0.1638 - val_acc: 0.9929
Epoch 5/40
 - 18s - loss: 0.1043 - acc: 0.9901 - val_loss: 0.2289 - val_acc: 0.9929
Epoch 6/40
 - 18s - loss: 0.1016 - acc: 0.9904 - val_loss: 0.2388 - val_acc: 0.9916
Epoch 7/40
 - 18s - loss: 0.1007 - acc: 0.9906 - val_loss: 0.2075 - val_acc: 0.9929
Epoch 8/40
 - 18s - loss: 0.0991 - acc: 0.9908 - val_loss: 0.2255 - val_acc: 0.9916
Epoch 9/40
 - 18s - loss: 0.0982 - acc: 0.9909 - val_loss: 0.2119 - val_acc: 0.9916
Epoch 10/40
 - 18s - loss: 0.0980 - acc: 0.9910 - val_loss: 0.1834 - val_acc: 0.9929
Epoch 11/40
 - 18s - loss: 0.0970 - acc: 0.9911 - val_loss: 0.2025 - val_acc: 0.9916
Epoch 12/40
 - 18s - loss: 0.0964 - acc: 0.9911 - val_loss: 0.1991 - val_acc: 0.9929
Epoch 13/40
 - 18s - loss: 0.0958 - acc: 0.9911 - val_loss: 0.2069 - val_acc: 0.9929
Epoch 14/40
 - 18s - loss: 0.0956 - acc: 0.9913 - val_loss: 0.1923 - val_acc: 0.9916
Epoch 15/40
 - 18s - loss: 0.0949 - acc: 0.9913 - val_loss: 0.2082 - val_acc: 0.9916
Epoch 16/40
 - 18s - loss: 0.0949 - acc: 0.9913 - val_loss: 0.2016 - val_acc: 0.9929
Epoch 17/40
 - 18s - loss: 0.0943 - acc: 0.9914 - val_loss: 0.1864 - val_acc: 0.9916
Epoch 18/40
 - 18s - loss: 0.0938 - acc: 0.9914 - val_loss: 0.1887 - val_acc: 0.9916
Epoch 19/40
 - 18s - loss: 0.0933 - acc: 0.9915 - val_loss: 0.2057 - val_acc: 0.9929
Epoch 20/40
 - 18s - loss: 0.0932 - acc: 0.9914 - val_loss: 0.2218 - val_acc: 0.9916
Epoch 21/40
 - 18s - loss: 0.0933 - acc: 0.9915 - val_loss: 0.1853 - val_acc: 0.9916
Epoch 22/40
 - 18s - loss: 0.0926 - acc: 0.9915 - val_loss: 0.2219 - val_acc: 0.9916
Epoch 23/40
 - 18s - loss: 0.0926 - acc: 0.9916 - val_loss: 0.1811 - val_acc: 0.9929
Epoch 24/40
 - 18s - loss: 0.0925 - acc: 0.9916 - val_loss: 0.1956 - val_acc: 0.9929
Epoch 25/40
 - 18s - loss: 0.0922 - acc: 0.9916 - val_loss: 0.1872 - val_acc: 0.9929
Epoch 26/40
 - 18s - loss: 0.0919 - acc: 0.9917 - val_loss: 0.1896 - val_acc: 0.9929
Epoch 27/40
 - 18s - loss: 0.0917 - acc: 0.9916 - val_loss: 0.1879 - val_acc: 0.9916
Epoch 28/40
 - 18s - loss: 0.0915 - acc: 0.9917 - val_loss: 0.2051 - val_acc: 0.9916
Epoch 29/40
 - 18s - loss: 0.0912 - acc: 0.9917 - val_loss: 0.2119 - val_acc: 0.9916
Epoch 30/40
 - 18s - loss: 0.0916 - acc: 0.9917 - val_loss: 0.2136 - val_acc: 0.9916
Epoch 31/40
 - 18s - loss: 0.0910 - acc: 0.9918 - val_loss: 0.1995 - val_acc: 0.9916
Epoch 32/40
 - 18s - loss: 0.0907 - acc: 0.9918 - val_loss: 0.2062 - val_acc: 0.9916
Epoch 33/40
 - 18s - loss: 0.0911 - acc: 0.9918 - val_loss: 0.1873 - val_acc: 0.9929
Epoch 34/40
 - 18s - loss: 0.0907 - acc: 0.9918 - val_loss: 0.1905 - val_acc: 0.9929
Epoch 35/40
 - 18s - loss: 0.0906 - acc: 0.9918 - val_loss: 0.2071 - val_acc: 0.9916
Epoch 36/40
 - 18s - loss: 0.0906 - acc: 0.9918 - val_loss: 0.2118 - val_acc: 0.9916
Epoch 37/40
 - 18s - loss: 0.0904 - acc: 0.9918 - val_loss: 0.1859 - val_acc: 0.9916
Epoch 38/40
 - 18s - loss: 0.0908 - acc: 0.9918 - val_loss: 0.2076 - val_acc: 0.9929
Epoch 39/40
 - 18s - loss: 0.0904 - acc: 0.9918 - val_loss: 0.1982 - val_acc: 0.9916
Epoch 40/40
 - 18s - loss: 0.0896 - acc: 0.9919 - val_loss: 0.2018 - val_acc: 0.9916

==================================================================================================
	Training time : 0:13:53.997572
==================================================================================================
	Identification : 0.436
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 6
	100 : 3
	5 : 73
	10 : 13
	50 : 5
	500 : 1
	25 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 178
	100 : 2
	5 : 91
	10 : 13
	50 : 4
	500 : 1
	25 : 5

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : LT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 297, Test : 6209
	MWEs in tain : 192, occurrences : 312
	Impotant words in tain : 263
	MWE length mean : 2.21
	Seen MWEs : 191 (38 %)
	New MWEs : 309 (61 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 1194 * POS : 57
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 200)       238800      input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 15)        855         input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 800)          0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 60)           0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 860)          0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 25)           21525       concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 25)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            208         dropout_14[0][0]                 
==================================================================================================
Total params: 261,388
Trainable params: 261,388
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	297 importatnt sents of 297
	data size before sampling = 13980
	data size after sampling = 34170
{0.0: 6834, 1.0: 6834, 2.0: 6834, 5.0: 6834, 6.0: 6834}
	Class weights : {0: 1, 1: 1, 2: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 30753 samples, validate on 3417 samples
Epoch 1/40
 - 1s - loss: 0.4447 - acc: 0.9371 - val_loss: 0.0259 - val_acc: 1.0000
Epoch 2/40
 - 1s - loss: 0.1302 - acc: 0.9801 - val_loss: 0.0379 - val_acc: 1.0000
Epoch 3/40
 - 1s - loss: 0.1097 - acc: 0.9836 - val_loss: 0.0282 - val_acc: 1.0000
Epoch 4/40
 - 1s - loss: 0.1029 - acc: 0.9856 - val_loss: 0.0407 - val_acc: 1.0000
Epoch 5/40
 - 1s - loss: 0.0980 - acc: 0.9855 - val_loss: 0.0333 - val_acc: 1.0000
Epoch 6/40
 - 1s - loss: 0.0921 - acc: 0.9863 - val_loss: 0.0371 - val_acc: 1.0000
Epoch 7/40
 - 1s - loss: 0.0924 - acc: 0.9870 - val_loss: 0.0327 - val_acc: 1.0000
Epoch 8/40
 - 1s - loss: 0.0900 - acc: 0.9870 - val_loss: 0.0492 - val_acc: 1.0000
Epoch 9/40
 - 1s - loss: 0.0882 - acc: 0.9874 - val_loss: 0.0404 - val_acc: 1.0000
Epoch 10/40
 - 1s - loss: 0.0850 - acc: 0.9879 - val_loss: 0.0599 - val_acc: 1.0000
Epoch 11/40
 - 1s - loss: 0.0844 - acc: 0.9880 - val_loss: 0.0479 - val_acc: 1.0000
Epoch 12/40
 - 1s - loss: 0.0837 - acc: 0.9881 - val_loss: 0.0528 - val_acc: 1.0000
Epoch 13/40
 - 1s - loss: 0.0835 - acc: 0.9880 - val_loss: 0.0578 - val_acc: 1.0000
Epoch 14/40
 - 1s - loss: 0.0822 - acc: 0.9886 - val_loss: 0.0557 - val_acc: 1.0000
Epoch 15/40
 - 1s - loss: 0.0812 - acc: 0.9885 - val_loss: 0.0503 - val_acc: 1.0000
Epoch 16/40
 - 1s - loss: 0.0805 - acc: 0.9886 - val_loss: 0.0411 - val_acc: 1.0000
Epoch 17/40
 - 1s - loss: 0.0795 - acc: 0.9888 - val_loss: 0.0458 - val_acc: 1.0000
Epoch 18/40
 - 1s - loss: 0.0811 - acc: 0.9884 - val_loss: 0.0469 - val_acc: 1.0000
Epoch 19/40
 - 1s - loss: 0.0792 - acc: 0.9887 - val_loss: 0.0467 - val_acc: 1.0000
Epoch 20/40
 - 1s - loss: 0.0790 - acc: 0.9886 - val_loss: 0.0683 - val_acc: 1.0000
Epoch 21/40
 - 1s - loss: 0.0790 - acc: 0.9887 - val_loss: 0.0506 - val_acc: 1.0000
Epoch 22/40
 - 1s - loss: 0.0796 - acc: 0.9887 - val_loss: 0.0644 - val_acc: 1.0000
Epoch 23/40
 - 1s - loss: 0.0780 - acc: 0.9887 - val_loss: 0.0499 - val_acc: 1.0000
Epoch 24/40
 - 1s - loss: 0.0774 - acc: 0.9888 - val_loss: 0.0490 - val_acc: 1.0000
Epoch 25/40
 - 1s - loss: 0.0778 - acc: 0.9889 - val_loss: 0.0541 - val_acc: 1.0000
Epoch 26/40
 - 1s - loss: 0.0785 - acc: 0.9886 - val_loss: 0.0614 - val_acc: 1.0000
Epoch 27/40
 - 1s - loss: 0.0768 - acc: 0.9889 - val_loss: 0.0541 - val_acc: 1.0000
Epoch 28/40
 - 1s - loss: 0.0773 - acc: 0.9889 - val_loss: 0.0561 - val_acc: 1.0000
Epoch 29/40
 - 1s - loss: 0.0771 - acc: 0.9888 - val_loss: 0.0434 - val_acc: 1.0000
Epoch 30/40
 - 1s - loss: 0.0773 - acc: 0.9889 - val_loss: 0.0551 - val_acc: 1.0000
Epoch 31/40
 - 1s - loss: 0.0760 - acc: 0.9890 - val_loss: 0.0516 - val_acc: 1.0000
Epoch 32/40
 - 1s - loss: 0.0770 - acc: 0.9889 - val_loss: 0.0536 - val_acc: 1.0000
Epoch 33/40
 - 1s - loss: 0.0762 - acc: 0.9889 - val_loss: 0.0519 - val_acc: 1.0000
Epoch 34/40
 - 1s - loss: 0.0770 - acc: 0.9889 - val_loss: 0.0560 - val_acc: 1.0000
Epoch 35/40
 - 1s - loss: 0.0756 - acc: 0.9889 - val_loss: 0.0518 - val_acc: 1.0000
Epoch 36/40
 - 1s - loss: 0.0754 - acc: 0.9888 - val_loss: 0.0442 - val_acc: 1.0000
Epoch 37/40
 - 1s - loss: 0.0769 - acc: 0.9889 - val_loss: 0.0643 - val_acc: 1.0000
Epoch 38/40
 - 1s - loss: 0.0775 - acc: 0.9889 - val_loss: 0.0545 - val_acc: 1.0000
Epoch 39/40
 - 1s - loss: 0.0761 - acc: 0.9890 - val_loss: 0.0565 - val_acc: 1.0000
Epoch 40/40
 - 1s - loss: 0.0757 - acc: 0.9888 - val_loss: 0.0527 - val_acc: 1.0000

==================================================================================================
	Training time : 0:00:31.278322
==================================================================================================
	Identification : 0.361
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	10 : 6
	5 : 60

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 253
	10 : 4
	5 : 26

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PL
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3918, Test : 1300
	MWEs in tain : 1824, occurrences : 4637
	Impotant words in tain : 1399
	MWE length mean : 2.13
	Seen MWEs : 355 (68 %)
	New MWEs : 160 (31 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7532 * POS : 158
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 200)       1506400     input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 15)        2370        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 800)          0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 60)           0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 860)          0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 25)           21525       concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 25)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            208         dropout_15[0][0]                 
==================================================================================================
Total params: 1,530,503
Trainable params: 1,530,503
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	3918 importatnt sents of 3918
	data size before sampling = 163545
	data size after sampling = 556626
{0.0: 79518, 1.0: 79518, 2.0: 79518, 3.0: 79518, 4.0: 79518, 5.0: 79518, 6.0: 79518}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 500963 samples, validate on 55663 samples
Epoch 1/40
 - 13s - loss: 0.1847 - acc: 0.9759 - val_loss: 0.1288 - val_acc: 0.9982
Epoch 2/40
 - 13s - loss: 0.1079 - acc: 0.9844 - val_loss: 0.1585 - val_acc: 0.9923
Epoch 3/40
 - 13s - loss: 0.1003 - acc: 0.9856 - val_loss: 0.1408 - val_acc: 0.9935
Epoch 4/40
 - 13s - loss: 0.0970 - acc: 0.9864 - val_loss: 0.1402 - val_acc: 0.9929
Epoch 5/40
 - 13s - loss: 0.0944 - acc: 0.9869 - val_loss: 0.1712 - val_acc: 0.9916
Epoch 6/40
 - 13s - loss: 0.0925 - acc: 0.9871 - val_loss: 0.1457 - val_acc: 0.9929
Epoch 7/40
 - 13s - loss: 0.0906 - acc: 0.9873 - val_loss: 0.1417 - val_acc: 0.9945
Epoch 8/40
 - 13s - loss: 0.0885 - acc: 0.9876 - val_loss: 0.1551 - val_acc: 0.9929
Epoch 9/40
 - 13s - loss: 0.0881 - acc: 0.9878 - val_loss: 0.1564 - val_acc: 0.9923
Epoch 10/40
 - 13s - loss: 0.0873 - acc: 0.9879 - val_loss: 0.1592 - val_acc: 0.9929
Epoch 11/40
 - 13s - loss: 0.0867 - acc: 0.9880 - val_loss: 0.1676 - val_acc: 0.9923
Epoch 12/40
 - 13s - loss: 0.0862 - acc: 0.9880 - val_loss: 0.1497 - val_acc: 0.9929
Epoch 13/40
 - 13s - loss: 0.0856 - acc: 0.9882 - val_loss: 0.1624 - val_acc: 0.9929
Epoch 14/40
 - 13s - loss: 0.0854 - acc: 0.9882 - val_loss: 0.1527 - val_acc: 0.9923
Epoch 15/40
 - 13s - loss: 0.0844 - acc: 0.9883 - val_loss: 0.1633 - val_acc: 0.9929
Epoch 16/40
 - 13s - loss: 0.0844 - acc: 0.9883 - val_loss: 0.1448 - val_acc: 0.9935
Epoch 17/40
 - 13s - loss: 0.0838 - acc: 0.9884 - val_loss: 0.1422 - val_acc: 0.9935
Epoch 18/40
 - 13s - loss: 0.0836 - acc: 0.9884 - val_loss: 0.1446 - val_acc: 0.9929
Epoch 19/40
 - 13s - loss: 0.0837 - acc: 0.9885 - val_loss: 0.1520 - val_acc: 0.9929
Epoch 20/40
 - 13s - loss: 0.0836 - acc: 0.9885 - val_loss: 0.1568 - val_acc: 0.9923
Epoch 21/40
 - 13s - loss: 0.0830 - acc: 0.9885 - val_loss: 0.1443 - val_acc: 0.9923
Epoch 22/40
 - 13s - loss: 0.0828 - acc: 0.9885 - val_loss: 0.1609 - val_acc: 0.9923
Epoch 23/40
 - 13s - loss: 0.0827 - acc: 0.9885 - val_loss: 0.1579 - val_acc: 0.9923
Epoch 24/40
 - 13s - loss: 0.0827 - acc: 0.9886 - val_loss: 0.1504 - val_acc: 0.9923
Epoch 25/40
 - 13s - loss: 0.0824 - acc: 0.9886 - val_loss: 0.1505 - val_acc: 0.9923
Epoch 26/40
 - 13s - loss: 0.0820 - acc: 0.9887 - val_loss: 0.1510 - val_acc: 0.9923
Epoch 27/40
 - 13s - loss: 0.0819 - acc: 0.9887 - val_loss: 0.1507 - val_acc: 0.9923
Epoch 28/40
 - 13s - loss: 0.0817 - acc: 0.9888 - val_loss: 0.1493 - val_acc: 0.9923
Epoch 29/40
 - 13s - loss: 0.0818 - acc: 0.9888 - val_loss: 0.1484 - val_acc: 0.9923
Epoch 30/40
 - 13s - loss: 0.0816 - acc: 0.9888 - val_loss: 0.1549 - val_acc: 0.9923
Epoch 31/40
 - 13s - loss: 0.0815 - acc: 0.9888 - val_loss: 0.1480 - val_acc: 0.9923
Epoch 32/40
 - 13s - loss: 0.0812 - acc: 0.9888 - val_loss: 0.1456 - val_acc: 0.9936
Epoch 33/40
 - 13s - loss: 0.0814 - acc: 0.9888 - val_loss: 0.1543 - val_acc: 0.9923
Epoch 34/40
 - 13s - loss: 0.0812 - acc: 0.9889 - val_loss: 0.1513 - val_acc: 0.9923
Epoch 35/40
 - 13s - loss: 0.0812 - acc: 0.9889 - val_loss: 0.1484 - val_acc: 0.9936
Epoch 36/40
 - 13s - loss: 0.0811 - acc: 0.9889 - val_loss: 0.1630 - val_acc: 0.9930
Epoch 37/40
 - 13s - loss: 0.0811 - acc: 0.9888 - val_loss: 0.1477 - val_acc: 0.9936
Epoch 38/40
 - 13s - loss: 0.0809 - acc: 0.9889 - val_loss: 0.1566 - val_acc: 0.9936
Epoch 39/40
 - 13s - loss: 0.0808 - acc: 0.9889 - val_loss: 0.1636 - val_acc: 0.9923
Epoch 40/40
 - 13s - loss: 0.0807 - acc: 0.9889 - val_loss: 0.1521 - val_acc: 0.9936

==================================================================================================
	Training time : 0:09:33.397777
==================================================================================================
	Identification : 0.562
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 5
	100 : 6
	5 : 51
	10 : 24
	50 : 12
	25 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 150
	100 : 4
	5 : 68
	10 : 18
	50 : 5
	25 : 11

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 4474, Test : 2770
	MWEs in tain : 2216, occurrences : 4983
	Impotant words in tain : 1594
	MWE length mean : 2.22
	Seen MWEs : 384 (69 %)
	New MWEs : 169 (30 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8134 * POS : 245
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 200)       1626800     input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 15)        3675        input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 800)          0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 60)           0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 860)          0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 25)           21525       concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 25)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            208         dropout_16[0][0]                 
==================================================================================================
Total params: 1,652,208
Trainable params: 1,652,208
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	4474 importatnt sents of 4474
	data size before sampling = 240159
	data size after sampling = 705870
{0.0: 117645, 1.0: 117645, 2.0: 117645, 4.0: 117645, 5.0: 117645, 6.0: 117645}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 635283 samples, validate on 70587 samples
Epoch 1/40
 - 16s - loss: 0.1432 - acc: 0.9800 - val_loss: 0.0563 - val_acc: 0.9996
Epoch 2/40
 - 16s - loss: 0.0812 - acc: 0.9872 - val_loss: 0.0609 - val_acc: 0.9996
Epoch 3/40
 - 16s - loss: 0.0753 - acc: 0.9882 - val_loss: 0.0649 - val_acc: 0.9992
Epoch 4/40
 - 16s - loss: 0.0724 - acc: 0.9887 - val_loss: 0.0758 - val_acc: 0.9996
Epoch 5/40
 - 16s - loss: 0.0707 - acc: 0.9890 - val_loss: 0.0876 - val_acc: 0.9992
Epoch 6/40
 - 16s - loss: 0.0695 - acc: 0.9892 - val_loss: 0.0763 - val_acc: 0.9992
Epoch 7/40
 - 16s - loss: 0.0686 - acc: 0.9894 - val_loss: 0.0664 - val_acc: 0.9992
Epoch 8/40
 - 16s - loss: 0.0678 - acc: 0.9895 - val_loss: 0.0679 - val_acc: 0.9996
Epoch 9/40
 - 16s - loss: 0.0672 - acc: 0.9897 - val_loss: 0.0714 - val_acc: 0.9992
Epoch 10/40
 - 16s - loss: 0.0667 - acc: 0.9897 - val_loss: 0.0741 - val_acc: 0.9996
Epoch 11/40
 - 16s - loss: 0.0663 - acc: 0.9898 - val_loss: 0.0675 - val_acc: 0.9992
Epoch 12/40
 - 16s - loss: 0.0660 - acc: 0.9899 - val_loss: 0.0783 - val_acc: 0.9992
Epoch 13/40
 - 16s - loss: 0.0658 - acc: 0.9899 - val_loss: 0.0692 - val_acc: 0.9992
Epoch 14/40
 - 16s - loss: 0.0653 - acc: 0.9900 - val_loss: 0.0813 - val_acc: 0.9992
Epoch 15/40
 - 16s - loss: 0.0649 - acc: 0.9900 - val_loss: 0.0728 - val_acc: 0.9992
Epoch 16/40
 - 16s - loss: 0.0647 - acc: 0.9900 - val_loss: 0.0719 - val_acc: 0.9992
Epoch 17/40
 - 16s - loss: 0.0644 - acc: 0.9901 - val_loss: 0.0802 - val_acc: 0.9992
Epoch 18/40
 - 16s - loss: 0.0644 - acc: 0.9901 - val_loss: 0.0759 - val_acc: 0.9992
Epoch 19/40
 - 16s - loss: 0.0643 - acc: 0.9902 - val_loss: 0.0705 - val_acc: 0.9992
Epoch 20/40
 - 16s - loss: 0.0643 - acc: 0.9901 - val_loss: 0.0832 - val_acc: 0.9992
Epoch 21/40
 - 16s - loss: 0.0640 - acc: 0.9903 - val_loss: 0.0747 - val_acc: 0.9992
Epoch 22/40
 - 16s - loss: 0.0637 - acc: 0.9903 - val_loss: 0.0760 - val_acc: 0.9992
Epoch 23/40
 - 16s - loss: 0.0636 - acc: 0.9903 - val_loss: 0.0742 - val_acc: 0.9992
Epoch 24/40
 - 16s - loss: 0.0632 - acc: 0.9903 - val_loss: 0.0853 - val_acc: 0.9992
Epoch 25/40
 - 16s - loss: 0.0632 - acc: 0.9904 - val_loss: 0.0903 - val_acc: 0.9992
Epoch 26/40
 - 16s - loss: 0.0631 - acc: 0.9904 - val_loss: 0.0748 - val_acc: 0.9992
Epoch 27/40
 - 16s - loss: 0.0632 - acc: 0.9904 - val_loss: 0.0808 - val_acc: 0.9992
Epoch 28/40
 - 16s - loss: 0.0631 - acc: 0.9904 - val_loss: 0.0839 - val_acc: 0.9992
Epoch 29/40
 - 16s - loss: 0.0629 - acc: 0.9904 - val_loss: 0.0781 - val_acc: 0.9992
Epoch 30/40
 - 16s - loss: 0.0626 - acc: 0.9904 - val_loss: 0.0795 - val_acc: 0.9992
Epoch 31/40
 - 16s - loss: 0.0627 - acc: 0.9905 - val_loss: 0.0778 - val_acc: 0.9992
Epoch 32/40
 - 16s - loss: 0.0627 - acc: 0.9904 - val_loss: 0.0674 - val_acc: 0.9992
Epoch 33/40
 - 16s - loss: 0.0625 - acc: 0.9905 - val_loss: 0.0780 - val_acc: 0.9992
Epoch 34/40
 - 16s - loss: 0.0624 - acc: 0.9905 - val_loss: 0.0779 - val_acc: 0.9992
Epoch 35/40
 - 16s - loss: 0.0624 - acc: 0.9905 - val_loss: 0.0804 - val_acc: 0.9992
Epoch 36/40
 - 16s - loss: 0.0622 - acc: 0.9905 - val_loss: 0.0741 - val_acc: 0.9992
Epoch 37/40
 - 16s - loss: 0.0622 - acc: 0.9905 - val_loss: 0.0772 - val_acc: 0.9992
Epoch 38/40
 - 16s - loss: 0.0622 - acc: 0.9905 - val_loss: 0.0836 - val_acc: 0.9992
Epoch 39/40
 - 16s - loss: 0.0618 - acc: 0.9906 - val_loss: 0.0741 - val_acc: 0.9992
Epoch 40/40
 - 16s - loss: 0.0620 - acc: 0.9905 - val_loss: 0.0773 - val_acc: 0.9992

==================================================================================================
	Training time : 0:12:15.547423
==================================================================================================
	Identification : 0.455
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 5
	25 : 20
	50 : 7
	10 : 25
	5 : 86

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 154
	25 : 20
	50 : 4
	10 : 32
	5 : 116

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : RO
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 4797, Test : 6934
	MWEs in tain : 591, occurrences : 5302
	Impotant words in tain : 528
	MWE length mean : 2.13
	Seen MWEs : 556 (94 %)
	New MWEs : 33 (5 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8078 * POS : 108
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 200)       1615600     input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 15)        1620        input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 800)          0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 60)           0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 860)          0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 25)           21525       concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 25)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            208         dropout_17[0][0]                 
==================================================================================================
Total params: 1,638,953
Trainable params: 1,638,953
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	4797 importatnt sents of 4797
	data size before sampling = 350536
	data size after sampling = 1035708
{0.0: 172618, 1.0: 172618, 2.0: 172618, 4.0: 172618, 5.0: 172618, 6.0: 172618}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 932137 samples, validate on 103571 samples
Epoch 1/40
 - 24s - loss: 0.1119 - acc: 0.9884 - val_loss: 0.1107 - val_acc: 0.9933
Epoch 2/40
 - 24s - loss: 0.0779 - acc: 0.9916 - val_loss: 0.0831 - val_acc: 0.9933
Epoch 3/40
 - 24s - loss: 0.0744 - acc: 0.9919 - val_loss: 0.0982 - val_acc: 0.9933
Epoch 4/40
 - 24s - loss: 0.0723 - acc: 0.9921 - val_loss: 0.0955 - val_acc: 0.9933
Epoch 5/40
 - 24s - loss: 0.0712 - acc: 0.9923 - val_loss: 0.0970 - val_acc: 0.9933
Epoch 6/40
 - 24s - loss: 0.0710 - acc: 0.9923 - val_loss: 0.1022 - val_acc: 0.9933
Epoch 7/40
 - 24s - loss: 0.0701 - acc: 0.9923 - val_loss: 0.1036 - val_acc: 0.9911
Epoch 8/40
 - 24s - loss: 0.0699 - acc: 0.9924 - val_loss: 0.1043 - val_acc: 0.9888
Epoch 9/40
 - 24s - loss: 0.0694 - acc: 0.9924 - val_loss: 0.0987 - val_acc: 0.9933
Epoch 10/40
 - 24s - loss: 0.0690 - acc: 0.9924 - val_loss: 0.1042 - val_acc: 0.9888
Epoch 11/40
 - 24s - loss: 0.0688 - acc: 0.9924 - val_loss: 0.0924 - val_acc: 0.9933
Epoch 12/40
 - 24s - loss: 0.0686 - acc: 0.9925 - val_loss: 0.1062 - val_acc: 0.9911
Epoch 13/40
 - 24s - loss: 0.0682 - acc: 0.9925 - val_loss: 0.1022 - val_acc: 0.9910
Epoch 14/40
 - 24s - loss: 0.0678 - acc: 0.9925 - val_loss: 0.1001 - val_acc: 0.9933
Epoch 15/40
 - 24s - loss: 0.0677 - acc: 0.9926 - val_loss: 0.0966 - val_acc: 0.9933
Epoch 16/40
 - 24s - loss: 0.0675 - acc: 0.9926 - val_loss: 0.1017 - val_acc: 0.9910
Epoch 17/40
 - 24s - loss: 0.0674 - acc: 0.9926 - val_loss: 0.1151 - val_acc: 0.9888
Epoch 18/40
 - 24s - loss: 0.0673 - acc: 0.9926 - val_loss: 0.0999 - val_acc: 0.9933
Epoch 19/40
 - 24s - loss: 0.0671 - acc: 0.9926 - val_loss: 0.0918 - val_acc: 0.9933
Epoch 20/40
 - 24s - loss: 0.0669 - acc: 0.9927 - val_loss: 0.1027 - val_acc: 0.9933
Epoch 21/40
 - 24s - loss: 0.0672 - acc: 0.9926 - val_loss: 0.0975 - val_acc: 0.9933
Epoch 22/40
 - 24s - loss: 0.0670 - acc: 0.9926 - val_loss: 0.0965 - val_acc: 0.9933
Epoch 23/40
 - 24s - loss: 0.0669 - acc: 0.9926 - val_loss: 0.0960 - val_acc: 0.9933
Epoch 24/40
 - 24s - loss: 0.0665 - acc: 0.9927 - val_loss: 0.0930 - val_acc: 0.9933
Epoch 25/40
 - 24s - loss: 0.0666 - acc: 0.9926 - val_loss: 0.1034 - val_acc: 0.9888
Epoch 26/40
 - 24s - loss: 0.0667 - acc: 0.9926 - val_loss: 0.1025 - val_acc: 0.9888
Epoch 27/40
 - 24s - loss: 0.0666 - acc: 0.9926 - val_loss: 0.1085 - val_acc: 0.9888
Epoch 28/40
 - 24s - loss: 0.0666 - acc: 0.9926 - val_loss: 0.0999 - val_acc: 0.9911
Epoch 29/40
 - 24s - loss: 0.0665 - acc: 0.9927 - val_loss: 0.0988 - val_acc: 0.9888
Epoch 30/40
 - 24s - loss: 0.0664 - acc: 0.9926 - val_loss: 0.1045 - val_acc: 0.9888
Epoch 31/40
 - 24s - loss: 0.0665 - acc: 0.9927 - val_loss: 0.0974 - val_acc: 0.9933
Epoch 32/40
 - 24s - loss: 0.0664 - acc: 0.9927 - val_loss: 0.0980 - val_acc: 0.9933
Epoch 33/40
 - 24s - loss: 0.0662 - acc: 0.9927 - val_loss: 0.1020 - val_acc: 0.9888
Epoch 34/40
 - 24s - loss: 0.0662 - acc: 0.9927 - val_loss: 0.0995 - val_acc: 0.9933
Epoch 35/40
 - 24s - loss: 0.0663 - acc: 0.9927 - val_loss: 0.1032 - val_acc: 0.9888
Epoch 36/40
 - 24s - loss: 0.0662 - acc: 0.9927 - val_loss: 0.0967 - val_acc: 0.9933
Epoch 37/40
 - 24s - loss: 0.0661 - acc: 0.9927 - val_loss: 0.1003 - val_acc: 0.9933
Epoch 38/40
 - 24s - loss: 0.0659 - acc: 0.9927 - val_loss: 0.1050 - val_acc: 0.9933
Epoch 39/40
 - 24s - loss: 0.0661 - acc: 0.9927 - val_loss: 0.0986 - val_acc: 0.9910
Epoch 40/40
 - 24s - loss: 0.0662 - acc: 0.9927 - val_loss: 0.0999 - val_acc: 0.9933

==================================================================================================
	Training time : 0:18:05.076692
==================================================================================================
	Identification : 0.657
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	100 : 13
	5 : 12
	10 : 21
	50 : 13
	500 : 5
	25 : 25
	None : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 31
	100 : 11
	5 : 52
	10 : 21
	50 : 6
	500 : 3
	25 : 19
	None : 1

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : SL
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2516, Test : 1994
	MWEs in tain : 1109, occurrences : 2878
	Impotant words in tain : 933
	MWE length mean : 2.23
	Seen MWEs : 360 (72 %)
	New MWEs : 140 (28 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5994 * POS : 11
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 200)       1198800     input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 15)        165         input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 800)          0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 60)           0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 860)          0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 25)           21525       concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 25)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            208         dropout_18[0][0]                 
==================================================================================================
Total params: 1,220,698
Trainable params: 1,220,698
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	2516 importatnt sents of 2516
	data size before sampling = 133529
	data size after sampling = 457366
{0.0: 65338, 1.0: 65338, 2.0: 65338, 3.0: 65338, 4.0: 65338, 5.0: 65338, 6.0: 65338}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 411629 samples, validate on 45737 samples
Epoch 1/40
 - 10s - loss: 0.2504 - acc: 0.9730 - val_loss: 0.0686 - val_acc: 0.9916
Epoch 2/40
 - 10s - loss: 0.1215 - acc: 0.9834 - val_loss: 0.0905 - val_acc: 0.9960
Epoch 3/40
 - 10s - loss: 0.1102 - acc: 0.9850 - val_loss: 0.1489 - val_acc: 0.9916
Epoch 4/40
 - 10s - loss: 0.1035 - acc: 0.9861 - val_loss: 0.0811 - val_acc: 0.9916
Epoch 5/40
 - 10s - loss: 0.1014 - acc: 0.9865 - val_loss: 0.0789 - val_acc: 0.9916
Epoch 6/40
 - 10s - loss: 0.0974 - acc: 0.9869 - val_loss: 0.0749 - val_acc: 0.9957
Epoch 7/40
 - 10s - loss: 0.0959 - acc: 0.9872 - val_loss: 0.0659 - val_acc: 0.9960
Epoch 8/40
 - 10s - loss: 0.0934 - acc: 0.9876 - val_loss: 0.1156 - val_acc: 0.9916
Epoch 9/40
 - 10s - loss: 0.0926 - acc: 0.9878 - val_loss: 0.0690 - val_acc: 0.9960
Epoch 10/40
 - 10s - loss: 0.0915 - acc: 0.9879 - val_loss: 0.0971 - val_acc: 0.9916
Epoch 11/40
 - 10s - loss: 0.0903 - acc: 0.9880 - val_loss: 0.0916 - val_acc: 0.9916
Epoch 12/40
 - 10s - loss: 0.0889 - acc: 0.9882 - val_loss: 0.0781 - val_acc: 0.9916
Epoch 13/40
 - 10s - loss: 0.0889 - acc: 0.9883 - val_loss: 0.0718 - val_acc: 0.9916
Epoch 14/40
 - 10s - loss: 0.0881 - acc: 0.9883 - val_loss: 0.0824 - val_acc: 0.9916
Epoch 15/40
 - 10s - loss: 0.0872 - acc: 0.9884 - val_loss: 0.0946 - val_acc: 0.9916
Epoch 16/40
 - 10s - loss: 0.0872 - acc: 0.9885 - val_loss: 0.0784 - val_acc: 0.9916
Epoch 17/40
 - 10s - loss: 0.0867 - acc: 0.9885 - val_loss: 0.0792 - val_acc: 0.9916
Epoch 18/40
 - 10s - loss: 0.0862 - acc: 0.9886 - val_loss: 0.0879 - val_acc: 0.9916
Epoch 19/40
 - 10s - loss: 0.0860 - acc: 0.9887 - val_loss: 0.0759 - val_acc: 0.9916
Epoch 20/40
 - 10s - loss: 0.0852 - acc: 0.9887 - val_loss: 0.0900 - val_acc: 0.9916
Epoch 21/40
 - 10s - loss: 0.0854 - acc: 0.9889 - val_loss: 0.0892 - val_acc: 0.9916
Epoch 22/40
 - 10s - loss: 0.0853 - acc: 0.9888 - val_loss: 0.0850 - val_acc: 0.9916
Epoch 23/40
 - 10s - loss: 0.0848 - acc: 0.9888 - val_loss: 0.0842 - val_acc: 0.9916
Epoch 24/40
 - 10s - loss: 0.0844 - acc: 0.9888 - val_loss: 0.0799 - val_acc: 0.9916
Epoch 25/40
 - 10s - loss: 0.0838 - acc: 0.9890 - val_loss: 0.0800 - val_acc: 0.9916
Epoch 26/40
 - 10s - loss: 0.0842 - acc: 0.9890 - val_loss: 0.0779 - val_acc: 0.9916
Epoch 27/40
 - 10s - loss: 0.0839 - acc: 0.9889 - val_loss: 0.0772 - val_acc: 0.9916
Epoch 28/40
 - 10s - loss: 0.0831 - acc: 0.9890 - val_loss: 0.0922 - val_acc: 0.9916
Epoch 29/40
 - 10s - loss: 0.0827 - acc: 0.9890 - val_loss: 0.0721 - val_acc: 0.9960
Epoch 30/40
 - 10s - loss: 0.0829 - acc: 0.9890 - val_loss: 0.0837 - val_acc: 0.9916
Epoch 31/40
 - 10s - loss: 0.0828 - acc: 0.9890 - val_loss: 0.0864 - val_acc: 0.9916
Epoch 32/40
 - 10s - loss: 0.0827 - acc: 0.9890 - val_loss: 0.0843 - val_acc: 0.9916
Epoch 33/40
 - 10s - loss: 0.0822 - acc: 0.9891 - val_loss: 0.0738 - val_acc: 0.9960
Epoch 34/40
 - 10s - loss: 0.0821 - acc: 0.9891 - val_loss: 0.0891 - val_acc: 0.9916
Epoch 35/40
 - 10s - loss: 0.0821 - acc: 0.9892 - val_loss: 0.0857 - val_acc: 0.9916
Epoch 36/40
 - 10s - loss: 0.0820 - acc: 0.9892 - val_loss: 0.0869 - val_acc: 0.9916
Epoch 37/40
 - 10s - loss: 0.0818 - acc: 0.9892 - val_loss: 0.0903 - val_acc: 0.9916
Epoch 38/40
 - 10s - loss: 0.0819 - acc: 0.9892 - val_loss: 0.0893 - val_acc: 0.9916
Epoch 39/40
 - 10s - loss: 0.0817 - acc: 0.9893 - val_loss: 0.0891 - val_acc: 0.9916
Epoch 40/40
 - 10s - loss: 0.0813 - acc: 0.9892 - val_loss: 0.0857 - val_acc: 0.9916

==================================================================================================
	Training time : 0:07:37.750947
==================================================================================================
	Identification : 0.572
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 29
	50 : 5
	10 : 24
	100 : 4
	5 : 69

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 131
	100 : 4
	5 : 50
	10 : 19
	50 : 3
	25 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 4843, Test : 589
	MWEs in tain : 2243, occurrences : 6635
	Impotant words in tain : 1428
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 152 (30 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8548 * POS : 124
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 200)       1709600     input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 15)        1860        input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 800)          0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 60)           0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 860)          0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 25)           21525       concatenate_19[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 25)           0           dense_37[0][0]                   
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            208         dropout_19[0][0]                 
==================================================================================================
Total params: 1,733,193
Trainable params: 1,733,193
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	4843 importatnt sents of 4843
	data size before sampling = 329139
	data size after sampling = 806345
{0.0: 161269, 1.0: 161269, 2.0: 161269, 5.0: 161269, 6.0: 161269}
	Class weights : {0: 1, 1: 1, 2: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 725710 samples, validate on 80635 samples
Epoch 1/40
 - 19s - loss: 0.2907 - acc: 0.9712 - val_loss: 0.3530 - val_acc: 0.9851
Epoch 2/40
 - 19s - loss: 0.1614 - acc: 0.9797 - val_loss: 0.2657 - val_acc: 0.9885
Epoch 3/40
 - 19s - loss: 0.1459 - acc: 0.9809 - val_loss: 0.2313 - val_acc: 0.9918
Epoch 4/40
 - 19s - loss: 0.1381 - acc: 0.9815 - val_loss: 0.2321 - val_acc: 0.9894
Epoch 5/40
 - 19s - loss: 0.1323 - acc: 0.9821 - val_loss: 0.2230 - val_acc: 0.9930
Epoch 6/40
 - 19s - loss: 0.1285 - acc: 0.9824 - val_loss: 0.2388 - val_acc: 0.9916
Epoch 7/40
 - 19s - loss: 0.1256 - acc: 0.9827 - val_loss: 0.1732 - val_acc: 0.9935
Epoch 8/40
 - 19s - loss: 0.1234 - acc: 0.9830 - val_loss: 0.1981 - val_acc: 0.9939
Epoch 9/40
 - 19s - loss: 0.1216 - acc: 0.9831 - val_loss: 0.1998 - val_acc: 0.9933
Epoch 10/40
 - 19s - loss: 0.1202 - acc: 0.9832 - val_loss: 0.1977 - val_acc: 0.9941
Epoch 11/40
 - 19s - loss: 0.1188 - acc: 0.9834 - val_loss: 0.1806 - val_acc: 0.9934
Epoch 12/40
 - 19s - loss: 0.1187 - acc: 0.9834 - val_loss: 0.1666 - val_acc: 0.9953
Epoch 13/40
 - 19s - loss: 0.1167 - acc: 0.9836 - val_loss: 0.1724 - val_acc: 0.9945
Epoch 14/40
 - 19s - loss: 0.1159 - acc: 0.9838 - val_loss: 0.1681 - val_acc: 0.9945
Epoch 15/40
 - 19s - loss: 0.1147 - acc: 0.9839 - val_loss: 0.1679 - val_acc: 0.9936
Epoch 16/40
 - 19s - loss: 0.1146 - acc: 0.9840 - val_loss: 0.1645 - val_acc: 0.9945
Epoch 17/40
 - 19s - loss: 0.1136 - acc: 0.9841 - val_loss: 0.1814 - val_acc: 0.9945
Epoch 18/40
 - 19s - loss: 0.1127 - acc: 0.9842 - val_loss: 0.1897 - val_acc: 0.9945
Epoch 19/40
 - 19s - loss: 0.1125 - acc: 0.9842 - val_loss: 0.1839 - val_acc: 0.9940
Epoch 20/40
 - 19s - loss: 0.1120 - acc: 0.9842 - val_loss: 0.1937 - val_acc: 0.9935
Epoch 21/40
 - 19s - loss: 0.1117 - acc: 0.9844 - val_loss: 0.1727 - val_acc: 0.9940
Epoch 22/40
 - 19s - loss: 0.1114 - acc: 0.9845 - val_loss: 0.1657 - val_acc: 0.9942
Epoch 23/40
 - 19s - loss: 0.1106 - acc: 0.9845 - val_loss: 0.1733 - val_acc: 0.9935
Epoch 24/40
 - 19s - loss: 0.1106 - acc: 0.9844 - val_loss: 0.1679 - val_acc: 0.9940
Epoch 25/40
 - 19s - loss: 0.1103 - acc: 0.9845 - val_loss: 0.1739 - val_acc: 0.9945
Epoch 26/40
 - 19s - loss: 0.1101 - acc: 0.9845 - val_loss: 0.1770 - val_acc: 0.9940
Epoch 27/40
 - 19s - loss: 0.1100 - acc: 0.9846 - val_loss: 0.1680 - val_acc: 0.9940
Epoch 28/40
 - 19s - loss: 0.1102 - acc: 0.9846 - val_loss: 0.1777 - val_acc: 0.9940
Epoch 29/40
 - 19s - loss: 0.1093 - acc: 0.9846 - val_loss: 0.1603 - val_acc: 0.9945
Epoch 30/40
 - 19s - loss: 0.1086 - acc: 0.9847 - val_loss: 0.1613 - val_acc: 0.9945
Epoch 31/40
 - 19s - loss: 0.1082 - acc: 0.9847 - val_loss: 0.1935 - val_acc: 0.9940
Epoch 32/40
 - 19s - loss: 0.1078 - acc: 0.9848 - val_loss: 0.1535 - val_acc: 0.9945
Epoch 33/40
 - 19s - loss: 0.1078 - acc: 0.9848 - val_loss: 0.1758 - val_acc: 0.9940
Epoch 34/40
 - 19s - loss: 0.1074 - acc: 0.9848 - val_loss: 0.1624 - val_acc: 0.9945
Epoch 35/40
 - 19s - loss: 0.1070 - acc: 0.9848 - val_loss: 0.1609 - val_acc: 0.9945
Epoch 36/40
 - 19s - loss: 0.1068 - acc: 0.9849 - val_loss: 0.1709 - val_acc: 0.9943
Epoch 37/40
 - 19s - loss: 0.1071 - acc: 0.9849 - val_loss: 0.1809 - val_acc: 0.9945
Epoch 38/40
 - 19s - loss: 0.1069 - acc: 0.9848 - val_loss: 0.1601 - val_acc: 0.9945
Epoch 39/40
 - 19s - loss: 0.1064 - acc: 0.9850 - val_loss: 0.1695 - val_acc: 0.9940
Epoch 40/40
 - 19s - loss: 0.1063 - acc: 0.9849 - val_loss: 0.1667 - val_acc: 0.9940

==================================================================================================
	Training time : 0:14:34.563960
==================================================================================================
	Identification : 0.52
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 16
	100 : 9
	5 : 60
	10 : 26
	50 : 10
	500 : 1
	25 : 19

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 124
	100 : 3
	5 : 60
	10 : 19
	50 : 6
	25 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
