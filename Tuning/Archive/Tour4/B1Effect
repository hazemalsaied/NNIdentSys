        GPU Enabled
==================================================================================================
        Dev Mode
==================================================================================================
        Language : BG
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 4674, Test : 1954
        MWEs in tain : 1780, occurrences : 5364
        Impotant words in tain : 1424
        MWE length mean : 2.13
        Seen MWEs : 405 (60 %)
        New MWEs : 265 (39 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 11478
        After : 9376

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 9376 * POS : 168
__________________________________________________________________________________________________
        Dashed keys in vocabulary 2113
        One occurrence keys in vocabulary 2113 / 9376
        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 3, 200)       1875200     input_1[0][0]
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 3, 15)        2520        input_2[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 600)          0           embedding_1[0][0]
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 45)           0           embedding_2[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 645)          0           flatten_1[0][0]
                                                                 flatten_2[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 25)           16150       concatenate_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 25)           0           dense_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            208         dropout_1[0][0]
==================================================================================================
Total params: 1,894,078
Trainable params: 1,894,078
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 271678
        data size after sampling = 932099
        7 Labels in train : Counter({0.0: 133157, 1.0: 133157, 2.0: 133157, 3.0: 133157, 4.0: 133157, 5.0: 133157, 6.0: 133157})
        7 Labels in valid : Counter({0.0: 13514, 2.0: 13362, 5.0: 13340, 6.0: 13315, 1.0: 13297, 3.0: 13206, 4.0: 13176})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 838889 samples, validate on 93210 samples
Epoch 1/40
 - 21s - loss: 0.1656 - acc: 0.9838 - val_loss: 0.0890 - val_acc: 0.9890
Epoch 2/40
 - 21s - loss: 0.0963 - acc: 0.9892 - val_loss: 0.0816 - val_acc: 0.9899
Epoch 3/40
 - 21s - loss: 0.0892 - acc: 0.9899 - val_loss: 0.0798 - val_acc: 0.9900

==================================================================================================
        Training time : 0:02:36.252988
==================================================================================================
        Identification : 0.582
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 77
        100 : 1
        5 : 69
        300 : 1
        50 : 5
        25 : 21

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 276
        25 : 4
        5 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
        Language : BG
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 4674, Test : 1954
        MWEs in tain : 1780, occurrences : 5364
        Impotant words in tain : 1424
        MWE length mean : 2.13
        Seen MWEs : 405 (60 %)
        New MWEs : 265 (39 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 11478
        After : 9282

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 9282 * POS : 168
__________________________________________________________________________________________________
        Dashed keys in vocabulary 2113
        One occurrence keys in vocabulary 2113 / 9282
        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_3 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 3, 200)       1856400     input_3[0][0]
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 3, 15)        2520        input_4[0][0]
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 600)          0           embedding_3[0][0]
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 45)           0           embedding_4[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 645)          0           flatten_3[0][0]
                                                                 flatten_4[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 25)           16150       concatenate_2[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 25)           0           dense_3[0][0]
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            208         dropout_2[0][0]
==================================================================================================
Total params: 1,875,278
Trainable params: 1,875,278
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 271678
        data size after sampling = 932099
        7 Labels in train : Counter({0.0: 133157, 1.0: 133157, 2.0: 133157, 3.0: 133157, 4.0: 133157, 5.0: 133157, 6.0: 133157})
        7 Labels in valid : Counter({6.0: 13484, 0.0: 13480, 3.0: 13409, 1.0: 13407, 2.0: 13191, 4.0: 13169, 5.0: 13070})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 838889 samples, validate on 93210 samples
Epoch 1/40
 - 21s - loss: 0.1673 - acc: 0.9838 - val_loss: 0.0834 - val_acc: 0.9895
Epoch 2/40
 - 21s - loss: 0.0966 - acc: 0.9896 - val_loss: 0.0793 - val_acc: 0.9900
Epoch 3/40
 - 21s - loss: 0.0893 - acc: 0.9902 - val_loss: 0.0784 - val_acc: 0.9901

==================================================================================================
        Training time : 0:01:47.888860
==================================================================================================
        Identification : 0.592
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 84
        100 : 1
        5 : 68
        300 : 1
        50 : 5
        25 : 21

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 268
        25 : 4
        5 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
        Language : BG
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 4674, Test : 1954
        MWEs in tain : 1780, occurrences : 5364
        Impotant words in tain : 1424
        MWE length mean : 2.13
        Seen MWEs : 405 (60 %)
        New MWEs : 265 (39 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 11478
        After : 9319

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 9319 * POS : 168
__________________________________________________________________________________________________
        Dashed keys in vocabulary 2113
        One occurrence keys in vocabulary 2113 / 9319
        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_5 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 3, 200)       1863800     input_5[0][0]
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 3, 15)        2520        input_6[0][0]
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 600)          0           embedding_5[0][0]
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 45)           0           embedding_6[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 645)          0           flatten_5[0][0]
                                                                 flatten_6[0][0]
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 25)           16150       concatenate_3[0][0]
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 25)           0           dense_5[0][0]
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            208         dropout_3[0][0]
==================================================================================================
Total params: 1,882,678
Trainable params: 1,882,678
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 271678
        data size after sampling = 932099
        7 Labels in train : Counter({0.0: 133157, 1.0: 133157, 2.0: 133157, 3.0: 133157, 4.0: 133157, 5.0: 133157, 6.0: 133157})
        7 Labels in valid : Counter({2.0: 13485, 6.0: 13342, 3.0: 13338, 4.0: 13321, 5.0: 13261, 0.0: 13239, 1.0: 13224})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 838889 samples, validate on 93210 samples
Epoch 1/40
 - 21s - loss: 0.1730 - acc: 0.9841 - val_loss: 0.0887 - val_acc: 0.9894
Epoch 2/40
 - 21s - loss: 0.1006 - acc: 0.9894 - val_loss: 0.0847 - val_acc: 0.9897
Epoch 3/40
 - 21s - loss: 0.0930 - acc: 0.9900 - val_loss: 0.0835 - val_acc: 0.9900

==================================================================================================
        Training time : 0:01:48.375408
==================================================================================================
        Identification : 0.574
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 77
        100 : 1
        5 : 69
        300 : 1
        50 : 5
        25 : 21

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 276
        25 : 4
        5 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
        Language : PT
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 3980, Test : 3117
        MWEs in tain : 2021, occurrences : 4337
        Impotant words in tain : 1512
        MWE length mean : 2.22
        Seen MWEs : 387 (69 %)
        New MWEs : 166 (30 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 12801
        After : 10148

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 10148 * POS : 224
__________________________________________________________________________________________________
        Important words not in vocabulary 1
se_vira_em_o_30
        MWE not in vocabulary 1
        Dashed keys in vocabulary 2391
        One occurrence keys in vocabulary 2391 / 10148
        Attention: important lemmas are not in vocbulary        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_7 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 3, 200)       2029600     input_7[0][0]
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 3, 15)        3360        input_8[0][0]
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 600)          0           embedding_7[0][0]
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 45)           0           embedding_8[0][0]
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 645)          0           flatten_7[0][0]
                                                                 flatten_8[0][0]
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 25)           16150       concatenate_4[0][0]
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 25)           0           dense_7[0][0]
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            208         dropout_4[0][0]
==================================================================================================
Total params: 2,049,318
Trainable params: 2,049,318
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 214447
        data size after sampling = 630330
        6 Labels in train : Counter({0.0: 105055, 1.0: 105055, 2.0: 105055, 4.0: 105055, 5.0: 105055, 6.0: 105055})
        6 Labels in valid : Counter({4.0: 10622, 5.0: 10605, 0.0: 10595, 6.0: 10473, 2.0: 10419, 1.0: 10319})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 567297 samples, validate on 63033 samples
Epoch 1/40
 - 14s - loss: 0.1867 - acc: 0.9751 - val_loss: 0.0771 - val_acc: 0.9881
Epoch 2/40
 - 14s - loss: 0.0976 - acc: 0.9860 - val_loss: 0.0740 - val_acc: 0.9886
Epoch 3/40
 - 14s - loss: 0.0889 - acc: 0.9876 - val_loss: 0.0743 - val_acc: 0.9888

==================================================================================================
        Training time : 0:01:14.828895
==================================================================================================
        Identification : 0.664
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 131
        25 : 8
        5 : 81

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 201
        25 : 2
        5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
        Language : PT
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 3980, Test : 3117
        MWEs in tain : 2021, occurrences : 4337
        Impotant words in tain : 1512
        MWE length mean : 2.22
        Seen MWEs : 387 (69 %)
        New MWEs : 166 (30 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 12801
        After : 10141

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 10141 * POS : 224
__________________________________________________________________________________________________
        Important words not in vocabulary 1
se_vira_em_o_30
        MWE not in vocabulary 1
        Dashed keys in vocabulary 2391
        One occurrence keys in vocabulary 2391 / 10141
        Attention: important lemmas are not in vocbulary        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_9 (InputLayer)            (None, 3)            0
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 3, 200)       2028200     input_9[0][0]
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 3, 15)        3360        input_10[0][0]
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 600)          0           embedding_9[0][0]
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 45)           0           embedding_10[0][0]
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 645)          0           flatten_9[0][0]
                                                                 flatten_10[0][0]
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 25)           16150       concatenate_5[0][0]
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 25)           0           dense_9[0][0]
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            208         dropout_5[0][0]
==================================================================================================
Total params: 2,047,918
Trainable params: 2,047,918
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 214447
        data size after sampling = 630330
        6 Labels in train : Counter({0.0: 105055, 1.0: 105055, 2.0: 105055, 4.0: 105055, 5.0: 105055, 6.0: 105055})
        6 Labels in valid : Counter({1.0: 10562, 0.0: 10551, 2.0: 10516, 6.0: 10516, 5.0: 10464, 4.0: 10424})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 567297 samples, validate on 63033 samples
Epoch 1/40
 - 14s - loss: 0.1493 - acc: 0.9805 - val_loss: 0.0738 - val_acc: 0.9876
Epoch 2/40
 - 14s - loss: 0.0827 - acc: 0.9879 - val_loss: 0.0707 - val_acc: 0.9885
Epoch 3/40
 - 15s - loss: 0.0776 - acc: 0.9887 - val_loss: 0.0702 - val_acc: 0.9887

==================================================================================================
        Training time : 0:01:14.840990
==================================================================================================
        Identification : 0.66
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 142
        25 : 8
        5 : 83

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 191
        25 : 2
        5 : 15

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
        Language : PT
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 3980, Test : 3117
        MWEs in tain : 2021, occurrences : 4337
        Impotant words in tain : 1512
        MWE length mean : 2.22
        Seen MWEs : 387 (69 %)
        New MWEs : 166 (30 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 12801
        After : 10175

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 10175 * POS : 224
__________________________________________________________________________________________________
        Important words not in vocabulary 1
se_vira_em_o_30
        MWE not in vocabulary 1
        Dashed keys in vocabulary 2391
        One occurrence keys in vocabulary 2391 / 10175
        Attention: important lemmas are not in vocbulary        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_11 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 3, 200)       2035000     input_11[0][0]
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 3, 15)        3360        input_12[0][0]
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 600)          0           embedding_11[0][0]
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 45)           0           embedding_12[0][0]
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 645)          0           flatten_11[0][0]
                                                                 flatten_12[0][0]
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 25)           16150       concatenate_6[0][0]
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 25)           0           dense_11[0][0]
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            208         dropout_6[0][0]
==================================================================================================
Total params: 2,054,718
Trainable params: 2,054,718
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 214447
        data size after sampling = 630330
        6 Labels in train : Counter({0.0: 105055, 1.0: 105055, 2.0: 105055, 4.0: 105055, 5.0: 105055, 6.0: 105055})
        6 Labels in valid : Counter({4.0: 10640, 0.0: 10588, 1.0: 10505, 5.0: 10486, 2.0: 10430, 6.0: 10384})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 567297 samples, validate on 63033 samples
Epoch 1/40
 - 14s - loss: 0.1520 - acc: 0.9804 - val_loss: 0.0778 - val_acc: 0.9874
Epoch 2/40
 - 14s - loss: 0.0828 - acc: 0.9876 - val_loss: 0.0762 - val_acc: 0.9880
Epoch 3/40
 - 14s - loss: 0.0774 - acc: 0.9885 - val_loss: 0.0751 - val_acc: 0.9886

==================================================================================================
        Training time : 0:01:17.939104
==================================================================================================
        Identification : 0.654
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 135
        25 : 8
        5 : 81

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 198
        25 : 2
        5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
        Language : TR
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 4458, Test : 1320
        MWEs in tain : 2083, occurrences : 6091
        Impotant words in tain : 1362
        MWE length mean : 2.06
        Seen MWEs : 354 (69 %)
        New MWEs : 156 (30 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 11694
        After : 9710

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 9710 * POS : 118
__________________________________________________________________________________________________
        Dashed keys in vocabulary 2295
        One occurrence keys in vocabulary 2295 / 9710
        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_13 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 3, 200)       1942000     input_13[0][0]
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 3, 15)        1770        input_14[0][0]
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 600)          0           embedding_13[0][0]
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 45)           0           embedding_14[0][0]
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 645)          0           flatten_13[0][0]
                                                                 flatten_14[0][0]
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 25)           16150       concatenate_7[0][0]
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 25)           0           dense_13[0][0]
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            208         dropout_7[0][0]
==================================================================================================
Total params: 1,960,128
Trainable params: 1,960,128
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 303221
        data size after sampling = 742825
        5 Labels in train : Counter({0.0: 148565, 1.0: 148565, 2.0: 148565, 5.0: 148565, 6.0: 148565})
        5 Labels in valid : Counter({6.0: 15075, 5.0: 14854, 0.0: 14793, 2.0: 14788, 1.0: 14773})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 668542 samples, validate on 74283 samples
Epoch 1/40
 - 17s - loss: 0.3905 - acc: 0.9685 - val_loss: 0.2458 - val_acc: 0.9772
Epoch 2/40
 - 17s - loss: 0.2490 - acc: 0.9768 - val_loss: 0.2169 - val_acc: 0.9793
Epoch 3/40
 - 17s - loss: 0.2215 - acc: 0.9786 - val_loss: 0.2023 - val_acc: 0.9800

==================================================================================================
        Training time : 0:01:35.500544
==================================================================================================
        Identification : 0.463
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 63
        25 : 16
        50 : 9
        5 : 56

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 182
        25 : 7
        50 : 4
        5 : 26

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
        Language : TR
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 4458, Test : 1320
        MWEs in tain : 2083, occurrences : 6091
        Impotant words in tain : 1362
        MWE length mean : 2.06
        Seen MWEs : 354 (69 %)
        New MWEs : 156 (30 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 11694
        After : 9726

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 9726 * POS : 118
__________________________________________________________________________________________________
        Dashed keys in vocabulary 2295
        One occurrence keys in vocabulary 2295 / 9726
        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_15 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 3, 200)       1945200     input_15[0][0]
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 3, 15)        1770        input_16[0][0]
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 600)          0           embedding_15[0][0]
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 45)           0           embedding_16[0][0]
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 645)          0           flatten_15[0][0]
                                                                 flatten_16[0][0]
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 25)           16150       concatenate_8[0][0]
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 25)           0           dense_15[0][0]
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            208         dropout_8[0][0]
==================================================================================================
Total params: 1,963,328
Trainable params: 1,963,328
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 303221
        data size after sampling = 742825
        5 Labels in train : Counter({0.0: 148565, 1.0: 148565, 2.0: 148565, 5.0: 148565, 6.0: 148565})
        5 Labels in valid : Counter({5.0: 14951, 6.0: 14899, 1.0: 14893, 0.0: 14782, 2.0: 14758})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 668542 samples, validate on 74283 samples
Epoch 1/40
 - 17s - loss: 0.3978 - acc: 0.9680 - val_loss: 0.2460 - val_acc: 0.9779
Epoch 2/40
 - 17s - loss: 0.2438 - acc: 0.9778 - val_loss: 0.2077 - val_acc: 0.9795
Epoch 3/40
 - 17s - loss: 0.2185 - acc: 0.9794 - val_loss: 0.1966 - val_acc: 0.9801

==================================================================================================
        Training time : 0:01:35.563512
==================================================================================================
        Identification : 0.478
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 74
        25 : 17
        50 : 9
        5 : 62

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 170
        25 : 5
        50 : 4
        5 : 22

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
        Language : TR
==================================================================================================
        Dataset : Sharedtask 1.1
        Training (Important) : 4458, Test : 1320
        MWEs in tain : 2083, occurrences : 6091
        Impotant words in tain : 1362
        MWE length mean : 2.06
        Seen MWEs : 354 (69 %)
        New MWEs : 156 (30 %)
==================================================================================================
        Non frequent word cleaning:
==================================================================================================
        Before : 11694
        After : 9828

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 9828 * POS : 118
__________________________________________________________________________________________________
        Dashed keys in vocabulary 2295
        One occurrence keys in vocabulary 2295 / 9828
        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_17 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 3)            0
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 3, 200)       1965600     input_17[0][0]
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 3, 15)        1770        input_18[0][0]
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 600)          0           embedding_17[0][0]
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 45)           0           embedding_18[0][0]
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 645)          0           flatten_17[0][0]
                                                                 flatten_18[0][0]
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 25)           16150       concatenate_9[0][0]
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 25)           0           dense_17[0][0]
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            208         dropout_9[0][0]
==================================================================================================
Total params: 1,983,728
Trainable params: 1,983,728
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        data size before sampling = 303221
        data size after sampling = 742825
        5 Labels in train : Counter({0.0: 148565, 1.0: 148565, 2.0: 148565, 5.0: 148565, 6.0: 148565})
        5 Labels in valid : Counter({0.0: 15004, 6.0: 14884, 2.0: 14871, 1.0: 14816, 5.0: 14708})
        Favorisation Coeff : 10

__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 668542 samples, validate on 74283 samples
Epoch 1/40
 - 17s - loss: 0.4038 - acc: 0.9673 - val_loss: 0.2272 - val_acc: 0.9786
Epoch 2/40
 - 17s - loss: 0.2470 - acc: 0.9769 - val_loss: 0.2001 - val_acc: 0.9802
Epoch 3/40
 - 17s - loss: 0.2245 - acc: 0.9782 - val_loss: 0.1845 - val_acc: 0.9813

==================================================================================================
        Training time : 0:01:34.954479
==================================================================================================
        Identification : 0.474
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 68
        25 : 17
        50 : 9
        5 : 63

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 176
        25 : 5
        50 : 2
        5 : 19

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
