INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_11spVe.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 3841/4043 Mb (0.950000) on cuda
Mapped name None to device cuda: GeForce GTX 980 (0000:03:00.0)
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 200)       1509000     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 15)        2280        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 800)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 60)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 860)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 25)           21525       concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 25)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            208         dropout_1[0][0]                  
==================================================================================================
Total params: 1,533,013
Trainable params: 1,533,013
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 623847
	7 Labels in train : Counter({0.0: 89121, 1.0: 89121, 2.0: 89121, 3.0: 89121, 4.0: 89121, 5.0: 89121, 6.0: 89121})	7 Labels in valid : Counter({0.0: 9010, 1.0: 8975, 2.0: 8966, 5.0: 8901, 6.0: 8886, 3.0: 8836, 4.0: 8811})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 561462 samples, validate on 62385 samples
Epoch 1/40
 - 15s - loss: 0.1659 - acc: 0.9828 - val_loss: 0.0789 - val_acc: 0.9894
Epoch 2/40
 - 14s - loss: 0.0840 - acc: 0.9898 - val_loss: 0.0753 - val_acc: 0.9901
Epoch 3/40
 - 14s - loss: 0.0780 - acc: 0.9905 - val_loss: 0.0746 - val_acc: 0.9901

==================================================================================================
	Training time : 0:02:18.079151
==================================================================================================
	Identification : 0.744
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 106
	100 : 1
	5 : 82
	25 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 176
	5 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7540

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7540 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7540
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 200)       1508000     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 15)        2280        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 800)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 60)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 860)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 25)           21525       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 25)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            208         dropout_2[0][0]                  
==================================================================================================
Total params: 1,532,013
Trainable params: 1,532,013
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 623847
	7 Labels in train : Counter({0.0: 89121, 1.0: 89121, 2.0: 89121, 3.0: 89121, 4.0: 89121, 5.0: 89121, 6.0: 89121})	7 Labels in valid : Counter({3.0: 8992, 0.0: 8965, 2.0: 8951, 1.0: 8933, 4.0: 8898, 6.0: 8893, 5.0: 8753})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 561462 samples, validate on 62385 samples
Epoch 1/40
 - 14s - loss: 0.1573 - acc: 0.9842 - val_loss: 0.0739 - val_acc: 0.9897
Epoch 2/40
 - 14s - loss: 0.0794 - acc: 0.9906 - val_loss: 0.0715 - val_acc: 0.9903
Epoch 3/40
 - 14s - loss: 0.0737 - acc: 0.9913 - val_loss: 0.0700 - val_acc: 0.9907

==================================================================================================
	Training time : 0:01:07.768788
==================================================================================================
	Identification : 0.744
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	25 : 16
	100 : 1
	5 : 82
	0 : 103

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 180
	200 : 1
	100 : 1
	5 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1931
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.12
	Seen MWEs : 476 (75 %)
	New MWEs : 155 (24 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7494

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7494 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7494
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 200)       1498800     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 15)        2280        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 800)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 60)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 860)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 25)           21525       concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 25)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            208         dropout_3[0][0]                  
==================================================================================================
Total params: 1,522,813
Trainable params: 1,522,813
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 623847
	7 Labels in train : Counter({0.0: 89121, 1.0: 89121, 2.0: 89121, 3.0: 89121, 4.0: 89121, 5.0: 89121, 6.0: 89121})	7 Labels in valid : Counter({0.0: 9030, 5.0: 9008, 4.0: 8962, 2.0: 8900, 1.0: 8854, 3.0: 8836, 6.0: 8795})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 561462 samples, validate on 62385 samples
Epoch 1/40
 - 14s - loss: 0.1584 - acc: 0.9838 - val_loss: 0.0711 - val_acc: 0.9899
Epoch 2/40
 - 14s - loss: 0.0794 - acc: 0.9903 - val_loss: 0.0689 - val_acc: 0.9905
Epoch 3/40
 - 14s - loss: 0.0740 - acc: 0.9911 - val_loss: 0.0683 - val_acc: 0.9908

==================================================================================================
	Training time : 0:01:07.877241
==================================================================================================
	Identification : 0.742
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	200 : 1
	0 : 103
	100 : 1
	5 : 81
	25 : 16

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 178
	5 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9179
	After : 7184

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7184 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 7184
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 200)       1436800     input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 15)        2535        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 800)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 60)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 860)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 25)           21525       concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 25)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            208         dropout_4[0][0]                  
==================================================================================================
Total params: 1,461,068
Trainable params: 1,461,068
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 123202
	data size after sampling = 362052
	6 Labels in train : Counter({0.0: 60342, 1.0: 60342, 2.0: 60342, 4.0: 60342, 5.0: 60342, 6.0: 60342})	6 Labels in valid : Counter({2.0: 6109, 1.0: 6057, 6.0: 6024, 0.0: 6008, 5.0: 6006, 4.0: 6002})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 325846 samples, validate on 36206 samples
Epoch 1/40
 - 8s - loss: 0.1851 - acc: 0.9767 - val_loss: 0.0794 - val_acc: 0.9869
Epoch 2/40
 - 8s - loss: 0.0861 - acc: 0.9873 - val_loss: 0.0768 - val_acc: 0.9883
Epoch 3/40
 - 8s - loss: 0.0795 - acc: 0.9886 - val_loss: 0.0757 - val_acc: 0.9886

==================================================================================================
	Training time : 0:00:38.801112
==================================================================================================
	Identification : 0.636
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 98
	5 : 38

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 149
	25 : 1
	5 : 13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9179
	After : 7198

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7198 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 7198
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 200)       1439600     input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 15)        2535        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 800)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 60)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 860)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 25)           21525       concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 25)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            208         dropout_5[0][0]                  
==================================================================================================
Total params: 1,463,868
Trainable params: 1,463,868
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 123202
	data size after sampling = 362052
	6 Labels in train : Counter({0.0: 60342, 1.0: 60342, 2.0: 60342, 4.0: 60342, 5.0: 60342, 6.0: 60342})	6 Labels in valid : Counter({4.0: 6121, 1.0: 6055, 2.0: 6017, 5.0: 6017, 6.0: 6002, 0.0: 5994})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 325846 samples, validate on 36206 samples
Epoch 1/40
 - 8s - loss: 0.1971 - acc: 0.9746 - val_loss: 0.0786 - val_acc: 0.9876
Epoch 2/40
 - 8s - loss: 0.0907 - acc: 0.9866 - val_loss: 0.0738 - val_acc: 0.9885
Epoch 3/40
 - 8s - loss: 0.0823 - acc: 0.9881 - val_loss: 0.0725 - val_acc: 0.9890

==================================================================================================
	Training time : 0:00:39.013851
==================================================================================================
	Identification : 0.587
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 97
	5 : 37

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 151
	25 : 1
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 1937
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 212 (60 %)
	New MWEs : 137 (39 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9179
	After : 7173

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7173 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_o_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 1660
	One occurrence keys in vocabulary 1660 / 7173
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 200)       1434600     input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 15)        2535        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 800)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 60)           0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 860)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 25)           21525       concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 25)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            208         dropout_6[0][0]                  
==================================================================================================
Total params: 1,458,868
Trainable params: 1,458,868
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 123202
	data size after sampling = 362052
	6 Labels in train : Counter({0.0: 60342, 1.0: 60342, 2.0: 60342, 4.0: 60342, 5.0: 60342, 6.0: 60342})	6 Labels in valid : Counter({4.0: 6112, 6.0: 6061, 2.0: 6029, 1.0: 6027, 0.0: 6017, 5.0: 5960})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 325846 samples, validate on 36206 samples
Epoch 1/40
 - 8s - loss: 0.1758 - acc: 0.9767 - val_loss: 0.0793 - val_acc: 0.9871
Epoch 2/40
 - 8s - loss: 0.0825 - acc: 0.9879 - val_loss: 0.0780 - val_acc: 0.9878
Epoch 3/40
 - 8s - loss: 0.0753 - acc: 0.9892 - val_loss: 0.0763 - val_acc: 0.9884

==================================================================================================
	Training time : 0:00:38.817405
==================================================================================================
	Identification : 0.642
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 103
	5 : 38

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 143
	25 : 1
	5 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 10498
	After : 8766

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8766 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1967
	One occurrence keys in vocabulary 1967 / 8766
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 200)       1753200     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 15)        1635        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 800)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 60)           0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 860)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 25)           21525       concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 25)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            208         dropout_7[0][0]                  
==================================================================================================
Total params: 1,776,568
Trainable params: 1,776,568
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 245104
	data size after sampling = 600620
	5 Labels in train : Counter({0.0: 120124, 1.0: 120124, 2.0: 120124, 5.0: 120124, 6.0: 120124})	5 Labels in valid : Counter({6.0: 12182, 5.0: 12059, 1.0: 12035, 2.0: 11943, 0.0: 11843})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 540558 samples, validate on 60062 samples
Epoch 1/40
 - 14s - loss: 0.2946 - acc: 0.9719 - val_loss: 0.1442 - val_acc: 0.9820
Epoch 2/40
 - 14s - loss: 0.1510 - acc: 0.9818 - val_loss: 0.1245 - val_acc: 0.9826
Epoch 3/40
 - 14s - loss: 0.1359 - acc: 0.9828 - val_loss: 0.1240 - val_acc: 0.9831

==================================================================================================
	Training time : 0:01:16.138104
==================================================================================================
	Identification : 0.525
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 126
	25 : 16
	50 : 6
	5 : 84

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 273
	25 : 7
	50 : 2
	5 : 39

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 10498
	After : 8646

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8646 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1967
	One occurrence keys in vocabulary 1967 / 8646
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 200)       1729200     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 15)        1635        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 800)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 60)           0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 860)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 25)           21525       concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 25)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            208         dropout_8[0][0]                  
==================================================================================================
Total params: 1,752,568
Trainable params: 1,752,568
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 245104
	data size after sampling = 600620
	5 Labels in train : Counter({0.0: 120124, 1.0: 120124, 2.0: 120124, 5.0: 120124, 6.0: 120124})	5 Labels in valid : Counter({6.0: 12077, 1.0: 12040, 5.0: 12022, 2.0: 11979, 0.0: 11944})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 540558 samples, validate on 60062 samples
Epoch 1/40
 - 14s - loss: 0.2895 - acc: 0.9729 - val_loss: 0.1373 - val_acc: 0.9832
Epoch 2/40
 - 14s - loss: 0.1478 - acc: 0.9821 - val_loss: 0.1198 - val_acc: 0.9844
Epoch 3/40
 - 14s - loss: 0.1317 - acc: 0.9833 - val_loss: 0.1148 - val_acc: 0.9847

==================================================================================================
	Training time : 0:01:16.235142
==================================================================================================
	Identification : 0.548
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 123
	25 : 15
	50 : 6
	5 : 84

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 279
	25 : 5
	50 : 1
	5 : 36

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 2136
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 613 (74 %)
	New MWEs : 207 (25 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 10498
	After : 8649

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 8649 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1967
	One occurrence keys in vocabulary 1967 / 8649
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 200)       1729800     input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 15)        1635        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 800)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 60)           0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 860)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 25)           21525       concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 25)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            208         dropout_9[0][0]                  
==================================================================================================
Total params: 1,753,168
Trainable params: 1,753,168
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before sampling = 245104
	data size after sampling = 600620
	5 Labels in train : Counter({0.0: 120124, 1.0: 120124, 2.0: 120124, 5.0: 120124, 6.0: 120124})	5 Labels in valid : Counter({5.0: 12091, 2.0: 12036, 1.0: 12035, 6.0: 11962, 0.0: 11938})	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 540558 samples, validate on 60062 samples
Epoch 1/40
 - 14s - loss: 0.2786 - acc: 0.9737 - val_loss: 0.1317 - val_acc: 0.9832
Epoch 2/40
 - 14s - loss: 0.1396 - acc: 0.9831 - val_loss: 0.1194 - val_acc: 0.9837
Epoch 3/40
 - 14s - loss: 0.1250 - acc: 0.9841 - val_loss: 0.1125 - val_acc: 0.9845

==================================================================================================
	Training time : 0:01:16.360437
==================================================================================================
	Identification : 0.532
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 138
	25 : 17
	50 : 6
	5 : 83

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 262
	25 : 6
	50 : 3
	5 : 34

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
