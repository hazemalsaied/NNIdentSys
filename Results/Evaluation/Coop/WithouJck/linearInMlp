INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
ERROR (theano.gpuarray): Could not initialize pygpu, support disabled
Traceback (most recent call last):
  File "/home/halsaied/miniconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py", line 227, in <module>
    use(config.device)
  File "/home/halsaied/miniconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py", line 214, in use
    init_dev(device, preallocate=preallocate)
  File "/home/halsaied/miniconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py", line 99, in init_dev
    **args)
  File "pygpu/gpuarray.pyx", line 651, in pygpu.gpuarray.init
  File "pygpu/gpuarray.pyx", line 587, in pygpu.gpuarray.pygpu_init
GpuArrayException: cuInit: CUDA_ERROR_UNKNOWN: unknown error
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
	Mode: NON.COMPO
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: CORPUS
==================================================================================================
	Attention: CPU used
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, sampleWeight, importantSentences, importantTransitions, favorisationCoeff, focused, overSampling, verbose, epochs, dense1, dense2, earlyStop, optimizer, features, dense2UnitNumber, batchSize, lemma, lr, compactVocab, dense2Activation, frequentTokens, dense1UnitNumber, inputItems, posEmb, dense1Activation, tokenEmb, dense1Dropout, chickPoint, dense2Dropout, bPadding, loss, s1Padding, minDelta, predictVerbose, validationSplit, s0Padding
==================================================================================================
# Configs: NonCompo, sharedtask2, corpus, True, False, True, True, False, 6, True, True, 0, 40, True, False, True, adagrad, False, 0, 64, True, 0.059, False, relu, True, 58, 4, 42, relu, 480, 0.429, False, 0, 2, categorical_crossentropy, 5, 0.2, False, 0.1, 5
==================================================================================================
	Language : BG
==================================================================================================
	Training  : 19767, Test : 1832
	MWEs in tain : 2021, occurrences : 6034
	Impotant words in tain : 1589
	MWE length mean : 2.12
	Seen MWEs : 439 (65 %)
	New MWEs : 231 (34 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 530304
==================================================================================================
	Linear training time : 0:01:26.759507
==================================================================================================
	Identification : 0.652

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Training (Important) : 5248, Test : 1832
	MWEs in tain : 2021, occurrences : 6034
	Impotant words in tain : 1589
	MWE length mean : 2.12
	Seen MWEs : 439 (65 %)
	New MWEs : 231 (34 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12475
	After : 10224
Deep model(Non compositional)
# Parameters = 5037094
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 480)       4907520     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 42)        7476        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 1920)         0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 168)          0           embedding_2[0][0]                
__________________________________________________________________________________________________
input_3 (InputLayer)            (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 2096)         0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
                                                                 input_3[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 58)           121626      concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            472         dropout_1[0][0]                  
==================================================================================================
Total params: 5,037,094
Trainable params: 5,037,094
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 303816
	data size after focused sampling = 535621
	data size before sampling = 535621
	data size after sampling = 1624462
	7 Labels in train : Counter({0: 232066, 1: 232066, 2: 232066, 4: 232066, 5: 232066, 6: 232066, 7: 232066})
	7 Labels in valid : Counter({1: 23487, 6: 23248, 0: 23207, 7: 23181, 2: 23165, 4: 23105, 5: 23054})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1462015 samples, validate on 162447 samples
Epoch 1/40
 - 475s - loss: 0.0213 - acc: 0.9969 - val_loss: 0.0061 - val_acc: 0.9988
Epoch 2/40
 - 474s - loss: 0.0061 - acc: 0.9989 - val_loss: 0.0053 - val_acc: 0.9990
Epoch 3/40
 - 474s - loss: 0.0052 - acc: 0.9991 - val_loss: 0.0052 - val_acc: 0.9990

==================================================================================================
	Linear training time : 0:28:57.719131
==================================================================================================

==================================================================================================
	Parsing time : 0:00:29.257782
==================================================================================================
	Identification : 0.65

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : DE
==================================================================================================
	Training  : 7918, Test : 1078
	MWEs in tain : 1914, occurrences : 3233
	Impotant words in tain : 1661
	MWE length mean : 1.96
	Seen MWEs : 242 (48 %)
	New MWEs : 258 (51 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 476147
==================================================================================================
	Linear training time : 0:00:34.322891
==================================================================================================
	Identification : 0.508

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : DE
==================================================================================================
	Training (Important) : 2799, Test : 1078
	MWEs in tain : 1914, occurrences : 3233
	Impotant words in tain : 1661
	MWE length mean : 1.96
	Seen MWEs : 242 (48 %)
	New MWEs : 258 (51 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15152
	After : 10691
	Attention: important lemmas are not in vocbularyDeep model(Non compositional)
# Parameters = 5265958
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 480)       5131680     input_4[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 42)        12180       input_5[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 1920)         0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 168)          0           embedding_4[0][0]                
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 2096)         0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
                                                                 input_6[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 58)           121626      concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 58)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            472         dropout_2[0][0]                  
==================================================================================================
Total params: 5,265,958
Trainable params: 5,265,958
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 127179
	data size after focused sampling = 333671
	data size before sampling = 333671
	data size after sampling = 924308
	7 Labels in train : Counter({0: 132044, 1: 132044, 2: 132044, 4: 132044, 5: 132044, 6: 132044, 7: 132044})
	7 Labels in valid : Counter({4: 13394, 5: 13219, 2: 13194, 7: 13182, 1: 13174, 0: 13153, 6: 13115})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 831877 samples, validate on 92431 samples
Epoch 1/40
 - 281s - loss: 0.0356 - acc: 0.9963 - val_loss: 0.0045 - val_acc: 0.9995
Epoch 2/40
 - 281s - loss: 0.0072 - acc: 0.9992 - val_loss: 0.0036 - val_acc: 0.9995
Epoch 3/40
 - 280s - loss: 0.0055 - acc: 0.9993 - val_loss: 0.0037 - val_acc: 0.9995

==================================================================================================
	Linear training time : 0:16:24.273741
==================================================================================================

==================================================================================================
	Parsing time : 0:00:16.603583
==================================================================================================
	Identification : 0.498

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EL
==================================================================================================
	Training  : 6989, Test : 1261
	MWEs in tain : 1109, occurrences : 1831
	Impotant words in tain : 961
	MWE length mean : 2.37
	Seen MWEs : 284 (56 %)
	New MWEs : 217 (43 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 388355
==================================================================================================
	Linear training time : 0:00:33.020992
==================================================================================================
	Identification : 0.599

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EL
==================================================================================================
	Training (Important) : 1657, Test : 1261
	MWEs in tain : 1109, occurrences : 1831
	Impotant words in tain : 961
	MWE length mean : 2.37
	Seen MWEs : 284 (56 %)
	New MWEs : 217 (43 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9396
	After : 7129
Deep model(Non compositional)
# Parameters = 3552334
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 480)       3421920     input_7[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 42)        8316        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 1920)         0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 168)          0           embedding_6[0][0]                
__________________________________________________________________________________________________
input_9 (InputLayer)            (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 2096)         0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
                                                                 input_9[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 58)           121626      concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 58)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            472         dropout_3[0][0]                  
==================================================================================================
Total params: 3,552,334
Trainable params: 3,552,334
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 109355
	data size after focused sampling = 252653
	data size before sampling = 252653
	data size after sampling = 640602
	6 Labels in train : Counter({0: 106767, 1: 106767, 2: 106767, 5: 106767, 6: 106767, 7: 106767})
	6 Labels in valid : Counter({0: 10765, 1: 10746, 6: 10684, 7: 10673, 2: 10664, 5: 10529})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 576541 samples, validate on 64061 samples
Epoch 1/40
 - 137s - loss: 0.0271 - acc: 0.9965 - val_loss: 0.0022 - val_acc: 0.9995
Epoch 2/40
 - 137s - loss: 0.0036 - acc: 0.9995 - val_loss: 0.0015 - val_acc: 0.9997
Epoch 3/40
 - 137s - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0015 - val_acc: 0.9996

==================================================================================================
	Linear training time : 0:08:24.611566
==================================================================================================

==================================================================================================
	Parsing time : 0:00:27.244976
==================================================================================================
	Identification : 0.573

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EN
==================================================================================================
	Training  : 3471, Test : 3965
	MWEs in tain : 233, occurrences : 331
	Impotant words in tain : 241
	MWE length mean : 2.16
	Seen MWEs : 139 (27 %)
	New MWEs : 362 (72 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 135301
==================================================================================================
	Linear training time : 0:00:06.638917
==================================================================================================
	Identification : 0.352

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EN
==================================================================================================
	Training (Important) : 300, Test : 3965
	MWEs in tain : 233, occurrences : 331
	Impotant words in tain : 241
	MWE length mean : 2.16
	Seen MWEs : 139 (27 %)
	New MWEs : 362 (72 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 1967
	After : 1459
Deep model(Non compositional)
# Parameters = 825022
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 480)       700320      input_10[0][0]                   
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 42)        2604        input_11[0][0]                   
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 1920)         0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 168)          0           embedding_8[0][0]                
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 2096)         0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
                                                                 input_12[0][0]                   
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 58)           121626      concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            472         dropout_4[0][0]                  
==================================================================================================
Total params: 825,022
Trainable params: 825,022
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 14463
	data size after focused sampling = 41063
	data size before sampling = 41063
	data size after sampling = 98724
	6 Labels in train : Counter({0: 16454, 1: 16454, 2: 16454, 5: 16454, 6: 16454, 7: 16454})
	6 Labels in valid : Counter({5: 1689, 0: 1671, 7: 1642, 6: 1637, 2: 1622, 1: 1612})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 88851 samples, validate on 9873 samples
Epoch 1/40
 - 8s - loss: 0.0981 - acc: 0.9869 - val_loss: 0.0048 - val_acc: 0.9983
Epoch 2/40
 - 8s - loss: 0.0068 - acc: 0.9987 - val_loss: 0.0019 - val_acc: 0.9989
Epoch 3/40
 - 8s - loss: 0.0036 - acc: 0.9994 - val_loss: 0.0012 - val_acc: 0.9997

==================================================================================================
	Linear training time : 0:00:54.407725
==================================================================================================

==================================================================================================
	Parsing time : 0:00:50.677716
==================================================================================================
	Identification : 0.344

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : ES
==================================================================================================
	Training  : 3469, Test : 2046
	MWEs in tain : 1098, occurrences : 2029
	Impotant words in tain : 827
	MWE length mean : 2.27
	Seen MWEs : 273 (54 %)
	New MWEs : 227 (45 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 265703
==================================================================================================
	Linear training time : 0:00:19.442061
==================================================================================================
	Identification : 0.392

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : ES
==================================================================================================
	Training (Important) : 1450, Test : 2046
	MWEs in tain : 1098, occurrences : 2029
	Impotant words in tain : 827
	MWE length mean : 2.27
	Seen MWEs : 273 (54 %)
	New MWEs : 227 (45 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 8412
	After : 6597
Deep model(Non compositional)
# Parameters = 3293908
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 480)       3166560     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 42)        5250        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 1920)         0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 168)          0           embedding_10[0][0]               
__________________________________________________________________________________________________
input_15 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 2096)         0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
                                                                 input_15[0][0]                   
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 58)           121626      concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 58)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            472         dropout_5[0][0]                  
==================================================================================================
Total params: 3,293,908
Trainable params: 3,293,908
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 122431
	data size after focused sampling = 258282
	data size before sampling = 258282
	data size after sampling = 753438
	7 Labels in train : Counter({0: 107634, 1: 107634, 2: 107634, 4: 107634, 5: 107634, 6: 107634, 7: 107634})
	7 Labels in valid : Counter({4: 10919, 7: 10850, 2: 10807, 0: 10797, 1: 10760, 5: 10638, 6: 10573})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 678094 samples, validate on 75344 samples
Epoch 1/40
 - 151s - loss: 0.0326 - acc: 0.9962 - val_loss: 0.0033 - val_acc: 0.9995
Epoch 2/40
 - 151s - loss: 0.0034 - acc: 0.9995 - val_loss: 0.0026 - val_acc: 0.9996
Epoch 3/40
 - 151s - loss: 0.0028 - acc: 0.9996 - val_loss: 0.0024 - val_acc: 0.9996

==================================================================================================
	Linear training time : 0:09:21.057455
==================================================================================================

==================================================================================================
	Parsing time : 0:00:54.481849
==================================================================================================
	Identification : 0.403

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EU
==================================================================================================
	Training  : 9754, Test : 1404
	MWEs in tain : 932, occurrences : 3270
	Impotant words in tain : 628
	MWE length mean : 2.02
	Seen MWEs : 433 (86 %)
	New MWEs : 67 (13 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 325269
==================================================================================================
	Linear training time : 0:00:23.900772
==================================================================================================
	Identification : 0.818

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EU
==================================================================================================
	Training (Important) : 2703, Test : 1404
	MWEs in tain : 932, occurrences : 3270
	Impotant words in tain : 628
	MWE length mean : 2.02
	Seen MWEs : 433 (86 %)
	New MWEs : 67 (13 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 6768
	After : 5335
Deep model(Non compositional)
# Parameters = 2688022
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 480)       2560800     input_16[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 42)        5124        input_17[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 1920)         0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 168)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 2096)         0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
                                                                 input_18[0][0]                   
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 58)           121626      concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 58)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            472         dropout_6[0][0]                  
==================================================================================================
Total params: 2,688,022
Trainable params: 2,688,022
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 95390
	data size after focused sampling = 188654
	data size before sampling = 188654
	data size after sampling = 387810
	5 Labels in train : Counter({0: 77562, 1: 77562, 2: 77562, 5: 77562, 6: 77562})
	5 Labels in valid : Counter({0: 7801, 1: 7767, 5: 7765, 2: 7752, 6: 7696})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 349029 samples, validate on 38781 samples
Epoch 1/40
 - 69s - loss: 0.0419 - acc: 0.9947 - val_loss: 0.0037 - val_acc: 0.9997
Epoch 2/40
 - 68s - loss: 0.0050 - acc: 0.9993 - val_loss: 0.0025 - val_acc: 0.9998
Epoch 3/40
 - 68s - loss: 0.0034 - acc: 0.9995 - val_loss: 0.0026 - val_acc: 0.9998

==================================================================================================
	Linear training time : 0:04:22.387113
==================================================================================================

==================================================================================================
	Parsing time : 0:00:14.202604
==================================================================================================
	Identification : 0.791

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FA
==================================================================================================
	Training  : 3258, Test : 359
	MWEs in tain : 1293, occurrences : 2944
	Impotant words in tain : 814
	MWE length mean : 2.14
	Seen MWEs : 330 (65 %)
	New MWEs : 171 (34 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 139197
==================================================================================================
	Linear training time : 0:00:08.375348
==================================================================================================
	Identification : 0.712

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FA
==================================================================================================
	Training (Important) : 1965, Test : 359
	MWEs in tain : 1293, occurrences : 2944
	Impotant words in tain : 814
	MWE length mean : 2.14
	Seen MWEs : 330 (65 %)
	New MWEs : 171 (34 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 5494
	After : 4679
Deep model(Non compositional)
# Parameters = 2371210
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 480)       2245920     input_19[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 42)        3192        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 1920)         0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 168)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
input_21 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 2096)         0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
                                                                 input_21[0][0]                   
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 58)           121626      concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 58)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            472         dropout_7[0][0]                  
==================================================================================================
Total params: 2,371,210
Trainable params: 2,371,210
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 80942
	data size after focused sampling = 222779
	data size before sampling = 222779
	data size after sampling = 531552
	6 Labels in train : Counter({0: 88592, 1: 88592, 2: 88592, 4: 88592, 5: 88592, 6: 88592})
	6 Labels in valid : Counter({2: 8978, 0: 8950, 6: 8904, 4: 8847, 1: 8758, 5: 8719})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 478396 samples, validate on 53156 samples
Epoch 1/40
 - 83s - loss: 0.0256 - acc: 0.9965 - val_loss: 0.0038 - val_acc: 0.9994
Epoch 2/40
 - 83s - loss: 0.0042 - acc: 0.9993 - val_loss: 0.0025 - val_acc: 0.9995
Epoch 3/40
 - 83s - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0022 - val_acc: 0.9996

==================================================================================================
	Linear training time : 0:05:05.232514
==================================================================================================

==================================================================================================
	Parsing time : 0:00:05.725722
==================================================================================================
	Identification : 0.81

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FR
==================================================================================================
	Training  : 19461, Test : 1606
	MWEs in tain : 1733, occurrences : 5116
	Impotant words in tain : 1295
	MWE length mean : 2.29
	Seen MWEs : 250 (50 %)
	New MWEs : 248 (49 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 750547
==================================================================================================
	Linear training time : 0:01:32.205825
==================================================================================================
	Identification : 0.594

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FR
==================================================================================================
	Training (Important) : 4426, Test : 1606
	MWEs in tain : 1733, occurrences : 5116
	Impotant words in tain : 1295
	MWE length mean : 2.29
	Seen MWEs : 250 (50 %)
	New MWEs : 248 (49 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15266
	After : 11636
Deep model(Non compositional)
# Parameters = 5716744
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 480)       5585280     input_22[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 42)        9366        input_23[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 1920)         0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 168)          0           embedding_16[0][0]               
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 2096)         0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
                                                                 input_24[0][0]                   
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 58)           121626      concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 58)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            472         dropout_8[0][0]                  
==================================================================================================
Total params: 5,716,744
Trainable params: 5,716,744
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 271347
	data size after focused sampling = 485741
	data size before sampling = 485741
	data size after sampling = 1271562
	6 Labels in train : Counter({0: 211927, 1: 211927, 2: 211927, 4: 211927, 5: 211927, 6: 211927})
	6 Labels in valid : Counter({0: 21319, 4: 21263, 1: 21240, 6: 21207, 5: 21104, 2: 21024})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1144405 samples, validate on 127157 samples
Epoch 1/40
 - 410s - loss: 0.0201 - acc: 0.9972 - val_loss: 0.0036 - val_acc: 0.9991
Epoch 2/40
 - 411s - loss: 0.0040 - acc: 0.9991 - val_loss: 0.0031 - val_acc: 0.9993
Epoch 3/40
 - 410s - loss: 0.0032 - acc: 0.9993 - val_loss: 0.0027 - val_acc: 0.9994

==================================================================================================
	Linear training time : 0:24:17.061693
==================================================================================================

==================================================================================================
	Parsing time : 0:00:33.192222
==================================================================================================
	Identification : 0.572

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HE
==================================================================================================
	Training  : 15491, Test : 3209
	MWEs in tain : 1226, occurrences : 1716
	Impotant words in tain : 1666
	MWE length mean : 2.39
	Seen MWEs : 199 (39 %)
	New MWEs : 303 (60 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 826017
==================================================================================================
	Linear training time : 0:00:46.397567
==================================================================================================
	Identification : 0.403

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HE
==================================================================================================
	Training (Important) : 1593, Test : 3209
	MWEs in tain : 1226, occurrences : 1716
	Impotant words in tain : 1666
	MWE length mean : 2.39
	Seen MWEs : 199 (39 %)
	New MWEs : 303 (60 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12063
	After : 9261
Deep model(Non compositional)
# Parameters = 4580818
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 480)       4445280     input_25[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 42)        13440       input_26[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 1920)         0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 168)          0           embedding_18[0][0]               
__________________________________________________________________________________________________
input_27 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 2096)         0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
                                                                 input_27[0][0]                   
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 58)           121626      concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 58)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            472         dropout_9[0][0]                  
==================================================================================================
Total params: 4,580,818
Trainable params: 4,580,818
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 77521
	data size after focused sampling = 239825
	data size before sampling = 239825
	data size after sampling = 590034
	6 Labels in train : Counter({0: 98339, 1: 98339, 2: 98339, 5: 98339, 6: 98339, 7: 98339})
	6 Labels in valid : Counter({0: 9930, 2: 9908, 7: 9885, 5: 9826, 1: 9763, 6: 9692})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 531030 samples, validate on 59004 samples
Epoch 1/40
 - 157s - loss: 0.0305 - acc: 0.9969 - val_loss: 0.0011 - val_acc: 0.9999
Epoch 2/40
 - 157s - loss: 0.0026 - acc: 0.9998 - val_loss: 0.0012 - val_acc: 0.9999
Epoch 3/40
 - 157s - loss: 0.0019 - acc: 0.9998 - val_loss: 0.0012 - val_acc: 0.9999

==================================================================================================
	Linear training time : 0:09:30.429514
==================================================================================================

==================================================================================================
	Parsing time : 0:00:49.697764
==================================================================================================
	Identification : 0.415

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HI
==================================================================================================
	Training  : 856, Test : 828
	MWEs in tain : 245, occurrences : 466
	Impotant words in tain : 247
	MWE length mean : 2.14
	Seen MWEs : 273 (54 %)
	New MWEs : 227 (45 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 54229
==================================================================================================
	Linear training time : 0:00:02.900474
==================================================================================================
	Identification : 0.716

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HI
==================================================================================================
	Training (Important) : 418, Test : 828
	MWEs in tain : 245, occurrences : 466
	Impotant words in tain : 247
	MWE length mean : 2.14
	Seen MWEs : 273 (54 %)
	New MWEs : 227 (45 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 2267
	After : 1764
Deep model(Non compositional)
# Parameters = 970708
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 480)       846720      input_28[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 42)        1890        input_29[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 1920)         0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 168)          0           embedding_20[0][0]               
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 2096)         0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
                                                                 input_30[0][0]                   
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 58)           121626      concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 58)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            472         dropout_10[0][0]                 
==================================================================================================
Total params: 970,708
Trainable params: 970,708
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 19978
	data size after focused sampling = 46105
	data size before sampling = 46105
	data size after sampling = 93825
	5 Labels in train : Counter({0: 18765, 1: 18765, 2: 18765, 5: 18765, 6: 18765})
	5 Labels in valid : Counter({6: 1930, 0: 1892, 1: 1876, 5: 1859, 2: 1826})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 84442 samples, validate on 9383 samples
Epoch 1/40
 - 8s - loss: 0.0630 - acc: 0.9901 - val_loss: 0.0030 - val_acc: 0.9994
Epoch 2/40
 - 8s - loss: 0.0074 - acc: 0.9986 - val_loss: 0.0018 - val_acc: 0.9995
Epoch 3/40
 - 8s - loss: 0.0042 - acc: 0.9991 - val_loss: 0.0015 - val_acc: 0.9995

==================================================================================================
	Linear training time : 0:00:39.848761
==================================================================================================

==================================================================================================
	Parsing time : 0:00:13.668202
==================================================================================================
	Identification : 0.721

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HR
==================================================================================================
	Training  : 3129, Test : 708
	MWEs in tain : 1075, occurrences : 1820
	Impotant words in tain : 901
	MWE length mean : 2.2
	Seen MWEs : 284 (56 %)
	New MWEs : 217 (43 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 237199
==================================================================================================
	Linear training time : 0:00:10.749958
==================================================================================================
	Identification : 0.588

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HR
==================================================================================================
	Training (Important) : 1363, Test : 708
	MWEs in tain : 1075, occurrences : 1820
	Impotant words in tain : 901
	MWE length mean : 2.2
	Seen MWEs : 284 (56 %)
	New MWEs : 217 (43 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 7740
	After : 5952
Deep model(Non compositional)
# Parameters = 2984098
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 480)       2856960     input_31[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 42)        5040        input_32[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 1920)         0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 168)          0           embedding_22[0][0]               
__________________________________________________________________________________________________
input_33 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 2096)         0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
                                                                 input_33[0][0]                   
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 58)           121626      concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 58)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            472         dropout_11[0][0]                 
==================================================================================================
Total params: 2,984,098
Trainable params: 2,984,098
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 72048
	data size after focused sampling = 197682
	data size before sampling = 197682
	data size after sampling = 532098
	7 Labels in train : Counter({0: 76014, 1: 76014, 2: 76014, 4: 76014, 5: 76014, 6: 76014, 7: 76014})
	7 Labels in valid : Counter({2: 7697, 0: 7673, 6: 7645, 5: 7578, 7: 7575, 1: 7539, 4: 7503})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 478888 samples, validate on 53210 samples
Epoch 1/40
 - 99s - loss: 0.0408 - acc: 0.9958 - val_loss: 0.0014 - val_acc: 0.9999
Epoch 2/40
 - 99s - loss: 0.0030 - acc: 0.9997 - val_loss: 0.0012 - val_acc: 0.9999
Epoch 3/40
 - 99s - loss: 0.0021 - acc: 0.9998 - val_loss: 0.0011 - val_acc: 0.9998

==================================================================================================
	Linear training time : 0:05:58.769279
==================================================================================================

==================================================================================================
	Parsing time : 0:00:12.841830
==================================================================================================
	Identification : 0.591

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HU
==================================================================================================
	Training  : 5404, Test : 755
	MWEs in tain : 734, occurrences : 6898
	Impotant words in tain : 602
	MWE length mean : 1.26
	Seen MWEs : 706 (90 %)
	New MWEs : 70 (9 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 220470
==================================================================================================
	Linear training time : 0:00:20.772251
==================================================================================================
	Identification : 0.925

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HU
==================================================================================================
	Training (Important) : 3595, Test : 755
	MWEs in tain : 734, occurrences : 6898
	Impotant words in tain : 602
	MWE length mean : 1.26
	Seen MWEs : 706 (90 %)
	New MWEs : 70 (9 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 6177
	After : 5049
Deep model(Non compositional)
# Parameters = 2547718
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 480)       2423520     input_34[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 42)        2100        input_35[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 1920)         0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 168)          0           embedding_24[0][0]               
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 2096)         0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
                                                                 input_36[0][0]                   
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 58)           121626      concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 58)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            472         dropout_12[0][0]                 
==================================================================================================
Total params: 2,547,718
Trainable params: 2,547,718
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 210093
	data size after focused sampling = 262401
	data size before sampling = 262401
	data size after sampling = 696996
	6 Labels in train : Counter({0: 116166, 1: 116166, 2: 116166, 5: 116166, 6: 116166, 7: 116166})
	6 Labels in valid : Counter({2: 11746, 0: 11733, 6: 11636, 7: 11581, 5: 11559, 1: 11445})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 627296 samples, validate on 69700 samples
Epoch 1/40
 - 114s - loss: 0.0273 - acc: 0.9973 - val_loss: 0.0024 - val_acc: 0.9998
Epoch 2/40
 - 118s - loss: 0.0027 - acc: 0.9997 - val_loss: 0.0011 - val_acc: 0.9998
Epoch 3/40
 - 118s - loss: 0.0019 - acc: 0.9998 - val_loss: 0.0015 - val_acc: 0.9998

==================================================================================================
	Linear training time : 0:07:29.728734
==================================================================================================

==================================================================================================
	Parsing time : 0:00:15.853718
==================================================================================================
	Identification : 0.926

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : IT
==================================================================================================
	Training  : 14472, Test : 1256
	MWEs in tain : 1573, occurrences : 3500
	Impotant words in tain : 1232
	MWE length mean : 2.48
	Seen MWEs : 297 (59 %)
	New MWEs : 206 (40 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 613822
==================================================================================================
	Linear training time : 0:01:14.710319
==================================================================================================
	Identification : 0.6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : IT
==================================================================================================
	Training (Important) : 2824, Test : 1256
	MWEs in tain : 1573, occurrences : 3500
	Impotant words in tain : 1232
	MWE length mean : 2.48
	Seen MWEs : 297 (59 %)
	New MWEs : 206 (40 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 13966
	After : 10875
	Attention: important lemmas are not in vocbularyDeep model(Non compositional)
# Parameters = 5352892
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 480)       5220000     input_37[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 42)        10794       input_38[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 1920)         0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 168)          0           embedding_26[0][0]               
__________________________________________________________________________________________________
input_39 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 2096)         0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
                                                                 input_39[0][0]                   
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 58)           121626      concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 58)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            472         dropout_13[0][0]                 
==================================================================================================
Total params: 5,352,892
Trainable params: 5,352,892
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 235420
	data size after focused sampling = 462691
	data size before sampling = 462691
	data size after sampling = 1416114
	7 Labels in train : Counter({0: 202302, 1: 202302, 2: 202302, 4: 202302, 5: 202302, 6: 202302, 7: 202302})
	7 Labels in valid : Counter({5: 20558, 1: 20373, 0: 20224, 4: 20204, 2: 20116, 6: 20084, 7: 20053})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1274502 samples, validate on 141612 samples
Epoch 1/40
 - 435s - loss: 0.0236 - acc: 0.9969 - val_loss: 0.0043 - val_acc: 0.9989
Epoch 2/40
 - 435s - loss: 0.0053 - acc: 0.9990 - val_loss: 0.0037 - val_acc: 0.9993
Epoch 3/40
 - 435s - loss: 0.0047 - acc: 0.9991 - val_loss: 0.0035 - val_acc: 0.9993

==================================================================================================
	Linear training time : 0:25:39.023768
==================================================================================================

==================================================================================================
	Parsing time : 0:00:28.845779
==================================================================================================
	Identification : 0.569

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : LT
==================================================================================================
	Training  : 4895, Test : 6209
	MWEs in tain : 192, occurrences : 312
	Impotant words in tain : 263
	MWE length mean : 2.21
	Seen MWEs : 191 (38 %)
	New MWEs : 309 (61 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 234578
==================================================================================================
	Linear training time : 0:00:10.874132
==================================================================================================
	Identification : 0.415

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : LT
==================================================================================================
	Training (Important) : 297, Test : 6209
	MWEs in tain : 192, occurrences : 312
	Impotant words in tain : 263
	MWE length mean : 2.21
	Seen MWEs : 191 (38 %)
	New MWEs : 309 (61 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 2353
	After : 1790
Deep model(Non compositional)
# Parameters = 983650
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 480)       859200      input_40[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 42)        2352        input_41[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 1920)         0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 168)          0           embedding_28[0][0]               
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 2096)         0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
                                                                 input_42[0][0]                   
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 58)           121626      concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 58)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            472         dropout_14[0][0]                 
==================================================================================================
Total params: 983,650
Trainable params: 983,650
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 13980
	data size after focused sampling = 36326
	data size before sampling = 36326
	data size after sampling = 74015
	5 Labels in train : Counter({0: 14803, 1: 14803, 2: 14803, 5: 14803, 6: 14803})
	5 Labels in valid : Counter({0: 1541, 1: 1492, 5: 1476, 6: 1448, 2: 1445})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 66613 samples, validate on 7402 samples
Epoch 1/40
 - 6s - loss: 0.1095 - acc: 0.9849 - val_loss: 0.0052 - val_acc: 0.9988
Epoch 2/40
 - 6s - loss: 0.0061 - acc: 0.9989 - val_loss: 0.0020 - val_acc: 0.9995
Epoch 3/40
 - 6s - loss: 0.0039 - acc: 0.9994 - val_loss: 7.9848e-04 - val_acc: 0.9996

==================================================================================================
	Linear training time : 0:01:01.671699
==================================================================================================

==================================================================================================
	Parsing time : 0:01:26.126811
==================================================================================================
	Identification : 0.401

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PL
==================================================================================================
	Training  : 14821, Test : 1300
	MWEs in tain : 1750, occurrences : 4510
	Impotant words in tain : 1399
	MWE length mean : 2.13
	Seen MWEs : 351 (68 %)
	New MWEs : 164 (31 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 579582
==================================================================================================
	Linear training time : 0:00:36.956255
==================================================================================================
	Identification : 0.688

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PL
==================================================================================================
	Training (Important) : 3918, Test : 1300
	MWEs in tain : 1750, occurrences : 4510
	Impotant words in tain : 1399
	MWE length mean : 2.13
	Seen MWEs : 351 (68 %)
	New MWEs : 164 (31 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 13185
	After : 10269
Deep model(Non compositional)
# Parameters = 5057812
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 480)       4929120     input_43[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 42)        6594        input_44[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 1920)         0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 168)          0           embedding_30[0][0]               
__________________________________________________________________________________________________
input_45 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 2096)         0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
                                                                 input_45[0][0]                   
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 58)           121626      concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 58)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            472         dropout_15[0][0]                 
==================================================================================================
Total params: 5,057,812
Trainable params: 5,057,812
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 163545
	data size after focused sampling = 357048
	data size before sampling = 357048
	data size after sampling = 1030869
	7 Labels in train : Counter({0: 147267, 1: 147267, 2: 147267, 4: 147267, 5: 147267, 6: 147267, 7: 147267})
	7 Labels in valid : Counter({7: 14932, 2: 14754, 5: 14731, 6: 14706, 1: 14667, 4: 14658, 0: 14639})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 927782 samples, validate on 103087 samples
Epoch 1/40
 - 302s - loss: 0.0198 - acc: 0.9980 - val_loss: 8.5151e-04 - val_acc: 0.9999
Epoch 2/40
 - 302s - loss: 0.0019 - acc: 0.9998 - val_loss: 5.6930e-04 - val_acc: 0.9999
Epoch 3/40
 - 302s - loss: 0.0012 - acc: 0.9998 - val_loss: 5.3576e-04 - val_acc: 0.9999

==================================================================================================
	Linear training time : 0:18:01.791862
==================================================================================================

==================================================================================================
	Parsing time : 0:00:20.584993
==================================================================================================
	Identification : 0.671

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Training  : 25134, Test : 2770
	MWEs in tain : 2174, occurrences : 4869
	Impotant words in tain : 1594
	MWE length mean : 2.22
	Seen MWEs : 378 (68 %)
	New MWEs : 175 (31 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 713520
==================================================================================================
	Linear training time : 0:01:45.647567
==================================================================================================
	Identification : 0.692

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Training (Important) : 4474, Test : 2770
	MWEs in tain : 2174, occurrences : 4869
	Impotant words in tain : 1594
	MWE length mean : 2.22
	Seen MWEs : 378 (68 %)
	New MWEs : 175 (31 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 13624
	After : 10731
	Attention: important lemmas are not in vocbularyDeep model(Non compositional)
# Parameters = 5283226
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 480)       5150880     input_46[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 42)        10248       input_47[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 1920)         0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 168)          0           embedding_32[0][0]               
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 2096)         0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
                                                                 input_48[0][0]                   
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 58)           121626      concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 58)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            472         dropout_16[0][0]                 
==================================================================================================
Total params: 5,283,226
Trainable params: 5,283,226
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 240159
	data size after focused sampling = 492672
	data size before sampling = 492672
	data size after sampling = 1248540
	6 Labels in train : Counter({0: 208090, 1: 208090, 2: 208090, 4: 208090, 5: 208090, 6: 208090})
	6 Labels in valid : Counter({0: 20937, 6: 20871, 2: 20842, 5: 20833, 1: 20712, 4: 20659})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1123686 samples, validate on 124854 samples
Epoch 1/40
 - 378s - loss: 0.0170 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 0.9994
Epoch 2/40
 - 377s - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0023 - val_acc: 0.9994
Epoch 3/40
 - 377s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0023 - val_acc: 0.9993

==================================================================================================
	Linear training time : 0:22:35.426920
==================================================================================================

==================================================================================================
	Parsing time : 0:00:43.697779
==================================================================================================
	Identification : 0.604

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : RO
==================================================================================================
	Training  : 49769, Test : 6934
	MWEs in tain : 591, occurrences : 5300
	Impotant words in tain : 528
	MWE length mean : 2.13
	Seen MWEs : 556 (94 %)
	New MWEs : 33 (5 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 811500
==================================================================================================
	Linear training time : 0:03:38.563238
==================================================================================================
	Identification : 0.783

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : RO
==================================================================================================
	Training (Important) : 4797, Test : 6934
	MWEs in tain : 591, occurrences : 5300
	Impotant words in tain : 528
	MWE length mean : 2.13
	Seen MWEs : 556 (94 %)
	New MWEs : 33 (5 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 11692
	After : 9702
Deep model(Non compositional)
# Parameters = 4783552
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 480)       4656960     input_49[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 42)        4494        input_50[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 1920)         0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 168)          0           embedding_34[0][0]               
__________________________________________________________________________________________________
input_51 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 2096)         0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
                                                                 input_51[0][0]                   
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 58)           121626      concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 58)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            472         dropout_17[0][0]                 
==================================================================================================
Total params: 4,783,552
Trainable params: 4,783,552
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 350536
	data size after focused sampling = 414337
	data size before sampling = 414337
	data size after sampling = 1173168
	6 Labels in train : Counter({0: 195528, 1: 195528, 2: 195528, 4: 195528, 5: 195528, 6: 195528})
	6 Labels in valid : Counter({6: 19749, 2: 19702, 0: 19513, 4: 19507, 5: 19501, 1: 19345})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1055851 samples, validate on 117317 samples
Epoch 1/40
 - 324s - loss: 0.0241 - acc: 0.9976 - val_loss: 0.0041 - val_acc: 0.9995
Epoch 2/40
 - 324s - loss: 0.0055 - acc: 0.9994 - val_loss: 0.0036 - val_acc: 0.9995
Epoch 3/40
 - 324s - loss: 0.0048 - acc: 0.9995 - val_loss: 0.0039 - val_acc: 0.9995

==================================================================================================
	Linear training time : 0:20:12.164320
==================================================================================================

==================================================================================================
	Parsing time : 0:01:29.444999
==================================================================================================
	Identification : 0.829

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : SL
==================================================================================================
	Training  : 11517, Test : 1994
	MWEs in tain : 1098, occurrences : 2853
	Impotant words in tain : 933
	MWE length mean : 2.23
	Seen MWEs : 360 (72 %)
	New MWEs : 140 (28 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 428479
==================================================================================================
	Linear training time : 0:00:32.671813
==================================================================================================
	Identification : 0.603

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : SL
==================================================================================================
	Training (Important) : 2516, Test : 1994
	MWEs in tain : 1098, occurrences : 2853
	Impotant words in tain : 933
	MWE length mean : 2.23
	Seen MWEs : 360 (72 %)
	New MWEs : 140 (28 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 11516
	After : 8701
Deep model(Non compositional)
# Parameters = 4298998
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 480)       4176480     input_52[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 42)        420         input_53[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 1920)         0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 168)          0           embedding_36[0][0]               
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 2096)         0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
                                                                 input_54[0][0]                   
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 58)           121626      concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 58)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            472         dropout_18[0][0]                 
==================================================================================================
Total params: 4,298,998
Trainable params: 4,298,998
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 133529
	data size after focused sampling = 274167
	data size before sampling = 274167
	data size after sampling = 823550
	7 Labels in train : Counter({0: 117650, 1: 117650, 2: 117650, 4: 117650, 5: 117650, 6: 117650, 7: 117650})
	7 Labels in valid : Counter({4: 11881, 7: 11877, 1: 11757, 0: 11755, 2: 11755, 5: 11725, 6: 11605})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 741195 samples, validate on 82355 samples
Epoch 1/40
 - 214s - loss: 0.0403 - acc: 0.9947 - val_loss: 0.0082 - val_acc: 0.9986
Epoch 2/40
 - 214s - loss: 0.0068 - acc: 0.9987 - val_loss: 0.0059 - val_acc: 0.9988
Epoch 3/40
 - 214s - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0056 - val_acc: 0.9989

==================================================================================================
	Linear training time : 0:12:49.372292
==================================================================================================

==================================================================================================
	Parsing time : 0:00:32.865120
==================================================================================================
	Identification : 0.64

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Training  : 18035, Test : 589
	MWEs in tain : 2224, occurrences : 6601
	Impotant words in tain : 1428
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 152 (30 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 526288
==================================================================================================
	Linear training time : 0:01:13.767644
==================================================================================================
	Identification : 0.554

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Training (Important) : 4843, Test : 589
	MWEs in tain : 2224, occurrences : 6601
	Impotant words in tain : 1428
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 152 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12373
	After : 10370
Deep model(Non compositional)
# Parameters = 5104864
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 480)       4977600     input_55[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 42)        5166        input_56[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 1920)         0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 168)          0           embedding_38[0][0]               
__________________________________________________________________________________________________
input_57 (InputLayer)           (None, 8)            0                                            
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 2096)         0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
                                                                 input_57[0][0]                   
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 58)           121626      concatenate_19[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 58)           0           dense_37[0][0]                   
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            472         dropout_19[0][0]                 
==================================================================================================
Total params: 5,104,864
Trainable params: 5,104,864
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 329139
	data size after focused sampling = 564064
	data size before sampling = 564064
	data size after sampling = 1214195
	5 Labels in train : Counter({0: 242839, 1: 242839, 2: 242839, 5: 242839, 6: 242839})
	5 Labels in valid : Counter({2: 24403, 1: 24355, 6: 24350, 0: 24226, 5: 24086})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1092775 samples, validate on 121420 samples
Epoch 1/40
 - 356s - loss: 0.0292 - acc: 0.9949 - val_loss: 0.0124 - val_acc: 0.9977
Epoch 2/40
 - 356s - loss: 0.0121 - acc: 0.9974 - val_loss: 0.0117 - val_acc: 0.9979
Epoch 3/40
 - 356s - loss: 0.0112 - acc: 0.9975 - val_loss: 0.0114 - val_acc: 0.9979

==================================================================================================
	Linear training time : 0:21:20.352438
==================================================================================================

==================================================================================================
	Parsing time : 0:00:11.865829
==================================================================================================
	Identification : 0.588

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
