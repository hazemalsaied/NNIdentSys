INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
ERROR (theano.gpuarray): Could not initialize pygpu, support disabled
Traceback (most recent call last):
  File "/home/halsaied/miniconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py", line 227, in <module>
    use(config.device)
  File "/home/halsaied/miniconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py", line 214, in use
    init_dev(device, preallocate=preallocate)
  File "/home/halsaied/miniconda2/lib/python2.7/site-packages/theano/gpuarray/__init__.py", line 99, in init_dev
    **args)
  File "pygpu/gpuarray.pyx", line 651, in pygpu.gpuarray.init
  File "pygpu/gpuarray.pyx", line 587, in pygpu.gpuarray.pygpu_init
GpuArrayException: cuInit: CUDA_ERROR_UNKNOWN: unknown error
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
	Mode: NON.COMPO
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: CORPUS
==================================================================================================
	Attention: CPU used
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, sampleWeight, importantSentences, importantTransitions, favorisationCoeff, focused, overSampling, verbose, epochs, dense1, dense2, earlyStop, optimizer, features, dense2UnitNumber, batchSize, lemma, lr, compactVocab, dense2Activation, frequentTokens, dense1UnitNumber, inputItems, posEmb, dense1Activation, tokenEmb, dense1Dropout, chickPoint, dense2Dropout, bPadding, loss, s1Padding, minDelta, predictVerbose, validationSplit, s0Padding
==================================================================================================
# Configs: NonCompo, sharedtask2, corpus, True, False, True, True, False, 6, True, True, 0, 40, True, False, True, adagrad, False, 0, 64, True, 0.059, False, relu, True, 58, 4, 42, relu, 480, 0.429, False, 0, 2, categorical_crossentropy, 5, 0.2, False, 0.1, 5
==================================================================================================
	Language : BG
==================================================================================================
	Training (Important) : 5248, Test : 1832
	MWEs in tain : 2021, occurrences : 6034
	Impotant words in tain : 1589
	MWE length mean : 2.12
	Seen MWEs : 439 (65 %)
	New MWEs : 231 (34 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12475
	After : 10224
Deep model(Non compositional)
# Parameters = 5036630
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 480)       4907520     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 42)        7476        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 1920)         0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 168)          0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 2088)         0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 58)           121162      concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            472         dropout_1[0][0]                  
==================================================================================================
Total params: 5,036,630
Trainable params: 5,036,630
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 303816
	data size after focused sampling = 535621
	data size before sampling = 535621
	data size after sampling = 1624462
	7 Labels in train : Counter({0: 232066, 1: 232066, 2: 232066, 4: 232066, 5: 232066, 6: 232066, 7: 232066})
	7 Labels in valid : Counter({1: 23487, 6: 23248, 0: 23207, 7: 23181, 2: 23165, 4: 23105, 5: 23054})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1462015 samples, validate on 162447 samples
Epoch 1/40
 - 837s - loss: 0.0916 - acc: 0.9866 - val_loss: 0.0544 - val_acc: 0.9911
Epoch 2/40
 - 839s - loss: 0.0540 - acc: 0.9915 - val_loss: 0.0513 - val_acc: 0.9915
Epoch 3/40
 - 838s - loss: 0.0502 - acc: 0.9920 - val_loss: 0.0505 - val_acc: 0.9915

==================================================================================================
	MLP training time : 0:47:50.009455
==================================================================================================
	Identification : 0.65

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Training  : 19767, Test : 1832
	MWEs in tain : 2021, occurrences : 6034
	Impotant words in tain : 1589
	MWE length mean : 2.12
	Seen MWEs : 439 (65 %)
	New MWEs : 231 (34 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 530305
==================================================================================================
	Linear training time : 0:06:50.006714
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.867910
==================================================================================================
	Identification : 0.664

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : DE
==================================================================================================
	Training (Important) : 2799, Test : 1078
	MWEs in tain : 1914, occurrences : 3233
	Impotant words in tain : 1661
	MWE length mean : 1.96
	Seen MWEs : 242 (48 %)
	New MWEs : 258 (51 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15152
	After : 10691
	Attention: important lemmas are not in vocbularyDeep model(Non compositional)
# Parameters = 5265494
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 480)       5131680     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 42)        12180       input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 1920)         0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 168)          0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 2088)         0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 58)           121162      concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 58)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            472         dropout_2[0][0]                  
==================================================================================================
Total params: 5,265,494
Trainable params: 5,265,494
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 127179
	data size after focused sampling = 333671
	data size before sampling = 333671
	data size after sampling = 924308
	7 Labels in train : Counter({0: 132044, 1: 132044, 2: 132044, 4: 132044, 5: 132044, 6: 132044, 7: 132044})
	7 Labels in valid : Counter({4: 13394, 5: 13219, 2: 13194, 7: 13182, 1: 13174, 0: 13153, 6: 13115})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 831877 samples, validate on 92431 samples
Epoch 1/40
 - 498s - loss: 0.1719 - acc: 0.9774 - val_loss: 0.1078 - val_acc: 0.9856
Epoch 2/40
 - 498s - loss: 0.1087 - acc: 0.9867 - val_loss: 0.1031 - val_acc: 0.9866
Epoch 3/40
 - 498s - loss: 0.1021 - acc: 0.9875 - val_loss: 0.1012 - val_acc: 0.9874

==================================================================================================
	MLP training time : 0:27:56.372313
==================================================================================================
	Identification : 0.477

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : DE
==================================================================================================
	Training  : 7918, Test : 1078
	MWEs in tain : 1914, occurrences : 3233
	Impotant words in tain : 1661
	MWE length mean : 1.96
	Seen MWEs : 242 (48 %)
	New MWEs : 258 (51 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 476148
==================================================================================================
	Linear training time : 0:02:20.786001
==================================================================================================

==================================================================================================
	Parsing time : 0:00:14.191320
==================================================================================================
	Identification : 0.509

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EL
==================================================================================================
	Training (Important) : 1657, Test : 1261
	MWEs in tain : 1109, occurrences : 1831
	Impotant words in tain : 961
	MWE length mean : 2.37
	Seen MWEs : 284 (56 %)
	New MWEs : 217 (43 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9396
	After : 7129
Deep model(Non compositional)
# Parameters = 3551870
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 480)       3421920     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 42)        8316        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 1920)         0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 168)          0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 2088)         0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 58)           121162      concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 58)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            472         dropout_3[0][0]                  
==================================================================================================
Total params: 3,551,870
Trainable params: 3,551,870
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 109355
	data size after focused sampling = 252653
	data size before sampling = 252653
	data size after sampling = 640602
	6 Labels in train : Counter({0: 106767, 1: 106767, 2: 106767, 5: 106767, 6: 106767, 7: 106767})
	6 Labels in valid : Counter({0: 10765, 1: 10746, 6: 10684, 7: 10673, 2: 10664, 5: 10529})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 576541 samples, validate on 64061 samples
Epoch 1/40
 - 238s - loss: 0.1043 - acc: 0.9845 - val_loss: 0.0514 - val_acc: 0.9917
Epoch 2/40
 - 238s - loss: 0.0550 - acc: 0.9918 - val_loss: 0.0491 - val_acc: 0.9924
Epoch 3/40
 - 238s - loss: 0.0507 - acc: 0.9924 - val_loss: 0.0507 - val_acc: 0.9924

==================================================================================================
	MLP training time : 0:13:22.631447
==================================================================================================
	Identification : 0.605

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EL
==================================================================================================
	Training  : 6989, Test : 1261
	MWEs in tain : 1109, occurrences : 1831
	Impotant words in tain : 961
	MWE length mean : 2.37
	Seen MWEs : 284 (56 %)
	New MWEs : 217 (43 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 388356
==================================================================================================
	Linear training time : 0:02:26.557084
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.255841
==================================================================================================
	Identification : 0.587

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EN
==================================================================================================
	Training (Important) : 300, Test : 3965
	MWEs in tain : 233, occurrences : 331
	Impotant words in tain : 241
	MWE length mean : 2.16
	Seen MWEs : 139 (27 %)
	New MWEs : 362 (72 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 1967
	After : 1459
Deep model(Non compositional)
# Parameters = 824558
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 480)       700320      input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 42)        2604        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 1920)         0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 168)          0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 2088)         0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 58)           121162      concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            472         dropout_4[0][0]                  
==================================================================================================
Total params: 824,558
Trainable params: 824,558
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 14463
	data size after focused sampling = 41063
	data size before sampling = 41063
	data size after sampling = 98724
	6 Labels in train : Counter({0: 16454, 1: 16454, 2: 16454, 5: 16454, 6: 16454, 7: 16454})
	6 Labels in valid : Counter({5: 1689, 0: 1671, 7: 1642, 6: 1637, 2: 1622, 1: 1612})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 88851 samples, validate on 9873 samples
Epoch 1/40
 - 10s - loss: 0.1778 - acc: 0.9762 - val_loss: 0.0665 - val_acc: 0.9894
Epoch 2/40
 - 10s - loss: 0.0740 - acc: 0.9896 - val_loss: 0.0657 - val_acc: 0.9905
Epoch 3/40
 - 10s - loss: 0.0664 - acc: 0.9903 - val_loss: 0.0629 - val_acc: 0.9906

==================================================================================================
	MLP training time : 0:00:36.886958
==================================================================================================
	Identification : 0.325

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EN
==================================================================================================
	Training  : 3471, Test : 3965
	MWEs in tain : 233, occurrences : 331
	Impotant words in tain : 241
	MWE length mean : 2.16
	Seen MWEs : 139 (27 %)
	New MWEs : 362 (72 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 135302
==================================================================================================
	Linear training time : 0:00:40.174866
==================================================================================================

==================================================================================================
	Parsing time : 0:00:45.145644
==================================================================================================
	Identification : 0.346

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : ES
==================================================================================================
	Training (Important) : 1450, Test : 2046
	MWEs in tain : 1098, occurrences : 2029
	Impotant words in tain : 827
	MWE length mean : 2.27
	Seen MWEs : 273 (54 %)
	New MWEs : 227 (45 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 8412
	After : 6597
Deep model(Non compositional)
# Parameters = 3293444
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 480)       3166560     input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 42)        5250        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 1920)         0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 168)          0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 2088)         0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 58)           121162      concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 58)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            472         dropout_5[0][0]                  
==================================================================================================
Total params: 3,293,444
Trainable params: 3,293,444
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 122431
	data size after focused sampling = 258282
	data size before sampling = 258282
	data size after sampling = 753438
	7 Labels in train : Counter({0: 107634, 1: 107634, 2: 107634, 4: 107634, 5: 107634, 6: 107634, 7: 107634})
	7 Labels in valid : Counter({4: 10919, 7: 10850, 2: 10807, 0: 10797, 1: 10760, 5: 10638, 6: 10573})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 678094 samples, validate on 75344 samples
Epoch 1/40
 - 263s - loss: 0.1506 - acc: 0.9817 - val_loss: 0.0994 - val_acc: 0.9876
Epoch 2/40
 - 263s - loss: 0.1001 - acc: 0.9878 - val_loss: 0.0945 - val_acc: 0.9883
Epoch 3/40
 - 262s - loss: 0.0941 - acc: 0.9888 - val_loss: 0.0934 - val_acc: 0.9887

==================================================================================================
	MLP training time : 0:14:47.114425
==================================================================================================
	Identification : 0.383

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : ES
==================================================================================================
	Training  : 3469, Test : 2046
	MWEs in tain : 1098, occurrences : 2029
	Impotant words in tain : 827
	MWE length mean : 2.27
	Seen MWEs : 273 (54 %)
	New MWEs : 227 (45 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 265704
==================================================================================================
	Linear training time : 0:01:42.991756
==================================================================================================

==================================================================================================
	Parsing time : 0:00:46.080969
==================================================================================================
	Identification : 0.39

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EU
==================================================================================================
	Training (Important) : 2703, Test : 1404
	MWEs in tain : 932, occurrences : 3270
	Impotant words in tain : 628
	MWE length mean : 2.02
	Seen MWEs : 433 (86 %)
	New MWEs : 67 (13 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 6768
	After : 5335
Deep model(Non compositional)
# Parameters = 2687558
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 480)       2560800     input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 42)        5124        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 1920)         0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 168)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 2088)         0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 58)           121162      concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 58)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            472         dropout_6[0][0]                  
==================================================================================================
Total params: 2,687,558
Trainable params: 2,687,558
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 95390
	data size after focused sampling = 188654
	data size before sampling = 188654
	data size after sampling = 387810
	5 Labels in train : Counter({0: 77562, 1: 77562, 2: 77562, 5: 77562, 6: 77562})
	5 Labels in valid : Counter({0: 7801, 1: 7767, 5: 7765, 2: 7752, 6: 7696})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 349029 samples, validate on 38781 samples
Epoch 1/40
 - 111s - loss: 0.1849 - acc: 0.9685 - val_loss: 0.1006 - val_acc: 0.9802
Epoch 2/40
 - 111s - loss: 0.1119 - acc: 0.9789 - val_loss: 0.0962 - val_acc: 0.9816
Epoch 3/40
 - 111s - loss: 0.1032 - acc: 0.9803 - val_loss: 0.0938 - val_acc: 0.9821

==================================================================================================
	MLP training time : 0:06:18.431446
==================================================================================================
	Identification : 0.78

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EU
==================================================================================================
	Training  : 9754, Test : 1404
	MWEs in tain : 932, occurrences : 3270
	Impotant words in tain : 628
	MWE length mean : 2.02
	Seen MWEs : 433 (86 %)
	New MWEs : 67 (13 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 325270
==================================================================================================
	Linear training time : 0:01:46.978492
==================================================================================================

==================================================================================================
	Parsing time : 0:00:12.701479
==================================================================================================
	Identification : 0.825

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FA
==================================================================================================
	Training (Important) : 1965, Test : 359
	MWEs in tain : 1293, occurrences : 2944
	Impotant words in tain : 814
	MWE length mean : 2.14
	Seen MWEs : 330 (65 %)
	New MWEs : 171 (34 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 5494
	After : 4679
Deep model(Non compositional)
# Parameters = 2370746
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 480)       2245920     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 42)        3192        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 1920)         0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 168)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 2088)         0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 58)           121162      concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 58)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            472         dropout_7[0][0]                  
==================================================================================================
Total params: 2,370,746
Trainable params: 2,370,746
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 80942
	data size after focused sampling = 222779
	data size before sampling = 222779
	data size after sampling = 531552
	6 Labels in train : Counter({0: 88592, 1: 88592, 2: 88592, 4: 88592, 5: 88592, 6: 88592})
	6 Labels in valid : Counter({2: 8978, 0: 8950, 6: 8904, 4: 8847, 1: 8758, 5: 8719})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 478396 samples, validate on 53156 samples
Epoch 1/40
 - 138s - loss: 0.1164 - acc: 0.9787 - val_loss: 0.0777 - val_acc: 0.9842
Epoch 2/40
 - 138s - loss: 0.0810 - acc: 0.9841 - val_loss: 0.0758 - val_acc: 0.9852
Epoch 3/40
 - 138s - loss: 0.0775 - acc: 0.9847 - val_loss: 0.0756 - val_acc: 0.9850

==================================================================================================
	MLP training time : 0:07:47.731829
==================================================================================================
	Identification : 0.763

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FA
==================================================================================================
	Training  : 3258, Test : 359
	MWEs in tain : 1293, occurrences : 2944
	Impotant words in tain : 814
	MWE length mean : 2.14
	Seen MWEs : 330 (65 %)
	New MWEs : 171 (34 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 139198
==================================================================================================
	Linear training time : 0:00:43.978096
==================================================================================================

==================================================================================================
	Parsing time : 0:00:05.545928
==================================================================================================
	Identification : 0.695

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FR
==================================================================================================
	Training (Important) : 4426, Test : 1606
	MWEs in tain : 1733, occurrences : 5116
	Impotant words in tain : 1295
	MWE length mean : 2.29
	Seen MWEs : 250 (50 %)
	New MWEs : 248 (49 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 15266
	After : 11636
Deep model(Non compositional)
# Parameters = 5716280
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 480)       5585280     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 42)        9366        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 1920)         0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 168)          0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 2088)         0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 58)           121162      concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 58)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            472         dropout_8[0][0]                  
==================================================================================================
Total params: 5,716,280
Trainable params: 5,716,280
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 271347
	data size after focused sampling = 485741
	data size before sampling = 485741
	data size after sampling = 1271562
	6 Labels in train : Counter({0: 211927, 1: 211927, 2: 211927, 4: 211927, 5: 211927, 6: 211927})
	6 Labels in valid : Counter({0: 21319, 4: 21263, 1: 21240, 6: 21207, 5: 21104, 2: 21024})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1144405 samples, validate on 127157 samples
Epoch 1/40
 - 866s - loss: 0.1006 - acc: 0.9845 - val_loss: 0.0610 - val_acc: 0.9895
Epoch 2/40
 - 866s - loss: 0.0610 - acc: 0.9900 - val_loss: 0.0604 - val_acc: 0.9899
Epoch 3/40
 - 866s - loss: 0.0571 - acc: 0.9906 - val_loss: 0.0585 - val_acc: 0.9902

==================================================================================================
	MLP training time : 0:48:27.400937
==================================================================================================
	Identification : 0.574

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FR
==================================================================================================
	Training  : 19461, Test : 1606
	MWEs in tain : 1733, occurrences : 5116
	Impotant words in tain : 1295
	MWE length mean : 2.29
	Seen MWEs : 250 (50 %)
	New MWEs : 248 (49 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 750548
==================================================================================================
	Linear training time : 0:06:29.685858
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.293183
==================================================================================================
	Identification : 0.588

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HE
==================================================================================================
	Training (Important) : 1593, Test : 3209
	MWEs in tain : 1226, occurrences : 1716
	Impotant words in tain : 1666
	MWE length mean : 2.39
	Seen MWEs : 199 (39 %)
	New MWEs : 303 (60 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12063
	After : 9261
Deep model(Non compositional)
# Parameters = 4580354
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 480)       4445280     input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 42)        13440       input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 1920)         0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 168)          0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 2088)         0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 58)           121162      concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 58)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            472         dropout_9[0][0]                  
==================================================================================================
Total params: 4,580,354
Trainable params: 4,580,354
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 77521
	data size after focused sampling = 239825
	data size before sampling = 239825
	data size after sampling = 590034
	6 Labels in train : Counter({0: 98339, 1: 98339, 2: 98339, 5: 98339, 6: 98339, 7: 98339})
	6 Labels in valid : Counter({0: 9930, 2: 9908, 7: 9885, 5: 9826, 1: 9763, 6: 9692})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 531030 samples, validate on 59004 samples
Epoch 1/40
 - 279s - loss: 0.1360 - acc: 0.9811 - val_loss: 0.0680 - val_acc: 0.9903
Epoch 2/40
 - 279s - loss: 0.0708 - acc: 0.9902 - val_loss: 0.0631 - val_acc: 0.9909
Epoch 3/40
 - 278s - loss: 0.0644 - acc: 0.9908 - val_loss: 0.0605 - val_acc: 0.9908

==================================================================================================
	MLP training time : 0:15:37.833210
==================================================================================================
	Identification : 0.455

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HE
==================================================================================================
	Training  : 15491, Test : 3209
	MWEs in tain : 1226, occurrences : 1716
	Impotant words in tain : 1666
	MWE length mean : 2.39
	Seen MWEs : 199 (39 %)
	New MWEs : 303 (60 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 826018
==================================================================================================
	Linear training time : 0:03:56.466714
==================================================================================================

==================================================================================================
	Parsing time : 0:00:43.534207
==================================================================================================
	Identification : 0.433

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HI
==================================================================================================
	Training (Important) : 418, Test : 828
	MWEs in tain : 245, occurrences : 466
	Impotant words in tain : 247
	MWE length mean : 2.14
	Seen MWEs : 273 (54 %)
	New MWEs : 227 (45 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 2267
	After : 1764
Deep model(Non compositional)
# Parameters = 970244
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 480)       846720      input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 42)        1890        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 1920)         0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 168)          0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 2088)         0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 58)           121162      concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 58)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            472         dropout_10[0][0]                 
==================================================================================================
Total params: 970,244
Trainable params: 970,244
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 19978
	data size after focused sampling = 46105
	data size before sampling = 46105
	data size after sampling = 93825
	5 Labels in train : Counter({0: 18765, 1: 18765, 2: 18765, 5: 18765, 6: 18765})
	5 Labels in valid : Counter({6: 1930, 0: 1892, 1: 1876, 5: 1859, 2: 1826})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 84442 samples, validate on 9383 samples
Epoch 1/40
 - 11s - loss: 0.1513 - acc: 0.9757 - val_loss: 0.0707 - val_acc: 0.9866
Epoch 2/40
 - 11s - loss: 0.0755 - acc: 0.9874 - val_loss: 0.0643 - val_acc: 0.9884
Epoch 3/40
 - 11s - loss: 0.0698 - acc: 0.9881 - val_loss: 0.0650 - val_acc: 0.9883

==================================================================================================
	MLP training time : 0:00:39.464865
==================================================================================================
	Identification : 0.622

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HI
==================================================================================================
	Training  : 856, Test : 828
	MWEs in tain : 245, occurrences : 466
	Impotant words in tain : 247
	MWE length mean : 2.14
	Seen MWEs : 273 (54 %)
	New MWEs : 227 (45 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 54230
==================================================================================================
	Linear training time : 0:00:13.586403
==================================================================================================

==================================================================================================
	Parsing time : 0:00:11.331268
==================================================================================================
	Identification : 0.627

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HR
==================================================================================================
	Training (Important) : 1363, Test : 708
	MWEs in tain : 1075, occurrences : 1820
	Impotant words in tain : 901
	MWE length mean : 2.2
	Seen MWEs : 284 (56 %)
	New MWEs : 217 (43 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 7740
	After : 5952
Deep model(Non compositional)
# Parameters = 2983634
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 480)       2856960     input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 42)        5040        input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 1920)         0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 168)          0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 2088)         0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 58)           121162      concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 58)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            472         dropout_11[0][0]                 
==================================================================================================
Total params: 2,983,634
Trainable params: 2,983,634
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 72048
	data size after focused sampling = 197682
	data size before sampling = 197682
	data size after sampling = 532098
	7 Labels in train : Counter({0: 76014, 1: 76014, 2: 76014, 4: 76014, 5: 76014, 6: 76014, 7: 76014})
	7 Labels in valid : Counter({2: 7697, 0: 7673, 6: 7645, 5: 7578, 7: 7575, 1: 7539, 4: 7503})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 478888 samples, validate on 53210 samples
Epoch 1/40
 - 168s - loss: 0.2147 - acc: 0.9720 - val_loss: 0.1322 - val_acc: 0.9810
Epoch 2/40
 - 169s - loss: 0.1431 - acc: 0.9805 - val_loss: 0.1288 - val_acc: 0.9816
Epoch 3/40
 - 168s - loss: 0.1356 - acc: 0.9813 - val_loss: 0.1271 - val_acc: 0.9820

==================================================================================================
	MLP training time : 0:09:30.381750
==================================================================================================
	Identification : 0.528

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HR
==================================================================================================
	Training  : 3129, Test : 708
	MWEs in tain : 1075, occurrences : 1820
	Impotant words in tain : 901
	MWE length mean : 2.2
	Seen MWEs : 284 (56 %)
	New MWEs : 217 (43 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 237200
==================================================================================================
	Linear training time : 0:00:58.087261
==================================================================================================

==================================================================================================
	Parsing time : 0:00:11.210564
==================================================================================================
	Identification : 0.59

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HU
==================================================================================================
	Training (Important) : 3595, Test : 755
	MWEs in tain : 734, occurrences : 6898
	Impotant words in tain : 602
	MWE length mean : 1.26
	Seen MWEs : 706 (90 %)
	New MWEs : 70 (9 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 6177
	After : 5049
Deep model(Non compositional)
# Parameters = 2547254
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 480)       2423520     input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 42)        2100        input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 1920)         0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 168)          0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 2088)         0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 58)           121162      concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 58)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            472         dropout_12[0][0]                 
==================================================================================================
Total params: 2,547,254
Trainable params: 2,547,254
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 210093
	data size after focused sampling = 262401
	data size before sampling = 262401
	data size after sampling = 696996
	6 Labels in train : Counter({0: 116166, 1: 116166, 2: 116166, 5: 116166, 6: 116166, 7: 116166})
	6 Labels in valid : Counter({2: 11746, 0: 11733, 6: 11636, 7: 11581, 5: 11559, 1: 11445})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 627296 samples, validate on 69700 samples
Epoch 1/40
 - 189s - loss: 0.1917 - acc: 0.9722 - val_loss: 0.1598 - val_acc: 0.9770
Epoch 2/40
 - 189s - loss: 0.1554 - acc: 0.9767 - val_loss: 0.1528 - val_acc: 0.9776
Epoch 3/40
 - 189s - loss: 0.1512 - acc: 0.9773 - val_loss: 0.1541 - val_acc: 0.9773

==================================================================================================
	MLP training time : 0:10:43.384634
==================================================================================================
	Identification : 0.928

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HU
==================================================================================================
	Training  : 5404, Test : 755
	MWEs in tain : 734, occurrences : 6898
	Impotant words in tain : 602
	MWE length mean : 1.26
	Seen MWEs : 706 (90 %)
	New MWEs : 70 (9 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 220471
==================================================================================================
	Linear training time : 0:02:08.824373
==================================================================================================

==================================================================================================
	Parsing time : 0:00:13.960182
==================================================================================================
	Identification : 0.914

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : IT
==================================================================================================
	Training (Important) : 2824, Test : 1256
	MWEs in tain : 1573, occurrences : 3500
	Impotant words in tain : 1232
	MWE length mean : 2.48
	Seen MWEs : 297 (59 %)
	New MWEs : 206 (40 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 13966
	After : 10875
	Attention: important lemmas are not in vocbularyDeep model(Non compositional)
# Parameters = 5352428
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 480)       5220000     input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 42)        10794       input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 1920)         0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 168)          0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 2088)         0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 58)           121162      concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 58)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            472         dropout_13[0][0]                 
==================================================================================================
Total params: 5,352,428
Trainable params: 5,352,428
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 235420
	data size after focused sampling = 462691
	data size before sampling = 462691
	data size after sampling = 1416114
	7 Labels in train : Counter({0: 202302, 1: 202302, 2: 202302, 4: 202302, 5: 202302, 6: 202302, 7: 202302})
	7 Labels in valid : Counter({5: 20558, 1: 20373, 0: 20224, 4: 20204, 2: 20116, 6: 20084, 7: 20053})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1274502 samples, validate on 141612 samples
Epoch 1/40
 - 773s - loss: 0.1057 - acc: 0.9858 - val_loss: 0.0690 - val_acc: 0.9904
Epoch 2/40
 - 772s - loss: 0.0675 - acc: 0.9910 - val_loss: 0.0669 - val_acc: 0.9912
Epoch 3/40
 - 772s - loss: 0.0632 - acc: 0.9917 - val_loss: 0.0657 - val_acc: 0.9916

==================================================================================================
	MLP training time : 0:43:14.641253
==================================================================================================
	Identification : 0.467

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : IT
==================================================================================================
	Training  : 14472, Test : 1256
	MWEs in tain : 1573, occurrences : 3500
	Impotant words in tain : 1232
	MWE length mean : 2.48
	Seen MWEs : 297 (59 %)
	New MWEs : 206 (40 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 613823
==================================================================================================
	Linear training time : 0:05:09.743981
==================================================================================================

==================================================================================================
	Parsing time : 0:00:21.560200
==================================================================================================
	Identification : 0.576

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : LT
==================================================================================================
	Training (Important) : 297, Test : 6209
	MWEs in tain : 192, occurrences : 312
	Impotant words in tain : 263
	MWE length mean : 2.21
	Seen MWEs : 191 (38 %)
	New MWEs : 309 (61 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 2353
	After : 1790
Deep model(Non compositional)
# Parameters = 983186
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 480)       859200      input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 42)        2352        input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 1920)         0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 168)          0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 2088)         0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 58)           121162      concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 58)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            472         dropout_14[0][0]                 
==================================================================================================
Total params: 983,186
Trainable params: 983,186
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 13980
	data size after focused sampling = 36326
	data size before sampling = 36326
	data size after sampling = 74015
	5 Labels in train : Counter({0: 14803, 1: 14803, 2: 14803, 5: 14803, 6: 14803})
	5 Labels in valid : Counter({0: 1541, 1: 1492, 5: 1476, 6: 1448, 2: 1445})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 66613 samples, validate on 7402 samples
Epoch 1/40
 - 9s - loss: 0.1910 - acc: 0.9725 - val_loss: 0.0701 - val_acc: 0.9866
Epoch 2/40
 - 9s - loss: 0.0700 - acc: 0.9893 - val_loss: 0.0706 - val_acc: 0.9889
Epoch 3/40
 - 9s - loss: 0.0658 - acc: 0.9898 - val_loss: 0.0695 - val_acc: 0.9891

==================================================================================================
	MLP training time : 0:00:31.630886
==================================================================================================
	Identification : 0.458

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : LT
==================================================================================================
	Training  : 4895, Test : 6209
	MWEs in tain : 192, occurrences : 312
	Impotant words in tain : 263
	MWE length mean : 2.21
	Seen MWEs : 191 (38 %)
	New MWEs : 309 (61 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 234579
==================================================================================================
	Linear training time : 0:01:05.030733
==================================================================================================

==================================================================================================
	Parsing time : 0:01:14.838164
==================================================================================================
	Identification : 0.413

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PL
==================================================================================================
	Training (Important) : 3918, Test : 1300
	MWEs in tain : 1750, occurrences : 4510
	Impotant words in tain : 1399
	MWE length mean : 2.13
	Seen MWEs : 351 (68 %)
	New MWEs : 164 (31 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 13185
	After : 10269
Deep model(Non compositional)
# Parameters = 5057348
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 480)       4929120     input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 42)        6594        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 1920)         0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 168)          0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 2088)         0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 58)           121162      concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 58)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            472         dropout_15[0][0]                 
==================================================================================================
Total params: 5,057,348
Trainable params: 5,057,348
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 163545
	data size after focused sampling = 357048
	data size before sampling = 357048
	data size after sampling = 1030869
	7 Labels in train : Counter({0: 147267, 1: 147267, 2: 147267, 4: 147267, 5: 147267, 6: 147267, 7: 147267})
	7 Labels in valid : Counter({7: 14932, 2: 14754, 5: 14731, 6: 14706, 1: 14667, 4: 14658, 0: 14639})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 927782 samples, validate on 103087 samples
Epoch 1/40
 - 535s - loss: 0.1141 - acc: 0.9834 - val_loss: 0.0720 - val_acc: 0.9880
Epoch 2/40
 - 535s - loss: 0.0737 - acc: 0.9887 - val_loss: 0.0698 - val_acc: 0.9888
Epoch 3/40
 - 534s - loss: 0.0697 - acc: 0.9893 - val_loss: 0.0695 - val_acc: 0.9891

==================================================================================================
	MLP training time : 0:29:57.423959
==================================================================================================
	Identification : 0.688

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PL
==================================================================================================
	Training  : 14821, Test : 1300
	MWEs in tain : 1750, occurrences : 4510
	Impotant words in tain : 1399
	MWE length mean : 2.13
	Seen MWEs : 351 (68 %)
	New MWEs : 164 (31 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 579583
==================================================================================================
	Linear training time : 0:03:19.042893
==================================================================================================

==================================================================================================
	Parsing time : 0:00:16.876155
==================================================================================================
	Identification : 0.695

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Training (Important) : 4474, Test : 2770
	MWEs in tain : 2174, occurrences : 4869
	Impotant words in tain : 1594
	MWE length mean : 2.22
	Seen MWEs : 378 (68 %)
	New MWEs : 175 (31 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 13624
	After : 10731
	Attention: important lemmas are not in vocbularyDeep model(Non compositional)
# Parameters = 5282762
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 480)       5150880     input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 42)        10248       input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 1920)         0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 168)          0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 2088)         0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 58)           121162      concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 58)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            472         dropout_16[0][0]                 
==================================================================================================
Total params: 5,282,762
Trainable params: 5,282,762
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 240159
	data size after focused sampling = 492672
	data size before sampling = 492672
	data size after sampling = 1248540
	6 Labels in train : Counter({0: 208090, 1: 208090, 2: 208090, 4: 208090, 5: 208090, 6: 208090})
	6 Labels in valid : Counter({0: 20937, 6: 20871, 2: 20842, 5: 20833, 1: 20712, 4: 20659})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1123686 samples, validate on 124854 samples
Epoch 1/40
 - 672s - loss: 0.0978 - acc: 0.9849 - val_loss: 0.0595 - val_acc: 0.9899
Epoch 2/40
 - 672s - loss: 0.0625 - acc: 0.9898 - val_loss: 0.0575 - val_acc: 0.9903
Epoch 3/40
 - 673s - loss: 0.0581 - acc: 0.9906 - val_loss: 0.0559 - val_acc: 0.9907

==================================================================================================
	MLP training time : 0:37:41.430353
==================================================================================================
	Identification : 0.658

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Training  : 25134, Test : 2770
	MWEs in tain : 2174, occurrences : 4869
	Impotant words in tain : 1594
	MWE length mean : 2.22
	Seen MWEs : 378 (68 %)
	New MWEs : 175 (31 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 713521
==================================================================================================
	Linear training time : 0:07:04.384907
==================================================================================================

==================================================================================================
	Parsing time : 0:00:39.320166
==================================================================================================
	Identification : 0.701

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : RO
==================================================================================================
	Training (Important) : 4797, Test : 6934
	MWEs in tain : 591, occurrences : 5300
	Impotant words in tain : 528
	MWE length mean : 2.13
	Seen MWEs : 556 (94 %)
	New MWEs : 33 (5 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 11692
	After : 9702
Deep model(Non compositional)
# Parameters = 4783088
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 480)       4656960     input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 42)        4494        input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 1920)         0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 168)          0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 2088)         0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 58)           121162      concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 58)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            472         dropout_17[0][0]                 
==================================================================================================
Total params: 4,783,088
Trainable params: 4,783,088
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 350536
	data size after focused sampling = 414337
	data size before sampling = 414337
	data size after sampling = 1173168
	6 Labels in train : Counter({0: 195528, 1: 195528, 2: 195528, 4: 195528, 5: 195528, 6: 195528})
	6 Labels in valid : Counter({6: 19749, 2: 19702, 0: 19513, 4: 19507, 5: 19501, 1: 19345})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1055851 samples, validate on 117317 samples
Epoch 1/40
 - 576s - loss: 0.0905 - acc: 0.9901 - val_loss: 0.0639 - val_acc: 0.9923
Epoch 2/40
 - 576s - loss: 0.0674 - acc: 0.9924 - val_loss: 0.0623 - val_acc: 0.9925
Epoch 3/40
 - 576s - loss: 0.0647 - acc: 0.9926 - val_loss: 0.0624 - val_acc: 0.9925

==================================================================================================
	MLP training time : 0:32:19.242608
==================================================================================================
	Identification : 0.794

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : RO
==================================================================================================
	Training  : 49769, Test : 6934
	MWEs in tain : 591, occurrences : 5300
	Impotant words in tain : 528
	MWE length mean : 2.13
	Seen MWEs : 556 (94 %)
	New MWEs : 33 (5 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 811501
==================================================================================================
	Linear training time : 0:12:51.679738
==================================================================================================

==================================================================================================
	Parsing time : 0:01:15.282371
==================================================================================================
	Identification : 0.861

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : SL
==================================================================================================
	Training (Important) : 2516, Test : 1994
	MWEs in tain : 1098, occurrences : 2853
	Impotant words in tain : 933
	MWE length mean : 2.23
	Seen MWEs : 360 (72 %)
	New MWEs : 140 (28 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 11516
	After : 8701
Deep model(Non compositional)
# Parameters = 4298534
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 480)       4176480     input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 42)        420         input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 1920)         0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 168)          0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 2088)         0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 58)           121162      concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 58)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            472         dropout_18[0][0]                 
==================================================================================================
Total params: 4,298,534
Trainable params: 4,298,534
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 133529
	data size after focused sampling = 274167
	data size before sampling = 274167
	data size after sampling = 823550
	7 Labels in train : Counter({0: 117650, 1: 117650, 2: 117650, 4: 117650, 5: 117650, 6: 117650, 7: 117650})
	7 Labels in valid : Counter({4: 11881, 7: 11877, 1: 11757, 0: 11755, 2: 11755, 5: 11725, 6: 11605})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 741195 samples, validate on 82355 samples
Epoch 1/40
 - 371s - loss: 0.1272 - acc: 0.9814 - val_loss: 0.0650 - val_acc: 0.9895
Epoch 2/40
 - 371s - loss: 0.0652 - acc: 0.9897 - val_loss: 0.0595 - val_acc: 0.9906
Epoch 3/40
 - 371s - loss: 0.0597 - acc: 0.9905 - val_loss: 0.0593 - val_acc: 0.9908

==================================================================================================
	MLP training time : 0:21:03.658044
==================================================================================================
	Identification : 0.576

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : SL
==================================================================================================
	Training  : 11517, Test : 1994
	MWEs in tain : 1098, occurrences : 2853
	Impotant words in tain : 933
	MWE length mean : 2.23
	Seen MWEs : 360 (72 %)
	New MWEs : 140 (28 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 428480
==================================================================================================
	Linear training time : 0:03:10.767936
==================================================================================================

==================================================================================================
	Parsing time : 0:00:27.896513
==================================================================================================
	Identification : 0.624

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Training (Important) : 4843, Test : 589
	MWEs in tain : 2224, occurrences : 6601
	Impotant words in tain : 1428
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 152 (30 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12373
	After : 10370
Deep model(Non compositional)
# Parameters = 5104400
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 480)       4977600     input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 42)        5166        input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 1920)         0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 168)          0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 2088)         0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 58)           121162      concatenate_19[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 58)           0           dense_37[0][0]                   
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            472         dropout_19[0][0]                 
==================================================================================================
Total params: 5,104,400
Trainable params: 5,104,400
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 329139
	data size after focused sampling = 564064
	data size before sampling = 564064
	data size after sampling = 1214195
	5 Labels in train : Counter({0: 242839, 1: 242839, 2: 242839, 5: 242839, 6: 242839})
	5 Labels in valid : Counter({2: 24403, 1: 24355, 6: 24350, 0: 24226, 5: 24086})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1092775 samples, validate on 121420 samples
Epoch 1/40
 - 744s - loss: 0.1369 - acc: 0.9784 - val_loss: 0.0794 - val_acc: 0.9859
Epoch 2/40
 - 745s - loss: 0.0819 - acc: 0.9855 - val_loss: 0.0753 - val_acc: 0.9867
Epoch 3/40
 - 745s - loss: 0.0746 - acc: 0.9865 - val_loss: 0.0730 - val_acc: 0.9872

==================================================================================================
	MLP training time : 0:41:09.213984
==================================================================================================
	Identification : 0.59

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Training  : 18035, Test : 589
	MWEs in tain : 2224, occurrences : 6601
	Impotant words in tain : 1428
	MWE length mean : 2.06
	Seen MWEs : 354 (69 %)
	New MWEs : 152 (30 %)
==================================================================================================
	Linear classifier:
==================================================================================================
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)	Feature number = 526289
==================================================================================================
	Linear training time : 0:05:23.807578
==================================================================================================

==================================================================================================
	Parsing time : 0:00:12.243365
==================================================================================================
	Identification : 0.549

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
