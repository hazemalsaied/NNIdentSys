	Language : BG
==================================================================================================
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1177, 240)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12882.490958
validation loss after epoch 0 : 1041.735847
	Epoch 1....
validAcc: 0.801
Epoch has taken 0:03:06.154746
Number of used sentences in train = 2811
Total loss for epoch 1: 9251.375604
validation loss after epoch 1 : 837.157851
validAcc: 0.834
	Epoch 2....
Epoch has taken 0:03:05.003478
Number of used sentences in train = 2811
Total loss for epoch 2: 8141.162218
validation loss after epoch 2 : 674.193796
	Epoch 3....
validAcc: 0.803
Epoch has taken 0:03:06.615663
Number of used sentences in train = 2811
Total loss for epoch 3: 7366.703734
validation loss after epoch 3 : 719.048978
	Epoch 4....
validAcc: 0.222
Epoch has taken 0:03:05.984014
Number of used sentences in train = 2811
Total loss for epoch 4: 6935.409153
validation loss after epoch 4 : 481.252659
	Epoch 5....
validAcc: 0.819
Epoch has taken 0:03:05.589887
Number of used sentences in train = 2811
Total loss for epoch 5: 6527.381371
validation loss after epoch 5 : 669.109455
	Epoch 6....
validAcc: 0.785
Epoch has taken 0:03:07.465040
Number of used sentences in train = 2811
Total loss for epoch 6: 6182.721209
validation loss after epoch 6 : 578.091145
	Epoch 7....
validAcc: 0.768
Epoch has taken 0:03:03.821308
Number of used sentences in train = 2811
Total loss for epoch 7: 5956.294220
validation loss after epoch 7 : 571.025879
	Epoch 8....
validAcc: 0.063
Epoch has taken 0:02:59.230565
Number of used sentences in train = 2811
Total loss for epoch 8: 5722.950670
validation loss after epoch 8 : 366.751963
	Epoch 9....
validAcc: 0.827
Epoch has taken 0:03:01.847201
Number of used sentences in train = 2811
Total loss for epoch 9: 5560.264552
validation loss after epoch 9 : 575.028183
	Epoch 10....
validAcc: 0.084
Epoch has taken 0:02:51.132718
Number of used sentences in train = 2811
Total loss for epoch 10: 5446.821429
validation loss after epoch 10 : 335.513223
	Epoch 11....
validAcc: 0.074
Epoch has taken 0:02:54.108040
Number of used sentences in train = 2811
Total loss for epoch 11: 5359.127973
validation loss after epoch 11 : 330.171337
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1177, 240)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
validAcc: 0.084
Epoch has taken 0:02:43.681388
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/Reports/0.kiperwasser.p
Number of used sentences in train = 313
Total loss for epoch 0: 877.906912
	Epoch 1....
Epoch has taken 0:00:16.178301
Number of used sentences in train = 313
Total loss for epoch 1: 375.455715
	Epoch 2....
Epoch has taken 0:00:16.167397
Number of used sentences in train = 313
Total loss for epoch 2: 221.220437
	Epoch 3....
Epoch has taken 0:00:16.152199
Number of used sentences in train = 313
Total loss for epoch 3: 160.504271
	Epoch 4....
Epoch has taken 0:00:16.418526
Number of used sentences in train = 313
Total loss for epoch 4: 110.612354
	Epoch 5....
Epoch has taken 0:00:16.154231
Number of used sentences in train = 313
Total loss for epoch 5: 98.817425
	Epoch 6....
Epoch has taken 0:00:16.144333
Number of used sentences in train = 313
Total loss for epoch 6: 98.947148
	Epoch 7....
Epoch has taken 0:00:16.155282
Number of used sentences in train = 313
Total loss for epoch 7: 89.769132
	Epoch 8....
Epoch has taken 0:00:16.147585
Number of used sentences in train = 313
Total loss for epoch 8: 86.147599
	Epoch 9....
Epoch has taken 0:00:16.153700
Number of used sentences in train = 313
Total loss for epoch 9: 84.546251
	Epoch 10....
Epoch has taken 0:00:16.162060
Number of used sentences in train = 313
Total loss for epoch 10: 81.409617
	Epoch 11....
Epoch has taken 0:00:16.159742
Number of used sentences in train = 313
Total loss for epoch 11: 78.826543
Epoch has taken 0:00:16.153027

==================================================================================================
	Training time : 0:39:25.278619
==================================================================================================
	Identification : 0.009
