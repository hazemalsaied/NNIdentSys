INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+C
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11354.017598
validation loss after epoch 0 : 866.454756
	Epoch 1....
Epoch has taken 0:02:46.164693
Number of used sentences in train = 2811
Total loss for epoch 1: 7612.758761
validation loss after epoch 1 : 843.044730
	Epoch 2....
Epoch has taken 0:03:02.884552
Number of used sentences in train = 2811
Total loss for epoch 2: 6858.608082
validation loss after epoch 2 : 803.027593
	Epoch 3....
Epoch has taken 0:02:48.575400
Number of used sentences in train = 2811
Total loss for epoch 3: 6385.488079
validation loss after epoch 3 : 793.367005
	Epoch 4....
Epoch has taken 0:03:04.496531
Number of used sentences in train = 2811
Total loss for epoch 4: 6085.406482
validation loss after epoch 4 : 774.751086
	Epoch 5....
Epoch has taken 0:03:01.161279
Number of used sentences in train = 2811
Total loss for epoch 5: 5739.886095
validation loss after epoch 5 : 838.206632
	Epoch 6....
Epoch has taken 0:02:50.320886
Number of used sentences in train = 2811
Total loss for epoch 6: 5491.828758
validation loss after epoch 6 : 842.585631
	Epoch 7....
Epoch has taken 0:03:01.851101
Number of used sentences in train = 2811
Total loss for epoch 7: 5348.627263
validation loss after epoch 7 : 819.915338
	Epoch 8....
Epoch has taken 0:03:05.135661
Number of used sentences in train = 2811
Total loss for epoch 8: 5185.333820
validation loss after epoch 8 : 845.496372
	Epoch 9....
Epoch has taken 0:02:46.500868
Number of used sentences in train = 2811
Total loss for epoch 9: 5106.997238
validation loss after epoch 9 : 860.032624
	Epoch 10....
Epoch has taken 0:03:13.927858
Number of used sentences in train = 2811
Total loss for epoch 10: 5015.988879
validation loss after epoch 10 : 868.138370
	Epoch 11....
Epoch has taken 0:02:46.072913
Number of used sentences in train = 2811
Total loss for epoch 11: 4903.079279
validation loss after epoch 11 : 933.219282
	Epoch 12....
Epoch has taken 0:02:54.362547
Number of used sentences in train = 2811
Total loss for epoch 12: 4866.431682
validation loss after epoch 12 : 937.839239
	Epoch 13....
Epoch has taken 0:02:50.480807
Number of used sentences in train = 2811
Total loss for epoch 13: 4836.422326
validation loss after epoch 13 : 963.357719
	Epoch 14....
Epoch has taken 0:02:51.230202
Number of used sentences in train = 2811
Total loss for epoch 14: 4807.044559
validation loss after epoch 14 : 982.480989
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:48.837548
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1269.924790
	Epoch 1....
Epoch has taken 0:00:19.479539
Number of used sentences in train = 313
Total loss for epoch 1: 793.844377
	Epoch 2....
Epoch has taken 0:00:19.460379
Number of used sentences in train = 313
Total loss for epoch 2: 716.926477
	Epoch 3....
Epoch has taken 0:00:19.437225
Number of used sentences in train = 313
Total loss for epoch 3: 621.397684
	Epoch 4....
Epoch has taken 0:00:19.399994
Number of used sentences in train = 313
Total loss for epoch 4: 597.966245
	Epoch 5....
Epoch has taken 0:00:19.423896
Number of used sentences in train = 313
Total loss for epoch 5: 571.770360
	Epoch 6....
Epoch has taken 0:00:19.380830
Number of used sentences in train = 313
Total loss for epoch 6: 558.664540
	Epoch 7....
Epoch has taken 0:00:19.412295
Number of used sentences in train = 313
Total loss for epoch 7: 553.820126
	Epoch 8....
Epoch has taken 0:00:19.342143
Number of used sentences in train = 313
Total loss for epoch 8: 544.278940
	Epoch 9....
Epoch has taken 0:00:19.385160
Number of used sentences in train = 313
Total loss for epoch 9: 539.025276
	Epoch 10....
Epoch has taken 0:00:19.390474
Number of used sentences in train = 313
Total loss for epoch 10: 536.994028
	Epoch 11....
Epoch has taken 0:00:19.351596
Number of used sentences in train = 313
Total loss for epoch 11: 537.272140
	Epoch 12....
Epoch has taken 0:00:19.353168
Number of used sentences in train = 313
Total loss for epoch 12: 536.162639
	Epoch 13....
Epoch has taken 0:00:19.163072
Number of used sentences in train = 313
Total loss for epoch 13: 535.663985
	Epoch 14....
Epoch has taken 0:00:19.343128
Number of used sentences in train = 313
Total loss for epoch 14: 535.530124
Epoch has taken 0:00:19.363005

==================================================================================================
	Training time : 0:48:49.187384
==================================================================================================
	Identification : 0.467

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9088.608989
validation loss after epoch 0 : 654.254293
	Epoch 1....
Epoch has taken 0:02:06.744196
Number of used sentences in train = 2074
Total loss for epoch 1: 5451.230651
validation loss after epoch 1 : 649.104341
	Epoch 2....
Epoch has taken 0:02:05.493576
Number of used sentences in train = 2074
Total loss for epoch 2: 4799.612832
validation loss after epoch 2 : 650.005339
	Epoch 3....
Epoch has taken 0:02:07.505243
Number of used sentences in train = 2074
Total loss for epoch 3: 4382.826671
validation loss after epoch 3 : 655.479226
	Epoch 4....
Epoch has taken 0:02:06.397404
Number of used sentences in train = 2074
Total loss for epoch 4: 4005.620282
validation loss after epoch 4 : 688.440311
	Epoch 5....
Epoch has taken 0:02:04.698610
Number of used sentences in train = 2074
Total loss for epoch 5: 3761.346738
validation loss after epoch 5 : 675.778915
	Epoch 6....
Epoch has taken 0:02:07.157353
Number of used sentences in train = 2074
Total loss for epoch 6: 3553.358722
validation loss after epoch 6 : 748.266921
	Epoch 7....
Epoch has taken 0:02:07.669700
Number of used sentences in train = 2074
Total loss for epoch 7: 3448.443132
validation loss after epoch 7 : 767.692205
	Epoch 8....
Epoch has taken 0:02:08.105906
Number of used sentences in train = 2074
Total loss for epoch 8: 3391.126084
validation loss after epoch 8 : 849.193012
	Epoch 9....
Epoch has taken 0:02:06.769170
Number of used sentences in train = 2074
Total loss for epoch 9: 3345.618516
validation loss after epoch 9 : 779.055673
	Epoch 10....
Epoch has taken 0:02:06.076720
Number of used sentences in train = 2074
Total loss for epoch 10: 3307.953354
validation loss after epoch 10 : 811.269098
	Epoch 11....
Epoch has taken 0:02:06.214763
Number of used sentences in train = 2074
Total loss for epoch 11: 3277.707860
validation loss after epoch 11 : 793.321617
	Epoch 12....
Epoch has taken 0:02:08.629685
Number of used sentences in train = 2074
Total loss for epoch 12: 3273.514832
validation loss after epoch 12 : 820.319923
	Epoch 13....
Epoch has taken 0:02:08.457116
Number of used sentences in train = 2074
Total loss for epoch 13: 3228.031591
validation loss after epoch 13 : 864.134277
	Epoch 14....
Epoch has taken 0:02:07.958348
Number of used sentences in train = 2074
Total loss for epoch 14: 3220.951910
validation loss after epoch 14 : 977.097102
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:07.277900
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1405.465862
	Epoch 1....
Epoch has taken 0:00:13.030091
Number of used sentences in train = 231
Total loss for epoch 1: 601.413521
	Epoch 2....
Epoch has taken 0:00:13.016246
Number of used sentences in train = 231
Total loss for epoch 2: 437.031852
	Epoch 3....
Epoch has taken 0:00:13.021111
Number of used sentences in train = 231
Total loss for epoch 3: 407.062737
	Epoch 4....
Epoch has taken 0:00:12.959020
Number of used sentences in train = 231
Total loss for epoch 4: 382.370567
	Epoch 5....
Epoch has taken 0:00:12.959173
Number of used sentences in train = 231
Total loss for epoch 5: 366.484039
	Epoch 6....
Epoch has taken 0:00:12.948161
Number of used sentences in train = 231
Total loss for epoch 6: 356.915113
	Epoch 7....
Epoch has taken 0:00:12.985198
Number of used sentences in train = 231
Total loss for epoch 7: 355.793580
	Epoch 8....
Epoch has taken 0:00:12.955504
Number of used sentences in train = 231
Total loss for epoch 8: 351.996756
	Epoch 9....
Epoch has taken 0:00:12.969167
Number of used sentences in train = 231
Total loss for epoch 9: 352.513711
	Epoch 10....
Epoch has taken 0:00:12.594924
Number of used sentences in train = 231
Total loss for epoch 10: 349.887915
	Epoch 11....
Epoch has taken 0:00:12.721177
Number of used sentences in train = 231
Total loss for epoch 11: 350.468814
	Epoch 12....
Epoch has taken 0:00:12.743550
Number of used sentences in train = 231
Total loss for epoch 12: 348.969832
	Epoch 13....
Epoch has taken 0:00:12.655405
Number of used sentences in train = 231
Total loss for epoch 13: 348.274477
	Epoch 14....
Epoch has taken 0:00:12.990178
Number of used sentences in train = 231
Total loss for epoch 14: 348.907451
Epoch has taken 0:00:13.017915

==================================================================================================
	Training time : 0:34:59.061219
==================================================================================================
	Identification : 0.318

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 26022.889513
validation loss after epoch 0 : 1200.788921
	Epoch 1....
Epoch has taken 0:04:01.495852
Number of used sentences in train = 3226
Total loss for epoch 1: 10242.427515
validation loss after epoch 1 : 1095.412873
	Epoch 2....
Epoch has taken 0:04:01.560537
Number of used sentences in train = 3226
Total loss for epoch 2: 9129.637850
validation loss after epoch 2 : 1078.736054
	Epoch 3....
Epoch has taken 0:04:01.082569
Number of used sentences in train = 3226
Total loss for epoch 3: 8541.357185
validation loss after epoch 3 : 1094.847199
	Epoch 4....
Epoch has taken 0:04:01.573529
Number of used sentences in train = 3226
Total loss for epoch 4: 8120.349236
validation loss after epoch 4 : 1108.893417
	Epoch 5....
Epoch has taken 0:04:02.965060
Number of used sentences in train = 3226
Total loss for epoch 5: 7791.531427
validation loss after epoch 5 : 1114.918948
	Epoch 6....
Epoch has taken 0:04:04.093629
Number of used sentences in train = 3226
Total loss for epoch 6: 7506.829431
validation loss after epoch 6 : 1186.920447
	Epoch 7....
Epoch has taken 0:04:05.477046
Number of used sentences in train = 3226
Total loss for epoch 7: 7278.070315
validation loss after epoch 7 : 1286.173839
	Epoch 8....
Epoch has taken 0:04:00.227458
Number of used sentences in train = 3226
Total loss for epoch 8: 7126.362793
validation loss after epoch 8 : 1192.159668
	Epoch 9....
Epoch has taken 0:04:04.794684
Number of used sentences in train = 3226
Total loss for epoch 9: 6967.074001
validation loss after epoch 9 : 1284.107947
	Epoch 10....
Epoch has taken 0:04:16.688987
Number of used sentences in train = 3226
Total loss for epoch 10: 6898.397374
validation loss after epoch 10 : 1313.128130
	Epoch 11....
Epoch has taken 0:04:09.825260
Number of used sentences in train = 3226
Total loss for epoch 11: 6796.812208
validation loss after epoch 11 : 1301.751980
	Epoch 12....
Epoch has taken 0:04:02.763549
Number of used sentences in train = 3226
Total loss for epoch 12: 6689.995410
validation loss after epoch 12 : 1379.871936
	Epoch 13....
Epoch has taken 0:04:02.304913
Number of used sentences in train = 3226
Total loss for epoch 13: 6632.245554
validation loss after epoch 13 : 1442.085934
	Epoch 14....
Epoch has taken 0:04:04.221261
Number of used sentences in train = 3226
Total loss for epoch 14: 6560.946379
validation loss after epoch 14 : 1464.611978
Epoch has taken 0:04:04.554056
# Network optimizer = Adagrad, learning rate = 0.07
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Number of used sentences in train = 359
Total loss for epoch 0: 1549.515919
	Epoch 1....
Epoch has taken 0:00:24.264702
Number of used sentences in train = 359
Total loss for epoch 1: 983.021814
	Epoch 2....
Epoch has taken 0:00:23.948781
Number of used sentences in train = 359
Total loss for epoch 2: 880.812747
	Epoch 3....
Epoch has taken 0:00:24.042442
Number of used sentences in train = 359
Total loss for epoch 3: 814.890700
	Epoch 4....
Epoch has taken 0:00:23.723996
Number of used sentences in train = 359
Total loss for epoch 4: 772.573687
	Epoch 5....
Epoch has taken 0:00:23.972322
Number of used sentences in train = 359
Total loss for epoch 5: 739.772774
	Epoch 6....
Epoch has taken 0:00:23.876324
Number of used sentences in train = 359
Total loss for epoch 6: 716.198056
	Epoch 7....
Epoch has taken 0:00:23.686488
Number of used sentences in train = 359
Total loss for epoch 7: 700.252350
	Epoch 8....
Epoch has taken 0:00:23.900283
Number of used sentences in train = 359
Total loss for epoch 8: 705.989960
	Epoch 9....
Epoch has taken 0:00:23.702770
Number of used sentences in train = 359
Total loss for epoch 9: 684.114018
	Epoch 10....
Epoch has taken 0:00:23.880868
Number of used sentences in train = 359
Total loss for epoch 10: 684.423565
	Epoch 11....
Epoch has taken 0:00:23.751729
Number of used sentences in train = 359
Total loss for epoch 11: 678.957939
	Epoch 12....
Epoch has taken 0:00:23.535993
Number of used sentences in train = 359
Total loss for epoch 12: 683.734787
	Epoch 13....
Epoch has taken 0:00:23.797957
Number of used sentences in train = 359
Total loss for epoch 13: 674.456145
	Epoch 14....
Epoch has taken 0:00:23.653007
Number of used sentences in train = 359
Total loss for epoch 14: 674.284725
Epoch has taken 0:00:23.617503

==================================================================================================
	Training time : 1:07:01.670593
==================================================================================================
	Identification : 0.023

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 29, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 24, 'lstmDropout': 0.12, 'denseActivation': 'tanh', 'wordDim': 120, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1177, 120)
  (lstm): LSTM(187, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11068.890495
validation loss after epoch 0 : 918.043534
	Epoch 1....
Epoch has taken 0:03:07.199388
Number of used sentences in train = 2811
Total loss for epoch 1: 8041.771067
validation loss after epoch 1 : 903.644853
	Epoch 2....
Epoch has taken 0:03:03.920889
Number of used sentences in train = 2811
Total loss for epoch 2: 7256.957756
validation loss after epoch 2 : 884.866770
	Epoch 3....
Epoch has taken 0:03:02.748223
Number of used sentences in train = 2811
Total loss for epoch 3: 6765.829590
validation loss after epoch 3 : 897.708213
	Epoch 4....
Epoch has taken 0:02:48.160765
Number of used sentences in train = 2811
Total loss for epoch 4: 6407.141098
validation loss after epoch 4 : 892.454316
	Epoch 5....
Epoch has taken 0:02:57.092130
Number of used sentences in train = 2811
Total loss for epoch 5: 6128.385435
validation loss after epoch 5 : 937.792248
	Epoch 6....
Epoch has taken 0:02:48.108340
Number of used sentences in train = 2811
Total loss for epoch 6: 5945.596066
validation loss after epoch 6 : 937.015837
	Epoch 7....
Epoch has taken 0:02:48.646241
Number of used sentences in train = 2811
Total loss for epoch 7: 5738.326769
validation loss after epoch 7 : 957.828792
	Epoch 8....
Epoch has taken 0:03:15.091026
Number of used sentences in train = 2811
Total loss for epoch 8: 5548.534636
validation loss after epoch 8 : 968.226069
	Epoch 9....
Epoch has taken 0:03:01.936532
Number of used sentences in train = 2811
Total loss for epoch 9: 5470.345182
validation loss after epoch 9 : 1001.851366
	Epoch 10....
Epoch has taken 0:03:00.359674
Number of used sentences in train = 2811
Total loss for epoch 10: 5345.441945
validation loss after epoch 10 : 1011.363953
	Epoch 11....
Epoch has taken 0:02:51.511810
Number of used sentences in train = 2811
Total loss for epoch 11: 5271.311683
validation loss after epoch 11 : 1016.603211
	Epoch 12....
Epoch has taken 0:02:52.768399
Number of used sentences in train = 2811
Total loss for epoch 12: 5162.700209
validation loss after epoch 12 : 1040.990014
	Epoch 13....
Epoch has taken 0:03:28.071411
Number of used sentences in train = 2811
Total loss for epoch 13: 5034.891538
validation loss after epoch 13 : 1009.819156
	Epoch 14....
Epoch has taken 0:02:53.935827
Number of used sentences in train = 2811
Total loss for epoch 14: 5007.426412
validation loss after epoch 14 : 1054.422737
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1177, 120)
  (lstm): LSTM(187, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.697182
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1303.653369
	Epoch 1....
Epoch has taken 0:00:18.470521
Number of used sentences in train = 313
Total loss for epoch 1: 769.507211
	Epoch 2....
Epoch has taken 0:00:18.225717
Number of used sentences in train = 313
Total loss for epoch 2: 663.097683
	Epoch 3....
Epoch has taken 0:00:18.211959
Number of used sentences in train = 313
Total loss for epoch 3: 586.550148
	Epoch 4....
Epoch has taken 0:00:18.228862
Number of used sentences in train = 313
Total loss for epoch 4: 556.968783
	Epoch 5....
Epoch has taken 0:00:18.215061
Number of used sentences in train = 313
Total loss for epoch 5: 545.444618
	Epoch 6....
Epoch has taken 0:00:18.221372
Number of used sentences in train = 313
Total loss for epoch 6: 545.171615
	Epoch 7....
Epoch has taken 0:00:18.240783
Number of used sentences in train = 313
Total loss for epoch 7: 522.281536
	Epoch 8....
Epoch has taken 0:00:18.222422
Number of used sentences in train = 313
Total loss for epoch 8: 515.769032
	Epoch 9....
Epoch has taken 0:00:18.224430
Number of used sentences in train = 313
Total loss for epoch 9: 516.642670
	Epoch 10....
Epoch has taken 0:00:18.191923
Number of used sentences in train = 313
Total loss for epoch 10: 516.680126
	Epoch 11....
Epoch has taken 0:00:18.142196
Number of used sentences in train = 313
Total loss for epoch 11: 507.507524
	Epoch 12....
Epoch has taken 0:00:18.143951
Number of used sentences in train = 313
Total loss for epoch 12: 507.958890
	Epoch 13....
Epoch has taken 0:00:18.196767
Number of used sentences in train = 313
Total loss for epoch 13: 506.485446
	Epoch 14....
Epoch has taken 0:00:18.192914
Number of used sentences in train = 313
Total loss for epoch 14: 513.487103
Epoch has taken 0:00:18.192524

==================================================================================================
	Training time : 0:49:31.068006
==================================================================================================
	Identification : 0.095

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1133, 120)
  (lstm): LSTM(187, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8158.166482
validation loss after epoch 0 : 707.241552
	Epoch 1....
Epoch has taken 0:01:58.468789
Number of used sentences in train = 2074
Total loss for epoch 1: 5495.998546
validation loss after epoch 1 : 668.561529
	Epoch 2....
Epoch has taken 0:01:58.639246
Number of used sentences in train = 2074
Total loss for epoch 2: 4804.828230
validation loss after epoch 2 : 677.502673
	Epoch 3....
Epoch has taken 0:01:58.126094
Number of used sentences in train = 2074
Total loss for epoch 3: 4374.960707
validation loss after epoch 3 : 682.574656
	Epoch 4....
Epoch has taken 0:01:57.941096
Number of used sentences in train = 2074
Total loss for epoch 4: 4055.393020
validation loss after epoch 4 : 726.976663
	Epoch 5....
Epoch has taken 0:01:59.718821
Number of used sentences in train = 2074
Total loss for epoch 5: 3851.770911
validation loss after epoch 5 : 720.972656
	Epoch 6....
Epoch has taken 0:01:58.313093
Number of used sentences in train = 2074
Total loss for epoch 6: 3686.055056
validation loss after epoch 6 : 780.993814
	Epoch 7....
Epoch has taken 0:01:58.315101
Number of used sentences in train = 2074
Total loss for epoch 7: 3568.420063
validation loss after epoch 7 : 811.781996
	Epoch 8....
Epoch has taken 0:02:02.012660
Number of used sentences in train = 2074
Total loss for epoch 8: 3470.659873
validation loss after epoch 8 : 788.646301
	Epoch 9....
Epoch has taken 0:01:58.104624
Number of used sentences in train = 2074
Total loss for epoch 9: 3426.940606
validation loss after epoch 9 : 817.270190
	Epoch 10....
Epoch has taken 0:01:59.134325
Number of used sentences in train = 2074
Total loss for epoch 10: 3347.398347
validation loss after epoch 10 : 885.326696
	Epoch 11....
Epoch has taken 0:01:59.381509
Number of used sentences in train = 2074
Total loss for epoch 11: 3324.698319
validation loss after epoch 11 : 837.352325
	Epoch 12....
Epoch has taken 0:01:59.020004
Number of used sentences in train = 2074
Total loss for epoch 12: 3321.978380
validation loss after epoch 12 : 892.712560
	Epoch 13....
Epoch has taken 0:01:58.434814
Number of used sentences in train = 2074
Total loss for epoch 13: 3257.241220
validation loss after epoch 13 : 901.635294
	Epoch 14....
Epoch has taken 0:01:57.610619
Number of used sentences in train = 2074
Total loss for epoch 14: 3255.004150
validation loss after epoch 14 : 860.153501
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1133, 120)
  (lstm): LSTM(187, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.967372
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1181.839913
	Epoch 1....
Epoch has taken 0:00:12.124934
Number of used sentences in train = 231
Total loss for epoch 1: 558.562931
	Epoch 2....
Epoch has taken 0:00:12.136607
Number of used sentences in train = 231
Total loss for epoch 2: 441.484432
	Epoch 3....
Epoch has taken 0:00:12.095253
Number of used sentences in train = 231
Total loss for epoch 3: 416.183164
	Epoch 4....
Epoch has taken 0:00:12.130776
Number of used sentences in train = 231
Total loss for epoch 4: 380.529372
	Epoch 5....
Epoch has taken 0:00:12.138328
Number of used sentences in train = 231
Total loss for epoch 5: 363.535915
	Epoch 6....
Epoch has taken 0:00:12.121766
Number of used sentences in train = 231
Total loss for epoch 6: 361.238801
	Epoch 7....
Epoch has taken 0:00:12.117809
Number of used sentences in train = 231
Total loss for epoch 7: 356.819085
	Epoch 8....
Epoch has taken 0:00:12.139906
Number of used sentences in train = 231
Total loss for epoch 8: 352.154927
	Epoch 9....
Epoch has taken 0:00:12.128923
Number of used sentences in train = 231
Total loss for epoch 9: 350.917285
	Epoch 10....
Epoch has taken 0:00:12.133893
Number of used sentences in train = 231
Total loss for epoch 10: 347.970851
	Epoch 11....
Epoch has taken 0:00:12.125326
Number of used sentences in train = 231
Total loss for epoch 11: 348.389455
	Epoch 12....
Epoch has taken 0:00:12.118798
Number of used sentences in train = 231
Total loss for epoch 12: 349.356866
	Epoch 13....
Epoch has taken 0:00:12.123072
Number of used sentences in train = 231
Total loss for epoch 13: 348.767164
	Epoch 14....
Epoch has taken 0:00:12.077183
Number of used sentences in train = 231
Total loss for epoch 14: 346.855558
Epoch has taken 0:00:12.121328

==================================================================================================
	Training time : 0:32:43.362808
==================================================================================================
	Identification : 0.255

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(1202, 120)
  (lstm): LSTM(187, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14709.623509
validation loss after epoch 0 : 1333.332356
	Epoch 1....
Epoch has taken 0:03:51.504330
Number of used sentences in train = 3226
Total loss for epoch 1: 11387.861323
validation loss after epoch 1 : 1248.736419
	Epoch 2....
Epoch has taken 0:03:51.431414
Number of used sentences in train = 3226
Total loss for epoch 2: 10540.918715
validation loss after epoch 2 : 1249.854983
	Epoch 3....
Epoch has taken 0:03:50.582975
Number of used sentences in train = 3226
Total loss for epoch 3: 10040.389674
validation loss after epoch 3 : 1288.322944
	Epoch 4....
Epoch has taken 0:03:55.886742
Number of used sentences in train = 3226
Total loss for epoch 4: 9668.363123
validation loss after epoch 4 : 1270.163007
	Epoch 5....
Epoch has taken 0:04:03.451085
Number of used sentences in train = 3226
Total loss for epoch 5: 9357.203828
validation loss after epoch 5 : 1276.507828
	Epoch 6....
Epoch has taken 0:03:59.593339
Number of used sentences in train = 3226
Total loss for epoch 6: 9063.561616
validation loss after epoch 6 : 1283.056094
	Epoch 7....
Epoch has taken 0:03:58.845566
Number of used sentences in train = 3226
Total loss for epoch 7: 8820.116188
validation loss after epoch 7 : 1289.774907
	Epoch 8....
Epoch has taken 0:03:53.519252
Number of used sentences in train = 3226
Total loss for epoch 8: 8582.669583
validation loss after epoch 8 : 1304.933917
	Epoch 9....
Epoch has taken 0:03:52.672918
Number of used sentences in train = 3226
Total loss for epoch 9: 8375.849400
validation loss after epoch 9 : 1382.898416
	Epoch 10....
Epoch has taken 0:04:00.367981
Number of used sentences in train = 3226
Total loss for epoch 10: 8219.888622
validation loss after epoch 10 : 1359.921414
	Epoch 11....
Epoch has taken 0:03:52.155203
Number of used sentences in train = 3226
Total loss for epoch 11: 8070.173488
validation loss after epoch 11 : 1381.515165
	Epoch 12....
Epoch has taken 0:03:54.304917
Number of used sentences in train = 3226
Total loss for epoch 12: 7943.316686
validation loss after epoch 12 : 1385.254605
	Epoch 13....
Epoch has taken 0:04:01.855341
Number of used sentences in train = 3226
Total loss for epoch 13: 7780.497836
validation loss after epoch 13 : 1428.546505
	Epoch 14....
Epoch has taken 0:03:58.093305
Number of used sentences in train = 3226
Total loss for epoch 14: 7752.812137
validation loss after epoch 14 : 1443.074642
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(1202, 120)
  (lstm): LSTM(187, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:51.945654
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1846.228974
	Epoch 1....
Epoch has taken 0:00:22.579075
Number of used sentences in train = 359
Total loss for epoch 1: 1227.825659
	Epoch 2....
Epoch has taken 0:00:22.548902
Number of used sentences in train = 359
Total loss for epoch 2: 1090.264861
	Epoch 3....
Epoch has taken 0:00:22.644286
Number of used sentences in train = 359
Total loss for epoch 3: 969.482254
	Epoch 4....
Epoch has taken 0:00:22.673568
Number of used sentences in train = 359
Total loss for epoch 4: 929.762857
	Epoch 5....
Epoch has taken 0:00:22.614774
Number of used sentences in train = 359
Total loss for epoch 5: 870.180491
	Epoch 6....
Epoch has taken 0:00:22.655404
Number of used sentences in train = 359
Total loss for epoch 6: 828.265787
	Epoch 7....
Epoch has taken 0:00:22.651423
Number of used sentences in train = 359
Total loss for epoch 7: 781.142941
	Epoch 8....
Epoch has taken 0:00:22.659539
Number of used sentences in train = 359
Total loss for epoch 8: 785.601759
	Epoch 9....
Epoch has taken 0:00:22.676201
Number of used sentences in train = 359
Total loss for epoch 9: 764.678866
	Epoch 10....
Epoch has taken 0:00:22.627949
Number of used sentences in train = 359
Total loss for epoch 10: 744.197978
	Epoch 11....
Epoch has taken 0:00:22.611637
Number of used sentences in train = 359
Total loss for epoch 11: 727.558531
	Epoch 12....
Epoch has taken 0:00:22.623541
Number of used sentences in train = 359
Total loss for epoch 12: 717.789802
	Epoch 13....
Epoch has taken 0:00:22.637363
Number of used sentences in train = 359
Total loss for epoch 13: 729.318340
	Epoch 14....
Epoch has taken 0:00:22.616358
Number of used sentences in train = 359
Total loss for epoch 14: 718.154734
Epoch has taken 0:00:22.628069

==================================================================================================
	Training time : 1:04:36.326013
==================================================================================================
	Identification : 0.121

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 24, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 16, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 43, 'lstmDropout': 0.24, 'denseActivation': 'tanh', 'wordDim': 241, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1177, 241)
  (lstm): LSTM(257, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11267.614917
validation loss after epoch 0 : 977.659578
	Epoch 1....
Epoch has taken 0:02:40.538204
Number of used sentences in train = 2811
Total loss for epoch 1: 7978.731677
validation loss after epoch 1 : 943.175789
	Epoch 2....
Epoch has taken 0:02:39.829857
Number of used sentences in train = 2811
Total loss for epoch 2: 7028.403415
validation loss after epoch 2 : 998.644868
	Epoch 3....
Epoch has taken 0:02:39.166902
Number of used sentences in train = 2811
Total loss for epoch 3: 6424.505357
validation loss after epoch 3 : 1012.447669
	Epoch 4....
Epoch has taken 0:02:39.377217
Number of used sentences in train = 2811
Total loss for epoch 4: 5997.533799
validation loss after epoch 4 : 1056.169330
	Epoch 5....
Epoch has taken 0:02:40.781655
Number of used sentences in train = 2811
Total loss for epoch 5: 5637.740433
validation loss after epoch 5 : 1098.941993
	Epoch 6....
Epoch has taken 0:02:40.237656
Number of used sentences in train = 2811
Total loss for epoch 6: 5395.510586
validation loss after epoch 6 : 1147.474224
	Epoch 7....
Epoch has taken 0:02:42.789636
Number of used sentences in train = 2811
Total loss for epoch 7: 5227.039300
validation loss after epoch 7 : 1148.137256
	Epoch 8....
Epoch has taken 0:02:47.469170
Number of used sentences in train = 2811
Total loss for epoch 8: 5095.711346
validation loss after epoch 8 : 1230.586953
	Epoch 9....
Epoch has taken 0:02:47.735608
Number of used sentences in train = 2811
Total loss for epoch 9: 4971.764298
validation loss after epoch 9 : 1251.076155
	Epoch 10....
Epoch has taken 0:02:48.087891
Number of used sentences in train = 2811
Total loss for epoch 10: 4887.216694
validation loss after epoch 10 : 1274.819671
	Epoch 11....
Epoch has taken 0:02:46.236621
Number of used sentences in train = 2811
Total loss for epoch 11: 4815.067964
validation loss after epoch 11 : 1302.642426
	Epoch 12....
Epoch has taken 0:02:44.266063
Number of used sentences in train = 2811
Total loss for epoch 12: 4747.994411
validation loss after epoch 12 : 1332.376299
	Epoch 13....
Epoch has taken 0:02:41.979870
Number of used sentences in train = 2811
Total loss for epoch 13: 4702.585305
validation loss after epoch 13 : 1368.207149
	Epoch 14....
Epoch has taken 0:02:39.155107
Number of used sentences in train = 2811
Total loss for epoch 14: 4666.919010
validation loss after epoch 14 : 1407.030607
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1177, 241)
  (lstm): LSTM(257, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:41.321026
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1337.873667
	Epoch 1....
Epoch has taken 0:00:16.877045
Number of used sentences in train = 313
Total loss for epoch 1: 778.106623
	Epoch 2....
Epoch has taken 0:00:16.883210
Number of used sentences in train = 313
Total loss for epoch 2: 655.919585
	Epoch 3....
Epoch has taken 0:00:16.876884
Number of used sentences in train = 313
Total loss for epoch 3: 610.829113
	Epoch 4....
Epoch has taken 0:00:16.854069
Number of used sentences in train = 313
Total loss for epoch 4: 571.769603
	Epoch 5....
Epoch has taken 0:00:16.844293
Number of used sentences in train = 313
Total loss for epoch 5: 560.520442
	Epoch 6....
Epoch has taken 0:00:16.822894
Number of used sentences in train = 313
Total loss for epoch 6: 543.906362
	Epoch 7....
Epoch has taken 0:00:16.868014
Number of used sentences in train = 313
Total loss for epoch 7: 538.891438
	Epoch 8....
Epoch has taken 0:00:16.866726
Number of used sentences in train = 313
Total loss for epoch 8: 534.649435
	Epoch 9....
Epoch has taken 0:00:16.829186
Number of used sentences in train = 313
Total loss for epoch 9: 531.433494
	Epoch 10....
Epoch has taken 0:00:16.795779
Number of used sentences in train = 313
Total loss for epoch 10: 528.656612
	Epoch 11....
Epoch has taken 0:00:16.813376
Number of used sentences in train = 313
Total loss for epoch 11: 525.672367
	Epoch 12....
Epoch has taken 0:00:16.797364
Number of used sentences in train = 313
Total loss for epoch 12: 524.026170
	Epoch 13....
Epoch has taken 0:00:16.815229
Number of used sentences in train = 313
Total loss for epoch 13: 522.881358
	Epoch 14....
Epoch has taken 0:00:16.806307
Number of used sentences in train = 313
Total loss for epoch 14: 521.786456
Epoch has taken 0:00:16.816687

==================================================================================================
	Training time : 0:44:52.050159
==================================================================================================
	Identification : 0.473

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1133, 241)
  (lstm): LSTM(257, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8394.571699
validation loss after epoch 0 : 782.778254
	Epoch 1....
Epoch has taken 0:01:51.102266
Number of used sentences in train = 2074
Total loss for epoch 1: 5385.628244
validation loss after epoch 1 : 753.547332
	Epoch 2....
Epoch has taken 0:01:49.963766
Number of used sentences in train = 2074
Total loss for epoch 2: 4477.528413
validation loss after epoch 2 : 752.865292
	Epoch 3....
Epoch has taken 0:01:49.073623
Number of used sentences in train = 2074
Total loss for epoch 3: 3968.113465
validation loss after epoch 3 : 808.092499
	Epoch 4....
Epoch has taken 0:01:49.115304
Number of used sentences in train = 2074
Total loss for epoch 4: 3721.468322
validation loss after epoch 4 : 819.407143
	Epoch 5....
Epoch has taken 0:01:50.395845
Number of used sentences in train = 2074
Total loss for epoch 5: 3525.326855
validation loss after epoch 5 : 889.530482
	Epoch 6....
Epoch has taken 0:01:50.182702
Number of used sentences in train = 2074
Total loss for epoch 6: 3411.157101
validation loss after epoch 6 : 902.324071
	Epoch 7....
Epoch has taken 0:01:50.061559
Number of used sentences in train = 2074
Total loss for epoch 7: 3363.585557
validation loss after epoch 7 : 924.162786
	Epoch 8....
Epoch has taken 0:01:48.821040
Number of used sentences in train = 2074
Total loss for epoch 8: 3308.280839
validation loss after epoch 8 : 946.632710
	Epoch 9....
Epoch has taken 0:01:50.003635
Number of used sentences in train = 2074
Total loss for epoch 9: 3271.875635
validation loss after epoch 9 : 966.842394
	Epoch 10....
Epoch has taken 0:01:50.082904
Number of used sentences in train = 2074
Total loss for epoch 10: 3252.209450
validation loss after epoch 10 : 986.006240
	Epoch 11....
Epoch has taken 0:01:51.950927
Number of used sentences in train = 2074
Total loss for epoch 11: 3230.437100
validation loss after epoch 11 : 1012.630306
	Epoch 12....
Epoch has taken 0:01:49.717715
Number of used sentences in train = 2074
Total loss for epoch 12: 3221.001266
validation loss after epoch 12 : 1022.225739
	Epoch 13....
Epoch has taken 0:01:49.121379
Number of used sentences in train = 2074
Total loss for epoch 13: 3203.958940
validation loss after epoch 13 : 1042.340322
	Epoch 14....
Epoch has taken 0:01:49.269276
Number of used sentences in train = 2074
Total loss for epoch 14: 3195.763136
validation loss after epoch 14 : 1062.433202
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1133, 241)
  (lstm): LSTM(257, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.047087
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1194.238748
	Epoch 1....
Epoch has taken 0:00:11.209805
Number of used sentences in train = 231
Total loss for epoch 1: 575.877259
	Epoch 2....
Epoch has taken 0:00:11.387272
Number of used sentences in train = 231
Total loss for epoch 2: 456.768441
	Epoch 3....
Epoch has taken 0:00:11.899661
Number of used sentences in train = 231
Total loss for epoch 3: 403.159006
	Epoch 4....
Epoch has taken 0:00:11.228287
Number of used sentences in train = 231
Total loss for epoch 4: 378.871252
	Epoch 5....
Epoch has taken 0:00:11.216591
Number of used sentences in train = 231
Total loss for epoch 5: 366.906785
	Epoch 6....
Epoch has taken 0:00:11.210962
Number of used sentences in train = 231
Total loss for epoch 6: 360.850051
	Epoch 7....
Epoch has taken 0:00:11.228125
Number of used sentences in train = 231
Total loss for epoch 7: 356.551018
	Epoch 8....
Epoch has taken 0:00:11.219760
Number of used sentences in train = 231
Total loss for epoch 8: 353.641084
	Epoch 9....
Epoch has taken 0:00:11.210062
Number of used sentences in train = 231
Total loss for epoch 9: 350.612551
	Epoch 10....
Epoch has taken 0:00:11.216454
Number of used sentences in train = 231
Total loss for epoch 10: 349.610073
	Epoch 11....
Epoch has taken 0:00:11.210207
Number of used sentences in train = 231
Total loss for epoch 11: 348.988387
	Epoch 12....
Epoch has taken 0:00:11.214924
Number of used sentences in train = 231
Total loss for epoch 12: 348.532748
	Epoch 13....
Epoch has taken 0:00:11.161385
Number of used sentences in train = 231
Total loss for epoch 13: 348.126239
	Epoch 14....
Epoch has taken 0:00:11.153276
Number of used sentences in train = 231
Total loss for epoch 14: 347.749819
Epoch has taken 0:00:11.161444

==================================================================================================
	Training time : 0:30:18.176176
==================================================================================================
	Identification : 0.363

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(1202, 241)
  (lstm): LSTM(257, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14866.194661
validation loss after epoch 0 : 1574.595633
	Epoch 1....
Epoch has taken 0:03:32.653569
Number of used sentences in train = 3226
Total loss for epoch 1: 11572.453633
validation loss after epoch 1 : 1392.786936
	Epoch 2....
Epoch has taken 0:03:32.610387
Number of used sentences in train = 3226
Total loss for epoch 2: 10425.926483
validation loss after epoch 2 : 1421.677274
	Epoch 3....
Epoch has taken 0:03:33.655357
Number of used sentences in train = 3226
Total loss for epoch 3: 9670.730138
validation loss after epoch 3 : 1458.004474
	Epoch 4....
Epoch has taken 0:03:33.983837
Number of used sentences in train = 3226
Total loss for epoch 4: 9093.831769
validation loss after epoch 4 : 1482.483512
	Epoch 5....
Epoch has taken 0:03:34.140121
Number of used sentences in train = 3226
Total loss for epoch 5: 8710.750038
validation loss after epoch 5 : 1603.296558
	Epoch 6....
Epoch has taken 0:03:34.792145
Number of used sentences in train = 3226
Total loss for epoch 6: 8462.628126
validation loss after epoch 6 : 1674.602417
	Epoch 7....
Epoch has taken 0:03:40.292594
Number of used sentences in train = 3226
Total loss for epoch 7: 8158.198843
validation loss after epoch 7 : 1699.796580
	Epoch 8....
Epoch has taken 0:03:37.450219
Number of used sentences in train = 3226
Total loss for epoch 8: 7848.759225
validation loss after epoch 8 : 1777.811803
	Epoch 9....
Epoch has taken 0:03:35.021274
Number of used sentences in train = 3226
Total loss for epoch 9: 7631.265519
validation loss after epoch 9 : 1813.637925
	Epoch 10....
Epoch has taken 0:03:46.860945
Number of used sentences in train = 3226
Total loss for epoch 10: 7438.868814
validation loss after epoch 10 : 1880.953060
	Epoch 11....
Epoch has taken 0:03:42.409777
Number of used sentences in train = 3226
Total loss for epoch 11: 7272.562982
validation loss after epoch 11 : 1936.152917
	Epoch 12....
Epoch has taken 0:03:37.366947
Number of used sentences in train = 3226
Total loss for epoch 12: 7163.451088
validation loss after epoch 12 : 1998.316445
	Epoch 13....
Epoch has taken 0:03:39.262616
Number of used sentences in train = 3226
Total loss for epoch 13: 7054.148083
validation loss after epoch 13 : 2041.068154
	Epoch 14....
Epoch has taken 0:03:42.195922
Number of used sentences in train = 3226
Total loss for epoch 14: 6996.490287
validation loss after epoch 14 : 2079.974596
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(1202, 241)
  (lstm): LSTM(257, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:42.810407
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1746.200855
	Epoch 1....
Epoch has taken 0:00:21.498946
Number of used sentences in train = 359
Total loss for epoch 1: 1166.314429
	Epoch 2....
Epoch has taken 0:00:21.608611
Number of used sentences in train = 359
Total loss for epoch 2: 951.902164
	Epoch 3....
Epoch has taken 0:00:21.646224
Number of used sentences in train = 359
Total loss for epoch 3: 856.887602
	Epoch 4....
Epoch has taken 0:00:21.657879
Number of used sentences in train = 359
Total loss for epoch 4: 792.040491
	Epoch 5....
Epoch has taken 0:00:21.685595
Number of used sentences in train = 359
Total loss for epoch 5: 754.272692
	Epoch 6....
Epoch has taken 0:00:21.623369
Number of used sentences in train = 359
Total loss for epoch 6: 729.813024
	Epoch 7....
Epoch has taken 0:00:21.652956
Number of used sentences in train = 359
Total loss for epoch 7: 710.536504
	Epoch 8....
Epoch has taken 0:00:21.613170
Number of used sentences in train = 359
Total loss for epoch 8: 706.738867
	Epoch 9....
Epoch has taken 0:00:21.356176
Number of used sentences in train = 359
Total loss for epoch 9: 698.373086
	Epoch 10....
Epoch has taken 0:00:21.705303
Number of used sentences in train = 359
Total loss for epoch 10: 695.692611
	Epoch 11....
Epoch has taken 0:00:21.608591
Number of used sentences in train = 359
Total loss for epoch 11: 698.733049
	Epoch 12....
Epoch has taken 0:00:21.709954
Number of used sentences in train = 359
Total loss for epoch 12: 691.877143
	Epoch 13....
Epoch has taken 0:00:21.403774
Number of used sentences in train = 359
Total loss for epoch 13: 688.407146
	Epoch 14....
Epoch has taken 0:00:21.653120
Number of used sentences in train = 359
Total loss for epoch 14: 685.786416
Epoch has taken 0:00:21.782520

==================================================================================================
	Training time : 0:59:50.366285
==================================================================================================
	Identification : 0.1

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 19, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 64, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 27, 'lstmDropout': 0.26, 'denseActivation': 'tanh', 'wordDim': 120, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(5907, 120)
  (lstm): LSTM(184, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12541.859039
validation loss after epoch 0 : 1234.180319
	Epoch 1....
Epoch has taken 0:02:47.624230
Number of used sentences in train = 2811
Total loss for epoch 1: 8830.878583
validation loss after epoch 1 : 1108.583250
	Epoch 2....
Epoch has taken 0:03:00.971060
Number of used sentences in train = 2811
Total loss for epoch 2: 7387.148071
validation loss after epoch 2 : 1131.356589
	Epoch 3....
Epoch has taken 0:02:48.842629
Number of used sentences in train = 2811
Total loss for epoch 3: 6469.620680
validation loss after epoch 3 : 1202.226687
	Epoch 4....
Epoch has taken 0:02:47.092341
Number of used sentences in train = 2811
Total loss for epoch 4: 5914.605937
validation loss after epoch 4 : 1305.722815
	Epoch 5....
Epoch has taken 0:02:47.651088
Number of used sentences in train = 2811
Total loss for epoch 5: 5562.733559
validation loss after epoch 5 : 1377.696099
	Epoch 6....
Epoch has taken 0:02:47.831649
Number of used sentences in train = 2811
Total loss for epoch 6: 5311.524469
validation loss after epoch 6 : 1427.663770
	Epoch 7....
Epoch has taken 0:02:57.404839
Number of used sentences in train = 2811
Total loss for epoch 7: 5097.678342
validation loss after epoch 7 : 1467.920676
	Epoch 8....
Epoch has taken 0:02:48.630218
Number of used sentences in train = 2811
Total loss for epoch 8: 4932.047781
validation loss after epoch 8 : 1536.789523
	Epoch 9....
Epoch has taken 0:02:47.153735
Number of used sentences in train = 2811
Total loss for epoch 9: 4804.917910
validation loss after epoch 9 : 1596.285992
	Epoch 10....
Epoch has taken 0:02:47.729305
Number of used sentences in train = 2811
Total loss for epoch 10: 4730.696043
validation loss after epoch 10 : 1629.433817
	Epoch 11....
Epoch has taken 0:02:38.728805
Number of used sentences in train = 2811
Total loss for epoch 11: 4679.425370
validation loss after epoch 11 : 1662.388354
	Epoch 12....
Epoch has taken 0:02:44.438963
Number of used sentences in train = 2811
Total loss for epoch 12: 4629.134046
validation loss after epoch 12 : 1722.928002
	Epoch 13....
Epoch has taken 0:02:37.469386
Number of used sentences in train = 2811
Total loss for epoch 13: 4601.438673
validation loss after epoch 13 : 1726.028043
	Epoch 14....
Epoch has taken 0:02:36.704856
Number of used sentences in train = 2811
Total loss for epoch 14: 4581.807443
validation loss after epoch 14 : 1755.419219
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(5907, 120)
  (lstm): LSTM(184, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:36.652930
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1592.782577
	Epoch 1....
Epoch has taken 0:00:16.631020
Number of used sentences in train = 313
Total loss for epoch 1: 830.573900
	Epoch 2....
Epoch has taken 0:00:16.591660
Number of used sentences in train = 313
Total loss for epoch 2: 643.869797
	Epoch 3....
Epoch has taken 0:00:16.614256
Number of used sentences in train = 313
Total loss for epoch 3: 575.702211
	Epoch 4....
Epoch has taken 0:00:16.621011
Number of used sentences in train = 313
Total loss for epoch 4: 541.915991
	Epoch 5....
Epoch has taken 0:00:16.637125
Number of used sentences in train = 313
Total loss for epoch 5: 529.255678
	Epoch 6....
Epoch has taken 0:00:16.633385
Number of used sentences in train = 313
Total loss for epoch 6: 521.378931
	Epoch 7....
Epoch has taken 0:00:16.658228
Number of used sentences in train = 313
Total loss for epoch 7: 518.570984
	Epoch 8....
Epoch has taken 0:00:16.631069
Number of used sentences in train = 313
Total loss for epoch 8: 516.986476
	Epoch 9....
Epoch has taken 0:00:16.636376
Number of used sentences in train = 313
Total loss for epoch 9: 514.737878
	Epoch 10....
Epoch has taken 0:00:16.626891
Number of used sentences in train = 313
Total loss for epoch 10: 513.468171
	Epoch 11....
Epoch has taken 0:00:16.644018
Number of used sentences in train = 313
Total loss for epoch 11: 512.199993
	Epoch 12....
Epoch has taken 0:00:16.637893
Number of used sentences in train = 313
Total loss for epoch 12: 511.477857
	Epoch 13....
Epoch has taken 0:00:16.643158
Number of used sentences in train = 313
Total loss for epoch 13: 512.419911
	Epoch 14....
Epoch has taken 0:00:16.650760
Number of used sentences in train = 313
Total loss for epoch 14: 510.595685
Epoch has taken 0:00:16.643050

==================================================================================================
	Training time : 0:45:44.952791
==================================================================================================
	Identification : 0.122

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(5651, 120)
  (lstm): LSTM(184, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9595.317856
validation loss after epoch 0 : 989.269050
	Epoch 1....
Epoch has taken 0:01:47.737352
Number of used sentences in train = 2074
Total loss for epoch 1: 6570.382703
validation loss after epoch 1 : 900.565409
	Epoch 2....
Epoch has taken 0:01:47.829946
Number of used sentences in train = 2074
Total loss for epoch 2: 5297.055195
validation loss after epoch 2 : 906.168791
	Epoch 3....
Epoch has taken 0:01:46.981909
Number of used sentences in train = 2074
Total loss for epoch 3: 4585.255329
validation loss after epoch 3 : 1020.089919
	Epoch 4....
Epoch has taken 0:01:47.005467
Number of used sentences in train = 2074
Total loss for epoch 4: 4099.211480
validation loss after epoch 4 : 1059.394710
	Epoch 5....
Epoch has taken 0:01:46.884740
Number of used sentences in train = 2074
Total loss for epoch 5: 3805.740154
validation loss after epoch 5 : 1080.049070
	Epoch 6....
Epoch has taken 0:01:46.884942
Number of used sentences in train = 2074
Total loss for epoch 6: 3581.175394
validation loss after epoch 6 : 1107.769553
	Epoch 7....
Epoch has taken 0:01:47.260237
Number of used sentences in train = 2074
Total loss for epoch 7: 3450.944428
validation loss after epoch 7 : 1147.226062
	Epoch 8....
Epoch has taken 0:01:47.780291
Number of used sentences in train = 2074
Total loss for epoch 8: 3363.156183
validation loss after epoch 8 : 1178.120619
	Epoch 9....
Epoch has taken 0:01:46.938133
Number of used sentences in train = 2074
Total loss for epoch 9: 3302.060527
validation loss after epoch 9 : 1176.584193
	Epoch 10....
Epoch has taken 0:01:47.064494
Number of used sentences in train = 2074
Total loss for epoch 10: 3265.173779
validation loss after epoch 10 : 1202.354263
	Epoch 11....
Epoch has taken 0:01:46.969837
Number of used sentences in train = 2074
Total loss for epoch 11: 3242.300656
validation loss after epoch 11 : 1228.552470
	Epoch 12....
Epoch has taken 0:02:03.402923
Number of used sentences in train = 2074
Total loss for epoch 12: 3221.476690
validation loss after epoch 12 : 1250.735461
	Epoch 13....
Epoch has taken 0:01:47.475596
Number of used sentences in train = 2074
Total loss for epoch 13: 3206.005605
validation loss after epoch 13 : 1267.751409
	Epoch 14....
Epoch has taken 0:01:46.599828
Number of used sentences in train = 2074
Total loss for epoch 14: 3196.599720
validation loss after epoch 14 : 1274.556851
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(5651, 120)
  (lstm): LSTM(184, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:46.793525
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1808.900432
	Epoch 1....
Epoch has taken 0:00:10.910887
Number of used sentences in train = 231
Total loss for epoch 1: 630.513454
	Epoch 2....
Epoch has taken 0:00:10.890423
Number of used sentences in train = 231
Total loss for epoch 2: 453.842200
	Epoch 3....
Epoch has taken 0:00:10.901040
Number of used sentences in train = 231
Total loss for epoch 3: 397.887682
	Epoch 4....
Epoch has taken 0:00:10.875675
Number of used sentences in train = 231
Total loss for epoch 4: 378.006467
	Epoch 5....
Epoch has taken 0:00:10.880646
Number of used sentences in train = 231
Total loss for epoch 5: 368.359998
	Epoch 6....
Epoch has taken 0:00:10.859765
Number of used sentences in train = 231
Total loss for epoch 6: 359.421466
	Epoch 7....
Epoch has taken 0:00:10.866367
Number of used sentences in train = 231
Total loss for epoch 7: 353.081354
	Epoch 8....
Epoch has taken 0:00:10.873902
Number of used sentences in train = 231
Total loss for epoch 8: 351.334271
	Epoch 9....
Epoch has taken 0:00:10.860609
Number of used sentences in train = 231
Total loss for epoch 9: 350.086445
	Epoch 10....
Epoch has taken 0:00:10.885226
Number of used sentences in train = 231
Total loss for epoch 10: 349.271680
	Epoch 11....
Epoch has taken 0:00:10.841197
Number of used sentences in train = 231
Total loss for epoch 11: 348.570185
	Epoch 12....
Epoch has taken 0:00:10.893273
Number of used sentences in train = 231
Total loss for epoch 12: 348.076930
	Epoch 13....
Epoch has taken 0:00:10.860896
Number of used sentences in train = 231
Total loss for epoch 13: 347.656393
	Epoch 14....
Epoch has taken 0:00:10.887627
Number of used sentences in train = 231
Total loss for epoch 14: 347.322643
Epoch has taken 0:00:10.854236

==================================================================================================
	Training time : 0:29:47.094410
==================================================================================================
	Identification : 0.208

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
	TransitionClassifier(
  (p_embeddings): Embedding(13, 64)
  (w_embeddings): Embedding(6826, 120)
  (lstm): LSTM(184, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Number of used sentences in train = 3226
Total loss for epoch 0: 16616.122844
validation loss after epoch 0 : 1574.723068
	Epoch 1....
Epoch has taken 0:03:31.242346
Number of used sentences in train = 3226
Total loss for epoch 1: 12590.213592
validation loss after epoch 1 : 1530.351235
	Epoch 2....
Epoch has taken 0:03:29.888672
Number of used sentences in train = 3226
Total loss for epoch 2: 11166.266238
validation loss after epoch 2 : 1555.306172
	Epoch 3....
Epoch has taken 0:03:34.738621
Number of used sentences in train = 3226
Total loss for epoch 3: 10062.391395
validation loss after epoch 3 : 1582.841509
	Epoch 4....
Epoch has taken 0:03:30.905804
Number of used sentences in train = 3226
Total loss for epoch 4: 9280.305067
validation loss after epoch 4 : 1637.062802
	Epoch 5....
Epoch has taken 0:03:29.419980
Number of used sentences in train = 3226
Total loss for epoch 5: 8630.695268
validation loss after epoch 5 : 1776.509316
	Epoch 6....
Epoch has taken 0:03:29.893897
Number of used sentences in train = 3226
Total loss for epoch 6: 8163.221043
validation loss after epoch 6 : 1808.849325
	Epoch 7....
Epoch has taken 0:03:30.423172
Number of used sentences in train = 3226
Total loss for epoch 7: 7739.616377
validation loss after epoch 7 : 1918.035840
	Epoch 8....
Epoch has taken 0:03:40.324478
Number of used sentences in train = 3226
Total loss for epoch 8: 7417.950356
validation loss after epoch 8 : 2005.181265
	Epoch 9....
Epoch has taken 0:03:31.367320
Number of used sentences in train = 3226
Total loss for epoch 9: 7165.416720
validation loss after epoch 9 : 2115.357844
	Epoch 10....
Epoch has taken 0:03:32.591139
Number of used sentences in train = 3226
Total loss for epoch 10: 6934.438380
validation loss after epoch 10 : 2162.282643
	Epoch 11....
Epoch has taken 0:03:54.460683
Number of used sentences in train = 3226
Total loss for epoch 11: 6729.985643
validation loss after epoch 11 : 2250.021603
	Epoch 12....
Epoch has taken 0:03:54.810033
Number of used sentences in train = 3226
Total loss for epoch 12: 6607.776177
validation loss after epoch 12 : 2331.924077
	Epoch 13....
Epoch has taken 0:03:36.149443
Number of used sentences in train = 3226
Total loss for epoch 13: 6518.599669
validation loss after epoch 13 : 2398.560650
	Epoch 14....
Epoch has taken 0:03:31.851043
Number of used sentences in train = 3226
Total loss for epoch 14: 6441.679506
validation loss after epoch 14 : 2471.802568
	TransitionClassifier(
  (p_embeddings): Embedding(13, 64)
  (w_embeddings): Embedding(6826, 120)
  (lstm): LSTM(184, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:34.401199
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1895.371837
	Epoch 1....
Epoch has taken 0:00:20.683947
Number of used sentences in train = 359
Total loss for epoch 1: 1188.776689
	Epoch 2....
Epoch has taken 0:00:20.620293
Number of used sentences in train = 359
Total loss for epoch 2: 915.171238
	Epoch 3....
Epoch has taken 0:00:22.191249
Number of used sentences in train = 359
Total loss for epoch 3: 773.091273
	Epoch 4....
Epoch has taken 0:00:20.839623
Number of used sentences in train = 359
Total loss for epoch 4: 711.420888
	Epoch 5....
Epoch has taken 0:00:22.582753
Number of used sentences in train = 359
Total loss for epoch 5: 689.336280
	Epoch 6....
Epoch has taken 0:00:22.601180
Number of used sentences in train = 359
Total loss for epoch 6: 681.550528
	Epoch 7....
Epoch has taken 0:00:21.828173
Number of used sentences in train = 359
Total loss for epoch 7: 677.740977
	Epoch 8....
Epoch has taken 0:00:22.295856
Number of used sentences in train = 359
Total loss for epoch 8: 675.331628
	Epoch 9....
Epoch has taken 0:00:20.613480
Number of used sentences in train = 359
Total loss for epoch 9: 674.309909
	Epoch 10....
Epoch has taken 0:00:21.712440
Number of used sentences in train = 359
Total loss for epoch 10: 673.672253
	Epoch 11....
Epoch has taken 0:00:20.651153
Number of used sentences in train = 359
Total loss for epoch 11: 673.215470
	Epoch 12....
Epoch has taken 0:00:20.640614
Number of used sentences in train = 359
Total loss for epoch 12: 672.845602
	Epoch 13....
Epoch has taken 0:00:21.815242
Number of used sentences in train = 359
Total loss for epoch 13: 672.548181
	Epoch 14....
Epoch has taken 0:00:22.634044
Number of used sentences in train = 359
Total loss for epoch 14: 672.295715
Epoch has taken 0:00:22.718115

==================================================================================================
	Training time : 0:59:17.569610
==================================================================================================
	Identification : 0.433

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 47, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 47, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 47, 'lstmDropout': 0.21, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 47)
  (w_embeddings): Embedding(1177, 63)
  (lstm): LSTM(110, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10762.559139
validation loss after epoch 0 : 942.445570
	Epoch 1....
Epoch has taken 0:02:53.416357
Number of used sentences in train = 2811
Total loss for epoch 1: 7937.048183
validation loss after epoch 1 : 904.220283
	Epoch 2....
Epoch has taken 0:02:45.439754
Number of used sentences in train = 2811
Total loss for epoch 2: 7035.283580
validation loss after epoch 2 : 930.464601
	Epoch 3....
Epoch has taken 0:02:41.832621
Number of used sentences in train = 2811
Total loss for epoch 3: 6412.990206
validation loss after epoch 3 : 958.511654
	Epoch 4....
Epoch has taken 0:02:40.998613
Number of used sentences in train = 2811
Total loss for epoch 4: 5977.832708
validation loss after epoch 4 : 987.265951
	Epoch 5....
Epoch has taken 0:02:44.062786
Number of used sentences in train = 2811
Total loss for epoch 5: 5666.555892
validation loss after epoch 5 : 1011.428058
	Epoch 6....
Epoch has taken 0:02:48.280540
Number of used sentences in train = 2811
Total loss for epoch 6: 5380.849208
validation loss after epoch 6 : 1052.484247
	Epoch 7....
Epoch has taken 0:02:45.495657
Number of used sentences in train = 2811
Total loss for epoch 7: 5180.084146
validation loss after epoch 7 : 1121.299867
	Epoch 8....
Epoch has taken 0:02:41.873278
Number of used sentences in train = 2811
Total loss for epoch 8: 5030.125451
validation loss after epoch 8 : 1135.155186
	Epoch 9....
Epoch has taken 0:02:48.259371
Number of used sentences in train = 2811
Total loss for epoch 9: 4882.679590
validation loss after epoch 9 : 1167.713158
	Epoch 10....
Epoch has taken 0:03:12.903646
Number of used sentences in train = 2811
Total loss for epoch 10: 4791.513115
validation loss after epoch 10 : 1206.874645
	Epoch 11....
Epoch has taken 0:03:06.122109
Number of used sentences in train = 2811
Total loss for epoch 11: 4710.252689
validation loss after epoch 11 : 1247.469312
	Epoch 12....
Epoch has taken 0:02:42.203094
Number of used sentences in train = 2811
Total loss for epoch 12: 4649.105112
validation loss after epoch 12 : 1236.086133
	Epoch 13....
Epoch has taken 0:02:41.328596
Number of used sentences in train = 2811
Total loss for epoch 13: 4602.103716
validation loss after epoch 13 : 1285.286052
	Epoch 14....
Epoch has taken 0:02:40.182266
Number of used sentences in train = 2811
Total loss for epoch 14: 4577.919541
validation loss after epoch 14 : 1314.620533
	TransitionClassifier(
  (p_embeddings): Embedding(18, 47)
  (w_embeddings): Embedding(1177, 63)
  (lstm): LSTM(110, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:40.783173
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1354.770916
	Epoch 1....
Epoch has taken 0:00:17.166893
Number of used sentences in train = 313
Total loss for epoch 1: 781.233040
	Epoch 2....
Epoch has taken 0:00:17.175855
Number of used sentences in train = 313
Total loss for epoch 2: 631.607633
	Epoch 3....
Epoch has taken 0:00:17.169571
Number of used sentences in train = 313
Total loss for epoch 3: 575.750400
	Epoch 4....
Epoch has taken 0:00:17.179941
Number of used sentences in train = 313
Total loss for epoch 4: 538.544111
	Epoch 5....
Epoch has taken 0:00:17.171280
Number of used sentences in train = 313
Total loss for epoch 5: 525.257570
	Epoch 6....
Epoch has taken 0:00:17.170213
Number of used sentences in train = 313
Total loss for epoch 6: 512.866751
	Epoch 7....
Epoch has taken 0:00:17.171957
Number of used sentences in train = 313
Total loss for epoch 7: 510.351439
	Epoch 8....
Epoch has taken 0:00:17.160852
Number of used sentences in train = 313
Total loss for epoch 8: 509.181093
	Epoch 9....
Epoch has taken 0:00:17.179813
Number of used sentences in train = 313
Total loss for epoch 9: 507.549236
	Epoch 10....
Epoch has taken 0:00:17.717708
Number of used sentences in train = 313
Total loss for epoch 10: 506.656750
	Epoch 11....
Epoch has taken 0:00:17.177069
Number of used sentences in train = 313
Total loss for epoch 11: 506.614721
	Epoch 12....
Epoch has taken 0:00:17.156446
Number of used sentences in train = 313
Total loss for epoch 12: 505.362803
	Epoch 13....
Epoch has taken 0:00:17.197449
Number of used sentences in train = 313
Total loss for epoch 13: 504.709760
	Epoch 14....
Epoch has taken 0:00:17.177024
Number of used sentences in train = 313
Total loss for epoch 14: 503.787624
Epoch has taken 0:00:17.178756

==================================================================================================
	Training time : 0:46:11.854521
==================================================================================================
	Identification : 0.408

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 47)
  (w_embeddings): Embedding(1133, 63)
  (lstm): LSTM(110, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
	Epoch 0....
Number of used sentences in train = 2074
Total loss for epoch 0: 8192.326144
validation loss after epoch 0 : 775.239437
	Epoch 1....
Epoch has taken 0:01:49.769173
Number of used sentences in train = 2074
Total loss for epoch 1: 5558.842440
validation loss after epoch 1 : 692.519652
	Epoch 2....
Epoch has taken 0:01:49.677653
Number of used sentences in train = 2074
Total loss for epoch 2: 4730.851873
validation loss after epoch 2 : 687.688474
	Epoch 3....
Epoch has taken 0:01:50.755920
Number of used sentences in train = 2074
Total loss for epoch 3: 4230.212151
validation loss after epoch 3 : 688.068794
	Epoch 4....
Epoch has taken 0:01:50.138735
Number of used sentences in train = 2074
Total loss for epoch 4: 3893.940335
validation loss after epoch 4 : 720.597911
	Epoch 5....
Epoch has taken 0:01:49.857281
Number of used sentences in train = 2074
Total loss for epoch 5: 3652.439666
validation loss after epoch 5 : 742.963084
	Epoch 6....
Epoch has taken 0:01:49.388814
Number of used sentences in train = 2074
Total loss for epoch 6: 3501.025843
validation loss after epoch 6 : 770.054208
	Epoch 7....
Epoch has taken 0:01:49.706318
Number of used sentences in train = 2074
Total loss for epoch 7: 3368.167343
validation loss after epoch 7 : 780.322521
	Epoch 8....
Epoch has taken 0:01:50.746271
Number of used sentences in train = 2074
Total loss for epoch 8: 3292.190786
validation loss after epoch 8 : 821.805200
	Epoch 9....
Epoch has taken 0:01:50.771169
Number of used sentences in train = 2074
Total loss for epoch 9: 3246.650641
validation loss after epoch 9 : 827.460066
	Epoch 10....
Epoch has taken 0:01:50.340383
Number of used sentences in train = 2074
Total loss for epoch 10: 3219.705332
validation loss after epoch 10 : 867.642525
	Epoch 11....
Epoch has taken 0:01:49.839117
Number of used sentences in train = 2074
Total loss for epoch 11: 3210.087749
validation loss after epoch 11 : 876.756088
	Epoch 12....
Epoch has taken 0:01:49.863898
Number of used sentences in train = 2074
Total loss for epoch 12: 3190.409886
validation loss after epoch 12 : 889.714241
	Epoch 13....
Epoch has taken 0:01:50.746703
Number of used sentences in train = 2074
Total loss for epoch 13: 3182.734098
validation loss after epoch 13 : 898.506122
	Epoch 14....
Epoch has taken 0:01:50.649203
Number of used sentences in train = 2074
Total loss for epoch 14: 3179.167527
validation loss after epoch 14 : 906.370796
	TransitionClassifier(
  (p_embeddings): Embedding(18, 47)
  (w_embeddings): Embedding(1133, 63)
  (lstm): LSTM(110, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.084489
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1208.117066
	Epoch 1....
Epoch has taken 0:00:11.239124
Number of used sentences in train = 231
Total loss for epoch 1: 586.731959
	Epoch 2....
Epoch has taken 0:00:11.241785
Number of used sentences in train = 231
Total loss for epoch 2: 459.337976
	Epoch 3....
Epoch has taken 0:00:11.221234
Number of used sentences in train = 231
Total loss for epoch 3: 397.703044
	Epoch 4....
Epoch has taken 0:00:11.225065
Number of used sentences in train = 231
Total loss for epoch 4: 374.031337
	Epoch 5....
Epoch has taken 0:00:11.214395
Number of used sentences in train = 231
Total loss for epoch 5: 359.942606
	Epoch 6....
Epoch has taken 0:00:11.220718
Number of used sentences in train = 231
Total loss for epoch 6: 355.965326
	Epoch 7....
Epoch has taken 0:00:11.221306
Number of used sentences in train = 231
Total loss for epoch 7: 348.991919
	Epoch 8....
Epoch has taken 0:00:11.226803
Number of used sentences in train = 231
Total loss for epoch 8: 347.723106
	Epoch 9....
Epoch has taken 0:00:11.216452
Number of used sentences in train = 231
Total loss for epoch 9: 347.186344
	Epoch 10....
Epoch has taken 0:00:11.218984
Number of used sentences in train = 231
Total loss for epoch 10: 346.782820
	Epoch 11....
Epoch has taken 0:00:11.205802
Number of used sentences in train = 231
Total loss for epoch 11: 346.450874
	Epoch 12....
Epoch has taken 0:00:11.213823
Number of used sentences in train = 231
Total loss for epoch 12: 346.177448
	Epoch 13....
Epoch has taken 0:00:11.234554
Number of used sentences in train = 231
Total loss for epoch 13: 345.955773
	Epoch 14....
Epoch has taken 0:00:11.231734
Number of used sentences in train = 231
Total loss for epoch 14: 345.800620
Epoch has taken 0:00:11.189503

==================================================================================================
	Training time : 0:30:20.994810
==================================================================================================
	Identification : 0.257

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 47)
  (w_embeddings): Embedding(1202, 63)
  (lstm): LSTM(110, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15132.418592
validation loss after epoch 0 : 1419.929988
	Epoch 1....
Epoch has taken 0:03:34.928442
Number of used sentences in train = 3226
Total loss for epoch 1: 11770.778034
validation loss after epoch 1 : 1352.280153
	Epoch 2....
Epoch has taken 0:03:42.272213
Number of used sentences in train = 3226
Total loss for epoch 2: 10572.980293
validation loss after epoch 2 : 1370.413250
	Epoch 3....
Epoch has taken 0:03:43.321671
Number of used sentences in train = 3226
Total loss for epoch 3: 9840.827353
validation loss after epoch 3 : 1357.763037
	Epoch 4....
Epoch has taken 0:03:44.017320
Number of used sentences in train = 3226
Total loss for epoch 4: 9297.813416
validation loss after epoch 4 : 1372.891393
	Epoch 5....
Epoch has taken 0:03:36.634891
Number of used sentences in train = 3226
Total loss for epoch 5: 8851.904328
validation loss after epoch 5 : 1471.576374
	Epoch 6....
Epoch has taken 0:03:36.824435
Number of used sentences in train = 3226
Total loss for epoch 6: 8477.993230
validation loss after epoch 6 : 1466.499032
	Epoch 7....
Epoch has taken 0:03:36.147665
Number of used sentences in train = 3226
Total loss for epoch 7: 8206.177481
validation loss after epoch 7 : 1473.732261
	Epoch 8....
Epoch has taken 0:03:35.560783
Number of used sentences in train = 3226
Total loss for epoch 8: 7932.638515
validation loss after epoch 8 : 1600.039088
	Epoch 9....
Epoch has taken 0:03:34.221775
Number of used sentences in train = 3226
Total loss for epoch 9: 7683.269264
validation loss after epoch 9 : 1653.364091
	Epoch 10....
Epoch has taken 0:03:34.071616
Number of used sentences in train = 3226
Total loss for epoch 10: 7485.495148
validation loss after epoch 10 : 1687.276727
	Epoch 11....
Epoch has taken 0:03:33.528147
Number of used sentences in train = 3226
Total loss for epoch 11: 7310.517766
validation loss after epoch 11 : 1737.873978
	Epoch 12....
Epoch has taken 0:03:38.892690
Number of used sentences in train = 3226
Total loss for epoch 12: 7185.520794
validation loss after epoch 12 : 1842.478725
	Epoch 13....
Epoch has taken 0:03:43.393383
Number of used sentences in train = 3226
Total loss for epoch 13: 7006.469027
validation loss after epoch 13 : 1854.499811
	Epoch 14....
Epoch has taken 0:04:03.251490
Number of used sentences in train = 3226
Total loss for epoch 14: 6891.143439
validation loss after epoch 14 : 1993.501524
	TransitionClassifier(
  (p_embeddings): Embedding(13, 47)
  (w_embeddings): Embedding(1202, 63)
  (lstm): LSTM(110, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:45.239476
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2114.751659
	Epoch 1....
Epoch has taken 0:00:22.021321
Number of used sentences in train = 359
Total loss for epoch 1: 1165.499094
	Epoch 2....
Epoch has taken 0:00:21.663908
Number of used sentences in train = 359
Total loss for epoch 2: 971.524951
	Epoch 3....
Epoch has taken 0:00:21.889936
Number of used sentences in train = 359
Total loss for epoch 3: 874.822698
	Epoch 4....
Epoch has taken 0:00:21.968960
Number of used sentences in train = 359
Total loss for epoch 4: 808.322993
	Epoch 5....
Epoch has taken 0:00:21.860262
Number of used sentences in train = 359
Total loss for epoch 5: 753.772777
	Epoch 6....
Epoch has taken 0:00:21.879266
Number of used sentences in train = 359
Total loss for epoch 6: 727.631643
	Epoch 7....
Epoch has taken 0:00:21.811264
Number of used sentences in train = 359
Total loss for epoch 7: 706.847675
	Epoch 8....
Epoch has taken 0:00:21.928795
Number of used sentences in train = 359
Total loss for epoch 8: 696.634222
	Epoch 9....
Epoch has taken 0:00:21.935228
Number of used sentences in train = 359
Total loss for epoch 9: 689.605362
	Epoch 10....
Epoch has taken 0:00:20.965531
Number of used sentences in train = 359
Total loss for epoch 10: 682.943931
	Epoch 11....
Epoch has taken 0:00:21.808368
Number of used sentences in train = 359
Total loss for epoch 11: 679.386690
	Epoch 12....
Epoch has taken 0:00:21.992332
Number of used sentences in train = 359
Total loss for epoch 12: 676.842923
	Epoch 13....
Epoch has taken 0:00:21.061044
Number of used sentences in train = 359
Total loss for epoch 13: 674.588579
	Epoch 14....
Epoch has taken 0:00:21.650435
Number of used sentences in train = 359
Total loss for epoch 14: 673.847253
Epoch has taken 0:00:21.862439

==================================================================================================
	Training time : 1:00:29.280277
==================================================================================================
	Identification : 0.441

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 74, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 53, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 98, 'lstmDropout': 0.27, 'denseActivation': 'tanh', 'wordDim': 202, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 53)
  (w_embeddings): Embedding(1177, 202)
  (lstm): LSTM(255, 98, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1568, out_features=74, bias=True)
  (linear2): Linear(in_features=74, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13113.067934
validation loss after epoch 0 : 953.551621
	Epoch 1....
Epoch has taken 0:03:02.147543
Number of used sentences in train = 2811
Total loss for epoch 1: 8375.536687
validation loss after epoch 1 : 887.771984
	Epoch 2....
Epoch has taken 0:03:02.099732
Number of used sentences in train = 2811
Total loss for epoch 2: 7428.860503
validation loss after epoch 2 : 884.021333
	Epoch 3....
Epoch has taken 0:03:01.756037
Number of used sentences in train = 2811
Total loss for epoch 3: 6830.517030
validation loss after epoch 3 : 887.128752
	Epoch 4....
Epoch has taken 0:03:03.360763
Number of used sentences in train = 2811
Total loss for epoch 4: 6356.763890
validation loss after epoch 4 : 929.649418
	Epoch 5....
Epoch has taken 0:03:01.483177
Number of used sentences in train = 2811
Total loss for epoch 5: 6050.951751
validation loss after epoch 5 : 963.154491
	Epoch 6....
Epoch has taken 0:03:02.426601
Number of used sentences in train = 2811
Total loss for epoch 6: 5795.326182
validation loss after epoch 6 : 957.924635
	Epoch 7....
Epoch has taken 0:03:02.383431
Number of used sentences in train = 2811
Total loss for epoch 7: 5602.371883
validation loss after epoch 7 : 945.314001
	Epoch 8....
Epoch has taken 0:03:28.856730
Number of used sentences in train = 2811
Total loss for epoch 8: 5443.967273
validation loss after epoch 8 : 916.315497
	Epoch 9....
Epoch has taken 0:03:02.226828
Number of used sentences in train = 2811
Total loss for epoch 9: 5270.596615
validation loss after epoch 9 : 941.675873
	Epoch 10....
Epoch has taken 0:03:02.509110
Number of used sentences in train = 2811
Total loss for epoch 10: 5148.289190
validation loss after epoch 10 : 1030.068882
	Epoch 11....
Epoch has taken 0:03:02.365051
Number of used sentences in train = 2811
Total loss for epoch 11: 5074.086579
validation loss after epoch 11 : 1000.584255
	Epoch 12....
Epoch has taken 0:03:31.170615
Number of used sentences in train = 2811
Total loss for epoch 12: 4996.199497
validation loss after epoch 12 : 1051.336902
	Epoch 13....
Epoch has taken 0:03:03.404984
Number of used sentences in train = 2811
Total loss for epoch 13: 4899.272851
validation loss after epoch 13 : 1041.423554
	Epoch 14....
Epoch has taken 0:03:01.883446
Number of used sentences in train = 2811
Total loss for epoch 14: 4879.923407
validation loss after epoch 14 : 1053.556548
	TransitionClassifier(
  (p_embeddings): Embedding(18, 53)
  (w_embeddings): Embedding(1177, 202)
  (lstm): LSTM(255, 98, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1568, out_features=74, bias=True)
  (linear2): Linear(in_features=74, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:02.136165
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1565.076680
	Epoch 1....
Epoch has taken 0:00:19.261443
Number of used sentences in train = 313
Total loss for epoch 1: 839.673272
	Epoch 2....
Epoch has taken 0:00:19.404186
Number of used sentences in train = 313
Total loss for epoch 2: 674.672856
	Epoch 3....
Epoch has taken 0:00:19.337537
Number of used sentences in train = 313
Total loss for epoch 3: 619.695642
	Epoch 4....
Epoch has taken 0:00:19.470780
Number of used sentences in train = 313
Total loss for epoch 4: 583.452301
	Epoch 5....
Epoch has taken 0:00:19.425450
Number of used sentences in train = 313
Total loss for epoch 5: 567.600272
	Epoch 6....
Epoch has taken 0:00:19.163403
Number of used sentences in train = 313
Total loss for epoch 6: 554.027657
	Epoch 7....
Epoch has taken 0:00:19.313657
Number of used sentences in train = 313
Total loss for epoch 7: 541.016541
	Epoch 8....
Epoch has taken 0:00:19.251105
Number of used sentences in train = 313
Total loss for epoch 8: 541.018683
	Epoch 9....
Epoch has taken 0:00:19.372472
Number of used sentences in train = 313
Total loss for epoch 9: 542.535143
	Epoch 10....
Epoch has taken 0:00:19.322371
Number of used sentences in train = 313
Total loss for epoch 10: 538.341813
	Epoch 11....
Epoch has taken 0:00:19.286434
Number of used sentences in train = 313
Total loss for epoch 11: 534.116878
	Epoch 12....
Epoch has taken 0:00:19.340422
Number of used sentences in train = 313
Total loss for epoch 12: 532.076519
	Epoch 13....
Epoch has taken 0:00:19.372989
Number of used sentences in train = 313
Total loss for epoch 13: 532.008693
	Epoch 14....
Epoch has taken 0:00:19.393365
Number of used sentences in train = 313
Total loss for epoch 14: 526.084901
Epoch has taken 0:00:19.578784

==================================================================================================
	Training time : 0:51:21.029102
==================================================================================================
	Identification : 0.073

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 53)
  (w_embeddings): Embedding(1133, 202)
  (lstm): LSTM(255, 98, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1568, out_features=74, bias=True)
  (linear2): Linear(in_features=74, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9811.236881
validation loss after epoch 0 : 707.313262
	Epoch 1....
Epoch has taken 0:02:05.708162
Number of used sentences in train = 2074
Total loss for epoch 1: 5783.230881
validation loss after epoch 1 : 703.523451
	Epoch 2....
Epoch has taken 0:02:05.257616
Number of used sentences in train = 2074
Total loss for epoch 2: 4950.572624
validation loss after epoch 2 : 663.591083
	Epoch 3....
Epoch has taken 0:02:04.116540
Number of used sentences in train = 2074
Total loss for epoch 3: 4396.653380
validation loss after epoch 3 : 676.785124
	Epoch 4....
Epoch has taken 0:02:04.113859
Number of used sentences in train = 2074
Total loss for epoch 4: 4033.664007
validation loss after epoch 4 : 712.570837
	Epoch 5....
Epoch has taken 0:02:04.351283
Number of used sentences in train = 2074
Total loss for epoch 5: 3801.673052
validation loss after epoch 5 : 708.214453
	Epoch 6....
Epoch has taken 0:02:04.441760
Number of used sentences in train = 2074
Total loss for epoch 6: 3648.931229
validation loss after epoch 6 : 719.241882
	Epoch 7....
Epoch has taken 0:02:03.880849
Number of used sentences in train = 2074
Total loss for epoch 7: 3524.052924
validation loss after epoch 7 : 783.114766
	Epoch 8....
Epoch has taken 0:02:05.214123
Number of used sentences in train = 2074
Total loss for epoch 8: 3451.588455
validation loss after epoch 8 : 827.004435
	Epoch 9....
Epoch has taken 0:02:04.655132
Number of used sentences in train = 2074
Total loss for epoch 9: 3383.392065
validation loss after epoch 9 : 876.236364
	Epoch 10....
Epoch has taken 0:02:04.998044
Number of used sentences in train = 2074
Total loss for epoch 10: 3357.422079
validation loss after epoch 10 : 859.023736
	Epoch 11....
Epoch has taken 0:02:04.313634
Number of used sentences in train = 2074
Total loss for epoch 11: 3300.367827
validation loss after epoch 11 : 907.144126
	Epoch 12....
Epoch has taken 0:02:04.392879
Number of used sentences in train = 2074
Total loss for epoch 12: 3251.187942
validation loss after epoch 12 : 884.071645
	Epoch 13....
Epoch has taken 0:02:04.740694
Number of used sentences in train = 2074
Total loss for epoch 13: 3253.303183
validation loss after epoch 13 : 908.680227
	Epoch 14....
Epoch has taken 0:02:04.735883
Number of used sentences in train = 2074
Total loss for epoch 14: 3272.594089
validation loss after epoch 14 : 918.088675
	TransitionClassifier(
  (p_embeddings): Embedding(18, 53)
  (w_embeddings): Embedding(1133, 202)
  (lstm): LSTM(255, 98, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1568, out_features=74, bias=True)
  (linear2): Linear(in_features=74, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:04.853425
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1514.545817
	Epoch 1....
Epoch has taken 0:00:12.850461
Number of used sentences in train = 231
Total loss for epoch 1: 594.015687
	Epoch 2....
Epoch has taken 0:00:12.736629
Number of used sentences in train = 231
Total loss for epoch 2: 444.448709
	Epoch 3....
Epoch has taken 0:00:12.872700
Number of used sentences in train = 231
Total loss for epoch 3: 399.126984
	Epoch 4....
Epoch has taken 0:00:12.763632
Number of used sentences in train = 231
Total loss for epoch 4: 380.149318
	Epoch 5....
Epoch has taken 0:00:12.855401
Number of used sentences in train = 231
Total loss for epoch 5: 361.508287
	Epoch 6....
Epoch has taken 0:00:12.840364
Number of used sentences in train = 231
Total loss for epoch 6: 359.127441
	Epoch 7....
Epoch has taken 0:00:12.795163
Number of used sentences in train = 231
Total loss for epoch 7: 351.808839
	Epoch 8....
Epoch has taken 0:00:13.047030
Number of used sentences in train = 231
Total loss for epoch 8: 348.133882
	Epoch 9....
Epoch has taken 0:00:12.711749
Number of used sentences in train = 231
Total loss for epoch 9: 354.767614
	Epoch 10....
Epoch has taken 0:00:13.042183
Number of used sentences in train = 231
Total loss for epoch 10: 354.240788
	Epoch 11....
Epoch has taken 0:00:12.841720
Number of used sentences in train = 231
Total loss for epoch 11: 348.648979
	Epoch 12....
Epoch has taken 0:00:12.939779
Number of used sentences in train = 231
Total loss for epoch 12: 347.429848
	Epoch 13....
Epoch has taken 0:00:12.727613
Number of used sentences in train = 231
Total loss for epoch 13: 346.134941
	Epoch 14....
Epoch has taken 0:00:13.022336
Number of used sentences in train = 231
Total loss for epoch 14: 348.155472
Epoch has taken 0:00:12.935618

==================================================================================================
	Training time : 0:34:23.102938
==================================================================================================
	Identification : 0.175

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 53)
  (w_embeddings): Embedding(1202, 202)
  (lstm): LSTM(255, 98, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1568, out_features=74, bias=True)
  (linear2): Linear(in_features=74, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20635.163166
validation loss after epoch 0 : 1374.949790
	Epoch 1....
Epoch has taken 0:04:01.949724
Number of used sentences in train = 3226
Total loss for epoch 1: 12069.089406
validation loss after epoch 1 : 1314.777003
	Epoch 2....
Epoch has taken 0:03:56.588011
Number of used sentences in train = 3226
Total loss for epoch 2: 10798.591317
validation loss after epoch 2 : 1283.624436
	Epoch 3....
Epoch has taken 0:03:54.247980
Number of used sentences in train = 3226
Total loss for epoch 3: 10030.465514
validation loss after epoch 3 : 1278.565958
	Epoch 4....
Epoch has taken 0:03:54.953026
Number of used sentences in train = 3226
Total loss for epoch 4: 9526.162383
validation loss after epoch 4 : 1343.499660
	Epoch 5....
Epoch has taken 0:04:00.536394
Number of used sentences in train = 3226
Total loss for epoch 5: 9092.144722
validation loss after epoch 5 : 1374.585406
	Epoch 6....
Epoch has taken 0:04:38.108328
Number of used sentences in train = 3226
Total loss for epoch 6: 8820.008190
validation loss after epoch 6 : 1309.668590
	Epoch 7....
Epoch has taken 0:04:01.925145
Number of used sentences in train = 3226
Total loss for epoch 7: 8525.173083
validation loss after epoch 7 : 1406.098250
	Epoch 8....
Epoch has taken 0:03:57.542498
Number of used sentences in train = 3226
Total loss for epoch 8: 8301.501910
validation loss after epoch 8 : 1468.719504
	Epoch 9....
Epoch has taken 0:04:02.226803
Number of used sentences in train = 3226
Total loss for epoch 9: 8127.126202
validation loss after epoch 9 : 1388.982511
	Epoch 10....
Epoch has taken 0:04:01.199146
Number of used sentences in train = 3226
Total loss for epoch 10: 7868.527923
validation loss after epoch 10 : 1475.829903
	Epoch 11....
Epoch has taken 0:03:59.465839
Number of used sentences in train = 3226
Total loss for epoch 11: 7721.846678
validation loss after epoch 11 : 1573.988703
	Epoch 12....
Epoch has taken 0:03:59.600674
Number of used sentences in train = 3226
Total loss for epoch 12: 7588.383626
validation loss after epoch 12 : 1521.060649
	Epoch 13....
Epoch has taken 0:04:00.147803
Number of used sentences in train = 3226
Total loss for epoch 13: 7496.634025
validation loss after epoch 13 : 1551.675696
	Epoch 14....
Epoch has taken 0:04:37.599077
Number of used sentences in train = 3226
Total loss for epoch 14: 7343.437948
validation loss after epoch 14 : 1610.153472
	TransitionClassifier(
  (p_embeddings): Embedding(13, 53)
  (w_embeddings): Embedding(1202, 202)
  (lstm): LSTM(255, 98, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1568, out_features=74, bias=True)
  (linear2): Linear(in_features=74, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:01.693525
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1907.659287
	Epoch 1....
Epoch has taken 0:00:23.545240
Number of used sentences in train = 359
Total loss for epoch 1: 1220.198328
	Epoch 2....
Epoch has taken 0:00:23.517042
Number of used sentences in train = 359
Total loss for epoch 2: 1086.692612
	Epoch 3....
Epoch has taken 0:00:23.561579
Number of used sentences in train = 359
Total loss for epoch 3: 961.247053
	Epoch 4....
Epoch has taken 0:00:23.512308
Number of used sentences in train = 359
Total loss for epoch 4: 906.646659
	Epoch 5....
Epoch has taken 0:00:23.450011
Number of used sentences in train = 359
Total loss for epoch 5: 816.764241
	Epoch 6....
Epoch has taken 0:00:23.554206
Number of used sentences in train = 359
Total loss for epoch 6: 822.143470
	Epoch 7....
Epoch has taken 0:00:23.557405
Number of used sentences in train = 359
Total loss for epoch 7: 777.569386
	Epoch 8....
Epoch has taken 0:00:23.524373
Number of used sentences in train = 359
Total loss for epoch 8: 735.542761
	Epoch 9....
Epoch has taken 0:00:23.684672
Number of used sentences in train = 359
Total loss for epoch 9: 733.303315
	Epoch 10....
Epoch has taken 0:00:23.504497
Number of used sentences in train = 359
Total loss for epoch 10: 728.940086
	Epoch 11....
Epoch has taken 0:00:23.571051
Number of used sentences in train = 359
Total loss for epoch 11: 717.037143
	Epoch 12....
Epoch has taken 0:00:23.498512
Number of used sentences in train = 359
Total loss for epoch 12: 708.301556
	Epoch 13....
Epoch has taken 0:00:23.547870
Number of used sentences in train = 359
Total loss for epoch 13: 702.254652
	Epoch 14....
Epoch has taken 0:00:23.595125
Number of used sentences in train = 359
Total loss for epoch 14: 708.366311
Epoch has taken 0:00:23.632004

==================================================================================================
	Training time : 1:07:01.704536
==================================================================================================
	Identification : 0.148

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 106, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 39, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 33, 'lstmDropout': 0.18, 'denseActivation': 'tanh', 'wordDim': 132, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 39)
  (w_embeddings): Embedding(1177, 132)
  (lstm): LSTM(171, 33, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11621.541272
validation loss after epoch 0 : 960.828431
	Epoch 1....
Epoch has taken 0:03:29.196889
Number of used sentences in train = 2811
Total loss for epoch 1: 7817.409539
validation loss after epoch 1 : 915.173180
	Epoch 2....
Epoch has taken 0:03:29.556145
Number of used sentences in train = 2811
Total loss for epoch 2: 6953.778457
validation loss after epoch 2 : 910.889988
	Epoch 3....
Epoch has taken 0:03:02.107791
Number of used sentences in train = 2811
Total loss for epoch 3: 6445.670004
validation loss after epoch 3 : 936.929353
	Epoch 4....
Epoch has taken 0:03:00.411248
Number of used sentences in train = 2811
Total loss for epoch 4: 6032.558246
validation loss after epoch 4 : 987.563326
	Epoch 5....
Epoch has taken 0:03:01.388405
Number of used sentences in train = 2811
Total loss for epoch 5: 5771.287684
validation loss after epoch 5 : 1011.853446
	Epoch 6....
Epoch has taken 0:03:03.517291
Number of used sentences in train = 2811
Total loss for epoch 6: 5535.057647
validation loss after epoch 6 : 1020.539048
	Epoch 7....
Epoch has taken 0:03:02.013417
Number of used sentences in train = 2811
Total loss for epoch 7: 5341.787536
validation loss after epoch 7 : 1069.733233
	Epoch 8....
Epoch has taken 0:03:02.480330
Number of used sentences in train = 2811
Total loss for epoch 8: 5195.073116
validation loss after epoch 8 : 1103.219972
	Epoch 9....
Epoch has taken 0:04:04.572985
Number of used sentences in train = 2811
Total loss for epoch 9: 5072.278171
validation loss after epoch 9 : 1129.409081
	Epoch 10....
Epoch has taken 0:03:00.907003
Number of used sentences in train = 2811
Total loss for epoch 10: 4981.331325
validation loss after epoch 10 : 1143.376528
	Epoch 11....
Epoch has taken 0:02:59.343101
Number of used sentences in train = 2811
Total loss for epoch 11: 4891.633525
validation loss after epoch 11 : 1186.078034
	Epoch 12....
Epoch has taken 0:02:59.996151
Number of used sentences in train = 2811
Total loss for epoch 12: 4858.512109
validation loss after epoch 12 : 1203.430391
	Epoch 13....
Epoch has taken 0:02:55.010379
Number of used sentences in train = 2811
Total loss for epoch 13: 4800.424587
validation loss after epoch 13 : 1225.168633
	Epoch 14....
Epoch has taken 0:02:54.582128
Number of used sentences in train = 2811
Total loss for epoch 14: 4796.785720
validation loss after epoch 14 : 1232.352833
	TransitionClassifier(
  (p_embeddings): Embedding(18, 39)
  (w_embeddings): Embedding(1177, 132)
  (lstm): LSTM(171, 33, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.412887
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1968.564962
	Epoch 1....
Epoch has taken 0:00:18.472481
Number of used sentences in train = 313
Total loss for epoch 1: 718.706532
	Epoch 2....
Epoch has taken 0:00:18.510204
Number of used sentences in train = 313
Total loss for epoch 2: 593.850435
	Epoch 3....
Epoch has taken 0:00:19.387600
Number of used sentences in train = 313
Total loss for epoch 3: 552.148273
	Epoch 4....
Epoch has taken 0:00:18.459162
Number of used sentences in train = 313
Total loss for epoch 4: 531.170955
	Epoch 5....
Epoch has taken 0:00:18.474381
Number of used sentences in train = 313
Total loss for epoch 5: 520.497087
	Epoch 6....
Epoch has taken 0:00:18.458934
Number of used sentences in train = 313
Total loss for epoch 6: 511.867778
	Epoch 7....
Epoch has taken 0:00:18.441677
Number of used sentences in train = 313
Total loss for epoch 7: 509.063788
	Epoch 8....
Epoch has taken 0:00:18.768832
Number of used sentences in train = 313
Total loss for epoch 8: 511.853894
	Epoch 9....
Epoch has taken 0:00:18.436407
Number of used sentences in train = 313
Total loss for epoch 9: 504.721302
	Epoch 10....
Epoch has taken 0:00:18.473719
Number of used sentences in train = 313
Total loss for epoch 10: 505.164346
	Epoch 11....
Epoch has taken 0:00:18.398724
Number of used sentences in train = 313
Total loss for epoch 11: 505.549647
	Epoch 12....
Epoch has taken 0:00:18.448052
Number of used sentences in train = 313
Total loss for epoch 12: 508.374525
	Epoch 13....
Epoch has taken 0:00:18.400039
Number of used sentences in train = 313
Total loss for epoch 13: 503.490485
	Epoch 14....
Epoch has taken 0:00:18.415918
Number of used sentences in train = 313
Total loss for epoch 14: 504.696322
Epoch has taken 0:00:18.399467

==================================================================================================
	Training time : 0:51:40.950203
==================================================================================================
	Identification : 0.27

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 39)
  (w_embeddings): Embedding(1133, 132)
  (lstm): LSTM(171, 33, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8790.052654
validation loss after epoch 0 : 657.760713
	Epoch 1....
Epoch has taken 0:01:59.807807
Number of used sentences in train = 2074
Total loss for epoch 1: 5217.977923
validation loss after epoch 1 : 602.525760
	Epoch 2....
Epoch has taken 0:01:59.627318
Number of used sentences in train = 2074
Total loss for epoch 2: 4526.141301
validation loss after epoch 2 : 661.889112
	Epoch 3....
Epoch has taken 0:01:58.945260
Number of used sentences in train = 2074
Total loss for epoch 3: 4064.253516
validation loss after epoch 3 : 672.044878
	Epoch 4....
Epoch has taken 0:01:58.717540
Number of used sentences in train = 2074
Total loss for epoch 4: 3796.134869
validation loss after epoch 4 : 677.649464
	Epoch 5....
Epoch has taken 0:01:59.456019
Number of used sentences in train = 2074
Total loss for epoch 5: 3591.377627
validation loss after epoch 5 : 686.015333
	Epoch 6....
Epoch has taken 0:01:59.776271
Number of used sentences in train = 2074
Total loss for epoch 6: 3518.542050
validation loss after epoch 6 : 728.776778
	Epoch 7....
Epoch has taken 0:01:58.834944
Number of used sentences in train = 2074
Total loss for epoch 7: 3408.077172
validation loss after epoch 7 : 767.884276
	Epoch 8....
Epoch has taken 0:01:58.187934
Number of used sentences in train = 2074
Total loss for epoch 8: 3373.389717
validation loss after epoch 8 : 808.813879
	Epoch 9....
Epoch has taken 0:01:58.614268
Number of used sentences in train = 2074
Total loss for epoch 9: 3332.685772
validation loss after epoch 9 : 779.564330
	Epoch 10....
Epoch has taken 0:01:59.938235
Number of used sentences in train = 2074
Total loss for epoch 10: 3279.240266
validation loss after epoch 10 : 832.612761
	Epoch 11....
Epoch has taken 0:01:59.588277
Number of used sentences in train = 2074
Total loss for epoch 11: 3285.483042
validation loss after epoch 11 : 783.885689
	Epoch 12....
Epoch has taken 0:01:59.509294
Number of used sentences in train = 2074
Total loss for epoch 12: 3259.162210
validation loss after epoch 12 : 868.433861
	Epoch 13....
Epoch has taken 0:01:59.027703
Number of used sentences in train = 2074
Total loss for epoch 13: 3235.417120
validation loss after epoch 13 : 897.947525
	Epoch 14....
Epoch has taken 0:01:58.107807
Number of used sentences in train = 2074
Total loss for epoch 14: 3231.796668
validation loss after epoch 14 : 814.143539
	TransitionClassifier(
  (p_embeddings): Embedding(18, 39)
  (w_embeddings): Embedding(1133, 132)
  (lstm): LSTM(171, 33, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:59.568806
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1222.442315
	Epoch 1....
Epoch has taken 0:00:12.250789
Number of used sentences in train = 231
Total loss for epoch 1: 517.557160
	Epoch 2....
Epoch has taken 0:00:12.229668
Number of used sentences in train = 231
Total loss for epoch 2: 408.218125
	Epoch 3....
Epoch has taken 0:00:12.191465
Number of used sentences in train = 231
Total loss for epoch 3: 381.879831
	Epoch 4....
Epoch has taken 0:00:12.223953
Number of used sentences in train = 231
Total loss for epoch 4: 356.317049
	Epoch 5....
Epoch has taken 0:00:12.197875
Number of used sentences in train = 231
Total loss for epoch 5: 352.227261
	Epoch 6....
Epoch has taken 0:00:12.213124
Number of used sentences in train = 231
Total loss for epoch 6: 352.734526
	Epoch 7....
Epoch has taken 0:00:12.236791
Number of used sentences in train = 231
Total loss for epoch 7: 352.138296
	Epoch 8....
Epoch has taken 0:00:12.258996
Number of used sentences in train = 231
Total loss for epoch 8: 348.282735
	Epoch 9....
Epoch has taken 0:00:12.244553
Number of used sentences in train = 231
Total loss for epoch 9: 346.889691
	Epoch 10....
Epoch has taken 0:00:12.236953
Number of used sentences in train = 231
Total loss for epoch 10: 345.866235
	Epoch 11....
Epoch has taken 0:00:12.222898
Number of used sentences in train = 231
Total loss for epoch 11: 345.580647
	Epoch 12....
Epoch has taken 0:00:12.230071
Number of used sentences in train = 231
Total loss for epoch 12: 345.687954
	Epoch 13....
Epoch has taken 0:00:12.221370
Number of used sentences in train = 231
Total loss for epoch 13: 346.198259
	Epoch 14....
Epoch has taken 0:00:12.220233
Number of used sentences in train = 231
Total loss for epoch 14: 344.995317
Epoch has taken 0:00:12.216521

==================================================================================================
	Training time : 0:32:51.474718
==================================================================================================
	Identification : 0.17

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 39)
  (w_embeddings): Embedding(1202, 132)
  (lstm): LSTM(171, 33, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15784.864381
validation loss after epoch 0 : 1397.631858
	Epoch 1....
Epoch has taken 0:03:50.268430
Number of used sentences in train = 3226
Total loss for epoch 1: 11282.496544
validation loss after epoch 1 : 1425.122026
	Epoch 2....
Epoch has taken 0:03:49.688238
Number of used sentences in train = 3226
Total loss for epoch 2: 10249.758661
validation loss after epoch 2 : 1276.901998
	Epoch 3....
Epoch has taken 0:03:51.970004
Number of used sentences in train = 3226
Total loss for epoch 3: 9568.317712
validation loss after epoch 3 : 1322.347003
	Epoch 4....
Epoch has taken 0:03:50.199416
Number of used sentences in train = 3226
Total loss for epoch 4: 9147.622094
validation loss after epoch 4 : 1415.090858
	Epoch 5....
Epoch has taken 0:03:50.323807
Number of used sentences in train = 3226
Total loss for epoch 5: 8795.599707
validation loss after epoch 5 : 1377.095562
	Epoch 6....
Epoch has taken 0:03:49.794560
Number of used sentences in train = 3226
Total loss for epoch 6: 8548.844158
validation loss after epoch 6 : 1384.081307
	Epoch 7....
Epoch has taken 0:03:51.367146
Number of used sentences in train = 3226
Total loss for epoch 7: 8279.429105
validation loss after epoch 7 : 1456.801355
	Epoch 8....
Epoch has taken 0:03:50.291562
Number of used sentences in train = 3226
Total loss for epoch 8: 8142.850143
validation loss after epoch 8 : 1450.581827
	Epoch 9....
Epoch has taken 0:03:49.692728
Number of used sentences in train = 3226
Total loss for epoch 9: 7983.604296
validation loss after epoch 9 : 1478.777070
	Epoch 10....
Epoch has taken 0:03:53.258248
Number of used sentences in train = 3226
Total loss for epoch 10: 7777.312844
validation loss after epoch 10 : 1555.416219
	Epoch 11....
Epoch has taken 0:04:00.270548
Number of used sentences in train = 3226
Total loss for epoch 11: 7697.920921
validation loss after epoch 11 : 1563.921251
	Epoch 12....
Epoch has taken 0:03:53.430201
Number of used sentences in train = 3226
Total loss for epoch 12: 7550.542842
validation loss after epoch 12 : 1516.336466
	Epoch 13....
Epoch has taken 0:03:51.758016
Number of used sentences in train = 3226
Total loss for epoch 13: 7401.167143
validation loss after epoch 13 : 1686.319480
	Epoch 14....
Epoch has taken 0:03:51.375685
Number of used sentences in train = 3226
Total loss for epoch 14: 7370.195853
validation loss after epoch 14 : 1678.373317
	TransitionClassifier(
  (p_embeddings): Embedding(13, 39)
  (w_embeddings): Embedding(1202, 132)
  (lstm): LSTM(171, 33, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:51.510654
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2191.446694
	Epoch 1....
Epoch has taken 0:00:26.985852
Number of used sentences in train = 359
Total loss for epoch 1: 1152.985399
	Epoch 2....
Epoch has taken 0:00:27.076641
Number of used sentences in train = 359
Total loss for epoch 2: 970.191097
	Epoch 3....
Epoch has taken 0:00:26.917845
Number of used sentences in train = 359
Total loss for epoch 3: 897.521530
	Epoch 4....
Epoch has taken 0:00:27.002239
Number of used sentences in train = 359
Total loss for epoch 4: 834.402032
	Epoch 5....
Epoch has taken 0:00:27.250788
Number of used sentences in train = 359
Total loss for epoch 5: 791.303461
	Epoch 6....
Epoch has taken 0:00:27.021125
Number of used sentences in train = 359
Total loss for epoch 6: 766.312588
	Epoch 7....
Epoch has taken 0:00:26.972633
Number of used sentences in train = 359
Total loss for epoch 7: 750.062707
	Epoch 8....
Epoch has taken 0:00:27.029340
Number of used sentences in train = 359
Total loss for epoch 8: 741.268192
	Epoch 9....
Epoch has taken 0:00:27.171594
Number of used sentences in train = 359
Total loss for epoch 9: 719.294960
	Epoch 10....
Epoch has taken 0:00:26.942371
Number of used sentences in train = 359
Total loss for epoch 10: 715.632089
	Epoch 11....
Epoch has taken 0:00:23.589783
Number of used sentences in train = 359
Total loss for epoch 11: 698.265358
	Epoch 12....
Epoch has taken 0:00:23.318999
Number of used sentences in train = 359
Total loss for epoch 12: 692.632965
	Epoch 13....
Epoch has taken 0:00:23.551650
Number of used sentences in train = 359
Total loss for epoch 13: 713.361656
	Epoch 14....
Epoch has taken 0:00:23.657564
Number of used sentences in train = 359
Total loss for epoch 14: 701.590169
Epoch has taken 0:00:23.771033

==================================================================================================
	Training time : 1:04:24.109791
==================================================================================================
	Identification : 0.321

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 12, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 21, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 159, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1882, 159)
  (lstm): LSTM(176, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10067.052771
validation loss after epoch 0 : 905.290555
	Epoch 1....
Epoch has taken 0:02:46.773464
Number of used sentences in train = 2811
Total loss for epoch 1: 7345.652425
validation loss after epoch 1 : 906.347336
	Epoch 2....
Epoch has taken 0:02:47.336336
Number of used sentences in train = 2811
Total loss for epoch 2: 6503.328994
validation loss after epoch 2 : 932.303231
	Epoch 3....
Epoch has taken 0:02:47.120060
Number of used sentences in train = 2811
Total loss for epoch 3: 6003.403303
validation loss after epoch 3 : 963.948756
	Epoch 4....
Epoch has taken 0:02:47.886392
Number of used sentences in train = 2811
Total loss for epoch 4: 5653.592984
validation loss after epoch 4 : 1020.670178
	Epoch 5....
Epoch has taken 0:02:47.660985
Number of used sentences in train = 2811
Total loss for epoch 5: 5423.971576
validation loss after epoch 5 : 1061.656892
	Epoch 6....
Epoch has taken 0:02:47.192152
Number of used sentences in train = 2811
Total loss for epoch 6: 5266.981167
validation loss after epoch 6 : 1088.487682
	Epoch 7....
Epoch has taken 0:02:44.548981
Number of used sentences in train = 2811
Total loss for epoch 7: 5158.349343
validation loss after epoch 7 : 1130.469593
	Epoch 8....
Epoch has taken 0:02:47.343047
Number of used sentences in train = 2811
Total loss for epoch 8: 5032.344040
validation loss after epoch 8 : 1178.740174
	Epoch 9....
Epoch has taken 0:02:46.587636
Number of used sentences in train = 2811
Total loss for epoch 9: 4972.003758
validation loss after epoch 9 : 1172.208434
	Epoch 10....
Epoch has taken 0:02:54.719852
Number of used sentences in train = 2811
Total loss for epoch 10: 4902.777984
validation loss after epoch 10 : 1206.721317
	Epoch 11....
Epoch has taken 0:03:14.326071
Number of used sentences in train = 2811
Total loss for epoch 11: 4857.101791
validation loss after epoch 11 : 1234.101214
	Epoch 12....
Epoch has taken 0:02:45.970077
Number of used sentences in train = 2811
Total loss for epoch 12: 4812.357493
validation loss after epoch 12 : 1281.572162
	Epoch 13....
Epoch has taken 0:02:46.555321
Number of used sentences in train = 2811
Total loss for epoch 13: 4796.500809
validation loss after epoch 13 : 1248.454379
	Epoch 14....
Epoch has taken 0:02:47.334437
Number of used sentences in train = 2811
Total loss for epoch 14: 4752.941107
validation loss after epoch 14 : 1257.409998
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1882, 159)
  (lstm): LSTM(176, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:48.222133
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1187.115175
	Epoch 1....
Epoch has taken 0:00:17.861434
Number of used sentences in train = 313
Total loss for epoch 1: 736.636720
	Epoch 2....
Epoch has taken 0:00:17.971233
Number of used sentences in train = 313
Total loss for epoch 2: 598.845915
	Epoch 3....
Epoch has taken 0:00:17.889805
Number of used sentences in train = 313
Total loss for epoch 3: 552.494976
	Epoch 4....
Epoch has taken 0:00:18.036537
Number of used sentences in train = 313
Total loss for epoch 4: 541.169478
	Epoch 5....
Epoch has taken 0:00:17.886313
Number of used sentences in train = 313
Total loss for epoch 5: 532.626486
	Epoch 6....
Epoch has taken 0:00:17.778671
Number of used sentences in train = 313
Total loss for epoch 6: 529.235003
	Epoch 7....
Epoch has taken 0:00:17.849278
Number of used sentences in train = 313
Total loss for epoch 7: 528.149481
	Epoch 8....
Epoch has taken 0:00:17.898953
Number of used sentences in train = 313
Total loss for epoch 8: 527.886770
	Epoch 9....
Epoch has taken 0:00:17.987451
Number of used sentences in train = 313
Total loss for epoch 9: 520.934973
	Epoch 10....
Epoch has taken 0:00:17.947508
Number of used sentences in train = 313
Total loss for epoch 10: 522.083723
	Epoch 11....
Epoch has taken 0:00:17.795702
Number of used sentences in train = 313
Total loss for epoch 11: 522.388477
	Epoch 12....
Epoch has taken 0:00:17.965933
Number of used sentences in train = 313
Total loss for epoch 12: 517.481299
	Epoch 13....
Epoch has taken 0:00:18.099205
Number of used sentences in train = 313
Total loss for epoch 13: 516.626329
	Epoch 14....
Epoch has taken 0:00:17.967449
Number of used sentences in train = 313
Total loss for epoch 14: 515.738530
Epoch has taken 0:00:18.118729

==================================================================================================
	Training time : 0:46:49.150280
==================================================================================================
	Identification : 0.309

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1680, 159)
  (lstm): LSTM(176, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8246.745166
validation loss after epoch 0 : 810.042166
	Epoch 1....
Epoch has taken 0:01:54.984878
Number of used sentences in train = 2074
Total loss for epoch 1: 5456.756795
validation loss after epoch 1 : 900.684578
	Epoch 2....
Epoch has taken 0:01:55.050720
Number of used sentences in train = 2074
Total loss for epoch 2: 4588.375517
validation loss after epoch 2 : 930.021312
	Epoch 3....
Epoch has taken 0:01:54.608102
Number of used sentences in train = 2074
Total loss for epoch 3: 4051.767163
validation loss after epoch 3 : 1005.278378
	Epoch 4....
Epoch has taken 0:02:12.937015
Number of used sentences in train = 2074
Total loss for epoch 4: 3746.209541
validation loss after epoch 4 : 1073.374711
	Epoch 5....
Epoch has taken 0:01:55.471059
Number of used sentences in train = 2074
Total loss for epoch 5: 3565.268089
validation loss after epoch 5 : 1092.776940
	Epoch 6....
Epoch has taken 0:01:53.998792
Number of used sentences in train = 2074
Total loss for epoch 6: 3448.313634
validation loss after epoch 6 : 1118.716866
	Epoch 7....
Epoch has taken 0:01:53.896409
Number of used sentences in train = 2074
Total loss for epoch 7: 3364.831326
validation loss after epoch 7 : 1132.570219
	Epoch 8....
Epoch has taken 0:01:54.760450
Number of used sentences in train = 2074
Total loss for epoch 8: 3303.259586
validation loss after epoch 8 : 1164.108891
	Epoch 9....
Epoch has taken 0:01:50.877519
Number of used sentences in train = 2074
Total loss for epoch 9: 3269.285699
validation loss after epoch 9 : 1213.520750
	Epoch 10....
Epoch has taken 0:01:54.952858
Number of used sentences in train = 2074
Total loss for epoch 10: 3243.637362
validation loss after epoch 10 : 1217.493902
	Epoch 11....
Epoch has taken 0:01:54.055334
Number of used sentences in train = 2074
Total loss for epoch 11: 3229.875074
validation loss after epoch 11 : 1260.084804
	Epoch 12....
Epoch has taken 0:01:55.391990
Number of used sentences in train = 2074
Total loss for epoch 12: 3220.174444
validation loss after epoch 12 : 1266.148183
	Epoch 13....
Epoch has taken 0:01:55.372549
Number of used sentences in train = 2074
Total loss for epoch 13: 3201.664008
validation loss after epoch 13 : 1283.336505
	Epoch 14....
Epoch has taken 0:01:54.910383
Number of used sentences in train = 2074
Total loss for epoch 14: 3194.623866
validation loss after epoch 14 : 1292.619705
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1680, 159)
  (lstm): LSTM(176, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:13.799401
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1126.051700
	Epoch 1....
Epoch has taken 0:00:11.811504
Number of used sentences in train = 231
Total loss for epoch 1: 591.516202
	Epoch 2....
Epoch has taken 0:00:11.855426
Number of used sentences in train = 231
Total loss for epoch 2: 441.596427
	Epoch 3....
Epoch has taken 0:00:11.639604
Number of used sentences in train = 231
Total loss for epoch 3: 394.612191
	Epoch 4....
Epoch has taken 0:00:11.921127
Number of used sentences in train = 231
Total loss for epoch 4: 373.037485
	Epoch 5....
Epoch has taken 0:00:11.919118
Number of used sentences in train = 231
Total loss for epoch 5: 363.699520
	Epoch 6....
Epoch has taken 0:00:11.958286
Number of used sentences in train = 231
Total loss for epoch 6: 358.617902
	Epoch 7....
Epoch has taken 0:00:11.742484
Number of used sentences in train = 231
Total loss for epoch 7: 354.272388
	Epoch 8....
Epoch has taken 0:00:12.052718
Number of used sentences in train = 231
Total loss for epoch 8: 351.797420
	Epoch 9....
Epoch has taken 0:00:11.801475
Number of used sentences in train = 231
Total loss for epoch 9: 350.324069
	Epoch 10....
Epoch has taken 0:00:11.825081
Number of used sentences in train = 231
Total loss for epoch 10: 348.869760
	Epoch 11....
Epoch has taken 0:00:11.830554
Number of used sentences in train = 231
Total loss for epoch 11: 347.829419
	Epoch 12....
Epoch has taken 0:00:11.996311
Number of used sentences in train = 231
Total loss for epoch 12: 347.261278
	Epoch 13....
Epoch has taken 0:00:11.575342
Number of used sentences in train = 231
Total loss for epoch 13: 346.966185
	Epoch 14....
Epoch has taken 0:00:11.837385
Number of used sentences in train = 231
Total loss for epoch 14: 346.585358
Epoch has taken 0:00:11.761589

==================================================================================================
	Training time : 0:32:12.940717
==================================================================================================
	Identification : 0.339

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(3369, 159)
  (lstm): LSTM(176, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 13321.947399
validation loss after epoch 0 : 1164.792482
	Epoch 1....
Epoch has taken 0:03:47.461703
Number of used sentences in train = 3226
Total loss for epoch 1: 9394.002229
validation loss after epoch 1 : 1143.835307
	Epoch 2....
Epoch has taken 0:03:48.004574
Number of used sentences in train = 3226
Total loss for epoch 2: 8402.427764
validation loss after epoch 2 : 1181.334276
	Epoch 3....
Epoch has taken 0:03:48.477478
Number of used sentences in train = 3226
Total loss for epoch 3: 7815.825331
validation loss after epoch 3 : 1216.839235
	Epoch 4....
Epoch has taken 0:04:19.851151
Number of used sentences in train = 3226
Total loss for epoch 4: 7444.620692
validation loss after epoch 4 : 1314.840884
	Epoch 5....
Epoch has taken 0:03:35.558299
Number of used sentences in train = 3226
Total loss for epoch 5: 7184.920473
validation loss after epoch 5 : 1346.350166
	Epoch 6....
Epoch has taken 0:03:47.764710
Number of used sentences in train = 3226
Total loss for epoch 6: 6999.079802
validation loss after epoch 6 : 1453.651774
	Epoch 7....
Epoch has taken 0:03:48.971866
Number of used sentences in train = 3226
Total loss for epoch 7: 6840.142284
validation loss after epoch 7 : 1457.216427
	Epoch 8....
Epoch has taken 0:04:11.789316
Number of used sentences in train = 3226
Total loss for epoch 8: 6746.823957
validation loss after epoch 8 : 1501.233600
	Epoch 9....
Epoch has taken 0:03:49.989107
Number of used sentences in train = 3226
Total loss for epoch 9: 6645.545082
validation loss after epoch 9 : 1537.008223
	Epoch 10....
Epoch has taken 0:03:46.457742
Number of used sentences in train = 3226
Total loss for epoch 10: 6565.011498
validation loss after epoch 10 : 1525.324945
	Epoch 11....
Epoch has taken 0:03:46.908510
Number of used sentences in train = 3226
Total loss for epoch 11: 6517.450844
validation loss after epoch 11 : 1545.740307
	Epoch 12....
Epoch has taken 0:03:37.955013
Number of used sentences in train = 3226
Total loss for epoch 12: 6471.210925
validation loss after epoch 12 : 1651.059376
	Epoch 13....
Epoch has taken 0:03:37.356858
Number of used sentences in train = 3226
Total loss for epoch 13: 6424.430060
validation loss after epoch 13 : 1647.682883
	Epoch 14....
Epoch has taken 0:03:40.201153
Number of used sentences in train = 3226
Total loss for epoch 14: 6453.282158
validation loss after epoch 14 : 1656.362739
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(3369, 159)
  (lstm): LSTM(176, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:38.804803
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1621.590056
	Epoch 1....
Epoch has taken 0:00:21.493026
Number of used sentences in train = 359
Total loss for epoch 1: 992.539480
	Epoch 2....
Epoch has taken 0:00:22.429448
Number of used sentences in train = 359
Total loss for epoch 2: 832.917223
	Epoch 3....
Epoch has taken 0:00:21.384693
Number of used sentences in train = 359
Total loss for epoch 3: 760.227388
	Epoch 4....
Epoch has taken 0:00:21.401785
Number of used sentences in train = 359
Total loss for epoch 4: 723.064876
	Epoch 5....
Epoch has taken 0:00:21.354884
Number of used sentences in train = 359
Total loss for epoch 5: 706.424255
	Epoch 6....
Epoch has taken 0:00:21.362792
Number of used sentences in train = 359
Total loss for epoch 6: 697.834924
	Epoch 7....
Epoch has taken 0:00:21.363910
Number of used sentences in train = 359
Total loss for epoch 7: 691.040345
	Epoch 8....
Epoch has taken 0:00:21.584941
Number of used sentences in train = 359
Total loss for epoch 8: 682.863878
	Epoch 9....
Epoch has taken 0:00:22.313544
Number of used sentences in train = 359
Total loss for epoch 9: 679.698188
	Epoch 10....
Epoch has taken 0:00:21.346776
Number of used sentences in train = 359
Total loss for epoch 10: 677.377031
	Epoch 11....
Epoch has taken 0:00:21.336302
Number of used sentences in train = 359
Total loss for epoch 11: 676.172945
	Epoch 12....
Epoch has taken 0:00:21.573575
Number of used sentences in train = 359
Total loss for epoch 12: 675.469245
	Epoch 13....
Epoch has taken 0:00:21.347593
Number of used sentences in train = 359
Total loss for epoch 13: 673.745410
	Epoch 14....
Epoch has taken 0:00:21.604058
Number of used sentences in train = 359
Total loss for epoch 14: 673.946468
Epoch has taken 0:00:22.369704

==================================================================================================
	Training time : 1:02:30.499155
==================================================================================================
	Identification : 0.1

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 18, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 44, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 130, 'lstmDropout': 0.39, 'denseActivation': 'tanh', 'wordDim': 50, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5858, 50)
  (lstm): LSTM(94, 130, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=2080, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 25283.593550
validation loss after epoch 0 : 2697.360085
	Epoch 1....
Epoch has taken 0:03:02.546708
Number of used sentences in train = 2811
Total loss for epoch 1: 24590.953810
validation loss after epoch 1 : 2701.844852
	Epoch 2....
Epoch has taken 0:03:01.933282
Number of used sentences in train = 2811
Total loss for epoch 2: 24871.903340
validation loss after epoch 2 : 2798.422356
	Epoch 3....
Epoch has taken 0:03:03.505159
Number of used sentences in train = 2811
Total loss for epoch 3: 19116.502787
validation loss after epoch 3 : 1437.353950
	Epoch 4....
Epoch has taken 0:03:03.004970
Number of used sentences in train = 2811
Total loss for epoch 4: 12201.279563
validation loss after epoch 4 : 1200.069531
	Epoch 5....
Epoch has taken 0:03:03.169179
Number of used sentences in train = 2811
Total loss for epoch 5: 10133.057303
validation loss after epoch 5 : 1154.490166
	Epoch 6....
Epoch has taken 0:03:01.761519
Number of used sentences in train = 2811
Total loss for epoch 6: 9195.709223
validation loss after epoch 6 : 1120.028009
	Epoch 7....
Epoch has taken 0:03:02.811066
Number of used sentences in train = 2811
Total loss for epoch 7: 8642.429536
validation loss after epoch 7 : 1156.609406
	Epoch 8....
Epoch has taken 0:03:01.553551
Number of used sentences in train = 2811
Total loss for epoch 8: 8099.911778
validation loss after epoch 8 : 1097.731344
	Epoch 9....
Epoch has taken 0:03:02.627919
Number of used sentences in train = 2811
Total loss for epoch 9: 7545.180382
validation loss after epoch 9 : 1110.753024
	Epoch 10....
Epoch has taken 0:03:02.825946
Number of used sentences in train = 2811
Total loss for epoch 10: 7170.186178
validation loss after epoch 10 : 1143.921619
	Epoch 11....
Epoch has taken 0:02:57.681765
Number of used sentences in train = 2811
Total loss for epoch 11: 6667.275738
validation loss after epoch 11 : 1163.034227
	Epoch 12....
Epoch has taken 0:02:55.811594
Number of used sentences in train = 2811
Total loss for epoch 12: 6330.547862
validation loss after epoch 12 : 1190.193017
	Epoch 13....
Epoch has taken 0:03:01.209825
Number of used sentences in train = 2811
Total loss for epoch 13: 6125.194693
validation loss after epoch 13 : 1177.245374
	Epoch 14....
Epoch has taken 0:03:01.468072
Number of used sentences in train = 2811
Total loss for epoch 14: 5944.802194
validation loss after epoch 14 : 1137.823903
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5858, 50)
  (lstm): LSTM(94, 130, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=2080, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:02.920356
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1632.714500
	Epoch 1....
Epoch has taken 0:00:19.500435
Number of used sentences in train = 313
Total loss for epoch 1: 1178.092429
	Epoch 2....
Epoch has taken 0:00:19.587203
Number of used sentences in train = 313
Total loss for epoch 2: 950.712046
	Epoch 3....
Epoch has taken 0:00:19.431281
Number of used sentences in train = 313
Total loss for epoch 3: 857.773207
	Epoch 4....
Epoch has taken 0:00:19.365772
Number of used sentences in train = 313
Total loss for epoch 4: 768.849390
	Epoch 5....
Epoch has taken 0:00:19.688356
Number of used sentences in train = 313
Total loss for epoch 5: 706.923734
	Epoch 6....
Epoch has taken 0:00:19.753034
Number of used sentences in train = 313
Total loss for epoch 6: 662.502815
	Epoch 7....
Epoch has taken 0:00:19.619171
Number of used sentences in train = 313
Total loss for epoch 7: 632.430064
	Epoch 8....
Epoch has taken 0:00:19.425279
Number of used sentences in train = 313
Total loss for epoch 8: 621.725022
	Epoch 9....
Epoch has taken 0:00:19.671888
Number of used sentences in train = 313
Total loss for epoch 9: 596.252841
	Epoch 10....
Epoch has taken 0:00:19.526708
Number of used sentences in train = 313
Total loss for epoch 10: 589.635862
	Epoch 11....
Epoch has taken 0:00:19.589640
Number of used sentences in train = 313
Total loss for epoch 11: 569.475195
	Epoch 12....
Epoch has taken 0:00:19.855019
Number of used sentences in train = 313
Total loss for epoch 12: 563.170611
	Epoch 13....
Epoch has taken 0:00:19.673447
Number of used sentences in train = 313
Total loss for epoch 13: 556.769341
	Epoch 14....
Epoch has taken 0:00:19.518076
Number of used sentences in train = 313
Total loss for epoch 14: 554.006305
Epoch has taken 0:00:19.662005

==================================================================================================
	Training time : 0:50:19.211242
==================================================================================================
	Identification : 0.445

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5624, 50)
  (lstm): LSTM(94, 130, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=2080, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 21520.329114
validation loss after epoch 0 : 1668.681344
	Epoch 1....
Epoch has taken 0:02:04.112096
Number of used sentences in train = 2074
Total loss for epoch 1: 10590.092209
validation loss after epoch 1 : 978.815124
	Epoch 2....
Epoch has taken 0:02:05.051417
Number of used sentences in train = 2074
Total loss for epoch 2: 7731.225250
validation loss after epoch 2 : 954.085076
	Epoch 3....
Epoch has taken 0:02:05.573290
Number of used sentences in train = 2074
Total loss for epoch 3: 6484.920015
validation loss after epoch 3 : 869.623065
	Epoch 4....
Epoch has taken 0:02:05.258352
Number of used sentences in train = 2074
Total loss for epoch 4: 5796.668046
validation loss after epoch 4 : 1070.094314
	Epoch 5....
Epoch has taken 0:02:07.350694
Number of used sentences in train = 2074
Total loss for epoch 5: 5250.040147
validation loss after epoch 5 : 956.056115
	Epoch 6....
Epoch has taken 0:02:06.635604
Number of used sentences in train = 2074
Total loss for epoch 6: 4728.624784
validation loss after epoch 6 : 1006.103843
	Epoch 7....
Epoch has taken 0:02:05.022034
Number of used sentences in train = 2074
Total loss for epoch 7: 4464.847899
validation loss after epoch 7 : 939.047688
	Epoch 8....
Epoch has taken 0:02:05.195324
Number of used sentences in train = 2074
Total loss for epoch 8: 4230.946796
validation loss after epoch 8 : 1118.427880
	Epoch 9....
Epoch has taken 0:02:05.572432
Number of used sentences in train = 2074
Total loss for epoch 9: 3985.008953
validation loss after epoch 9 : 1118.168482
	Epoch 10....
Epoch has taken 0:02:05.515435
Number of used sentences in train = 2074
Total loss for epoch 10: 3867.532383
validation loss after epoch 10 : 1393.015626
	Epoch 11....
Epoch has taken 0:02:12.498317
Number of used sentences in train = 2074
Total loss for epoch 11: 3784.823105
validation loss after epoch 11 : 1298.127295
	Epoch 12....
Epoch has taken 0:02:06.032513
Number of used sentences in train = 2074
Total loss for epoch 12: 3689.343326
validation loss after epoch 12 : 1347.870160
	Epoch 13....
Epoch has taken 0:02:04.572180
Number of used sentences in train = 2074
Total loss for epoch 13: 3564.528057
validation loss after epoch 13 : 1367.695789
	Epoch 14....
Epoch has taken 0:02:05.232805
Number of used sentences in train = 2074
Total loss for epoch 14: 3502.174264
validation loss after epoch 14 : 1133.235941
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5624, 50)
  (lstm): LSTM(94, 130, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=2080, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:05.201398
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1544.927182
	Epoch 1....
Epoch has taken 0:00:12.770751
Number of used sentences in train = 231
Total loss for epoch 1: 959.629116
	Epoch 2....
Epoch has taken 0:00:14.881211
Number of used sentences in train = 231
Total loss for epoch 2: 727.500046
	Epoch 3....
Epoch has taken 0:00:14.706463
Number of used sentences in train = 231
Total loss for epoch 3: 582.872209
	Epoch 4....
Epoch has taken 0:00:14.782570
Number of used sentences in train = 231
Total loss for epoch 4: 527.722118
	Epoch 5....
Epoch has taken 0:00:14.742631
Number of used sentences in train = 231
Total loss for epoch 5: 479.775767
	Epoch 6....
Epoch has taken 0:00:14.777376
Number of used sentences in train = 231
Total loss for epoch 6: 450.148984
	Epoch 7....
Epoch has taken 0:00:14.933494
Number of used sentences in train = 231
Total loss for epoch 7: 410.248269
	Epoch 8....
Epoch has taken 0:00:12.732980
Number of used sentences in train = 231
Total loss for epoch 8: 386.862046
	Epoch 9....
Epoch has taken 0:00:12.979927
Number of used sentences in train = 231
Total loss for epoch 9: 401.945259
	Epoch 10....
Epoch has taken 0:00:13.056860
Number of used sentences in train = 231
Total loss for epoch 10: 380.106834
	Epoch 11....
Epoch has taken 0:00:12.888138
Number of used sentences in train = 231
Total loss for epoch 11: 371.740159
	Epoch 12....
Epoch has taken 0:00:13.046381
Number of used sentences in train = 231
Total loss for epoch 12: 366.805165
	Epoch 13....
Epoch has taken 0:00:12.929407
Number of used sentences in train = 231
Total loss for epoch 13: 361.621087
	Epoch 14....
Epoch has taken 0:00:13.017051
Number of used sentences in train = 231
Total loss for epoch 14: 357.530083
Epoch has taken 0:00:12.677812

==================================================================================================
	Training time : 0:34:54.094344
==================================================================================================
	Identification : 0.372

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 44)
  (w_embeddings): Embedding(6819, 50)
  (lstm): LSTM(94, 130, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=2080, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 37841.199349
validation loss after epoch 0 : 3967.565491
	Epoch 1....
Epoch has taken 0:04:03.551243
Number of used sentences in train = 3226
Total loss for epoch 1: 37051.325061
validation loss after epoch 1 : 3969.016439
	Epoch 2....
Epoch has taken 0:04:03.547898
Number of used sentences in train = 3226
Total loss for epoch 2: 34349.962300
validation loss after epoch 2 : 2241.023198
	Epoch 3....
Epoch has taken 0:04:04.300690
Number of used sentences in train = 3226
Total loss for epoch 3: 16773.007182
validation loss after epoch 3 : 1668.598168
	Epoch 4....
Epoch has taken 0:04:22.955953
Number of used sentences in train = 3226
Total loss for epoch 4: 13966.669226
validation loss after epoch 4 : 1563.700171
	Epoch 5....
Epoch has taken 0:04:05.830532
Number of used sentences in train = 3226
Total loss for epoch 5: 12557.478684
validation loss after epoch 5 : 1498.014501
	Epoch 6....
Epoch has taken 0:04:05.030694
Number of used sentences in train = 3226
Total loss for epoch 6: 11366.769572
validation loss after epoch 6 : 1525.517644
	Epoch 7....
Epoch has taken 0:04:04.278713
Number of used sentences in train = 3226
Total loss for epoch 7: 10396.454213
validation loss after epoch 7 : 1585.344149
	Epoch 8....
Epoch has taken 0:04:04.473558
Number of used sentences in train = 3226
Total loss for epoch 8: 9778.256479
validation loss after epoch 8 : 1688.022929
	Epoch 9....
Epoch has taken 0:04:03.882657
Number of used sentences in train = 3226
Total loss for epoch 9: 9030.482560
validation loss after epoch 9 : 1702.141910
	Epoch 10....
Epoch has taken 0:04:05.676695
Number of used sentences in train = 3226
Total loss for epoch 10: 8651.767262
validation loss after epoch 10 : 1687.348780
	Epoch 11....
Epoch has taken 0:04:03.547015
Number of used sentences in train = 3226
Total loss for epoch 11: 8198.043439
validation loss after epoch 11 : 1801.747877
	Epoch 12....
Epoch has taken 0:04:04.561287
Number of used sentences in train = 3226
Total loss for epoch 12: 7838.028321
validation loss after epoch 12 : 1879.832522
	Epoch 13....
Epoch has taken 0:04:05.427544
Number of used sentences in train = 3226
Total loss for epoch 13: 7536.859834
validation loss after epoch 13 : 1973.976921
	Epoch 14....
Epoch has taken 0:04:04.475116
Number of used sentences in train = 3226
Total loss for epoch 14: 7365.945084
validation loss after epoch 14 : 2026.733062
	TransitionClassifier(
  (p_embeddings): Embedding(13, 44)
  (w_embeddings): Embedding(6819, 50)
  (lstm): LSTM(94, 130, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=2080, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:03.469731
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2601.735975
	Epoch 1....
Epoch has taken 0:00:24.276855
Number of used sentences in train = 359
Total loss for epoch 1: 1530.849823
	Epoch 2....
Epoch has taken 0:00:24.336867
Number of used sentences in train = 359
Total loss for epoch 2: 1320.440468
	Epoch 3....
Epoch has taken 0:00:24.111464
Number of used sentences in train = 359
Total loss for epoch 3: 1178.778976
	Epoch 4....
Epoch has taken 0:00:24.322761
Number of used sentences in train = 359
Total loss for epoch 4: 1000.621745
	Epoch 5....
Epoch has taken 0:00:24.281332
Number of used sentences in train = 359
Total loss for epoch 5: 926.446226
	Epoch 6....
Epoch has taken 0:00:24.189870
Number of used sentences in train = 359
Total loss for epoch 6: 905.480547
	Epoch 7....
Epoch has taken 0:00:24.118275
Number of used sentences in train = 359
Total loss for epoch 7: 822.104209
	Epoch 8....
Epoch has taken 0:00:24.303660
Number of used sentences in train = 359
Total loss for epoch 8: 779.263891
	Epoch 9....
Epoch has taken 0:00:24.344506
Number of used sentences in train = 359
Total loss for epoch 9: 745.572131
	Epoch 10....
Epoch has taken 0:00:24.224467
Number of used sentences in train = 359
Total loss for epoch 10: 749.830032
	Epoch 11....
Epoch has taken 0:00:24.293410
Number of used sentences in train = 359
Total loss for epoch 11: 719.330076
	Epoch 12....
Epoch has taken 0:00:24.409407
Number of used sentences in train = 359
Total loss for epoch 12: 733.419581
	Epoch 13....
Epoch has taken 0:00:24.316674
Number of used sentences in train = 359
Total loss for epoch 13: 720.321373
	Epoch 14....
Epoch has taken 0:00:24.308541
Number of used sentences in train = 359
Total loss for epoch 14: 705.867912
Epoch has taken 0:00:24.174780

==================================================================================================
	Training time : 1:07:29.688127
==================================================================================================
	Identification : 0.027

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 88, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 44, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 147, 'lstmDropout': 0.29, 'denseActivation': 'tanh', 'wordDim': 125, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5889, 125)
  (lstm): LSTM(169, 147, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 20433.148684
validation loss after epoch 0 : 1643.361215
	Epoch 1....
Epoch has taken 0:02:50.523103
Number of used sentences in train = 2811
Total loss for epoch 1: 13450.885321
validation loss after epoch 1 : 1344.600687
	Epoch 2....
Epoch has taken 0:02:50.766438
Number of used sentences in train = 2811
Total loss for epoch 2: 11003.571790
validation loss after epoch 2 : 1203.718067
	Epoch 3....
Epoch has taken 0:02:51.109798
Number of used sentences in train = 2811
Total loss for epoch 3: 9496.047258
validation loss after epoch 3 : 1196.608386
	Epoch 4....
Epoch has taken 0:02:50.962646
Number of used sentences in train = 2811
Total loss for epoch 4: 8322.427066
validation loss after epoch 4 : 1180.430929
	Epoch 5....
Epoch has taken 0:02:52.088252
Number of used sentences in train = 2811
Total loss for epoch 5: 7523.660493
validation loss after epoch 5 : 1162.218647
	Epoch 6....
Epoch has taken 0:02:52.096978
Number of used sentences in train = 2811
Total loss for epoch 6: 6874.153426
validation loss after epoch 6 : 1256.842833
	Epoch 7....
Epoch has taken 0:02:50.306643
Number of used sentences in train = 2811
Total loss for epoch 7: 6448.180799
validation loss after epoch 7 : 1168.291350
	Epoch 8....
Epoch has taken 0:02:51.193199
Number of used sentences in train = 2811
Total loss for epoch 8: 6004.566289
validation loss after epoch 8 : 1200.325297
	Epoch 9....
Epoch has taken 0:02:51.161959
Number of used sentences in train = 2811
Total loss for epoch 9: 5769.134767
validation loss after epoch 9 : 1207.813250
	Epoch 10....
Epoch has taken 0:02:52.003712
Number of used sentences in train = 2811
Total loss for epoch 10: 5652.940486
validation loss after epoch 10 : 1267.496399
	Epoch 11....
Epoch has taken 0:02:50.224873
Number of used sentences in train = 2811
Total loss for epoch 11: 5565.250639
validation loss after epoch 11 : 1314.366298
	Epoch 12....
Epoch has taken 0:02:50.767671
Number of used sentences in train = 2811
Total loss for epoch 12: 5333.346527
validation loss after epoch 12 : 1318.251694
	Epoch 13....
Epoch has taken 0:02:51.297709
Number of used sentences in train = 2811
Total loss for epoch 13: 5195.537233
validation loss after epoch 13 : 1395.852758
	Epoch 14....
Epoch has taken 0:03:18.412314
Number of used sentences in train = 2811
Total loss for epoch 14: 5126.533789
validation loss after epoch 14 : 1390.480673
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5889, 125)
  (lstm): LSTM(169, 147, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.021455
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1960.242071
	Epoch 1....
Epoch has taken 0:00:18.056379
Number of used sentences in train = 313
Total loss for epoch 1: 1065.449717
	Epoch 2....
Epoch has taken 0:00:18.062062
Number of used sentences in train = 313
Total loss for epoch 2: 944.662019
	Epoch 3....
Epoch has taken 0:00:18.078960
Number of used sentences in train = 313
Total loss for epoch 3: 788.013596
	Epoch 4....
Epoch has taken 0:00:18.064342
Number of used sentences in train = 313
Total loss for epoch 4: 703.080619
	Epoch 5....
Epoch has taken 0:00:18.085357
Number of used sentences in train = 313
Total loss for epoch 5: 675.536961
	Epoch 6....
Epoch has taken 0:00:18.051604
Number of used sentences in train = 313
Total loss for epoch 6: 631.808254
	Epoch 7....
Epoch has taken 0:00:18.093193
Number of used sentences in train = 313
Total loss for epoch 7: 592.159740
	Epoch 8....
Epoch has taken 0:00:18.052110
Number of used sentences in train = 313
Total loss for epoch 8: 572.147665
	Epoch 9....
Epoch has taken 0:00:18.082163
Number of used sentences in train = 313
Total loss for epoch 9: 570.564800
	Epoch 10....
Epoch has taken 0:00:18.076195
Number of used sentences in train = 313
Total loss for epoch 10: 548.370934
	Epoch 11....
Epoch has taken 0:00:18.105790
Number of used sentences in train = 313
Total loss for epoch 11: 527.921795
	Epoch 12....
Epoch has taken 0:00:18.068276
Number of used sentences in train = 313
Total loss for epoch 12: 519.307331
	Epoch 13....
Epoch has taken 0:00:18.044576
Number of used sentences in train = 313
Total loss for epoch 13: 514.949610
	Epoch 14....
Epoch has taken 0:00:18.054591
Number of used sentences in train = 313
Total loss for epoch 14: 510.316427
Epoch has taken 0:00:18.082215

==================================================================================================
	Training time : 0:47:46.526031
==================================================================================================
	Identification : 0.409

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5694, 125)
  (lstm): LSTM(169, 147, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 22595.837449
validation loss after epoch 0 : 2356.229448
	Epoch 1....
Epoch has taken 0:01:57.079745
Number of used sentences in train = 2074
Total loss for epoch 1: 12822.283625
validation loss after epoch 1 : 974.022285
	Epoch 2....
Epoch has taken 0:01:58.647877
Number of used sentences in train = 2074
Total loss for epoch 2: 7012.980596
validation loss after epoch 2 : 1077.315331
	Epoch 3....
Epoch has taken 0:02:09.694701
Number of used sentences in train = 2074
Total loss for epoch 3: 5605.625365
validation loss after epoch 3 : 905.616823
	Epoch 4....
Epoch has taken 0:02:08.168362
Number of used sentences in train = 2074
Total loss for epoch 4: 4688.667464
validation loss after epoch 4 : 891.282847
	Epoch 5....
Epoch has taken 0:01:58.629565
Number of used sentences in train = 2074
Total loss for epoch 5: 4112.883421
validation loss after epoch 5 : 1087.721311
	Epoch 6....
Epoch has taken 0:02:05.037387
Number of used sentences in train = 2074
Total loss for epoch 6: 3818.117140
validation loss after epoch 6 : 1074.264777
	Epoch 7....
Epoch has taken 0:02:03.407826
Number of used sentences in train = 2074
Total loss for epoch 7: 3622.673426
validation loss after epoch 7 : 1197.604165
	Epoch 8....
Epoch has taken 0:02:07.566643
Number of used sentences in train = 2074
Total loss for epoch 8: 3527.461761
validation loss after epoch 8 : 1302.121957
	Epoch 9....
Epoch has taken 0:02:08.161484
Number of used sentences in train = 2074
Total loss for epoch 9: 3414.069939
validation loss after epoch 9 : 1408.944302
	Epoch 10....
Epoch has taken 0:01:57.625947
Number of used sentences in train = 2074
Total loss for epoch 10: 3334.449191
validation loss after epoch 10 : 1352.988187
	Epoch 11....
Epoch has taken 0:01:57.126436
Number of used sentences in train = 2074
Total loss for epoch 11: 3306.057354
validation loss after epoch 11 : 1410.183073
	Epoch 12....
Epoch has taken 0:02:03.516216
Number of used sentences in train = 2074
Total loss for epoch 12: 3235.768137
validation loss after epoch 12 : 1453.178183
	Epoch 13....
Epoch has taken 0:01:57.700555
Number of used sentences in train = 2074
Total loss for epoch 13: 3248.787533
validation loss after epoch 13 : 1387.432871
	Epoch 14....
Epoch has taken 0:01:57.326777
Number of used sentences in train = 2074
Total loss for epoch 14: 3225.831275
validation loss after epoch 14 : 1574.987057
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5694, 125)
  (lstm): LSTM(169, 147, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.647105
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1858.509212
	Epoch 1....
Epoch has taken 0:00:11.931247
Number of used sentences in train = 231
Total loss for epoch 1: 841.677602
	Epoch 2....
Epoch has taken 0:00:11.918879
Number of used sentences in train = 231
Total loss for epoch 2: 542.064775
	Epoch 3....
Epoch has taken 0:00:11.920927
Number of used sentences in train = 231
Total loss for epoch 3: 447.752796
	Epoch 4....
Epoch has taken 0:00:11.897969
Number of used sentences in train = 231
Total loss for epoch 4: 396.996729
	Epoch 5....
Epoch has taken 0:00:11.924749
Number of used sentences in train = 231
Total loss for epoch 5: 374.067187
	Epoch 6....
Epoch has taken 0:00:11.916591
Number of used sentences in train = 231
Total loss for epoch 6: 360.257611
	Epoch 7....
Epoch has taken 0:00:11.918507
Number of used sentences in train = 231
Total loss for epoch 7: 355.804121
	Epoch 8....
Epoch has taken 0:00:11.929918
Number of used sentences in train = 231
Total loss for epoch 8: 349.619427
	Epoch 9....
Epoch has taken 0:00:11.911810
Number of used sentences in train = 231
Total loss for epoch 9: 347.333125
	Epoch 10....
Epoch has taken 0:00:11.925687
Number of used sentences in train = 231
Total loss for epoch 10: 351.014881
	Epoch 11....
Epoch has taken 0:00:11.930188
Number of used sentences in train = 231
Total loss for epoch 11: 349.364803
	Epoch 12....
Epoch has taken 0:00:11.938521
Number of used sentences in train = 231
Total loss for epoch 12: 348.062344
	Epoch 13....
Epoch has taken 0:00:11.926253
Number of used sentences in train = 231
Total loss for epoch 13: 346.121471
	Epoch 14....
Epoch has taken 0:00:11.948864
Number of used sentences in train = 231
Total loss for epoch 14: 345.593915
Epoch has taken 0:00:11.941417

==================================================================================================
	Training time : 0:33:25.574120
==================================================================================================
	Identification : 0.216

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 44)
  (w_embeddings): Embedding(6917, 125)
  (lstm): LSTM(169, 147, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 38511.606166
validation loss after epoch 0 : 3896.502282
	Epoch 1....
Epoch has taken 0:03:48.082766
Number of used sentences in train = 3226
Total loss for epoch 1: 31269.929374
validation loss after epoch 1 : 1895.516291
	Epoch 2....
Epoch has taken 0:03:48.109822
Number of used sentences in train = 3226
Total loss for epoch 2: 15340.611521
validation loss after epoch 2 : 1564.520725
	Epoch 3....
Epoch has taken 0:03:58.558491
Number of used sentences in train = 3226
Total loss for epoch 3: 12809.564629
validation loss after epoch 3 : 1674.674436
	Epoch 4....
Epoch has taken 0:04:11.833753
Number of used sentences in train = 3226
Total loss for epoch 4: 11349.013450
validation loss after epoch 4 : 1607.472242
	Epoch 5....
Epoch has taken 0:03:48.141706
Number of used sentences in train = 3226
Total loss for epoch 5: 9868.613098
validation loss after epoch 5 : 1537.909862
	Epoch 6....
Epoch has taken 0:03:46.990718
Number of used sentences in train = 3226
Total loss for epoch 6: 8779.139507
validation loss after epoch 6 : 1696.371263
	Epoch 7....
Epoch has taken 0:04:12.715986
Number of used sentences in train = 3226
Total loss for epoch 7: 7927.050663
validation loss after epoch 7 : 1845.712626
	Epoch 8....
Epoch has taken 0:04:14.208587
Number of used sentences in train = 3226
Total loss for epoch 8: 7372.931417
validation loss after epoch 8 : 2031.790679
	Epoch 9....
Epoch has taken 0:04:12.146790
Number of used sentences in train = 3226
Total loss for epoch 9: 7131.813831
validation loss after epoch 9 : 1971.668889
	Epoch 10....
Epoch has taken 0:04:12.828776
Number of used sentences in train = 3226
Total loss for epoch 10: 6868.215048
validation loss after epoch 10 : 2141.278754
	Epoch 11....
Epoch has taken 0:04:14.018642
Number of used sentences in train = 3226
Total loss for epoch 11: 6696.338058
validation loss after epoch 11 : 2258.789940
	Epoch 12....
Epoch has taken 0:04:02.321183
Number of used sentences in train = 3226
Total loss for epoch 12: 6572.333741
validation loss after epoch 12 : 2184.315042
	Epoch 13....
Epoch has taken 0:03:46.696555
Number of used sentences in train = 3226
Total loss for epoch 13: 6450.382396
validation loss after epoch 13 : 2173.694193
	Epoch 14....
Epoch has taken 0:03:47.615663
Number of used sentences in train = 3226
Total loss for epoch 14: 6417.834868
validation loss after epoch 14 : 2342.220374
	TransitionClassifier(
  (p_embeddings): Embedding(13, 44)
  (w_embeddings): Embedding(6917, 125)
  (lstm): LSTM(169, 147, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.156220
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 3244.163744
	Epoch 1....
Epoch has taken 0:00:22.274584
Number of used sentences in train = 359
Total loss for epoch 1: 1463.318681
	Epoch 2....
Epoch has taken 0:00:22.237905
Number of used sentences in train = 359
Total loss for epoch 2: 1157.423071
	Epoch 3....
Epoch has taken 0:00:22.290797
Number of used sentences in train = 359
Total loss for epoch 3: 986.255259
	Epoch 4....
Epoch has taken 0:00:22.290024
Number of used sentences in train = 359
Total loss for epoch 4: 858.423388
	Epoch 5....
Epoch has taken 0:00:22.263042
Number of used sentences in train = 359
Total loss for epoch 5: 781.738190
	Epoch 6....
Epoch has taken 0:00:22.281298
Number of used sentences in train = 359
Total loss for epoch 6: 752.614497
	Epoch 7....
Epoch has taken 0:00:22.294375
Number of used sentences in train = 359
Total loss for epoch 7: 702.389611
	Epoch 8....
Epoch has taken 0:00:22.308236
Number of used sentences in train = 359
Total loss for epoch 8: 691.051796
	Epoch 9....
Epoch has taken 0:00:22.296119
Number of used sentences in train = 359
Total loss for epoch 9: 693.453008
	Epoch 10....
Epoch has taken 0:00:22.275892
Number of used sentences in train = 359
Total loss for epoch 10: 677.554765
	Epoch 11....
Epoch has taken 0:00:22.256126
Number of used sentences in train = 359
Total loss for epoch 11: 672.217763
	Epoch 12....
Epoch has taken 0:00:22.277717
Number of used sentences in train = 359
Total loss for epoch 12: 672.197256
	Epoch 13....
Epoch has taken 0:00:22.291222
Number of used sentences in train = 359
Total loss for epoch 13: 671.428295
	Epoch 14....
Epoch has taken 0:00:22.281708
Number of used sentences in train = 359
Total loss for epoch 14: 673.275806
Epoch has taken 0:00:22.264297

==================================================================================================
	Training time : 1:05:27.276717
==================================================================================================
	Identification : 0.448

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 33, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 16, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 114, 'lstmDropout': 0.17, 'denseActivation': 'tanh', 'wordDim': 204, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1882, 204)
  (lstm): LSTM(220, 114, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1824, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 16303.344197
validation loss after epoch 0 : 1017.103614
	Epoch 1....
Epoch has taken 0:02:50.898360
Number of used sentences in train = 2811
Total loss for epoch 1: 9090.254762
validation loss after epoch 1 : 1002.930499
	Epoch 2....
Epoch has taken 0:02:50.940742
Number of used sentences in train = 2811
Total loss for epoch 2: 8186.530849
validation loss after epoch 2 : 957.465137
	Epoch 3....
Epoch has taken 0:02:51.231797
Number of used sentences in train = 2811
Total loss for epoch 3: 7512.029859
validation loss after epoch 3 : 933.808318
	Epoch 4....
Epoch has taken 0:02:51.156073
Number of used sentences in train = 2811
Total loss for epoch 4: 7002.363096
validation loss after epoch 4 : 949.667386
	Epoch 5....
Epoch has taken 0:03:18.615979
Number of used sentences in train = 2811
Total loss for epoch 5: 6561.716310
validation loss after epoch 5 : 916.891917
	Epoch 6....
Epoch has taken 0:02:51.466319
Number of used sentences in train = 2811
Total loss for epoch 6: 6309.363361
validation loss after epoch 6 : 928.690638
	Epoch 7....
Epoch has taken 0:02:50.562542
Number of used sentences in train = 2811
Total loss for epoch 7: 5966.518768
validation loss after epoch 7 : 948.848054
	Epoch 8....
Epoch has taken 0:02:51.243826
Number of used sentences in train = 2811
Total loss for epoch 8: 5729.049377
validation loss after epoch 8 : 958.402888
	Epoch 9....
Epoch has taken 0:02:51.371974
Number of used sentences in train = 2811
Total loss for epoch 9: 5559.603402
validation loss after epoch 9 : 963.759952
	Epoch 10....
Epoch has taken 0:02:51.804053
Number of used sentences in train = 2811
Total loss for epoch 10: 5400.083957
validation loss after epoch 10 : 1058.575174
	Epoch 11....
Epoch has taken 0:02:50.955958
Number of used sentences in train = 2811
Total loss for epoch 11: 5334.453565
validation loss after epoch 11 : 977.960231
	Epoch 12....
Epoch has taken 0:02:50.020178
Number of used sentences in train = 2811
Total loss for epoch 12: 5219.775409
validation loss after epoch 12 : 1023.372181
	Epoch 13....
Epoch has taken 0:02:50.522743
Number of used sentences in train = 2811
Total loss for epoch 13: 5118.193507
validation loss after epoch 13 : 1079.757286
	Epoch 14....
Epoch has taken 0:02:50.622111
Number of used sentences in train = 2811
Total loss for epoch 14: 5089.944097
validation loss after epoch 14 : 1068.427155
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1882, 204)
  (lstm): LSTM(220, 114, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1824, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:07.238773
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1652.785468
	Epoch 1....
Epoch has taken 0:00:20.158229
Number of used sentences in train = 313
Total loss for epoch 1: 924.160559
	Epoch 2....
Epoch has taken 0:00:20.158353
Number of used sentences in train = 313
Total loss for epoch 2: 776.023784
	Epoch 3....
Epoch has taken 0:00:20.137001
Number of used sentences in train = 313
Total loss for epoch 3: 664.135002
	Epoch 4....
Epoch has taken 0:00:20.157618
Number of used sentences in train = 313
Total loss for epoch 4: 614.352852
	Epoch 5....
Epoch has taken 0:00:20.188694
Number of used sentences in train = 313
Total loss for epoch 5: 585.690698
	Epoch 6....
Epoch has taken 0:00:20.171052
Number of used sentences in train = 313
Total loss for epoch 6: 560.308022
	Epoch 7....
Epoch has taken 0:00:19.303426
Number of used sentences in train = 313
Total loss for epoch 7: 545.380341
	Epoch 8....
Epoch has taken 0:00:20.177392
Number of used sentences in train = 313
Total loss for epoch 8: 533.219720
	Epoch 9....
Epoch has taken 0:00:20.176654
Number of used sentences in train = 313
Total loss for epoch 9: 527.715239
	Epoch 10....
Epoch has taken 0:00:20.146231
Number of used sentences in train = 313
Total loss for epoch 10: 525.330736
	Epoch 11....
Epoch has taken 0:00:20.156689
Number of used sentences in train = 313
Total loss for epoch 11: 518.472991
	Epoch 12....
Epoch has taken 0:00:20.077170
Number of used sentences in train = 313
Total loss for epoch 12: 517.526843
	Epoch 13....
Epoch has taken 0:00:19.865995
Number of used sentences in train = 313
Total loss for epoch 13: 520.056802
	Epoch 14....
Epoch has taken 0:00:19.721320
Number of used sentences in train = 313
Total loss for epoch 14: 516.380412
Epoch has taken 0:00:18.901909

==================================================================================================
	Training time : 0:48:28.673497
==================================================================================================
	Identification : 0.455

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1680, 204)
  (lstm): LSTM(220, 114, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1824, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10944.853677
validation loss after epoch 0 : 787.266154
	Epoch 1....
Epoch has taken 0:02:07.400240
Number of used sentences in train = 2074
Total loss for epoch 1: 5966.324073
validation loss after epoch 1 : 711.074463
	Epoch 2....
Epoch has taken 0:02:07.605347
Number of used sentences in train = 2074
Total loss for epoch 2: 4907.485996
validation loss after epoch 2 : 720.345767
	Epoch 3....
Epoch has taken 0:02:07.228220
Number of used sentences in train = 2074
Total loss for epoch 3: 4293.357809
validation loss after epoch 3 : 738.753430
	Epoch 4....
Epoch has taken 0:02:07.299687
Number of used sentences in train = 2074
Total loss for epoch 4: 3909.578774
validation loss after epoch 4 : 729.311181
	Epoch 5....
Epoch has taken 0:02:09.198598
Number of used sentences in train = 2074
Total loss for epoch 5: 3680.237780
validation loss after epoch 5 : 755.310196
	Epoch 6....
Epoch has taken 0:02:09.739183
Number of used sentences in train = 2074
Total loss for epoch 6: 3515.353407
validation loss after epoch 6 : 794.981244
	Epoch 7....
Epoch has taken 0:02:09.317354
Number of used sentences in train = 2074
Total loss for epoch 7: 3390.647328
validation loss after epoch 7 : 817.892133
	Epoch 8....
Epoch has taken 0:02:08.618624
Number of used sentences in train = 2074
Total loss for epoch 8: 3335.625017
validation loss after epoch 8 : 857.468337
	Epoch 9....
Epoch has taken 0:02:08.729743
Number of used sentences in train = 2074
Total loss for epoch 9: 3292.943890
validation loss after epoch 9 : 895.798729
	Epoch 10....
Epoch has taken 0:02:07.884815
Number of used sentences in train = 2074
Total loss for epoch 10: 3265.001709
validation loss after epoch 10 : 861.832191
	Epoch 11....
Epoch has taken 0:02:08.112678
Number of used sentences in train = 2074
Total loss for epoch 11: 3245.816521
validation loss after epoch 11 : 922.536739
	Epoch 12....
Epoch has taken 0:02:07.732452
Number of used sentences in train = 2074
Total loss for epoch 12: 3224.725014
validation loss after epoch 12 : 912.027434
	Epoch 13....
Epoch has taken 0:02:06.441700
Number of used sentences in train = 2074
Total loss for epoch 13: 3208.697635
validation loss after epoch 13 : 889.811234
	Epoch 14....
Epoch has taken 0:01:59.020485
Number of used sentences in train = 2074
Total loss for epoch 14: 3189.194621
validation loss after epoch 14 : 879.292272
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1680, 204)
  (lstm): LSTM(220, 114, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1824, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.015553
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1611.900241
	Epoch 1....
Epoch has taken 0:00:11.852545
Number of used sentences in train = 231
Total loss for epoch 1: 704.071525
	Epoch 2....
Epoch has taken 0:00:11.874454
Number of used sentences in train = 231
Total loss for epoch 2: 492.680545
	Epoch 3....
Epoch has taken 0:00:11.890242
Number of used sentences in train = 231
Total loss for epoch 3: 421.257362
	Epoch 4....
Epoch has taken 0:00:11.881673
Number of used sentences in train = 231
Total loss for epoch 4: 378.845856
	Epoch 5....
Epoch has taken 0:00:11.894795
Number of used sentences in train = 231
Total loss for epoch 5: 366.503349
	Epoch 6....
Epoch has taken 0:00:11.853988
Number of used sentences in train = 231
Total loss for epoch 6: 362.048133
	Epoch 7....
Epoch has taken 0:00:11.882531
Number of used sentences in train = 231
Total loss for epoch 7: 361.518528
	Epoch 8....
Epoch has taken 0:00:11.871582
Number of used sentences in train = 231
Total loss for epoch 8: 355.488665
	Epoch 9....
Epoch has taken 0:00:11.896652
Number of used sentences in train = 231
Total loss for epoch 9: 353.985514
	Epoch 10....
Epoch has taken 0:00:11.872949
Number of used sentences in train = 231
Total loss for epoch 10: 352.945414
	Epoch 11....
Epoch has taken 0:00:11.902022
Number of used sentences in train = 231
Total loss for epoch 11: 351.977889
	Epoch 12....
Epoch has taken 0:00:11.882448
Number of used sentences in train = 231
Total loss for epoch 12: 351.752566
	Epoch 13....
Epoch has taken 0:00:11.876374
Number of used sentences in train = 231
Total loss for epoch 13: 351.288891
	Epoch 14....
Epoch has taken 0:00:11.889165
Number of used sentences in train = 231
Total loss for epoch 14: 351.020816
Epoch has taken 0:00:11.920221

==================================================================================================
	Training time : 0:34:39.934244
==================================================================================================
	Identification : 0.441

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(3369, 204)
  (lstm): LSTM(220, 114, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1824, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 23984.798066
validation loss after epoch 0 : 1274.212597
	Epoch 1....
Epoch has taken 0:03:48.829949
Number of used sentences in train = 3226
Total loss for epoch 1: 10622.068595
validation loss after epoch 1 : 1151.979465
	Epoch 2....
Epoch has taken 0:03:48.728540
Number of used sentences in train = 3226
Total loss for epoch 2: 9274.420410
validation loss after epoch 2 : 1092.478009
	Epoch 3....
Epoch has taken 0:04:25.268641
Number of used sentences in train = 3226
Total loss for epoch 3: 8605.820246
validation loss after epoch 3 : 1130.915910
	Epoch 4....
Epoch has taken 0:03:49.127074
Number of used sentences in train = 3226
Total loss for epoch 4: 8088.764520
validation loss after epoch 4 : 1182.694414
	Epoch 5....
Epoch has taken 0:03:48.137467
Number of used sentences in train = 3226
Total loss for epoch 5: 7733.687439
validation loss after epoch 5 : 1152.669298
	Epoch 6....
Epoch has taken 0:03:48.983157
Number of used sentences in train = 3226
Total loss for epoch 6: 7388.571971
validation loss after epoch 6 : 1306.257615
	Epoch 7....
Epoch has taken 0:03:48.441741
Number of used sentences in train = 3226
Total loss for epoch 7: 7121.922502
validation loss after epoch 7 : 1309.036137
	Epoch 8....
Epoch has taken 0:04:08.215872
Number of used sentences in train = 3226
Total loss for epoch 8: 6939.982518
validation loss after epoch 8 : 1299.691818
	Epoch 9....
Epoch has taken 0:03:48.230753
Number of used sentences in train = 3226
Total loss for epoch 9: 6782.361179
validation loss after epoch 9 : 1321.827164
	Epoch 10....
Epoch has taken 0:03:48.677430
Number of used sentences in train = 3226
Total loss for epoch 10: 6664.001009
validation loss after epoch 10 : 1349.932677
	Epoch 11....
Epoch has taken 0:03:48.844040
Number of used sentences in train = 3226
Total loss for epoch 11: 6604.533706
validation loss after epoch 11 : 1373.222316
	Epoch 12....
Epoch has taken 0:03:48.099342
Number of used sentences in train = 3226
Total loss for epoch 12: 6550.748119
validation loss after epoch 12 : 1494.712932
	Epoch 13....
Epoch has taken 0:03:48.727501
Number of used sentences in train = 3226
Total loss for epoch 13: 6504.543682
validation loss after epoch 13 : 1415.122390
	Epoch 14....
Epoch has taken 0:04:16.058988
Number of used sentences in train = 3226
Total loss for epoch 14: 6438.696823
validation loss after epoch 14 : 1541.781755
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(3369, 204)
  (lstm): LSTM(220, 114, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1824, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:59.659053
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1634.502798
	Epoch 1....
Epoch has taken 0:00:24.438278
Number of used sentences in train = 359
Total loss for epoch 1: 1102.063455
	Epoch 2....
Epoch has taken 0:00:24.688849
Number of used sentences in train = 359
Total loss for epoch 2: 874.927121
	Epoch 3....
Epoch has taken 0:00:24.214818
Number of used sentences in train = 359
Total loss for epoch 3: 809.193523
	Epoch 4....
Epoch has taken 0:00:23.496157
Number of used sentences in train = 359
Total loss for epoch 4: 761.911943
	Epoch 5....
Epoch has taken 0:00:22.951511
Number of used sentences in train = 359
Total loss for epoch 5: 730.793758
	Epoch 6....
Epoch has taken 0:00:24.518614
Number of used sentences in train = 359
Total loss for epoch 6: 714.099163
	Epoch 7....
Epoch has taken 0:00:24.685625
Number of used sentences in train = 359
Total loss for epoch 7: 704.104898
	Epoch 8....
Epoch has taken 0:00:24.614194
Number of used sentences in train = 359
Total loss for epoch 8: 704.496491
	Epoch 9....
Epoch has taken 0:00:23.414402
Number of used sentences in train = 359
Total loss for epoch 9: 699.917021
	Epoch 10....
Epoch has taken 0:00:24.462142
Number of used sentences in train = 359
Total loss for epoch 10: 687.789874
	Epoch 11....
Epoch has taken 0:00:24.714888
Number of used sentences in train = 359
Total loss for epoch 11: 679.499404
	Epoch 12....
Epoch has taken 0:00:24.665778
Number of used sentences in train = 359
Total loss for epoch 12: 679.249590
	Epoch 13....
Epoch has taken 0:00:24.680155
Number of used sentences in train = 359
Total loss for epoch 13: 681.675778
	Epoch 14....
Epoch has taken 0:00:24.547552
Number of used sentences in train = 359
Total loss for epoch 14: 675.407834
Epoch has taken 0:00:24.680858

==================================================================================================
	Training time : 1:04:49.501503
==================================================================================================
	Identification : 0.118

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 45, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 40, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 122, 'lstmDropout': 0.15, 'denseActivation': 'tanh', 'wordDim': 170, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1177, 170)
  (lstm): LSTM(210, 122, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15037.039425
validation loss after epoch 0 : 982.686786
	Epoch 1....
Epoch has taken 0:02:59.252129
Number of used sentences in train = 2811
Total loss for epoch 1: 8822.334448
validation loss after epoch 1 : 937.862542
	Epoch 2....
Epoch has taken 0:03:09.407705
Number of used sentences in train = 2811
Total loss for epoch 2: 7889.715615
validation loss after epoch 2 : 930.731098
	Epoch 3....
Epoch has taken 0:03:08.198165
Number of used sentences in train = 2811
Total loss for epoch 3: 7223.550792
validation loss after epoch 3 : 885.515212
	Epoch 4....
Epoch has taken 0:03:08.536398
Number of used sentences in train = 2811
Total loss for epoch 4: 6684.602935
validation loss after epoch 4 : 894.290346
	Epoch 5....
Epoch has taken 0:03:07.614390
Number of used sentences in train = 2811
Total loss for epoch 5: 6343.537369
validation loss after epoch 5 : 911.313455
	Epoch 6....
Epoch has taken 0:02:59.007251
Number of used sentences in train = 2811
Total loss for epoch 6: 6097.770798
validation loss after epoch 6 : 913.151055
	Epoch 7....
Epoch has taken 0:03:00.188447
Number of used sentences in train = 2811
Total loss for epoch 7: 5836.450842
validation loss after epoch 7 : 935.581639
	Epoch 8....
Epoch has taken 0:03:07.335279
Number of used sentences in train = 2811
Total loss for epoch 8: 5660.703885
validation loss after epoch 8 : 938.298221
	Epoch 9....
Epoch has taken 0:02:57.997784
Number of used sentences in train = 2811
Total loss for epoch 9: 5418.705441
validation loss after epoch 9 : 963.834944
	Epoch 10....
Epoch has taken 0:03:08.088644
Number of used sentences in train = 2811
Total loss for epoch 10: 5400.365420
validation loss after epoch 10 : 989.380394
	Epoch 11....
Epoch has taken 0:03:05.290591
Number of used sentences in train = 2811
Total loss for epoch 11: 5297.883663
validation loss after epoch 11 : 983.984753
	Epoch 12....
Epoch has taken 0:03:06.019888
Number of used sentences in train = 2811
Total loss for epoch 12: 5152.373367
validation loss after epoch 12 : 1003.435441
	Epoch 13....
Epoch has taken 0:03:06.035336
Number of used sentences in train = 2811
Total loss for epoch 13: 5062.109658
validation loss after epoch 13 : 1035.041613
	Epoch 14....
Epoch has taken 0:02:58.061840
Number of used sentences in train = 2811
Total loss for epoch 14: 4997.572915
validation loss after epoch 14 : 1044.726331
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1177, 170)
  (lstm): LSTM(210, 122, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:58.642852
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1508.009551
	Epoch 1....
Epoch has taken 0:00:19.478643
Number of used sentences in train = 313
Total loss for epoch 1: 895.047777
	Epoch 2....
Epoch has taken 0:00:19.581708
Number of used sentences in train = 313
Total loss for epoch 2: 744.035082
	Epoch 3....
Epoch has taken 0:00:19.551417
Number of used sentences in train = 313
Total loss for epoch 3: 658.261159
	Epoch 4....
Epoch has taken 0:00:19.420344
Number of used sentences in train = 313
Total loss for epoch 4: 603.704710
	Epoch 5....
Epoch has taken 0:00:19.420322
Number of used sentences in train = 313
Total loss for epoch 5: 572.508843
	Epoch 6....
Epoch has taken 0:00:19.536197
Number of used sentences in train = 313
Total loss for epoch 6: 556.095517
	Epoch 7....
Epoch has taken 0:00:19.464927
Number of used sentences in train = 313
Total loss for epoch 7: 538.955619
	Epoch 8....
Epoch has taken 0:00:19.326214
Number of used sentences in train = 313
Total loss for epoch 8: 530.819590
	Epoch 9....
Epoch has taken 0:00:19.596052
Number of used sentences in train = 313
Total loss for epoch 9: 525.857873
	Epoch 10....
Epoch has taken 0:00:19.584178
Number of used sentences in train = 313
Total loss for epoch 10: 525.523920
	Epoch 11....
Epoch has taken 0:00:19.482147
Number of used sentences in train = 313
Total loss for epoch 11: 524.271415
	Epoch 12....
Epoch has taken 0:00:19.534949
Number of used sentences in train = 313
Total loss for epoch 12: 524.318500
	Epoch 13....
Epoch has taken 0:00:19.613647
Number of used sentences in train = 313
Total loss for epoch 13: 521.724060
	Epoch 14....
Epoch has taken 0:00:19.452834
Number of used sentences in train = 313
Total loss for epoch 14: 520.035199
Epoch has taken 0:00:19.791201

==================================================================================================
	Training time : 0:50:53.043727
==================================================================================================
	Identification : 0.066

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1133, 170)
  (lstm): LSTM(210, 122, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 13929.689066
validation loss after epoch 0 : 789.083887
	Epoch 1....
Epoch has taken 0:01:59.872918
Number of used sentences in train = 2074
Total loss for epoch 1: 6147.679514
validation loss after epoch 1 : 718.245532
	Epoch 2....
Epoch has taken 0:01:59.975352
Number of used sentences in train = 2074
Total loss for epoch 2: 5037.131152
validation loss after epoch 2 : 673.886504
	Epoch 3....
Epoch has taken 0:01:59.471691
Number of used sentences in train = 2074
Total loss for epoch 3: 4445.002651
validation loss after epoch 3 : 656.781423
	Epoch 4....
Epoch has taken 0:02:00.674444
Number of used sentences in train = 2074
Total loss for epoch 4: 3960.765132
validation loss after epoch 4 : 693.726962
	Epoch 5....
Epoch has taken 0:02:00.081861
Number of used sentences in train = 2074
Total loss for epoch 5: 3716.371869
validation loss after epoch 5 : 708.656491
	Epoch 6....
Epoch has taken 0:02:00.207970
Number of used sentences in train = 2074
Total loss for epoch 6: 3539.420357
validation loss after epoch 6 : 711.890187
	Epoch 7....
Epoch has taken 0:02:05.422448
Number of used sentences in train = 2074
Total loss for epoch 7: 3426.306983
validation loss after epoch 7 : 697.505490
	Epoch 8....
Epoch has taken 0:01:59.612635
Number of used sentences in train = 2074
Total loss for epoch 8: 3362.242301
validation loss after epoch 8 : 768.920139
	Epoch 9....
Epoch has taken 0:02:00.198743
Number of used sentences in train = 2074
Total loss for epoch 9: 3325.632991
validation loss after epoch 9 : 731.041633
	Epoch 10....
Epoch has taken 0:02:00.485216
Number of used sentences in train = 2074
Total loss for epoch 10: 3267.308285
validation loss after epoch 10 : 783.350051
	Epoch 11....
Epoch has taken 0:01:59.805149
Number of used sentences in train = 2074
Total loss for epoch 11: 3244.390993
validation loss after epoch 11 : 775.943818
	Epoch 12....
Epoch has taken 0:01:59.594729
Number of used sentences in train = 2074
Total loss for epoch 12: 3235.431028
validation loss after epoch 12 : 795.844570
	Epoch 13....
Epoch has taken 0:01:59.456483
Number of used sentences in train = 2074
Total loss for epoch 13: 3243.624420
validation loss after epoch 13 : 837.639732
	Epoch 14....
Epoch has taken 0:01:59.430070
Number of used sentences in train = 2074
Total loss for epoch 14: 3212.008386
validation loss after epoch 14 : 822.737710
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1133, 170)
  (lstm): LSTM(210, 122, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:59.368558
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1232.776079
	Epoch 1....
Epoch has taken 0:00:13.046970
Number of used sentences in train = 231
Total loss for epoch 1: 607.867478
	Epoch 2....
Epoch has taken 0:00:12.336524
Number of used sentences in train = 231
Total loss for epoch 2: 467.693569
	Epoch 3....
Epoch has taken 0:00:12.323158
Number of used sentences in train = 231
Total loss for epoch 3: 407.360750
	Epoch 4....
Epoch has taken 0:00:12.692620
Number of used sentences in train = 231
Total loss for epoch 4: 380.981763
	Epoch 5....
Epoch has taken 0:00:12.702035
Number of used sentences in train = 231
Total loss for epoch 5: 359.573272
	Epoch 6....
Epoch has taken 0:00:12.474832
Number of used sentences in train = 231
Total loss for epoch 6: 356.072117
	Epoch 7....
Epoch has taken 0:00:12.674111
Number of used sentences in train = 231
Total loss for epoch 7: 353.793845
	Epoch 8....
Epoch has taken 0:00:12.937782
Number of used sentences in train = 231
Total loss for epoch 8: 352.062364
	Epoch 9....
Epoch has taken 0:00:12.563119
Number of used sentences in train = 231
Total loss for epoch 9: 351.666500
	Epoch 10....
Epoch has taken 0:00:13.014500
Number of used sentences in train = 231
Total loss for epoch 10: 351.224449
	Epoch 11....
Epoch has taken 0:00:12.756397
Number of used sentences in train = 231
Total loss for epoch 11: 351.263401
	Epoch 12....
Epoch has taken 0:00:13.019982
Number of used sentences in train = 231
Total loss for epoch 12: 351.302046
	Epoch 13....
Epoch has taken 0:00:12.622855
Number of used sentences in train = 231
Total loss for epoch 13: 350.618932
	Epoch 14....
Epoch has taken 0:00:13.012509
Number of used sentences in train = 231
Total loss for epoch 14: 350.358312
Epoch has taken 0:00:12.732522

==================================================================================================
	Training time : 0:33:14.921174
==================================================================================================
	Identification : 0.378

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 40)
  (w_embeddings): Embedding(1202, 170)
  (lstm): LSTM(210, 122, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 39697.706932
validation loss after epoch 0 : 4058.492495
	Epoch 1....
Epoch has taken 0:04:10.862821
Number of used sentences in train = 3226
Total loss for epoch 1: 26364.598095
validation loss after epoch 1 : 1829.235720
	Epoch 2....
Epoch has taken 0:04:10.369958
Number of used sentences in train = 3226
Total loss for epoch 2: 14702.798938
validation loss after epoch 2 : 1459.048041
	Epoch 3....
Epoch has taken 0:04:08.315782
Number of used sentences in train = 3226
Total loss for epoch 3: 12016.493913
validation loss after epoch 3 : 1352.597530
	Epoch 4....
Epoch has taken 0:04:08.345956
Number of used sentences in train = 3226
Total loss for epoch 4: 10937.202542
validation loss after epoch 4 : 1326.451639
	Epoch 5....
Epoch has taken 0:04:09.316088
Number of used sentences in train = 3226
Total loss for epoch 5: 10074.801294
validation loss after epoch 5 : 1318.570080
	Epoch 6....
Epoch has taken 0:04:12.445257
Number of used sentences in train = 3226
Total loss for epoch 6: 9364.076436
validation loss after epoch 6 : 1384.072578
	Epoch 7....
Epoch has taken 0:04:10.973805
Number of used sentences in train = 3226
Total loss for epoch 7: 8962.492300
validation loss after epoch 7 : 1370.375214
	Epoch 8....
Epoch has taken 0:04:09.550871
Number of used sentences in train = 3226
Total loss for epoch 8: 8547.901305
validation loss after epoch 8 : 1411.410309
	Epoch 9....
Epoch has taken 0:04:08.233371
Number of used sentences in train = 3226
Total loss for epoch 9: 8244.926131
validation loss after epoch 9 : 1425.497408
	Epoch 10....
Epoch has taken 0:04:09.602832
Number of used sentences in train = 3226
Total loss for epoch 10: 8036.675735
validation loss after epoch 10 : 1463.075830
	Epoch 11....
Epoch has taken 0:04:09.411387
Number of used sentences in train = 3226
Total loss for epoch 11: 7724.034433
validation loss after epoch 11 : 1499.187292
	Epoch 12....
Epoch has taken 0:04:09.836337
Number of used sentences in train = 3226
Total loss for epoch 12: 7499.098986
validation loss after epoch 12 : 1561.810061
	Epoch 13....
Epoch has taken 0:04:07.980832
Number of used sentences in train = 3226
Total loss for epoch 13: 7359.504207
validation loss after epoch 13 : 1587.259534
	Epoch 14....
Epoch has taken 0:04:08.812339
Number of used sentences in train = 3226
Total loss for epoch 14: 7181.990305
validation loss after epoch 14 : 1753.985359
	TransitionClassifier(
  (p_embeddings): Embedding(13, 40)
  (w_embeddings): Embedding(1202, 170)
  (lstm): LSTM(210, 122, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=45, bias=True)
  (linear2): Linear(in_features=45, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:09.591421
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2182.917000
	Epoch 1....
Epoch has taken 0:00:28.110273
Number of used sentences in train = 359
Total loss for epoch 1: 1600.820345
	Epoch 2....
Epoch has taken 0:00:27.957700
Number of used sentences in train = 359
Total loss for epoch 2: 1294.734632
	Epoch 3....
Epoch has taken 0:00:28.216447
Number of used sentences in train = 359
Total loss for epoch 3: 1140.555650
	Epoch 4....
Epoch has taken 0:00:28.104044
Number of used sentences in train = 359
Total loss for epoch 4: 1013.641491
	Epoch 5....
Epoch has taken 0:00:28.234461
Number of used sentences in train = 359
Total loss for epoch 5: 920.804387
	Epoch 6....
Epoch has taken 0:00:28.326547
Number of used sentences in train = 359
Total loss for epoch 6: 861.219634
	Epoch 7....
Epoch has taken 0:00:28.175442
Number of used sentences in train = 359
Total loss for epoch 7: 821.959522
	Epoch 8....
Epoch has taken 0:00:28.320757
Number of used sentences in train = 359
Total loss for epoch 8: 782.546366
	Epoch 9....
Epoch has taken 0:00:28.252588
Number of used sentences in train = 359
Total loss for epoch 9: 758.939488
	Epoch 10....
Epoch has taken 0:00:28.221514
Number of used sentences in train = 359
Total loss for epoch 10: 752.604020
	Epoch 11....
Epoch has taken 0:00:24.595782
Number of used sentences in train = 359
Total loss for epoch 11: 742.840506
	Epoch 12....
Epoch has taken 0:00:24.752043
Number of used sentences in train = 359
Total loss for epoch 12: 723.912359
	Epoch 13....
Epoch has taken 0:00:24.687973
Number of used sentences in train = 359
Total loss for epoch 13: 701.291560
	Epoch 14....
Epoch has taken 0:00:24.633646
Number of used sentences in train = 359
Total loss for epoch 14: 695.617814
Epoch has taken 0:00:24.663866

==================================================================================================
	Training time : 1:09:09.570265
==================================================================================================
	Identification : 0.109

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 59, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 19, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 24, 'lstmDropout': 0.12, 'denseActivation': 'tanh', 'wordDim': 66, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(9288, 66)
  (lstm): LSTM(85, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=59, bias=True)
  (linear2): Linear(in_features=59, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13187.866497
validation loss after epoch 0 : 1122.391871
	Epoch 1....
Epoch has taken 0:03:02.464476
Number of used sentences in train = 2811
Total loss for epoch 1: 9015.423077
validation loss after epoch 1 : 1069.175642
	Epoch 2....
Epoch has taken 0:03:02.329655
Number of used sentences in train = 2811
Total loss for epoch 2: 7411.241370
validation loss after epoch 2 : 1116.727839
	Epoch 3....
Epoch has taken 0:03:02.315938
Number of used sentences in train = 2811
Total loss for epoch 3: 6459.159552
validation loss after epoch 3 : 1157.316558
	Epoch 4....
Epoch has taken 0:03:03.133484
Number of used sentences in train = 2811
Total loss for epoch 4: 5882.319878
validation loss after epoch 4 : 1207.660114
	Epoch 5....
Epoch has taken 0:03:01.080622
Number of used sentences in train = 2811
Total loss for epoch 5: 5530.647905
validation loss after epoch 5 : 1288.089946
	Epoch 6....
Epoch has taken 0:03:14.994257
Number of used sentences in train = 2811
Total loss for epoch 6: 5242.572908
validation loss after epoch 6 : 1292.996788
	Epoch 7....
Epoch has taken 0:03:02.358158
Number of used sentences in train = 2811
Total loss for epoch 7: 5083.639945
validation loss after epoch 7 : 1380.939551
	Epoch 8....
Epoch has taken 0:03:01.253733
Number of used sentences in train = 2811
Total loss for epoch 8: 4935.818283
validation loss after epoch 8 : 1448.152667
	Epoch 9....
Epoch has taken 0:03:02.246722
Number of used sentences in train = 2811
Total loss for epoch 9: 4799.015568
validation loss after epoch 9 : 1526.905333
	Epoch 10....
Epoch has taken 0:03:01.787043
Number of used sentences in train = 2811
Total loss for epoch 10: 4792.213431
validation loss after epoch 10 : 1530.823432
	Epoch 11....
Epoch has taken 0:03:03.353655
Number of used sentences in train = 2811
Total loss for epoch 11: 4738.410382
validation loss after epoch 11 : 1519.418126
	Epoch 12....
Epoch has taken 0:03:02.340044
Number of used sentences in train = 2811
Total loss for epoch 12: 4683.055522
validation loss after epoch 12 : 1529.979063
	Epoch 13....
Epoch has taken 0:03:01.254655
Number of used sentences in train = 2811
Total loss for epoch 13: 4654.068289
validation loss after epoch 13 : 1663.287450
	Epoch 14....
Epoch has taken 0:03:02.502708
Number of used sentences in train = 2811
Total loss for epoch 14: 4610.811296
validation loss after epoch 14 : 1676.941968
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(9288, 66)
  (lstm): LSTM(85, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=59, bias=True)
  (linear2): Linear(in_features=59, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:10.556185
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1537.747036
	Epoch 1....
Epoch has taken 0:00:19.444881
Number of used sentences in train = 313
Total loss for epoch 1: 807.701833
	Epoch 2....
Epoch has taken 0:00:19.482314
Number of used sentences in train = 313
Total loss for epoch 2: 651.705743
	Epoch 3....
Epoch has taken 0:00:19.481559
Number of used sentences in train = 313
Total loss for epoch 3: 569.082214
	Epoch 4....
Epoch has taken 0:00:19.439886
Number of used sentences in train = 313
Total loss for epoch 4: 538.198877
	Epoch 5....
Epoch has taken 0:00:19.516962
Number of used sentences in train = 313
Total loss for epoch 5: 526.568464
	Epoch 6....
Epoch has taken 0:00:19.526314
Number of used sentences in train = 313
Total loss for epoch 6: 526.576226
	Epoch 7....
Epoch has taken 0:00:19.398507
Number of used sentences in train = 313
Total loss for epoch 7: 520.244014
	Epoch 8....
Epoch has taken 0:00:19.576568
Number of used sentences in train = 313
Total loss for epoch 8: 507.430553
	Epoch 9....
Epoch has taken 0:00:19.345174
Number of used sentences in train = 313
Total loss for epoch 9: 505.046787
	Epoch 10....
Epoch has taken 0:00:19.430964
Number of used sentences in train = 313
Total loss for epoch 10: 505.257859
	Epoch 11....
Epoch has taken 0:00:19.452032
Number of used sentences in train = 313
Total loss for epoch 11: 507.522744
	Epoch 12....
Epoch has taken 0:00:19.300249
Number of used sentences in train = 313
Total loss for epoch 12: 503.300380
	Epoch 13....
Epoch has taken 0:00:19.297556
Number of used sentences in train = 313
Total loss for epoch 13: 502.294003
	Epoch 14....
Epoch has taken 0:00:19.448253
Number of used sentences in train = 313
Total loss for epoch 14: 502.362834
Epoch has taken 0:00:19.172562

==================================================================================================
	Training time : 0:50:45.806891
==================================================================================================
	Identification : 0.305

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(7092, 66)
  (lstm): LSTM(85, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=59, bias=True)
  (linear2): Linear(in_features=59, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 12597.427527
validation loss after epoch 0 : 932.491809
	Epoch 1....
Epoch has taken 0:02:20.691233
Number of used sentences in train = 2074
Total loss for epoch 1: 7137.310844
validation loss after epoch 1 : 901.229918
	Epoch 2....
Epoch has taken 0:02:27.138008
Number of used sentences in train = 2074
Total loss for epoch 2: 5650.369469
validation loss after epoch 2 : 943.181770
	Epoch 3....
Epoch has taken 0:02:01.344668
Number of used sentences in train = 2074
Total loss for epoch 3: 4866.926204
validation loss after epoch 3 : 978.169392
	Epoch 4....
Epoch has taken 0:02:06.718972
Number of used sentences in train = 2074
Total loss for epoch 4: 4267.588704
validation loss after epoch 4 : 1043.205235
	Epoch 5....
Epoch has taken 0:02:07.120974
Number of used sentences in train = 2074
Total loss for epoch 5: 3944.126144
validation loss after epoch 5 : 1086.924868
	Epoch 6....
Epoch has taken 0:02:06.836196
Number of used sentences in train = 2074
Total loss for epoch 6: 3737.117063
validation loss after epoch 6 : 1151.784148
	Epoch 7....
Epoch has taken 0:02:06.908800
Number of used sentences in train = 2074
Total loss for epoch 7: 3587.997346
validation loss after epoch 7 : 1174.008177
	Epoch 8....
Epoch has taken 0:02:07.220690
Number of used sentences in train = 2074
Total loss for epoch 8: 3464.160452
validation loss after epoch 8 : 1353.089686
	Epoch 9....
Epoch has taken 0:02:26.772815
Number of used sentences in train = 2074
Total loss for epoch 9: 3419.358947
validation loss after epoch 9 : 1291.843275
	Epoch 10....
Epoch has taken 0:02:07.183097
Number of used sentences in train = 2074
Total loss for epoch 10: 3371.422813
validation loss after epoch 10 : 1409.867069
	Epoch 11....
Epoch has taken 0:02:07.102937
Number of used sentences in train = 2074
Total loss for epoch 11: 3309.504105
validation loss after epoch 11 : 1351.727570
	Epoch 12....
Epoch has taken 0:02:01.631467
Number of used sentences in train = 2074
Total loss for epoch 12: 3305.882523
validation loss after epoch 12 : 1424.205734
	Epoch 13....
Epoch has taken 0:02:00.636582
Number of used sentences in train = 2074
Total loss for epoch 13: 3253.498239
validation loss after epoch 13 : 1411.383527
	Epoch 14....
Epoch has taken 0:02:00.962982
Number of used sentences in train = 2074
Total loss for epoch 14: 3245.709655
validation loss after epoch 14 : 1454.508533
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(7092, 66)
  (lstm): LSTM(85, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=59, bias=True)
  (linear2): Linear(in_features=59, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:03.684133
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1753.400683
	Epoch 1....
Epoch has taken 0:00:12.677706
Number of used sentences in train = 231
Total loss for epoch 1: 636.355468
	Epoch 2....
Epoch has taken 0:00:12.924727
Number of used sentences in train = 231
Total loss for epoch 2: 444.561945
	Epoch 3....
Epoch has taken 0:00:12.983152
Number of used sentences in train = 231
Total loss for epoch 3: 392.636951
	Epoch 4....
Epoch has taken 0:00:12.248878
Number of used sentences in train = 231
Total loss for epoch 4: 381.906156
	Epoch 5....
Epoch has taken 0:00:12.245969
Number of used sentences in train = 231
Total loss for epoch 5: 364.959386
	Epoch 6....
Epoch has taken 0:00:12.248120
Number of used sentences in train = 231
Total loss for epoch 6: 362.050341
	Epoch 7....
Epoch has taken 0:00:12.241623
Number of used sentences in train = 231
Total loss for epoch 7: 356.284377
	Epoch 8....
Epoch has taken 0:00:12.243569
Number of used sentences in train = 231
Total loss for epoch 8: 356.322731
	Epoch 9....
Epoch has taken 0:00:12.238917
Number of used sentences in train = 231
Total loss for epoch 9: 355.425232
	Epoch 10....
Epoch has taken 0:00:12.651370
Number of used sentences in train = 231
Total loss for epoch 10: 355.453907
	Epoch 11....
Epoch has taken 0:00:12.244164
Number of used sentences in train = 231
Total loss for epoch 11: 348.253437
	Epoch 12....
Epoch has taken 0:00:12.243235
Number of used sentences in train = 231
Total loss for epoch 12: 348.456335
	Epoch 13....
Epoch has taken 0:00:12.248341
Number of used sentences in train = 231
Total loss for epoch 13: 347.684471
	Epoch 14....
Epoch has taken 0:00:12.227584
Number of used sentences in train = 231
Total loss for epoch 14: 347.837900
Epoch has taken 0:00:12.228416

==================================================================================================
	Training time : 0:35:18.196010
==================================================================================================
	Identification : 0.306

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(17988, 66)
  (lstm): LSTM(85, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=59, bias=True)
  (linear2): Linear(in_features=59, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18925.343486
validation loss after epoch 0 : 1655.726509
	Epoch 1....
Epoch has taken 0:03:53.904532
Number of used sentences in train = 3226
Total loss for epoch 1: 12167.534508
validation loss after epoch 1 : 1620.454535
	Epoch 2....
Epoch has taken 0:03:53.834611
Number of used sentences in train = 3226
Total loss for epoch 2: 9967.346379
validation loss after epoch 2 : 1738.909786
	Epoch 3....
Epoch has taken 0:03:52.277583
Number of used sentences in train = 3226
Total loss for epoch 3: 8672.127259
validation loss after epoch 3 : 1861.729678
	Epoch 4....
Epoch has taken 0:03:51.316311
Number of used sentences in train = 3226
Total loss for epoch 4: 7844.277676
validation loss after epoch 4 : 1926.822685
	Epoch 5....
Epoch has taken 0:03:53.762561
Number of used sentences in train = 3226
Total loss for epoch 5: 7282.836943
validation loss after epoch 5 : 2137.472174
	Epoch 6....
Epoch has taken 0:03:54.225659
Number of used sentences in train = 3226
Total loss for epoch 6: 6918.879964
validation loss after epoch 6 : 2262.582740
	Epoch 7....
Epoch has taken 0:03:52.899155
Number of used sentences in train = 3226
Total loss for epoch 7: 6769.271466
validation loss after epoch 7 : 2255.032303
	Epoch 8....
Epoch has taken 0:03:54.470558
Number of used sentences in train = 3226
Total loss for epoch 8: 6652.065968
validation loss after epoch 8 : 2275.209126
	Epoch 9....
Epoch has taken 0:04:02.072642
Number of used sentences in train = 3226
Total loss for epoch 9: 6542.332653
validation loss after epoch 9 : 2313.692569
	Epoch 10....
Epoch has taken 0:04:00.399781
Number of used sentences in train = 3226
Total loss for epoch 10: 6485.858756
validation loss after epoch 10 : 2334.488730
	Epoch 11....
Epoch has taken 0:03:55.929589
Number of used sentences in train = 3226
Total loss for epoch 11: 6444.331840
validation loss after epoch 11 : 2337.769620
	Epoch 12....
Epoch has taken 0:03:51.548450
Number of used sentences in train = 3226
Total loss for epoch 12: 6383.871258
validation loss after epoch 12 : 2430.922038
	Epoch 13....
Epoch has taken 0:03:51.610338
Number of used sentences in train = 3226
Total loss for epoch 13: 6341.393129
validation loss after epoch 13 : 2386.092963
	Epoch 14....
Epoch has taken 0:03:53.307149
Number of used sentences in train = 3226
Total loss for epoch 14: 6324.229319
validation loss after epoch 14 : 2450.558023
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(17988, 66)
  (lstm): LSTM(85, 24, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=59, bias=True)
  (linear2): Linear(in_features=59, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:53.293665
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2326.144691
	Epoch 1....
Epoch has taken 0:00:22.806793
Number of used sentences in train = 359
Total loss for epoch 1: 1156.300585
	Epoch 2....
Epoch has taken 0:00:22.830773
Number of used sentences in train = 359
Total loss for epoch 2: 862.824978
	Epoch 3....
Epoch has taken 0:00:22.731921
Number of used sentences in train = 359
Total loss for epoch 3: 755.164159
	Epoch 4....
Epoch has taken 0:00:22.696452
Number of used sentences in train = 359
Total loss for epoch 4: 702.610384
	Epoch 5....
Epoch has taken 0:00:22.692075
Number of used sentences in train = 359
Total loss for epoch 5: 701.259420
	Epoch 6....
Epoch has taken 0:00:22.822585
Number of used sentences in train = 359
Total loss for epoch 6: 685.098467
	Epoch 7....
Epoch has taken 0:00:22.795788
Number of used sentences in train = 359
Total loss for epoch 7: 682.292811
	Epoch 8....
Epoch has taken 0:00:22.728253
Number of used sentences in train = 359
Total loss for epoch 8: 674.663795
	Epoch 9....
Epoch has taken 0:00:22.759658
Number of used sentences in train = 359
Total loss for epoch 9: 672.993871
	Epoch 10....
Epoch has taken 0:00:22.776873
Number of used sentences in train = 359
Total loss for epoch 10: 673.804412
	Epoch 11....
Epoch has taken 0:00:22.746692
Number of used sentences in train = 359
Total loss for epoch 11: 680.350486
	Epoch 12....
Epoch has taken 0:00:22.759156
Number of used sentences in train = 359
Total loss for epoch 12: 673.081434
	Epoch 13....
Epoch has taken 0:00:22.726833
Number of used sentences in train = 359
Total loss for epoch 13: 672.088725
	Epoch 14....
Epoch has taken 0:00:22.676277
Number of used sentences in train = 359
Total loss for epoch 14: 675.474506
Epoch has taken 0:00:22.701183

==================================================================================================
	Training time : 1:04:16.803681
==================================================================================================
	Identification : 0.386

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 12, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 30, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 34, 'lstmDropout': 0.3, 'denseActivation': 'tanh', 'wordDim': 66, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5930, 66)
  (lstm): LSTM(96, 34, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15892.723458
validation loss after epoch 0 : 1327.709648
	Epoch 1....
Epoch has taken 0:02:53.931606
Number of used sentences in train = 2811
Total loss for epoch 1: 10491.498308
validation loss after epoch 1 : 1151.907700
	Epoch 2....
Epoch has taken 0:02:57.175404
Number of used sentences in train = 2811
Total loss for epoch 2: 8474.839986
validation loss after epoch 2 : 1109.942988
	Epoch 3....
Epoch has taken 0:02:56.351169
Number of used sentences in train = 2811
Total loss for epoch 3: 7545.144256
validation loss after epoch 3 : 1147.608150
	Epoch 4....
Epoch has taken 0:02:53.131748
Number of used sentences in train = 2811
Total loss for epoch 4: 6879.971905
validation loss after epoch 4 : 1139.014813
	Epoch 5....
Epoch has taken 0:02:54.060564
Number of used sentences in train = 2811
Total loss for epoch 5: 6380.146044
validation loss after epoch 5 : 1191.372436
	Epoch 6....
Epoch has taken 0:02:54.167025
Number of used sentences in train = 2811
Total loss for epoch 6: 5947.437741
validation loss after epoch 6 : 1237.479173
	Epoch 7....
Epoch has taken 0:02:53.304294
Number of used sentences in train = 2811
Total loss for epoch 7: 5723.811562
validation loss after epoch 7 : 1250.475381
	Epoch 8....
Epoch has taken 0:02:52.471736
Number of used sentences in train = 2811
Total loss for epoch 8: 5533.382783
validation loss after epoch 8 : 1257.813929
	Epoch 9....
Epoch has taken 0:02:52.718800
Number of used sentences in train = 2811
Total loss for epoch 9: 5410.566447
validation loss after epoch 9 : 1325.497688
	Epoch 10....
Epoch has taken 0:02:54.272239
Number of used sentences in train = 2811
Total loss for epoch 10: 5310.346243
validation loss after epoch 10 : 1285.848848
	Epoch 11....
Epoch has taken 0:02:53.985404
Number of used sentences in train = 2811
Total loss for epoch 11: 5149.150945
validation loss after epoch 11 : 1348.386508
	Epoch 12....
Epoch has taken 0:02:55.425665
Number of used sentences in train = 2811
Total loss for epoch 12: 4996.452459
validation loss after epoch 12 : 1438.554439
	Epoch 13....
Epoch has taken 0:03:04.257702
Number of used sentences in train = 2811
Total loss for epoch 13: 4967.599514
validation loss after epoch 13 : 1345.504518
	Epoch 14....
Epoch has taken 0:03:04.430757
Number of used sentences in train = 2811
Total loss for epoch 14: 4937.616866
validation loss after epoch 14 : 1424.195882
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5930, 66)
  (lstm): LSTM(96, 34, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:49.244390
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2322.038934
	Epoch 1....
Epoch has taken 0:00:17.969123
Number of used sentences in train = 313
Total loss for epoch 1: 912.346639
	Epoch 2....
Epoch has taken 0:00:17.968402
Number of used sentences in train = 313
Total loss for epoch 2: 762.747540
	Epoch 3....
Epoch has taken 0:00:17.983889
Number of used sentences in train = 313
Total loss for epoch 3: 671.874447
	Epoch 4....
Epoch has taken 0:00:17.994093
Number of used sentences in train = 313
Total loss for epoch 4: 594.931676
	Epoch 5....
Epoch has taken 0:00:18.411356
Number of used sentences in train = 313
Total loss for epoch 5: 571.269182
	Epoch 6....
Epoch has taken 0:00:18.985395
Number of used sentences in train = 313
Total loss for epoch 6: 570.842581
	Epoch 7....
Epoch has taken 0:00:19.817687
Number of used sentences in train = 313
Total loss for epoch 7: 556.142989
	Epoch 8....
Epoch has taken 0:00:19.664133
Number of used sentences in train = 313
Total loss for epoch 8: 553.723617
	Epoch 9....
Epoch has taken 0:00:19.814529
Number of used sentences in train = 313
Total loss for epoch 9: 537.348292
	Epoch 10....
Epoch has taken 0:00:19.907274
Number of used sentences in train = 313
Total loss for epoch 10: 540.452106
	Epoch 11....
Epoch has taken 0:00:19.631882
Number of used sentences in train = 313
Total loss for epoch 11: 523.562752
	Epoch 12....
Epoch has taken 0:00:19.556073
Number of used sentences in train = 313
Total loss for epoch 12: 528.491110
	Epoch 13....
Epoch has taken 0:00:20.653486
Number of used sentences in train = 313
Total loss for epoch 13: 534.684906
	Epoch 14....
Epoch has taken 0:00:19.669033
Number of used sentences in train = 313
Total loss for epoch 14: 523.231431
Epoch has taken 0:00:19.735138

==================================================================================================
	Training time : 0:48:37.197670
==================================================================================================
	Identification : 0.189

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5677, 66)
  (lstm): LSTM(96, 34, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11405.566166
validation loss after epoch 0 : 954.227572
	Epoch 1....
Epoch has taken 0:02:05.410987
Number of used sentences in train = 2074
Total loss for epoch 1: 7053.011272
validation loss after epoch 1 : 879.568085
	Epoch 2....
Epoch has taken 0:02:05.478118
Number of used sentences in train = 2074
Total loss for epoch 2: 5948.498562
validation loss after epoch 2 : 893.941260
	Epoch 3....
Epoch has taken 0:02:06.976113
Number of used sentences in train = 2074
Total loss for epoch 3: 5280.130839
validation loss after epoch 3 : 834.980065
	Epoch 4....
Epoch has taken 0:02:11.700372
Number of used sentences in train = 2074
Total loss for epoch 4: 4816.273652
validation loss after epoch 4 : 872.393329
	Epoch 5....
Epoch has taken 0:02:15.391376
Number of used sentences in train = 2074
Total loss for epoch 5: 4438.991312
validation loss after epoch 5 : 973.555831
	Epoch 6....
Epoch has taken 0:02:10.638956
Number of used sentences in train = 2074
Total loss for epoch 6: 4245.202439
validation loss after epoch 6 : 983.274206
	Epoch 7....
Epoch has taken 0:02:06.332392
Number of used sentences in train = 2074
Total loss for epoch 7: 4047.943321
validation loss after epoch 7 : 1032.574890
	Epoch 8....
Epoch has taken 0:02:07.483835
Number of used sentences in train = 2074
Total loss for epoch 8: 3965.712295
validation loss after epoch 8 : 991.829001
	Epoch 9....
Epoch has taken 0:02:07.019889
Number of used sentences in train = 2074
Total loss for epoch 9: 3829.162852
validation loss after epoch 9 : 1140.379508
	Epoch 10....
Epoch has taken 0:02:06.831449
Number of used sentences in train = 2074
Total loss for epoch 10: 3707.755537
validation loss after epoch 10 : 1082.626899
	Epoch 11....
Epoch has taken 0:02:06.136065
Number of used sentences in train = 2074
Total loss for epoch 11: 3645.952886
validation loss after epoch 11 : 1110.476709
	Epoch 12....
Epoch has taken 0:02:15.302682
Number of used sentences in train = 2074
Total loss for epoch 12: 3604.726364
validation loss after epoch 12 : 1193.186240
	Epoch 13....
Epoch has taken 0:02:10.969268
Number of used sentences in train = 2074
Total loss for epoch 13: 3570.544149
validation loss after epoch 13 : 1081.999154
	Epoch 14....
Epoch has taken 0:02:13.739744
Number of used sentences in train = 2074
Total loss for epoch 14: 3539.274541
validation loss after epoch 14 : 1115.554835
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5677, 66)
  (lstm): LSTM(96, 34, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:11.560276
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1463.850473
	Epoch 1....
Epoch has taken 0:00:14.069756
Number of used sentences in train = 231
Total loss for epoch 1: 669.676824
	Epoch 2....
Epoch has taken 0:00:13.902759
Number of used sentences in train = 231
Total loss for epoch 2: 539.380152
	Epoch 3....
Epoch has taken 0:00:13.893100
Number of used sentences in train = 231
Total loss for epoch 3: 456.017001
	Epoch 4....
Epoch has taken 0:00:14.131562
Number of used sentences in train = 231
Total loss for epoch 4: 447.306945
	Epoch 5....
Epoch has taken 0:00:13.965303
Number of used sentences in train = 231
Total loss for epoch 5: 392.795767
	Epoch 6....
Epoch has taken 0:00:14.250523
Number of used sentences in train = 231
Total loss for epoch 6: 384.300326
	Epoch 7....
Epoch has taken 0:00:14.025858
Number of used sentences in train = 231
Total loss for epoch 7: 373.785159
	Epoch 8....
Epoch has taken 0:00:13.943563
Number of used sentences in train = 231
Total loss for epoch 8: 368.874283
	Epoch 9....
Epoch has taken 0:00:13.833100
Number of used sentences in train = 231
Total loss for epoch 9: 364.627584
	Epoch 10....
Epoch has taken 0:00:13.305483
Number of used sentences in train = 231
Total loss for epoch 10: 359.598614
	Epoch 11....
Epoch has taken 0:00:12.956226
Number of used sentences in train = 231
Total loss for epoch 11: 357.730803
	Epoch 12....
Epoch has taken 0:00:13.154408
Number of used sentences in train = 231
Total loss for epoch 12: 356.365765
	Epoch 13....
Epoch has taken 0:00:13.086777
Number of used sentences in train = 231
Total loss for epoch 13: 369.576730
	Epoch 14....
Epoch has taken 0:00:12.905850
Number of used sentences in train = 231
Total loss for epoch 14: 357.091443
Epoch has taken 0:00:13.085883

==================================================================================================
	Training time : 0:35:45.823091
==================================================================================================
	Identification : 0.369

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(6838, 66)
  (lstm): LSTM(96, 34, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18817.774218
validation loss after epoch 0 : 1704.624327
	Epoch 1....
Epoch has taken 0:04:01.268527
Number of used sentences in train = 3226
Total loss for epoch 1: 12988.071287
validation loss after epoch 1 : 1441.504788
	Epoch 2....
Epoch has taken 0:03:46.928168
Number of used sentences in train = 3226
Total loss for epoch 2: 11379.997571
validation loss after epoch 2 : 1424.703395
	Epoch 3....
Epoch has taken 0:03:56.199425
Number of used sentences in train = 3226
Total loss for epoch 3: 10281.703307
validation loss after epoch 3 : 1400.015474
	Epoch 4....
Epoch has taken 0:04:03.345166
Number of used sentences in train = 3226
Total loss for epoch 4: 9430.010369
validation loss after epoch 4 : 1441.818471
	Epoch 5....
Epoch has taken 0:04:02.021930
Number of used sentences in train = 3226
Total loss for epoch 5: 8934.949574
validation loss after epoch 5 : 1473.640903
	Epoch 6....
Epoch has taken 0:04:19.015078
Number of used sentences in train = 3226
Total loss for epoch 6: 8442.691011
validation loss after epoch 6 : 1605.330523
	Epoch 7....
Epoch has taken 0:04:22.925580
Number of used sentences in train = 3226
Total loss for epoch 7: 8091.692812
validation loss after epoch 7 : 1586.569663
	Epoch 8....
Epoch has taken 0:04:07.684957
Number of used sentences in train = 3226
Total loss for epoch 8: 7815.369662
validation loss after epoch 8 : 1734.744481
	Epoch 9....
Epoch has taken 0:04:22.319242
Number of used sentences in train = 3226
Total loss for epoch 9: 7562.331479
validation loss after epoch 9 : 1671.192241
	Epoch 10....
Epoch has taken 0:04:19.652768
Number of used sentences in train = 3226
Total loss for epoch 10: 7488.652301
validation loss after epoch 10 : 1753.270361
	Epoch 11....
Epoch has taken 0:04:02.341640
Number of used sentences in train = 3226
Total loss for epoch 11: 7279.133497
validation loss after epoch 11 : 1810.489892
	Epoch 12....
Epoch has taken 0:04:19.196349
Number of used sentences in train = 3226
Total loss for epoch 12: 7087.360742
validation loss after epoch 12 : 1915.458642
	Epoch 13....
Epoch has taken 0:04:27.375621
Number of used sentences in train = 3226
Total loss for epoch 13: 6961.597326
validation loss after epoch 13 : 1928.071312
	Epoch 14....
Epoch has taken 0:04:11.963529
Number of used sentences in train = 3226
Total loss for epoch 14: 6889.279005
validation loss after epoch 14 : 1950.169459
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(6838, 66)
  (lstm): LSTM(96, 34, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:25.206675
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2163.523920
	Epoch 1....
Epoch has taken 0:00:24.668816
Number of used sentences in train = 359
Total loss for epoch 1: 1220.208202
	Epoch 2....
Epoch has taken 0:00:26.356677
Number of used sentences in train = 359
Total loss for epoch 2: 1019.982787
	Epoch 3....
Epoch has taken 0:00:26.132319
Number of used sentences in train = 359
Total loss for epoch 3: 887.078739
	Epoch 4....
Epoch has taken 0:00:25.992500
Number of used sentences in train = 359
Total loss for epoch 4: 817.562713
	Epoch 5....
Epoch has taken 0:00:26.387053
Number of used sentences in train = 359
Total loss for epoch 5: 783.074439
	Epoch 6....
Epoch has taken 0:00:26.442188
Number of used sentences in train = 359
Total loss for epoch 6: 738.785301
	Epoch 7....
Epoch has taken 0:00:26.504659
Number of used sentences in train = 359
Total loss for epoch 7: 735.003600
	Epoch 8....
Epoch has taken 0:00:26.463794
Number of used sentences in train = 359
Total loss for epoch 8: 704.830316
	Epoch 9....
Epoch has taken 0:00:26.580660
Number of used sentences in train = 359
Total loss for epoch 9: 725.770027
	Epoch 10....
Epoch has taken 0:00:26.448614
Number of used sentences in train = 359
Total loss for epoch 10: 710.771102
	Epoch 11....
Epoch has taken 0:00:26.285084
Number of used sentences in train = 359
Total loss for epoch 11: 713.786506
	Epoch 12....
Epoch has taken 0:00:26.109184
Number of used sentences in train = 359
Total loss for epoch 12: 686.919662
	Epoch 13....
Epoch has taken 0:00:26.248741
Number of used sentences in train = 359
Total loss for epoch 13: 688.708627
	Epoch 14....
Epoch has taken 0:00:26.306104
Number of used sentences in train = 359
Total loss for epoch 14: 688.170777
Epoch has taken 0:00:26.213394

==================================================================================================
	Training time : 1:09:21.243073
==================================================================================================
	Identification : 0.45

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 118, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 29, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 28, 'lstmDropout': 0.24, 'denseActivation': 'tanh', 'wordDim': 90, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 29)
  (w_embeddings): Embedding(1177, 90)
  (lstm): LSTM(119, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=118, bias=True)
  (linear2): Linear(in_features=118, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11013.246019
validation loss after epoch 0 : 916.749253
	Epoch 1....
Epoch has taken 0:03:06.309452
Number of used sentences in train = 2811
Total loss for epoch 1: 7770.221764
validation loss after epoch 1 : 947.944617
	Epoch 2....
Epoch has taken 0:03:06.807974
Number of used sentences in train = 2811
Total loss for epoch 2: 6792.444160
validation loss after epoch 2 : 942.033505
	Epoch 3....
Epoch has taken 0:03:05.708435
Number of used sentences in train = 2811
Total loss for epoch 3: 6145.545255
validation loss after epoch 3 : 951.674738
	Epoch 4....
Epoch has taken 0:03:05.412729
Number of used sentences in train = 2811
Total loss for epoch 4: 5731.421780
validation loss after epoch 4 : 1020.258765
	Epoch 5....
Epoch has taken 0:02:56.464031
Number of used sentences in train = 2811
Total loss for epoch 5: 5430.688015
validation loss after epoch 5 : 1032.610234
	Epoch 6....
Epoch has taken 0:02:55.115173
Number of used sentences in train = 2811
Total loss for epoch 6: 5181.124401
validation loss after epoch 6 : 1099.082322
	Epoch 7....
Epoch has taken 0:03:03.772342
Number of used sentences in train = 2811
Total loss for epoch 7: 5039.333878
validation loss after epoch 7 : 1114.662998
	Epoch 8....
Epoch has taken 0:02:55.010333
Number of used sentences in train = 2811
Total loss for epoch 8: 4894.156903
validation loss after epoch 8 : 1159.914928
	Epoch 9....
Epoch has taken 0:02:47.200959
Number of used sentences in train = 2811
Total loss for epoch 9: 4820.727292
validation loss after epoch 9 : 1193.524968
	Epoch 10....
Epoch has taken 0:02:54.185351
Number of used sentences in train = 2811
Total loss for epoch 10: 4736.391257
validation loss after epoch 10 : 1229.383578
	Epoch 11....
Epoch has taken 0:02:54.522960
Number of used sentences in train = 2811
Total loss for epoch 11: 4677.369762
validation loss after epoch 11 : 1262.016250
	Epoch 12....
Epoch has taken 0:02:54.138660
Number of used sentences in train = 2811
Total loss for epoch 12: 4636.989983
validation loss after epoch 12 : 1293.223366
	Epoch 13....
Epoch has taken 0:02:58.393252
Number of used sentences in train = 2811
Total loss for epoch 13: 4601.240253
validation loss after epoch 13 : 1328.350666
	Epoch 14....
Epoch has taken 0:03:06.332318
Number of used sentences in train = 2811
Total loss for epoch 14: 4567.283377
validation loss after epoch 14 : 1337.564936
	TransitionClassifier(
  (p_embeddings): Embedding(18, 29)
  (w_embeddings): Embedding(1177, 90)
  (lstm): LSTM(119, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=118, bias=True)
  (linear2): Linear(in_features=118, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:59.268898
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1599.133298
	Epoch 1....
Epoch has taken 0:00:18.554243
Number of used sentences in train = 313
Total loss for epoch 1: 714.564734
	Epoch 2....
Epoch has taken 0:00:18.632920
Number of used sentences in train = 313
Total loss for epoch 2: 590.996274
	Epoch 3....
Epoch has taken 0:00:18.136198
Number of used sentences in train = 313
Total loss for epoch 3: 552.722184
	Epoch 4....
Epoch has taken 0:00:18.546414
Number of used sentences in train = 313
Total loss for epoch 4: 534.268850
	Epoch 5....
Epoch has taken 0:00:18.004585
Number of used sentences in train = 313
Total loss for epoch 5: 520.991444
	Epoch 6....
Epoch has taken 0:00:18.640940
Number of used sentences in train = 313
Total loss for epoch 6: 514.016349
	Epoch 7....
Epoch has taken 0:00:18.645935
Number of used sentences in train = 313
Total loss for epoch 7: 510.188281
	Epoch 8....
Epoch has taken 0:00:18.628542
Number of used sentences in train = 313
Total loss for epoch 8: 509.303516
	Epoch 9....
Epoch has taken 0:00:18.638885
Number of used sentences in train = 313
Total loss for epoch 9: 506.377963
	Epoch 10....
Epoch has taken 0:00:18.645276
Number of used sentences in train = 313
Total loss for epoch 10: 506.742299
	Epoch 11....
Epoch has taken 0:00:18.659121
Number of used sentences in train = 313
Total loss for epoch 11: 505.144037
	Epoch 12....
Epoch has taken 0:00:18.569279
Number of used sentences in train = 313
Total loss for epoch 12: 504.176989
	Epoch 13....
Epoch has taken 0:00:18.631554
Number of used sentences in train = 313
Total loss for epoch 13: 503.975947
	Epoch 14....
Epoch has taken 0:00:18.423452
Number of used sentences in train = 313
Total loss for epoch 14: 505.508858
Epoch has taken 0:00:18.561804

==================================================================================================
	Training time : 0:49:27.062228
==================================================================================================
	Identification : 0.298

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 29)
  (w_embeddings): Embedding(1133, 90)
  (lstm): LSTM(119, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=118, bias=True)
  (linear2): Linear(in_features=118, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8965.426792
validation loss after epoch 0 : 754.536301
	Epoch 1....
Epoch has taken 0:02:00.512333
Number of used sentences in train = 2074
Total loss for epoch 1: 5385.910100
validation loss after epoch 1 : 705.937654
	Epoch 2....
Epoch has taken 0:02:00.417338
Number of used sentences in train = 2074
Total loss for epoch 2: 4603.365677
validation loss after epoch 2 : 725.310603
	Epoch 3....
Epoch has taken 0:02:00.618339
Number of used sentences in train = 2074
Total loss for epoch 3: 4068.203897
validation loss after epoch 3 : 741.060706
	Epoch 4....
Epoch has taken 0:01:59.666774
Number of used sentences in train = 2074
Total loss for epoch 4: 3721.625849
validation loss after epoch 4 : 776.405942
	Epoch 5....
Epoch has taken 0:01:59.634864
Number of used sentences in train = 2074
Total loss for epoch 5: 3500.904522
validation loss after epoch 5 : 796.932509
	Epoch 6....
Epoch has taken 0:01:59.685596
Number of used sentences in train = 2074
Total loss for epoch 6: 3368.546882
validation loss after epoch 6 : 832.419254
	Epoch 7....
Epoch has taken 0:01:59.403688
Number of used sentences in train = 2074
Total loss for epoch 7: 3266.698792
validation loss after epoch 7 : 856.686608
	Epoch 8....
Epoch has taken 0:02:00.970335
Number of used sentences in train = 2074
Total loss for epoch 8: 3229.393242
validation loss after epoch 8 : 884.196379
	Epoch 9....
Epoch has taken 0:01:59.324585
Number of used sentences in train = 2074
Total loss for epoch 9: 3201.246208
validation loss after epoch 9 : 892.837107
	Epoch 10....
Epoch has taken 0:02:00.065359
Number of used sentences in train = 2074
Total loss for epoch 10: 3187.174653
validation loss after epoch 10 : 902.883297
	Epoch 11....
Epoch has taken 0:01:57.672926
Number of used sentences in train = 2074
Total loss for epoch 11: 3175.980224
validation loss after epoch 11 : 918.972073
	Epoch 12....
Epoch has taken 0:01:56.993945
Number of used sentences in train = 2074
Total loss for epoch 12: 3171.307153
validation loss after epoch 12 : 923.836703
	Epoch 13....
Epoch has taken 0:01:57.665796
Number of used sentences in train = 2074
Total loss for epoch 13: 3166.595467
validation loss after epoch 13 : 936.146399
	Epoch 14....
Epoch has taken 0:01:58.630512
Number of used sentences in train = 2074
Total loss for epoch 14: 3165.101669
validation loss after epoch 14 : 945.851371
	TransitionClassifier(
  (p_embeddings): Embedding(18, 29)
  (w_embeddings): Embedding(1133, 90)
  (lstm): LSTM(119, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=118, bias=True)
  (linear2): Linear(in_features=118, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:59.441105
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1945.609005
	Epoch 1....
Epoch has taken 0:00:12.164863
Number of used sentences in train = 231
Total loss for epoch 1: 491.274482
	Epoch 2....
Epoch has taken 0:00:12.191823
Number of used sentences in train = 231
Total loss for epoch 2: 398.605047
	Epoch 3....
Epoch has taken 0:00:12.122948
Number of used sentences in train = 231
Total loss for epoch 3: 366.418842
	Epoch 4....
Epoch has taken 0:00:12.283588
Number of used sentences in train = 231
Total loss for epoch 4: 353.592984
	Epoch 5....
Epoch has taken 0:00:12.127563
Number of used sentences in train = 231
Total loss for epoch 5: 350.051978
	Epoch 6....
Epoch has taken 0:00:12.273311
Number of used sentences in train = 231
Total loss for epoch 6: 348.477257
	Epoch 7....
Epoch has taken 0:00:12.275144
Number of used sentences in train = 231
Total loss for epoch 7: 347.544587
	Epoch 8....
Epoch has taken 0:00:12.185448
Number of used sentences in train = 231
Total loss for epoch 8: 346.928304
	Epoch 9....
Epoch has taken 0:00:12.122166
Number of used sentences in train = 231
Total loss for epoch 9: 346.525495
	Epoch 10....
Epoch has taken 0:00:12.191346
Number of used sentences in train = 231
Total loss for epoch 10: 346.187730
	Epoch 11....
Epoch has taken 0:00:12.149045
Number of used sentences in train = 231
Total loss for epoch 11: 345.933498
	Epoch 12....
Epoch has taken 0:00:12.298263
Number of used sentences in train = 231
Total loss for epoch 12: 345.731956
	Epoch 13....
Epoch has taken 0:00:12.107831
Number of used sentences in train = 231
Total loss for epoch 13: 345.566196
	Epoch 14....
Epoch has taken 0:00:12.231700
Number of used sentences in train = 231
Total loss for epoch 14: 345.417256
Epoch has taken 0:00:12.170306

==================================================================================================
	Training time : 0:32:53.934322
==================================================================================================
	Identification : 0.446

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 29)
  (w_embeddings): Embedding(1202, 90)
  (lstm): LSTM(119, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=118, bias=True)
  (linear2): Linear(in_features=118, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15188.088200
validation loss after epoch 0 : 1335.920745
	Epoch 1....
Epoch has taken 0:03:48.400727
Number of used sentences in train = 3226
Total loss for epoch 1: 11197.324453
validation loss after epoch 1 : 1329.619629
	Epoch 2....
Epoch has taken 0:03:56.192717
Number of used sentences in train = 3226
Total loss for epoch 2: 10116.214341
validation loss after epoch 2 : 1325.907373
	Epoch 3....
Epoch has taken 0:03:47.939046
Number of used sentences in train = 3226
Total loss for epoch 3: 9380.166950
validation loss after epoch 3 : 1419.076152
	Epoch 4....
Epoch has taken 0:03:42.097531
Number of used sentences in train = 3226
Total loss for epoch 4: 8849.625360
validation loss after epoch 4 : 1454.306873
	Epoch 5....
Epoch has taken 0:03:46.692662
Number of used sentences in train = 3226
Total loss for epoch 5: 8440.568059
validation loss after epoch 5 : 1499.164087
	Epoch 6....
Epoch has taken 0:03:49.366166
Number of used sentences in train = 3226
Total loss for epoch 6: 8075.031671
validation loss after epoch 6 : 1601.454565
	Epoch 7....
Epoch has taken 0:03:33.089481
Number of used sentences in train = 3226
Total loss for epoch 7: 7806.852439
validation loss after epoch 7 : 1642.279529
	Epoch 8....
Epoch has taken 0:03:33.439361
Number of used sentences in train = 3226
Total loss for epoch 8: 7513.476480
validation loss after epoch 8 : 1685.443757
	Epoch 9....
Epoch has taken 0:03:45.742519
Number of used sentences in train = 3226
Total loss for epoch 9: 7299.947187
validation loss after epoch 9 : 1783.233395
	Epoch 10....
Epoch has taken 0:03:38.852836
Number of used sentences in train = 3226
Total loss for epoch 10: 7126.739269
validation loss after epoch 10 : 1861.731306
	Epoch 11....
Epoch has taken 0:03:50.672020
Number of used sentences in train = 3226
Total loss for epoch 11: 7018.578509
validation loss after epoch 11 : 1877.905165
	Epoch 12....
Epoch has taken 0:03:49.684793
Number of used sentences in train = 3226
Total loss for epoch 12: 6855.184612
validation loss after epoch 12 : 1947.571866
	Epoch 13....
Epoch has taken 0:03:32.459280
Number of used sentences in train = 3226
Total loss for epoch 13: 6739.687575
validation loss after epoch 13 : 2069.405633
	Epoch 14....
Epoch has taken 0:04:07.400435
Number of used sentences in train = 3226
Total loss for epoch 14: 6646.179627
validation loss after epoch 14 : 2203.429965
	TransitionClassifier(
  (p_embeddings): Embedding(13, 29)
  (w_embeddings): Embedding(1202, 90)
  (lstm): LSTM(119, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=118, bias=True)
  (linear2): Linear(in_features=118, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:31.461797
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2429.026142
	Epoch 1....
Epoch has taken 0:00:20.682732
Number of used sentences in train = 359
Total loss for epoch 1: 1088.323893
	Epoch 2....
Epoch has taken 0:00:20.669815
Number of used sentences in train = 359
Total loss for epoch 2: 903.109107
	Epoch 3....
Epoch has taken 0:00:20.705877
Number of used sentences in train = 359
Total loss for epoch 3: 803.929867
	Epoch 4....
Epoch has taken 0:00:20.654894
Number of used sentences in train = 359
Total loss for epoch 4: 744.497763
	Epoch 5....
Epoch has taken 0:00:20.690165
Number of used sentences in train = 359
Total loss for epoch 5: 721.153597
	Epoch 6....
Epoch has taken 0:00:20.686360
Number of used sentences in train = 359
Total loss for epoch 6: 707.077776
	Epoch 7....
Epoch has taken 0:00:20.675896
Number of used sentences in train = 359
Total loss for epoch 7: 696.349101
	Epoch 8....
Epoch has taken 0:00:20.669825
Number of used sentences in train = 359
Total loss for epoch 8: 687.657086
	Epoch 9....
Epoch has taken 0:00:20.746774
Number of used sentences in train = 359
Total loss for epoch 9: 681.740017
	Epoch 10....
Epoch has taken 0:00:20.737313
Number of used sentences in train = 359
Total loss for epoch 10: 678.583587
	Epoch 11....
Epoch has taken 0:00:20.761485
Number of used sentences in train = 359
Total loss for epoch 11: 676.390778
	Epoch 12....
Epoch has taken 0:00:20.751110
Number of used sentences in train = 359
Total loss for epoch 12: 675.373119
	Epoch 13....
Epoch has taken 0:00:20.748069
Number of used sentences in train = 359
Total loss for epoch 13: 674.443928
	Epoch 14....
Epoch has taken 0:00:20.755797
Number of used sentences in train = 359
Total loss for epoch 14: 674.169686
Epoch has taken 0:00:20.741963

==================================================================================================
	Training time : 1:01:24.821656
==================================================================================================
	Identification : 0.453

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 138, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 20, 'lstmDropout': 0.2, 'denseActivation': 'tanh', 'wordDim': 118, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5925, 118)
  (lstm): LSTM(135, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=138, bias=True)
  (linear2): Linear(in_features=138, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13911.347086
validation loss after epoch 0 : 1163.915663
	Epoch 1....
Epoch has taken 0:02:39.015387
Number of used sentences in train = 2811
Total loss for epoch 1: 8656.413546
validation loss after epoch 1 : 1134.005174
	Epoch 2....
Epoch has taken 0:02:38.881609
Number of used sentences in train = 2811
Total loss for epoch 2: 6989.110238
validation loss after epoch 2 : 1184.979249
	Epoch 3....
Epoch has taken 0:02:37.483611
Number of used sentences in train = 2811
Total loss for epoch 3: 6046.808303
validation loss after epoch 3 : 1297.293677
	Epoch 4....
Epoch has taken 0:02:37.836012
Number of used sentences in train = 2811
Total loss for epoch 4: 5445.601110
validation loss after epoch 4 : 1379.227547
	Epoch 5....
Epoch has taken 0:02:37.888244
Number of used sentences in train = 2811
Total loss for epoch 5: 5050.992464
validation loss after epoch 5 : 1526.761167
	Epoch 6....
Epoch has taken 0:03:04.173840
Number of used sentences in train = 2811
Total loss for epoch 6: 4837.464936
validation loss after epoch 6 : 1646.614054
	Epoch 7....
Epoch has taken 0:02:38.693694
Number of used sentences in train = 2811
Total loss for epoch 7: 4698.224632
validation loss after epoch 7 : 1709.694495
	Epoch 8....
Epoch has taken 0:02:37.268648
Number of used sentences in train = 2811
Total loss for epoch 8: 4624.094396
validation loss after epoch 8 : 1737.057104
	Epoch 9....
Epoch has taken 0:02:37.754714
Number of used sentences in train = 2811
Total loss for epoch 9: 4580.047423
validation loss after epoch 9 : 1835.327357
	Epoch 10....
Epoch has taken 0:02:37.844774
Number of used sentences in train = 2811
Total loss for epoch 10: 4537.269607
validation loss after epoch 10 : 1873.736840
	Epoch 11....
Epoch has taken 0:02:38.813521
Number of used sentences in train = 2811
Total loss for epoch 11: 4518.774592
validation loss after epoch 11 : 1926.809463
	Epoch 12....
Epoch has taken 0:02:38.185987
Number of used sentences in train = 2811
Total loss for epoch 12: 4516.023880
validation loss after epoch 12 : 1956.312496
	Epoch 13....
Epoch has taken 0:02:38.296477
Number of used sentences in train = 2811
Total loss for epoch 13: 4504.412884
validation loss after epoch 13 : 1976.403976
	Epoch 14....
Epoch has taken 0:02:38.597324
Number of used sentences in train = 2811
Total loss for epoch 14: 4493.477396
validation loss after epoch 14 : 2012.145787
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5925, 118)
  (lstm): LSTM(135, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=138, bias=True)
  (linear2): Linear(in_features=138, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:02.462351
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2185.652298
	Epoch 1....
Epoch has taken 0:00:18.800359
Number of used sentences in train = 313
Total loss for epoch 1: 808.109533
	Epoch 2....
Epoch has taken 0:00:18.619730
Number of used sentences in train = 313
Total loss for epoch 2: 634.571337
	Epoch 3....
Epoch has taken 0:00:18.704341
Number of used sentences in train = 313
Total loss for epoch 3: 552.245407
	Epoch 4....
Epoch has taken 0:00:18.745757
Number of used sentences in train = 313
Total loss for epoch 4: 527.826426
	Epoch 5....
Epoch has taken 0:00:18.882641
Number of used sentences in train = 313
Total loss for epoch 5: 517.528994
	Epoch 6....
Epoch has taken 0:00:18.533764
Number of used sentences in train = 313
Total loss for epoch 6: 511.238786
	Epoch 7....
Epoch has taken 0:00:18.431388
Number of used sentences in train = 313
Total loss for epoch 7: 507.547252
	Epoch 8....
Epoch has taken 0:00:17.662744
Number of used sentences in train = 313
Total loss for epoch 8: 508.507125
	Epoch 9....
Epoch has taken 0:00:18.603351
Number of used sentences in train = 313
Total loss for epoch 9: 504.874470
	Epoch 10....
Epoch has taken 0:00:18.411059
Number of used sentences in train = 313
Total loss for epoch 10: 504.120484
	Epoch 11....
Epoch has taken 0:00:18.582583
Number of used sentences in train = 313
Total loss for epoch 11: 505.638014
	Epoch 12....
Epoch has taken 0:00:18.492581
Number of used sentences in train = 313
Total loss for epoch 12: 503.233536
	Epoch 13....
Epoch has taken 0:00:18.462425
Number of used sentences in train = 313
Total loss for epoch 13: 502.192304
	Epoch 14....
Epoch has taken 0:00:18.474723
Number of used sentences in train = 313
Total loss for epoch 14: 501.622879
Epoch has taken 0:00:18.375161

==================================================================================================
	Training time : 0:45:01.506292
==================================================================================================
	Identification : 0.181

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5622, 118)
  (lstm): LSTM(135, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=138, bias=True)
  (linear2): Linear(in_features=138, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10967.141083
validation loss after epoch 0 : 985.356579
	Epoch 1....
Epoch has taken 0:01:58.708256
Number of used sentences in train = 2074
Total loss for epoch 1: 6076.689664
validation loss after epoch 1 : 989.814124
	Epoch 2....
Epoch has taken 0:01:58.047598
Number of used sentences in train = 2074
Total loss for epoch 2: 4794.867127
validation loss after epoch 2 : 1043.798969
	Epoch 3....
Epoch has taken 0:01:57.970363
Number of used sentences in train = 2074
Total loss for epoch 3: 4033.075162
validation loss after epoch 3 : 1187.978184
	Epoch 4....
Epoch has taken 0:02:09.622865
Number of used sentences in train = 2074
Total loss for epoch 4: 3652.406198
validation loss after epoch 4 : 1270.409932
	Epoch 5....
Epoch has taken 0:02:10.034031
Number of used sentences in train = 2074
Total loss for epoch 5: 3434.781210
validation loss after epoch 5 : 1434.866013
	Epoch 6....
Epoch has taken 0:02:04.108008
Number of used sentences in train = 2074
Total loss for epoch 6: 3322.115865
validation loss after epoch 6 : 1498.158506
	Epoch 7....
Epoch has taken 0:02:07.236104
Number of used sentences in train = 2074
Total loss for epoch 7: 3245.333009
validation loss after epoch 7 : 1590.128101
	Epoch 8....
Epoch has taken 0:02:07.711286
Number of used sentences in train = 2074
Total loss for epoch 8: 3212.226726
validation loss after epoch 8 : 1597.240841
	Epoch 9....
Epoch has taken 0:02:10.050893
Number of used sentences in train = 2074
Total loss for epoch 9: 3189.985575
validation loss after epoch 9 : 1645.097462
	Epoch 10....
Epoch has taken 0:02:09.233329
Number of used sentences in train = 2074
Total loss for epoch 10: 3181.624246
validation loss after epoch 10 : 1680.997681
	Epoch 11....
Epoch has taken 0:02:08.080857
Number of used sentences in train = 2074
Total loss for epoch 11: 3175.354671
validation loss after epoch 11 : 1691.955884
	Epoch 12....
Epoch has taken 0:02:07.442764
Number of used sentences in train = 2074
Total loss for epoch 12: 3172.055720
validation loss after epoch 12 : 1740.228650
	Epoch 13....
Epoch has taken 0:02:08.395151
Number of used sentences in train = 2074
Total loss for epoch 13: 3168.426323
validation loss after epoch 13 : 1742.515316
	Epoch 14....
Epoch has taken 0:02:00.429142
Number of used sentences in train = 2074
Total loss for epoch 14: 3166.239913
validation loss after epoch 14 : 1780.286155
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5622, 118)
  (lstm): LSTM(135, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=138, bias=True)
  (linear2): Linear(in_features=138, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:00.562352
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1986.665974
	Epoch 1....
Epoch has taken 0:00:14.079186
Number of used sentences in train = 231
Total loss for epoch 1: 677.398820
	Epoch 2....
Epoch has taken 0:00:12.413932
Number of used sentences in train = 231
Total loss for epoch 2: 441.259160
	Epoch 3....
Epoch has taken 0:00:12.418335
Number of used sentences in train = 231
Total loss for epoch 3: 382.085972
	Epoch 4....
Epoch has taken 0:00:12.415273
Number of used sentences in train = 231
Total loss for epoch 4: 360.296384
	Epoch 5....
Epoch has taken 0:00:12.422218
Number of used sentences in train = 231
Total loss for epoch 5: 351.786215
	Epoch 6....
Epoch has taken 0:00:12.408475
Number of used sentences in train = 231
Total loss for epoch 6: 349.809458
	Epoch 7....
Epoch has taken 0:00:12.421740
Number of used sentences in train = 231
Total loss for epoch 7: 348.523014
	Epoch 8....
Epoch has taken 0:00:12.878231
Number of used sentences in train = 231
Total loss for epoch 8: 348.029314
	Epoch 9....
Epoch has taken 0:00:12.420832
Number of used sentences in train = 231
Total loss for epoch 9: 347.176661
	Epoch 10....
Epoch has taken 0:00:12.405556
Number of used sentences in train = 231
Total loss for epoch 10: 346.719986
	Epoch 11....
Epoch has taken 0:00:12.417385
Number of used sentences in train = 231
Total loss for epoch 11: 346.210038
	Epoch 12....
Epoch has taken 0:00:12.405730
Number of used sentences in train = 231
Total loss for epoch 12: 345.935853
	Epoch 13....
Epoch has taken 0:00:12.415403
Number of used sentences in train = 231
Total loss for epoch 13: 345.700662
	Epoch 14....
Epoch has taken 0:00:12.406433
Number of used sentences in train = 231
Total loss for epoch 14: 345.518574
Epoch has taken 0:00:12.395974

==================================================================================================
	Training time : 0:34:26.339141
==================================================================================================
	Identification : 0.271

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(6799, 118)
  (lstm): LSTM(135, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=138, bias=True)
  (linear2): Linear(in_features=138, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20635.894279
validation loss after epoch 0 : 1679.287478
	Epoch 1....
Epoch has taken 0:03:53.112154
Number of used sentences in train = 3226
Total loss for epoch 1: 11807.324829
validation loss after epoch 1 : 1537.840111
	Epoch 2....
Epoch has taken 0:03:53.180955
Number of used sentences in train = 3226
Total loss for epoch 2: 10072.762256
validation loss after epoch 2 : 1622.125723
	Epoch 3....
Epoch has taken 0:03:53.705790
Number of used sentences in train = 3226
Total loss for epoch 3: 9022.722823
validation loss after epoch 3 : 1716.946013
	Epoch 4....
Epoch has taken 0:03:54.256946
Number of used sentences in train = 3226
Total loss for epoch 4: 8240.872740
validation loss after epoch 4 : 1868.555428
	Epoch 5....
Epoch has taken 0:04:01.261021
Number of used sentences in train = 3226
Total loss for epoch 5: 7669.892444
validation loss after epoch 5 : 2038.213471
	Epoch 6....
Epoch has taken 0:03:30.896647
Number of used sentences in train = 3226
Total loss for epoch 6: 7243.588360
validation loss after epoch 6 : 2088.098562
	Epoch 7....
Epoch has taken 0:03:29.205567
Number of used sentences in train = 3226
Total loss for epoch 7: 6923.615035
validation loss after epoch 7 : 2228.051754
	Epoch 8....
Epoch has taken 0:03:28.920746
Number of used sentences in train = 3226
Total loss for epoch 8: 6692.939570
validation loss after epoch 8 : 2368.945942
	Epoch 9....
Epoch has taken 0:03:29.925771
Number of used sentences in train = 3226
Total loss for epoch 9: 6561.921460
validation loss after epoch 9 : 2519.939660
	Epoch 10....
Epoch has taken 0:03:44.074964
Number of used sentences in train = 3226
Total loss for epoch 10: 6487.835127
validation loss after epoch 10 : 2713.906987
	Epoch 11....
Epoch has taken 0:03:28.689839
Number of used sentences in train = 3226
Total loss for epoch 11: 6406.972593
validation loss after epoch 11 : 2639.900198
	Epoch 12....
Epoch has taken 0:03:29.098997
Number of used sentences in train = 3226
Total loss for epoch 12: 6363.508800
validation loss after epoch 12 : 2692.581667
	Epoch 13....
Epoch has taken 0:03:29.861955
Number of used sentences in train = 3226
Total loss for epoch 13: 6302.222593
validation loss after epoch 13 : 2736.604695
	Epoch 14....
Epoch has taken 0:03:29.796359
Number of used sentences in train = 3226
Total loss for epoch 14: 6295.536850
validation loss after epoch 14 : 2790.647865
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(6799, 118)
  (lstm): LSTM(135, 20, bidirectional=True)
  (linear1): Linear(in_features=320, out_features=138, bias=True)
  (linear2): Linear(in_features=138, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.416642
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 3054.342526
	Epoch 1....
Epoch has taken 0:00:20.622559
Number of used sentences in train = 359
Total loss for epoch 1: 1157.001091
	Epoch 2....
Epoch has taken 0:00:20.590287
Number of used sentences in train = 359
Total loss for epoch 2: 870.240466
	Epoch 3....
Epoch has taken 0:00:20.622701
Number of used sentences in train = 359
Total loss for epoch 3: 745.668665
	Epoch 4....
Epoch has taken 0:00:20.600352
Number of used sentences in train = 359
Total loss for epoch 4: 702.864535
	Epoch 5....
Epoch has taken 0:00:20.618356
Number of used sentences in train = 359
Total loss for epoch 5: 685.655402
	Epoch 6....
Epoch has taken 0:00:20.584979
Number of used sentences in train = 359
Total loss for epoch 6: 681.472503
	Epoch 7....
Epoch has taken 0:00:20.604624
Number of used sentences in train = 359
Total loss for epoch 7: 679.054821
	Epoch 8....
Epoch has taken 0:00:20.604390
Number of used sentences in train = 359
Total loss for epoch 8: 676.905054
	Epoch 9....
Epoch has taken 0:00:20.584632
Number of used sentences in train = 359
Total loss for epoch 9: 674.627644
	Epoch 10....
Epoch has taken 0:00:20.599550
Number of used sentences in train = 359
Total loss for epoch 10: 674.627353
	Epoch 11....
Epoch has taken 0:00:20.629099
Number of used sentences in train = 359
Total loss for epoch 11: 672.773434
	Epoch 12....
Epoch has taken 0:00:20.615375
Number of used sentences in train = 359
Total loss for epoch 12: 672.184628
	Epoch 13....
Epoch has taken 0:00:20.594782
Number of used sentences in train = 359
Total loss for epoch 13: 671.814437
	Epoch 14....
Epoch has taken 0:00:20.615790
Number of used sentences in train = 359
Total loss for epoch 14: 671.554172
Epoch has taken 0:00:20.619323

==================================================================================================
	Training time : 1:00:09.170801
==================================================================================================
	Identification : 0.395

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 20, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 66, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 31, 'lstmDropout': 0.27, 'denseActivation': 'tanh', 'wordDim': 53, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1882, 53)
  (lstm): LSTM(119, 31, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10495.172896
validation loss after epoch 0 : 885.031993
	Epoch 1....
Epoch has taken 0:02:49.413304
Number of used sentences in train = 2811
Total loss for epoch 1: 7688.848024
validation loss after epoch 1 : 839.924417
	Epoch 2....
Epoch has taken 0:02:49.376692
Number of used sentences in train = 2811
Total loss for epoch 2: 7067.525637
validation loss after epoch 2 : 835.638861
	Epoch 3....
Epoch has taken 0:02:49.691918
Number of used sentences in train = 2811
Total loss for epoch 3: 6589.880030
validation loss after epoch 3 : 834.265048
	Epoch 4....
Epoch has taken 0:02:56.934829
Number of used sentences in train = 2811
Total loss for epoch 4: 6359.880713
validation loss after epoch 4 : 843.816396
	Epoch 5....
Epoch has taken 0:02:49.106640
Number of used sentences in train = 2811
Total loss for epoch 5: 6109.330435
validation loss after epoch 5 : 832.802777
	Epoch 6....
Epoch has taken 0:02:49.370051
Number of used sentences in train = 2811
Total loss for epoch 6: 5896.865801
validation loss after epoch 6 : 834.253996
	Epoch 7....
Epoch has taken 0:02:49.882603
Number of used sentences in train = 2811
Total loss for epoch 7: 5696.695840
validation loss after epoch 7 : 841.134695
	Epoch 8....
Epoch has taken 0:02:49.709535
Number of used sentences in train = 2811
Total loss for epoch 8: 5546.496192
validation loss after epoch 8 : 837.780445
	Epoch 9....
Epoch has taken 0:03:17.036743
Number of used sentences in train = 2811
Total loss for epoch 9: 5492.626527
validation loss after epoch 9 : 861.515877
	Epoch 10....
Epoch has taken 0:02:50.464296
Number of used sentences in train = 2811
Total loss for epoch 10: 5351.179017
validation loss after epoch 10 : 886.779830
	Epoch 11....
Epoch has taken 0:02:49.878323
Number of used sentences in train = 2811
Total loss for epoch 11: 5328.485145
validation loss after epoch 11 : 844.145585
	Epoch 12....
Epoch has taken 0:02:49.095736
Number of used sentences in train = 2811
Total loss for epoch 12: 5134.936479
validation loss after epoch 12 : 901.021519
	Epoch 13....
Epoch has taken 0:02:49.478531
Number of used sentences in train = 2811
Total loss for epoch 13: 5125.996633
validation loss after epoch 13 : 894.996378
	Epoch 14....
Epoch has taken 0:02:49.903160
Number of used sentences in train = 2811
Total loss for epoch 14: 5031.836040
validation loss after epoch 14 : 900.696686
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1882, 53)
  (lstm): LSTM(119, 31, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.572683
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1444.643334
	Epoch 1....
Epoch has taken 0:00:17.995336
Number of used sentences in train = 313
Total loss for epoch 1: 793.301021
	Epoch 2....
Epoch has taken 0:00:18.013702
Number of used sentences in train = 313
Total loss for epoch 2: 665.916928
	Epoch 3....
Epoch has taken 0:00:18.035047
Number of used sentences in train = 313
Total loss for epoch 3: 614.306968
	Epoch 4....
Epoch has taken 0:00:18.050957
Number of used sentences in train = 313
Total loss for epoch 4: 581.224290
	Epoch 5....
Epoch has taken 0:00:18.023837
Number of used sentences in train = 313
Total loss for epoch 5: 571.677817
	Epoch 6....
Epoch has taken 0:00:18.025139
Number of used sentences in train = 313
Total loss for epoch 6: 560.642674
	Epoch 7....
Epoch has taken 0:00:18.025049
Number of used sentences in train = 313
Total loss for epoch 7: 552.223571
	Epoch 8....
Epoch has taken 0:00:18.025274
Number of used sentences in train = 313
Total loss for epoch 8: 548.828457
	Epoch 9....
Epoch has taken 0:00:17.999199
Number of used sentences in train = 313
Total loss for epoch 9: 528.196351
	Epoch 10....
Epoch has taken 0:00:18.038987
Number of used sentences in train = 313
Total loss for epoch 10: 529.405568
	Epoch 11....
Epoch has taken 0:00:17.992325
Number of used sentences in train = 313
Total loss for epoch 11: 531.369448
	Epoch 12....
Epoch has taken 0:00:18.026928
Number of used sentences in train = 313
Total loss for epoch 12: 524.138221
	Epoch 13....
Epoch has taken 0:00:17.997118
Number of used sentences in train = 313
Total loss for epoch 13: 518.745861
	Epoch 14....
Epoch has taken 0:00:17.988240
Number of used sentences in train = 313
Total loss for epoch 14: 520.111703
Epoch has taken 0:00:17.891973

==================================================================================================
	Training time : 0:47:30.560101
==================================================================================================
	Identification : 0.26

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1680, 53)
  (lstm): LSTM(119, 31, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8052.172149
validation loss after epoch 0 : 693.876328
	Epoch 1....
Epoch has taken 0:01:56.568005
Number of used sentences in train = 2074
Total loss for epoch 1: 5561.471829
validation loss after epoch 1 : 642.729046
	Epoch 2....
Epoch has taken 0:01:56.598710
Number of used sentences in train = 2074
Total loss for epoch 2: 5014.926721
validation loss after epoch 2 : 627.960147
	Epoch 3....
Epoch has taken 0:02:15.180444
Number of used sentences in train = 2074
Total loss for epoch 3: 4625.810363
validation loss after epoch 3 : 633.657546
	Epoch 4....
Epoch has taken 0:02:03.899686
Number of used sentences in train = 2074
Total loss for epoch 4: 4324.919868
validation loss after epoch 4 : 651.054216
	Epoch 5....
Epoch has taken 0:02:05.711495
Number of used sentences in train = 2074
Total loss for epoch 5: 4183.943713
validation loss after epoch 5 : 645.360108
	Epoch 6....
Epoch has taken 0:01:57.917081
Number of used sentences in train = 2074
Total loss for epoch 6: 3992.739725
validation loss after epoch 6 : 670.421384
	Epoch 7....
Epoch has taken 0:01:56.931354
Number of used sentences in train = 2074
Total loss for epoch 7: 3819.870496
validation loss after epoch 7 : 720.606527
	Epoch 8....
Epoch has taken 0:02:08.535069
Number of used sentences in train = 2074
Total loss for epoch 8: 3734.571574
validation loss after epoch 8 : 705.312006
	Epoch 9....
Epoch has taken 0:02:08.528379
Number of used sentences in train = 2074
Total loss for epoch 9: 3599.340767
validation loss after epoch 9 : 692.848600
	Epoch 10....
Epoch has taken 0:02:00.208282
Number of used sentences in train = 2074
Total loss for epoch 10: 3574.141141
validation loss after epoch 10 : 674.144274
	Epoch 11....
Epoch has taken 0:02:13.389451
Number of used sentences in train = 2074
Total loss for epoch 11: 3452.367160
validation loss after epoch 11 : 739.517981
	Epoch 12....
Epoch has taken 0:02:09.579415
Number of used sentences in train = 2074
Total loss for epoch 12: 3421.601707
validation loss after epoch 12 : 770.301058
	Epoch 13....
Epoch has taken 0:02:09.698990
Number of used sentences in train = 2074
Total loss for epoch 13: 3432.462575
validation loss after epoch 13 : 723.439861
	Epoch 14....
Epoch has taken 0:02:09.188809
Number of used sentences in train = 2074
Total loss for epoch 14: 3409.077225
validation loss after epoch 14 : 800.954460
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1680, 53)
  (lstm): LSTM(119, 31, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:04.855394
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1360.739082
	Epoch 1....
Epoch has taken 0:00:13.743847
Number of used sentences in train = 231
Total loss for epoch 1: 679.390042
	Epoch 2....
Epoch has taken 0:00:13.721833
Number of used sentences in train = 231
Total loss for epoch 2: 524.548493
	Epoch 3....
Epoch has taken 0:00:13.734273
Number of used sentences in train = 231
Total loss for epoch 3: 455.690318
	Epoch 4....
Epoch has taken 0:00:13.714748
Number of used sentences in train = 231
Total loss for epoch 4: 412.189760
	Epoch 5....
Epoch has taken 0:00:13.730895
Number of used sentences in train = 231
Total loss for epoch 5: 396.636009
	Epoch 6....
Epoch has taken 0:00:13.716738
Number of used sentences in train = 231
Total loss for epoch 6: 385.487280
	Epoch 7....
Epoch has taken 0:00:13.733647
Number of used sentences in train = 231
Total loss for epoch 7: 372.403079
	Epoch 8....
Epoch has taken 0:00:13.733632
Number of used sentences in train = 231
Total loss for epoch 8: 368.325559
	Epoch 9....
Epoch has taken 0:00:13.729177
Number of used sentences in train = 231
Total loss for epoch 9: 360.561366
	Epoch 10....
Epoch has taken 0:00:13.723364
Number of used sentences in train = 231
Total loss for epoch 10: 361.532804
	Epoch 11....
Epoch has taken 0:00:13.717782
Number of used sentences in train = 231
Total loss for epoch 11: 362.598077
	Epoch 12....
Epoch has taken 0:00:11.966508
Number of used sentences in train = 231
Total loss for epoch 12: 354.330979
	Epoch 13....
Epoch has taken 0:00:11.931224
Number of used sentences in train = 231
Total loss for epoch 13: 354.662355
	Epoch 14....
Epoch has taken 0:00:11.951167
Number of used sentences in train = 231
Total loss for epoch 14: 355.341856
Epoch has taken 0:00:11.927360

==================================================================================================
	Training time : 0:34:35.906011
==================================================================================================
	Identification : 0.421

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 66)
  (w_embeddings): Embedding(3369, 53)
  (lstm): LSTM(119, 31, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12557.395748
validation loss after epoch 0 : 1197.576229
	Epoch 1....
Epoch has taken 0:03:44.640167
Number of used sentences in train = 3226
Total loss for epoch 1: 9710.720766
validation loss after epoch 1 : 1116.977075
	Epoch 2....
Epoch has taken 0:03:44.542144
Number of used sentences in train = 3226
Total loss for epoch 2: 9159.000543
validation loss after epoch 2 : 1058.681563
	Epoch 3....
Epoch has taken 0:03:44.975857
Number of used sentences in train = 3226
Total loss for epoch 3: 8761.780036
validation loss after epoch 3 : 1057.880314
	Epoch 4....
Epoch has taken 0:03:49.549226
Number of used sentences in train = 3226
Total loss for epoch 4: 8437.290078
validation loss after epoch 4 : 1043.406567
	Epoch 5....
Epoch has taken 0:03:46.077872
Number of used sentences in train = 3226
Total loss for epoch 5: 8176.416028
validation loss after epoch 5 : 1077.851741
	Epoch 6....
Epoch has taken 0:03:45.333180
Number of used sentences in train = 3226
Total loss for epoch 6: 7882.193195
validation loss after epoch 6 : 1121.899363
	Epoch 7....
Epoch has taken 0:03:44.549422
Number of used sentences in train = 3226
Total loss for epoch 7: 7726.298835
validation loss after epoch 7 : 1138.491807
	Epoch 8....
Epoch has taken 0:03:44.716769
Number of used sentences in train = 3226
Total loss for epoch 8: 7563.723986
validation loss after epoch 8 : 1134.425549
	Epoch 9....
Epoch has taken 0:03:45.457351
Number of used sentences in train = 3226
Total loss for epoch 9: 7389.080059
validation loss after epoch 9 : 1187.881140
	Epoch 10....
Epoch has taken 0:04:21.559304
Number of used sentences in train = 3226
Total loss for epoch 10: 7290.097752
validation loss after epoch 10 : 1215.972623
	Epoch 11....
Epoch has taken 0:03:46.553200
Number of used sentences in train = 3226
Total loss for epoch 11: 7161.982200
validation loss after epoch 11 : 1256.835497
	Epoch 12....
Epoch has taken 0:03:44.223999
Number of used sentences in train = 3226
Total loss for epoch 12: 7099.704079
validation loss after epoch 12 : 1265.174828
	Epoch 13....
Epoch has taken 0:03:44.967614
Number of used sentences in train = 3226
Total loss for epoch 13: 6958.737382
validation loss after epoch 13 : 1287.221162
	Epoch 14....
Epoch has taken 0:03:48.474359
Number of used sentences in train = 3226
Total loss for epoch 14: 6918.879271
validation loss after epoch 14 : 1321.647478
	TransitionClassifier(
  (p_embeddings): Embedding(13, 66)
  (w_embeddings): Embedding(3369, 53)
  (lstm): LSTM(119, 31, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:46.252219
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2173.017003
	Epoch 1....
Epoch has taken 0:00:21.917184
Number of used sentences in train = 359
Total loss for epoch 1: 1006.327330
	Epoch 2....
Epoch has taken 0:00:21.904361
Number of used sentences in train = 359
Total loss for epoch 2: 910.482908
	Epoch 3....
Epoch has taken 0:00:21.918809
Number of used sentences in train = 359
Total loss for epoch 3: 849.599277
	Epoch 4....
Epoch has taken 0:00:21.923924
Number of used sentences in train = 359
Total loss for epoch 4: 813.473943
	Epoch 5....
Epoch has taken 0:00:21.909143
Number of used sentences in train = 359
Total loss for epoch 5: 768.741567
	Epoch 6....
Epoch has taken 0:00:21.919804
Number of used sentences in train = 359
Total loss for epoch 6: 754.882268
	Epoch 7....
Epoch has taken 0:00:21.922456
Number of used sentences in train = 359
Total loss for epoch 7: 754.050436
	Epoch 8....
Epoch has taken 0:00:21.953710
Number of used sentences in train = 359
Total loss for epoch 8: 715.983407
	Epoch 9....
Epoch has taken 0:00:21.950027
Number of used sentences in train = 359
Total loss for epoch 9: 721.051683
	Epoch 10....
Epoch has taken 0:00:21.941951
Number of used sentences in train = 359
Total loss for epoch 10: 710.054199
	Epoch 11....
Epoch has taken 0:00:21.950067
Number of used sentences in train = 359
Total loss for epoch 11: 696.740544
	Epoch 12....
Epoch has taken 0:00:21.954910
Number of used sentences in train = 359
Total loss for epoch 12: 700.786412
	Epoch 13....
Epoch has taken 0:00:21.946856
Number of used sentences in train = 359
Total loss for epoch 13: 689.739906
	Epoch 14....
Epoch has taken 0:00:21.944138
Number of used sentences in train = 359
Total loss for epoch 14: 696.053564
Epoch has taken 0:00:21.932549

==================================================================================================
	Training time : 1:02:31.549070
==================================================================================================
	Identification : 0.413

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 65, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 32, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 67, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 220, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1177, 220)
  (lstm): LSTM(252, 67, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13942.933957
validation loss after epoch 0 : 1599.662203
	Epoch 1....
Epoch has taken 0:02:50.663385
Number of used sentences in train = 2811
Total loss for epoch 1: 8968.774868
validation loss after epoch 1 : 946.444616
	Epoch 2....
Epoch has taken 0:02:50.820951
Number of used sentences in train = 2811
Total loss for epoch 2: 7863.408154
validation loss after epoch 2 : 883.079769
	Epoch 3....
Epoch has taken 0:03:01.460486
Number of used sentences in train = 2811
Total loss for epoch 3: 6954.384193
validation loss after epoch 3 : 864.408823
	Epoch 4....
Epoch has taken 0:02:50.947305
Number of used sentences in train = 2811
Total loss for epoch 4: 6485.179499
validation loss after epoch 4 : 889.228165
	Epoch 5....
Epoch has taken 0:03:08.606032
Number of used sentences in train = 2811
Total loss for epoch 5: 6123.938190
validation loss after epoch 5 : 911.928819
	Epoch 6....
Epoch has taken 0:03:07.152574
Number of used sentences in train = 2811
Total loss for epoch 6: 5893.184184
validation loss after epoch 6 : 908.291877
	Epoch 7....
Epoch has taken 0:03:04.833722
Number of used sentences in train = 2811
Total loss for epoch 7: 5676.879911
validation loss after epoch 7 : 930.086526
	Epoch 8....
Epoch has taken 0:02:50.319126
Number of used sentences in train = 2811
Total loss for epoch 8: 5467.699654
validation loss after epoch 8 : 929.340428
	Epoch 9....
Epoch has taken 0:02:50.455273
Number of used sentences in train = 2811
Total loss for epoch 9: 5328.519945
validation loss after epoch 9 : 944.905468
	Epoch 10....
Epoch has taken 0:03:18.142177
Number of used sentences in train = 2811
Total loss for epoch 10: 5253.163823
validation loss after epoch 10 : 941.584840
	Epoch 11....
Epoch has taken 0:02:51.395406
Number of used sentences in train = 2811
Total loss for epoch 11: 5178.864319
validation loss after epoch 11 : 952.852617
	Epoch 12....
Epoch has taken 0:02:50.606852
Number of used sentences in train = 2811
Total loss for epoch 12: 5112.133247
validation loss after epoch 12 : 980.420305
	Epoch 13....
Epoch has taken 0:02:50.246005
Number of used sentences in train = 2811
Total loss for epoch 13: 4970.963042
validation loss after epoch 13 : 1001.561185
	Epoch 14....
Epoch has taken 0:02:50.687166
Number of used sentences in train = 2811
Total loss for epoch 14: 4933.334172
validation loss after epoch 14 : 1013.589363
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1177, 220)
  (lstm): LSTM(252, 67, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.015980
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1445.527808
	Epoch 1....
Epoch has taken 0:00:18.028768
Number of used sentences in train = 313
Total loss for epoch 1: 752.365305
	Epoch 2....
Epoch has taken 0:00:18.022042
Number of used sentences in train = 313
Total loss for epoch 2: 628.813218
	Epoch 3....
Epoch has taken 0:00:18.048915
Number of used sentences in train = 313
Total loss for epoch 3: 592.970168
	Epoch 4....
Epoch has taken 0:00:18.041530
Number of used sentences in train = 313
Total loss for epoch 4: 557.227729
	Epoch 5....
Epoch has taken 0:00:18.047714
Number of used sentences in train = 313
Total loss for epoch 5: 539.009207
	Epoch 6....
Epoch has taken 0:00:18.061265
Number of used sentences in train = 313
Total loss for epoch 6: 531.937788
	Epoch 7....
Epoch has taken 0:00:18.053440
Number of used sentences in train = 313
Total loss for epoch 7: 517.743632
	Epoch 8....
Epoch has taken 0:00:18.069015
Number of used sentences in train = 313
Total loss for epoch 8: 517.129476
	Epoch 9....
Epoch has taken 0:00:18.039854
Number of used sentences in train = 313
Total loss for epoch 9: 507.631992
	Epoch 10....
Epoch has taken 0:00:18.063791
Number of used sentences in train = 313
Total loss for epoch 10: 509.225214
	Epoch 11....
Epoch has taken 0:00:18.045889
Number of used sentences in train = 313
Total loss for epoch 11: 508.350060
	Epoch 12....
Epoch has taken 0:00:18.066031
Number of used sentences in train = 313
Total loss for epoch 12: 522.383775
	Epoch 13....
Epoch has taken 0:00:18.038465
Number of used sentences in train = 313
Total loss for epoch 13: 517.595603
	Epoch 14....
Epoch has taken 0:00:18.076816
Number of used sentences in train = 313
Total loss for epoch 14: 510.868573
Epoch has taken 0:00:18.076520

==================================================================================================
	Training time : 0:48:37.644434
==================================================================================================
	Identification : 0.194

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1133, 220)
  (lstm): LSTM(252, 67, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9331.301337
validation loss after epoch 0 : 730.322404
	Epoch 1....
Epoch has taken 0:02:15.308000
Number of used sentences in train = 2074
Total loss for epoch 1: 5702.348501
validation loss after epoch 1 : 692.368089
	Epoch 2....
Epoch has taken 0:02:15.282868
Number of used sentences in train = 2074
Total loss for epoch 2: 4848.348488
validation loss after epoch 2 : 677.922974
	Epoch 3....
Epoch has taken 0:01:56.939106
Number of used sentences in train = 2074
Total loss for epoch 3: 4323.243734
validation loss after epoch 3 : 663.861647
	Epoch 4....
Epoch has taken 0:01:56.160474
Number of used sentences in train = 2074
Total loss for epoch 4: 3968.895104
validation loss after epoch 4 : 683.478584
	Epoch 5....
Epoch has taken 0:01:56.608364
Number of used sentences in train = 2074
Total loss for epoch 5: 3723.436705
validation loss after epoch 5 : 782.590026
	Epoch 6....
Epoch has taken 0:02:15.277740
Number of used sentences in train = 2074
Total loss for epoch 6: 3622.639673
validation loss after epoch 6 : 749.185673
	Epoch 7....
Epoch has taken 0:01:57.192802
Number of used sentences in train = 2074
Total loss for epoch 7: 3547.604553
validation loss after epoch 7 : 808.081114
	Epoch 8....
Epoch has taken 0:01:56.225366
Number of used sentences in train = 2074
Total loss for epoch 8: 3415.983489
validation loss after epoch 8 : 790.430895
	Epoch 9....
Epoch has taken 0:01:56.503384
Number of used sentences in train = 2074
Total loss for epoch 9: 3337.938506
validation loss after epoch 9 : 850.265560
	Epoch 10....
Epoch has taken 0:01:56.678394
Number of used sentences in train = 2074
Total loss for epoch 10: 3319.230279
validation loss after epoch 10 : 826.721024
	Epoch 11....
Epoch has taken 0:01:56.533457
Number of used sentences in train = 2074
Total loss for epoch 11: 3304.511669
validation loss after epoch 11 : 779.749385
	Epoch 12....
Epoch has taken 0:01:57.176309
Number of used sentences in train = 2074
Total loss for epoch 12: 3286.958781
validation loss after epoch 12 : 818.827289
	Epoch 13....
Epoch has taken 0:01:56.214263
Number of used sentences in train = 2074
Total loss for epoch 13: 3282.076478
validation loss after epoch 13 : 910.819204
	Epoch 14....
Epoch has taken 0:01:56.711129
Number of used sentences in train = 2074
Total loss for epoch 14: 3262.840499
validation loss after epoch 14 : 935.712432
	TransitionClassifier(
  (p_embeddings): Embedding(18, 32)
  (w_embeddings): Embedding(1133, 220)
  (lstm): LSTM(252, 67, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.695523
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1455.840442
	Epoch 1....
Epoch has taken 0:00:11.996273
Number of used sentences in train = 231
Total loss for epoch 1: 565.156695
	Epoch 2....
Epoch has taken 0:00:11.969629
Number of used sentences in train = 231
Total loss for epoch 2: 447.913142
	Epoch 3....
Epoch has taken 0:00:11.978070
Number of used sentences in train = 231
Total loss for epoch 3: 396.142083
	Epoch 4....
Epoch has taken 0:00:11.985865
Number of used sentences in train = 231
Total loss for epoch 4: 375.211367
	Epoch 5....
Epoch has taken 0:00:11.989987
Number of used sentences in train = 231
Total loss for epoch 5: 365.890161
	Epoch 6....
Epoch has taken 0:00:11.979267
Number of used sentences in train = 231
Total loss for epoch 6: 353.385805
	Epoch 7....
Epoch has taken 0:00:11.994353
Number of used sentences in train = 231
Total loss for epoch 7: 349.745464
	Epoch 8....
Epoch has taken 0:00:12.000588
Number of used sentences in train = 231
Total loss for epoch 8: 348.473051
	Epoch 9....
Epoch has taken 0:00:11.983125
Number of used sentences in train = 231
Total loss for epoch 9: 353.053102
	Epoch 10....
Epoch has taken 0:00:11.995746
Number of used sentences in train = 231
Total loss for epoch 10: 353.374665
	Epoch 11....
Epoch has taken 0:00:11.988451
Number of used sentences in train = 231
Total loss for epoch 11: 350.171617
	Epoch 12....
Epoch has taken 0:00:11.994924
Number of used sentences in train = 231
Total loss for epoch 12: 352.400787
	Epoch 13....
Epoch has taken 0:00:12.007096
Number of used sentences in train = 231
Total loss for epoch 13: 359.712397
	Epoch 14....
Epoch has taken 0:00:11.980693
Number of used sentences in train = 231
Total loss for epoch 14: 349.962533
Epoch has taken 0:00:12.022494

==================================================================================================
	Training time : 0:33:05.717054
==================================================================================================
	Identification : 0.099

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 32)
  (w_embeddings): Embedding(1202, 220)
  (lstm): LSTM(252, 67, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=65, bias=True)
  (linear2): Linear(in_features=65, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17580.013159
validation loss after epoch 0 : 1396.187547
	Epoch 1....
Epoch has taken 0:03:46.929879
Number of used sentences in train = 3226
Total loss for epoch 1: 11849.915522
validation loss after epoch 1 : 1340.397497
	Epoch 2....
Epoch has taken 0:03:46.792635
Number of used sentences in train = 3226
Total loss for epoch 2: 10758.220285
validation loss after epoch 2 : 1349.431033
	Epoch 3....
Epoch has taken 0:03:47.719184
Number of used sentences in train = 3226
Total loss for epoch 3: 9940.942245
validation loss after epoch 3 : 1382.344138
	Epoch 4....
Epoch has taken 0:03:48.099982
Number of used sentences in train = 3226
Total loss for epoch 4: 9358.872761
validation loss after epoch 4 : 1356.752000
	Epoch 5....
Epoch has taken 0:03:49.261028
Number of used sentences in train = 3226
Total loss for epoch 5: 8950.014850
validation loss after epoch 5 : 1370.155289
	Epoch 6....
Epoch has taken 0:03:47.033637
Number of used sentences in train = 3226
Total loss for epoch 6: 8554.719885
validation loss after epoch 6 : 1412.493333
	Epoch 7....
Epoch has taken 0:03:47.690802
Number of used sentences in train = 3226
Total loss for epoch 7: 8315.361679
validation loss after epoch 7 : 1542.368355
	Epoch 8....
Epoch has taken 0:03:58.299862
