INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11516.642882
validation loss after epoch 0 : 870.156004
	Epoch 1....
Epoch has taken 0:02:50.799975
Number of used sentences in train = 2811
Total loss for epoch 1: 7625.484057
validation loss after epoch 1 : 836.592843
	Epoch 2....
Epoch has taken 0:02:51.616497
Number of used sentences in train = 2811
Total loss for epoch 2: 6861.480819
validation loss after epoch 2 : 799.198269
	Epoch 3....
Epoch has taken 0:02:52.147816
Number of used sentences in train = 2811
Total loss for epoch 3: 6386.207736
validation loss after epoch 3 : 810.271608
	Epoch 4....
Epoch has taken 0:03:03.810550
Number of used sentences in train = 2811
Total loss for epoch 4: 6082.850177
validation loss after epoch 4 : 805.193180
	Epoch 5....
Epoch has taken 0:02:56.674090
Number of used sentences in train = 2811
Total loss for epoch 5: 5790.993169
validation loss after epoch 5 : 830.015203
	Epoch 6....
Epoch has taken 0:02:53.863260
Number of used sentences in train = 2811
Total loss for epoch 6: 5568.581025
validation loss after epoch 6 : 858.487507
	Epoch 7....
Epoch has taken 0:02:53.926488
Number of used sentences in train = 2811
Total loss for epoch 7: 5381.460029
validation loss after epoch 7 : 885.863126
	Epoch 8....
Epoch has taken 0:02:52.668324
Number of used sentences in train = 2811
Total loss for epoch 8: 5223.274376
validation loss after epoch 8 : 925.069114
	Epoch 9....
Epoch has taken 0:02:52.420162
Number of used sentences in train = 2811
Total loss for epoch 9: 5088.302332
validation loss after epoch 9 : 884.737816
	Epoch 10....
Epoch has taken 0:02:58.247228
Number of used sentences in train = 2811
Total loss for epoch 10: 5029.031427
validation loss after epoch 10 : 918.996926
	Epoch 11....
Epoch has taken 0:03:10.477250
Number of used sentences in train = 2811
Total loss for epoch 11: 4922.858214
validation loss after epoch 11 : 941.225250
	Epoch 12....
Epoch has taken 0:03:10.525104
Number of used sentences in train = 2811
Total loss for epoch 12: 4872.880331
validation loss after epoch 12 : 962.155792
	Epoch 13....
Epoch has taken 0:03:10.588647
Number of used sentences in train = 2811
Total loss for epoch 13: 4820.930986
validation loss after epoch 13 : 977.179301
	Epoch 14....
Epoch has taken 0:03:09.502479
Number of used sentences in train = 2811
Total loss for epoch 14: 4788.646313
validation loss after epoch 14 : 987.988089
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:54.390655
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1307.187780
	Epoch 1....
Epoch has taken 0:00:18.371567
Number of used sentences in train = 313
Total loss for epoch 1: 785.149563
	Epoch 2....
Epoch has taken 0:00:18.365771
Number of used sentences in train = 313
Total loss for epoch 2: 679.559570
	Epoch 3....
Epoch has taken 0:00:18.383459
Number of used sentences in train = 313
Total loss for epoch 3: 615.347435
	Epoch 4....
Epoch has taken 0:00:18.381783
Number of used sentences in train = 313
Total loss for epoch 4: 591.094233
	Epoch 5....
Epoch has taken 0:00:18.369565
Number of used sentences in train = 313
Total loss for epoch 5: 573.812155
	Epoch 6....
Epoch has taken 0:00:18.360996
Number of used sentences in train = 313
Total loss for epoch 6: 566.726883
	Epoch 7....
Epoch has taken 0:00:18.386332
Number of used sentences in train = 313
Total loss for epoch 7: 557.023820
	Epoch 8....
Epoch has taken 0:00:18.379701
Number of used sentences in train = 313
Total loss for epoch 8: 552.637328
	Epoch 9....
Epoch has taken 0:00:18.378166
Number of used sentences in train = 313
Total loss for epoch 9: 547.721515
	Epoch 10....
Epoch has taken 0:00:18.363260
Number of used sentences in train = 313
Total loss for epoch 10: 546.569711
	Epoch 11....
Epoch has taken 0:00:18.383811
Number of used sentences in train = 313
Total loss for epoch 11: 541.432191
	Epoch 12....
Epoch has taken 0:00:18.554750
Number of used sentences in train = 313
Total loss for epoch 12: 540.064530
	Epoch 13....
Epoch has taken 0:00:20.274630
Number of used sentences in train = 313
Total loss for epoch 13: 543.703142
	Epoch 14....
Epoch has taken 0:00:20.286861
Number of used sentences in train = 313
Total loss for epoch 14: 549.610592
Epoch has taken 0:00:20.281374

==================================================================================================
	Training time : 0:49:29.733692
==================================================================================================
	Identification : 0.417

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9483.566756
validation loss after epoch 0 : 654.833909
	Epoch 1....
Epoch has taken 0:02:10.977650
Number of used sentences in train = 2074
Total loss for epoch 1: 5381.466573
validation loss after epoch 1 : 650.381577
	Epoch 2....
Epoch has taken 0:02:10.308864
Number of used sentences in train = 2074
Total loss for epoch 2: 4725.244909
validation loss after epoch 2 : 630.232702
	Epoch 3....
Epoch has taken 0:02:10.628722
Number of used sentences in train = 2074
Total loss for epoch 3: 4239.193058
validation loss after epoch 3 : 660.569420
	Epoch 4....
Epoch has taken 0:02:11.377350
Number of used sentences in train = 2074
Total loss for epoch 4: 3900.380597
validation loss after epoch 4 : 666.009353
	Epoch 5....
Epoch has taken 0:02:10.534284
Number of used sentences in train = 2074
Total loss for epoch 5: 3684.419699
validation loss after epoch 5 : 707.498701
	Epoch 6....
Epoch has taken 0:02:12.293146
Number of used sentences in train = 2074
Total loss for epoch 6: 3496.160044
validation loss after epoch 6 : 775.059514
	Epoch 7....
Epoch has taken 0:02:01.432971
Number of used sentences in train = 2074
Total loss for epoch 7: 3384.560502
validation loss after epoch 7 : 721.907809
	Epoch 8....
Epoch has taken 0:02:00.707239
Number of used sentences in train = 2074
Total loss for epoch 8: 3326.028897
validation loss after epoch 8 : 770.268828
	Epoch 9....
Epoch has taken 0:02:00.684075
Number of used sentences in train = 2074
Total loss for epoch 9: 3299.757787
validation loss after epoch 9 : 758.710927
	Epoch 10....
Epoch has taken 0:02:00.418221
Number of used sentences in train = 2074
Total loss for epoch 10: 3272.951834
validation loss after epoch 10 : 775.777453
	Epoch 11....
Epoch has taken 0:02:00.296453
Number of used sentences in train = 2074
Total loss for epoch 11: 3258.211090
validation loss after epoch 11 : 784.723701
	Epoch 12....
Epoch has taken 0:01:59.675355
Number of used sentences in train = 2074
Total loss for epoch 12: 3242.955455
validation loss after epoch 12 : 780.938734
	Epoch 13....
Epoch has taken 0:02:00.698966
Number of used sentences in train = 2074
Total loss for epoch 13: 3224.898465
validation loss after epoch 13 : 795.245364
	Epoch 14....
Epoch has taken 0:02:00.611979
Number of used sentences in train = 2074
Total loss for epoch 14: 3208.217678
validation loss after epoch 14 : 808.028841
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:00.555755
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1479.819466
	Epoch 1....
Epoch has taken 0:00:12.314511
Number of used sentences in train = 231
Total loss for epoch 1: 604.484587
	Epoch 2....
Epoch has taken 0:00:12.312307
Number of used sentences in train = 231
Total loss for epoch 2: 502.779872
	Epoch 3....
Epoch has taken 0:00:12.317357
Number of used sentences in train = 231
Total loss for epoch 3: 393.784270
	Epoch 4....
Epoch has taken 0:00:12.317968
Number of used sentences in train = 231
Total loss for epoch 4: 363.923214
	Epoch 5....
Epoch has taken 0:00:12.324319
Number of used sentences in train = 231
Total loss for epoch 5: 354.460499
	Epoch 6....
Epoch has taken 0:00:12.316985
Number of used sentences in train = 231
Total loss for epoch 6: 350.070197
	Epoch 7....
Epoch has taken 0:00:12.317373
Number of used sentences in train = 231
Total loss for epoch 7: 348.814038
	Epoch 8....
Epoch has taken 0:00:12.318096
Number of used sentences in train = 231
Total loss for epoch 8: 348.176302
	Epoch 9....
Epoch has taken 0:00:12.317851
Number of used sentences in train = 231
Total loss for epoch 9: 347.856295
	Epoch 10....
Epoch has taken 0:00:12.326845
Number of used sentences in train = 231
Total loss for epoch 10: 347.392049
	Epoch 11....
Epoch has taken 0:00:12.330532
Number of used sentences in train = 231
Total loss for epoch 11: 347.068073
	Epoch 12....
Epoch has taken 0:00:12.316640
Number of used sentences in train = 231
Total loss for epoch 12: 346.754337
	Epoch 13....
Epoch has taken 0:00:12.313324
Number of used sentences in train = 231
Total loss for epoch 13: 346.632392
	Epoch 14....
Epoch has taken 0:00:12.302300
Number of used sentences in train = 231
Total loss for epoch 14: 346.493361
Epoch has taken 0:00:12.304194

==================================================================================================
	Training time : 0:34:16.287911
==================================================================================================
	Identification : 0.383

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20941.887290
validation loss after epoch 0 : 1125.183122
	Epoch 1....
Epoch has taken 0:03:53.223234
Number of used sentences in train = 3226
Total loss for epoch 1: 9908.741483
validation loss after epoch 1 : 1077.553914
	Epoch 2....
Epoch has taken 0:03:52.056067
Number of used sentences in train = 3226
Total loss for epoch 2: 9071.885227
validation loss after epoch 2 : 1035.231926
	Epoch 3....
Epoch has taken 0:03:51.895229
Number of used sentences in train = 3226
Total loss for epoch 3: 8513.579493
validation loss after epoch 3 : 1041.584078
	Epoch 4....
Epoch has taken 0:03:52.132699
Number of used sentences in train = 3226
Total loss for epoch 4: 8093.194879
validation loss after epoch 4 : 1072.487437
	Epoch 5....
Epoch has taken 0:03:51.930738
Number of used sentences in train = 3226
Total loss for epoch 5: 7774.557379
validation loss after epoch 5 : 1102.083838
	Epoch 6....
Epoch has taken 0:03:53.730485
Number of used sentences in train = 3226
Total loss for epoch 6: 7492.110587
validation loss after epoch 6 : 1175.641186
	Epoch 7....
Epoch has taken 0:03:51.975002
Number of used sentences in train = 3226
Total loss for epoch 7: 7281.778668
validation loss after epoch 7 : 1309.567299
	Epoch 8....
Epoch has taken 0:03:53.575689
Number of used sentences in train = 3226
Total loss for epoch 8: 7122.528643
validation loss after epoch 8 : 1168.795651
	Epoch 9....
Epoch has taken 0:03:53.055432
Number of used sentences in train = 3226
Total loss for epoch 9: 6949.571895
validation loss after epoch 9 : 1278.815143
	Epoch 10....
Epoch has taken 0:03:53.566101
Number of used sentences in train = 3226
Total loss for epoch 10: 6829.921578
validation loss after epoch 10 : 1312.810788
	Epoch 11....
Epoch has taken 0:04:04.023690
Number of used sentences in train = 3226
Total loss for epoch 11: 6725.445338
validation loss after epoch 11 : 1312.692427
	Epoch 12....
Epoch has taken 0:04:00.944679
Number of used sentences in train = 3226
Total loss for epoch 12: 6641.432133
validation loss after epoch 12 : 1387.339068
	Epoch 13....
Epoch has taken 0:03:52.089788
Number of used sentences in train = 3226
Total loss for epoch 13: 6547.082190
validation loss after epoch 13 : 1371.881423
	Epoch 14....
Epoch has taken 0:03:53.659693
Number of used sentences in train = 3226
Total loss for epoch 14: 6515.589683
validation loss after epoch 14 : 1471.132691
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:53.726576
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1680.748902
	Epoch 1....
Epoch has taken 0:00:22.909574
Number of used sentences in train = 359
Total loss for epoch 1: 984.566505
	Epoch 2....
Epoch has taken 0:00:22.887147
Number of used sentences in train = 359
Total loss for epoch 2: 885.077110
	Epoch 3....
Epoch has taken 0:00:22.884012
Number of used sentences in train = 359
Total loss for epoch 3: 808.232161
	Epoch 4....
Epoch has taken 0:00:22.897209
Number of used sentences in train = 359
Total loss for epoch 4: 763.151025
	Epoch 5....
Epoch has taken 0:00:22.889409
Number of used sentences in train = 359
Total loss for epoch 5: 734.891357
	Epoch 6....
Epoch has taken 0:00:22.886882
Number of used sentences in train = 359
Total loss for epoch 6: 700.224436
	Epoch 7....
Epoch has taken 0:00:22.725221
Number of used sentences in train = 359
Total loss for epoch 7: 692.240186
	Epoch 8....
Epoch has taken 0:00:22.886572
Number of used sentences in train = 359
Total loss for epoch 8: 692.858652
	Epoch 9....
Epoch has taken 0:00:22.874686
Number of used sentences in train = 359
Total loss for epoch 9: 679.053235
	Epoch 10....
Epoch has taken 0:00:22.752817
Number of used sentences in train = 359
Total loss for epoch 10: 673.427022
	Epoch 11....
Epoch has taken 0:00:22.869999
Number of used sentences in train = 359
Total loss for epoch 11: 677.106072
	Epoch 12....
Epoch has taken 0:00:22.869097
Number of used sentences in train = 359
Total loss for epoch 12: 675.441532
	Epoch 13....
Epoch has taken 0:00:22.890395
Number of used sentences in train = 359
Total loss for epoch 13: 672.560776
	Epoch 14....
Epoch has taken 0:00:22.899336
Number of used sentences in train = 359
Total loss for epoch 14: 672.467520
Epoch has taken 0:00:22.894892

==================================================================================================
	Training time : 1:04:15.269209
==================================================================================================
	Identification : 0.234

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 62, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 35, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 46, 'lstmDropout': 0.17, 'denseActivation': 'tanh', 'wordDim': 142, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1882, 142)
  (lstm): LSTM(177, 46, bidirectional=True)
  (linear1): Linear(in_features=736, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10048.058805
validation loss after epoch 0 : 845.536959
	Epoch 1....
Epoch has taken 0:02:43.451336
Number of used sentences in train = 2811
Total loss for epoch 1: 7076.009661
validation loss after epoch 1 : 864.099357
	Epoch 2....
Epoch has taken 0:02:44.604874
Number of used sentences in train = 2811
Total loss for epoch 2: 6156.081842
validation loss after epoch 2 : 873.030442
	Epoch 3....
Epoch has taken 0:02:44.565580
Number of used sentences in train = 2811
Total loss for epoch 3: 5579.178507
validation loss after epoch 3 : 895.993707
	Epoch 4....
Epoch has taken 0:02:49.091541
Number of used sentences in train = 2811
Total loss for epoch 4: 5250.155050
validation loss after epoch 4 : 951.438974
	Epoch 5....
Epoch has taken 0:02:58.342184
Number of used sentences in train = 2811
Total loss for epoch 5: 5006.821242
validation loss after epoch 5 : 998.429135
	Epoch 6....
Epoch has taken 0:03:00.865778
Number of used sentences in train = 2811
Total loss for epoch 6: 4840.628002
validation loss after epoch 6 : 1027.843367
	Epoch 7....
Epoch has taken 0:02:59.417243
Number of used sentences in train = 2811
Total loss for epoch 7: 4746.555763
validation loss after epoch 7 : 1072.867093
	Epoch 8....
Epoch has taken 0:02:49.274208
Number of used sentences in train = 2811
Total loss for epoch 8: 4681.411765
validation loss after epoch 8 : 1123.085336
	Epoch 9....
Epoch has taken 0:02:44.420695
Number of used sentences in train = 2811
Total loss for epoch 9: 4624.594574
validation loss after epoch 9 : 1120.134890
	Epoch 10....
Epoch has taken 0:02:44.528101
Number of used sentences in train = 2811
Total loss for epoch 10: 4598.304063
validation loss after epoch 10 : 1131.961925
	Epoch 11....
Epoch has taken 0:02:44.631265
Number of used sentences in train = 2811
Total loss for epoch 11: 4578.394568
validation loss after epoch 11 : 1160.614322
	Epoch 12....
Epoch has taken 0:02:47.833730
Number of used sentences in train = 2811
Total loss for epoch 12: 4553.796663
validation loss after epoch 12 : 1170.529492
	Epoch 13....
Epoch has taken 0:02:50.211145
Number of used sentences in train = 2811
Total loss for epoch 13: 4538.680127
validation loss after epoch 13 : 1203.181866
	Epoch 14....
Epoch has taken 0:02:44.523145
Number of used sentences in train = 2811
Total loss for epoch 14: 4532.368190
validation loss after epoch 14 : 1216.035363
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1882, 142)
  (lstm): LSTM(177, 46, bidirectional=True)
  (linear1): Linear(in_features=736, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:44.568762
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1339.147900
	Epoch 1....
Epoch has taken 0:00:17.392031
Number of used sentences in train = 313
Total loss for epoch 1: 668.583649
	Epoch 2....
Epoch has taken 0:00:17.407015
Number of used sentences in train = 313
Total loss for epoch 2: 563.539857
	Epoch 3....
Epoch has taken 0:00:17.394956
Number of used sentences in train = 313
Total loss for epoch 3: 532.888134
	Epoch 4....
Epoch has taken 0:00:17.409038
Number of used sentences in train = 313
Total loss for epoch 4: 526.900346
	Epoch 5....
Epoch has taken 0:00:17.404884
Number of used sentences in train = 313
Total loss for epoch 5: 515.455334
	Epoch 6....
Epoch has taken 0:00:17.401240
Number of used sentences in train = 313
Total loss for epoch 6: 517.767382
	Epoch 7....
Epoch has taken 0:00:17.385808
Number of used sentences in train = 313
Total loss for epoch 7: 510.985905
	Epoch 8....
Epoch has taken 0:00:17.384598
Number of used sentences in train = 313
Total loss for epoch 8: 510.205687
	Epoch 9....
Epoch has taken 0:00:17.370197
Number of used sentences in train = 313
Total loss for epoch 9: 508.837809
	Epoch 10....
Epoch has taken 0:00:17.371376
Number of used sentences in train = 313
Total loss for epoch 10: 508.123485
	Epoch 11....
Epoch has taken 0:00:17.380584
Number of used sentences in train = 313
Total loss for epoch 11: 507.638537
	Epoch 12....
Epoch has taken 0:00:17.383265
Number of used sentences in train = 313
Total loss for epoch 12: 507.276379
	Epoch 13....
Epoch has taken 0:00:17.375614
Number of used sentences in train = 313
Total loss for epoch 13: 509.846937
	Epoch 14....
Epoch has taken 0:00:17.387017
Number of used sentences in train = 313
Total loss for epoch 14: 506.045195
Epoch has taken 0:00:17.385597

==================================================================================================
	Training time : 0:46:31.660601
==================================================================================================
	Identification : 0.276

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1680, 142)
  (lstm): LSTM(177, 46, bidirectional=True)
  (linear1): Linear(in_features=736, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7674.389270
validation loss after epoch 0 : 707.352533
	Epoch 1....
Epoch has taken 0:01:57.577017
Number of used sentences in train = 2074
Total loss for epoch 1: 4713.432460
validation loss after epoch 1 : 687.751101
	Epoch 2....
Epoch has taken 0:02:04.154926
Number of used sentences in train = 2074
Total loss for epoch 2: 3979.342507
validation loss after epoch 2 : 725.928623
	Epoch 3....
Epoch has taken 0:01:53.898889
Number of used sentences in train = 2074
Total loss for epoch 3: 3565.066238
validation loss after epoch 3 : 764.389549
	Epoch 4....
Epoch has taken 0:01:53.100695
Number of used sentences in train = 2074
Total loss for epoch 4: 3359.174161
validation loss after epoch 4 : 842.637529
	Epoch 5....
Epoch has taken 0:01:53.027587
Number of used sentences in train = 2074
Total loss for epoch 5: 3259.053947
validation loss after epoch 5 : 856.589961
	Epoch 6....
Epoch has taken 0:01:53.064647
Number of used sentences in train = 2074
Total loss for epoch 6: 3213.000336
validation loss after epoch 6 : 889.501265
	Epoch 7....
Epoch has taken 0:01:52.154382
Number of used sentences in train = 2074
Total loss for epoch 7: 3188.937456
validation loss after epoch 7 : 926.551789
	Epoch 8....
Epoch has taken 0:01:53.125716
Number of used sentences in train = 2074
Total loss for epoch 8: 3179.027498
validation loss after epoch 8 : 935.199886
	Epoch 9....
Epoch has taken 0:01:52.301409
Number of used sentences in train = 2074
Total loss for epoch 9: 3168.032254
validation loss after epoch 9 : 955.541967
	Epoch 10....
Epoch has taken 0:01:58.656416
Number of used sentences in train = 2074
Total loss for epoch 10: 3164.337742
validation loss after epoch 10 : 960.905238
	Epoch 11....
Epoch has taken 0:02:04.242761
Number of used sentences in train = 2074
Total loss for epoch 11: 3161.589774
validation loss after epoch 11 : 976.001405
	Epoch 12....
Epoch has taken 0:02:04.273991
Number of used sentences in train = 2074
Total loss for epoch 12: 3160.061706
validation loss after epoch 12 : 981.771719
	Epoch 13....
Epoch has taken 0:01:58.016925
Number of used sentences in train = 2074
Total loss for epoch 13: 3158.372442
validation loss after epoch 13 : 990.674020
	Epoch 14....
Epoch has taken 0:01:53.143603
Number of used sentences in train = 2074
Total loss for epoch 14: 3157.456017
validation loss after epoch 14 : 998.430807
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1680, 142)
  (lstm): LSTM(177, 46, bidirectional=True)
  (linear1): Linear(in_features=736, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.081893
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1265.419405
	Epoch 1....
Epoch has taken 0:00:11.516969
Number of used sentences in train = 231
Total loss for epoch 1: 532.765123
	Epoch 2....
Epoch has taken 0:00:11.528542
Number of used sentences in train = 231
Total loss for epoch 2: 404.030785
	Epoch 3....
Epoch has taken 0:00:11.531630
Number of used sentences in train = 231
Total loss for epoch 3: 365.083934
	Epoch 4....
Epoch has taken 0:00:11.535219
Number of used sentences in train = 231
Total loss for epoch 4: 354.776863
	Epoch 5....
Epoch has taken 0:00:11.522158
Number of used sentences in train = 231
Total loss for epoch 5: 349.581683
	Epoch 6....
Epoch has taken 0:00:11.530761
Number of used sentences in train = 231
Total loss for epoch 6: 347.504752
	Epoch 7....
Epoch has taken 0:00:11.533527
Number of used sentences in train = 231
Total loss for epoch 7: 346.750560
	Epoch 8....
Epoch has taken 0:00:11.521428
Number of used sentences in train = 231
Total loss for epoch 8: 346.161038
	Epoch 9....
Epoch has taken 0:00:11.525587
Number of used sentences in train = 231
Total loss for epoch 9: 345.794098
	Epoch 10....
Epoch has taken 0:00:11.388550
Number of used sentences in train = 231
Total loss for epoch 10: 345.546049
	Epoch 11....
Epoch has taken 0:00:11.347371
Number of used sentences in train = 231
Total loss for epoch 11: 345.681556
	Epoch 12....
Epoch has taken 0:00:11.527467
Number of used sentences in train = 231
Total loss for epoch 12: 345.433253
	Epoch 13....
Epoch has taken 0:00:11.539603
Number of used sentences in train = 231
Total loss for epoch 13: 345.182834
	Epoch 14....
Epoch has taken 0:00:11.529776
Number of used sentences in train = 231
Total loss for epoch 14: 345.028441
Epoch has taken 0:00:11.534620

==================================================================================================
	Training time : 0:31:56.769441
==================================================================================================
	Identification : 0.214

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(3369, 142)
  (lstm): LSTM(177, 46, bidirectional=True)
  (linear1): Linear(in_features=736, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12096.250649
validation loss after epoch 0 : 1161.269518
	Epoch 1....
Epoch has taken 0:03:42.760653
Number of used sentences in train = 3226
Total loss for epoch 1: 9108.602249
validation loss after epoch 1 : 1124.082077
	Epoch 2....
Epoch has taken 0:03:37.942459
Number of used sentences in train = 3226
Total loss for epoch 2: 8115.576680
validation loss after epoch 2 : 1123.499359
	Epoch 3....
Epoch has taken 0:03:38.284312
Number of used sentences in train = 3226
Total loss for epoch 3: 7441.354514
validation loss after epoch 3 : 1242.987525
	Epoch 4....
Epoch has taken 0:03:38.960494
Number of used sentences in train = 3226
Total loss for epoch 4: 7058.319063
validation loss after epoch 4 : 1247.683419
	Epoch 5....
Epoch has taken 0:03:38.562083
Number of used sentences in train = 3226
Total loss for epoch 5: 6785.051358
validation loss after epoch 5 : 1378.376795
	Epoch 6....
Epoch has taken 0:03:39.172695
Number of used sentences in train = 3226
Total loss for epoch 6: 6639.797732
validation loss after epoch 6 : 1397.389106
	Epoch 7....
Epoch has taken 0:03:39.000692
Number of used sentences in train = 3226
Total loss for epoch 7: 6526.519367
validation loss after epoch 7 : 1441.553607
	Epoch 8....
Epoch has taken 0:03:59.673259
Number of used sentences in train = 3226
Total loss for epoch 8: 6429.295378
validation loss after epoch 8 : 1493.872503
	Epoch 9....
Epoch has taken 0:03:38.987828
Number of used sentences in train = 3226
Total loss for epoch 9: 6362.730912
validation loss after epoch 9 : 1568.555668
	Epoch 10....
Epoch has taken 0:03:41.146314
Number of used sentences in train = 3226
Total loss for epoch 10: 6325.605696
validation loss after epoch 10 : 1616.688265
	Epoch 11....
Epoch has taken 0:03:38.929931
Number of used sentences in train = 3226
Total loss for epoch 11: 6278.807466
validation loss after epoch 11 : 1564.637569
	Epoch 12....
Epoch has taken 0:03:44.773952
Number of used sentences in train = 3226
Total loss for epoch 12: 6259.445202
validation loss after epoch 12 : 1695.088438
	Epoch 13....
Epoch has taken 0:03:37.492937
Number of used sentences in train = 3226
Total loss for epoch 13: 6233.907544
validation loss after epoch 13 : 1648.902830
	Epoch 14....
Epoch has taken 0:03:39.191814
Number of used sentences in train = 3226
Total loss for epoch 14: 6218.142397
validation loss after epoch 14 : 1746.171314
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(3369, 142)
  (lstm): LSTM(177, 46, bidirectional=True)
  (linear1): Linear(in_features=736, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:38.416062
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2025.252151
	Epoch 1....
Epoch has taken 0:00:21.141814
Number of used sentences in train = 359
Total loss for epoch 1: 932.099231
	Epoch 2....
Epoch has taken 0:00:20.891818
Number of used sentences in train = 359
Total loss for epoch 2: 789.858918
	Epoch 3....
Epoch has taken 0:00:20.823737
Number of used sentences in train = 359
Total loss for epoch 3: 722.136460
	Epoch 4....
Epoch has taken 0:00:20.814430
Number of used sentences in train = 359
Total loss for epoch 4: 697.812307
	Epoch 5....
Epoch has taken 0:00:20.816934
Number of used sentences in train = 359
Total loss for epoch 5: 681.976005
	Epoch 6....
Epoch has taken 0:00:20.835054
Number of used sentences in train = 359
Total loss for epoch 6: 676.755732
	Epoch 7....
Epoch has taken 0:00:21.369418
Number of used sentences in train = 359
Total loss for epoch 7: 675.272206
	Epoch 8....
Epoch has taken 0:00:21.224194
Number of used sentences in train = 359
Total loss for epoch 8: 672.708718
	Epoch 9....
Epoch has taken 0:00:21.482145
Number of used sentences in train = 359
Total loss for epoch 9: 672.038853
	Epoch 10....
Epoch has taken 0:00:20.833661
Number of used sentences in train = 359
Total loss for epoch 10: 671.631070
	Epoch 11....
Epoch has taken 0:00:20.950743
Number of used sentences in train = 359
Total loss for epoch 11: 671.600389
	Epoch 12....
Epoch has taken 0:00:20.832753
Number of used sentences in train = 359
Total loss for epoch 12: 671.143887
	Epoch 13....
Epoch has taken 0:00:20.829925
Number of used sentences in train = 359
Total loss for epoch 13: 670.995914
	Epoch 14....
Epoch has taken 0:00:21.256796
Number of used sentences in train = 359
Total loss for epoch 14: 670.848233
Epoch has taken 0:00:20.835780

==================================================================================================
	Training time : 1:00:28.903159
==================================================================================================
	Identification : 0.215

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 126, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 35, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 36, 'lstmDropout': 0.32, 'denseActivation': 'tanh', 'wordDim': 218, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1177, 218)
  (lstm): LSTM(253, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=126, bias=True)
  (linear2): Linear(in_features=126, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11154.979469
validation loss after epoch 0 : 927.865676
	Epoch 1....
Epoch has taken 0:02:44.859088
Number of used sentences in train = 2811
Total loss for epoch 1: 7693.337808
validation loss after epoch 1 : 937.680726
	Epoch 2....
Epoch has taken 0:02:42.639744
Number of used sentences in train = 2811
Total loss for epoch 2: 6721.240559
validation loss after epoch 2 : 948.225219
	Epoch 3....
Epoch has taken 0:02:44.272661
Number of used sentences in train = 2811
Total loss for epoch 3: 6118.293071
validation loss after epoch 3 : 974.634143
	Epoch 4....
Epoch has taken 0:02:43.211097
Number of used sentences in train = 2811
Total loss for epoch 4: 5675.787289
validation loss after epoch 4 : 1022.854388
	Epoch 5....
Epoch has taken 0:02:43.964780
Number of used sentences in train = 2811
Total loss for epoch 5: 5409.796204
validation loss after epoch 5 : 1064.457996
	Epoch 6....
Epoch has taken 0:02:43.104429
Number of used sentences in train = 2811
Total loss for epoch 6: 5187.614152
validation loss after epoch 6 : 1093.151369
	Epoch 7....
Epoch has taken 0:02:45.425459
Number of used sentences in train = 2811
Total loss for epoch 7: 5031.021235
validation loss after epoch 7 : 1158.600738
	Epoch 8....
Epoch has taken 0:02:44.422285
Number of used sentences in train = 2811
Total loss for epoch 8: 4912.621989
validation loss after epoch 8 : 1205.386678
	Epoch 9....
Epoch has taken 0:02:45.325445
Number of used sentences in train = 2811
Total loss for epoch 9: 4831.481995
validation loss after epoch 9 : 1218.884107
	Epoch 10....
Epoch has taken 0:02:42.869895
Number of used sentences in train = 2811
Total loss for epoch 10: 4774.205736
validation loss after epoch 10 : 1330.136383
	Epoch 11....
Epoch has taken 0:02:45.278768
Number of used sentences in train = 2811
Total loss for epoch 11: 4712.700591
validation loss after epoch 11 : 1353.586371
	Epoch 12....
Epoch has taken 0:02:43.649701
Number of used sentences in train = 2811
Total loss for epoch 12: 4682.361416
validation loss after epoch 12 : 1387.064908
	Epoch 13....
Epoch has taken 0:02:42.907690
Number of used sentences in train = 2811
Total loss for epoch 13: 4642.912561
validation loss after epoch 13 : 1395.669750
	Epoch 14....
Epoch has taken 0:02:43.407393
Number of used sentences in train = 2811
Total loss for epoch 14: 4616.630332
validation loss after epoch 14 : 1434.460519
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1177, 218)
  (lstm): LSTM(253, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=126, bias=True)
  (linear2): Linear(in_features=126, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:45.480016
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1841.852151
	Epoch 1....
Epoch has taken 0:00:17.541881
Number of used sentences in train = 313
Total loss for epoch 1: 742.698775
	Epoch 2....
Epoch has taken 0:00:17.549501
Number of used sentences in train = 313
Total loss for epoch 2: 609.791586
	Epoch 3....
Epoch has taken 0:00:17.852922
Number of used sentences in train = 313
Total loss for epoch 3: 553.479684
	Epoch 4....
Epoch has taken 0:00:19.404378
Number of used sentences in train = 313
Total loss for epoch 4: 531.565592
	Epoch 5....
Epoch has taken 0:00:19.402603
Number of used sentences in train = 313
Total loss for epoch 5: 518.280498
	Epoch 6....
Epoch has taken 0:00:18.628390
Number of used sentences in train = 313
Total loss for epoch 6: 510.694459
	Epoch 7....
Epoch has taken 0:00:16.962726
Number of used sentences in train = 313
Total loss for epoch 7: 507.761557
	Epoch 8....
Epoch has taken 0:00:16.961173
Number of used sentences in train = 313
Total loss for epoch 8: 506.763647
	Epoch 9....
Epoch has taken 0:00:17.089329
Number of used sentences in train = 313
Total loss for epoch 9: 505.007511
	Epoch 10....
Epoch has taken 0:00:16.971106
Number of used sentences in train = 313
Total loss for epoch 10: 503.812408
	Epoch 11....
Epoch has taken 0:00:16.973125
Number of used sentences in train = 313
Total loss for epoch 11: 503.504538
	Epoch 12....
Epoch has taken 0:00:16.972368
Number of used sentences in train = 313
Total loss for epoch 12: 502.648411
	Epoch 13....
Epoch has taken 0:00:16.974585
Number of used sentences in train = 313
Total loss for epoch 13: 502.146533
	Epoch 14....
Epoch has taken 0:00:16.967675
Number of used sentences in train = 313
Total loss for epoch 14: 501.841787
Epoch has taken 0:00:16.974147

==================================================================================================
	Training time : 0:45:24.541160
==================================================================================================
	Identification : 0.519

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1133, 218)
  (lstm): LSTM(253, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=126, bias=True)
  (linear2): Linear(in_features=126, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9018.928512
validation loss after epoch 0 : 795.303673
	Epoch 1....
Epoch has taken 0:01:49.630873
Number of used sentences in train = 2074
Total loss for epoch 1: 5229.969831
validation loss after epoch 1 : 865.071751
	Epoch 2....
Epoch has taken 0:01:49.603020
Number of used sentences in train = 2074
Total loss for epoch 2: 4276.360069
validation loss after epoch 2 : 894.712974
	Epoch 3....
Epoch has taken 0:01:49.693175
Number of used sentences in train = 2074
Total loss for epoch 3: 3804.745998
validation loss after epoch 3 : 1019.065418
	Epoch 4....
Epoch has taken 0:01:49.659738
Number of used sentences in train = 2074
Total loss for epoch 4: 3550.220524
validation loss after epoch 4 : 1045.811699
	Epoch 5....
Epoch has taken 0:01:49.624798
Number of used sentences in train = 2074
Total loss for epoch 5: 3401.734398
validation loss after epoch 5 : 1136.864277
	Epoch 6....
Epoch has taken 0:01:49.587646
Number of used sentences in train = 2074
Total loss for epoch 6: 3301.718384
validation loss after epoch 6 : 1157.344048
	Epoch 7....
Epoch has taken 0:01:51.293837
Number of used sentences in train = 2074
Total loss for epoch 7: 3263.496086
validation loss after epoch 7 : 1157.529654
	Epoch 8....
Epoch has taken 0:01:49.658046
Number of used sentences in train = 2074
Total loss for epoch 8: 3224.506744
validation loss after epoch 8 : 1223.452159
	Epoch 9....
Epoch has taken 0:01:49.690215
Number of used sentences in train = 2074
Total loss for epoch 9: 3202.856925
validation loss after epoch 9 : 1272.060481
	Epoch 10....
Epoch has taken 0:01:49.620654
Number of used sentences in train = 2074
Total loss for epoch 10: 3189.695614
validation loss after epoch 10 : 1278.768458
	Epoch 11....
Epoch has taken 0:01:49.639649
Number of used sentences in train = 2074
Total loss for epoch 11: 3182.407876
validation loss after epoch 11 : 1347.003796
	Epoch 12....
Epoch has taken 0:01:49.671378
Number of used sentences in train = 2074
Total loss for epoch 12: 3177.038365
validation loss after epoch 12 : 1335.221901
	Epoch 13....
Epoch has taken 0:01:49.729517
Number of used sentences in train = 2074
Total loss for epoch 13: 3172.650130
validation loss after epoch 13 : 1364.512884
	Epoch 14....
Epoch has taken 0:01:49.786881
Number of used sentences in train = 2074
Total loss for epoch 14: 3169.262287
validation loss after epoch 14 : 1394.045316
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(1133, 218)
  (lstm): LSTM(253, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=126, bias=True)
  (linear2): Linear(in_features=126, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.804268
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1593.924991
	Epoch 1....
Epoch has taken 0:00:11.154256
Number of used sentences in train = 231
Total loss for epoch 1: 598.937249
	Epoch 2....
Epoch has taken 0:00:11.148301
Number of used sentences in train = 231
Total loss for epoch 2: 453.597937
	Epoch 3....
Epoch has taken 0:00:11.153717
Number of used sentences in train = 231
Total loss for epoch 3: 392.424744
	Epoch 4....
Epoch has taken 0:00:11.156762
Number of used sentences in train = 231
Total loss for epoch 4: 367.057195
	Epoch 5....
Epoch has taken 0:00:11.149543
Number of used sentences in train = 231
Total loss for epoch 5: 356.291024
	Epoch 6....
Epoch has taken 0:00:11.143657
Number of used sentences in train = 231
Total loss for epoch 6: 351.934627
	Epoch 7....
Epoch has taken 0:00:11.148370
Number of used sentences in train = 231
Total loss for epoch 7: 349.972386
	Epoch 8....
Epoch has taken 0:00:11.246403
Number of used sentences in train = 231
Total loss for epoch 8: 348.665360
	Epoch 9....
Epoch has taken 0:00:11.141451
Number of used sentences in train = 231
Total loss for epoch 9: 348.168785
	Epoch 10....
Epoch has taken 0:00:11.141135
Number of used sentences in train = 231
Total loss for epoch 10: 347.242578
	Epoch 11....
Epoch has taken 0:00:11.145452
Number of used sentences in train = 231
Total loss for epoch 11: 346.651600
	Epoch 12....
Epoch has taken 0:00:11.148901
Number of used sentences in train = 231
Total loss for epoch 12: 346.302841
	Epoch 13....
Epoch has taken 0:00:11.144264
Number of used sentences in train = 231
Total loss for epoch 13: 346.051107
	Epoch 14....
Epoch has taken 0:00:11.146440
Number of used sentences in train = 231
Total loss for epoch 14: 345.815025
Epoch has taken 0:00:11.149972

==================================================================================================
	Training time : 0:30:14.348119
==================================================================================================
	Identification : 0.321

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(1202, 218)
  (lstm): LSTM(253, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=126, bias=True)
  (linear2): Linear(in_features=126, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14855.920759
validation loss after epoch 0 : 1345.571482
	Epoch 1....
Epoch has taken 0:03:34.180460
Number of used sentences in train = 3226
Total loss for epoch 1: 11238.597526
validation loss after epoch 1 : 1331.112409
	Epoch 2....
Epoch has taken 0:03:36.823246
Number of used sentences in train = 3226
Total loss for epoch 2: 10137.151328
validation loss after epoch 2 : 1421.025754
	Epoch 3....
Epoch has taken 0:03:33.941693
Number of used sentences in train = 3226
Total loss for epoch 3: 9497.916061
validation loss after epoch 3 : 1431.016837
	Epoch 4....
Epoch has taken 0:03:38.467219
Number of used sentences in train = 3226
Total loss for epoch 4: 8989.409519
validation loss after epoch 4 : 1500.121209
	Epoch 5....
Epoch has taken 0:03:37.230504
Number of used sentences in train = 3226
Total loss for epoch 5: 8498.484758
validation loss after epoch 5 : 1561.181809
	Epoch 6....
Epoch has taken 0:03:38.179878
Number of used sentences in train = 3226
Total loss for epoch 6: 8195.934345
validation loss after epoch 6 : 1610.240489
	Epoch 7....
Epoch has taken 0:03:36.122367
Number of used sentences in train = 3226
Total loss for epoch 7: 7952.829731
validation loss after epoch 7 : 1638.556289
	Epoch 8....
Epoch has taken 0:03:34.534729
Number of used sentences in train = 3226
Total loss for epoch 8: 7711.941760
validation loss after epoch 8 : 1765.543224
	Epoch 9....
Epoch has taken 0:03:38.418275
Number of used sentences in train = 3226
Total loss for epoch 9: 7558.376801
validation loss after epoch 9 : 1821.673971
	Epoch 10....
Epoch has taken 0:03:38.921799
Number of used sentences in train = 3226
Total loss for epoch 10: 7485.862453
validation loss after epoch 10 : 1839.390347
	Epoch 11....
Epoch has taken 0:03:48.036442
Number of used sentences in train = 3226
Total loss for epoch 11: 7378.973805
validation loss after epoch 11 : 1871.132977
	Epoch 12....
Epoch has taken 0:03:58.329069
Number of used sentences in train = 3226
Total loss for epoch 12: 7248.666022
validation loss after epoch 12 : 1943.471567
	Epoch 13....
Epoch has taken 0:03:59.474558
Number of used sentences in train = 3226
Total loss for epoch 13: 7163.274964
validation loss after epoch 13 : 1968.416571
	Epoch 14....
Epoch has taken 0:03:58.870688
Number of used sentences in train = 3226
Total loss for epoch 14: 7055.778602
validation loss after epoch 14 : 2054.707218
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(1202, 218)
  (lstm): LSTM(253, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=126, bias=True)
  (linear2): Linear(in_features=126, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.636950
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2197.520988
	Epoch 1....
Epoch has taken 0:00:23.695439
Number of used sentences in train = 359
Total loss for epoch 1: 1096.901691
	Epoch 2....
Epoch has taken 0:00:23.505036
Number of used sentences in train = 359
Total loss for epoch 2: 932.945360
	Epoch 3....
Epoch has taken 0:00:23.477086
Number of used sentences in train = 359
Total loss for epoch 3: 846.053238
	Epoch 4....
Epoch has taken 0:00:23.693150
Number of used sentences in train = 359
Total loss for epoch 4: 782.646292
	Epoch 5....
Epoch has taken 0:00:23.698233
Number of used sentences in train = 359
Total loss for epoch 5: 748.858068
	Epoch 6....
Epoch has taken 0:00:23.693632
Number of used sentences in train = 359
Total loss for epoch 6: 736.290134
	Epoch 7....
Epoch has taken 0:00:23.697893
Number of used sentences in train = 359
Total loss for epoch 7: 719.867076
	Epoch 8....
Epoch has taken 0:00:21.470196
Number of used sentences in train = 359
Total loss for epoch 8: 709.946553
	Epoch 9....
Epoch has taken 0:00:21.408196
Number of used sentences in train = 359
Total loss for epoch 9: 705.267290
	Epoch 10....
Epoch has taken 0:00:21.414802
Number of used sentences in train = 359
Total loss for epoch 10: 700.239657
	Epoch 11....
Epoch has taken 0:00:21.405452
Number of used sentences in train = 359
Total loss for epoch 11: 694.182043
	Epoch 12....
Epoch has taken 0:00:21.425281
Number of used sentences in train = 359
Total loss for epoch 12: 690.230653
	Epoch 13....
Epoch has taken 0:00:21.417771
Number of used sentences in train = 359
Total loss for epoch 13: 686.432437
	Epoch 14....
Epoch has taken 0:00:21.415332
Number of used sentences in train = 359
Total loss for epoch 14: 684.396919
Epoch has taken 0:00:21.417903

==================================================================================================
	Training time : 1:01:12.647109
==================================================================================================
	Identification : 0.14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 38, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 18, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 74, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 57, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1882, 57)
  (lstm): LSTM(75, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=38, bias=True)
  (linear2): Linear(in_features=38, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10636.414217
validation loss after epoch 0 : 866.359675
	Epoch 1....
Epoch has taken 0:02:45.637114
Number of used sentences in train = 2811
Total loss for epoch 1: 7718.618502
validation loss after epoch 1 : 861.700155
	Epoch 2....
Epoch has taken 0:02:43.658917
Number of used sentences in train = 2811
Total loss for epoch 2: 7019.749527
validation loss after epoch 2 : 849.848736
	Epoch 3....
Epoch has taken 0:02:45.763723
Number of used sentences in train = 2811
Total loss for epoch 3: 6436.366670
validation loss after epoch 3 : 846.349365
	Epoch 4....
Epoch has taken 0:02:43.864510
Number of used sentences in train = 2811
Total loss for epoch 4: 6018.899073
validation loss after epoch 4 : 887.943698
	Epoch 5....
Epoch has taken 0:02:45.936307
Number of used sentences in train = 2811
Total loss for epoch 5: 5738.984264
validation loss after epoch 5 : 888.185680
	Epoch 6....
Epoch has taken 0:02:45.838094
Number of used sentences in train = 2811
Total loss for epoch 6: 5489.118441
validation loss after epoch 6 : 935.409689
	Epoch 7....
Epoch has taken 0:03:00.819793
Number of used sentences in train = 2811
Total loss for epoch 7: 5274.681248
validation loss after epoch 7 : 926.447425
	Epoch 8....
Epoch has taken 0:02:45.836580
Number of used sentences in train = 2811
Total loss for epoch 8: 5141.834416
validation loss after epoch 8 : 931.517914
	Epoch 9....
Epoch has taken 0:02:43.851080
Number of used sentences in train = 2811
Total loss for epoch 9: 4996.702643
validation loss after epoch 9 : 983.423406
	Epoch 10....
Epoch has taken 0:02:44.545648
Number of used sentences in train = 2811
Total loss for epoch 10: 4884.816744
validation loss after epoch 10 : 984.653726
	Epoch 11....
Epoch has taken 0:02:44.221320
Number of used sentences in train = 2811
Total loss for epoch 11: 4822.001768
validation loss after epoch 11 : 997.437863
	Epoch 12....
Epoch has taken 0:02:45.685380
Number of used sentences in train = 2811
Total loss for epoch 12: 4737.283990
validation loss after epoch 12 : 1043.635624
	Epoch 13....
Epoch has taken 0:02:44.598265
Number of used sentences in train = 2811
Total loss for epoch 13: 4712.171611
validation loss after epoch 13 : 1056.096136
	Epoch 14....
Epoch has taken 0:02:45.634901
Number of used sentences in train = 2811
Total loss for epoch 14: 4685.093807
validation loss after epoch 14 : 1056.811374
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1882, 57)
  (lstm): LSTM(75, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=38, bias=True)
  (linear2): Linear(in_features=38, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:45.590958
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1387.925263
	Epoch 1....
Epoch has taken 0:00:17.564388
Number of used sentences in train = 313
Total loss for epoch 1: 769.893609
	Epoch 2....
Epoch has taken 0:00:17.579434
Number of used sentences in train = 313
Total loss for epoch 2: 658.409711
	Epoch 3....
Epoch has taken 0:00:17.571675
Number of used sentences in train = 313
Total loss for epoch 3: 596.027731
	Epoch 4....
Epoch has taken 0:00:17.594596
Number of used sentences in train = 313
Total loss for epoch 4: 555.138355
	Epoch 5....
Epoch has taken 0:00:17.575728
Number of used sentences in train = 313
Total loss for epoch 5: 540.305737
	Epoch 6....
Epoch has taken 0:00:17.572946
Number of used sentences in train = 313
Total loss for epoch 6: 527.466232
	Epoch 7....
Epoch has taken 0:00:17.580806
Number of used sentences in train = 313
Total loss for epoch 7: 526.039991
	Epoch 8....
Epoch has taken 0:00:17.418228
Number of used sentences in train = 313
Total loss for epoch 8: 523.354095
	Epoch 9....
Epoch has taken 0:00:17.580908
Number of used sentences in train = 313
Total loss for epoch 9: 521.227485
	Epoch 10....
Epoch has taken 0:00:17.581586
Number of used sentences in train = 313
Total loss for epoch 10: 519.833870
	Epoch 11....
Epoch has taken 0:00:17.569464
Number of used sentences in train = 313
Total loss for epoch 11: 518.725081
	Epoch 12....
Epoch has taken 0:00:17.566062
Number of used sentences in train = 313
Total loss for epoch 12: 515.164199
	Epoch 13....
Epoch has taken 0:00:17.571889
Number of used sentences in train = 313
Total loss for epoch 13: 516.070118
	Epoch 14....
Epoch has taken 0:00:17.566566
Number of used sentences in train = 313
Total loss for epoch 14: 514.458371
Epoch has taken 0:00:17.566644

==================================================================================================
	Training time : 0:45:55.449197
==================================================================================================
	Identification : 0.459

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1680, 57)
  (lstm): LSTM(75, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=38, bias=True)
  (linear2): Linear(in_features=38, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7725.331889
validation loss after epoch 0 : 697.140295
	Epoch 1....
Epoch has taken 0:01:54.906941
Number of used sentences in train = 2074
Total loss for epoch 1: 5072.040523
validation loss after epoch 1 : 645.476826
	Epoch 2....
Epoch has taken 0:01:55.104502
Number of used sentences in train = 2074
Total loss for epoch 2: 4323.366266
validation loss after epoch 2 : 687.786399
	Epoch 3....
Epoch has taken 0:01:53.712463
Number of used sentences in train = 2074
Total loss for epoch 3: 3843.583259
validation loss after epoch 3 : 706.954401
	Epoch 4....
Epoch has taken 0:01:54.977120
Number of used sentences in train = 2074
Total loss for epoch 4: 3586.193038
validation loss after epoch 4 : 705.085506
	Epoch 5....
Epoch has taken 0:01:55.164835
Number of used sentences in train = 2074
Total loss for epoch 5: 3399.143526
validation loss after epoch 5 : 772.207021
	Epoch 6....
Epoch has taken 0:01:55.165016
Number of used sentences in train = 2074
Total loss for epoch 6: 3300.947057
validation loss after epoch 6 : 813.231179
	Epoch 7....
Epoch has taken 0:01:55.184928
Number of used sentences in train = 2074
Total loss for epoch 7: 3254.026473
validation loss after epoch 7 : 835.593311
	Epoch 8....
Epoch has taken 0:01:55.160275
Number of used sentences in train = 2074
Total loss for epoch 8: 3222.548582
validation loss after epoch 8 : 842.628211
	Epoch 9....
Epoch has taken 0:01:55.256125
Number of used sentences in train = 2074
Total loss for epoch 9: 3198.105054
validation loss after epoch 9 : 875.855258
	Epoch 10....
Epoch has taken 0:01:55.239334
Number of used sentences in train = 2074
Total loss for epoch 10: 3191.615312
validation loss after epoch 10 : 894.129611
	Epoch 11....
Epoch has taken 0:01:55.274470
Number of used sentences in train = 2074
Total loss for epoch 11: 3184.697744
validation loss after epoch 11 : 902.133612
	Epoch 12....
Epoch has taken 0:01:54.577615
Number of used sentences in train = 2074
Total loss for epoch 12: 3180.109458
validation loss after epoch 12 : 914.799730
	Epoch 13....
Epoch has taken 0:01:55.304092
Number of used sentences in train = 2074
Total loss for epoch 13: 3177.016248
validation loss after epoch 13 : 922.641052
	Epoch 14....
Epoch has taken 0:01:55.313070
Number of used sentences in train = 2074
Total loss for epoch 14: 3172.712569
validation loss after epoch 14 : 936.225840
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1680, 57)
  (lstm): LSTM(75, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=38, bias=True)
  (linear2): Linear(in_features=38, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:55.261487
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1138.130996
	Epoch 1....
Epoch has taken 0:00:11.671605
Number of used sentences in train = 231
Total loss for epoch 1: 560.134726
	Epoch 2....
Epoch has taken 0:00:11.648285
Number of used sentences in train = 231
Total loss for epoch 2: 439.025664
	Epoch 3....
Epoch has taken 0:00:11.646712
Number of used sentences in train = 231
Total loss for epoch 3: 380.127212
	Epoch 4....
Epoch has taken 0:00:11.660681
Number of used sentences in train = 231
Total loss for epoch 4: 358.793504
	Epoch 5....
Epoch has taken 0:00:11.657439
Number of used sentences in train = 231
Total loss for epoch 5: 352.193056
	Epoch 6....
Epoch has taken 0:00:11.677393
Number of used sentences in train = 231
Total loss for epoch 6: 350.505367
	Epoch 7....
Epoch has taken 0:00:11.658724
Number of used sentences in train = 231
Total loss for epoch 7: 348.762219
	Epoch 8....
Epoch has taken 0:00:11.663122
Number of used sentences in train = 231
Total loss for epoch 8: 347.822787
	Epoch 9....
Epoch has taken 0:00:11.654407
Number of used sentences in train = 231
Total loss for epoch 9: 347.246742
	Epoch 10....
Epoch has taken 0:00:11.660115
Number of used sentences in train = 231
Total loss for epoch 10: 346.791566
	Epoch 11....
Epoch has taken 0:00:11.655046
Number of used sentences in train = 231
Total loss for epoch 11: 346.478154
	Epoch 12....
Epoch has taken 0:00:11.663428
Number of used sentences in train = 231
Total loss for epoch 12: 346.220634
	Epoch 13....
Epoch has taken 0:00:11.664061
Number of used sentences in train = 231
Total loss for epoch 13: 346.252690
	Epoch 14....
Epoch has taken 0:00:11.651817
Number of used sentences in train = 231
Total loss for epoch 14: 345.925376
Epoch has taken 0:00:11.652566

==================================================================================================
	Training time : 0:31:40.824340
==================================================================================================
	Identification : 0.179

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(3369, 57)
  (lstm): LSTM(75, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=38, bias=True)
  (linear2): Linear(in_features=38, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12835.929809
validation loss after epoch 0 : 1108.499286
	Epoch 1....
Epoch has taken 0:03:48.801212
Number of used sentences in train = 3226
Total loss for epoch 1: 9319.347248
validation loss after epoch 1 : 1092.294880
	Epoch 2....
Epoch has taken 0:03:41.124568
Number of used sentences in train = 3226
Total loss for epoch 2: 8429.805094
validation loss after epoch 2 : 1091.922740
	Epoch 3....
Epoch has taken 0:03:39.955397
Number of used sentences in train = 3226
Total loss for epoch 3: 7844.606413
validation loss after epoch 3 : 1153.450687
	Epoch 4....
Epoch has taken 0:03:39.531357
Number of used sentences in train = 3226
Total loss for epoch 4: 7450.913547
validation loss after epoch 4 : 1182.851670
	Epoch 5....
Epoch has taken 0:03:36.475665
Number of used sentences in train = 3226
Total loss for epoch 5: 7124.012743
validation loss after epoch 5 : 1302.330154
	Epoch 6....
Epoch has taken 0:03:40.471866
Number of used sentences in train = 3226
Total loss for epoch 6: 6904.579883
validation loss after epoch 6 : 1238.864248
	Epoch 7....
Epoch has taken 0:03:38.862816
Number of used sentences in train = 3226
Total loss for epoch 7: 6725.408120
validation loss after epoch 7 : 1280.682813
	Epoch 8....
Epoch has taken 0:03:35.712226
Number of used sentences in train = 3226
Total loss for epoch 8: 6598.032335
validation loss after epoch 8 : 1336.993534
	Epoch 9....
Epoch has taken 0:03:35.685385
Number of used sentences in train = 3226
Total loss for epoch 9: 6485.874218
validation loss after epoch 9 : 1386.879651
	Epoch 10....
Epoch has taken 0:03:37.188073
Number of used sentences in train = 3226
Total loss for epoch 10: 6415.109617
validation loss after epoch 10 : 1428.370336
	Epoch 11....
Epoch has taken 0:03:40.005615
Number of used sentences in train = 3226
Total loss for epoch 11: 6355.352097
validation loss after epoch 11 : 1493.800126
	Epoch 12....
Epoch has taken 0:03:35.743890
Number of used sentences in train = 3226
Total loss for epoch 12: 6315.251342
validation loss after epoch 12 : 1545.138431
	Epoch 13....
Epoch has taken 0:03:41.875980
Number of used sentences in train = 3226
Total loss for epoch 13: 6279.200984
validation loss after epoch 13 : 1603.139906
	Epoch 14....
Epoch has taken 0:03:40.229231
Number of used sentences in train = 3226
Total loss for epoch 14: 6238.187931
validation loss after epoch 14 : 1544.645841
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(3369, 57)
  (lstm): LSTM(75, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=38, bias=True)
  (linear2): Linear(in_features=38, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:38.081418
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1524.900221
	Epoch 1....
Epoch has taken 0:00:21.018255
Number of used sentences in train = 359
Total loss for epoch 1: 938.682841
	Epoch 2....
Epoch has taken 0:00:21.024276
Number of used sentences in train = 359
Total loss for epoch 2: 829.010814
	Epoch 3....
Epoch has taken 0:00:21.005749
Number of used sentences in train = 359
Total loss for epoch 3: 761.390147
	Epoch 4....
Epoch has taken 0:00:21.013039
Number of used sentences in train = 359
Total loss for epoch 4: 723.565240
	Epoch 5....
Epoch has taken 0:00:21.013120
Number of used sentences in train = 359
Total loss for epoch 5: 695.901035
	Epoch 6....
Epoch has taken 0:00:21.008336
Number of used sentences in train = 359
Total loss for epoch 6: 691.794587
	Epoch 7....
Epoch has taken 0:00:21.017338
Number of used sentences in train = 359
Total loss for epoch 7: 677.975403
	Epoch 8....
Epoch has taken 0:00:21.037054
Number of used sentences in train = 359
Total loss for epoch 8: 673.972671
	Epoch 9....
Epoch has taken 0:00:21.041125
Number of used sentences in train = 359
Total loss for epoch 9: 672.801413
	Epoch 10....
Epoch has taken 0:00:21.010414
Number of used sentences in train = 359
Total loss for epoch 10: 672.467486
	Epoch 11....
Epoch has taken 0:00:21.008542
Number of used sentences in train = 359
Total loss for epoch 11: 671.721176
	Epoch 12....
Epoch has taken 0:00:21.012740
Number of used sentences in train = 359
Total loss for epoch 12: 671.286990
	Epoch 13....
Epoch has taken 0:00:21.029643
Number of used sentences in train = 359
Total loss for epoch 13: 671.279748
	Epoch 14....
Epoch has taken 0:00:21.036121
Number of used sentences in train = 359
Total loss for epoch 14: 670.906438
Epoch has taken 0:00:21.011965

==================================================================================================
	Training time : 1:00:05.697539
==================================================================================================
	Identification : 0.485

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 67, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 63, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 30, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 119, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(9319, 119)
  (lstm): LSTM(182, 30, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=67, bias=True)
  (linear2): Linear(in_features=67, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13622.646000
validation loss after epoch 0 : 1110.598036
	Epoch 1....
Epoch has taken 0:02:55.688411
Number of used sentences in train = 2811
Total loss for epoch 1: 8656.626223
validation loss after epoch 1 : 1050.704437
	Epoch 2....
Epoch has taken 0:02:51.933300
Number of used sentences in train = 2811
Total loss for epoch 2: 6931.914235
validation loss after epoch 2 : 1061.267673
	Epoch 3....
Epoch has taken 0:02:57.398982
Number of used sentences in train = 2811
Total loss for epoch 3: 5967.377127
validation loss after epoch 3 : 1136.684655
	Epoch 4....
Epoch has taken 0:02:51.973285
Number of used sentences in train = 2811
Total loss for epoch 4: 5423.051668
validation loss after epoch 4 : 1237.868686
	Epoch 5....
Epoch has taken 0:02:54.214240
Number of used sentences in train = 2811
Total loss for epoch 5: 5095.667740
validation loss after epoch 5 : 1299.620852
	Epoch 6....
Epoch has taken 0:02:54.913507
Number of used sentences in train = 2811
Total loss for epoch 6: 4883.558004
validation loss after epoch 6 : 1381.928131
	Epoch 7....
Epoch has taken 0:02:51.980416
Number of used sentences in train = 2811
Total loss for epoch 7: 4777.038125
validation loss after epoch 7 : 1426.159871
	Epoch 8....
Epoch has taken 0:02:51.945005
Number of used sentences in train = 2811
Total loss for epoch 8: 4713.776484
validation loss after epoch 8 : 1514.991404
	Epoch 9....
Epoch has taken 0:02:52.651069
Number of used sentences in train = 2811
Total loss for epoch 9: 4634.655162
validation loss after epoch 9 : 1555.749883
	Epoch 10....
Epoch has taken 0:02:52.093446
Number of used sentences in train = 2811
Total loss for epoch 10: 4628.878470
validation loss after epoch 10 : 1531.878716
	Epoch 11....
Epoch has taken 0:02:52.095750
Number of used sentences in train = 2811
Total loss for epoch 11: 4564.493785
validation loss after epoch 11 : 1595.316573
	Epoch 12....
Epoch has taken 0:02:52.050428
Number of used sentences in train = 2811
Total loss for epoch 12: 4573.819824
validation loss after epoch 12 : 1645.220955
	Epoch 13....
Epoch has taken 0:03:05.107978
Number of used sentences in train = 2811
Total loss for epoch 13: 4542.730165
validation loss after epoch 13 : 1635.190150
	Epoch 14....
Epoch has taken 0:02:51.979482
Number of used sentences in train = 2811
Total loss for epoch 14: 4529.793756
validation loss after epoch 14 : 1674.386679
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(9319, 119)
  (lstm): LSTM(182, 30, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=67, bias=True)
  (linear2): Linear(in_features=67, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.813944
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1558.953933
	Epoch 1....
Epoch has taken 0:00:18.149388
Number of used sentences in train = 313
Total loss for epoch 1: 761.784438
	Epoch 2....
Epoch has taken 0:00:18.151050
Number of used sentences in train = 313
Total loss for epoch 2: 576.234902
	Epoch 3....
Epoch has taken 0:00:18.188783
Number of used sentences in train = 313
Total loss for epoch 3: 543.540690
	Epoch 4....
Epoch has taken 0:00:18.202567
Number of used sentences in train = 313
Total loss for epoch 4: 541.345735
	Epoch 5....
Epoch has taken 0:00:18.189426
Number of used sentences in train = 313
Total loss for epoch 5: 530.393976
	Epoch 6....
Epoch has taken 0:00:18.200289
Number of used sentences in train = 313
Total loss for epoch 6: 514.486969
	Epoch 7....
Epoch has taken 0:00:18.177362
Number of used sentences in train = 313
Total loss for epoch 7: 510.803858
	Epoch 8....
Epoch has taken 0:00:18.198505
Number of used sentences in train = 313
Total loss for epoch 8: 504.569069
	Epoch 9....
Epoch has taken 0:00:18.201378
Number of used sentences in train = 313
Total loss for epoch 9: 502.074118
	Epoch 10....
Epoch has taken 0:00:18.195937
Number of used sentences in train = 313
Total loss for epoch 10: 502.533996
	Epoch 11....
Epoch has taken 0:00:18.199797
Number of used sentences in train = 313
Total loss for epoch 11: 501.319038
	Epoch 12....
Epoch has taken 0:00:18.197559
Number of used sentences in train = 313
Total loss for epoch 12: 500.935149
	Epoch 13....
Epoch has taken 0:00:18.182289
Number of used sentences in train = 313
Total loss for epoch 13: 500.490522
	Epoch 14....
Epoch has taken 0:00:18.195276
Number of used sentences in train = 313
Total loss for epoch 14: 500.782728
Epoch has taken 0:00:18.193052

==================================================================================================
	Training time : 0:48:01.174424
==================================================================================================
	Identification : 0.2

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(7134, 119)
  (lstm): LSTM(182, 30, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=67, bias=True)
  (linear2): Linear(in_features=67, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10518.699222
validation loss after epoch 0 : 940.022369
	Epoch 1....
Epoch has taken 0:01:57.659902
Number of used sentences in train = 2074
Total loss for epoch 1: 6823.295354
validation loss after epoch 1 : 888.691587
	Epoch 2....
Epoch has taken 0:01:57.579948
Number of used sentences in train = 2074
Total loss for epoch 2: 5514.669998
validation loss after epoch 2 : 880.997291
	Epoch 3....
Epoch has taken 0:01:59.710701
Number of used sentences in train = 2074
Total loss for epoch 3: 4595.050973
validation loss after epoch 3 : 933.650957
	Epoch 4....
Epoch has taken 0:01:57.641059
Number of used sentences in train = 2074
Total loss for epoch 4: 4077.239291
validation loss after epoch 4 : 1089.184385
	Epoch 5....
Epoch has taken 0:01:57.775403
Number of used sentences in train = 2074
Total loss for epoch 5: 3741.425689
validation loss after epoch 5 : 1171.851563
	Epoch 6....
Epoch has taken 0:02:02.871032
Number of used sentences in train = 2074
Total loss for epoch 6: 3562.605926
validation loss after epoch 6 : 1238.967848
	Epoch 7....
Epoch has taken 0:01:57.782974
Number of used sentences in train = 2074
Total loss for epoch 7: 3491.268651
validation loss after epoch 7 : 1299.688051
	Epoch 8....
Epoch has taken 0:02:06.845758
Number of used sentences in train = 2074
Total loss for epoch 8: 3353.368846
validation loss after epoch 8 : 1360.944492
	Epoch 9....
Epoch has taken 0:02:05.195432
Number of used sentences in train = 2074
Total loss for epoch 9: 3333.216761
validation loss after epoch 9 : 1359.856445
	Epoch 10....
Epoch has taken 0:02:01.387892
Number of used sentences in train = 2074
Total loss for epoch 10: 3273.864294
validation loss after epoch 10 : 1320.746554
	Epoch 11....
Epoch has taken 0:02:01.245771
Number of used sentences in train = 2074
Total loss for epoch 11: 3249.431274
validation loss after epoch 11 : 1353.851892
	Epoch 12....
Epoch has taken 0:02:01.341197
Number of used sentences in train = 2074
Total loss for epoch 12: 3212.072823
validation loss after epoch 12 : 1407.937118
	Epoch 13....
Epoch has taken 0:01:59.942155
Number of used sentences in train = 2074
Total loss for epoch 13: 3193.610044
validation loss after epoch 13 : 1459.092723
	Epoch 14....
Epoch has taken 0:02:01.196087
Number of used sentences in train = 2074
Total loss for epoch 14: 3182.338116
validation loss after epoch 14 : 1440.788412
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(7134, 119)
  (lstm): LSTM(182, 30, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=67, bias=True)
  (linear2): Linear(in_features=67, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.020738
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1983.443273
	Epoch 1....
Epoch has taken 0:00:12.423345
Number of used sentences in train = 231
Total loss for epoch 1: 660.697978
	Epoch 2....
Epoch has taken 0:00:12.409833
Number of used sentences in train = 231
Total loss for epoch 2: 435.344718
	Epoch 3....
Epoch has taken 0:00:12.416387
Number of used sentences in train = 231
Total loss for epoch 3: 381.969118
	Epoch 4....
Epoch has taken 0:00:12.406768
Number of used sentences in train = 231
Total loss for epoch 4: 362.872797
	Epoch 5....
Epoch has taken 0:00:12.414589
Number of used sentences in train = 231
Total loss for epoch 5: 355.327183
	Epoch 6....
Epoch has taken 0:00:12.412829
Number of used sentences in train = 231
Total loss for epoch 6: 349.662506
	Epoch 7....
Epoch has taken 0:00:12.403162
Number of used sentences in train = 231
Total loss for epoch 7: 348.578304
	Epoch 8....
Epoch has taken 0:00:12.413162
Number of used sentences in train = 231
Total loss for epoch 8: 348.149294
	Epoch 9....
Epoch has taken 0:00:12.411367
Number of used sentences in train = 231
Total loss for epoch 9: 347.297381
	Epoch 10....
Epoch has taken 0:00:12.280193
Number of used sentences in train = 231
Total loss for epoch 10: 346.762556
	Epoch 11....
Epoch has taken 0:00:12.407890
Number of used sentences in train = 231
Total loss for epoch 11: 346.736686
	Epoch 12....
Epoch has taken 0:00:12.424242
Number of used sentences in train = 231
Total loss for epoch 12: 347.129390
	Epoch 13....
Epoch has taken 0:00:12.420193
Number of used sentences in train = 231
Total loss for epoch 13: 345.983284
	Epoch 14....
Epoch has taken 0:00:12.417368
Number of used sentences in train = 231
Total loss for epoch 14: 345.740364
Epoch has taken 0:00:12.401555

==================================================================================================
	Training time : 0:33:15.603283
==================================================================================================
	Identification : 0.215

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 63)
  (w_embeddings): Embedding(17948, 119)
  (lstm): LSTM(182, 30, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=67, bias=True)
  (linear2): Linear(in_features=67, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18019.236663
validation loss after epoch 0 : 1642.544845
	Epoch 1....
Epoch has taken 0:03:52.472167
Number of used sentences in train = 3226
Total loss for epoch 1: 12257.620661
validation loss after epoch 1 : 1596.191770
	Epoch 2....
Epoch has taken 0:03:54.201158
Number of used sentences in train = 3226
Total loss for epoch 2: 9979.580247
validation loss after epoch 2 : 1622.861966
	Epoch 3....
Epoch has taken 0:03:54.349148
Number of used sentences in train = 3226
Total loss for epoch 3: 8591.732129
validation loss after epoch 3 : 1814.418355
	Epoch 4....
Epoch has taken 0:03:52.653266
Number of used sentences in train = 3226
Total loss for epoch 4: 7733.715137
validation loss after epoch 4 : 1989.658366
	Epoch 5....
Epoch has taken 0:03:53.042631
Number of used sentences in train = 3226
Total loss for epoch 5: 7141.183898
validation loss after epoch 5 : 2115.178712
	Epoch 6....
Epoch has taken 0:03:54.039986
Number of used sentences in train = 3226
Total loss for epoch 6: 6833.594658
validation loss after epoch 6 : 2263.069071
	Epoch 7....
Epoch has taken 0:03:53.953767
Number of used sentences in train = 3226
Total loss for epoch 7: 6581.669025
validation loss after epoch 7 : 2360.311592
	Epoch 8....
Epoch has taken 0:03:54.340474
Number of used sentences in train = 3226
Total loss for epoch 8: 6497.842612
validation loss after epoch 8 : 2350.911030
	Epoch 9....
Epoch has taken 0:03:52.887374
Number of used sentences in train = 3226
Total loss for epoch 9: 6402.353245
validation loss after epoch 9 : 2470.316326
	Epoch 10....
Epoch has taken 0:03:54.468206
Number of used sentences in train = 3226
Total loss for epoch 10: 6353.001813
validation loss after epoch 10 : 2564.100793
	Epoch 11....
Epoch has taken 0:03:52.556168
Number of used sentences in train = 3226
Total loss for epoch 11: 6302.954913
validation loss after epoch 11 : 2631.115794
	Epoch 12....
Epoch has taken 0:03:53.776645
Number of used sentences in train = 3226
Total loss for epoch 12: 6285.423038
validation loss after epoch 12 : 2602.118298
	Epoch 13....
Epoch has taken 0:03:53.276805
Number of used sentences in train = 3226
Total loss for epoch 13: 6282.197410
validation loss after epoch 13 : 2732.398234
	Epoch 14....
Epoch has taken 0:03:53.098621
Number of used sentences in train = 3226
Total loss for epoch 14: 6276.660949
validation loss after epoch 14 : 2655.728195
	TransitionClassifier(
  (p_embeddings): Embedding(13, 63)
  (w_embeddings): Embedding(17948, 119)
  (lstm): LSTM(182, 30, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=67, bias=True)
  (linear2): Linear(in_features=67, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:53.416085
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2222.880926
	Epoch 1....
Epoch has taken 0:00:23.162917
Number of used sentences in train = 359
Total loss for epoch 1: 1160.250276
	Epoch 2....
Epoch has taken 0:00:23.157342
Number of used sentences in train = 359
Total loss for epoch 2: 873.321324
	Epoch 3....
Epoch has taken 0:00:23.154858
Number of used sentences in train = 359
Total loss for epoch 3: 734.876532
	Epoch 4....
Epoch has taken 0:00:23.162070
Number of used sentences in train = 359
Total loss for epoch 4: 695.752065
	Epoch 5....
Epoch has taken 0:00:22.949777
Number of used sentences in train = 359
Total loss for epoch 5: 685.992228
	Epoch 6....
Epoch has taken 0:00:22.834263
Number of used sentences in train = 359
Total loss for epoch 6: 677.152442
	Epoch 7....
Epoch has taken 0:00:23.163012
Number of used sentences in train = 359
Total loss for epoch 7: 673.286828
	Epoch 8....
Epoch has taken 0:00:23.158406
Number of used sentences in train = 359
Total loss for epoch 8: 676.011559
	Epoch 9....
Epoch has taken 0:00:23.177605
Number of used sentences in train = 359
Total loss for epoch 9: 679.187534
	Epoch 10....
Epoch has taken 0:00:23.157004
Number of used sentences in train = 359
Total loss for epoch 10: 672.111156
	Epoch 11....
Epoch has taken 0:00:23.170039
Number of used sentences in train = 359
Total loss for epoch 11: 671.679199
	Epoch 12....
Epoch has taken 0:00:23.152040
Number of used sentences in train = 359
Total loss for epoch 12: 671.248800
	Epoch 13....
Epoch has taken 0:00:23.153154
Number of used sentences in train = 359
Total loss for epoch 13: 671.289634
	Epoch 14....
Epoch has taken 0:00:23.168805
Number of used sentences in train = 359
Total loss for epoch 14: 672.611789
Epoch has taken 0:00:23.166630

==================================================================================================
	Training time : 1:04:10.111342
==================================================================================================
	Identification : 0.252

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 40, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 60, 'lstmDropout': 0.13, 'denseActivation': 'tanh', 'wordDim': 136, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1177, 136)
  (lstm): LSTM(176, 60, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11343.224773
validation loss after epoch 0 : 929.337050
	Epoch 1....
Epoch has taken 0:02:55.191016
Number of used sentences in train = 2811
Total loss for epoch 1: 7861.499893
validation loss after epoch 1 : 888.460070
	Epoch 2....
Epoch has taken 0:02:54.996078
Number of used sentences in train = 2811
Total loss for epoch 2: 6858.774022
validation loss after epoch 2 : 888.012723
	Epoch 3....
Epoch has taken 0:02:55.449884
Number of used sentences in train = 2811
Total loss for epoch 3: 6295.790670
validation loss after epoch 3 : 915.459088
	Epoch 4....
Epoch has taken 0:02:57.138811
Number of used sentences in train = 2811
Total loss for epoch 4: 5908.055736
validation loss after epoch 4 : 923.008518
	Epoch 5....
Epoch has taken 0:02:55.445294
Number of used sentences in train = 2811
Total loss for epoch 5: 5593.284128
validation loss after epoch 5 : 938.958467
	Epoch 6....
Epoch has taken 0:02:55.251582
Number of used sentences in train = 2811
Total loss for epoch 6: 5377.148542
validation loss after epoch 6 : 991.485749
	Epoch 7....
Epoch has taken 0:02:56.876330
Number of used sentences in train = 2811
Total loss for epoch 7: 5216.873771
validation loss after epoch 7 : 1032.765437
	Epoch 8....
Epoch has taken 0:03:06.585239
Number of used sentences in train = 2811
Total loss for epoch 8: 5131.389376
validation loss after epoch 8 : 1080.344289
	Epoch 9....
Epoch has taken 0:03:12.693825
Number of used sentences in train = 2811
Total loss for epoch 9: 4993.204927
validation loss after epoch 9 : 1104.731568
	Epoch 10....
Epoch has taken 0:03:08.041626
Number of used sentences in train = 2811
Total loss for epoch 10: 4912.907681
validation loss after epoch 10 : 1086.969583
	Epoch 11....
Epoch has taken 0:03:04.376814
Number of used sentences in train = 2811
Total loss for epoch 11: 4856.175029
validation loss after epoch 11 : 1113.052375
	Epoch 12....
Epoch has taken 0:02:57.310747
Number of used sentences in train = 2811
Total loss for epoch 12: 4823.860202
validation loss after epoch 12 : 1171.954679
	Epoch 13....
Epoch has taken 0:02:57.326507
Number of used sentences in train = 2811
Total loss for epoch 13: 4779.912212
validation loss after epoch 13 : 1151.601229
	Epoch 14....
Epoch has taken 0:02:57.918830
Number of used sentences in train = 2811
Total loss for epoch 14: 4723.563248
validation loss after epoch 14 : 1142.162299
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1177, 136)
  (lstm): LSTM(176, 60, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:05.266059
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1334.830779
	Epoch 1....
Epoch has taken 0:00:18.794850
Number of used sentences in train = 313
Total loss for epoch 1: 805.629229
	Epoch 2....
Epoch has taken 0:00:18.892993
Number of used sentences in train = 313
Total loss for epoch 2: 662.578373
	Epoch 3....
Epoch has taken 0:00:20.766408
Number of used sentences in train = 313
Total loss for epoch 3: 595.609497
	Epoch 4....
Epoch has taken 0:00:20.744379
Number of used sentences in train = 313
Total loss for epoch 4: 587.014687
	Epoch 5....
Epoch has taken 0:00:20.752316
Number of used sentences in train = 313
Total loss for epoch 5: 565.534200
	Epoch 6....
Epoch has taken 0:00:20.732232
Number of used sentences in train = 313
Total loss for epoch 6: 561.135790
	Epoch 7....
Epoch has taken 0:00:19.123968
Number of used sentences in train = 313
Total loss for epoch 7: 561.111585
	Epoch 8....
Epoch has taken 0:00:18.785252
Number of used sentences in train = 313
Total loss for epoch 8: 553.181477
	Epoch 9....
Epoch has taken 0:00:18.792420
Number of used sentences in train = 313
Total loss for epoch 9: 547.265650
	Epoch 10....
Epoch has taken 0:00:18.787894
Number of used sentences in train = 313
Total loss for epoch 10: 535.636573
	Epoch 11....
Epoch has taken 0:00:18.786302
Number of used sentences in train = 313
Total loss for epoch 11: 526.960773
	Epoch 12....
Epoch has taken 0:00:18.786245
Number of used sentences in train = 313
Total loss for epoch 12: 525.907898
	Epoch 13....
Epoch has taken 0:00:18.786612
Number of used sentences in train = 313
Total loss for epoch 13: 526.090668
	Epoch 14....
Epoch has taken 0:00:18.779112
Number of used sentences in train = 313
Total loss for epoch 14: 525.001058
Epoch has taken 0:00:18.771397

==================================================================================================
	Training time : 0:49:50.450136
==================================================================================================
	Identification : 0.468

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1133, 136)
  (lstm): LSTM(176, 60, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10088.864989
validation loss after epoch 0 : 678.425174
	Epoch 1....
Epoch has taken 0:02:09.532067
Number of used sentences in train = 2074
Total loss for epoch 1: 5639.331078
validation loss after epoch 1 : 619.659692
	Epoch 2....
Epoch has taken 0:02:07.053124
Number of used sentences in train = 2074
Total loss for epoch 2: 4746.980938
validation loss after epoch 2 : 631.765507
	Epoch 3....
Epoch has taken 0:02:02.778858
Number of used sentences in train = 2074
Total loss for epoch 3: 4225.075052
validation loss after epoch 3 : 622.500644
	Epoch 4....
Epoch has taken 0:02:01.281635
Number of used sentences in train = 2074
Total loss for epoch 4: 3887.514583
validation loss after epoch 4 : 669.880832
	Epoch 5....
Epoch has taken 0:02:10.786826
Number of used sentences in train = 2074
Total loss for epoch 5: 3684.354777
validation loss after epoch 5 : 669.042607
	Epoch 6....
Epoch has taken 0:02:11.306668
Number of used sentences in train = 2074
Total loss for epoch 6: 3515.952916
validation loss after epoch 6 : 761.191195
	Epoch 7....
Epoch has taken 0:02:12.820714
Number of used sentences in train = 2074
Total loss for epoch 7: 3421.499328
validation loss after epoch 7 : 736.129412
	Epoch 8....
Epoch has taken 0:02:13.241215
Number of used sentences in train = 2074
Total loss for epoch 8: 3348.381699
validation loss after epoch 8 : 753.249116
	Epoch 9....
Epoch has taken 0:02:10.405162
Number of used sentences in train = 2074
Total loss for epoch 9: 3285.906533
validation loss after epoch 9 : 806.263333
	Epoch 10....
Epoch has taken 0:02:01.496032
Number of used sentences in train = 2074
Total loss for epoch 10: 3250.119312
validation loss after epoch 10 : 743.082783
	Epoch 11....
Epoch has taken 0:02:01.538827
Number of used sentences in train = 2074
Total loss for epoch 11: 3227.820922
validation loss after epoch 11 : 805.097991
	Epoch 12....
Epoch has taken 0:02:01.490081
Number of used sentences in train = 2074
Total loss for epoch 12: 3231.438505
validation loss after epoch 12 : 768.757566
	Epoch 13....
Epoch has taken 0:02:01.520406
Number of used sentences in train = 2074
Total loss for epoch 13: 3211.119384
validation loss after epoch 13 : 855.049199
	Epoch 14....
Epoch has taken 0:02:00.532322
Number of used sentences in train = 2074
Total loss for epoch 14: 3196.939916
validation loss after epoch 14 : 877.112785
	TransitionClassifier(
  (p_embeddings): Embedding(18, 40)
  (w_embeddings): Embedding(1133, 136)
  (lstm): LSTM(176, 60, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.511916
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1345.094549
	Epoch 1....
Epoch has taken 0:00:12.422211
Number of used sentences in train = 231
Total loss for epoch 1: 632.704318
	Epoch 2....
Epoch has taken 0:00:12.407952
Number of used sentences in train = 231
Total loss for epoch 2: 460.045926
	Epoch 3....
Epoch has taken 0:00:13.018469
Number of used sentences in train = 231
Total loss for epoch 3: 405.442872
	Epoch 4....
Epoch has taken 0:00:13.633911
Number of used sentences in train = 231
Total loss for epoch 4: 386.915265
	Epoch 5....
Epoch has taken 0:00:13.690212
Number of used sentences in train = 231
Total loss for epoch 5: 369.045376
	Epoch 6....
Epoch has taken 0:00:13.688654
Number of used sentences in train = 231
Total loss for epoch 6: 357.244359
	Epoch 7....
Epoch has taken 0:00:13.131675
Number of used sentences in train = 231
Total loss for epoch 7: 351.878202
	Epoch 8....
Epoch has taken 0:00:12.215531
Number of used sentences in train = 231
Total loss for epoch 8: 349.753120
	Epoch 9....
Epoch has taken 0:00:12.216128
Number of used sentences in train = 231
Total loss for epoch 9: 349.116935
	Epoch 10....
Epoch has taken 0:00:12.411545
Number of used sentences in train = 231
Total loss for epoch 10: 348.763615
	Epoch 11....
Epoch has taken 0:00:12.413080
Number of used sentences in train = 231
Total loss for epoch 11: 348.304235
	Epoch 12....
Epoch has taken 0:00:12.404992
Number of used sentences in train = 231
Total loss for epoch 12: 347.986724
	Epoch 13....
Epoch has taken 0:00:12.415488
Number of used sentences in train = 231
Total loss for epoch 13: 348.157923
	Epoch 14....
Epoch has taken 0:00:12.404419
Number of used sentences in train = 231
Total loss for epoch 14: 348.219809
Epoch has taken 0:00:12.423408

==================================================================================================
	Training time : 0:34:38.528566
==================================================================================================
	Identification : 0.111

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 40)
  (w_embeddings): Embedding(1202, 136)
  (lstm): LSTM(176, 60, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15723.777712
validation loss after epoch 0 : 1332.474886
	Epoch 1....
Epoch has taken 0:03:53.696563
Number of used sentences in train = 3226
Total loss for epoch 1: 11375.290679
validation loss after epoch 1 : 1316.882404
	Epoch 2....
Epoch has taken 0:03:53.073086
Number of used sentences in train = 3226
Total loss for epoch 2: 10237.453857
validation loss after epoch 2 : 1308.360174
	Epoch 3....
Epoch has taken 0:03:53.285897
Number of used sentences in train = 3226
Total loss for epoch 3: 9479.159626
validation loss after epoch 3 : 1341.205508
	Epoch 4....
Epoch has taken 0:03:52.451968
Number of used sentences in train = 3226
Total loss for epoch 4: 8965.703997
validation loss after epoch 4 : 1352.195294
	Epoch 5....
Epoch has taken 0:03:53.422263
Number of used sentences in train = 3226
Total loss for epoch 5: 8535.124849
validation loss after epoch 5 : 1440.625482
	Epoch 6....
Epoch has taken 0:03:52.251857
Number of used sentences in train = 3226
Total loss for epoch 6: 8253.129125
validation loss after epoch 6 : 1452.311153
	Epoch 7....
Epoch has taken 0:03:53.487201
Number of used sentences in train = 3226
Total loss for epoch 7: 7958.861748
validation loss after epoch 7 : 1574.147619
	Epoch 8....
Epoch has taken 0:04:16.598522
Number of used sentences in train = 3226
Total loss for epoch 8: 7737.572575
validation loss after epoch 8 : 1602.913601
	Epoch 9....
Epoch has taken 0:03:55.136169
Number of used sentences in train = 3226
Total loss for epoch 9: 7539.292925
validation loss after epoch 9 : 1572.118476
	Epoch 10....
Epoch has taken 0:03:52.118557
Number of used sentences in train = 3226
Total loss for epoch 10: 7388.882759
validation loss after epoch 10 : 1623.502361
	Epoch 11....
Epoch has taken 0:03:53.067732
Number of used sentences in train = 3226
Total loss for epoch 11: 7219.834225
validation loss after epoch 11 : 1638.150867
	Epoch 12....
Epoch has taken 0:03:52.105218
Number of used sentences in train = 3226
Total loss for epoch 12: 7098.910228
validation loss after epoch 12 : 1754.758152
	Epoch 13....
Epoch has taken 0:03:52.548459
Number of used sentences in train = 3226
Total loss for epoch 13: 6992.939167
validation loss after epoch 13 : 1732.226495
	Epoch 14....
Epoch has taken 0:03:53.167950
Number of used sentences in train = 3226
Total loss for epoch 14: 6907.494275
validation loss after epoch 14 : 1772.139087
	TransitionClassifier(
  (p_embeddings): Embedding(13, 40)
  (w_embeddings): Embedding(1202, 136)
  (lstm): LSTM(176, 60, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:09.292102
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1729.594152
	Epoch 1....
Epoch has taken 0:00:25.360094
Number of used sentences in train = 359
Total loss for epoch 1: 1239.512024
	Epoch 2....
Epoch has taken 0:00:25.347166
Number of used sentences in train = 359
Total loss for epoch 2: 1078.506476
	Epoch 3....
Epoch has taken 0:00:25.342846
Number of used sentences in train = 359
Total loss for epoch 3: 940.304729
	Epoch 4....
Epoch has taken 0:00:23.820136
Number of used sentences in train = 359
Total loss for epoch 4: 866.418039
	Epoch 5....
Epoch has taken 0:00:22.973525
Number of used sentences in train = 359
Total loss for epoch 5: 796.892115
	Epoch 6....
Epoch has taken 0:00:22.965143
Number of used sentences in train = 359
Total loss for epoch 6: 758.725079
	Epoch 7....
Epoch has taken 0:00:22.940791
Number of used sentences in train = 359
Total loss for epoch 7: 741.337422
	Epoch 8....
Epoch has taken 0:00:22.938195
Number of used sentences in train = 359
Total loss for epoch 8: 720.927550
	Epoch 9....
Epoch has taken 0:00:22.941927
Number of used sentences in train = 359
Total loss for epoch 9: 700.382263
	Epoch 10....
Epoch has taken 0:00:22.948073
Number of used sentences in train = 359
Total loss for epoch 10: 692.773247
	Epoch 11....
Epoch has taken 0:00:22.949177
Number of used sentences in train = 359
Total loss for epoch 11: 690.358084
	Epoch 12....
Epoch has taken 0:00:22.946186
Number of used sentences in train = 359
Total loss for epoch 12: 696.780096
	Epoch 13....
Epoch has taken 0:00:22.952843
Number of used sentences in train = 359
Total loss for epoch 13: 687.897872
	Epoch 14....
Epoch has taken 0:00:22.943227
Number of used sentences in train = 359
Total loss for epoch 14: 680.314444
Epoch has taken 0:00:22.950831

==================================================================================================
	Training time : 1:04:48.673263
==================================================================================================
	Identification : 0.507

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 33, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 25, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 38, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 245, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(1177, 245)
  (lstm): LSTM(270, 38, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10788.226185
validation loss after epoch 0 : 978.610246
	Epoch 1....
Epoch has taken 0:02:43.809192
Number of used sentences in train = 2811
Total loss for epoch 1: 7946.847209
validation loss after epoch 1 : 993.515439
	Epoch 2....
Epoch has taken 0:02:43.297719
Number of used sentences in train = 2811
Total loss for epoch 2: 7060.667003
validation loss after epoch 2 : 987.531432
	Epoch 3....
Epoch has taken 0:02:45.792017
Number of used sentences in train = 2811
Total loss for epoch 3: 6493.507835
validation loss after epoch 3 : 1000.970244
	Epoch 4....
Epoch has taken 0:02:45.688259
Number of used sentences in train = 2811
Total loss for epoch 4: 6118.289288
validation loss after epoch 4 : 1027.933983
	Epoch 5....
Epoch has taken 0:02:43.326294
Number of used sentences in train = 2811
Total loss for epoch 5: 5811.238386
validation loss after epoch 5 : 1063.378736
	Epoch 6....
Epoch has taken 0:02:45.676155
Number of used sentences in train = 2811
Total loss for epoch 6: 5568.555562
validation loss after epoch 6 : 1126.611048
	Epoch 7....
Epoch has taken 0:02:45.835106
Number of used sentences in train = 2811
Total loss for epoch 7: 5381.315641
validation loss after epoch 7 : 1147.895501
	Epoch 8....
Epoch has taken 0:02:45.754888
Number of used sentences in train = 2811
Total loss for epoch 8: 5266.263354
validation loss after epoch 8 : 1169.235886
	Epoch 9....
Epoch has taken 0:02:45.683000
Number of used sentences in train = 2811
Total loss for epoch 9: 5135.263761
validation loss after epoch 9 : 1193.249904
	Epoch 10....
Epoch has taken 0:03:05.744474
Number of used sentences in train = 2811
Total loss for epoch 10: 5046.949594
validation loss after epoch 10 : 1228.091356
	Epoch 11....
Epoch has taken 0:03:21.052254
Number of used sentences in train = 2811
Total loss for epoch 11: 4969.995817
validation loss after epoch 11 : 1244.721544
	Epoch 12....
Epoch has taken 0:02:49.413637
Number of used sentences in train = 2811
Total loss for epoch 12: 4893.867220
validation loss after epoch 12 : 1299.690272
	Epoch 13....
Epoch has taken 0:02:43.698680
Number of used sentences in train = 2811
Total loss for epoch 13: 4843.072065
validation loss after epoch 13 : 1299.180953
	Epoch 14....
Epoch has taken 0:02:48.628421
Number of used sentences in train = 2811
Total loss for epoch 14: 4794.180237
validation loss after epoch 14 : 1330.317401
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(1177, 245)
  (lstm): LSTM(270, 38, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:43.025166
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1390.175260
	Epoch 1....
Epoch has taken 0:00:17.302053
Number of used sentences in train = 313
Total loss for epoch 1: 732.444352
	Epoch 2....
Epoch has taken 0:00:16.975966
Number of used sentences in train = 313
Total loss for epoch 2: 624.402981
	Epoch 3....
Epoch has taken 0:00:16.965589
Number of used sentences in train = 313
Total loss for epoch 3: 585.649922
	Epoch 4....
Epoch has taken 0:00:16.972459
Number of used sentences in train = 313
Total loss for epoch 4: 563.405198
	Epoch 5....
Epoch has taken 0:00:16.966751
Number of used sentences in train = 313
Total loss for epoch 5: 548.627019
	Epoch 6....
Epoch has taken 0:00:16.967120
Number of used sentences in train = 313
Total loss for epoch 6: 540.538786
	Epoch 7....
Epoch has taken 0:00:16.960625
Number of used sentences in train = 313
Total loss for epoch 7: 537.235793
	Epoch 8....
Epoch has taken 0:00:16.966394
Number of used sentences in train = 313
Total loss for epoch 8: 532.576723
	Epoch 9....
Epoch has taken 0:00:16.960365
Number of used sentences in train = 313
Total loss for epoch 9: 529.278836
	Epoch 10....
Epoch has taken 0:00:16.971199
Number of used sentences in train = 313
Total loss for epoch 10: 524.302536
	Epoch 11....
Epoch has taken 0:00:16.963416
Number of used sentences in train = 313
Total loss for epoch 11: 522.592893
	Epoch 12....
Epoch has taken 0:00:16.969450
Number of used sentences in train = 313
Total loss for epoch 12: 518.910485
	Epoch 13....
Epoch has taken 0:00:16.962646
Number of used sentences in train = 313
Total loss for epoch 13: 516.889427
	Epoch 14....
Epoch has taken 0:00:16.980225
Number of used sentences in train = 313
Total loss for epoch 14: 516.041966
Epoch has taken 0:00:16.975702

==================================================================================================
	Training time : 0:46:31.779932
==================================================================================================
	Identification : 0.492

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(1133, 245)
  (lstm): LSTM(270, 38, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8450.281641
validation loss after epoch 0 : 814.194529
	Epoch 1....
Epoch has taken 0:01:50.031057
Number of used sentences in train = 2074
Total loss for epoch 1: 5620.303081
validation loss after epoch 1 : 775.957026
	Epoch 2....
Epoch has taken 0:01:53.802985
Number of used sentences in train = 2074
Total loss for epoch 2: 4760.785096
validation loss after epoch 2 : 764.673159
	Epoch 3....
Epoch has taken 0:01:52.286387
Number of used sentences in train = 2074
Total loss for epoch 3: 4191.075593
validation loss after epoch 3 : 813.417813
	Epoch 4....
Epoch has taken 0:01:54.525178
Number of used sentences in train = 2074
Total loss for epoch 4: 3874.206506
validation loss after epoch 4 : 836.050483
	Epoch 5....
Epoch has taken 0:01:50.060553
Number of used sentences in train = 2074
Total loss for epoch 5: 3643.274931
validation loss after epoch 5 : 855.415373
	Epoch 6....
Epoch has taken 0:01:50.046833
Number of used sentences in train = 2074
Total loss for epoch 6: 3539.154879
validation loss after epoch 6 : 928.652137
	Epoch 7....
Epoch has taken 0:01:51.410130
Number of used sentences in train = 2074
Total loss for epoch 7: 3441.599322
validation loss after epoch 7 : 922.886145
	Epoch 8....
Epoch has taken 0:02:00.357599
Number of used sentences in train = 2074
Total loss for epoch 8: 3367.078339
validation loss after epoch 8 : 977.528983
	Epoch 9....
Epoch has taken 0:01:52.060671
Number of used sentences in train = 2074
Total loss for epoch 9: 3329.159390
validation loss after epoch 9 : 971.867614
	Epoch 10....
Epoch has taken 0:02:05.803162
Number of used sentences in train = 2074
Total loss for epoch 10: 3296.008997
validation loss after epoch 10 : 990.429057
	Epoch 11....
Epoch has taken 0:01:50.082561
Number of used sentences in train = 2074
Total loss for epoch 11: 3273.077949
validation loss after epoch 11 : 1014.834467
	Epoch 12....
Epoch has taken 0:01:50.033732
Number of used sentences in train = 2074
Total loss for epoch 12: 3255.037120
validation loss after epoch 12 : 1054.221893
	Epoch 13....
Epoch has taken 0:01:50.013873
Number of used sentences in train = 2074
Total loss for epoch 13: 3248.590921
validation loss after epoch 13 : 1036.375348
	Epoch 14....
Epoch has taken 0:01:50.075564
Number of used sentences in train = 2074
Total loss for epoch 14: 3233.085172
validation loss after epoch 14 : 1044.819359
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(1133, 245)
  (lstm): LSTM(270, 38, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.050884
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1548.713571
	Epoch 1....
Epoch has taken 0:00:11.379910
Number of used sentences in train = 231
Total loss for epoch 1: 583.734862
	Epoch 2....
Epoch has taken 0:00:11.222224
Number of used sentences in train = 231
Total loss for epoch 2: 463.798140
	Epoch 3....
Epoch has taken 0:00:11.364306
Number of used sentences in train = 231
Total loss for epoch 3: 397.497180
	Epoch 4....
Epoch has taken 0:00:11.216095
Number of used sentences in train = 231
Total loss for epoch 4: 381.075323
	Epoch 5....
Epoch has taken 0:00:11.370841
Number of used sentences in train = 231
Total loss for epoch 5: 368.058859
	Epoch 6....
Epoch has taken 0:00:11.210771
Number of used sentences in train = 231
Total loss for epoch 6: 362.142359
	Epoch 7....
Epoch has taken 0:00:11.374094
Number of used sentences in train = 231
Total loss for epoch 7: 358.698151
	Epoch 8....
Epoch has taken 0:00:11.215026
Number of used sentences in train = 231
Total loss for epoch 8: 355.048954
	Epoch 9....
Epoch has taken 0:00:11.385159
Number of used sentences in train = 231
Total loss for epoch 9: 353.610292
	Epoch 10....
Epoch has taken 0:00:11.216575
Number of used sentences in train = 231
Total loss for epoch 10: 352.472167
	Epoch 11....
Epoch has taken 0:00:11.214123
Number of used sentences in train = 231
Total loss for epoch 11: 351.385216
	Epoch 12....
Epoch has taken 0:00:11.364966
Number of used sentences in train = 231
Total loss for epoch 12: 350.643783
	Epoch 13....
Epoch has taken 0:00:11.205474
Number of used sentences in train = 231
Total loss for epoch 13: 350.034295
	Epoch 14....
Epoch has taken 0:00:11.370182
Number of used sentences in train = 231
Total loss for epoch 14: 349.526570
Epoch has taken 0:00:11.214955

==================================================================================================
	Training time : 0:31:00.300241
==================================================================================================
	Identification : 0.239

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 25)
  (w_embeddings): Embedding(1202, 245)
  (lstm): LSTM(270, 38, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14659.369641
validation loss after epoch 0 : 1381.661870
	Epoch 1....
Epoch has taken 0:03:34.367557
Number of used sentences in train = 3226
Total loss for epoch 1: 11549.765730
validation loss after epoch 1 : 1348.060014
	Epoch 2....
Epoch has taken 0:03:37.906095
Number of used sentences in train = 3226
Total loss for epoch 2: 10449.727719
validation loss after epoch 2 : 1370.142230
	Epoch 3....
Epoch has taken 0:03:53.699002
Number of used sentences in train = 3226
Total loss for epoch 3: 9774.364016
validation loss after epoch 3 : 1412.777240
	Epoch 4....
Epoch has taken 0:03:39.697150
Number of used sentences in train = 3226
Total loss for epoch 4: 9227.201692
validation loss after epoch 4 : 1447.438725
	Epoch 5....
Epoch has taken 0:03:39.743385
Number of used sentences in train = 3226
Total loss for epoch 5: 8785.002273
validation loss after epoch 5 : 1479.218282
	Epoch 6....
Epoch has taken 0:03:38.752111
Number of used sentences in train = 3226
Total loss for epoch 6: 8423.896364
validation loss after epoch 6 : 1546.784289
	Epoch 7....
Epoch has taken 0:03:39.856348
Number of used sentences in train = 3226
Total loss for epoch 7: 8165.223132
validation loss after epoch 7 : 1545.262594
	Epoch 8....
Epoch has taken 0:03:39.240230
Number of used sentences in train = 3226
Total loss for epoch 8: 7867.451125
validation loss after epoch 8 : 1613.932050
	Epoch 9....
Epoch has taken 0:03:46.458157
Number of used sentences in train = 3226
Total loss for epoch 9: 7705.818871
validation loss after epoch 9 : 1654.253368
	Epoch 10....
Epoch has taken 0:03:40.105010
Number of used sentences in train = 3226
Total loss for epoch 10: 7552.748902
validation loss after epoch 10 : 1708.432734
	Epoch 11....
Epoch has taken 0:03:39.924330
Number of used sentences in train = 3226
Total loss for epoch 11: 7430.356524
validation loss after epoch 11 : 1770.311528
	Epoch 12....
Epoch has taken 0:03:41.844046
Number of used sentences in train = 3226
Total loss for epoch 12: 7310.587677
validation loss after epoch 12 : 1835.246600
	Epoch 13....
Epoch has taken 0:03:52.876187
Number of used sentences in train = 3226
Total loss for epoch 13: 7205.866478
validation loss after epoch 13 : 1863.842430
	Epoch 14....
Epoch has taken 0:03:39.802018
Number of used sentences in train = 3226
Total loss for epoch 14: 7121.026252
validation loss after epoch 14 : 1877.558271
	TransitionClassifier(
  (p_embeddings): Embedding(13, 25)
  (w_embeddings): Embedding(1202, 245)
  (lstm): LSTM(270, 38, bidirectional=True)
  (linear1): Linear(in_features=608, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:39.415902
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1929.395812
	Epoch 1....
Epoch has taken 0:00:21.525345
Number of used sentences in train = 359
Total loss for epoch 1: 1187.956837
	Epoch 2....
Epoch has taken 0:00:21.544982
Number of used sentences in train = 359
Total loss for epoch 2: 984.303885
	Epoch 3....
Epoch has taken 0:00:21.540399
Number of used sentences in train = 359
Total loss for epoch 3: 886.656068
	Epoch 4....
Epoch has taken 0:00:21.530435
Number of used sentences in train = 359
Total loss for epoch 4: 836.326269
	Epoch 5....
Epoch has taken 0:00:21.552079
Number of used sentences in train = 359
Total loss for epoch 5: 789.096640
	Epoch 6....
Epoch has taken 0:00:21.533330
Number of used sentences in train = 359
Total loss for epoch 6: 764.763676
	Epoch 7....
Epoch has taken 0:00:21.522838
Number of used sentences in train = 359
Total loss for epoch 7: 744.825207
	Epoch 8....
Epoch has taken 0:00:23.535288
Number of used sentences in train = 359
Total loss for epoch 8: 734.383074
	Epoch 9....
Epoch has taken 0:00:21.530160
Number of used sentences in train = 359
Total loss for epoch 9: 730.558306
	Epoch 10....
Epoch has taken 0:00:21.544036
Number of used sentences in train = 359
Total loss for epoch 10: 717.298512
	Epoch 11....
Epoch has taken 0:00:21.533672
Number of used sentences in train = 359
Total loss for epoch 11: 709.716162
	Epoch 12....
Epoch has taken 0:00:21.526487
Number of used sentences in train = 359
Total loss for epoch 12: 702.816376
	Epoch 13....
Epoch has taken 0:00:21.534437
Number of used sentences in train = 359
Total loss for epoch 13: 698.395870
	Epoch 14....
Epoch has taken 0:00:21.549385
Number of used sentences in train = 359
Total loss for epoch 14: 695.646159
Epoch has taken 0:00:21.526744

==================================================================================================
	Training time : 1:00:49.360880
==================================================================================================
	Identification : 0.285

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 15, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 16, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 74, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 76, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(9223, 76)
  (lstm): LSTM(92, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15385.201916
validation loss after epoch 0 : 1192.324906
	Epoch 1....
Epoch has taken 0:02:45.881053
Number of used sentences in train = 2811
Total loss for epoch 1: 9778.238771
validation loss after epoch 1 : 1074.214832
	Epoch 2....
Epoch has taken 0:02:45.866142
Number of used sentences in train = 2811
Total loss for epoch 2: 7806.845887
validation loss after epoch 2 : 1072.266509
	Epoch 3....
Epoch has taken 0:02:45.979657
Number of used sentences in train = 2811
Total loss for epoch 3: 6596.322762
validation loss after epoch 3 : 1130.676659
	Epoch 4....
Epoch has taken 0:02:53.359113
Number of used sentences in train = 2811
Total loss for epoch 4: 5854.222839
validation loss after epoch 4 : 1200.875544
	Epoch 5....
Epoch has taken 0:02:45.982753
Number of used sentences in train = 2811
Total loss for epoch 5: 5382.158383
validation loss after epoch 5 : 1274.878176
	Epoch 6....
Epoch has taken 0:02:58.453806
Number of used sentences in train = 2811
Total loss for epoch 6: 5172.766699
validation loss after epoch 6 : 1337.956936
	Epoch 7....
Epoch has taken 0:02:47.600216
Number of used sentences in train = 2811
Total loss for epoch 7: 4994.657409
validation loss after epoch 7 : 1393.801796
	Epoch 8....
Epoch has taken 0:02:46.261755
Number of used sentences in train = 2811
Total loss for epoch 8: 4882.784415
validation loss after epoch 8 : 1494.280758
	Epoch 9....
Epoch has taken 0:02:45.956682
Number of used sentences in train = 2811
Total loss for epoch 9: 4797.439228
validation loss after epoch 9 : 1490.360042
	Epoch 10....
Epoch has taken 0:02:46.012349
Number of used sentences in train = 2811
Total loss for epoch 10: 4747.209606
validation loss after epoch 10 : 1488.275934
	Epoch 11....
Epoch has taken 0:02:46.065451
Number of used sentences in train = 2811
Total loss for epoch 11: 4661.811281
validation loss after epoch 11 : 1545.200228
	Epoch 12....
Epoch has taken 0:02:45.969126
Number of used sentences in train = 2811
Total loss for epoch 12: 4619.425457
validation loss after epoch 12 : 1586.380587
	Epoch 13....
Epoch has taken 0:02:45.938162
Number of used sentences in train = 2811
Total loss for epoch 13: 4578.988460
validation loss after epoch 13 : 1596.093886
	Epoch 14....
Epoch has taken 0:02:57.063091
Number of used sentences in train = 2811
Total loss for epoch 14: 4557.344009
validation loss after epoch 14 : 1629.953912
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(9223, 76)
  (lstm): LSTM(92, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.252209
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1536.803225
	Epoch 1....
Epoch has taken 0:00:17.519498
Number of used sentences in train = 313
Total loss for epoch 1: 843.336158
	Epoch 2....
Epoch has taken 0:00:17.482223
Number of used sentences in train = 313
Total loss for epoch 2: 644.834819
	Epoch 3....
Epoch has taken 0:00:17.497208
Number of used sentences in train = 313
Total loss for epoch 3: 594.134211
	Epoch 4....
Epoch has taken 0:00:17.499372
Number of used sentences in train = 313
Total loss for epoch 4: 558.090590
	Epoch 5....
Epoch has taken 0:00:17.504208
Number of used sentences in train = 313
Total loss for epoch 5: 543.400779
	Epoch 6....
Epoch has taken 0:00:17.500530
Number of used sentences in train = 313
Total loss for epoch 6: 532.887905
	Epoch 7....
Epoch has taken 0:00:17.510917
Number of used sentences in train = 313
Total loss for epoch 7: 527.860852
	Epoch 8....
Epoch has taken 0:00:18.473469
Number of used sentences in train = 313
Total loss for epoch 8: 533.115708
	Epoch 9....
Epoch has taken 0:00:19.378194
Number of used sentences in train = 313
Total loss for epoch 9: 518.710077
	Epoch 10....
Epoch has taken 0:00:19.350871
Number of used sentences in train = 313
Total loss for epoch 10: 511.857463
	Epoch 11....
Epoch has taken 0:00:19.368654
Number of used sentences in train = 313
Total loss for epoch 11: 508.522299
	Epoch 12....
Epoch has taken 0:00:19.368070
Number of used sentences in train = 313
Total loss for epoch 12: 506.235370
	Epoch 13....
Epoch has taken 0:00:19.369062
Number of used sentences in train = 313
Total loss for epoch 13: 505.499697
	Epoch 14....
Epoch has taken 0:00:19.369393
Number of used sentences in train = 313
Total loss for epoch 14: 505.074115
Epoch has taken 0:00:19.382263

==================================================================================================
	Training time : 0:46:50.719590
==================================================================================================
	Identification : 0.436

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(7074, 76)
  (lstm): LSTM(92, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11271.329023
validation loss after epoch 0 : 948.821750
	Epoch 1....
Epoch has taken 0:02:04.736591
Number of used sentences in train = 2074
Total loss for epoch 1: 6698.705292
validation loss after epoch 1 : 933.983418
	Epoch 2....
Epoch has taken 0:02:01.904267
Number of used sentences in train = 2074
Total loss for epoch 2: 5180.577409
validation loss after epoch 2 : 966.217903
	Epoch 3....
Epoch has taken 0:02:04.592477
Number of used sentences in train = 2074
Total loss for epoch 3: 4336.959027
validation loss after epoch 3 : 1030.727279
	Epoch 4....
Epoch has taken 0:02:04.866581
Number of used sentences in train = 2074
Total loss for epoch 4: 3856.691972
validation loss after epoch 4 : 1132.058873
	Epoch 5....
Epoch has taken 0:01:54.229889
Number of used sentences in train = 2074
Total loss for epoch 5: 3619.007622
validation loss after epoch 5 : 1184.386963
	Epoch 6....
Epoch has taken 0:01:53.413237
Number of used sentences in train = 2074
Total loss for epoch 6: 3488.826593
validation loss after epoch 6 : 1222.549366
	Epoch 7....
Epoch has taken 0:01:53.521742
Number of used sentences in train = 2074
Total loss for epoch 7: 3415.156901
validation loss after epoch 7 : 1240.919062
	Epoch 8....
Epoch has taken 0:01:53.445295
Number of used sentences in train = 2074
Total loss for epoch 8: 3383.300644
validation loss after epoch 8 : 1259.824148
	Epoch 9....
Epoch has taken 0:01:53.455931
Number of used sentences in train = 2074
Total loss for epoch 9: 3355.804029
validation loss after epoch 9 : 1268.054111
	Epoch 10....
Epoch has taken 0:01:56.899147
Number of used sentences in train = 2074
Total loss for epoch 10: 3340.953572
validation loss after epoch 10 : 1291.658777
	Epoch 11....
Epoch has taken 0:01:53.604761
Number of used sentences in train = 2074
Total loss for epoch 11: 3311.506033
validation loss after epoch 11 : 1316.511056
	Epoch 12....
Epoch has taken 0:01:53.587227
Number of used sentences in train = 2074
Total loss for epoch 12: 3291.679738
validation loss after epoch 12 : 1309.079094
	Epoch 13....
Epoch has taken 0:01:52.230441
Number of used sentences in train = 2074
Total loss for epoch 13: 3273.381926
validation loss after epoch 13 : 1334.847753
	Epoch 14....
Epoch has taken 0:01:53.536219
Number of used sentences in train = 2074
Total loss for epoch 14: 3261.865978
validation loss after epoch 14 : 1341.257147
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(7074, 76)
  (lstm): LSTM(92, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.628010
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1861.261555
	Epoch 1....
Epoch has taken 0:00:11.720187
Number of used sentences in train = 231
Total loss for epoch 1: 762.149595
	Epoch 2....
Epoch has taken 0:00:12.837957
Number of used sentences in train = 231
Total loss for epoch 2: 576.155312
	Epoch 3....
Epoch has taken 0:00:12.844014
Number of used sentences in train = 231
Total loss for epoch 3: 483.329559
	Epoch 4....
Epoch has taken 0:00:12.860848
Number of used sentences in train = 231
Total loss for epoch 4: 418.499277
	Epoch 5....
Epoch has taken 0:00:12.842171
Number of used sentences in train = 231
Total loss for epoch 5: 384.175417
	Epoch 6....
Epoch has taken 0:00:12.860620
Number of used sentences in train = 231
Total loss for epoch 6: 369.155352
	Epoch 7....
Epoch has taken 0:00:12.396637
Number of used sentences in train = 231
Total loss for epoch 7: 369.759244
	Epoch 8....
Epoch has taken 0:00:12.667786
Number of used sentences in train = 231
Total loss for epoch 8: 366.869173
	Epoch 9....
Epoch has taken 0:00:12.846838
Number of used sentences in train = 231
Total loss for epoch 9: 358.879697
	Epoch 10....
Epoch has taken 0:00:12.660158
Number of used sentences in train = 231
Total loss for epoch 10: 356.215788
	Epoch 11....
Epoch has taken 0:00:12.597498
Number of used sentences in train = 231
Total loss for epoch 11: 355.556777
	Epoch 12....
Epoch has taken 0:00:12.587059
Number of used sentences in train = 231
Total loss for epoch 12: 354.997296
	Epoch 13....
Epoch has taken 0:00:12.860721
Number of used sentences in train = 231
Total loss for epoch 13: 354.393657
	Epoch 14....
Epoch has taken 0:00:12.856600
Number of used sentences in train = 231
Total loss for epoch 14: 354.097272
Epoch has taken 0:00:12.854022

==================================================================================================
	Training time : 0:32:18.286890
==================================================================================================
	Identification : 0.343

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(18026, 76)
  (lstm): LSTM(92, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18657.611423
validation loss after epoch 0 : 1643.150639
	Epoch 1....
Epoch has taken 0:04:05.569956
Number of used sentences in train = 3226
Total loss for epoch 1: 12154.839079
validation loss after epoch 1 : 1613.072748
	Epoch 2....
Epoch has taken 0:04:05.585841
Number of used sentences in train = 3226
Total loss for epoch 2: 9576.837464
validation loss after epoch 2 : 1705.477917
	Epoch 3....
Epoch has taken 0:03:56.384037
Number of used sentences in train = 3226
Total loss for epoch 3: 7922.101972
validation loss after epoch 3 : 1885.172989
	Epoch 4....
Epoch has taken 0:03:41.491430
Number of used sentences in train = 3226
Total loss for epoch 4: 7083.533932
validation loss after epoch 4 : 2052.278419
	Epoch 5....
Epoch has taken 0:03:43.233454
Number of used sentences in train = 3226
Total loss for epoch 5: 6673.542743
validation loss after epoch 5 : 2129.285899
	Epoch 6....
Epoch has taken 0:03:43.350051
Number of used sentences in train = 3226
Total loss for epoch 6: 6454.954056
validation loss after epoch 6 : 2352.308034
	Epoch 7....
Epoch has taken 0:03:43.267866
Number of used sentences in train = 3226
Total loss for epoch 7: 6358.862172
validation loss after epoch 7 : 2298.694986
	Epoch 8....
Epoch has taken 0:03:40.382372
Number of used sentences in train = 3226
Total loss for epoch 8: 6288.262122
validation loss after epoch 8 : 2377.634938
	Epoch 9....
Epoch has taken 0:03:55.581473
Number of used sentences in train = 3226
Total loss for epoch 9: 6244.026407
validation loss after epoch 9 : 2537.164570
	Epoch 10....
Epoch has taken 0:03:46.288996
Number of used sentences in train = 3226
Total loss for epoch 10: 6224.698904
validation loss after epoch 10 : 2536.377732
	Epoch 11....
Epoch has taken 0:03:43.282406
Number of used sentences in train = 3226
Total loss for epoch 11: 6207.718077
validation loss after epoch 11 : 2598.447781
	Epoch 12....
Epoch has taken 0:03:40.839782
Number of used sentences in train = 3226
Total loss for epoch 12: 6197.067746
validation loss after epoch 12 : 2661.073881
	Epoch 13....
Epoch has taken 0:03:41.297301
Number of used sentences in train = 3226
Total loss for epoch 13: 6191.320263
validation loss after epoch 13 : 2691.077127
	Epoch 14....
Epoch has taken 0:03:43.381475
Number of used sentences in train = 3226
Total loss for epoch 14: 6180.025285
validation loss after epoch 14 : 2681.527876
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(18026, 76)
  (lstm): LSTM(92, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.462144
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2542.892553
	Epoch 1....
Epoch has taken 0:00:21.866661
Number of used sentences in train = 359
Total loss for epoch 1: 1120.529117
	Epoch 2....
Epoch has taken 0:00:21.861658
Number of used sentences in train = 359
Total loss for epoch 2: 808.416057
	Epoch 3....
Epoch has taken 0:00:21.856142
Number of used sentences in train = 359
Total loss for epoch 3: 737.856850
	Epoch 4....
Epoch has taken 0:00:21.872658
Number of used sentences in train = 359
Total loss for epoch 4: 711.677055
	Epoch 5....
Epoch has taken 0:00:21.859106
Number of used sentences in train = 359
Total loss for epoch 5: 690.791923
	Epoch 6....
Epoch has taken 0:00:21.853709
Number of used sentences in train = 359
Total loss for epoch 6: 681.080659
	Epoch 7....
Epoch has taken 0:00:21.864025
Number of used sentences in train = 359
Total loss for epoch 7: 676.274078
	Epoch 8....
Epoch has taken 0:00:21.861593
Number of used sentences in train = 359
Total loss for epoch 8: 674.925415
	Epoch 9....
Epoch has taken 0:00:21.856337
Number of used sentences in train = 359
Total loss for epoch 9: 674.197814
	Epoch 10....
Epoch has taken 0:00:21.858929
Number of used sentences in train = 359
Total loss for epoch 10: 673.729163
	Epoch 11....
Epoch has taken 0:00:21.856687
Number of used sentences in train = 359
Total loss for epoch 11: 673.350419
	Epoch 12....
Epoch has taken 0:00:21.865097
Number of used sentences in train = 359
Total loss for epoch 12: 673.036669
	Epoch 13....
Epoch has taken 0:00:21.851700
Number of used sentences in train = 359
Total loss for epoch 13: 672.784216
	Epoch 14....
Epoch has taken 0:00:21.869953
Number of used sentences in train = 359
Total loss for epoch 14: 672.574487
Epoch has taken 0:00:21.856679

==================================================================================================
	Training time : 1:02:21.994864
==================================================================================================
	Identification : 0.043

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 93, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 24, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 58, 'lstmDropout': 0.35, 'denseActivation': 'tanh', 'wordDim': 166, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 24)
  (w_embeddings): Embedding(9249, 166)
  (lstm): LSTM(190, 58, num_layers=2, dropout=0.35, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=93, bias=True)
  (linear2): Linear(in_features=93, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 18252.959564
validation loss after epoch 0 : 1668.608997
	Epoch 1....
Epoch has taken 0:03:11.795089
Number of used sentences in train = 2811
Total loss for epoch 1: 9669.302748
validation loss after epoch 1 : 1068.704678
	Epoch 2....
Epoch has taken 0:02:57.766054
Number of used sentences in train = 2811
Total loss for epoch 2: 7512.971994
validation loss after epoch 2 : 1076.309893
	Epoch 3....
Epoch has taken 0:03:06.638860
Number of used sentences in train = 2811
Total loss for epoch 3: 6321.807639
validation loss after epoch 3 : 1157.048052
	Epoch 4....
Epoch has taken 0:02:57.875927
Number of used sentences in train = 2811
Total loss for epoch 4: 5752.955994
validation loss after epoch 4 : 1092.154076
	Epoch 5....
Epoch has taken 0:02:57.785663
Number of used sentences in train = 2811
Total loss for epoch 5: 5351.222165
validation loss after epoch 5 : 1245.664928
	Epoch 6....
Epoch has taken 0:02:56.420426
Number of used sentences in train = 2811
Total loss for epoch 6: 5186.275578
validation loss after epoch 6 : 1274.824368
	Epoch 7....
Epoch has taken 0:02:57.736306
Number of used sentences in train = 2811
Total loss for epoch 7: 5005.981465
validation loss after epoch 7 : 1302.910600
	Epoch 8....
Epoch has taken 0:02:57.406368
Number of used sentences in train = 2811
Total loss for epoch 8: 4943.951717
validation loss after epoch 8 : 1376.046476
	Epoch 9....
Epoch has taken 0:02:57.399151
Number of used sentences in train = 2811
Total loss for epoch 9: 4871.042426
validation loss after epoch 9 : 1391.297347
	Epoch 10....
Epoch has taken 0:02:56.653247
Number of used sentences in train = 2811
Total loss for epoch 10: 4794.766154
validation loss after epoch 10 : 1356.162916
	Epoch 11....
Epoch has taken 0:03:02.933695
Number of used sentences in train = 2811
Total loss for epoch 11: 4768.624858
validation loss after epoch 11 : 1432.645182
	Epoch 12....
Epoch has taken 0:03:14.550948
Number of used sentences in train = 2811
Total loss for epoch 12: 4733.921453
validation loss after epoch 12 : 1471.372561
	Epoch 13....
Epoch has taken 0:03:13.248393
Number of used sentences in train = 2811
Total loss for epoch 13: 4674.944546
validation loss after epoch 13 : 1434.967515
	Epoch 14....
Epoch has taken 0:03:04.795158
Number of used sentences in train = 2811
Total loss for epoch 14: 4623.356049
validation loss after epoch 14 : 1419.472159
	TransitionClassifier(
  (p_embeddings): Embedding(18, 24)
  (w_embeddings): Embedding(9249, 166)
  (lstm): LSTM(190, 58, num_layers=2, dropout=0.35, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=93, bias=True)
  (linear2): Linear(in_features=93, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.284286
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2114.926831
	Epoch 1....
Epoch has taken 0:00:18.860400
Number of used sentences in train = 313
Total loss for epoch 1: 792.255120
	Epoch 2....
Epoch has taken 0:00:18.870784
Number of used sentences in train = 313
Total loss for epoch 2: 626.375352
	Epoch 3....
Epoch has taken 0:00:18.837482
Number of used sentences in train = 313
Total loss for epoch 3: 560.256241
	Epoch 4....
Epoch has taken 0:00:18.831378
Number of used sentences in train = 313
Total loss for epoch 4: 534.694622
	Epoch 5....
Epoch has taken 0:00:18.834413
Number of used sentences in train = 313
Total loss for epoch 5: 519.738979
	Epoch 6....
Epoch has taken 0:00:18.856105
Number of used sentences in train = 313
Total loss for epoch 6: 517.855767
	Epoch 7....
Epoch has taken 0:00:18.799453
Number of used sentences in train = 313
Total loss for epoch 7: 520.122726
	Epoch 8....
Epoch has taken 0:00:18.835154
Number of used sentences in train = 313
Total loss for epoch 8: 520.805466
	Epoch 9....
Epoch has taken 0:00:18.838592
Number of used sentences in train = 313
Total loss for epoch 9: 512.143752
	Epoch 10....
Epoch has taken 0:00:18.832959
Number of used sentences in train = 313
Total loss for epoch 10: 508.395221
	Epoch 11....
Epoch has taken 0:00:18.829309
Number of used sentences in train = 313
Total loss for epoch 11: 506.225134
	Epoch 12....
Epoch has taken 0:00:18.833938
Number of used sentences in train = 313
Total loss for epoch 12: 506.082203
	Epoch 13....
Epoch has taken 0:00:18.840416
Number of used sentences in train = 313
Total loss for epoch 13: 505.761157
	Epoch 14....
Epoch has taken 0:00:18.832560
Number of used sentences in train = 313
Total loss for epoch 14: 503.610218
Epoch has taken 0:00:18.842192

==================================================================================================
	Training time : 0:50:13.386353
==================================================================================================
	Identification : 0.182

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 24)
  (w_embeddings): Embedding(7108, 166)
  (lstm): LSTM(190, 58, num_layers=2, dropout=0.35, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=93, bias=True)
  (linear2): Linear(in_features=93, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 13924.435290
validation loss after epoch 0 : 1016.101301
	Epoch 1....
Epoch has taken 0:02:01.376125
Number of used sentences in train = 2074
Total loss for epoch 1: 7104.925391
validation loss after epoch 1 : 915.471159
	Epoch 2....
Epoch has taken 0:02:01.534667
Number of used sentences in train = 2074
Total loss for epoch 2: 5324.296744
validation loss after epoch 2 : 974.415329
	Epoch 3....
Epoch has taken 0:02:01.703259
Number of used sentences in train = 2074
Total loss for epoch 3: 4595.732695
validation loss after epoch 3 : 963.242186
	Epoch 4....
Epoch has taken 0:02:01.541984
Number of used sentences in train = 2074
Total loss for epoch 4: 4126.347055
validation loss after epoch 4 : 1024.468151
	Epoch 5....
Epoch has taken 0:02:01.655275
Number of used sentences in train = 2074
Total loss for epoch 5: 3824.804430
validation loss after epoch 5 : 1058.330193
	Epoch 6....
Epoch has taken 0:02:01.659894
Number of used sentences in train = 2074
Total loss for epoch 6: 3658.962961
validation loss after epoch 6 : 1096.934002
	Epoch 7....
Epoch has taken 0:02:00.670279
Number of used sentences in train = 2074
Total loss for epoch 7: 3531.022791
validation loss after epoch 7 : 1164.212287
	Epoch 8....
Epoch has taken 0:02:01.111308
Number of used sentences in train = 2074
Total loss for epoch 8: 3482.493293
validation loss after epoch 8 : 1306.946797
	Epoch 9....
Epoch has taken 0:02:01.615566
Number of used sentences in train = 2074
Total loss for epoch 9: 3426.802620
validation loss after epoch 9 : 1325.914757
	Epoch 10....
Epoch has taken 0:02:01.484973
Number of used sentences in train = 2074
Total loss for epoch 10: 3373.991000
validation loss after epoch 10 : 1237.960620
	Epoch 11....
Epoch has taken 0:02:01.503475
Number of used sentences in train = 2074
Total loss for epoch 11: 3324.358533
validation loss after epoch 11 : 1307.148062
	Epoch 12....
Epoch has taken 0:02:00.591735
Number of used sentences in train = 2074
Total loss for epoch 12: 3307.888377
validation loss after epoch 12 : 1286.210572
	Epoch 13....
Epoch has taken 0:02:01.510305
Number of used sentences in train = 2074
Total loss for epoch 13: 3280.521683
validation loss after epoch 13 : 1381.380759
	Epoch 14....
Epoch has taken 0:02:01.598534
Number of used sentences in train = 2074
Total loss for epoch 14: 3308.183417
validation loss after epoch 14 : 1290.740858
	TransitionClassifier(
  (p_embeddings): Embedding(18, 24)
  (w_embeddings): Embedding(7108, 166)
  (lstm): LSTM(190, 58, num_layers=2, dropout=0.35, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=93, bias=True)
  (linear2): Linear(in_features=93, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.551864
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2269.823930
	Epoch 1....
Epoch has taken 0:00:12.439049
Number of used sentences in train = 231
Total loss for epoch 1: 801.824104
	Epoch 2....
Epoch has taken 0:00:12.947510
Number of used sentences in train = 231
Total loss for epoch 2: 570.092986
	Epoch 3....
Epoch has taken 0:00:13.729907
Number of used sentences in train = 231
Total loss for epoch 3: 449.102295
	Epoch 4....
Epoch has taken 0:00:13.662290
Number of used sentences in train = 231
Total loss for epoch 4: 406.231143
	Epoch 5....
Epoch has taken 0:00:13.746492
Number of used sentences in train = 231
Total loss for epoch 5: 377.207107
	Epoch 6....
Epoch has taken 0:00:13.755478
Number of used sentences in train = 231
Total loss for epoch 6: 365.271538
	Epoch 7....
Epoch has taken 0:00:13.734512
Number of used sentences in train = 231
Total loss for epoch 7: 357.432294
	Epoch 8....
Epoch has taken 0:00:13.743604
Number of used sentences in train = 231
Total loss for epoch 8: 364.087641
	Epoch 9....
Epoch has taken 0:00:13.747286
Number of used sentences in train = 231
Total loss for epoch 9: 361.685596
	Epoch 10....
Epoch has taken 0:00:13.511874
Number of used sentences in train = 231
Total loss for epoch 10: 361.593214
	Epoch 11....
Epoch has taken 0:00:13.747008
Number of used sentences in train = 231
Total loss for epoch 11: 354.954776
	Epoch 12....
Epoch has taken 0:00:13.743529
Number of used sentences in train = 231
Total loss for epoch 12: 350.136485
	Epoch 13....
Epoch has taken 0:00:13.742118
Number of used sentences in train = 231
Total loss for epoch 13: 359.416274
	Epoch 14....
Epoch has taken 0:00:13.744516
Number of used sentences in train = 231
Total loss for epoch 14: 348.468827
Epoch has taken 0:00:13.743180

==================================================================================================
	Training time : 0:33:45.199778
==================================================================================================
	Identification : 0.196

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 24)
  (w_embeddings): Embedding(17954, 166)
  (lstm): LSTM(190, 58, num_layers=2, dropout=0.35, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=93, bias=True)
  (linear2): Linear(in_features=93, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 21345.361149
validation loss after epoch 0 : 1758.204717
	Epoch 1....
Epoch has taken 0:04:13.061313
Number of used sentences in train = 3226
Total loss for epoch 1: 12500.572069
validation loss after epoch 1 : 1611.479276
	Epoch 2....
Epoch has taken 0:03:56.336032
Number of used sentences in train = 3226
Total loss for epoch 2: 9869.926315
validation loss after epoch 2 : 1637.440035
	Epoch 3....
Epoch has taken 0:03:58.026669
Number of used sentences in train = 3226
Total loss for epoch 3: 8448.431783
validation loss after epoch 3 : 1781.785229
	Epoch 4....
Epoch has taken 0:03:57.398325
Number of used sentences in train = 3226
Total loss for epoch 4: 7559.620477
validation loss after epoch 4 : 1843.436863
	Epoch 5....
Epoch has taken 0:03:58.558763
Number of used sentences in train = 3226
Total loss for epoch 5: 7120.164768
validation loss after epoch 5 : 2030.951956
	Epoch 6....
Epoch has taken 0:03:56.494714
Number of used sentences in train = 3226
Total loss for epoch 6: 6982.282537
validation loss after epoch 6 : 2063.873593
	Epoch 7....
Epoch has taken 0:03:58.559399
Number of used sentences in train = 3226
Total loss for epoch 7: 6692.462383
validation loss after epoch 7 : 2211.335287
	Epoch 8....
Epoch has taken 0:03:57.439351
Number of used sentences in train = 3226
Total loss for epoch 8: 6599.946308
validation loss after epoch 8 : 2219.927914
	Epoch 9....
Epoch has taken 0:03:56.836857
Number of used sentences in train = 3226
Total loss for epoch 9: 6497.131138
validation loss after epoch 9 : 2361.561538
	Epoch 10....
Epoch has taken 0:03:57.088218
Number of used sentences in train = 3226
Total loss for epoch 10: 6449.107952
validation loss after epoch 10 : 2324.623494
	Epoch 11....
Epoch has taken 0:03:57.604047
Number of used sentences in train = 3226
Total loss for epoch 11: 6394.306882
validation loss after epoch 11 : 2297.918122
	Epoch 12....
Epoch has taken 0:03:57.605798
Number of used sentences in train = 3226
Total loss for epoch 12: 6353.404082
validation loss after epoch 12 : 2387.996782
	Epoch 13....
Epoch has taken 0:03:58.507969
Number of used sentences in train = 3226
Total loss for epoch 13: 6355.371038
validation loss after epoch 13 : 2245.952492
	Epoch 14....
Epoch has taken 0:03:58.526783
Number of used sentences in train = 3226
Total loss for epoch 14: 6301.991920
validation loss after epoch 14 : 2313.243554
	TransitionClassifier(
  (p_embeddings): Embedding(13, 24)
  (w_embeddings): Embedding(17954, 166)
  (lstm): LSTM(190, 58, num_layers=2, dropout=0.35, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=93, bias=True)
  (linear2): Linear(in_features=93, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:57.806299
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2435.495364
	Epoch 1....
Epoch has taken 0:00:23.423361
Number of used sentences in train = 359
Total loss for epoch 1: 1198.025915
	Epoch 2....
Epoch has taken 0:00:23.416635
Number of used sentences in train = 359
Total loss for epoch 2: 951.652149
	Epoch 3....
Epoch has taken 0:00:23.404607
Number of used sentences in train = 359
Total loss for epoch 3: 817.912075
	Epoch 4....
Epoch has taken 0:00:23.399335
Number of used sentences in train = 359
Total loss for epoch 4: 751.154502
	Epoch 5....
Epoch has taken 0:00:23.400902
Number of used sentences in train = 359
Total loss for epoch 5: 703.780142
	Epoch 6....
Epoch has taken 0:00:25.609868
Number of used sentences in train = 359
Total loss for epoch 6: 705.036040
	Epoch 7....
Epoch has taken 0:00:23.414046
Number of used sentences in train = 359
Total loss for epoch 7: 696.921859
	Epoch 8....
Epoch has taken 0:00:23.390185
Number of used sentences in train = 359
Total loss for epoch 8: 687.643945
	Epoch 9....
Epoch has taken 0:00:23.410053
Number of used sentences in train = 359
Total loss for epoch 9: 710.486406
	Epoch 10....
Epoch has taken 0:00:23.408174
Number of used sentences in train = 359
Total loss for epoch 10: 681.347700
	Epoch 11....
Epoch has taken 0:00:23.406452
Number of used sentences in train = 359
Total loss for epoch 11: 678.379814
	Epoch 12....
Epoch has taken 0:00:23.398229
Number of used sentences in train = 359
Total loss for epoch 12: 677.588347
	Epoch 13....
Epoch has taken 0:00:23.406682
Number of used sentences in train = 359
Total loss for epoch 13: 680.410665
	Epoch 14....
Epoch has taken 0:00:23.387192
Number of used sentences in train = 359
Total loss for epoch 14: 673.988814
Epoch has taken 0:00:23.392944

==================================================================================================
	Training time : 1:05:33.827580
==================================================================================================
	Identification : 0.301

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 25, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 42, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 55, 'lstmDropout': 0.13, 'denseActivation': 'tanh', 'wordDim': 71, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(1882, 71)
  (lstm): LSTM(113, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10418.105343
validation loss after epoch 0 : 855.013180
	Epoch 1....
Epoch has taken 0:02:44.346294
Number of used sentences in train = 2811
Total loss for epoch 1: 7641.628958
validation loss after epoch 1 : 859.492342
	Epoch 2....
Epoch has taken 0:02:45.732055
Number of used sentences in train = 2811
Total loss for epoch 2: 6920.793462
validation loss after epoch 2 : 818.411251
	Epoch 3....
Epoch has taken 0:02:45.961672
Number of used sentences in train = 2811
Total loss for epoch 3: 6307.959709
validation loss after epoch 3 : 847.285272
	Epoch 4....
Epoch has taken 0:02:43.661439
Number of used sentences in train = 2811
Total loss for epoch 4: 5855.518096
validation loss after epoch 4 : 870.394204
	Epoch 5....
Epoch has taken 0:02:43.755258
Number of used sentences in train = 2811
Total loss for epoch 5: 5501.777841
validation loss after epoch 5 : 867.841594
	Epoch 6....
Epoch has taken 0:02:46.141308
Number of used sentences in train = 2811
Total loss for epoch 6: 5280.967164
validation loss after epoch 6 : 910.746148
	Epoch 7....
Epoch has taken 0:02:46.214137
Number of used sentences in train = 2811
Total loss for epoch 7: 5108.804888
validation loss after epoch 7 : 933.899323
	Epoch 8....
Epoch has taken 0:02:46.167943
Number of used sentences in train = 2811
Total loss for epoch 8: 4962.968626
validation loss after epoch 8 : 958.221324
	Epoch 9....
Epoch has taken 0:02:46.090055
Number of used sentences in train = 2811
Total loss for epoch 9: 4852.705813
validation loss after epoch 9 : 1000.729776
	Epoch 10....
Epoch has taken 0:02:46.025859
Number of used sentences in train = 2811
Total loss for epoch 10: 4788.991046
validation loss after epoch 10 : 1008.072375
	Epoch 11....
Epoch has taken 0:02:45.704243
Number of used sentences in train = 2811
Total loss for epoch 11: 4729.949661
validation loss after epoch 11 : 1008.310768
	Epoch 12....
Epoch has taken 0:02:46.099006
Number of used sentences in train = 2811
Total loss for epoch 12: 4668.286458
validation loss after epoch 12 : 1015.877635
	Epoch 13....
Epoch has taken 0:02:45.999134
Number of used sentences in train = 2811
Total loss for epoch 13: 4627.561557
validation loss after epoch 13 : 1042.832621
	Epoch 14....
Epoch has taken 0:02:46.158962
Number of used sentences in train = 2811
Total loss for epoch 14: 4622.652271
validation loss after epoch 14 : 1087.613640
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(1882, 71)
  (lstm): LSTM(113, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:54.034134
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1355.544670
	Epoch 1....
Epoch has taken 0:00:17.599963
Number of used sentences in train = 313
Total loss for epoch 1: 716.100620
	Epoch 2....
Epoch has taken 0:00:17.581615
Number of used sentences in train = 313
Total loss for epoch 2: 608.746373
	Epoch 3....
Epoch has taken 0:00:17.603426
Number of used sentences in train = 313
Total loss for epoch 3: 563.947536
	Epoch 4....
Epoch has taken 0:00:17.608956
Number of used sentences in train = 313
Total loss for epoch 4: 544.678420
	Epoch 5....
Epoch has taken 0:00:17.599848
Number of used sentences in train = 313
Total loss for epoch 5: 540.296283
	Epoch 6....
Epoch has taken 0:00:17.574546
Number of used sentences in train = 313
Total loss for epoch 6: 532.287768
	Epoch 7....
Epoch has taken 0:00:17.581510
Number of used sentences in train = 313
Total loss for epoch 7: 522.560824
	Epoch 8....
Epoch has taken 0:00:17.583346
Number of used sentences in train = 313
Total loss for epoch 8: 518.726592
	Epoch 9....
Epoch has taken 0:00:17.584428
Number of used sentences in train = 313
Total loss for epoch 9: 516.409822
	Epoch 10....
Epoch has taken 0:00:17.575475
Number of used sentences in train = 313
Total loss for epoch 10: 517.019353
	Epoch 11....
Epoch has taken 0:00:18.224310
Number of used sentences in train = 313
Total loss for epoch 11: 514.473158
	Epoch 12....
Epoch has taken 0:00:19.442967
Number of used sentences in train = 313
Total loss for epoch 12: 513.144791
	Epoch 13....
Epoch has taken 0:00:19.458352
Number of used sentences in train = 313
Total loss for epoch 13: 511.104914
	Epoch 14....
Epoch has taken 0:00:19.461245
Number of used sentences in train = 313
Total loss for epoch 14: 511.531894
Epoch has taken 0:00:19.459204

==================================================================================================
	Training time : 0:46:04.531900
==================================================================================================
	Identification : 0.305

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(1680, 71)
  (lstm): LSTM(113, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7691.070333
validation loss after epoch 0 : 715.654132
	Epoch 1....
Epoch has taken 0:01:59.477708
Number of used sentences in train = 2074
Total loss for epoch 1: 5265.421369
validation loss after epoch 1 : 759.239221
	Epoch 2....
Epoch has taken 0:01:53.474214
Number of used sentences in train = 2074
Total loss for epoch 2: 4423.723181
validation loss after epoch 2 : 660.142192
	Epoch 3....
Epoch has taken 0:01:52.294691
Number of used sentences in train = 2074
Total loss for epoch 3: 3911.754563
validation loss after epoch 3 : 679.781268
	Epoch 4....
Epoch has taken 0:01:53.729990
Number of used sentences in train = 2074
Total loss for epoch 4: 3610.751213
validation loss after epoch 4 : 712.172898
	Epoch 5....
Epoch has taken 0:01:53.752419
Number of used sentences in train = 2074
Total loss for epoch 5: 3445.076035
validation loss after epoch 5 : 742.176802
	Epoch 6....
Epoch has taken 0:01:53.746019
Number of used sentences in train = 2074
Total loss for epoch 6: 3345.788078
validation loss after epoch 6 : 756.944906
	Epoch 7....
Epoch has taken 0:01:53.791307
Number of used sentences in train = 2074
Total loss for epoch 7: 3281.821228
validation loss after epoch 7 : 779.198867
	Epoch 8....
Epoch has taken 0:01:53.731874
Number of used sentences in train = 2074
Total loss for epoch 8: 3236.165329
validation loss after epoch 8 : 798.699334
	Epoch 9....
Epoch has taken 0:01:53.716614
Number of used sentences in train = 2074
Total loss for epoch 9: 3214.131788
validation loss after epoch 9 : 815.063192
	Epoch 10....
Epoch has taken 0:01:53.717670
Number of used sentences in train = 2074
Total loss for epoch 10: 3189.805543
validation loss after epoch 10 : 830.413160
	Epoch 11....
Epoch has taken 0:01:52.774472
Number of used sentences in train = 2074
Total loss for epoch 11: 3181.412986
validation loss after epoch 11 : 838.998607
	Epoch 12....
Epoch has taken 0:01:53.707901
Number of used sentences in train = 2074
Total loss for epoch 12: 3177.636793
validation loss after epoch 12 : 850.672509
	Epoch 13....
Epoch has taken 0:01:53.733818
Number of used sentences in train = 2074
Total loss for epoch 13: 3172.556024
validation loss after epoch 13 : 859.010485
	Epoch 14....
Epoch has taken 0:01:52.590379
Number of used sentences in train = 2074
Total loss for epoch 14: 3170.496751
validation loss after epoch 14 : 861.715269
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(1680, 71)
  (lstm): LSTM(113, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.672604
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1095.817014
	Epoch 1....
Epoch has taken 0:00:11.655971
Number of used sentences in train = 231
Total loss for epoch 1: 645.120543
	Epoch 2....
Epoch has taken 0:00:11.643006
Number of used sentences in train = 231
Total loss for epoch 2: 497.241431
	Epoch 3....
Epoch has taken 0:00:11.656733
Number of used sentences in train = 231
Total loss for epoch 3: 412.069629
	Epoch 4....
Epoch has taken 0:00:11.654109
Number of used sentences in train = 231
Total loss for epoch 4: 366.470596
	Epoch 5....
Epoch has taken 0:00:11.654307
Number of used sentences in train = 231
Total loss for epoch 5: 354.785073
	Epoch 6....
Epoch has taken 0:00:12.871555
Number of used sentences in train = 231
Total loss for epoch 6: 353.563235
	Epoch 7....
Epoch has taken 0:00:12.870252
Number of used sentences in train = 231
Total loss for epoch 7: 349.914196
	Epoch 8....
Epoch has taken 0:00:12.878923
Number of used sentences in train = 231
Total loss for epoch 8: 348.977311
	Epoch 9....
Epoch has taken 0:00:12.881904
Number of used sentences in train = 231
Total loss for epoch 9: 348.342469
	Epoch 10....
Epoch has taken 0:00:12.878531
Number of used sentences in train = 231
Total loss for epoch 10: 347.795370
	Epoch 11....
Epoch has taken 0:00:12.242141
Number of used sentences in train = 231
Total loss for epoch 11: 347.264530
	Epoch 12....
Epoch has taken 0:00:11.653721
Number of used sentences in train = 231
Total loss for epoch 12: 346.941072
	Epoch 13....
Epoch has taken 0:00:11.636205
Number of used sentences in train = 231
Total loss for epoch 13: 346.650446
	Epoch 14....
Epoch has taken 0:00:11.647962
Number of used sentences in train = 231
Total loss for epoch 14: 346.509918
Epoch has taken 0:00:11.600920

==================================================================================================
	Training time : 0:31:29.672348
==================================================================================================
	Identification : 0.225

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 42)
  (w_embeddings): Embedding(3369, 71)
  (lstm): LSTM(113, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12862.962001
validation loss after epoch 0 : 1180.148424
	Epoch 1....
Epoch has taken 0:03:45.057212
Number of used sentences in train = 3226
Total loss for epoch 1: 9832.350932
validation loss after epoch 1 : 1136.432191
	Epoch 2....
Epoch has taken 0:03:34.806224
Number of used sentences in train = 3226
Total loss for epoch 2: 8808.665680
validation loss after epoch 2 : 1126.728676
	Epoch 3....
Epoch has taken 0:03:34.747910
Number of used sentences in train = 3226
Total loss for epoch 3: 8177.402508
validation loss after epoch 3 : 1147.861099
	Epoch 4....
Epoch has taken 0:03:43.529877
Number of used sentences in train = 3226
Total loss for epoch 4: 7666.530035
validation loss after epoch 4 : 1219.003471
	Epoch 5....
Epoch has taken 0:03:38.726318
Number of used sentences in train = 3226
Total loss for epoch 5: 7285.397151
validation loss after epoch 5 : 1246.953089
	Epoch 6....
Epoch has taken 0:04:00.480082
Number of used sentences in train = 3226
Total loss for epoch 6: 7036.365804
validation loss after epoch 6 : 1317.001101
	Epoch 7....
Epoch has taken 0:03:59.662930
Number of used sentences in train = 3226
Total loss for epoch 7: 6839.162246
validation loss after epoch 7 : 1409.267960
	Epoch 8....
Epoch has taken 0:03:41.612119
Number of used sentences in train = 3226
Total loss for epoch 8: 6673.713135
validation loss after epoch 8 : 1470.640007
	Epoch 9....
Epoch has taken 0:03:40.085048
Number of used sentences in train = 3226
Total loss for epoch 9: 6542.576373
validation loss after epoch 9 : 1411.456763
	Epoch 10....
Epoch has taken 0:03:38.657802
Number of used sentences in train = 3226
Total loss for epoch 10: 6496.591620
validation loss after epoch 10 : 1440.749504
	Epoch 11....
Epoch has taken 0:03:37.983331
Number of used sentences in train = 3226
Total loss for epoch 11: 6425.870951
validation loss after epoch 11 : 1542.443265
	Epoch 12....
Epoch has taken 0:03:38.253557
Number of used sentences in train = 3226
Total loss for epoch 12: 6361.356961
validation loss after epoch 12 : 1606.954664
	Epoch 13....
Epoch has taken 0:03:38.903273
Number of used sentences in train = 3226
Total loss for epoch 13: 6324.625585
validation loss after epoch 13 : 1550.053347
	Epoch 14....
Epoch has taken 0:03:39.885744
Number of used sentences in train = 3226
Total loss for epoch 14: 6287.824559
validation loss after epoch 14 : 1583.156477
	TransitionClassifier(
  (p_embeddings): Embedding(13, 42)
  (w_embeddings): Embedding(3369, 71)
  (lstm): LSTM(113, 55, bidirectional=True)
  (linear1): Linear(in_features=880, out_features=25, bias=True)
  (linear2): Linear(in_features=25, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:39.620303
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1875.528243
	Epoch 1....
Epoch has taken 0:00:21.549789
Number of used sentences in train = 359
Total loss for epoch 1: 979.013029
	Epoch 2....
Epoch has taken 0:00:21.547915
Number of used sentences in train = 359
Total loss for epoch 2: 836.650389
	Epoch 3....
Epoch has taken 0:00:21.565418
Number of used sentences in train = 359
Total loss for epoch 3: 747.807212
	Epoch 4....
Epoch has taken 0:00:21.563863
Number of used sentences in train = 359
Total loss for epoch 4: 703.212319
	Epoch 5....
Epoch has taken 0:00:21.553003
Number of used sentences in train = 359
Total loss for epoch 5: 689.273084
	Epoch 6....
Epoch has taken 0:00:21.562986
Number of used sentences in train = 359
Total loss for epoch 6: 680.757101
	Epoch 7....
Epoch has taken 0:00:21.573902
Number of used sentences in train = 359
Total loss for epoch 7: 674.497005
	Epoch 8....
Epoch has taken 0:00:21.560859
Number of used sentences in train = 359
Total loss for epoch 8: 676.338347
	Epoch 9....
Epoch has taken 0:00:21.566052
Number of used sentences in train = 359
Total loss for epoch 9: 672.877351
	Epoch 10....
Epoch has taken 0:00:21.550572
Number of used sentences in train = 359
Total loss for epoch 10: 671.931481
	Epoch 11....
Epoch has taken 0:00:21.548494
Number of used sentences in train = 359
Total loss for epoch 11: 671.639037
	Epoch 12....
Epoch has taken 0:00:21.537274
Number of used sentences in train = 359
Total loss for epoch 12: 671.435853
	Epoch 13....
Epoch has taken 0:00:21.435967
Number of used sentences in train = 359
Total loss for epoch 13: 671.279090
	Epoch 14....
Epoch has taken 0:00:21.261915
Number of used sentences in train = 359
Total loss for epoch 14: 671.150820
Epoch has taken 0:00:21.558415

==================================================================================================
	Training time : 1:00:55.612047
==================================================================================================
	Identification : 0.072

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 57, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 33, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 28, 'lstmDropout': 0.19, 'denseActivation': 'tanh', 'wordDim': 191, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1882, 191)
  (lstm): LSTM(224, 28, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10411.637124
validation loss after epoch 0 : 831.681463
	Epoch 1....
Epoch has taken 0:03:03.529241
Number of used sentences in train = 2811
Total loss for epoch 1: 7244.886160
validation loss after epoch 1 : 817.644357
	Epoch 2....
Epoch has taken 0:03:12.827776
Number of used sentences in train = 2811
Total loss for epoch 2: 6538.380886
validation loss after epoch 2 : 839.597751
	Epoch 3....
Epoch has taken 0:02:59.631689
Number of used sentences in train = 2811
Total loss for epoch 3: 6063.380620
validation loss after epoch 3 : 866.749145
	Epoch 4....
Epoch has taken 0:02:56.119261
Number of used sentences in train = 2811
Total loss for epoch 4: 5742.650130
validation loss after epoch 4 : 871.719603
	Epoch 5....
Epoch has taken 0:02:57.529747
Number of used sentences in train = 2811
Total loss for epoch 5: 5474.170151
validation loss after epoch 5 : 927.830425
	Epoch 6....
Epoch has taken 0:02:57.528883
Number of used sentences in train = 2811
Total loss for epoch 6: 5269.340593
validation loss after epoch 6 : 907.685183
	Epoch 7....
Epoch has taken 0:02:56.031372
Number of used sentences in train = 2811
Total loss for epoch 7: 5196.839322
validation loss after epoch 7 : 938.421755
	Epoch 8....
Epoch has taken 0:02:55.229897
Number of used sentences in train = 2811
Total loss for epoch 8: 5049.510188
validation loss after epoch 8 : 946.608514
	Epoch 9....
Epoch has taken 0:02:55.341799
Number of used sentences in train = 2811
Total loss for epoch 9: 5019.775734
validation loss after epoch 9 : 956.651926
	Epoch 10....
Epoch has taken 0:02:57.413101
Number of used sentences in train = 2811
Total loss for epoch 10: 4925.221829
validation loss after epoch 10 : 927.184134
	Epoch 11....
Epoch has taken 0:02:57.049424
Number of used sentences in train = 2811
Total loss for epoch 11: 4876.515351
validation loss after epoch 11 : 967.544435
	Epoch 12....
Epoch has taken 0:02:55.633846
Number of used sentences in train = 2811
Total loss for epoch 12: 4817.873827
validation loss after epoch 12 : 1005.732527
	Epoch 13....
Epoch has taken 0:02:57.406640
Number of used sentences in train = 2811
Total loss for epoch 13: 4792.148000
validation loss after epoch 13 : 943.957700
	Epoch 14....
Epoch has taken 0:02:57.183710
Number of used sentences in train = 2811
Total loss for epoch 14: 4756.816388
validation loss after epoch 14 : 1046.283808
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1882, 191)
  (lstm): LSTM(224, 28, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.127980
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1429.453087
	Epoch 1....
Epoch has taken 0:00:18.823069
Number of used sentences in train = 313
Total loss for epoch 1: 687.393184
	Epoch 2....
Epoch has taken 0:00:18.810470
Number of used sentences in train = 313
Total loss for epoch 2: 595.935807
	Epoch 3....
Epoch has taken 0:00:18.807642
Number of used sentences in train = 313
Total loss for epoch 3: 572.712398
	Epoch 4....
Epoch has taken 0:00:18.807696
Number of used sentences in train = 313
Total loss for epoch 4: 540.638168
	Epoch 5....
Epoch has taken 0:00:18.811209
Number of used sentences in train = 313
Total loss for epoch 5: 519.633617
	Epoch 6....
Epoch has taken 0:00:18.810117
Number of used sentences in train = 313
Total loss for epoch 6: 517.402028
	Epoch 7....
Epoch has taken 0:00:18.796423
Number of used sentences in train = 313
Total loss for epoch 7: 515.437115
	Epoch 8....
Epoch has taken 0:00:18.807662
Number of used sentences in train = 313
Total loss for epoch 8: 510.750166
	Epoch 9....
Epoch has taken 0:00:18.819776
Number of used sentences in train = 313
Total loss for epoch 9: 505.574546
	Epoch 10....
Epoch has taken 0:00:18.801627
Number of used sentences in train = 313
Total loss for epoch 10: 505.861580
	Epoch 11....
Epoch has taken 0:00:18.820884
Number of used sentences in train = 313
Total loss for epoch 11: 505.790917
	Epoch 12....
Epoch has taken 0:00:18.821348
Number of used sentences in train = 313
Total loss for epoch 12: 504.406453
	Epoch 13....
Epoch has taken 0:00:18.807884
Number of used sentences in train = 313
Total loss for epoch 13: 505.064104
	Epoch 14....
Epoch has taken 0:00:18.818305
Number of used sentences in train = 313
Total loss for epoch 14: 505.031562
Epoch has taken 0:00:18.812011

==================================================================================================
	Training time : 0:49:17.265072
==================================================================================================
	Identification : 0.484

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1680, 191)
  (lstm): LSTM(224, 28, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8187.188993
validation loss after epoch 0 : 694.622051
	Epoch 1....
Epoch has taken 0:01:59.753680
Number of used sentences in train = 2074
Total loss for epoch 1: 5151.434409
validation loss after epoch 1 : 666.789708
	Epoch 2....
Epoch has taken 0:02:01.347901
Number of used sentences in train = 2074
Total loss for epoch 2: 4440.279611
validation loss after epoch 2 : 690.138187
	Epoch 3....
Epoch has taken 0:02:01.485908
Number of used sentences in train = 2074
Total loss for epoch 3: 4013.944904
validation loss after epoch 3 : 731.998390
	Epoch 4....
Epoch has taken 0:01:59.703773
Number of used sentences in train = 2074
Total loss for epoch 4: 3759.495091
validation loss after epoch 4 : 771.650909
	Epoch 5....
Epoch has taken 0:02:01.246809
Number of used sentences in train = 2074
Total loss for epoch 5: 3577.232792
validation loss after epoch 5 : 792.115790
	Epoch 6....
Epoch has taken 0:02:00.135347
Number of used sentences in train = 2074
Total loss for epoch 6: 3502.175962
validation loss after epoch 6 : 757.380598
	Epoch 7....
Epoch has taken 0:02:01.409956
Number of used sentences in train = 2074
Total loss for epoch 7: 3375.327505
validation loss after epoch 7 : 881.894628
	Epoch 8....
Epoch has taken 0:02:01.378981
Number of used sentences in train = 2074
Total loss for epoch 8: 3388.574103
validation loss after epoch 8 : 896.689432
	Epoch 9....
Epoch has taken 0:02:01.176613
Number of used sentences in train = 2074
Total loss for epoch 9: 3320.233337
validation loss after epoch 9 : 832.814128
	Epoch 10....
Epoch has taken 0:02:01.520478
Number of used sentences in train = 2074
Total loss for epoch 10: 3304.767954
validation loss after epoch 10 : 877.041683
	Epoch 11....
Epoch has taken 0:02:00.224718
Number of used sentences in train = 2074
Total loss for epoch 11: 3273.670785
validation loss after epoch 11 : 908.148733
	Epoch 12....
Epoch has taken 0:02:08.718613
Number of used sentences in train = 2074
Total loss for epoch 12: 3257.191914
validation loss after epoch 12 : 940.543737
	Epoch 13....
Epoch has taken 0:02:01.496839
Number of used sentences in train = 2074
Total loss for epoch 13: 3245.168977
validation loss after epoch 13 : 931.392794
	Epoch 14....
Epoch has taken 0:02:00.111610
Number of used sentences in train = 2074
Total loss for epoch 14: 3214.772305
validation loss after epoch 14 : 949.053624
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1680, 191)
  (lstm): LSTM(224, 28, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:59.862432
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1197.163221
	Epoch 1....
Epoch has taken 0:00:12.383132
Number of used sentences in train = 231
Total loss for epoch 1: 467.171174
	Epoch 2....
Epoch has taken 0:00:12.397084
Number of used sentences in train = 231
Total loss for epoch 2: 402.502829
	Epoch 3....
Epoch has taken 0:00:12.396833
Number of used sentences in train = 231
Total loss for epoch 3: 371.587067
	Epoch 4....
Epoch has taken 0:00:12.390291
Number of used sentences in train = 231
Total loss for epoch 4: 359.648007
	Epoch 5....
Epoch has taken 0:00:12.402226
Number of used sentences in train = 231
Total loss for epoch 5: 353.384763
	Epoch 6....
Epoch has taken 0:00:12.408791
Number of used sentences in train = 231
Total loss for epoch 6: 348.889422
	Epoch 7....
Epoch has taken 0:00:12.412453
Number of used sentences in train = 231
Total loss for epoch 7: 354.972328
	Epoch 8....
Epoch has taken 0:00:12.406986
Number of used sentences in train = 231
Total loss for epoch 8: 350.629785
	Epoch 9....
Epoch has taken 0:00:12.405377
Number of used sentences in train = 231
Total loss for epoch 9: 349.328089
	Epoch 10....
Epoch has taken 0:00:12.409894
Number of used sentences in train = 231
Total loss for epoch 10: 348.741223
	Epoch 11....
Epoch has taken 0:00:13.782811
Number of used sentences in train = 231
Total loss for epoch 11: 347.554210
	Epoch 12....
Epoch has taken 0:00:12.404135
Number of used sentences in train = 231
Total loss for epoch 12: 347.081857
	Epoch 13....
Epoch has taken 0:00:12.406077
Number of used sentences in train = 231
Total loss for epoch 13: 346.573769
	Epoch 14....
Epoch has taken 0:00:12.397593
Number of used sentences in train = 231
Total loss for epoch 14: 345.932723
Epoch has taken 0:00:12.394156

==================================================================================================
	Training time : 0:33:27.311008
==================================================================================================
	Identification : 0.221

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 33)
  (w_embeddings): Embedding(3369, 191)
  (lstm): LSTM(224, 28, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12412.206481
validation loss after epoch 0 : 1147.692098
	Epoch 1....
Epoch has taken 0:03:53.127647
Number of used sentences in train = 3226
Total loss for epoch 1: 9222.925425
validation loss after epoch 1 : 1065.330319
	Epoch 2....
Epoch has taken 0:03:52.983872
Number of used sentences in train = 3226
Total loss for epoch 2: 8440.938714
validation loss after epoch 2 : 1121.243630
	Epoch 3....
Epoch has taken 0:03:53.054513
Number of used sentences in train = 3226
Total loss for epoch 3: 7883.882692
validation loss after epoch 3 : 1145.854734
	Epoch 4....
Epoch has taken 0:04:00.270475
Number of used sentences in train = 3226
Total loss for epoch 4: 7549.103408
validation loss after epoch 4 : 1171.685026
	Epoch 5....
Epoch has taken 0:04:22.205293
Number of used sentences in train = 3226
Total loss for epoch 5: 7290.105887
validation loss after epoch 5 : 1188.217288
	Epoch 6....
Epoch has taken 0:03:57.852172
Number of used sentences in train = 3226
Total loss for epoch 6: 7097.413616
validation loss after epoch 6 : 1250.816648
	Epoch 7....
Epoch has taken 0:04:01.003160
Number of used sentences in train = 3226
Total loss for epoch 7: 7000.524605
validation loss after epoch 7 : 1284.280877
	Epoch 8....
Epoch has taken 0:04:00.728687
Number of used sentences in train = 3226
Total loss for epoch 8: 6851.826258
validation loss after epoch 8 : 1317.802510
	Epoch 9....
Epoch has taken 0:04:00.950481
Number of used sentences in train = 3226
Total loss for epoch 9: 6785.902783
validation loss after epoch 9 : 1347.709743
	Epoch 10....
Epoch has taken 0:04:08.662403
Number of used sentences in train = 3226
Total loss for epoch 10: 6730.648394
validation loss after epoch 10 : 1381.533448
	Epoch 11....
Epoch has taken 0:04:06.316426
Number of used sentences in train = 3226
Total loss for epoch 11: 6653.531325
validation loss after epoch 11 : 1442.760815
	Epoch 12....
Epoch has taken 0:04:01.359953
Number of used sentences in train = 3226
Total loss for epoch 12: 6568.518414
validation loss after epoch 12 : 1455.511674
	Epoch 13....
Epoch has taken 0:03:58.394649
Number of used sentences in train = 3226
Total loss for epoch 13: 6544.162052
validation loss after epoch 13 : 1469.425147
	Epoch 14....
Epoch has taken 0:04:00.967898
Number of used sentences in train = 3226
Total loss for epoch 14: 6470.180578
validation loss after epoch 14 : 1528.396073
	TransitionClassifier(
  (p_embeddings): Embedding(13, 33)
  (w_embeddings): Embedding(3369, 191)
  (lstm): LSTM(224, 28, num_layers=2, dropout=0.19, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:59.728421
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1967.216037
	Epoch 1....
Epoch has taken 0:00:23.642148
Number of used sentences in train = 359
Total loss for epoch 1: 927.354482
	Epoch 2....
Epoch has taken 0:00:23.626569
Number of used sentences in train = 359
Total loss for epoch 2: 802.359173
	Epoch 3....
Epoch has taken 0:00:23.673216
Number of used sentences in train = 359
Total loss for epoch 3: 751.475517
	Epoch 4....
Epoch has taken 0:00:23.673442
Number of used sentences in train = 359
Total loss for epoch 4: 726.352910
	Epoch 5....
Epoch has taken 0:00:23.667137
Number of used sentences in train = 359
Total loss for epoch 5: 728.409925
	Epoch 6....
Epoch has taken 0:00:23.657208
Number of used sentences in train = 359
Total loss for epoch 6: 703.027296
	Epoch 7....
Epoch has taken 0:00:23.669428
Number of used sentences in train = 359
Total loss for epoch 7: 694.293156
	Epoch 8....
Epoch has taken 0:00:23.652311
Number of used sentences in train = 359
Total loss for epoch 8: 686.904850
	Epoch 9....
Epoch has taken 0:00:26.042399
Number of used sentences in train = 359
Total loss for epoch 9: 686.858376
	Epoch 10....
Epoch has taken 0:00:26.102402
Number of used sentences in train = 359
Total loss for epoch 10: 680.764704
	Epoch 11....
Epoch has taken 0:00:26.077040
Number of used sentences in train = 359
Total loss for epoch 11: 675.935438
	Epoch 12....
Epoch has taken 0:00:24.078091
Number of used sentences in train = 359
Total loss for epoch 12: 678.590098
	Epoch 13....
Epoch has taken 0:00:23.653788
Number of used sentences in train = 359
Total loss for epoch 13: 675.000366
	Epoch 14....
Epoch has taken 0:00:23.648420
Number of used sentences in train = 359
Total loss for epoch 14: 686.616242
Epoch has taken 0:00:23.663622

==================================================================================================
	Training time : 1:06:20.806031
==================================================================================================
	Identification : 0.258

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 41, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 35, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 60, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 54, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(5862, 54)
  (lstm): LSTM(89, 60, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14554.278109
validation loss after epoch 0 : 1217.758868
	Epoch 1....
Epoch has taken 0:02:56.570658
Number of used sentences in train = 2811
Total loss for epoch 1: 9828.987942
validation loss after epoch 1 : 1033.869407
	Epoch 2....
Epoch has taken 0:02:56.874643
Number of used sentences in train = 2811
Total loss for epoch 2: 8285.557872
validation loss after epoch 2 : 1013.315542
	Epoch 3....
Epoch has taken 0:02:57.082208
Number of used sentences in train = 2811
Total loss for epoch 3: 7322.326095
validation loss after epoch 3 : 1069.670829
	Epoch 4....
Epoch has taken 0:03:02.035548
Number of used sentences in train = 2811
Total loss for epoch 4: 6624.660354
validation loss after epoch 4 : 1058.222189
	Epoch 5....
Epoch has taken 0:03:13.816890
Number of used sentences in train = 2811
Total loss for epoch 5: 6150.317496
validation loss after epoch 5 : 1104.993098
	Epoch 6....
Epoch has taken 0:03:14.001126
Number of used sentences in train = 2811
Total loss for epoch 6: 5749.223432
validation loss after epoch 6 : 1142.454995
	Epoch 7....
Epoch has taken 0:03:12.109419
Number of used sentences in train = 2811
Total loss for epoch 7: 5529.449054
validation loss after epoch 7 : 1219.688760
	Epoch 8....
Epoch has taken 0:03:15.871518
Number of used sentences in train = 2811
Total loss for epoch 8: 5265.460844
validation loss after epoch 8 : 1171.664731
	Epoch 9....
Epoch has taken 0:02:57.099978
Number of used sentences in train = 2811
Total loss for epoch 9: 5170.140167
validation loss after epoch 9 : 1320.800140
	Epoch 10....
Epoch has taken 0:02:56.994916
Number of used sentences in train = 2811
Total loss for epoch 10: 5014.383650
validation loss after epoch 10 : 1305.018187
	Epoch 11....
Epoch has taken 0:02:55.503328
Number of used sentences in train = 2811
Total loss for epoch 11: 4956.886331
validation loss after epoch 11 : 1429.351966
	Epoch 12....
Epoch has taken 0:02:57.025888
Number of used sentences in train = 2811
Total loss for epoch 12: 4890.901065
validation loss after epoch 12 : 1368.413441
	Epoch 13....
Epoch has taken 0:02:57.000008
Number of used sentences in train = 2811
Total loss for epoch 13: 4859.605593
validation loss after epoch 13 : 1403.249734
	Epoch 14....
Epoch has taken 0:02:57.148789
Number of used sentences in train = 2811
Total loss for epoch 14: 4768.308971
validation loss after epoch 14 : 1376.514030
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(5862, 54)
  (lstm): LSTM(89, 60, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:11.170840
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1675.019714
	Epoch 1....
Epoch has taken 0:00:18.748551
Number of used sentences in train = 313
Total loss for epoch 1: 910.153816
	Epoch 2....
Epoch has taken 0:00:18.728853
Number of used sentences in train = 313
Total loss for epoch 2: 723.355645
	Epoch 3....
Epoch has taken 0:00:18.731927
Number of used sentences in train = 313
Total loss for epoch 3: 609.861406
	Epoch 4....
Epoch has taken 0:00:18.751188
Number of used sentences in train = 313
Total loss for epoch 4: 566.475715
	Epoch 5....
Epoch has taken 0:00:18.736514
Number of used sentences in train = 313
Total loss for epoch 5: 569.015790
	Epoch 6....
Epoch has taken 0:00:18.746034
Number of used sentences in train = 313
Total loss for epoch 6: 546.171089
	Epoch 7....
Epoch has taken 0:00:18.732867
Number of used sentences in train = 313
Total loss for epoch 7: 533.338216
	Epoch 8....
Epoch has taken 0:00:18.737576
Number of used sentences in train = 313
Total loss for epoch 8: 526.196471
	Epoch 9....
Epoch has taken 0:00:18.717718
Number of used sentences in train = 313
Total loss for epoch 9: 523.229189
	Epoch 10....
Epoch has taken 0:00:18.740022
Number of used sentences in train = 313
Total loss for epoch 10: 520.707921
	Epoch 11....
Epoch has taken 0:00:18.750396
Number of used sentences in train = 313
Total loss for epoch 11: 519.275913
	Epoch 12....
Epoch has taken 0:00:18.745445
Number of used sentences in train = 313
Total loss for epoch 12: 514.251157
	Epoch 13....
Epoch has taken 0:00:18.733380
Number of used sentences in train = 313
Total loss for epoch 13: 519.552093
	Epoch 14....
Epoch has taken 0:00:18.749184
Number of used sentences in train = 313
Total loss for epoch 14: 517.154371
Epoch has taken 0:00:18.744080

==================================================================================================
	Training time : 0:50:21.892639
==================================================================================================
	Identification : 0.221

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(5617, 54)
  (lstm): LSTM(89, 60, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11924.981356
validation loss after epoch 0 : 1003.601663
	Epoch 1....
Epoch has taken 0:02:00.735901
Number of used sentences in train = 2074
Total loss for epoch 1: 7173.382085
validation loss after epoch 1 : 799.766396
	Epoch 2....
Epoch has taken 0:01:58.053694
Number of used sentences in train = 2074
Total loss for epoch 2: 5705.779310
validation loss after epoch 2 : 903.019123
	Epoch 3....
Epoch has taken 0:01:58.067180
Number of used sentences in train = 2074
Total loss for epoch 3: 5043.850204
validation loss after epoch 3 : 856.935933
	Epoch 4....
Epoch has taken 0:01:58.132057
Number of used sentences in train = 2074
Total loss for epoch 4: 4547.945800
validation loss after epoch 4 : 969.002046
	Epoch 5....
Epoch has taken 0:01:58.152084
Number of used sentences in train = 2074
Total loss for epoch 5: 4207.774567
validation loss after epoch 5 : 929.847333
	Epoch 6....
Epoch has taken 0:01:58.114651
Number of used sentences in train = 2074
Total loss for epoch 6: 3948.324828
validation loss after epoch 6 : 969.872792
	Epoch 7....
Epoch has taken 0:01:58.121301
Number of used sentences in train = 2074
Total loss for epoch 7: 3752.762773
validation loss after epoch 7 : 995.032760
	Epoch 8....
Epoch has taken 0:01:58.149331
Number of used sentences in train = 2074
Total loss for epoch 8: 3596.367943
validation loss after epoch 8 : 1062.595205
	Epoch 9....
Epoch has taken 0:01:59.829251
Number of used sentences in train = 2074
Total loss for epoch 9: 3502.390981
validation loss after epoch 9 : 1031.204908
	Epoch 10....
Epoch has taken 0:01:58.035900
Number of used sentences in train = 2074
Total loss for epoch 10: 3453.911731
validation loss after epoch 10 : 1130.783980
	Epoch 11....
Epoch has taken 0:01:58.015088
Number of used sentences in train = 2074
Total loss for epoch 11: 3418.596450
validation loss after epoch 11 : 1205.861173
	Epoch 12....
Epoch has taken 0:01:58.207428
Number of used sentences in train = 2074
Total loss for epoch 12: 3375.393675
validation loss after epoch 12 : 1158.335878
	Epoch 13....
Epoch has taken 0:01:58.160836
Number of used sentences in train = 2074
Total loss for epoch 13: 3329.613104
validation loss after epoch 13 : 1207.996041
	Epoch 14....
Epoch has taken 0:01:58.206873
Number of used sentences in train = 2074
Total loss for epoch 14: 3319.016153
validation loss after epoch 14 : 1161.729318
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(5617, 54)
  (lstm): LSTM(89, 60, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:58.175642
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1424.875844
	Epoch 1....
Epoch has taken 0:00:12.099307
Number of used sentences in train = 231
Total loss for epoch 1: 749.815805
	Epoch 2....
Epoch has taken 0:00:12.092065
Number of used sentences in train = 231
Total loss for epoch 2: 568.468608
	Epoch 3....
Epoch has taken 0:00:12.108739
Number of used sentences in train = 231
Total loss for epoch 3: 471.885125
	Epoch 4....
Epoch has taken 0:00:12.090049
Number of used sentences in train = 231
Total loss for epoch 4: 431.608444
	Epoch 5....
Epoch has taken 0:00:12.098725
Number of used sentences in train = 231
Total loss for epoch 5: 388.012805
	Epoch 6....
Epoch has taken 0:00:12.094235
Number of used sentences in train = 231
Total loss for epoch 6: 373.215623
	Epoch 7....
Epoch has taken 0:00:12.102458
Number of used sentences in train = 231
Total loss for epoch 7: 360.537481
	Epoch 8....
Epoch has taken 0:00:12.094955
Number of used sentences in train = 231
Total loss for epoch 8: 357.273754
	Epoch 9....
Epoch has taken 0:00:12.110221
Number of used sentences in train = 231
Total loss for epoch 9: 361.741287
	Epoch 10....
Epoch has taken 0:00:12.093871
Number of used sentences in train = 231
Total loss for epoch 10: 358.232586
	Epoch 11....
Epoch has taken 0:00:12.092687
Number of used sentences in train = 231
Total loss for epoch 11: 351.015205
	Epoch 12....
Epoch has taken 0:00:12.080066
Number of used sentences in train = 231
Total loss for epoch 12: 353.519913
	Epoch 13....
Epoch has taken 0:00:12.104698
Number of used sentences in train = 231
Total loss for epoch 13: 350.374591
	Epoch 14....
Epoch has taken 0:00:12.086620
Number of used sentences in train = 231
Total loss for epoch 14: 348.293332
Epoch has taken 0:00:12.095324

==================================================================================================
	Training time : 0:32:37.939821
==================================================================================================
	Identification : 0.395

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(6838, 54)
  (lstm): LSTM(89, 60, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17916.262308
validation loss after epoch 0 : 1418.880939
	Epoch 1....
Epoch has taken 0:03:57.504921
Number of used sentences in train = 3226
Total loss for epoch 1: 12298.482644
validation loss after epoch 1 : 1399.318124
	Epoch 2....
Epoch has taken 0:03:57.633528
Number of used sentences in train = 3226
Total loss for epoch 2: 10780.061816
validation loss after epoch 2 : 1407.119961
	Epoch 3....
Epoch has taken 0:03:57.760609
Number of used sentences in train = 3226
Total loss for epoch 3: 9801.393484
validation loss after epoch 3 : 1408.550670
	Epoch 4....
Epoch has taken 0:03:58.437285
Number of used sentences in train = 3226
Total loss for epoch 4: 9095.859219
validation loss after epoch 4 : 1420.843518
	Epoch 5....
Epoch has taken 0:03:58.087451
Number of used sentences in train = 3226
Total loss for epoch 5: 8537.081922
validation loss after epoch 5 : 1511.973713
	Epoch 6....
Epoch has taken 0:03:51.913061
Number of used sentences in train = 3226
Total loss for epoch 6: 8041.642177
validation loss after epoch 6 : 1612.072998
	Epoch 7....
Epoch has taken 0:04:22.772687
Number of used sentences in train = 3226
Total loss for epoch 7: 7631.117761
validation loss after epoch 7 : 1758.762699
	Epoch 8....
Epoch has taken 0:03:57.954757
Number of used sentences in train = 3226
Total loss for epoch 8: 7306.254365
validation loss after epoch 8 : 1594.033212
	Epoch 9....
Epoch has taken 0:04:24.060529
Number of used sentences in train = 3226
Total loss for epoch 9: 7138.118541
validation loss after epoch 9 : 1778.979614
	Epoch 10....
Epoch has taken 0:04:03.959956
Number of used sentences in train = 3226
Total loss for epoch 10: 6903.188627
validation loss after epoch 10 : 1763.452603
	Epoch 11....
Epoch has taken 0:04:05.161256
Number of used sentences in train = 3226
Total loss for epoch 11: 6769.412882
validation loss after epoch 11 : 2020.109185
	Epoch 12....
Epoch has taken 0:04:03.670771
Number of used sentences in train = 3226
Total loss for epoch 12: 6585.207498
validation loss after epoch 12 : 2020.892740
	Epoch 13....
Epoch has taken 0:03:58.813674
Number of used sentences in train = 3226
Total loss for epoch 13: 6559.086434
validation loss after epoch 13 : 2174.986848
	Epoch 14....
Epoch has taken 0:04:04.872995
Number of used sentences in train = 3226
Total loss for epoch 14: 6484.277720
validation loss after epoch 14 : 2040.421295
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(6838, 54)
  (lstm): LSTM(89, 60, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:55.817757
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2112.365705
	Epoch 1....
Epoch has taken 0:00:22.765385
Number of used sentences in train = 359
Total loss for epoch 1: 1214.293822
	Epoch 2....
Epoch has taken 0:00:22.832852
Number of used sentences in train = 359
Total loss for epoch 2: 963.918010
	Epoch 3....
Epoch has taken 0:00:22.810161
Number of used sentences in train = 359
Total loss for epoch 3: 819.533722
	Epoch 4....
Epoch has taken 0:00:22.724449
Number of used sentences in train = 359
Total loss for epoch 4: 763.316950
	Epoch 5....
Epoch has taken 0:00:23.423172
Number of used sentences in train = 359
Total loss for epoch 5: 718.620238
	Epoch 6....
Epoch has taken 0:00:22.722550
Number of used sentences in train = 359
Total loss for epoch 6: 696.464180
	Epoch 7....
Epoch has taken 0:00:22.724282
Number of used sentences in train = 359
Total loss for epoch 7: 699.012913
	Epoch 8....
Epoch has taken 0:00:22.710645
Number of used sentences in train = 359
Total loss for epoch 8: 702.627210
	Epoch 9....
Epoch has taken 0:00:22.729381
Number of used sentences in train = 359
Total loss for epoch 9: 679.732020
	Epoch 10....
Epoch has taken 0:00:22.715356
Number of used sentences in train = 359
Total loss for epoch 10: 677.373193
	Epoch 11....
Epoch has taken 0:00:22.732738
Number of used sentences in train = 359
Total loss for epoch 11: 691.215271
	Epoch 12....
Epoch has taken 0:00:22.726963
Number of used sentences in train = 359
Total loss for epoch 12: 688.775042
	Epoch 13....
Epoch has taken 0:00:22.734201
Number of used sentences in train = 359
Total loss for epoch 13: 675.100780
	Epoch 14....
Epoch has taken 0:00:22.727350
Number of used sentences in train = 359
Total loss for epoch 14: 674.042667
Epoch has taken 0:00:22.722393

==================================================================================================
	Training time : 1:06:20.877280
==================================================================================================
	Identification : 0.455

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 50, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 42, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 140, 'lstmDropout': 0.36, 'denseActivation': 'tanh', 'wordDim': 78, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(1177, 78)
  (lstm): LSTM(120, 140, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=2240, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14323.207002
validation loss after epoch 0 : 985.870843
	Epoch 1....
Epoch has taken 0:02:56.238467
Number of used sentences in train = 2811
Total loss for epoch 1: 8977.489219
validation loss after epoch 1 : 1038.254665
	Epoch 2....
Epoch has taken 0:02:57.528786
Number of used sentences in train = 2811
Total loss for epoch 2: 8008.164633
validation loss after epoch 2 : 876.429414
	Epoch 3....
Epoch has taken 0:02:57.106854
Number of used sentences in train = 2811
Total loss for epoch 3: 7399.417555
validation loss after epoch 3 : 866.740729
	Epoch 4....
Epoch has taken 0:02:57.385824
Number of used sentences in train = 2811
Total loss for epoch 4: 6983.119704
validation loss after epoch 4 : 862.567073
	Epoch 5....
Epoch has taken 0:02:56.461142
Number of used sentences in train = 2811
Total loss for epoch 5: 6636.758229
validation loss after epoch 5 : 863.823602
	Epoch 6....
Epoch has taken 0:02:59.469087
Number of used sentences in train = 2811
Total loss for epoch 6: 6341.716806
validation loss after epoch 6 : 884.974257
	Epoch 7....
Epoch has taken 0:02:58.650240
Number of used sentences in train = 2811
Total loss for epoch 7: 6095.534300
validation loss after epoch 7 : 883.250559
	Epoch 8....
Epoch has taken 0:02:56.321299
Number of used sentences in train = 2811
Total loss for epoch 8: 5854.158720
validation loss after epoch 8 : 934.301944
	Epoch 9....
Epoch has taken 0:02:53.832450
Number of used sentences in train = 2811
Total loss for epoch 9: 5645.529200
validation loss after epoch 9 : 928.675565
	Epoch 10....
Epoch has taken 0:02:53.943425
Number of used sentences in train = 2811
Total loss for epoch 10: 5504.414456
validation loss after epoch 10 : 946.316716
	Epoch 11....
Epoch has taken 0:02:58.096993
Number of used sentences in train = 2811
Total loss for epoch 11: 5402.789804
validation loss after epoch 11 : 982.043377
	Epoch 12....
Epoch has taken 0:02:53.876202
Number of used sentences in train = 2811
Total loss for epoch 12: 5309.531159
validation loss after epoch 12 : 1016.648096
	Epoch 13....
Epoch has taken 0:02:53.918497
Number of used sentences in train = 2811
Total loss for epoch 13: 5152.456362
validation loss after epoch 13 : 986.253549
	Epoch 14....
Epoch has taken 0:02:53.795868
Number of used sentences in train = 2811
Total loss for epoch 14: 5017.721422
validation loss after epoch 14 : 1019.498969
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(1177, 78)
  (lstm): LSTM(120, 140, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=2240, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.149090
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1321.161526
	Epoch 1....
Epoch has taken 0:00:18.431736
Number of used sentences in train = 313
Total loss for epoch 1: 862.941550
	Epoch 2....
Epoch has taken 0:00:18.438764
Number of used sentences in train = 313
Total loss for epoch 2: 763.754056
	Epoch 3....
Epoch has taken 0:00:18.513726
Number of used sentences in train = 313
Total loss for epoch 3: 647.732482
	Epoch 4....
Epoch has taken 0:00:19.045309
Number of used sentences in train = 313
Total loss for epoch 4: 631.438064
	Epoch 5....
Epoch has taken 0:00:19.042755
Number of used sentences in train = 313
Total loss for epoch 5: 577.090958
	Epoch 6....
Epoch has taken 0:00:18.425756
Number of used sentences in train = 313
Total loss for epoch 6: 576.828655
	Epoch 7....
Epoch has taken 0:00:18.402271
Number of used sentences in train = 313
Total loss for epoch 7: 554.753354
	Epoch 8....
Epoch has taken 0:00:18.401355
Number of used sentences in train = 313
Total loss for epoch 8: 550.149455
	Epoch 9....
Epoch has taken 0:00:18.393576
Number of used sentences in train = 313
Total loss for epoch 9: 540.566491
	Epoch 10....
Epoch has taken 0:00:18.392493
Number of used sentences in train = 313
Total loss for epoch 10: 533.082765
	Epoch 11....
Epoch has taken 0:00:18.395125
Number of used sentences in train = 313
Total loss for epoch 11: 533.778325
	Epoch 12....
Epoch has taken 0:00:18.420477
Number of used sentences in train = 313
Total loss for epoch 12: 534.778760
	Epoch 13....
Epoch has taken 0:00:19.024674
Number of used sentences in train = 313
Total loss for epoch 13: 528.760754
	Epoch 14....
Epoch has taken 0:00:19.029427
Number of used sentences in train = 313
Total loss for epoch 14: 535.673282
Epoch has taken 0:00:19.047953

==================================================================================================
	Training time : 0:48:42.686678
==================================================================================================
	Identification : 0.025

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(1133, 78)
  (lstm): LSTM(120, 140, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=2240, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 13150.238974
validation loss after epoch 0 : 871.768911
	Epoch 1....
Epoch has taken 0:02:01.241724
Number of used sentences in train = 2074
Total loss for epoch 1: 6557.754833
validation loss after epoch 1 : 783.157306
	Epoch 2....
Epoch has taken 0:02:12.755918
Number of used sentences in train = 2074
Total loss for epoch 2: 5563.374923
validation loss after epoch 2 : 676.107023
	Epoch 3....
Epoch has taken 0:02:13.798314
Number of used sentences in train = 2074
Total loss for epoch 3: 5061.626746
validation loss after epoch 3 : 707.455010
	Epoch 4....
Epoch has taken 0:02:14.643913
Number of used sentences in train = 2074
Total loss for epoch 4: 4598.498131
validation loss after epoch 4 : 678.214039
	Epoch 5....
Epoch has taken 0:02:13.895979
Number of used sentences in train = 2074
Total loss for epoch 5: 4322.712118
validation loss after epoch 5 : 679.431169
	Epoch 6....
Epoch has taken 0:02:11.385005
Number of used sentences in train = 2074
Total loss for epoch 6: 4135.332698
validation loss after epoch 6 : 673.071964
	Epoch 7....
Epoch has taken 0:02:02.796846
Number of used sentences in train = 2074
Total loss for epoch 7: 3915.572946
validation loss after epoch 7 : 685.172054
	Epoch 8....
Epoch has taken 0:02:02.750214
Number of used sentences in train = 2074
Total loss for epoch 8: 3783.529853
validation loss after epoch 8 : 707.911329
	Epoch 9....
Epoch has taken 0:02:02.790951
Number of used sentences in train = 2074
Total loss for epoch 9: 3660.954426
validation loss after epoch 9 : 750.952305
	Epoch 10....
Epoch has taken 0:02:02.112489
Number of used sentences in train = 2074
Total loss for epoch 10: 3564.592130
validation loss after epoch 10 : 849.385501
	Epoch 11....
Epoch has taken 0:02:02.707128
Number of used sentences in train = 2074
Total loss for epoch 11: 3519.498402
validation loss after epoch 11 : 820.500456
	Epoch 12....
Epoch has taken 0:02:02.444429
Number of used sentences in train = 2074
Total loss for epoch 12: 3488.597260
validation loss after epoch 12 : 841.165358
	Epoch 13....
Epoch has taken 0:02:02.830832
Number of used sentences in train = 2074
Total loss for epoch 13: 3434.549347
validation loss after epoch 13 : 846.625090
	Epoch 14....
Epoch has taken 0:02:02.756333
Number of used sentences in train = 2074
Total loss for epoch 14: 3399.108690
validation loss after epoch 14 : 871.166956
	TransitionClassifier(
  (p_embeddings): Embedding(18, 42)
  (w_embeddings): Embedding(1133, 78)
  (lstm): LSTM(120, 140, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=2240, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:02.094219
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1626.532632
	Epoch 1....
Epoch has taken 0:00:14.139653
Number of used sentences in train = 231
Total loss for epoch 1: 796.222284
	Epoch 2....
Epoch has taken 0:00:13.548990
Number of used sentences in train = 231
Total loss for epoch 2: 651.688707
	Epoch 3....
Epoch has taken 0:00:14.145112
Number of used sentences in train = 231
Total loss for epoch 3: 528.638457
	Epoch 4....
Epoch has taken 0:00:12.555815
Number of used sentences in train = 231
Total loss for epoch 4: 451.194619
	Epoch 5....
Epoch has taken 0:00:13.046511
Number of used sentences in train = 231
Total loss for epoch 5: 434.293110
	Epoch 6....
Epoch has taken 0:00:14.147267
Number of used sentences in train = 231
Total loss for epoch 6: 409.969527
	Epoch 7....
Epoch has taken 0:00:12.560846
Number of used sentences in train = 231
Total loss for epoch 7: 392.837668
	Epoch 8....
Epoch has taken 0:00:13.310685
Number of used sentences in train = 231
Total loss for epoch 8: 380.356986
	Epoch 9....
Epoch has taken 0:00:15.424069
Number of used sentences in train = 231
Total loss for epoch 9: 373.059059
	Epoch 10....
Epoch has taken 0:00:14.674336
Number of used sentences in train = 231
Total loss for epoch 10: 367.628419
	Epoch 11....
Epoch has taken 0:00:13.631428
Number of used sentences in train = 231
Total loss for epoch 11: 372.562741
	Epoch 12....
Epoch has taken 0:00:14.239735
Number of used sentences in train = 231
Total loss for epoch 12: 367.566178
	Epoch 13....
Epoch has taken 0:00:13.636340
Number of used sentences in train = 231
Total loss for epoch 13: 364.565493
	Epoch 14....
Epoch has taken 0:00:14.417531
Number of used sentences in train = 231
Total loss for epoch 14: 361.684967
Epoch has taken 0:00:12.549464

==================================================================================================
	Training time : 0:34:57.375346
==================================================================================================
	Identification : 0.367

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 42)
  (w_embeddings): Embedding(1202, 78)
  (lstm): LSTM(120, 140, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=2240, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 38652.011584
validation loss after epoch 0 : 3986.256319
	Epoch 1....
Epoch has taken 0:03:56.126357
Number of used sentences in train = 3226
Total loss for epoch 1: 37192.912039
validation loss after epoch 1 : 3969.200593
	Epoch 2....
Epoch has taken 0:04:17.194407
Number of used sentences in train = 3226
Total loss for epoch 2: 37096.203690
validation loss after epoch 2 : 3969.609605
	Epoch 3....
Epoch has taken 0:04:19.070577
Number of used sentences in train = 3226
Total loss for epoch 3: 37021.943319
validation loss after epoch 3 : 3968.702809
	Epoch 4....
Epoch has taken 0:04:13.863295
Number of used sentences in train = 3226
Total loss for epoch 4: 37004.856925
validation loss after epoch 4 : 4015.258429
	Epoch 5....
Epoch has taken 0:04:19.089086
Number of used sentences in train = 3226
Total loss for epoch 5: 36960.168792
validation loss after epoch 5 : 3957.711442
	Epoch 6....
Epoch has taken 0:04:03.878925
Number of used sentences in train = 3226
Total loss for epoch 6: 36936.711608
validation loss after epoch 6 : 4017.851493
	Epoch 7....
Epoch has taken 0:04:04.874305
Number of used sentences in train = 3226
Total loss for epoch 7: 36948.841188
validation loss after epoch 7 : 3955.635497
	Epoch 8....
Epoch has taken 0:04:21.368775
Number of used sentences in train = 3226
Total loss for epoch 8: 36919.685696
validation loss after epoch 8 : 3960.068796
	Epoch 9....
Epoch has taken 0:04:19.855489
Number of used sentences in train = 3226
Total loss for epoch 9: 36921.572075
validation loss after epoch 9 : 3960.725371
	Epoch 10....
Epoch has taken 0:04:19.991036
Number of used sentences in train = 3226
Total loss for epoch 10: 36914.434891
validation loss after epoch 10 : 3955.501934
	Epoch 11....
Epoch has taken 0:04:14.021143
Number of used sentences in train = 3226
Total loss for epoch 11: 36917.113613
validation loss after epoch 11 : 3954.808837
	Epoch 12....
Epoch has taken 0:03:55.450722
Number of used sentences in train = 3226
Total loss for epoch 12: 36882.824035
validation loss after epoch 12 : 3967.779369
	Epoch 13....
Epoch has taken 0:04:02.780039
Number of used sentences in train = 3226
Total loss for epoch 13: 36888.490417
validation loss after epoch 13 : 3958.523882
	Epoch 14....
Epoch has taken 0:04:11.562750
Number of used sentences in train = 3226
Total loss for epoch 14: 36883.204592
validation loss after epoch 14 : 3955.536554
	TransitionClassifier(
  (p_embeddings): Embedding(13, 42)
  (w_embeddings): Embedding(1202, 78)
  (lstm): LSTM(120, 140, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=2240, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:14.624841
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 4284.156641
	Epoch 1....
Epoch has taken 0:00:23.239443
Number of used sentences in train = 359
Total loss for epoch 1: 3972.358121
	Epoch 2....
Epoch has taken 0:00:23.236076
Number of used sentences in train = 359
Total loss for epoch 2: 3959.817109
	Epoch 3....
Epoch has taken 0:00:23.224713
Number of used sentences in train = 359
Total loss for epoch 3: 3945.582950
	Epoch 4....
Epoch has taken 0:00:23.238847
Number of used sentences in train = 359
Total loss for epoch 4: 3933.265074
	Epoch 5....
Epoch has taken 0:00:23.236580
Number of used sentences in train = 359
Total loss for epoch 5: 3928.396586
	Epoch 6....
Epoch has taken 0:00:23.234361
Number of used sentences in train = 359
Total loss for epoch 6: 3916.770453
	Epoch 7....
Epoch has taken 0:00:23.242050
Number of used sentences in train = 359
Total loss for epoch 7: 3917.699663
	Epoch 8....
Epoch has taken 0:00:25.804450
Number of used sentences in train = 359
Total loss for epoch 8: 3914.276705
	Epoch 9....
Epoch has taken 0:00:23.237118
Number of used sentences in train = 359
Total loss for epoch 9: 3908.854388
	Epoch 10....
Epoch has taken 0:00:23.243076
Number of used sentences in train = 359
Total loss for epoch 10: 3904.720237
	Epoch 11....
Epoch has taken 0:00:23.234465
Number of used sentences in train = 359
Total loss for epoch 11: 3901.944056
	Epoch 12....
Epoch has taken 0:00:23.234637
Number of used sentences in train = 359
Total loss for epoch 12: 3899.390002
	Epoch 13....
Epoch has taken 0:00:23.249607
Number of used sentences in train = 359
Total loss for epoch 13: 3934.071374
	Epoch 14....
Epoch has taken 0:00:23.242151
Number of used sentences in train = 359
Total loss for epoch 14: 3188.622082
Epoch has taken 0:00:23.204922

==================================================================================================
	Training time : 1:08:45.507047
==================================================================================================
	Identification : 0.299

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 23, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 18, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 43, 'lstmDropout': 0.24, 'denseActivation': 'tanh', 'wordDim': 51, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1177, 51)
  (lstm): LSTM(69, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10869.212037
validation loss after epoch 0 : 949.556054
	Epoch 1....
Epoch has taken 0:02:44.171908
Number of used sentences in train = 2811
Total loss for epoch 1: 7992.168119
validation loss after epoch 1 : 913.350169
	Epoch 2....
Epoch has taken 0:02:45.975066
Number of used sentences in train = 2811
Total loss for epoch 2: 6991.276214
validation loss after epoch 2 : 935.488417
	Epoch 3....
Epoch has taken 0:02:51.347965
Number of used sentences in train = 2811
Total loss for epoch 3: 6362.993616
validation loss after epoch 3 : 941.222172
	Epoch 4....
Epoch has taken 0:03:00.691303
Number of used sentences in train = 2811
Total loss for epoch 4: 5950.060956
validation loss after epoch 4 : 1005.441709
	Epoch 5....
Epoch has taken 0:03:02.741084
Number of used sentences in train = 2811
Total loss for epoch 5: 5633.244017
validation loss after epoch 5 : 1065.227322
	Epoch 6....
Epoch has taken 0:02:52.746005
Number of used sentences in train = 2811
Total loss for epoch 6: 5396.928971
validation loss after epoch 6 : 1070.196223
	Epoch 7....
Epoch has taken 0:02:46.146562
Number of used sentences in train = 2811
Total loss for epoch 7: 5241.854466
validation loss after epoch 7 : 1100.746030
	Epoch 8....
Epoch has taken 0:02:54.585382
Number of used sentences in train = 2811
Total loss for epoch 8: 5071.251157
validation loss after epoch 8 : 1131.369526
	Epoch 9....
Epoch has taken 0:02:55.555245
Number of used sentences in train = 2811
Total loss for epoch 9: 4947.224791
validation loss after epoch 9 : 1146.725256
	Epoch 10....
Epoch has taken 0:02:59.815401
Number of used sentences in train = 2811
Total loss for epoch 10: 4852.961875
validation loss after epoch 10 : 1171.490857
	Epoch 11....
Epoch has taken 0:02:46.084952
Number of used sentences in train = 2811
Total loss for epoch 11: 4766.124222
validation loss after epoch 11 : 1228.422429
	Epoch 12....
Epoch has taken 0:02:49.531426
Number of used sentences in train = 2811
Total loss for epoch 12: 4710.489721
validation loss after epoch 12 : 1246.866422
	Epoch 13....
Epoch has taken 0:02:53.409746
Number of used sentences in train = 2811
Total loss for epoch 13: 4691.532893
validation loss after epoch 13 : 1274.966521
	Epoch 14....
Epoch has taken 0:03:00.720431
Number of used sentences in train = 2811
Total loss for epoch 14: 4644.573222
validation loss after epoch 14 : 1299.244743
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1177, 51)
  (lstm): LSTM(69, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.956426
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1402.767307
	Epoch 1....
Epoch has taken 0:00:17.619490
Number of used sentences in train = 313
Total loss for epoch 1: 744.626182
	Epoch 2....
Epoch has taken 0:00:17.618731
Number of used sentences in train = 313
Total loss for epoch 2: 618.782371
	Epoch 3....
Epoch has taken 0:00:17.842647
Number of used sentences in train = 313
Total loss for epoch 3: 560.923228
	Epoch 4....
Epoch has taken 0:00:19.487113
Number of used sentences in train = 313
Total loss for epoch 4: 538.557646
	Epoch 5....
Epoch has taken 0:00:19.481619
Number of used sentences in train = 313
Total loss for epoch 5: 534.504331
	Epoch 6....
Epoch has taken 0:00:19.415281
Number of used sentences in train = 313
Total loss for epoch 6: 523.899162
	Epoch 7....
Epoch has taken 0:00:19.442584
Number of used sentences in train = 313
Total loss for epoch 7: 517.531556
	Epoch 8....
Epoch has taken 0:00:19.487817
Number of used sentences in train = 313
Total loss for epoch 8: 515.867675
	Epoch 9....
Epoch has taken 0:00:19.466536
Number of used sentences in train = 313
Total loss for epoch 9: 513.303560
	Epoch 10....
Epoch has taken 0:00:19.483277
Number of used sentences in train = 313
Total loss for epoch 10: 511.421031
	Epoch 11....
Epoch has taken 0:00:19.480879
Number of used sentences in train = 313
Total loss for epoch 11: 510.042514
	Epoch 12....
Epoch has taken 0:00:19.475715
Number of used sentences in train = 313
Total loss for epoch 12: 509.278390
	Epoch 13....
Epoch has taken 0:00:17.700829
Number of used sentences in train = 313
Total loss for epoch 13: 508.531741
	Epoch 14....
Epoch has taken 0:00:17.619472
Number of used sentences in train = 313
Total loss for epoch 14: 506.561197
Epoch has taken 0:00:17.613590

==================================================================================================
	Training time : 0:47:53.206064
==================================================================================================
	Identification : 0.535

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1133, 51)
  (lstm): LSTM(69, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8188.431995
validation loss after epoch 0 : 680.548532
	Epoch 1....
Epoch has taken 0:01:53.583167
Number of used sentences in train = 2074
Total loss for epoch 1: 5373.909333
validation loss after epoch 1 : 700.588016
	Epoch 2....
Epoch has taken 0:01:53.657853
Number of used sentences in train = 2074
Total loss for epoch 2: 4696.573719
validation loss after epoch 2 : 650.295842
	Epoch 3....
Epoch has taken 0:01:53.691829
Number of used sentences in train = 2074
Total loss for epoch 3: 4203.815851
validation loss after epoch 3 : 641.265288
	Epoch 4....
Epoch has taken 0:01:59.532588
Number of used sentences in train = 2074
Total loss for epoch 4: 3857.180737
validation loss after epoch 4 : 656.638020
	Epoch 5....
Epoch has taken 0:02:03.407780
Number of used sentences in train = 2074
Total loss for epoch 5: 3606.609780
validation loss after epoch 5 : 661.527288
	Epoch 6....
Epoch has taken 0:02:04.296182
Number of used sentences in train = 2074
Total loss for epoch 6: 3477.423055
validation loss after epoch 6 : 697.003163
	Epoch 7....
Epoch has taken 0:02:05.213700
Number of used sentences in train = 2074
Total loss for epoch 7: 3368.860996
validation loss after epoch 7 : 707.182002
	Epoch 8....
Epoch has taken 0:02:03.584218
Number of used sentences in train = 2074
Total loss for epoch 8: 3301.411693
validation loss after epoch 8 : 729.513581
	Epoch 9....
Epoch has taken 0:02:03.328350
Number of used sentences in train = 2074
Total loss for epoch 9: 3263.865686
validation loss after epoch 9 : 738.908326
	Epoch 10....
Epoch has taken 0:01:53.837466
Number of used sentences in train = 2074
Total loss for epoch 10: 3233.739683
validation loss after epoch 10 : 747.232594
	Epoch 11....
Epoch has taken 0:01:53.793422
Number of used sentences in train = 2074
Total loss for epoch 11: 3217.471766
validation loss after epoch 11 : 754.220678
	Epoch 12....
Epoch has taken 0:02:03.736738
Number of used sentences in train = 2074
Total loss for epoch 12: 3209.326124
validation loss after epoch 12 : 764.596203
	Epoch 13....
Epoch has taken 0:01:53.780975
Number of used sentences in train = 2074
Total loss for epoch 13: 3203.172669
validation loss after epoch 13 : 777.956057
	Epoch 14....
Epoch has taken 0:01:53.838903
Number of used sentences in train = 2074
Total loss for epoch 14: 3196.805467
validation loss after epoch 14 : 782.926190
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(1133, 51)
  (lstm): LSTM(69, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.886204
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1176.257255
	Epoch 1....
Epoch has taken 0:00:11.609026
Number of used sentences in train = 231
Total loss for epoch 1: 541.263514
	Epoch 2....
Epoch has taken 0:00:11.609202
Number of used sentences in train = 231
Total loss for epoch 2: 437.126201
	Epoch 3....
Epoch has taken 0:00:11.605098
Number of used sentences in train = 231
Total loss for epoch 3: 373.878494
	Epoch 4....
Epoch has taken 0:00:11.614023
Number of used sentences in train = 231
Total loss for epoch 4: 355.360590
	Epoch 5....
Epoch has taken 0:00:11.606318
Number of used sentences in train = 231
Total loss for epoch 5: 349.323908
	Epoch 6....
Epoch has taken 0:00:11.604599
Number of used sentences in train = 231
Total loss for epoch 6: 347.811104
	Epoch 7....
Epoch has taken 0:00:11.600919
Number of used sentences in train = 231
Total loss for epoch 7: 347.098043
	Epoch 8....
Epoch has taken 0:00:11.610902
Number of used sentences in train = 231
Total loss for epoch 8: 346.645313
	Epoch 9....
Epoch has taken 0:00:11.621884
Number of used sentences in train = 231
Total loss for epoch 9: 346.323207
	Epoch 10....
Epoch has taken 0:00:11.626562
Number of used sentences in train = 231
Total loss for epoch 10: 346.083823
	Epoch 11....
Epoch has taken 0:00:11.623140
Number of used sentences in train = 231
Total loss for epoch 11: 345.891582
	Epoch 12....
Epoch has taken 0:00:11.620341
Number of used sentences in train = 231
Total loss for epoch 12: 345.733214
	Epoch 13....
Epoch has taken 0:00:11.620154
Number of used sentences in train = 231
Total loss for epoch 13: 345.598738
	Epoch 14....
Epoch has taken 0:00:11.620819
Number of used sentences in train = 231
Total loss for epoch 14: 345.488123
Epoch has taken 0:00:11.620701

==================================================================================================
	Training time : 0:32:27.715171
==================================================================================================
	Identification : 0.313

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(1202, 51)
  (lstm): LSTM(69, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14290.681973
validation loss after epoch 0 : 1323.185570
	Epoch 1....
Epoch has taken 0:03:40.543479
Number of used sentences in train = 3226
Total loss for epoch 1: 11151.039874
validation loss after epoch 1 : 1271.682656
	Epoch 2....
Epoch has taken 0:03:40.450927
Number of used sentences in train = 3226
Total loss for epoch 2: 10084.100180
validation loss after epoch 2 : 1269.273702
	Epoch 3....
Epoch has taken 0:03:38.870044
Number of used sentences in train = 3226
Total loss for epoch 3: 9413.145915
validation loss after epoch 3 : 1293.862828
	Epoch 4....
Epoch has taken 0:03:45.546324
Number of used sentences in train = 3226
Total loss for epoch 4: 8937.802605
validation loss after epoch 4 : 1314.382048
	Epoch 5....
Epoch has taken 0:03:57.985235
Number of used sentences in train = 3226
Total loss for epoch 5: 8539.628087
validation loss after epoch 5 : 1373.377329
	Epoch 6....
Epoch has taken 0:03:39.786890
Number of used sentences in train = 3226
Total loss for epoch 6: 8217.584643
validation loss after epoch 6 : 1435.495150
	Epoch 7....
Epoch has taken 0:03:41.073733
Number of used sentences in train = 3226
Total loss for epoch 7: 7936.949900
validation loss after epoch 7 : 1455.274357
	Epoch 8....
Epoch has taken 0:04:03.130773
Number of used sentences in train = 3226
Total loss for epoch 8: 7690.272662
validation loss after epoch 8 : 1518.768115
	Epoch 9....
Epoch has taken 0:03:54.995069
Number of used sentences in train = 3226
Total loss for epoch 9: 7498.838558
validation loss after epoch 9 : 1616.069778
	Epoch 10....
Epoch has taken 0:03:40.392461
Number of used sentences in train = 3226
Total loss for epoch 10: 7288.371417
validation loss after epoch 10 : 1692.178530
	Epoch 11....
Epoch has taken 0:03:42.662589
Number of used sentences in train = 3226
Total loss for epoch 11: 7148.063943
validation loss after epoch 11 : 1774.027014
	Epoch 12....
Epoch has taken 0:04:03.159378
Number of used sentences in train = 3226
Total loss for epoch 12: 7014.011896
validation loss after epoch 12 : 1797.173733
	Epoch 13....
Epoch has taken 0:03:46.139620
Number of used sentences in train = 3226
Total loss for epoch 13: 6875.331350
validation loss after epoch 13 : 1805.795296
	Epoch 14....
Epoch has taken 0:03:40.796178
Number of used sentences in train = 3226
Total loss for epoch 14: 6756.426402
validation loss after epoch 14 : 1909.657973
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(1202, 51)
  (lstm): LSTM(69, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:42.895648
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1800.633046
	Epoch 1....
Epoch has taken 0:00:23.931730
Number of used sentences in train = 359
Total loss for epoch 1: 1110.940514
	Epoch 2....
Epoch has taken 0:00:23.908836
Number of used sentences in train = 359
Total loss for epoch 2: 935.671652
	Epoch 3....
Epoch has taken 0:00:23.910460
Number of used sentences in train = 359
Total loss for epoch 3: 833.563405
	Epoch 4....
Epoch has taken 0:00:23.924531
Number of used sentences in train = 359
Total loss for epoch 4: 765.731203
	Epoch 5....
Epoch has taken 0:00:23.929978
Number of used sentences in train = 359
Total loss for epoch 5: 715.770638
	Epoch 6....
Epoch has taken 0:00:23.913060
Number of used sentences in train = 359
Total loss for epoch 6: 707.788208
	Epoch 7....
Epoch has taken 0:00:23.924140
Number of used sentences in train = 359
Total loss for epoch 7: 688.559000
	Epoch 8....
Epoch has taken 0:00:23.906697
Number of used sentences in train = 359
Total loss for epoch 8: 678.250006
	Epoch 9....
Epoch has taken 0:00:23.925306
Number of used sentences in train = 359
Total loss for epoch 9: 673.586092
	Epoch 10....
Epoch has taken 0:00:23.923896
Number of used sentences in train = 359
Total loss for epoch 10: 672.735484
	Epoch 11....
Epoch has taken 0:00:23.932398
Number of used sentences in train = 359
Total loss for epoch 11: 672.308489
	Epoch 12....
Epoch has taken 0:00:23.926100
Number of used sentences in train = 359
Total loss for epoch 12: 672.004444
	Epoch 13....
Epoch has taken 0:00:23.917185
Number of used sentences in train = 359
Total loss for epoch 13: 671.768707
	Epoch 14....
Epoch has taken 0:00:23.937297
Number of used sentences in train = 359
Total loss for epoch 14: 671.587270
Epoch has taken 0:00:23.925144

==================================================================================================
	Training time : 1:02:37.914723
==================================================================================================
	Identification : 0.216

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 42, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 22, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 79, 'lstmDropout': 0.28, 'denseActivation': 'tanh', 'wordDim': 96, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1882, 96)
  (lstm): LSTM(118, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 9888.974058
validation loss after epoch 0 : 855.628356
	Epoch 1....
Epoch has taken 0:03:03.554663
Number of used sentences in train = 2811
Total loss for epoch 1: 7228.371868
validation loss after epoch 1 : 829.097337
	Epoch 2....
Epoch has taken 0:03:01.865189
Number of used sentences in train = 2811
Total loss for epoch 2: 6317.909211
validation loss after epoch 2 : 840.494827
	Epoch 3....
Epoch has taken 0:03:03.527268
Number of used sentences in train = 2811
Total loss for epoch 3: 5674.204398
validation loss after epoch 3 : 850.442591
	Epoch 4....
Epoch has taken 0:03:03.480300
Number of used sentences in train = 2811
Total loss for epoch 4: 5301.587952
validation loss after epoch 4 : 902.882855
	Epoch 5....
Epoch has taken 0:03:01.486009
Number of used sentences in train = 2811
Total loss for epoch 5: 5024.826543
validation loss after epoch 5 : 943.450563
	Epoch 6....
Epoch has taken 0:03:02.158736
Number of used sentences in train = 2811
Total loss for epoch 6: 4882.815328
validation loss after epoch 6 : 979.291502
	Epoch 7....
Epoch has taken 0:03:01.187711
Number of used sentences in train = 2811
Total loss for epoch 7: 4770.875887
validation loss after epoch 7 : 994.698153
	Epoch 8....
Epoch has taken 0:02:43.502119
Number of used sentences in train = 2811
Total loss for epoch 8: 4693.517941
validation loss after epoch 8 : 1022.713419
	Epoch 9....
Epoch has taken 0:02:41.850065
Number of used sentences in train = 2811
Total loss for epoch 9: 4638.655043
validation loss after epoch 9 : 1049.023279
	Epoch 10....
Epoch has taken 0:02:42.018152
Number of used sentences in train = 2811
Total loss for epoch 10: 4608.574382
validation loss after epoch 10 : 1060.241288
	Epoch 11....
Epoch has taken 0:02:42.126796
Number of used sentences in train = 2811
Total loss for epoch 11: 4587.110031
validation loss after epoch 11 : 1074.946386
	Epoch 12....
Epoch has taken 0:02:41.910393
Number of used sentences in train = 2811
Total loss for epoch 12: 4573.778855
validation loss after epoch 12 : 1088.434757
	Epoch 13....
Epoch has taken 0:02:41.979639
Number of used sentences in train = 2811
Total loss for epoch 13: 4554.918172
validation loss after epoch 13 : 1092.504578
	Epoch 14....
Epoch has taken 0:02:42.015033
Number of used sentences in train = 2811
Total loss for epoch 14: 4540.107203
validation loss after epoch 14 : 1109.903731
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1882, 96)
  (lstm): LSTM(118, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.169574
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1215.875311
	Epoch 1....
Epoch has taken 0:00:17.090735
Number of used sentences in train = 313
Total loss for epoch 1: 769.617827
	Epoch 2....
Epoch has taken 0:00:17.094399
Number of used sentences in train = 313
Total loss for epoch 2: 640.939449
	Epoch 3....
Epoch has taken 0:00:17.090303
Number of used sentences in train = 313
Total loss for epoch 3: 577.608851
	Epoch 4....
Epoch has taken 0:00:17.083408
Number of used sentences in train = 313
Total loss for epoch 4: 547.314933
	Epoch 5....
Epoch has taken 0:00:17.090478
Number of used sentences in train = 313
Total loss for epoch 5: 536.114622
	Epoch 6....
Epoch has taken 0:00:17.086599
Number of used sentences in train = 313
Total loss for epoch 6: 533.585619
	Epoch 7....
Epoch has taken 0:00:17.080567
Number of used sentences in train = 313
Total loss for epoch 7: 527.473549
	Epoch 8....
Epoch has taken 0:00:17.082362
Number of used sentences in train = 313
Total loss for epoch 8: 525.594518
	Epoch 9....
Epoch has taken 0:00:17.084654
Number of used sentences in train = 313
Total loss for epoch 9: 523.708570
	Epoch 10....
Epoch has taken 0:00:17.082646
Number of used sentences in train = 313
Total loss for epoch 10: 521.984950
	Epoch 11....
Epoch has taken 0:00:17.081355
Number of used sentences in train = 313
Total loss for epoch 11: 520.205672
	Epoch 12....
Epoch has taken 0:00:17.085745
Number of used sentences in train = 313
Total loss for epoch 12: 519.621214
	Epoch 13....
Epoch has taken 0:00:17.088520
Number of used sentences in train = 313
Total loss for epoch 13: 518.495742
	Epoch 14....
Epoch has taken 0:00:17.088729
Number of used sentences in train = 313
Total loss for epoch 14: 516.024862
Epoch has taken 0:00:17.101349

==================================================================================================
	Training time : 0:47:16.642994
==================================================================================================
	Identification : 0.484

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1680, 96)
  (lstm): LSTM(118, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7971.855338
validation loss after epoch 0 : 703.002335
	Epoch 1....
Epoch has taken 0:01:50.710581
Number of used sentences in train = 2074
Total loss for epoch 1: 5057.382157
validation loss after epoch 1 : 713.392055
	Epoch 2....
Epoch has taken 0:01:50.764559
Number of used sentences in train = 2074
Total loss for epoch 2: 4278.334675
validation loss after epoch 2 : 663.218577
	Epoch 3....
Epoch has taken 0:01:50.812541
Number of used sentences in train = 2074
Total loss for epoch 3: 3781.950779
validation loss after epoch 3 : 703.727861
	Epoch 4....
Epoch has taken 0:01:52.170582
Number of used sentences in train = 2074
Total loss for epoch 4: 3507.600841
validation loss after epoch 4 : 791.407453
	Epoch 5....
Epoch has taken 0:01:50.753575
Number of used sentences in train = 2074
Total loss for epoch 5: 3390.265574
validation loss after epoch 5 : 820.056617
	Epoch 6....
Epoch has taken 0:01:50.849103
Number of used sentences in train = 2074
Total loss for epoch 6: 3306.998540
validation loss after epoch 6 : 816.943647
	Epoch 7....
Epoch has taken 0:01:51.585560
Number of used sentences in train = 2074
Total loss for epoch 7: 3262.369637
validation loss after epoch 7 : 850.607167
	Epoch 8....
Epoch has taken 0:01:50.707355
Number of used sentences in train = 2074
Total loss for epoch 8: 3242.746889
validation loss after epoch 8 : 862.531442
	Epoch 9....
Epoch has taken 0:01:50.880001
Number of used sentences in train = 2074
Total loss for epoch 9: 3224.757413
validation loss after epoch 9 : 873.536640
	Epoch 10....
Epoch has taken 0:01:50.881604
Number of used sentences in train = 2074
Total loss for epoch 10: 3217.689657
validation loss after epoch 10 : 896.227778
	Epoch 11....
Epoch has taken 0:01:50.865684
Number of used sentences in train = 2074
Total loss for epoch 11: 3206.179325
validation loss after epoch 11 : 898.929151
	Epoch 12....
Epoch has taken 0:01:50.793928
Number of used sentences in train = 2074
Total loss for epoch 12: 3200.491631
validation loss after epoch 12 : 909.638962
	Epoch 13....
Epoch has taken 0:01:50.792353
Number of used sentences in train = 2074
Total loss for epoch 13: 3193.857644
validation loss after epoch 13 : 917.176877
	Epoch 14....
Epoch has taken 0:01:52.601790
Number of used sentences in train = 2074
Total loss for epoch 14: 3189.251707
validation loss after epoch 14 : 921.643077
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1680, 96)
  (lstm): LSTM(118, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.828615
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1180.048334
	Epoch 1....
Epoch has taken 0:00:11.263923
Number of used sentences in train = 231
Total loss for epoch 1: 583.380729
	Epoch 2....
Epoch has taken 0:00:11.279992
Number of used sentences in train = 231
Total loss for epoch 2: 412.809170
	Epoch 3....
Epoch has taken 0:00:11.283475
Number of used sentences in train = 231
Total loss for epoch 3: 364.048121
	Epoch 4....
Epoch has taken 0:00:11.278994
Number of used sentences in train = 231
Total loss for epoch 4: 357.117438
	Epoch 5....
Epoch has taken 0:00:11.291017
Number of used sentences in train = 231
Total loss for epoch 5: 355.399004
	Epoch 6....
Epoch has taken 0:00:11.276814
Number of used sentences in train = 231
Total loss for epoch 6: 353.816203
	Epoch 7....
Epoch has taken 0:00:11.278034
Number of used sentences in train = 231
Total loss for epoch 7: 350.911519
	Epoch 8....
Epoch has taken 0:00:11.277835
Number of used sentences in train = 231
Total loss for epoch 8: 349.379086
	Epoch 9....
Epoch has taken 0:00:11.284389
Number of used sentences in train = 231
Total loss for epoch 9: 348.835049
	Epoch 10....
Epoch has taken 0:00:11.282930
Number of used sentences in train = 231
Total loss for epoch 10: 348.401963
	Epoch 11....
Epoch has taken 0:00:11.272544
Number of used sentences in train = 231
Total loss for epoch 11: 347.950972
	Epoch 12....
Epoch has taken 0:00:11.274372
Number of used sentences in train = 231
Total loss for epoch 12: 347.400661
	Epoch 13....
Epoch has taken 0:00:11.281449
Number of used sentences in train = 231
Total loss for epoch 13: 346.662951
	Epoch 14....
Epoch has taken 0:00:11.272704
Number of used sentences in train = 231
Total loss for epoch 14: 346.435554
Epoch has taken 0:00:11.270461

==================================================================================================
	Training time : 0:30:35.503415
==================================================================================================
	Identification : 0.481

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 22)
  (w_embeddings): Embedding(3369, 96)
  (lstm): LSTM(118, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12521.683805
validation loss after epoch 0 : 1109.424510
	Epoch 1....
Epoch has taken 0:03:36.972214
Number of used sentences in train = 3226
Total loss for epoch 1: 9121.007495
validation loss after epoch 1 : 1075.431181
	Epoch 2....
Epoch has taken 0:03:36.711356
Number of used sentences in train = 3226
Total loss for epoch 2: 8233.963608
validation loss after epoch 2 : 1070.239019
	Epoch 3....
Epoch has taken 0:03:40.293357
Number of used sentences in train = 3226
Total loss for epoch 3: 7613.614249
validation loss after epoch 3 : 1140.396191
	Epoch 4....
Epoch has taken 0:03:37.814938
Number of used sentences in train = 3226
Total loss for epoch 4: 7219.494229
validation loss after epoch 4 : 1114.044924
	Epoch 5....
Epoch has taken 0:03:42.285260
Number of used sentences in train = 3226
Total loss for epoch 5: 6910.982506
validation loss after epoch 5 : 1207.941695
	Epoch 6....
Epoch has taken 0:03:48.191103
Number of used sentences in train = 3226
Total loss for epoch 6: 6709.797506
validation loss after epoch 6 : 1264.942199
	Epoch 7....
Epoch has taken 0:04:00.940258
Number of used sentences in train = 3226
Total loss for epoch 7: 6556.171559
validation loss after epoch 7 : 1270.371361
	Epoch 8....
Epoch has taken 0:03:54.893921
Number of used sentences in train = 3226
Total loss for epoch 8: 6441.655188
validation loss after epoch 8 : 1379.385576
	Epoch 9....
Epoch has taken 0:03:39.821243
Number of used sentences in train = 3226
Total loss for epoch 9: 6387.850931
validation loss after epoch 9 : 1322.916694
	Epoch 10....
Epoch has taken 0:03:38.671321
Number of used sentences in train = 3226
Total loss for epoch 10: 6312.860664
validation loss after epoch 10 : 1412.332441
	Epoch 11....
Epoch has taken 0:03:39.730123
Number of used sentences in train = 3226
Total loss for epoch 11: 6272.275238
validation loss after epoch 11 : 1385.315613
	Epoch 12....
Epoch has taken 0:03:39.991015
Number of used sentences in train = 3226
Total loss for epoch 12: 6256.323638
validation loss after epoch 12 : 1416.602135
	Epoch 13....
Epoch has taken 0:03:40.640446
Number of used sentences in train = 3226
Total loss for epoch 13: 6226.987340
validation loss after epoch 13 : 1462.447556
	Epoch 14....
Epoch has taken 0:03:38.803281
Number of used sentences in train = 3226
Total loss for epoch 14: 6209.949307
validation loss after epoch 14 : 1469.289575
	TransitionClassifier(
  (p_embeddings): Embedding(13, 22)
  (w_embeddings): Embedding(3369, 96)
  (lstm): LSTM(118, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=42, bias=True)
  (linear2): Linear(in_features=42, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:39.627319
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1597.176476
	Epoch 1....
Epoch has taken 0:00:21.561490
Number of used sentences in train = 359
Total loss for epoch 1: 938.493842
	Epoch 2....
Epoch has taken 0:00:21.571411
Number of used sentences in train = 359
Total loss for epoch 2: 822.867908
	Epoch 3....
Epoch has taken 0:00:21.574966
Number of used sentences in train = 359
Total loss for epoch 3: 757.560110
	Epoch 4....
Epoch has taken 0:00:21.568853
Number of used sentences in train = 359
Total loss for epoch 4: 726.574422
	Epoch 5....
Epoch has taken 0:00:21.305948
Number of used sentences in train = 359
Total loss for epoch 5: 699.446224
	Epoch 6....
Epoch has taken 0:00:21.561788
Number of used sentences in train = 359
Total loss for epoch 6: 692.872975
	Epoch 7....
Epoch has taken 0:00:21.556909
Number of used sentences in train = 359
Total loss for epoch 7: 681.781115
	Epoch 8....
Epoch has taken 0:00:21.589298
Number of used sentences in train = 359
Total loss for epoch 8: 676.978657
	Epoch 9....
Epoch has taken 0:00:21.987987
Number of used sentences in train = 359
Total loss for epoch 9: 675.942775
	Epoch 10....
Epoch has taken 0:00:21.572477
Number of used sentences in train = 359
Total loss for epoch 10: 673.505584
	Epoch 11....
Epoch has taken 0:00:21.590945
Number of used sentences in train = 359
Total loss for epoch 11: 676.078842
	Epoch 12....
Epoch has taken 0:00:21.575953
Number of used sentences in train = 359
Total loss for epoch 12: 672.292216
	Epoch 13....
Epoch has taken 0:00:21.561047
Number of used sentences in train = 359
Total loss for epoch 13: 671.669456
	Epoch 14....
Epoch has taken 0:00:22.545005
Number of used sentences in train = 359
Total loss for epoch 14: 671.436404
Epoch has taken 0:00:23.876169

==================================================================================================
	Training time : 1:01:03.054646
==================================================================================================
	Identification : 0.465

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 27, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 18, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 56, 'lstmDropout': 0.32, 'denseActivation': 'tanh', 'wordDim': 119, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5860, 119)
  (lstm): LSTM(137, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12313.512975
validation loss after epoch 0 : 1114.064731
	Epoch 1....
Epoch has taken 0:02:51.385318
Number of used sentences in train = 2811
Total loss for epoch 1: 8150.416026
validation loss after epoch 1 : 1079.903753
	Epoch 2....
Epoch has taken 0:02:48.878543
Number of used sentences in train = 2811
Total loss for epoch 2: 6546.762676
validation loss after epoch 2 : 1168.140581
	Epoch 3....
Epoch has taken 0:02:59.355932
Number of used sentences in train = 2811
Total loss for epoch 3: 5651.422105
validation loss after epoch 3 : 1231.431724
	Epoch 4....
Epoch has taken 0:02:46.253420
Number of used sentences in train = 2811
Total loss for epoch 4: 5183.470620
validation loss after epoch 4 : 1308.760281
	Epoch 5....
Epoch has taken 0:02:46.146024
Number of used sentences in train = 2811
Total loss for epoch 5: 4921.670129
validation loss after epoch 5 : 1388.367273
	Epoch 6....
Epoch has taken 0:02:46.109434
Number of used sentences in train = 2811
Total loss for epoch 6: 4771.767389
validation loss after epoch 6 : 1442.090980
	Epoch 7....
Epoch has taken 0:02:46.151352
Number of used sentences in train = 2811
Total loss for epoch 7: 4690.237160
validation loss after epoch 7 : 1463.246499
	Epoch 8....
Epoch has taken 0:02:46.231222
Number of used sentences in train = 2811
Total loss for epoch 8: 4643.104340
validation loss after epoch 8 : 1522.309870
	Epoch 9....
Epoch has taken 0:02:46.166640
Number of used sentences in train = 2811
Total loss for epoch 9: 4603.884364
validation loss after epoch 9 : 1546.702894
	Epoch 10....
Epoch has taken 0:02:46.034451
Number of used sentences in train = 2811
Total loss for epoch 10: 4576.406235
validation loss after epoch 10 : 1562.920043
	Epoch 11....
Epoch has taken 0:02:45.892534
Number of used sentences in train = 2811
Total loss for epoch 11: 4556.365475
validation loss after epoch 11 : 1579.705441
	Epoch 12....
Epoch has taken 0:02:46.783013
Number of used sentences in train = 2811
Total loss for epoch 12: 4544.129061
validation loss after epoch 12 : 1610.076806
	Epoch 13....
Epoch has taken 0:02:54.868603
Number of used sentences in train = 2811
Total loss for epoch 13: 4535.082484
validation loss after epoch 13 : 1632.686388
	Epoch 14....
Epoch has taken 0:02:46.178259
Number of used sentences in train = 2811
Total loss for epoch 14: 4527.070879
validation loss after epoch 14 : 1637.857733
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5860, 119)
  (lstm): LSTM(137, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:45.513811
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1647.228150
	Epoch 1....
Epoch has taken 0:00:17.614498
Number of used sentences in train = 313
Total loss for epoch 1: 749.316674
	Epoch 2....
Epoch has taken 0:00:17.601337
Number of used sentences in train = 313
Total loss for epoch 2: 632.793279
	Epoch 3....
Epoch has taken 0:00:17.611751
Number of used sentences in train = 313
Total loss for epoch 3: 560.669453
	Epoch 4....
Epoch has taken 0:00:17.612495
Number of used sentences in train = 313
Total loss for epoch 4: 526.725335
	Epoch 5....
Epoch has taken 0:00:17.610596
Number of used sentences in train = 313
Total loss for epoch 5: 521.910662
	Epoch 6....
Epoch has taken 0:00:17.610264
Number of used sentences in train = 313
Total loss for epoch 6: 511.179209
	Epoch 7....
Epoch has taken 0:00:17.605710
Number of used sentences in train = 313
Total loss for epoch 7: 508.299318
	Epoch 8....
Epoch has taken 0:00:17.603460
Number of used sentences in train = 313
Total loss for epoch 8: 506.069435
	Epoch 9....
Epoch has taken 0:00:17.609618
Number of used sentences in train = 313
Total loss for epoch 9: 504.878213
	Epoch 10....
Epoch has taken 0:00:17.600055
Number of used sentences in train = 313
Total loss for epoch 10: 504.787505
	Epoch 11....
Epoch has taken 0:00:17.598180
Number of used sentences in train = 313
Total loss for epoch 11: 503.532103
	Epoch 12....
Epoch has taken 0:00:17.621691
Number of used sentences in train = 313
Total loss for epoch 12: 503.597677
	Epoch 13....
Epoch has taken 0:00:17.621103
Number of used sentences in train = 313
Total loss for epoch 13: 502.820918
	Epoch 14....
Epoch has taken 0:00:17.613844
Number of used sentences in train = 313
Total loss for epoch 14: 502.520874
Epoch has taken 0:00:17.610493

==================================================================================================
	Training time : 0:46:26.595322
==================================================================================================
	Identification : 0.032

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5620, 119)
  (lstm): LSTM(137, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9955.625428
validation loss after epoch 0 : 877.747408
	Epoch 1....
Epoch has taken 0:01:53.275857
Number of used sentences in train = 2074
Total loss for epoch 1: 6033.442231
validation loss after epoch 1 : 879.529233
	Epoch 2....
Epoch has taken 0:01:53.544204
Number of used sentences in train = 2074
Total loss for epoch 2: 4719.106519
validation loss after epoch 2 : 907.848320
	Epoch 3....
Epoch has taken 0:01:53.665393
Number of used sentences in train = 2074
Total loss for epoch 3: 4024.148784
validation loss after epoch 3 : 974.243028
	Epoch 4....
Epoch has taken 0:01:52.282977
Number of used sentences in train = 2074
Total loss for epoch 4: 3677.654512
validation loss after epoch 4 : 982.826243
	Epoch 5....
Epoch has taken 0:01:52.826318
Number of used sentences in train = 2074
Total loss for epoch 5: 3442.037680
validation loss after epoch 5 : 1071.774112
	Epoch 6....
Epoch has taken 0:01:59.542234
Number of used sentences in train = 2074
Total loss for epoch 6: 3315.956541
validation loss after epoch 6 : 1138.288344
	Epoch 7....
Epoch has taken 0:01:53.689988
Number of used sentences in train = 2074
Total loss for epoch 7: 3263.992732
validation loss after epoch 7 : 1162.530048
	Epoch 8....
Epoch has taken 0:01:52.754998
Number of used sentences in train = 2074
Total loss for epoch 8: 3237.392046
validation loss after epoch 8 : 1185.146043
	Epoch 9....
Epoch has taken 0:01:53.621482
Number of used sentences in train = 2074
Total loss for epoch 9: 3224.881371
validation loss after epoch 9 : 1210.850628
	Epoch 10....
Epoch has taken 0:01:53.646954
Number of used sentences in train = 2074
Total loss for epoch 10: 3210.631932
validation loss after epoch 10 : 1239.018676
	Epoch 11....
Epoch has taken 0:01:53.736930
Number of used sentences in train = 2074
Total loss for epoch 11: 3204.432095
validation loss after epoch 11 : 1247.820929
	Epoch 12....
Epoch has taken 0:01:53.764860
Number of used sentences in train = 2074
Total loss for epoch 12: 3198.902961
validation loss after epoch 12 : 1259.769404
	Epoch 13....
Epoch has taken 0:01:53.764094
Number of used sentences in train = 2074
Total loss for epoch 13: 3192.057571
validation loss after epoch 13 : 1267.631544
	Epoch 14....
Epoch has taken 0:01:53.647655
Number of used sentences in train = 2074
Total loss for epoch 14: 3183.703058
validation loss after epoch 14 : 1278.931985
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5620, 119)
  (lstm): LSTM(137, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.643645
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1575.046682
	Epoch 1....
Epoch has taken 0:00:11.620501
Number of used sentences in train = 231
Total loss for epoch 1: 591.218355
	Epoch 2....
Epoch has taken 0:00:11.605430
Number of used sentences in train = 231
Total loss for epoch 2: 422.931987
	Epoch 3....
Epoch has taken 0:00:12.716527
Number of used sentences in train = 231
Total loss for epoch 3: 384.522931
	Epoch 4....
Epoch has taken 0:00:12.834670
Number of used sentences in train = 231
Total loss for epoch 4: 362.064493
	Epoch 5....
Epoch has taken 0:00:12.847927
Number of used sentences in train = 231
Total loss for epoch 5: 356.141383
	Epoch 6....
Epoch has taken 0:00:12.619550
Number of used sentences in train = 231
Total loss for epoch 6: 354.851327
	Epoch 7....
Epoch has taken 0:00:12.577927
Number of used sentences in train = 231
Total loss for epoch 7: 353.740314
	Epoch 8....
Epoch has taken 0:00:12.594088
Number of used sentences in train = 231
Total loss for epoch 8: 352.713067
	Epoch 9....
Epoch has taken 0:00:12.813600
Number of used sentences in train = 231
Total loss for epoch 9: 352.044064
	Epoch 10....
Epoch has taken 0:00:12.822665
Number of used sentences in train = 231
Total loss for epoch 10: 348.653562
	Epoch 11....
Epoch has taken 0:00:12.802611
Number of used sentences in train = 231
Total loss for epoch 11: 349.606646
	Epoch 12....
Epoch has taken 0:00:12.813644
Number of used sentences in train = 231
Total loss for epoch 12: 346.949504
	Epoch 13....
Epoch has taken 0:00:12.809165
Number of used sentences in train = 231
Total loss for epoch 13: 346.577291
	Epoch 14....
Epoch has taken 0:00:11.817370
Number of used sentences in train = 231
Total loss for epoch 14: 346.364984
Epoch has taken 0:00:11.599621

==================================================================================================
	Training time : 0:31:34.641779
==================================================================================================
	Identification : 0.133

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(6857, 119)
  (lstm): LSTM(137, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16958.553369
validation loss after epoch 0 : 1601.433872
	Epoch 1....
Epoch has taken 0:03:42.915074
Number of used sentences in train = 3226
Total loss for epoch 1: 12294.275615
validation loss after epoch 1 : 1544.740551
	Epoch 2....
Epoch has taken 0:03:42.870397
Number of used sentences in train = 3226
Total loss for epoch 2: 10478.230043
validation loss after epoch 2 : 1599.806788
	Epoch 3....
Epoch has taken 0:03:51.101965
Number of used sentences in train = 3226
Total loss for epoch 3: 9145.634021
validation loss after epoch 3 : 1802.136663
	Epoch 4....
Epoch has taken 0:04:01.775268
Number of used sentences in train = 3226
Total loss for epoch 4: 8185.659635
validation loss after epoch 4 : 1817.461209
	Epoch 5....
Epoch has taken 0:03:43.419717
Number of used sentences in train = 3226
Total loss for epoch 5: 7501.479540
validation loss after epoch 5 : 2011.020249
	Epoch 6....
Epoch has taken 0:04:03.431509
Number of used sentences in train = 3226
Total loss for epoch 6: 7000.679387
validation loss after epoch 6 : 2118.728018
	Epoch 7....
Epoch has taken 0:03:43.582519
Number of used sentences in train = 3226
Total loss for epoch 7: 6685.682747
validation loss after epoch 7 : 2236.584917
	Epoch 8....
Epoch has taken 0:04:04.335799
Number of used sentences in train = 3226
Total loss for epoch 8: 6493.675786
validation loss after epoch 8 : 2306.776753
	Epoch 9....
Epoch has taken 0:03:43.912931
Number of used sentences in train = 3226
Total loss for epoch 9: 6396.316904
validation loss after epoch 9 : 2443.450253
	Epoch 10....
Epoch has taken 0:03:41.948863
Number of used sentences in train = 3226
Total loss for epoch 10: 6326.581579
validation loss after epoch 10 : 2520.177126
	Epoch 11....
Epoch has taken 0:03:41.493023
Number of used sentences in train = 3226
Total loss for epoch 11: 6284.150823
validation loss after epoch 11 : 2526.993145
	Epoch 12....
Epoch has taken 0:03:43.556497
Number of used sentences in train = 3226
Total loss for epoch 12: 6244.283079
validation loss after epoch 12 : 2587.532482
	Epoch 13....
Epoch has taken 0:03:43.350511
Number of used sentences in train = 3226
Total loss for epoch 13: 6233.600203
validation loss after epoch 13 : 2610.441170
	Epoch 14....
Epoch has taken 0:03:55.911748
Number of used sentences in train = 3226
Total loss for epoch 14: 6218.344144
validation loss after epoch 14 : 2608.573937
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(6857, 119)
  (lstm): LSTM(137, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.315553
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2483.974376
	Epoch 1....
Epoch has taken 0:00:21.844491
Number of used sentences in train = 359
Total loss for epoch 1: 1153.408094
	Epoch 2....
Epoch has taken 0:00:21.870681
Number of used sentences in train = 359
Total loss for epoch 2: 889.951089
	Epoch 3....
Epoch has taken 0:00:21.847714
Number of used sentences in train = 359
Total loss for epoch 3: 769.873943
	Epoch 4....
Epoch has taken 0:00:21.844048
Number of used sentences in train = 359
Total loss for epoch 4: 712.965586
	Epoch 5....
Epoch has taken 0:00:21.854781
Number of used sentences in train = 359
Total loss for epoch 5: 697.464820
	Epoch 6....
Epoch has taken 0:00:21.843072
Number of used sentences in train = 359
Total loss for epoch 6: 685.518811
	Epoch 7....
Epoch has taken 0:00:21.834950
Number of used sentences in train = 359
Total loss for epoch 7: 677.457174
	Epoch 8....
Epoch has taken 0:00:21.852658
Number of used sentences in train = 359
Total loss for epoch 8: 674.573357
	Epoch 9....
Epoch has taken 0:00:21.858504
Number of used sentences in train = 359
Total loss for epoch 9: 673.498844
	Epoch 10....
Epoch has taken 0:00:21.847746
Number of used sentences in train = 359
Total loss for epoch 10: 672.945684
	Epoch 11....
Epoch has taken 0:00:21.838956
Number of used sentences in train = 359
Total loss for epoch 11: 672.617024
	Epoch 12....
Epoch has taken 0:00:21.846787
Number of used sentences in train = 359
Total loss for epoch 12: 672.373660
	Epoch 13....
Epoch has taken 0:00:21.852137
Number of used sentences in train = 359
Total loss for epoch 13: 672.180611
	Epoch 14....
Epoch has taken 0:00:21.857281
Number of used sentences in train = 359
Total loss for epoch 14: 672.023061
Epoch has taken 0:00:21.850544

==================================================================================================
	Training time : 1:02:35.322262
==================================================================================================
	Identification : 0.423

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 26, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 23, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 71, 'lstmDropout': 0.37, 'denseActivation': 'tanh', 'wordDim': 88, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 23)
  (w_embeddings): Embedding(5889, 88)
  (lstm): LSTM(111, 71, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 17861.996300
validation loss after epoch 0 : 1205.591181
	Epoch 1....
Epoch has taken 0:03:15.331092
Number of used sentences in train = 2811
Total loss for epoch 1: 9749.460145
validation loss after epoch 1 : 1064.822070
	Epoch 2....
Epoch has taken 0:03:11.089545
Number of used sentences in train = 2811
Total loss for epoch 2: 8065.093064
validation loss after epoch 2 : 1026.930387
	Epoch 3....
Epoch has taken 0:02:58.448553
Number of used sentences in train = 2811
Total loss for epoch 3: 7095.070477
validation loss after epoch 3 : 1011.221807
	Epoch 4....
Epoch has taken 0:02:58.387308
Number of used sentences in train = 2811
Total loss for epoch 4: 6452.219742
validation loss after epoch 4 : 1057.402744
	Epoch 5....
Epoch has taken 0:02:58.308151
Number of used sentences in train = 2811
Total loss for epoch 5: 5971.884421
validation loss after epoch 5 : 1134.262042
	Epoch 6....
Epoch has taken 0:03:13.264939
Number of used sentences in train = 2811
Total loss for epoch 6: 5751.608456
validation loss after epoch 6 : 1147.046636
	Epoch 7....
Epoch has taken 0:03:14.482735
Number of used sentences in train = 2811
Total loss for epoch 7: 5523.131215
validation loss after epoch 7 : 1195.201889
	Epoch 8....
Epoch has taken 0:03:16.502405
Number of used sentences in train = 2811
Total loss for epoch 8: 5296.270202
validation loss after epoch 8 : 1213.271744
	Epoch 9....
Epoch has taken 0:03:03.238408
Number of used sentences in train = 2811
Total loss for epoch 9: 5132.352035
validation loss after epoch 9 : 1291.477946
	Epoch 10....
Epoch has taken 0:02:58.353976
Number of used sentences in train = 2811
Total loss for epoch 10: 5069.981518
validation loss after epoch 10 : 1266.803681
	Epoch 11....
Epoch has taken 0:02:58.361258
Number of used sentences in train = 2811
Total loss for epoch 11: 4936.009978
validation loss after epoch 11 : 1322.870773
	Epoch 12....
Epoch has taken 0:03:16.273426
Number of used sentences in train = 2811
Total loss for epoch 12: 4865.910555
validation loss after epoch 12 : 1395.095356
	Epoch 13....
Epoch has taken 0:02:58.445726
Number of used sentences in train = 2811
Total loss for epoch 13: 4834.619987
validation loss after epoch 13 : 1422.273887
	Epoch 14....
Epoch has taken 0:02:58.417277
Number of used sentences in train = 2811
Total loss for epoch 14: 4795.634076
validation loss after epoch 14 : 1346.129553
	TransitionClassifier(
  (p_embeddings): Embedding(18, 23)
  (w_embeddings): Embedding(5889, 88)
  (lstm): LSTM(111, 71, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:58.266696
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1572.726233
	Epoch 1....
Epoch has taken 0:00:18.927550
Number of used sentences in train = 313
Total loss for epoch 1: 928.481832
	Epoch 2....
Epoch has taken 0:00:18.929086
Number of used sentences in train = 313
Total loss for epoch 2: 735.435006
	Epoch 3....
Epoch has taken 0:00:18.931643
Number of used sentences in train = 313
Total loss for epoch 3: 648.091603
	Epoch 4....
Epoch has taken 0:00:18.921759
Number of used sentences in train = 313
Total loss for epoch 4: 584.794837
	Epoch 5....
Epoch has taken 0:00:18.919512
Number of used sentences in train = 313
Total loss for epoch 5: 576.959525
	Epoch 6....
Epoch has taken 0:00:18.929083
Number of used sentences in train = 313
Total loss for epoch 6: 573.548035
	Epoch 7....
Epoch has taken 0:00:18.926491
Number of used sentences in train = 313
Total loss for epoch 7: 538.772355
	Epoch 8....
Epoch has taken 0:00:20.154564
Number of used sentences in train = 313
Total loss for epoch 8: 544.754375
	Epoch 9....
Epoch has taken 0:00:18.936783
Number of used sentences in train = 313
Total loss for epoch 9: 531.887466
	Epoch 10....
Epoch has taken 0:00:18.929102
Number of used sentences in train = 313
Total loss for epoch 10: 515.257313
	Epoch 11....
Epoch has taken 0:00:18.938854
Number of used sentences in train = 313
Total loss for epoch 11: 508.115102
	Epoch 12....
Epoch has taken 0:00:18.944111
Number of used sentences in train = 313
Total loss for epoch 12: 506.678122
	Epoch 13....
Epoch has taken 0:00:18.925160
Number of used sentences in train = 313
Total loss for epoch 13: 509.708717
	Epoch 14....
Epoch has taken 0:00:18.930192
Number of used sentences in train = 313
Total loss for epoch 14: 518.952125
Epoch has taken 0:00:18.931413

==================================================================================================
	Training time : 0:51:02.850508
==================================================================================================
	Identification : 0.495

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 23)
  (w_embeddings): Embedding(5644, 88)
  (lstm): LSTM(111, 71, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 16937.884194
validation loss after epoch 0 : 1288.718359
	Epoch 1....
Epoch has taken 0:02:02.003766
Number of used sentences in train = 2074
Total loss for epoch 1: 8394.605385
validation loss after epoch 1 : 918.269176
	Epoch 2....
Epoch has taken 0:02:02.150618
Number of used sentences in train = 2074
Total loss for epoch 2: 6360.644861
validation loss after epoch 2 : 839.573679
	Epoch 3....
Epoch has taken 0:02:02.284341
Number of used sentences in train = 2074
Total loss for epoch 3: 5367.329322
validation loss after epoch 3 : 858.925513
	Epoch 4....
Epoch has taken 0:02:03.706810
Number of used sentences in train = 2074
Total loss for epoch 4: 4799.013907
validation loss after epoch 4 : 877.977009
	Epoch 5....
Epoch has taken 0:02:02.225088
Number of used sentences in train = 2074
Total loss for epoch 5: 4403.941510
validation loss after epoch 5 : 916.002686
	Epoch 6....
Epoch has taken 0:02:02.283538
Number of used sentences in train = 2074
Total loss for epoch 6: 4103.940546
validation loss after epoch 6 : 976.120205
	Epoch 7....
Epoch has taken 0:02:02.240756
Number of used sentences in train = 2074
Total loss for epoch 7: 3965.298473
validation loss after epoch 7 : 1033.136598
	Epoch 8....
Epoch has taken 0:02:03.939788
Number of used sentences in train = 2074
Total loss for epoch 8: 3795.797593
validation loss after epoch 8 : 1119.129838
	Epoch 9....
Epoch has taken 0:02:13.436204
Number of used sentences in train = 2074
Total loss for epoch 9: 3705.954425
validation loss after epoch 9 : 1069.144939
	Epoch 10....
Epoch has taken 0:02:12.269615
Number of used sentences in train = 2074
Total loss for epoch 10: 3610.164056
validation loss after epoch 10 : 1124.099341
	Epoch 11....
Epoch has taken 0:02:05.416312
Number of used sentences in train = 2074
Total loss for epoch 11: 3539.953094
validation loss after epoch 11 : 1140.024345
	Epoch 12....
Epoch has taken 0:02:02.244551
Number of used sentences in train = 2074
Total loss for epoch 12: 3511.544245
validation loss after epoch 12 : 1196.131628
	Epoch 13....
Epoch has taken 0:02:02.243912
Number of used sentences in train = 2074
Total loss for epoch 13: 3445.960179
validation loss after epoch 13 : 1279.735961
	Epoch 14....
Epoch has taken 0:02:02.194945
Number of used sentences in train = 2074
Total loss for epoch 14: 3424.354682
validation loss after epoch 14 : 1337.172744
	TransitionClassifier(
  (p_embeddings): Embedding(18, 23)
  (w_embeddings): Embedding(5644, 88)
  (lstm): LSTM(111, 71, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:02.122408
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1546.193352
	Epoch 1....
Epoch has taken 0:00:12.479489
Number of used sentences in train = 231
Total loss for epoch 1: 794.835914
	Epoch 2....
Epoch has taken 0:00:12.500943
Number of used sentences in train = 231
Total loss for epoch 2: 633.411365
	Epoch 3....
Epoch has taken 0:00:12.498902
Number of used sentences in train = 231
Total loss for epoch 3: 512.714808
	Epoch 4....
Epoch has taken 0:00:12.353821
Number of used sentences in train = 231
Total loss for epoch 4: 466.319682
	Epoch 5....
Epoch has taken 0:00:12.306362
Number of used sentences in train = 231
Total loss for epoch 5: 444.513633
	Epoch 6....
Epoch has taken 0:00:12.991104
Number of used sentences in train = 231
Total loss for epoch 6: 405.102551
	Epoch 7....
Epoch has taken 0:00:13.790534
Number of used sentences in train = 231
Total loss for epoch 7: 399.499617
	Epoch 8....
Epoch has taken 0:00:13.786307
Number of used sentences in train = 231
Total loss for epoch 8: 389.978537
	Epoch 9....
Epoch has taken 0:00:13.785855
Number of used sentences in train = 231
Total loss for epoch 9: 376.088880
	Epoch 10....
Epoch has taken 0:00:13.785333
Number of used sentences in train = 231
Total loss for epoch 10: 373.882813
	Epoch 11....
Epoch has taken 0:00:13.788943
Number of used sentences in train = 231
Total loss for epoch 11: 382.110882
	Epoch 12....
Epoch has taken 0:00:13.773662
Number of used sentences in train = 231
Total loss for epoch 12: 371.019229
	Epoch 13....
Epoch has taken 0:00:13.793009
Number of used sentences in train = 231
Total loss for epoch 13: 364.103756
	Epoch 14....
Epoch has taken 0:00:13.789412
Number of used sentences in train = 231
Total loss for epoch 14: 361.468623
Epoch has taken 0:00:13.796303

==================================================================================================
	Training time : 0:34:20.322161
==================================================================================================
	Identification : 0.304

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 23)
  (w_embeddings): Embedding(6846, 88)
  (lstm): LSTM(111, 71, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18062.121640
validation loss after epoch 0 : 1579.936883
	Epoch 1....
Epoch has taken 0:04:12.923538
Number of used sentences in train = 3226
Total loss for epoch 1: 12651.774064
validation loss after epoch 1 : 1471.629789
	Epoch 2....
Epoch has taken 0:04:20.882000
Number of used sentences in train = 3226
Total loss for epoch 2: 10902.141445
validation loss after epoch 2 : 1455.603699
	Epoch 3....
Epoch has taken 0:04:06.602354
Number of used sentences in train = 3226
Total loss for epoch 3: 9759.464461
validation loss after epoch 3 : 1551.673255
	Epoch 4....
Epoch has taken 0:03:59.652730
Number of used sentences in train = 3226
Total loss for epoch 4: 8910.281244
validation loss after epoch 4 : 1705.552732
	Epoch 5....
Epoch has taken 0:03:59.016762
Number of used sentences in train = 3226
Total loss for epoch 5: 8328.616950
validation loss after epoch 5 : 1684.543162
	Epoch 6....
Epoch has taken 0:03:57.259758
Number of used sentences in train = 3226
Total loss for epoch 6: 7975.368022
validation loss after epoch 6 : 1902.702385
	Epoch 7....
Epoch has taken 0:03:57.442472
Number of used sentences in train = 3226
Total loss for epoch 7: 7682.028588
validation loss after epoch 7 : 1746.865404
	Epoch 8....
Epoch has taken 0:03:59.112367
Number of used sentences in train = 3226
Total loss for epoch 8: 7348.660263
validation loss after epoch 8 : 1914.033337
	Epoch 9....
Epoch has taken 0:03:57.136772
Number of used sentences in train = 3226
Total loss for epoch 9: 7116.194130
validation loss after epoch 9 : 2045.938240
	Epoch 10....
Epoch has taken 0:03:57.800233
Number of used sentences in train = 3226
Total loss for epoch 10: 6973.445511
validation loss after epoch 10 : 2139.522894
	Epoch 11....
Epoch has taken 0:03:59.141396
Number of used sentences in train = 3226
Total loss for epoch 11: 6814.789559
validation loss after epoch 11 : 2177.321849
	Epoch 12....
Epoch has taken 0:03:58.257671
Number of used sentences in train = 3226
Total loss for epoch 12: 6724.438309
validation loss after epoch 12 : 2191.445878
	Epoch 13....
Epoch has taken 0:03:56.893184
Number of used sentences in train = 3226
Total loss for epoch 13: 6638.758506
validation loss after epoch 13 : 2294.631818
	Epoch 14....
Epoch has taken 0:03:58.062391
Number of used sentences in train = 3226
Total loss for epoch 14: 6613.297813
validation loss after epoch 14 : 2239.993525
	TransitionClassifier(
  (p_embeddings): Embedding(13, 23)
  (w_embeddings): Embedding(6846, 88)
  (lstm): LSTM(111, 71, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=1136, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:59.123749
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1958.807929
	Epoch 1....
Epoch has taken 0:00:23.447533
Number of used sentences in train = 359
Total loss for epoch 1: 1315.487838
	Epoch 2....
Epoch has taken 0:00:23.429917
Number of used sentences in train = 359
Total loss for epoch 2: 1043.153126
	Epoch 3....
Epoch has taken 0:00:23.451249
Number of used sentences in train = 359
Total loss for epoch 3: 889.626328
	Epoch 4....
Epoch has taken 0:00:23.449519
Number of used sentences in train = 359
Total loss for epoch 4: 833.921742
	Epoch 5....
Epoch has taken 0:00:23.446076
Number of used sentences in train = 359
Total loss for epoch 5: 773.089407
	Epoch 6....
Epoch has taken 0:00:23.451038
Number of used sentences in train = 359
Total loss for epoch 6: 769.168537
	Epoch 7....
Epoch has taken 0:00:23.448614
Number of used sentences in train = 359
Total loss for epoch 7: 720.067548
	Epoch 8....
Epoch has taken 0:00:23.189628
Number of used sentences in train = 359
Total loss for epoch 8: 720.433819
	Epoch 9....
Epoch has taken 0:00:23.477701
Number of used sentences in train = 359
Total loss for epoch 9: 713.408340
	Epoch 10....
Epoch has taken 0:00:23.450221
Number of used sentences in train = 359
Total loss for epoch 10: 691.220400
	Epoch 11....
Epoch has taken 0:00:23.441959
Number of used sentences in train = 359
Total loss for epoch 11: 693.673122
	Epoch 12....
Epoch has taken 0:00:23.460835
Number of used sentences in train = 359
Total loss for epoch 12: 685.417133
	Epoch 13....
Epoch has taken 0:00:23.452347
Number of used sentences in train = 359
Total loss for epoch 13: 700.331437
	Epoch 14....
Epoch has taken 0:00:23.348795
Number of used sentences in train = 359
Total loss for epoch 14: 697.391076
Epoch has taken 0:00:23.465010

==================================================================================================
	Training time : 1:06:11.371056
==================================================================================================
	Identification : 0.035

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 55, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 30, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 73, 'lstmDropout': 0.14, 'denseActivation': 'tanh', 'wordDim': 80, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1177, 80)
  (lstm): LSTM(110, 73, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=1168, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12655.241405
validation loss after epoch 0 : 964.715589
	Epoch 1....
Epoch has taken 0:02:57.580670
Number of used sentences in train = 2811
Total loss for epoch 1: 7877.544111
validation loss after epoch 1 : 849.937036
	Epoch 2....
Epoch has taken 0:02:56.938035
Number of used sentences in train = 2811
Total loss for epoch 2: 7043.445961
validation loss after epoch 2 : 829.825691
	Epoch 3....
Epoch has taken 0:02:58.682469
Number of used sentences in train = 2811
Total loss for epoch 3: 6346.791585
validation loss after epoch 3 : 826.333058
	Epoch 4....
Epoch has taken 0:02:58.037379
Number of used sentences in train = 2811
Total loss for epoch 4: 5862.759351
validation loss after epoch 4 : 864.124783
	Epoch 5....
Epoch has taken 0:02:57.070499
Number of used sentences in train = 2811
Total loss for epoch 5: 5547.275809
validation loss after epoch 5 : 878.491687
	Epoch 6....
Epoch has taken 0:02:57.522893
Number of used sentences in train = 2811
Total loss for epoch 6: 5296.065094
validation loss after epoch 6 : 929.650998
	Epoch 7....
Epoch has taken 0:02:58.773916
Number of used sentences in train = 2811
Total loss for epoch 7: 5143.371122
validation loss after epoch 7 : 945.676068
	Epoch 8....
Epoch has taken 0:02:58.018885
Number of used sentences in train = 2811
Total loss for epoch 8: 4973.696625
validation loss after epoch 8 : 980.795972
	Epoch 9....
Epoch has taken 0:02:58.684307
Number of used sentences in train = 2811
Total loss for epoch 9: 4903.912031
validation loss after epoch 9 : 1008.906581
	Epoch 10....
Epoch has taken 0:02:58.642538
Number of used sentences in train = 2811
Total loss for epoch 10: 4787.859607
validation loss after epoch 10 : 1051.543952
	Epoch 11....
Epoch has taken 0:02:58.896620
Number of used sentences in train = 2811
Total loss for epoch 11: 4716.994425
validation loss after epoch 11 : 1058.346451
	Epoch 12....
Epoch has taken 0:02:57.052636
Number of used sentences in train = 2811
Total loss for epoch 12: 4662.264500
validation loss after epoch 12 : 1021.326370
	Epoch 13....
Epoch has taken 0:02:57.071003
Number of used sentences in train = 2811
Total loss for epoch 13: 4638.448065
validation loss after epoch 13 : 1070.398206
	Epoch 14....
Epoch has taken 0:02:57.063816
Number of used sentences in train = 2811
Total loss for epoch 14: 4586.294180
validation loss after epoch 14 : 1029.571990
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1177, 80)
  (lstm): LSTM(110, 73, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=1168, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.224461
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1306.854001
	Epoch 1....
Epoch has taken 0:00:18.977261
Number of used sentences in train = 313
Total loss for epoch 1: 764.165127
	Epoch 2....
Epoch has taken 0:00:18.968842
Number of used sentences in train = 313
Total loss for epoch 2: 618.343661
	Epoch 3....
Epoch has taken 0:00:18.995009
Number of used sentences in train = 313
Total loss for epoch 3: 554.519008
	Epoch 4....
Epoch has taken 0:00:18.982009
Number of used sentences in train = 313
Total loss for epoch 4: 534.511967
	Epoch 5....
Epoch has taken 0:00:18.994920
Number of used sentences in train = 313
Total loss for epoch 5: 529.080683
	Epoch 6....
Epoch has taken 0:00:18.975565
Number of used sentences in train = 313
Total loss for epoch 6: 520.190068
	Epoch 7....
Epoch has taken 0:00:18.990380
Number of used sentences in train = 313
Total loss for epoch 7: 518.783839
	Epoch 8....
Epoch has taken 0:00:18.983994
Number of used sentences in train = 313
Total loss for epoch 8: 506.962584
	Epoch 9....
Epoch has taken 0:00:18.984787
Number of used sentences in train = 313
Total loss for epoch 9: 503.277174
	Epoch 10....
Epoch has taken 0:00:18.978476
Number of used sentences in train = 313
Total loss for epoch 10: 503.511297
	Epoch 11....
Epoch has taken 0:00:18.987360
Number of used sentences in train = 313
Total loss for epoch 11: 503.194392
	Epoch 12....
Epoch has taken 0:00:18.976870
Number of used sentences in train = 313
Total loss for epoch 12: 503.236694
	Epoch 13....
Epoch has taken 0:00:19.031299
Number of used sentences in train = 313
Total loss for epoch 13: 503.197632
	Epoch 14....
Epoch has taken 0:00:20.926716
Number of used sentences in train = 313
Total loss for epoch 14: 502.163346
Epoch has taken 0:00:20.939025

==================================================================================================
	Training time : 0:49:16.450782
==================================================================================================
	Identification : 0.316

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1133, 80)
  (lstm): LSTM(110, 73, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=1168, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11498.353505
validation loss after epoch 0 : 685.644292
	Epoch 1....
Epoch has taken 0:02:14.308334
Number of used sentences in train = 2074
Total loss for epoch 1: 5855.615931
validation loss after epoch 1 : 622.564353
	Epoch 2....
Epoch has taken 0:02:14.282269
Number of used sentences in train = 2074
Total loss for epoch 2: 5056.995163
validation loss after epoch 2 : 604.610122
	Epoch 3....
Epoch has taken 0:02:14.339917
Number of used sentences in train = 2074
Total loss for epoch 3: 4515.052132
validation loss after epoch 3 : 608.284024
	Epoch 4....
Epoch has taken 0:02:13.483980
Number of used sentences in train = 2074
Total loss for epoch 4: 4171.829934
validation loss after epoch 4 : 614.037147
	Epoch 5....
Epoch has taken 0:02:14.369389
Number of used sentences in train = 2074
Total loss for epoch 5: 3905.834275
validation loss after epoch 5 : 656.840659
	Epoch 6....
Epoch has taken 0:02:06.079613
Number of used sentences in train = 2074
Total loss for epoch 6: 3640.409392
validation loss after epoch 6 : 695.096687
	Epoch 7....
Epoch has taken 0:02:02.474838
Number of used sentences in train = 2074
Total loss for epoch 7: 3526.757135
validation loss after epoch 7 : 741.975392
	Epoch 8....
Epoch has taken 0:02:08.148451
Number of used sentences in train = 2074
Total loss for epoch 8: 3466.824558
validation loss after epoch 8 : 683.908173
	Epoch 9....
Epoch has taken 0:02:05.033845
Number of used sentences in train = 2074
Total loss for epoch 9: 3404.979636
validation loss after epoch 9 : 751.504335
	Epoch 10....
Epoch has taken 0:02:10.831797
Number of used sentences in train = 2074
Total loss for epoch 10: 3342.380357
validation loss after epoch 10 : 738.301739
	Epoch 11....
Epoch has taken 0:02:14.448742
Number of used sentences in train = 2074
Total loss for epoch 11: 3302.432140
validation loss after epoch 11 : 714.322181
	Epoch 12....
Epoch has taken 0:02:02.049740
Number of used sentences in train = 2074
Total loss for epoch 12: 3266.802733
validation loss after epoch 12 : 729.588031
	Epoch 13....
Epoch has taken 0:02:02.425007
Number of used sentences in train = 2074
Total loss for epoch 13: 3231.157533
validation loss after epoch 13 : 757.156850
	Epoch 14....
Epoch has taken 0:02:01.996566
Number of used sentences in train = 2074
Total loss for epoch 14: 3240.015784
validation loss after epoch 14 : 817.776110
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1133, 80)
  (lstm): LSTM(110, 73, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=1168, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.453137
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1181.377185
	Epoch 1....
Epoch has taken 0:00:12.559812
Number of used sentences in train = 231
Total loss for epoch 1: 683.992571
	Epoch 2....
Epoch has taken 0:00:12.539788
Number of used sentences in train = 231
Total loss for epoch 2: 482.480340
	Epoch 3....
Epoch has taken 0:00:12.449769
Number of used sentences in train = 231
Total loss for epoch 3: 412.093286
	Epoch 4....
Epoch has taken 0:00:12.548710
Number of used sentences in train = 231
Total loss for epoch 4: 380.026915
	Epoch 5....
Epoch has taken 0:00:12.535199
Number of used sentences in train = 231
Total loss for epoch 5: 369.134431
	Epoch 6....
Epoch has taken 0:00:12.546474
Number of used sentences in train = 231
Total loss for epoch 6: 361.348759
	Epoch 7....
Epoch has taken 0:00:12.541785
Number of used sentences in train = 231
Total loss for epoch 7: 355.690036
	Epoch 8....
Epoch has taken 0:00:12.533687
Number of used sentences in train = 231
Total loss for epoch 8: 350.700394
	Epoch 9....
Epoch has taken 0:00:12.545954
Number of used sentences in train = 231
Total loss for epoch 9: 351.410029
	Epoch 10....
Epoch has taken 0:00:12.546754
Number of used sentences in train = 231
Total loss for epoch 10: 355.273120
	Epoch 11....
Epoch has taken 0:00:12.560669
Number of used sentences in train = 231
Total loss for epoch 11: 346.818643
	Epoch 12....
Epoch has taken 0:00:12.542054
Number of used sentences in train = 231
Total loss for epoch 12: 345.538453
	Epoch 13....
Epoch has taken 0:00:12.547448
Number of used sentences in train = 231
Total loss for epoch 13: 345.299867
	Epoch 14....
Epoch has taken 0:00:12.536139
Number of used sentences in train = 231
Total loss for epoch 14: 345.183208
Epoch has taken 0:00:12.547379

==================================================================================================
	Training time : 0:35:14.169395
==================================================================================================
	Identification : 0.456

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(1202, 80)
  (lstm): LSTM(110, 73, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=1168, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16268.264680
validation loss after epoch 0 : 1299.330731
	Epoch 1....
Epoch has taken 0:04:02.391111
Number of used sentences in train = 3226
Total loss for epoch 1: 10996.833120
validation loss after epoch 1 : 1242.918029
	Epoch 2....
Epoch has taken 0:03:59.732760
Number of used sentences in train = 3226
Total loss for epoch 2: 9939.131539
validation loss after epoch 2 : 1244.610045
	Epoch 3....
Epoch has taken 0:04:00.838391
Number of used sentences in train = 3226
Total loss for epoch 3: 9274.471923
validation loss after epoch 3 : 1336.650206
	Epoch 4....
Epoch has taken 0:04:03.392217
Number of used sentences in train = 3226
Total loss for epoch 4: 8797.112161
validation loss after epoch 4 : 1363.744599
	Epoch 5....
Epoch has taken 0:04:03.124242
Number of used sentences in train = 3226
Total loss for epoch 5: 8384.257481
validation loss after epoch 5 : 1372.219719
	Epoch 6....
Epoch has taken 0:04:00.560878
Number of used sentences in train = 3226
Total loss for epoch 6: 8127.159907
validation loss after epoch 6 : 1422.592669
	Epoch 7....
Epoch has taken 0:04:03.341011
Number of used sentences in train = 3226
Total loss for epoch 7: 7860.550673
validation loss after epoch 7 : 1469.016344
	Epoch 8....
Epoch has taken 0:04:01.335447
Number of used sentences in train = 3226
Total loss for epoch 8: 7556.921539
validation loss after epoch 8 : 1537.960750
	Epoch 9....
Epoch has taken 0:04:00.800575
Number of used sentences in train = 3226
Total loss for epoch 9: 7305.539015
validation loss after epoch 9 : 1505.259706
	Epoch 10....
Epoch has taken 0:04:03.462936
Number of used sentences in train = 3226
Total loss for epoch 10: 7199.770637
validation loss after epoch 10 : 1671.132348
	Epoch 11....
Epoch has taken 0:04:01.265267
Number of used sentences in train = 3226
Total loss for epoch 11: 6945.102105
validation loss after epoch 11 : 1702.165676
	Epoch 12....
Epoch has taken 0:04:21.708510
Number of used sentences in train = 3226
Total loss for epoch 12: 6890.339277
validation loss after epoch 12 : 1643.601923
	Epoch 13....
Epoch has taken 0:04:10.525823
Number of used sentences in train = 3226
Total loss for epoch 13: 6763.857263
validation loss after epoch 13 : 1754.289792
	Epoch 14....
Epoch has taken 0:04:04.828360
Number of used sentences in train = 3226
Total loss for epoch 14: 6661.882568
validation loss after epoch 14 : 1879.979943
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(1202, 80)
  (lstm): LSTM(110, 73, num_layers=2, dropout=0.14, bidirectional=True)
  (linear1): Linear(in_features=1168, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:27.094468
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1939.497931
	Epoch 1....
Epoch has taken 0:00:26.349614
Number of used sentences in train = 359
Total loss for epoch 1: 1176.892459
	Epoch 2....
Epoch has taken 0:00:26.362890
Number of used sentences in train = 359
Total loss for epoch 2: 1015.053752
	Epoch 3....
Epoch has taken 0:00:25.978567
Number of used sentences in train = 359
Total loss for epoch 3: 884.209528
	Epoch 4....
Epoch has taken 0:00:26.351870
Number of used sentences in train = 359
Total loss for epoch 4: 802.045078
	Epoch 5....
Epoch has taken 0:00:25.299552
