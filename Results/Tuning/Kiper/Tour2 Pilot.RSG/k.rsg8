INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11354.017598
validation loss after epoch 0 : 866.454756
	Epoch 1....
Epoch has taken 0:03:04.542087
Number of used sentences in train = 2811
Total loss for epoch 1: 7612.758761
validation loss after epoch 1 : 843.044730
	Epoch 2....
Epoch has taken 0:03:04.410966
Number of used sentences in train = 2811
Total loss for epoch 2: 6858.608082
validation loss after epoch 2 : 803.027593
	Epoch 3....
Epoch has taken 0:03:07.246244
Number of used sentences in train = 2811
Total loss for epoch 3: 6385.488079
validation loss after epoch 3 : 793.367005
	Epoch 4....
Epoch has taken 0:03:06.534376
Number of used sentences in train = 2811
Total loss for epoch 4: 6085.406482
validation loss after epoch 4 : 774.751086
	Epoch 5....
Epoch has taken 0:03:08.664619
Number of used sentences in train = 2811
Total loss for epoch 5: 5739.886095
validation loss after epoch 5 : 838.206632
	Epoch 6....
Epoch has taken 0:03:36.762487
Number of used sentences in train = 2811
Total loss for epoch 6: 5491.828758
validation loss after epoch 6 : 842.585631
	Epoch 7....
Epoch has taken 0:03:07.986184
Number of used sentences in train = 2811
Total loss for epoch 7: 5348.627263
validation loss after epoch 7 : 819.915338
	Epoch 8....
Epoch has taken 0:03:07.340970
Number of used sentences in train = 2811
Total loss for epoch 8: 5185.333820
validation loss after epoch 8 : 845.496372
	Epoch 9....
Epoch has taken 0:03:08.527827
Number of used sentences in train = 2811
Total loss for epoch 9: 5106.997238
validation loss after epoch 9 : 860.032624
	Epoch 10....
Epoch has taken 0:03:14.196182
Number of used sentences in train = 2811
Total loss for epoch 10: 5015.988879
validation loss after epoch 10 : 868.138370
	Epoch 11....
Epoch has taken 0:03:07.146914
Number of used sentences in train = 2811
Total loss for epoch 11: 4903.079279
validation loss after epoch 11 : 933.219282
	Epoch 12....
Epoch has taken 0:03:03.963099
Number of used sentences in train = 2811
Total loss for epoch 12: 4866.431682
validation loss after epoch 12 : 937.839239
	Epoch 13....
Epoch has taken 0:03:05.889749
Number of used sentences in train = 2811
Total loss for epoch 13: 4836.422326
validation loss after epoch 13 : 963.357719
	Epoch 14....
Epoch has taken 0:03:01.858357
Number of used sentences in train = 2811
Total loss for epoch 14: 4807.044559
validation loss after epoch 14 : 982.480989
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:03.724532
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1269.924790
	Epoch 1....
Epoch has taken 0:00:19.510188
Number of used sentences in train = 313
Total loss for epoch 1: 793.844377
	Epoch 2....
Epoch has taken 0:00:19.582659
Number of used sentences in train = 313
Total loss for epoch 2: 716.926477
	Epoch 3....
Epoch has taken 0:00:19.210760
Number of used sentences in train = 313
Total loss for epoch 3: 621.397684
	Epoch 4....
Epoch has taken 0:00:19.350370
Number of used sentences in train = 313
Total loss for epoch 4: 597.966245
	Epoch 5....
Epoch has taken 0:00:19.218336
Number of used sentences in train = 313
Total loss for epoch 5: 571.770360
	Epoch 6....
Epoch has taken 0:00:19.231691
Number of used sentences in train = 313
Total loss for epoch 6: 558.664540
	Epoch 7....
Epoch has taken 0:00:19.235157
Number of used sentences in train = 313
Total loss for epoch 7: 553.820126
	Epoch 8....
Epoch has taken 0:00:19.226450
Number of used sentences in train = 313
Total loss for epoch 8: 544.278940
	Epoch 9....
Epoch has taken 0:00:19.218143
Number of used sentences in train = 313
Total loss for epoch 9: 539.025276
	Epoch 10....
Epoch has taken 0:00:19.360265
Number of used sentences in train = 313
Total loss for epoch 10: 536.994028
	Epoch 11....
Epoch has taken 0:00:19.151969
Number of used sentences in train = 313
Total loss for epoch 11: 537.272140
	Epoch 12....
Epoch has taken 0:00:19.131629
Number of used sentences in train = 313
Total loss for epoch 12: 536.162639
	Epoch 13....
Epoch has taken 0:00:19.145062
Number of used sentences in train = 313
Total loss for epoch 13: 535.663985
	Epoch 14....
Epoch has taken 0:00:19.144021
Number of used sentences in train = 313
Total loss for epoch 14: 535.530124
Epoch has taken 0:00:19.143401

==================================================================================================
	Training time : 0:52:03.953854
==================================================================================================
	Identification : 0.467

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9088.608989
validation loss after epoch 0 : 654.254293
	Epoch 1....
Epoch has taken 0:02:05.575416
Number of used sentences in train = 2074
Total loss for epoch 1: 5451.230651
validation loss after epoch 1 : 649.104341
	Epoch 2....
Epoch has taken 0:02:08.521536
Number of used sentences in train = 2074
Total loss for epoch 2: 4799.612832
validation loss after epoch 2 : 650.005339
	Epoch 3....
Epoch has taken 0:02:08.836726
Number of used sentences in train = 2074
Total loss for epoch 3: 4382.826671
validation loss after epoch 3 : 655.479226
	Epoch 4....
Epoch has taken 0:02:08.274642
Number of used sentences in train = 2074
Total loss for epoch 4: 4005.620282
validation loss after epoch 4 : 688.440311
	Epoch 5....
Epoch has taken 0:02:10.376222
Number of used sentences in train = 2074
Total loss for epoch 5: 3761.346738
validation loss after epoch 5 : 675.778915
	Epoch 6....
Epoch has taken 0:02:10.450253
Number of used sentences in train = 2074
Total loss for epoch 6: 3553.358722
validation loss after epoch 6 : 748.266921
	Epoch 7....
Epoch has taken 0:02:10.088325
Number of used sentences in train = 2074
Total loss for epoch 7: 3448.443132
validation loss after epoch 7 : 767.692205
	Epoch 8....
Epoch has taken 0:02:08.825385
Number of used sentences in train = 2074
Total loss for epoch 8: 3391.126084
validation loss after epoch 8 : 849.193012
	Epoch 9....
Epoch has taken 0:02:09.134630
Number of used sentences in train = 2074
Total loss for epoch 9: 3345.618516
validation loss after epoch 9 : 779.055673
	Epoch 10....
Epoch has taken 0:02:08.155921
Number of used sentences in train = 2074
Total loss for epoch 10: 3307.953354
validation loss after epoch 10 : 811.269098
	Epoch 11....
Epoch has taken 0:02:09.255593
Number of used sentences in train = 2074
Total loss for epoch 11: 3277.707860
validation loss after epoch 11 : 793.321617
	Epoch 12....
Epoch has taken 0:02:10.675122
Number of used sentences in train = 2074
Total loss for epoch 12: 3273.514832
validation loss after epoch 12 : 820.319923
	Epoch 13....
Epoch has taken 0:02:11.946372
Number of used sentences in train = 2074
Total loss for epoch 13: 3228.031591
validation loss after epoch 13 : 864.134277
	Epoch 14....
Epoch has taken 0:02:02.085180
Number of used sentences in train = 2074
Total loss for epoch 14: 3220.951910
validation loss after epoch 14 : 977.097102
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:20.571167
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1405.465862
	Epoch 1....
Epoch has taken 0:00:12.478283
Number of used sentences in train = 231
Total loss for epoch 1: 601.413521
	Epoch 2....
Epoch has taken 0:00:12.488017
Number of used sentences in train = 231
Total loss for epoch 2: 437.031852
	Epoch 3....
Epoch has taken 0:00:12.453826
Number of used sentences in train = 231
Total loss for epoch 3: 407.062737
	Epoch 4....
Epoch has taken 0:00:12.492522
Number of used sentences in train = 231
Total loss for epoch 4: 382.370567
	Epoch 5....
Epoch has taken 0:00:12.456761
Number of used sentences in train = 231
Total loss for epoch 5: 366.484039
	Epoch 6....
Epoch has taken 0:00:12.481879
Number of used sentences in train = 231
Total loss for epoch 6: 356.915113
	Epoch 7....
Epoch has taken 0:00:12.475032
Number of used sentences in train = 231
Total loss for epoch 7: 355.793580
	Epoch 8....
Epoch has taken 0:00:12.484422
Number of used sentences in train = 231
Total loss for epoch 8: 351.996756
	Epoch 9....
Epoch has taken 0:00:12.462698
Number of used sentences in train = 231
Total loss for epoch 9: 352.513711
	Epoch 10....
Epoch has taken 0:00:12.476031
Number of used sentences in train = 231
Total loss for epoch 10: 349.887915
	Epoch 11....
Epoch has taken 0:00:12.481179
Number of used sentences in train = 231
Total loss for epoch 11: 350.468814
	Epoch 12....
Epoch has taken 0:00:12.467968
Number of used sentences in train = 231
Total loss for epoch 12: 348.969832
	Epoch 13....
Epoch has taken 0:00:12.465174
Number of used sentences in train = 231
Total loss for epoch 13: 348.274477
	Epoch 14....
Epoch has taken 0:00:12.468225
Number of used sentences in train = 231
Total loss for epoch 14: 348.907451
Epoch has taken 0:00:12.473446

==================================================================================================
	Training time : 0:35:30.213424
==================================================================================================
	Identification : 0.318

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 26022.889513
validation loss after epoch 0 : 1200.788921
	Epoch 1....
Epoch has taken 0:03:56.453665
Number of used sentences in train = 3226
Total loss for epoch 1: 10242.427515
validation loss after epoch 1 : 1095.412873
	Epoch 2....
Epoch has taken 0:03:56.534024
Number of used sentences in train = 3226
Total loss for epoch 2: 9129.637850
validation loss after epoch 2 : 1078.736054
	Epoch 3....
Epoch has taken 0:03:58.508513
Number of used sentences in train = 3226
Total loss for epoch 3: 8541.357185
validation loss after epoch 3 : 1094.847199
	Epoch 4....
Epoch has taken 0:04:34.300531
Number of used sentences in train = 3226
Total loss for epoch 4: 8120.349236
validation loss after epoch 4 : 1108.893417
	Epoch 5....
Epoch has taken 0:03:58.548706
Number of used sentences in train = 3226
Total loss for epoch 5: 7791.531427
validation loss after epoch 5 : 1114.918948
	Epoch 6....
Epoch has taken 0:03:56.326200
Number of used sentences in train = 3226
Total loss for epoch 6: 7506.829431
validation loss after epoch 6 : 1186.920447
	Epoch 7....
Epoch has taken 0:04:00.551536
Number of used sentences in train = 3226
Total loss for epoch 7: 7278.070315
validation loss after epoch 7 : 1286.173839
	Epoch 8....
Epoch has taken 0:03:58.663124
Number of used sentences in train = 3226
Total loss for epoch 8: 7126.362793
validation loss after epoch 8 : 1192.159668
	Epoch 9....
Epoch has taken 0:03:58.408645
Number of used sentences in train = 3226
Total loss for epoch 9: 6967.074001
validation loss after epoch 9 : 1284.107947
	Epoch 10....
Epoch has taken 0:03:56.633299
Number of used sentences in train = 3226
Total loss for epoch 10: 6898.397374
validation loss after epoch 10 : 1313.128130
	Epoch 11....
Epoch has taken 0:03:56.385101
Number of used sentences in train = 3226
Total loss for epoch 11: 6796.812208
validation loss after epoch 11 : 1301.751980
	Epoch 12....
Epoch has taken 0:03:58.353517
Number of used sentences in train = 3226
Total loss for epoch 12: 6689.995410
validation loss after epoch 12 : 1379.871936
	Epoch 13....
Epoch has taken 0:03:58.118426
Number of used sentences in train = 3226
Total loss for epoch 13: 6632.245554
validation loss after epoch 13 : 1442.085934
	Epoch 14....
Epoch has taken 0:03:57.429226
Number of used sentences in train = 3226
Total loss for epoch 14: 6560.946379
validation loss after epoch 14 : 1464.611978
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:57.584498
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1549.515919
	Epoch 1....
Epoch has taken 0:00:23.087995
Number of used sentences in train = 359
Total loss for epoch 1: 983.021814
	Epoch 2....
Epoch has taken 0:00:23.054801
Number of used sentences in train = 359
Total loss for epoch 2: 880.812747
	Epoch 3....
Epoch has taken 0:00:23.048966
Number of used sentences in train = 359
Total loss for epoch 3: 814.890700
	Epoch 4....
Epoch has taken 0:00:23.040301
Number of used sentences in train = 359
Total loss for epoch 4: 772.573687
	Epoch 5....
Epoch has taken 0:00:23.043706
Number of used sentences in train = 359
Total loss for epoch 5: 739.772774
	Epoch 6....
Epoch has taken 0:00:23.050881
Number of used sentences in train = 359
Total loss for epoch 6: 716.198056
	Epoch 7....
Epoch has taken 0:00:23.037109
Number of used sentences in train = 359
Total loss for epoch 7: 700.252350
	Epoch 8....
Epoch has taken 0:00:23.048651
Number of used sentences in train = 359
Total loss for epoch 8: 705.989960
	Epoch 9....
Epoch has taken 0:00:23.241173
Number of used sentences in train = 359
Total loss for epoch 9: 684.114018
	Epoch 10....
Epoch has taken 0:00:23.237852
Number of used sentences in train = 359
Total loss for epoch 10: 684.423565
	Epoch 11....
Epoch has taken 0:00:23.240739
Number of used sentences in train = 359
Total loss for epoch 11: 678.957939
	Epoch 12....
Epoch has taken 0:00:23.239937
Number of used sentences in train = 359
Total loss for epoch 12: 683.734787
	Epoch 13....
Epoch has taken 0:00:23.249330
Number of used sentences in train = 359
Total loss for epoch 13: 674.456145
	Epoch 14....
Epoch has taken 0:00:23.261752
Number of used sentences in train = 359
Total loss for epoch 14: 674.284725
Epoch has taken 0:00:23.256727

==================================================================================================
	Training time : 1:05:50.613650
==================================================================================================
	Identification : 0.023

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 55, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 66, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 33, 'lstmDropout': 0.37, 'denseActivation': 'tanh', 'wordDim': 221, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(9265, 221)
  (lstm): LSTM(287, 33, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15146.783890
validation loss after epoch 0 : 1170.280222
	Epoch 1....
Epoch has taken 0:02:57.373739
Number of used sentences in train = 2811
Total loss for epoch 1: 9404.564352
validation loss after epoch 1 : 1092.110758
	Epoch 2....
Epoch has taken 0:02:56.025047
Number of used sentences in train = 2811
Total loss for epoch 2: 7606.025448
validation loss after epoch 2 : 1058.678320
	Epoch 3....
Epoch has taken 0:02:56.428322
Number of used sentences in train = 2811
Total loss for epoch 3: 6626.735689
validation loss after epoch 3 : 1084.197003
	Epoch 4....
Epoch has taken 0:02:57.996476
Number of used sentences in train = 2811
Total loss for epoch 4: 6113.825175
validation loss after epoch 4 : 1294.678918
	Epoch 5....
Epoch has taken 0:02:58.015879
Number of used sentences in train = 2811
Total loss for epoch 5: 5744.192367
validation loss after epoch 5 : 1240.166302
	Epoch 6....
Epoch has taken 0:02:57.796712
Number of used sentences in train = 2811
Total loss for epoch 6: 5480.135962
validation loss after epoch 6 : 1257.192928
	Epoch 7....
Epoch has taken 0:02:56.541991
Number of used sentences in train = 2811
Total loss for epoch 7: 5308.170854
validation loss after epoch 7 : 1263.102686
	Epoch 8....
Epoch has taken 0:02:57.210923
Number of used sentences in train = 2811
Total loss for epoch 8: 5180.702993
validation loss after epoch 8 : 1242.157607
	Epoch 9....
Epoch has taken 0:03:01.440088
Number of used sentences in train = 2811
Total loss for epoch 9: 5011.386761
validation loss after epoch 9 : 1325.563785
	Epoch 10....
Epoch has taken 0:02:56.882459
Number of used sentences in train = 2811
Total loss for epoch 10: 5078.429361
validation loss after epoch 10 : 1339.992665
	Epoch 11....
Epoch has taken 0:02:57.128467
Number of used sentences in train = 2811
Total loss for epoch 11: 4930.458551
validation loss after epoch 11 : 1378.731688
	Epoch 12....
Epoch has taken 0:02:57.215327
Number of used sentences in train = 2811
Total loss for epoch 12: 4927.519441
validation loss after epoch 12 : 1420.803282
	Epoch 13....
Epoch has taken 0:03:01.234915
Number of used sentences in train = 2811
Total loss for epoch 13: 4854.643090
validation loss after epoch 13 : 1380.002519
	Epoch 14....
Epoch has taken 0:02:58.637766
Number of used sentences in train = 2811
Total loss for epoch 14: 4803.653980
validation loss after epoch 14 : 1426.834506
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(9265, 221)
  (lstm): LSTM(287, 33, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.247555
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1748.719898
	Epoch 1....
Epoch has taken 0:00:18.887623
Number of used sentences in train = 313
Total loss for epoch 1: 871.315869
	Epoch 2....
Epoch has taken 0:00:18.892330
Number of used sentences in train = 313
Total loss for epoch 2: 708.469541
	Epoch 3....
Epoch has taken 0:00:18.896998
Number of used sentences in train = 313
Total loss for epoch 3: 634.777251
	Epoch 4....
Epoch has taken 0:00:18.900540
Number of used sentences in train = 313
Total loss for epoch 4: 592.520533
	Epoch 5....
Epoch has taken 0:00:21.443791
Number of used sentences in train = 313
Total loss for epoch 5: 550.571712
	Epoch 6....
Epoch has taken 0:00:21.651918
Number of used sentences in train = 313
Total loss for epoch 6: 540.505306
	Epoch 7....
Epoch has taken 0:00:21.659268
Number of used sentences in train = 313
Total loss for epoch 7: 526.338472
	Epoch 8....
Epoch has taken 0:00:21.654705
Number of used sentences in train = 313
Total loss for epoch 8: 525.975751
	Epoch 9....
Epoch has taken 0:00:21.655259
Number of used sentences in train = 313
Total loss for epoch 9: 524.457781
	Epoch 10....
Epoch has taken 0:00:21.658730
Number of used sentences in train = 313
Total loss for epoch 10: 522.498889
	Epoch 11....
Epoch has taken 0:00:21.677269
Number of used sentences in train = 313
Total loss for epoch 11: 520.089012
	Epoch 12....
Epoch has taken 0:00:21.642930
Number of used sentences in train = 313
Total loss for epoch 12: 517.199341
	Epoch 13....
Epoch has taken 0:00:21.638010
Number of used sentences in train = 313
Total loss for epoch 13: 511.864939
	Epoch 14....
Epoch has taken 0:00:21.642126
Number of used sentences in train = 313
Total loss for epoch 14: 517.264531
Epoch has taken 0:00:21.649308

==================================================================================================
	Training time : 0:49:41.249811
==================================================================================================
	Identification : 0.172

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(7067, 221)
  (lstm): LSTM(287, 33, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11008.552661
validation loss after epoch 0 : 950.417600
	Epoch 1....
Epoch has taken 0:02:02.261932
Number of used sentences in train = 2074
Total loss for epoch 1: 7042.016379
validation loss after epoch 1 : 866.928843
	Epoch 2....
Epoch has taken 0:02:01.362436
Number of used sentences in train = 2074
Total loss for epoch 2: 5781.913821
validation loss after epoch 2 : 880.923487
	Epoch 3....
Epoch has taken 0:02:01.567375
Number of used sentences in train = 2074
Total loss for epoch 3: 5138.999325
validation loss after epoch 3 : 905.659254
	Epoch 4....
Epoch has taken 0:02:04.697288
Number of used sentences in train = 2074
Total loss for epoch 4: 4607.905081
validation loss after epoch 4 : 901.246409
	Epoch 5....
Epoch has taken 0:02:02.617600
Number of used sentences in train = 2074
Total loss for epoch 5: 4237.015396
validation loss after epoch 5 : 1057.234641
	Epoch 6....
Epoch has taken 0:02:01.746827
Number of used sentences in train = 2074
Total loss for epoch 6: 4064.907524
validation loss after epoch 6 : 1070.702298
	Epoch 7....
Epoch has taken 0:02:01.681780
Number of used sentences in train = 2074
Total loss for epoch 7: 3874.430355
validation loss after epoch 7 : 1114.955797
	Epoch 8....
Epoch has taken 0:02:02.678293
Number of used sentences in train = 2074
Total loss for epoch 8: 3796.792216
validation loss after epoch 8 : 1142.028008
	Epoch 9....
Epoch has taken 0:02:02.714601
Number of used sentences in train = 2074
Total loss for epoch 9: 3689.950876
validation loss after epoch 9 : 1097.692204
	Epoch 10....
Epoch has taken 0:02:01.639952
Number of used sentences in train = 2074
Total loss for epoch 10: 3582.352223
validation loss after epoch 10 : 1088.044455
	Epoch 11....
Epoch has taken 0:02:02.534200
Number of used sentences in train = 2074
Total loss for epoch 11: 3523.867015
validation loss after epoch 11 : 1106.861598
	Epoch 12....
Epoch has taken 0:02:02.465889
Number of used sentences in train = 2074
Total loss for epoch 12: 3515.484535
validation loss after epoch 12 : 1138.781942
	Epoch 13....
Epoch has taken 0:02:02.679075
Number of used sentences in train = 2074
Total loss for epoch 13: 3478.420522
validation loss after epoch 13 : 1198.365015
	Epoch 14....
Epoch has taken 0:02:01.606580
Number of used sentences in train = 2074
Total loss for epoch 14: 3435.919311
validation loss after epoch 14 : 1085.553268
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(7067, 221)
  (lstm): LSTM(287, 33, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.378363
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1630.774905
	Epoch 1....
Epoch has taken 0:00:12.513164
Number of used sentences in train = 231
Total loss for epoch 1: 720.329371
	Epoch 2....
Epoch has taken 0:00:12.529527
Number of used sentences in train = 231
Total loss for epoch 2: 556.793405
	Epoch 3....
Epoch has taken 0:00:12.492492
Number of used sentences in train = 231
Total loss for epoch 3: 498.110611
	Epoch 4....
Epoch has taken 0:00:12.506388
Number of used sentences in train = 231
Total loss for epoch 4: 420.145672
	Epoch 5....
Epoch has taken 0:00:12.502216
Number of used sentences in train = 231
Total loss for epoch 5: 401.974542
	Epoch 6....
Epoch has taken 0:00:12.515006
Number of used sentences in train = 231
Total loss for epoch 6: 384.817991
	Epoch 7....
Epoch has taken 0:00:12.492844
Number of used sentences in train = 231
Total loss for epoch 7: 392.930251
	Epoch 8....
Epoch has taken 0:00:12.495351
Number of used sentences in train = 231
Total loss for epoch 8: 392.667919
	Epoch 9....
Epoch has taken 0:00:12.496688
Number of used sentences in train = 231
Total loss for epoch 9: 370.042079
	Epoch 10....
Epoch has taken 0:00:12.496889
Number of used sentences in train = 231
Total loss for epoch 10: 368.452967
	Epoch 11....
Epoch has taken 0:00:12.432316
Number of used sentences in train = 231
Total loss for epoch 11: 368.564729
	Epoch 12....
Epoch has taken 0:00:12.460565
Number of used sentences in train = 231
Total loss for epoch 12: 370.099247
	Epoch 13....
Epoch has taken 0:00:12.439414
Number of used sentences in train = 231
Total loss for epoch 13: 363.823890
	Epoch 14....
Epoch has taken 0:00:12.439241
Number of used sentences in train = 231
Total loss for epoch 14: 359.422894
Epoch has taken 0:00:12.443494

==================================================================================================
	Training time : 0:33:41.243900
==================================================================================================
	Identification : 0.252

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 66)
  (w_embeddings): Embedding(17996, 221)
  (lstm): LSTM(287, 33, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18817.886397
validation loss after epoch 0 : 1651.728045
	Epoch 1....
Epoch has taken 0:04:12.985861
Number of used sentences in train = 3226
Total loss for epoch 1: 12812.669702
validation loss after epoch 1 : 1614.772857
	Epoch 2....
Epoch has taken 0:03:59.674862
Number of used sentences in train = 3226
Total loss for epoch 2: 10605.700325
validation loss after epoch 2 : 1558.488439
	Epoch 3....
Epoch has taken 0:04:00.932780
Number of used sentences in train = 3226
Total loss for epoch 3: 9240.907573
validation loss after epoch 3 : 1691.832494
	Epoch 4....
Epoch has taken 0:03:59.847567
Number of used sentences in train = 3226
Total loss for epoch 4: 8420.151074
validation loss after epoch 4 : 1855.632859
	Epoch 5....
Epoch has taken 0:03:59.566460
Number of used sentences in train = 3226
Total loss for epoch 5: 7903.322018
validation loss after epoch 5 : 1970.731742
	Epoch 6....
Epoch has taken 0:03:57.606785
Number of used sentences in train = 3226
Total loss for epoch 6: 7594.090079
validation loss after epoch 6 : 1929.877921
	Epoch 7....
Epoch has taken 0:03:58.451996
Number of used sentences in train = 3226
Total loss for epoch 7: 7283.541886
validation loss after epoch 7 : 2000.395967
	Epoch 8....
Epoch has taken 0:03:58.094812
Number of used sentences in train = 3226
Total loss for epoch 8: 7150.528462
validation loss after epoch 8 : 2044.616981
	Epoch 9....
Epoch has taken 0:03:59.545101
Number of used sentences in train = 3226
Total loss for epoch 9: 6910.776380
validation loss after epoch 9 : 2164.493362
	Epoch 10....
Epoch has taken 0:03:59.738235
Number of used sentences in train = 3226
Total loss for epoch 10: 6859.138729
validation loss after epoch 10 : 2211.391680
	Epoch 11....
Epoch has taken 0:03:57.972215
Number of used sentences in train = 3226
Total loss for epoch 11: 6722.600844
validation loss after epoch 11 : 2147.125204
	Epoch 12....
Epoch has taken 0:03:59.574293
Number of used sentences in train = 3226
Total loss for epoch 12: 6685.855470
validation loss after epoch 12 : 2280.456144
	Epoch 13....
Epoch has taken 0:03:58.157830
Number of used sentences in train = 3226
Total loss for epoch 13: 6598.633021
validation loss after epoch 13 : 2261.575544
	Epoch 14....
Epoch has taken 0:04:31.365376
Number of used sentences in train = 3226
Total loss for epoch 14: 6616.829649
validation loss after epoch 14 : 2384.816632
	TransitionClassifier(
  (p_embeddings): Embedding(13, 66)
  (w_embeddings): Embedding(17996, 221)
  (lstm): LSTM(287, 33, num_layers=2, dropout=0.37, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=55, bias=True)
  (linear2): Linear(in_features=55, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:24.419125
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2249.896250
	Epoch 1....
Epoch has taken 0:00:26.476155
Number of used sentences in train = 359
Total loss for epoch 1: 1244.795152
	Epoch 2....
Epoch has taken 0:00:26.804431
Number of used sentences in train = 359
Total loss for epoch 2: 941.298506
	Epoch 3....
Epoch has taken 0:00:25.690949
Number of used sentences in train = 359
Total loss for epoch 3: 861.167686
	Epoch 4....
Epoch has taken 0:00:25.774902
Number of used sentences in train = 359
Total loss for epoch 4: 782.898139
	Epoch 5....
Epoch has taken 0:00:25.709278
Number of used sentences in train = 359
Total loss for epoch 5: 766.898674
	Epoch 6....
Epoch has taken 0:00:26.085715
Number of used sentences in train = 359
Total loss for epoch 6: 740.645539
	Epoch 7....
Epoch has taken 0:00:25.579562
Number of used sentences in train = 359
Total loss for epoch 7: 737.877660
	Epoch 8....
Epoch has taken 0:00:25.747853
Number of used sentences in train = 359
Total loss for epoch 8: 716.435582
	Epoch 9....
Epoch has taken 0:00:26.939799
Number of used sentences in train = 359
Total loss for epoch 9: 717.806191
	Epoch 10....
Epoch has taken 0:00:26.901554
Number of used sentences in train = 359
Total loss for epoch 10: 698.572979
	Epoch 11....
Epoch has taken 0:00:25.722610
Number of used sentences in train = 359
Total loss for epoch 11: 690.741985
	Epoch 12....
Epoch has taken 0:00:25.792257
Number of used sentences in train = 359
Total loss for epoch 12: 695.263701
	Epoch 13....
Epoch has taken 0:00:25.723095
Number of used sentences in train = 359
Total loss for epoch 13: 690.080921
	Epoch 14....
Epoch has taken 0:00:25.743573
Number of used sentences in train = 359
Total loss for epoch 14: 699.409809
Epoch has taken 0:00:25.740081

==================================================================================================
	Training time : 1:07:29.106603
==================================================================================================
	Identification : 0.418

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 32, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 30, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 48, 'lstmDropout': 0.15, 'denseActivation': 'tanh', 'wordDim': 219, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1177, 219)
  (lstm): LSTM(249, 48, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=768, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11391.631799
validation loss after epoch 0 : 925.235371
	Epoch 1....
Epoch has taken 0:03:17.600776
Number of used sentences in train = 2811
Total loss for epoch 1: 7936.344467
validation loss after epoch 1 : 920.455552
	Epoch 2....
Epoch has taken 0:03:17.593533
Number of used sentences in train = 2811
Total loss for epoch 2: 7029.266028
validation loss after epoch 2 : 890.794496
	Epoch 3....
Epoch has taken 0:03:17.424662
Number of used sentences in train = 2811
Total loss for epoch 3: 6404.546590
validation loss after epoch 3 : 930.682785
	Epoch 4....
Epoch has taken 0:03:16.532335
Number of used sentences in train = 2811
Total loss for epoch 4: 5995.670357
validation loss after epoch 4 : 896.697779
	Epoch 5....
Epoch has taken 0:03:24.850334
Number of used sentences in train = 2811
Total loss for epoch 5: 5738.207262
validation loss after epoch 5 : 927.113063
	Epoch 6....
Epoch has taken 0:03:20.958018
Number of used sentences in train = 2811
Total loss for epoch 6: 5478.587084
validation loss after epoch 6 : 949.119946
	Epoch 7....
Epoch has taken 0:03:17.349801
Number of used sentences in train = 2811
Total loss for epoch 7: 5297.296039
validation loss after epoch 7 : 971.892619
	Epoch 8....
Epoch has taken 0:03:15.540761
Number of used sentences in train = 2811
Total loss for epoch 8: 5194.397370
validation loss after epoch 8 : 1009.950107
	Epoch 9....
Epoch has taken 0:03:15.383027
Number of used sentences in train = 2811
Total loss for epoch 9: 5094.217790
validation loss after epoch 9 : 1012.393012
	Epoch 10....
Epoch has taken 0:03:17.546340
Number of used sentences in train = 2811
Total loss for epoch 10: 4990.051007
validation loss after epoch 10 : 1014.941960
	Epoch 11....
Epoch has taken 0:03:17.197969
Number of used sentences in train = 2811
Total loss for epoch 11: 4851.272313
validation loss after epoch 11 : 1075.979092
	Epoch 12....
Epoch has taken 0:03:13.936963
Number of used sentences in train = 2811
Total loss for epoch 12: 4850.346084
validation loss after epoch 12 : 1057.406747
	Epoch 13....
Epoch has taken 0:03:17.679237
Number of used sentences in train = 2811
Total loss for epoch 13: 4774.323855
validation loss after epoch 13 : 1066.120254
	Epoch 14....
Epoch has taken 0:03:16.651726
Number of used sentences in train = 2811
Total loss for epoch 14: 4750.649831
validation loss after epoch 14 : 1093.742797
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1177, 219)
  (lstm): LSTM(249, 48, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=768, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:16.529166
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1360.771224
	Epoch 1....
Epoch has taken 0:00:18.809723
Number of used sentences in train = 313
Total loss for epoch 1: 797.472384
	Epoch 2....
Epoch has taken 0:00:18.810338
Number of used sentences in train = 313
Total loss for epoch 2: 636.253610
	Epoch 3....
Epoch has taken 0:00:18.820868
Number of used sentences in train = 313
Total loss for epoch 3: 578.629421
	Epoch 4....
Epoch has taken 0:00:18.924920
Number of used sentences in train = 313
Total loss for epoch 4: 557.179142
	Epoch 5....
Epoch has taken 0:00:18.947984
Number of used sentences in train = 313
Total loss for epoch 5: 527.181012
	Epoch 6....
Epoch has taken 0:00:18.940810
Number of used sentences in train = 313
Total loss for epoch 6: 523.709084
	Epoch 7....
Epoch has taken 0:00:18.921922
Number of used sentences in train = 313
Total loss for epoch 7: 515.808378
	Epoch 8....
Epoch has taken 0:00:18.935212
Number of used sentences in train = 313
Total loss for epoch 8: 530.237507
	Epoch 9....
Epoch has taken 0:00:18.935069
Number of used sentences in train = 313
Total loss for epoch 9: 525.426142
	Epoch 10....
Epoch has taken 0:00:18.934564
Number of used sentences in train = 313
Total loss for epoch 10: 516.093476
	Epoch 11....
Epoch has taken 0:00:18.935385
Number of used sentences in train = 313
Total loss for epoch 11: 512.958637
	Epoch 12....
Epoch has taken 0:00:18.949542
Number of used sentences in train = 313
Total loss for epoch 12: 514.348980
	Epoch 13....
Epoch has taken 0:00:18.927500
Number of used sentences in train = 313
Total loss for epoch 13: 511.701773
	Epoch 14....
Epoch has taken 0:00:18.941282
Number of used sentences in train = 313
Total loss for epoch 14: 509.959867
Epoch has taken 0:00:18.934097

==================================================================================================
	Training time : 0:54:06.966088
==================================================================================================
	Identification : 0.448

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1133, 219)
  (lstm): LSTM(249, 48, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=768, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9299.923402
validation loss after epoch 0 : 733.882774
	Epoch 1....
Epoch has taken 0:02:07.989135
Number of used sentences in train = 2074
Total loss for epoch 1: 5640.684822
validation loss after epoch 1 : 672.645370
	Epoch 2....
Epoch has taken 0:02:01.454204
Number of used sentences in train = 2074
Total loss for epoch 2: 4759.957811
validation loss after epoch 2 : 685.919862
	Epoch 3....
Epoch has taken 0:02:02.370646
Number of used sentences in train = 2074
Total loss for epoch 3: 4293.650801
validation loss after epoch 3 : 699.052085
	Epoch 4....
Epoch has taken 0:02:02.594660
Number of used sentences in train = 2074
Total loss for epoch 4: 3977.319118
validation loss after epoch 4 : 732.844145
	Epoch 5....
Epoch has taken 0:02:01.399768
Number of used sentences in train = 2074
Total loss for epoch 5: 3805.933020
validation loss after epoch 5 : 687.930637
	Epoch 6....
Epoch has taken 0:02:02.495594
Number of used sentences in train = 2074
Total loss for epoch 6: 3623.489183
validation loss after epoch 6 : 727.564940
	Epoch 7....
Epoch has taken 0:02:20.987450
Number of used sentences in train = 2074
Total loss for epoch 7: 3487.650062
validation loss after epoch 7 : 797.625840
	Epoch 8....
Epoch has taken 0:02:05.277101
Number of used sentences in train = 2074
Total loss for epoch 8: 3467.643074
validation loss after epoch 8 : 871.400091
	Epoch 9....
Epoch has taken 0:02:02.488433
Number of used sentences in train = 2074
Total loss for epoch 9: 3389.673692
validation loss after epoch 9 : 811.590284
	Epoch 10....
Epoch has taken 0:02:02.593527
Number of used sentences in train = 2074
Total loss for epoch 10: 3367.135432
validation loss after epoch 10 : 870.452904
	Epoch 11....
Epoch has taken 0:02:01.628708
Number of used sentences in train = 2074
Total loss for epoch 11: 3337.872479
validation loss after epoch 11 : 845.474083
	Epoch 12....
Epoch has taken 0:02:02.680076
Number of used sentences in train = 2074
Total loss for epoch 12: 3285.941140
validation loss after epoch 12 : 887.939751
	Epoch 13....
Epoch has taken 0:02:02.973410
Number of used sentences in train = 2074
Total loss for epoch 13: 3289.494258
validation loss after epoch 13 : 864.360622
	Epoch 14....
Epoch has taken 0:02:21.481143
Number of used sentences in train = 2074
Total loss for epoch 14: 3266.436492
validation loss after epoch 14 : 889.026262
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1133, 219)
  (lstm): LSTM(249, 48, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=768, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:02.705140
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1344.725338
	Epoch 1....
Epoch has taken 0:00:12.444414
Number of used sentences in train = 231
Total loss for epoch 1: 555.685891
	Epoch 2....
Epoch has taken 0:00:12.456054
Number of used sentences in train = 231
Total loss for epoch 2: 443.489523
	Epoch 3....
Epoch has taken 0:00:12.459857
Number of used sentences in train = 231
Total loss for epoch 3: 386.199923
	Epoch 4....
Epoch has taken 0:00:12.466558
Number of used sentences in train = 231
Total loss for epoch 4: 367.208255
	Epoch 5....
Epoch has taken 0:00:12.462159
Number of used sentences in train = 231
Total loss for epoch 5: 355.000127
	Epoch 6....
Epoch has taken 0:00:12.447813
Number of used sentences in train = 231
Total loss for epoch 6: 351.525604
	Epoch 7....
Epoch has taken 0:00:12.458591
Number of used sentences in train = 231
Total loss for epoch 7: 350.239282
	Epoch 8....
Epoch has taken 0:00:12.453016
Number of used sentences in train = 231
Total loss for epoch 8: 348.388962
	Epoch 9....
Epoch has taken 0:00:12.425799
Number of used sentences in train = 231
Total loss for epoch 9: 348.545545
	Epoch 10....
Epoch has taken 0:00:12.450626
Number of used sentences in train = 231
Total loss for epoch 10: 346.969008
	Epoch 11....
Epoch has taken 0:00:12.417595
Number of used sentences in train = 231
Total loss for epoch 11: 347.067302
	Epoch 12....
Epoch has taken 0:00:12.453731
Number of used sentences in train = 231
Total loss for epoch 12: 346.699028
	Epoch 13....
Epoch has taken 0:00:12.441899
Number of used sentences in train = 231
Total loss for epoch 13: 346.303173
	Epoch 14....
Epoch has taken 0:00:12.432209
Number of used sentences in train = 231
Total loss for epoch 14: 346.346173
Epoch has taken 0:00:12.419206

==================================================================================================
	Training time : 0:34:28.146686
==================================================================================================
	Identification : 0.232

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(1202, 219)
  (lstm): LSTM(249, 48, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=768, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14384.072244
validation loss after epoch 0 : 1298.631386
	Epoch 1....
Epoch has taken 0:04:35.987474
Number of used sentences in train = 3226
Total loss for epoch 1: 10655.936540
validation loss after epoch 1 : 1284.869701
	Epoch 2....
Epoch has taken 0:04:40.922609
Number of used sentences in train = 3226
Total loss for epoch 2: 9691.267540
validation loss after epoch 2 : 1278.745160
	Epoch 3....
Epoch has taken 0:04:39.019700
Number of used sentences in train = 3226
Total loss for epoch 3: 9009.867195
validation loss after epoch 3 : 1313.368854
	Epoch 4....
Epoch has taken 0:04:21.344008
Number of used sentences in train = 3226
Total loss for epoch 4: 8477.438856
validation loss after epoch 4 : 1399.832763
	Epoch 5....
Epoch has taken 0:04:21.781899
Number of used sentences in train = 3226
Total loss for epoch 5: 8028.743538
validation loss after epoch 5 : 1436.768392
	Epoch 6....
Epoch has taken 0:04:16.314329
Number of used sentences in train = 3226
Total loss for epoch 6: 7744.959957
validation loss after epoch 6 : 1512.972224
	Epoch 7....
Epoch has taken 0:04:15.742288
Number of used sentences in train = 3226
Total loss for epoch 7: 7517.274449
validation loss after epoch 7 : 1619.287414
	Epoch 8....
Epoch has taken 0:04:16.345584
Number of used sentences in train = 3226
Total loss for epoch 8: 7325.566098
validation loss after epoch 8 : 1599.964266
	Epoch 9....
Epoch has taken 0:04:15.641426
Number of used sentences in train = 3226
Total loss for epoch 9: 7199.826083
validation loss after epoch 9 : 1666.434898
	Epoch 10....
Epoch has taken 0:04:13.348667
Number of used sentences in train = 3226
Total loss for epoch 10: 7073.526871
validation loss after epoch 10 : 1615.849515
	Epoch 11....
Epoch has taken 0:03:57.217572
Number of used sentences in train = 3226
Total loss for epoch 11: 6876.474831
validation loss after epoch 11 : 1713.655768
	Epoch 12....
Epoch has taken 0:03:57.653578
Number of used sentences in train = 3226
Total loss for epoch 12: 6812.264168
validation loss after epoch 12 : 1795.727715
	Epoch 13....
Epoch has taken 0:04:09.112476
Number of used sentences in train = 3226
Total loss for epoch 13: 6782.197401
validation loss after epoch 13 : 1760.709648
	Epoch 14....
Epoch has taken 0:03:56.336245
Number of used sentences in train = 3226
Total loss for epoch 14: 6683.121568
validation loss after epoch 14 : 1839.533015
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(1202, 219)
  (lstm): LSTM(249, 48, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=768, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:57.233119
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1742.189012
	Epoch 1....
Epoch has taken 0:00:23.215423
Number of used sentences in train = 359
Total loss for epoch 1: 1146.228720
	Epoch 2....
Epoch has taken 0:00:23.216395
Number of used sentences in train = 359
Total loss for epoch 2: 962.759341
	Epoch 3....
Epoch has taken 0:00:23.225461
Number of used sentences in train = 359
Total loss for epoch 3: 861.252570
	Epoch 4....
Epoch has taken 0:00:23.212059
Number of used sentences in train = 359
Total loss for epoch 4: 787.487127
	Epoch 5....
Epoch has taken 0:00:23.200783
Number of used sentences in train = 359
Total loss for epoch 5: 774.270848
	Epoch 6....
Epoch has taken 0:00:23.222085
Number of used sentences in train = 359
Total loss for epoch 6: 729.416605
	Epoch 7....
Epoch has taken 0:00:23.193463
Number of used sentences in train = 359
Total loss for epoch 7: 723.545192
	Epoch 8....
Epoch has taken 0:00:23.236020
Number of used sentences in train = 359
Total loss for epoch 8: 725.990358
	Epoch 9....
Epoch has taken 0:00:23.210581
Number of used sentences in train = 359
Total loss for epoch 9: 698.714906
	Epoch 10....
Epoch has taken 0:00:23.205716
Number of used sentences in train = 359
Total loss for epoch 10: 688.056569
	Epoch 11....
Epoch has taken 0:00:26.639947
Number of used sentences in train = 359
Total loss for epoch 11: 701.988384
	Epoch 12....
Epoch has taken 0:00:26.602265
Number of used sentences in train = 359
Total loss for epoch 12: 689.553194
	Epoch 13....
Epoch has taken 0:00:23.243693
Number of used sentences in train = 359
Total loss for epoch 13: 705.409230
	Epoch 14....
Epoch has taken 0:00:23.261493
Number of used sentences in train = 359
Total loss for epoch 14: 691.124458
Epoch has taken 0:00:23.251988

==================================================================================================
	Training time : 1:09:49.789818
==================================================================================================
	Identification : 0.196

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 30, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 22, 'lstmDropout': 0.32, 'denseActivation': 'tanh', 'wordDim': 225, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5925, 225)
  (lstm): LSTM(255, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12855.655428
validation loss after epoch 0 : 1187.906611
	Epoch 1....
Epoch has taken 0:02:45.541943
Number of used sentences in train = 2811
Total loss for epoch 1: 8503.063034
validation loss after epoch 1 : 1206.946535
	Epoch 2....
Epoch has taken 0:02:45.598125
Number of used sentences in train = 2811
Total loss for epoch 2: 6879.365561
validation loss after epoch 2 : 1259.720337
	Epoch 3....
Epoch has taken 0:02:45.985132
Number of used sentences in train = 2811
Total loss for epoch 3: 6051.386723
validation loss after epoch 3 : 1373.610958
	Epoch 4....
Epoch has taken 0:02:47.158670
Number of used sentences in train = 2811
Total loss for epoch 4: 5547.767765
validation loss after epoch 4 : 1472.461423
	Epoch 5....
Epoch has taken 0:02:46.467926
Number of used sentences in train = 2811
Total loss for epoch 5: 5242.050762
validation loss after epoch 5 : 1511.602646
	Epoch 6....
Epoch has taken 0:03:03.585072
Number of used sentences in train = 2811
Total loss for epoch 6: 5040.174124
validation loss after epoch 6 : 1568.760115
	Epoch 7....
Epoch has taken 0:02:49.982440
Number of used sentences in train = 2811
Total loss for epoch 7: 4916.743174
validation loss after epoch 7 : 1643.296303
	Epoch 8....
Epoch has taken 0:02:50.088388
Number of used sentences in train = 2811
Total loss for epoch 8: 4827.455066
validation loss after epoch 8 : 1701.474573
	Epoch 9....
Epoch has taken 0:03:04.473734
Number of used sentences in train = 2811
Total loss for epoch 9: 4759.857663
validation loss after epoch 9 : 1731.963882
	Epoch 10....
Epoch has taken 0:02:46.920915
Number of used sentences in train = 2811
Total loss for epoch 10: 4721.359336
validation loss after epoch 10 : 1777.850163
	Epoch 11....
Epoch has taken 0:02:51.751367
Number of used sentences in train = 2811
Total loss for epoch 11: 4683.700198
validation loss after epoch 11 : 1801.232386
	Epoch 12....
Epoch has taken 0:02:46.026167
Number of used sentences in train = 2811
Total loss for epoch 12: 4658.204775
validation loss after epoch 12 : 1839.979807
	Epoch 13....
Epoch has taken 0:02:46.120938
Number of used sentences in train = 2811
Total loss for epoch 13: 4633.553044
validation loss after epoch 13 : 1866.148614
	Epoch 14....
Epoch has taken 0:03:09.503524
Number of used sentences in train = 2811
Total loss for epoch 14: 4608.524139
validation loss after epoch 14 : 1893.043156
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5925, 225)
  (lstm): LSTM(255, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.217055
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1472.738938
	Epoch 1....
Epoch has taken 0:00:17.461033
Number of used sentences in train = 313
Total loss for epoch 1: 809.633548
	Epoch 2....
Epoch has taken 0:00:17.423652
Number of used sentences in train = 313
Total loss for epoch 2: 646.420920
	Epoch 3....
Epoch has taken 0:00:17.419691
Number of used sentences in train = 313
Total loss for epoch 3: 569.278095
	Epoch 4....
Epoch has taken 0:00:17.429270
Number of used sentences in train = 313
Total loss for epoch 4: 536.377310
	Epoch 5....
Epoch has taken 0:00:17.425229
Number of used sentences in train = 313
Total loss for epoch 5: 525.395131
	Epoch 6....
Epoch has taken 0:00:17.430290
Number of used sentences in train = 313
Total loss for epoch 6: 519.360018
	Epoch 7....
Epoch has taken 0:00:17.411583
Number of used sentences in train = 313
Total loss for epoch 7: 514.819686
	Epoch 8....
Epoch has taken 0:00:17.423893
Number of used sentences in train = 313
Total loss for epoch 8: 512.603026
	Epoch 9....
Epoch has taken 0:00:17.420973
Number of used sentences in train = 313
Total loss for epoch 9: 510.607831
	Epoch 10....
Epoch has taken 0:00:17.416611
Number of used sentences in train = 313
Total loss for epoch 10: 508.184551
	Epoch 11....
Epoch has taken 0:00:17.400701
Number of used sentences in train = 313
Total loss for epoch 11: 506.248995
	Epoch 12....
Epoch has taken 0:00:17.562102
Number of used sentences in train = 313
Total loss for epoch 12: 504.475587
	Epoch 13....
Epoch has taken 0:00:17.524710
Number of used sentences in train = 313
Total loss for epoch 13: 503.149047
	Epoch 14....
Epoch has taken 0:00:17.544273
Number of used sentences in train = 313
Total loss for epoch 14: 502.688734
Epoch has taken 0:00:17.544159

==================================================================================================
	Training time : 0:47:07.788352
==================================================================================================
	Identification : 0.188

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5626, 225)
  (lstm): LSTM(255, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10660.069361
validation loss after epoch 0 : 1026.497437
	Epoch 1....
Epoch has taken 0:02:11.348757
Number of used sentences in train = 2074
Total loss for epoch 1: 6358.140073
validation loss after epoch 1 : 1075.240738
	Epoch 2....
Epoch has taken 0:02:11.306473
Number of used sentences in train = 2074
Total loss for epoch 2: 5064.480660
validation loss after epoch 2 : 1196.403396
	Epoch 3....
Epoch has taken 0:01:53.730854
Number of used sentences in train = 2074
Total loss for epoch 3: 4379.334484
validation loss after epoch 3 : 1242.094040
	Epoch 4....
Epoch has taken 0:01:53.507562
Number of used sentences in train = 2074
Total loss for epoch 4: 3986.466812
validation loss after epoch 4 : 1262.100590
	Epoch 5....
Epoch has taken 0:01:52.556278
Number of used sentences in train = 2074
Total loss for epoch 5: 3743.057115
validation loss after epoch 5 : 1337.560992
	Epoch 6....
Epoch has taken 0:01:53.529193
Number of used sentences in train = 2074
Total loss for epoch 6: 3597.122439
validation loss after epoch 6 : 1413.397946
	Epoch 7....
Epoch has taken 0:02:11.294930
Number of used sentences in train = 2074
Total loss for epoch 7: 3479.970887
validation loss after epoch 7 : 1414.717455
	Epoch 8....
Epoch has taken 0:01:53.568381
Number of used sentences in train = 2074
Total loss for epoch 8: 3402.384678
validation loss after epoch 8 : 1437.859689
	Epoch 9....
Epoch has taken 0:01:52.615991
Number of used sentences in train = 2074
Total loss for epoch 9: 3365.059856
validation loss after epoch 9 : 1472.213454
	Epoch 10....
Epoch has taken 0:01:54.077796
Number of used sentences in train = 2074
Total loss for epoch 10: 3305.441769
validation loss after epoch 10 : 1510.765002
	Epoch 11....
Epoch has taken 0:01:53.699554
Number of used sentences in train = 2074
Total loss for epoch 11: 3266.975416
validation loss after epoch 11 : 1539.259926
	Epoch 12....
Epoch has taken 0:02:11.356537
Number of used sentences in train = 2074
Total loss for epoch 12: 3250.392983
validation loss after epoch 12 : 1573.544504
	Epoch 13....
Epoch has taken 0:01:53.639186
Number of used sentences in train = 2074
Total loss for epoch 13: 3235.967723
validation loss after epoch 13 : 1599.141866
	Epoch 14....
Epoch has taken 0:01:52.561399
Number of used sentences in train = 2074
Total loss for epoch 14: 3226.563131
validation loss after epoch 14 : 1636.453436
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(5626, 225)
  (lstm): LSTM(255, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:52.568299
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1632.774233
	Epoch 1....
Epoch has taken 0:00:11.438355
Number of used sentences in train = 231
Total loss for epoch 1: 651.741228
	Epoch 2....
Epoch has taken 0:00:11.432643
Number of used sentences in train = 231
Total loss for epoch 2: 491.706029
	Epoch 3....
Epoch has taken 0:00:11.450652
Number of used sentences in train = 231
Total loss for epoch 3: 426.531832
	Epoch 4....
Epoch has taken 0:00:11.435294
Number of used sentences in train = 231
Total loss for epoch 4: 396.994151
	Epoch 5....
Epoch has taken 0:00:11.477336
Number of used sentences in train = 231
Total loss for epoch 5: 381.933124
	Epoch 6....
Epoch has taken 0:00:11.524894
Number of used sentences in train = 231
Total loss for epoch 6: 367.985019
	Epoch 7....
Epoch has taken 0:00:11.546358
Number of used sentences in train = 231
Total loss for epoch 7: 362.747259
	Epoch 8....
Epoch has taken 0:00:11.542923
Number of used sentences in train = 231
Total loss for epoch 8: 360.093471
	Epoch 9....
Epoch has taken 0:00:11.539237
Number of used sentences in train = 231
Total loss for epoch 9: 357.793268
	Epoch 10....
Epoch has taken 0:00:11.533444
Number of used sentences in train = 231
Total loss for epoch 10: 354.924129
	Epoch 11....
Epoch has taken 0:00:11.526684
Number of used sentences in train = 231
Total loss for epoch 11: 352.784027
	Epoch 12....
Epoch has taken 0:00:11.512509
Number of used sentences in train = 231
Total loss for epoch 12: 350.989307
	Epoch 13....
Epoch has taken 0:00:11.552137
Number of used sentences in train = 231
Total loss for epoch 13: 350.164353
	Epoch 14....
Epoch has taken 0:00:11.552935
Number of used sentences in train = 231
Total loss for epoch 14: 349.664803
Epoch has taken 0:00:11.567393

==================================================================================================
	Training time : 0:32:24.361850
==================================================================================================
	Identification : 0.252

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(6887, 225)
  (lstm): LSTM(255, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16605.114229
validation loss after epoch 0 : 1592.046302
	Epoch 1....
Epoch has taken 0:03:38.355423
Number of used sentences in train = 3226
Total loss for epoch 1: 12250.842171
validation loss after epoch 1 : 1557.502310
	Epoch 2....
Epoch has taken 0:03:38.569232
Number of used sentences in train = 3226
Total loss for epoch 2: 10593.591200
validation loss after epoch 2 : 1575.524889
	Epoch 3....
Epoch has taken 0:03:39.460413
Number of used sentences in train = 3226
Total loss for epoch 3: 9549.678284
validation loss after epoch 3 : 1669.779144
	Epoch 4....
Epoch has taken 0:03:41.148959
Number of used sentences in train = 3226
Total loss for epoch 4: 8848.342573
validation loss after epoch 4 : 1746.302686
	Epoch 5....
Epoch has taken 0:04:15.695156
Number of used sentences in train = 3226
Total loss for epoch 5: 8315.402620
validation loss after epoch 5 : 1825.741487
	Epoch 6....
Epoch has taken 0:03:41.222011
Number of used sentences in train = 3226
Total loss for epoch 6: 7861.521474
validation loss after epoch 6 : 1882.510245
	Epoch 7....
Epoch has taken 0:03:39.543461
Number of used sentences in train = 3226
Total loss for epoch 7: 7519.387328
validation loss after epoch 7 : 1994.113338
	Epoch 8....
Epoch has taken 0:03:42.039552
Number of used sentences in train = 3226
Total loss for epoch 8: 7256.881594
validation loss after epoch 8 : 2039.872905
	Epoch 9....
Epoch has taken 0:04:16.558206
Number of used sentences in train = 3226
Total loss for epoch 9: 7005.890695
validation loss after epoch 9 : 2191.006925
	Epoch 10....
Epoch has taken 0:03:42.222233
Number of used sentences in train = 3226
Total loss for epoch 10: 6853.525733
validation loss after epoch 10 : 2170.981186
	Epoch 11....
Epoch has taken 0:03:42.061533
Number of used sentences in train = 3226
Total loss for epoch 11: 6737.459529
validation loss after epoch 11 : 2244.827083
	Epoch 12....
Epoch has taken 0:03:39.949933
Number of used sentences in train = 3226
Total loss for epoch 12: 6620.594701
validation loss after epoch 12 : 2350.327840
	Epoch 13....
Epoch has taken 0:03:41.934982
Number of used sentences in train = 3226
Total loss for epoch 13: 6519.416951
validation loss after epoch 13 : 2395.530586
	Epoch 14....
Epoch has taken 0:04:16.646648
Number of used sentences in train = 3226
Total loss for epoch 14: 6477.798830
validation loss after epoch 14 : 2399.903731
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(6887, 225)
  (lstm): LSTM(255, 22, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:39.303304
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2215.496090
	Epoch 1....
Epoch has taken 0:00:21.525879
Number of used sentences in train = 359
Total loss for epoch 1: 1170.534800
	Epoch 2....
Epoch has taken 0:00:21.556984
Number of used sentences in train = 359
Total loss for epoch 2: 906.656919
	Epoch 3....
Epoch has taken 0:00:21.561276
Number of used sentences in train = 359
Total loss for epoch 3: 788.896125
	Epoch 4....
Epoch has taken 0:00:21.563251
Number of used sentences in train = 359
Total loss for epoch 4: 743.897346
	Epoch 5....
Epoch has taken 0:00:21.581279
Number of used sentences in train = 359
Total loss for epoch 5: 716.582587
	Epoch 6....
Epoch has taken 0:00:21.544616
Number of used sentences in train = 359
Total loss for epoch 6: 701.536342
	Epoch 7....
Epoch has taken 0:00:21.575187
Number of used sentences in train = 359
Total loss for epoch 7: 694.370842
	Epoch 8....
Epoch has taken 0:00:21.569825
Number of used sentences in train = 359
Total loss for epoch 8: 689.355781
	Epoch 9....
Epoch has taken 0:00:21.544895
Number of used sentences in train = 359
Total loss for epoch 9: 685.401793
	Epoch 10....
Epoch has taken 0:00:21.566381
Number of used sentences in train = 359
Total loss for epoch 10: 683.679385
	Epoch 11....
Epoch has taken 0:00:21.598805
Number of used sentences in train = 359
Total loss for epoch 11: 682.076744
	Epoch 12....
Epoch has taken 0:00:21.591617
Number of used sentences in train = 359
Total loss for epoch 12: 681.014006
	Epoch 13....
Epoch has taken 0:00:21.597101
Number of used sentences in train = 359
Total loss for epoch 13: 679.757257
	Epoch 14....
Epoch has taken 0:00:21.584311
Number of used sentences in train = 359
Total loss for epoch 14: 678.963491
Epoch has taken 0:00:21.602617

==================================================================================================
	Training time : 1:02:18.964440
==================================================================================================
	Identification : 0.419

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 103, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 26, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 43, 'lstmDropout': 0.28, 'denseActivation': 'tanh', 'wordDim': 113, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(9324, 113)
  (lstm): LSTM(139, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=103, bias=True)
  (linear2): Linear(in_features=103, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13610.473050
validation loss after epoch 0 : 1231.189466
	Epoch 1....
Epoch has taken 0:02:46.092601
Number of used sentences in train = 2811
Total loss for epoch 1: 8310.364131
validation loss after epoch 1 : 1134.884849
	Epoch 2....
Epoch has taken 0:02:46.111875
Number of used sentences in train = 2811
Total loss for epoch 2: 6373.442611
validation loss after epoch 2 : 1186.943046
	Epoch 3....
Epoch has taken 0:02:45.701889
Number of used sentences in train = 2811
Total loss for epoch 3: 5471.076169
validation loss after epoch 3 : 1276.092457
	Epoch 4....
Epoch has taken 0:02:45.498898
Number of used sentences in train = 2811
Total loss for epoch 4: 4985.157945
validation loss after epoch 4 : 1422.191203
	Epoch 5....
Epoch has taken 0:02:47.097373
Number of used sentences in train = 2811
Total loss for epoch 5: 4747.976012
validation loss after epoch 5 : 1501.915300
	Epoch 6....
Epoch has taken 0:02:47.197305
Number of used sentences in train = 2811
Total loss for epoch 6: 4632.656411
validation loss after epoch 6 : 1561.127215
	Epoch 7....
Epoch has taken 0:02:46.633473
Number of used sentences in train = 2811
Total loss for epoch 7: 4561.277649
validation loss after epoch 7 : 1640.386192
	Epoch 8....
Epoch has taken 0:02:45.717607
Number of used sentences in train = 2811
Total loss for epoch 8: 4523.806526
validation loss after epoch 8 : 1673.304940
	Epoch 9....
Epoch has taken 0:02:47.286542
Number of used sentences in train = 2811
Total loss for epoch 9: 4511.482255
validation loss after epoch 9 : 1707.714999
	Epoch 10....
Epoch has taken 0:03:10.812073
Number of used sentences in train = 2811
Total loss for epoch 10: 4497.291824
validation loss after epoch 10 : 1757.985244
	Epoch 11....
Epoch has taken 0:02:47.209361
Number of used sentences in train = 2811
Total loss for epoch 11: 4491.418187
validation loss after epoch 11 : 1765.885041
	Epoch 12....
Epoch has taken 0:02:45.302823
Number of used sentences in train = 2811
Total loss for epoch 12: 4488.311563
validation loss after epoch 12 : 1803.371806
	Epoch 13....
Epoch has taken 0:02:45.742087
Number of used sentences in train = 2811
Total loss for epoch 13: 4483.703494
validation loss after epoch 13 : 1804.848360
	Epoch 14....
Epoch has taken 0:02:47.195450
Number of used sentences in train = 2811
Total loss for epoch 14: 4483.268465
validation loss after epoch 14 : 1823.742773
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(9324, 113)
  (lstm): LSTM(139, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=103, bias=True)
  (linear2): Linear(in_features=103, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:13.519004
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1847.073000
	Epoch 1....
Epoch has taken 0:00:17.740759
Number of used sentences in train = 313
Total loss for epoch 1: 810.766865
	Epoch 2....
Epoch has taken 0:00:17.745910
Number of used sentences in train = 313
Total loss for epoch 2: 610.878819
	Epoch 3....
Epoch has taken 0:00:17.733251
Number of used sentences in train = 313
Total loss for epoch 3: 547.314269
	Epoch 4....
Epoch has taken 0:00:17.740185
Number of used sentences in train = 313
Total loss for epoch 4: 525.606301
	Epoch 5....
Epoch has taken 0:00:17.744645
Number of used sentences in train = 313
Total loss for epoch 5: 511.464483
	Epoch 6....
Epoch has taken 0:00:17.751288
Number of used sentences in train = 313
Total loss for epoch 6: 507.924287
	Epoch 7....
Epoch has taken 0:00:17.736280
Number of used sentences in train = 313
Total loss for epoch 7: 506.085919
	Epoch 8....
Epoch has taken 0:00:17.736883
Number of used sentences in train = 313
Total loss for epoch 8: 504.643931
	Epoch 9....
Epoch has taken 0:00:17.748431
Number of used sentences in train = 313
Total loss for epoch 9: 506.326041
	Epoch 10....
Epoch has taken 0:00:17.749899
Number of used sentences in train = 313
Total loss for epoch 10: 504.794226
	Epoch 11....
Epoch has taken 0:00:17.738863
Number of used sentences in train = 313
Total loss for epoch 11: 502.300483
	Epoch 12....
Epoch has taken 0:00:17.742141
Number of used sentences in train = 313
Total loss for epoch 12: 502.207027
	Epoch 13....
Epoch has taken 0:00:17.747938
Number of used sentences in train = 313
Total loss for epoch 13: 501.556769
	Epoch 14....
Epoch has taken 0:00:17.756021
Number of used sentences in train = 313
Total loss for epoch 14: 501.513810
Epoch has taken 0:00:17.757379

==================================================================================================
	Training time : 0:46:53.800489
==================================================================================================
	Identification : 0.432

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(7045, 113)
  (lstm): LSTM(139, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=103, bias=True)
  (linear2): Linear(in_features=103, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10978.704189
validation loss after epoch 0 : 962.711010
	Epoch 1....
Epoch has taken 0:01:54.426252
Number of used sentences in train = 2074
Total loss for epoch 1: 5879.447224
validation loss after epoch 1 : 1018.583366
	Epoch 2....
Epoch has taken 0:01:54.423043
Number of used sentences in train = 2074
Total loss for epoch 2: 4346.215459
validation loss after epoch 2 : 1112.602613
	Epoch 3....
Epoch has taken 0:02:12.156921
Number of used sentences in train = 2074
Total loss for epoch 3: 3702.649539
validation loss after epoch 3 : 1225.618554
	Epoch 4....
Epoch has taken 0:01:54.425363
Number of used sentences in train = 2074
Total loss for epoch 4: 3401.006848
validation loss after epoch 4 : 1309.692690
	Epoch 5....
Epoch has taken 0:01:53.434900
Number of used sentences in train = 2074
Total loss for epoch 5: 3285.470446
validation loss after epoch 5 : 1415.822410
	Epoch 6....
Epoch has taken 0:01:53.494238
Number of used sentences in train = 2074
Total loss for epoch 6: 3228.271979
validation loss after epoch 6 : 1429.426146
	Epoch 7....
Epoch has taken 0:02:00.851318
Number of used sentences in train = 2074
Total loss for epoch 7: 3204.400869
validation loss after epoch 7 : 1530.199088
	Epoch 8....
Epoch has taken 0:01:54.427467
Number of used sentences in train = 2074
Total loss for epoch 8: 3187.237414
validation loss after epoch 8 : 1513.897629
	Epoch 9....
Epoch has taken 0:01:53.186949
Number of used sentences in train = 2074
Total loss for epoch 9: 3173.727735
validation loss after epoch 9 : 1537.156002
	Epoch 10....
Epoch has taken 0:01:53.464080
Number of used sentences in train = 2074
Total loss for epoch 10: 3167.159654
validation loss after epoch 10 : 1560.030824
	Epoch 11....
Epoch has taken 0:01:54.410669
Number of used sentences in train = 2074
Total loss for epoch 11: 3163.274759
validation loss after epoch 11 : 1586.756823
	Epoch 12....
Epoch has taken 0:02:12.075940
Number of used sentences in train = 2074
Total loss for epoch 12: 3159.922204
validation loss after epoch 12 : 1598.542099
	Epoch 13....
Epoch has taken 0:01:54.173235
Number of used sentences in train = 2074
Total loss for epoch 13: 3157.304415
validation loss after epoch 13 : 1618.302971
	Epoch 14....
Epoch has taken 0:01:53.067117
Number of used sentences in train = 2074
Total loss for epoch 14: 3156.936920
validation loss after epoch 14 : 1629.955840
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(7045, 113)
  (lstm): LSTM(139, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=103, bias=True)
  (linear2): Linear(in_features=103, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.366893
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2019.831102
	Epoch 1....
Epoch has taken 0:00:11.676102
Number of used sentences in train = 231
Total loss for epoch 1: 651.938191
	Epoch 2....
Epoch has taken 0:00:11.682721
Number of used sentences in train = 231
Total loss for epoch 2: 457.621318
	Epoch 3....
Epoch has taken 0:00:11.673721
Number of used sentences in train = 231
Total loss for epoch 3: 390.041770
	Epoch 4....
Epoch has taken 0:00:11.685301
Number of used sentences in train = 231
Total loss for epoch 4: 357.345509
	Epoch 5....
Epoch has taken 0:00:11.688487
Number of used sentences in train = 231
Total loss for epoch 5: 351.992644
	Epoch 6....
Epoch has taken 0:00:11.685527
Number of used sentences in train = 231
Total loss for epoch 6: 349.970359
	Epoch 7....
Epoch has taken 0:00:11.691004
Number of used sentences in train = 231
Total loss for epoch 7: 348.589431
	Epoch 8....
Epoch has taken 0:00:11.681964
Number of used sentences in train = 231
Total loss for epoch 8: 347.693623
	Epoch 9....
Epoch has taken 0:00:11.683419
Number of used sentences in train = 231
Total loss for epoch 9: 347.109299
	Epoch 10....
Epoch has taken 0:00:11.695606
Number of used sentences in train = 231
Total loss for epoch 10: 346.723797
	Epoch 11....
Epoch has taken 0:00:11.669090
Number of used sentences in train = 231
Total loss for epoch 11: 346.392423
	Epoch 12....
Epoch has taken 0:00:11.677761
Number of used sentences in train = 231
Total loss for epoch 12: 346.158223
	Epoch 13....
Epoch has taken 0:00:11.685206
Number of used sentences in train = 231
Total loss for epoch 13: 345.921593
	Epoch 14....
Epoch has taken 0:00:11.687962
Number of used sentences in train = 231
Total loss for epoch 14: 345.780810
Epoch has taken 0:00:11.676088

==================================================================================================
	Training time : 0:32:06.968527
==================================================================================================
	Identification : 0.337

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(17995, 113)
  (lstm): LSTM(139, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=103, bias=True)
  (linear2): Linear(in_features=103, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20119.122184
validation loss after epoch 0 : 1628.995310
	Epoch 1....
Epoch has taken 0:04:17.075605
Number of used sentences in train = 3226
Total loss for epoch 1: 11507.413111
validation loss after epoch 1 : 1644.174608
	Epoch 2....
Epoch has taken 0:04:17.311846
Number of used sentences in train = 3226
Total loss for epoch 2: 8984.264279
validation loss after epoch 2 : 1732.512872
	Epoch 3....
Epoch has taken 0:03:42.913873
Number of used sentences in train = 3226
Total loss for epoch 3: 7539.250023
validation loss after epoch 3 : 1975.262013
	Epoch 4....
Epoch has taken 0:03:40.944937
Number of used sentences in train = 3226
Total loss for epoch 4: 6898.844578
validation loss after epoch 4 : 2071.755355
	Epoch 5....
Epoch has taken 0:03:42.595593
Number of used sentences in train = 3226
Total loss for epoch 5: 6548.328792
validation loss after epoch 5 : 2204.718383
	Epoch 6....
Epoch has taken 0:04:17.430760
Number of used sentences in train = 3226
Total loss for epoch 6: 6377.495861
validation loss after epoch 6 : 2302.868718
	Epoch 7....
Epoch has taken 0:03:42.543796
Number of used sentences in train = 3226
Total loss for epoch 7: 6326.529623
validation loss after epoch 7 : 2305.570295
	Epoch 8....
Epoch has taken 0:03:40.744226
Number of used sentences in train = 3226
Total loss for epoch 8: 6272.451700
validation loss after epoch 8 : 2369.009069
	Epoch 9....
Epoch has taken 0:03:41.216403
Number of used sentences in train = 3226
Total loss for epoch 9: 6238.308184
validation loss after epoch 9 : 2324.621180
	Epoch 10....
Epoch has taken 0:03:43.322129
Number of used sentences in train = 3226
Total loss for epoch 10: 6223.105100
validation loss after epoch 10 : 2442.334943
	Epoch 11....
Epoch has taken 0:04:17.621915
Number of used sentences in train = 3226
Total loss for epoch 11: 6205.020248
validation loss after epoch 11 : 2450.072716
	Epoch 12....
Epoch has taken 0:03:42.456224
Number of used sentences in train = 3226
Total loss for epoch 12: 6198.116565
validation loss after epoch 12 : 2440.544932
	Epoch 13....
Epoch has taken 0:03:40.948396
Number of used sentences in train = 3226
Total loss for epoch 13: 6189.047212
validation loss after epoch 13 : 2452.235394
	Epoch 14....
Epoch has taken 0:03:42.838083
Number of used sentences in train = 3226
Total loss for epoch 14: 6179.117342
validation loss after epoch 14 : 2525.923130
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(17995, 113)
  (lstm): LSTM(139, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=103, bias=True)
  (linear2): Linear(in_features=103, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:17.748418
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2955.198746
	Epoch 1....
Epoch has taken 0:00:21.565459
Number of used sentences in train = 359
Total loss for epoch 1: 1098.085460
	Epoch 2....
Epoch has taken 0:00:21.569605
Number of used sentences in train = 359
Total loss for epoch 2: 789.234218
	Epoch 3....
Epoch has taken 0:00:21.598173
Number of used sentences in train = 359
Total loss for epoch 3: 708.640068
	Epoch 4....
Epoch has taken 0:00:21.594059
Number of used sentences in train = 359
Total loss for epoch 4: 686.935963
	Epoch 5....
Epoch has taken 0:00:21.580966
Number of used sentences in train = 359
Total loss for epoch 5: 680.544749
	Epoch 6....
Epoch has taken 0:00:21.571415
Number of used sentences in train = 359
Total loss for epoch 6: 678.331000
	Epoch 7....
Epoch has taken 0:00:21.588899
Number of used sentences in train = 359
Total loss for epoch 7: 676.756078
	Epoch 8....
Epoch has taken 0:00:21.601324
Number of used sentences in train = 359
Total loss for epoch 8: 675.228203
	Epoch 9....
Epoch has taken 0:00:21.596091
Number of used sentences in train = 359
Total loss for epoch 9: 673.913333
	Epoch 10....
Epoch has taken 0:00:21.612573
Number of used sentences in train = 359
Total loss for epoch 10: 672.460045
	Epoch 11....
Epoch has taken 0:00:21.598515
Number of used sentences in train = 359
Total loss for epoch 11: 671.793525
	Epoch 12....
Epoch has taken 0:00:21.570761
Number of used sentences in train = 359
Total loss for epoch 12: 671.481204
	Epoch 13....
Epoch has taken 0:00:21.582063
Number of used sentences in train = 359
Total loss for epoch 13: 671.257980
	Epoch 14....
Epoch has taken 0:00:21.582141
Number of used sentences in train = 359
Total loss for epoch 14: 671.085491
Epoch has taken 0:00:21.596437

==================================================================================================
	Training time : 1:03:52.211929
==================================================================================================
	Identification : 0.31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 141, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 18, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 34, 'lstmDropout': 0.25, 'denseActivation': 'tanh', 'wordDim': 86, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5898, 86)
  (lstm): LSTM(104, 34, num_layers=2, dropout=0.25, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14522.556479
validation loss after epoch 0 : 1161.342100
	Epoch 1....
Epoch has taken 0:02:59.792844
Number of used sentences in train = 2811
Total loss for epoch 1: 8755.917440
validation loss after epoch 1 : 1101.005169
	Epoch 2....
Epoch has taken 0:03:05.554086
Number of used sentences in train = 2811
Total loss for epoch 2: 7228.343070
validation loss after epoch 2 : 1097.488338
	Epoch 3....
Epoch has taken 0:02:59.932371
Number of used sentences in train = 2811
Total loss for epoch 3: 6398.971820
validation loss after epoch 3 : 1156.896401
	Epoch 4....
Epoch has taken 0:02:59.580748
Number of used sentences in train = 2811
Total loss for epoch 4: 5878.896612
validation loss after epoch 4 : 1250.547198
	Epoch 5....
Epoch has taken 0:02:58.405215
Number of used sentences in train = 2811
Total loss for epoch 5: 5490.735917
validation loss after epoch 5 : 1255.872661
	Epoch 6....
Epoch has taken 0:02:59.979173
Number of used sentences in train = 2811
Total loss for epoch 6: 5341.684852
validation loss after epoch 6 : 1312.802691
	Epoch 7....
Epoch has taken 0:02:59.917409
Number of used sentences in train = 2811
Total loss for epoch 7: 5101.604746
validation loss after epoch 7 : 1286.691045
	Epoch 8....
Epoch has taken 0:02:59.682187
Number of used sentences in train = 2811
Total loss for epoch 8: 4977.358686
validation loss after epoch 8 : 1394.963995
	Epoch 9....
Epoch has taken 0:02:58.631005
Number of used sentences in train = 2811
Total loss for epoch 9: 4912.063170
validation loss after epoch 9 : 1447.746002
	Epoch 10....
Epoch has taken 0:02:59.936049
Number of used sentences in train = 2811
Total loss for epoch 10: 4808.637686
validation loss after epoch 10 : 1557.426838
	Epoch 11....
Epoch has taken 0:03:26.874433
Number of used sentences in train = 2811
Total loss for epoch 11: 4723.875393
validation loss after epoch 11 : 1570.171048
	Epoch 12....
Epoch has taken 0:03:00.002629
Number of used sentences in train = 2811
Total loss for epoch 12: 4681.358058
validation loss after epoch 12 : 1602.871183
	Epoch 13....
Epoch has taken 0:02:58.051076
Number of used sentences in train = 2811
Total loss for epoch 13: 4689.168356
validation loss after epoch 13 : 1583.228170
	Epoch 14....
Epoch has taken 0:02:58.487755
Number of used sentences in train = 2811
Total loss for epoch 14: 4663.994858
validation loss after epoch 14 : 1543.428735
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5898, 86)
  (lstm): LSTM(104, 34, num_layers=2, dropout=0.25, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:00.000328
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2508.527034
	Epoch 1....
Epoch has taken 0:00:21.788443
Number of used sentences in train = 313
Total loss for epoch 1: 786.205286
	Epoch 2....
Epoch has taken 0:00:21.785404
Number of used sentences in train = 313
Total loss for epoch 2: 637.599484
	Epoch 3....
Epoch has taken 0:00:21.789203
Number of used sentences in train = 313
Total loss for epoch 3: 589.831661
	Epoch 4....
Epoch has taken 0:00:21.799524
Number of used sentences in train = 313
Total loss for epoch 4: 545.762939
	Epoch 5....
Epoch has taken 0:00:21.771707
Number of used sentences in train = 313
Total loss for epoch 5: 539.953167
	Epoch 6....
Epoch has taken 0:00:19.052894
Number of used sentences in train = 313
Total loss for epoch 6: 524.801000
	Epoch 7....
Epoch has taken 0:00:19.038858
Number of used sentences in train = 313
Total loss for epoch 7: 517.988035
	Epoch 8....
Epoch has taken 0:00:19.066932
Number of used sentences in train = 313
Total loss for epoch 8: 521.530159
	Epoch 9....
Epoch has taken 0:00:19.034606
Number of used sentences in train = 313
Total loss for epoch 9: 518.669005
	Epoch 10....
Epoch has taken 0:00:19.045133
Number of used sentences in train = 313
Total loss for epoch 10: 509.962323
	Epoch 11....
Epoch has taken 0:00:19.027780
Number of used sentences in train = 313
Total loss for epoch 11: 508.913541
	Epoch 12....
Epoch has taken 0:00:19.016970
Number of used sentences in train = 313
Total loss for epoch 12: 509.996946
	Epoch 13....
Epoch has taken 0:00:19.027125
Number of used sentences in train = 313
Total loss for epoch 13: 517.968230
	Epoch 14....
Epoch has taken 0:00:19.054007
Number of used sentences in train = 313
Total loss for epoch 14: 505.202666
Epoch has taken 0:00:19.031454

==================================================================================================
	Training time : 0:50:24.664785
==================================================================================================
	Identification : 0.32

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5662, 86)
  (lstm): LSTM(104, 34, num_layers=2, dropout=0.25, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 12096.609702
validation loss after epoch 0 : 973.112787
	Epoch 1....
Epoch has taken 0:02:02.062782
Number of used sentences in train = 2074
Total loss for epoch 1: 6454.614422
validation loss after epoch 1 : 828.770121
	Epoch 2....
Epoch has taken 0:02:02.029320
Number of used sentences in train = 2074
Total loss for epoch 2: 5203.981806
validation loss after epoch 2 : 869.072865
	Epoch 3....
Epoch has taken 0:02:03.104210
Number of used sentences in train = 2074
Total loss for epoch 3: 4556.371870
validation loss after epoch 3 : 930.133594
	Epoch 4....
Epoch has taken 0:02:03.024170
Number of used sentences in train = 2074
Total loss for epoch 4: 4163.760782
validation loss after epoch 4 : 980.977165
	Epoch 5....
Epoch has taken 0:02:03.119026
Number of used sentences in train = 2074
Total loss for epoch 5: 3834.684839
validation loss after epoch 5 : 1047.762181
	Epoch 6....
Epoch has taken 0:02:01.852055
Number of used sentences in train = 2074
Total loss for epoch 6: 3720.085220
validation loss after epoch 6 : 1103.729435
	Epoch 7....
Epoch has taken 0:02:02.161865
Number of used sentences in train = 2074
Total loss for epoch 7: 3576.005545
validation loss after epoch 7 : 1201.843017
	Epoch 8....
Epoch has taken 0:02:03.005493
Number of used sentences in train = 2074
Total loss for epoch 8: 3480.342388
validation loss after epoch 8 : 1180.873018
	Epoch 9....
Epoch has taken 0:02:03.166370
Number of used sentences in train = 2074
Total loss for epoch 9: 3409.398093
validation loss after epoch 9 : 1328.798980
	Epoch 10....
Epoch has taken 0:02:01.944939
Number of used sentences in train = 2074
Total loss for epoch 10: 3364.989976
validation loss after epoch 10 : 1254.429807
	Epoch 11....
Epoch has taken 0:02:02.132339
Number of used sentences in train = 2074
Total loss for epoch 11: 3346.984473
validation loss after epoch 11 : 1199.153801
	Epoch 12....
Epoch has taken 0:02:03.005298
Number of used sentences in train = 2074
Total loss for epoch 12: 3341.133420
validation loss after epoch 12 : 1189.697336
	Epoch 13....
Epoch has taken 0:02:03.131029
Number of used sentences in train = 2074
Total loss for epoch 13: 3306.036769
validation loss after epoch 13 : 1249.615196
	Epoch 14....
Epoch has taken 0:02:03.354099
Number of used sentences in train = 2074
Total loss for epoch 14: 3301.544339
validation loss after epoch 14 : 1333.579206
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5662, 86)
  (lstm): LSTM(104, 34, num_layers=2, dropout=0.25, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:03.149656
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2316.448930
	Epoch 1....
Epoch has taken 0:00:12.448531
Number of used sentences in train = 231
Total loss for epoch 1: 638.603963
	Epoch 2....
Epoch has taken 0:00:12.457124
Number of used sentences in train = 231
Total loss for epoch 2: 479.342752
	Epoch 3....
Epoch has taken 0:00:12.450080
Number of used sentences in train = 231
Total loss for epoch 3: 402.712956
	Epoch 4....
Epoch has taken 0:00:12.460438
Number of used sentences in train = 231
Total loss for epoch 4: 369.747728
	Epoch 5....
Epoch has taken 0:00:12.458036
Number of used sentences in train = 231
Total loss for epoch 5: 365.748042
	Epoch 6....
Epoch has taken 0:00:12.463019
Number of used sentences in train = 231
Total loss for epoch 6: 363.805143
	Epoch 7....
Epoch has taken 0:00:12.460703
Number of used sentences in train = 231
Total loss for epoch 7: 355.527716
	Epoch 8....
Epoch has taken 0:00:12.459946
Number of used sentences in train = 231
Total loss for epoch 8: 351.590071
	Epoch 9....
Epoch has taken 0:00:12.459566
Number of used sentences in train = 231
Total loss for epoch 9: 350.261578
	Epoch 10....
Epoch has taken 0:00:12.435635
Number of used sentences in train = 231
Total loss for epoch 10: 354.154123
	Epoch 11....
Epoch has taken 0:00:12.485354
Number of used sentences in train = 231
Total loss for epoch 11: 347.761426
	Epoch 12....
Epoch has taken 0:00:12.487964
Number of used sentences in train = 231
Total loss for epoch 12: 350.347613
	Epoch 13....
Epoch has taken 0:00:12.500989
Number of used sentences in train = 231
Total loss for epoch 13: 347.082103
	Epoch 14....
Epoch has taken 0:00:12.489050
Number of used sentences in train = 231
Total loss for epoch 14: 347.425640
Epoch has taken 0:00:12.484748

==================================================================================================
	Training time : 0:33:47.583782
==================================================================================================
	Identification : 0.308

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(6881, 86)
  (lstm): LSTM(104, 34, num_layers=2, dropout=0.25, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17584.901209
validation loss after epoch 0 : 1502.496098
	Epoch 1....
Epoch has taken 0:03:59.250985
Number of used sentences in train = 3226
Total loss for epoch 1: 11591.865076
validation loss after epoch 1 : 1505.907602
	Epoch 2....
Epoch has taken 0:03:59.438471
Number of used sentences in train = 3226
Total loss for epoch 2: 10108.617999
validation loss after epoch 2 : 1485.362310
	Epoch 3....
Epoch has taken 0:03:59.521466
Number of used sentences in train = 3226
Total loss for epoch 3: 9028.197715
validation loss after epoch 3 : 1577.484127
	Epoch 4....
Epoch has taken 0:03:59.066820
Number of used sentences in train = 3226
Total loss for epoch 4: 8350.855210
validation loss after epoch 4 : 1720.029626
	Epoch 5....
Epoch has taken 0:03:59.563741
Number of used sentences in train = 3226
Total loss for epoch 5: 7817.633744
validation loss after epoch 5 : 1774.492825
	Epoch 6....
Epoch has taken 0:04:19.634339
Number of used sentences in train = 3226
Total loss for epoch 6: 7460.220366
validation loss after epoch 6 : 1981.466801
	Epoch 7....
Epoch has taken 0:04:04.902820
Number of used sentences in train = 3226
Total loss for epoch 7: 7171.771763
validation loss after epoch 7 : 2051.043802
	Epoch 8....
Epoch has taken 0:04:01.127467
Number of used sentences in train = 3226
Total loss for epoch 8: 6955.747360
validation loss after epoch 8 : 2089.786213
	Epoch 9....
Epoch has taken 0:04:07.564616
Number of used sentences in train = 3226
Total loss for epoch 9: 6840.181774
validation loss after epoch 9 : 2163.095345
	Epoch 10....
Epoch has taken 0:04:15.731102
Number of used sentences in train = 3226
Total loss for epoch 10: 6753.095011
validation loss after epoch 10 : 2121.083907
	Epoch 11....
Epoch has taken 0:04:12.481059
Number of used sentences in train = 3226
Total loss for epoch 11: 6572.669060
validation loss after epoch 11 : 2274.842108
	Epoch 12....
Epoch has taken 0:03:59.423591
Number of used sentences in train = 3226
Total loss for epoch 12: 6559.013980
validation loss after epoch 12 : 2269.863145
	Epoch 13....
Epoch has taken 0:06:09.827663
Number of used sentences in train = 3226
Total loss for epoch 13: 6459.909738
validation loss after epoch 13 : 2405.675689
	Epoch 14....
Epoch has taken 0:04:03.648929
Number of used sentences in train = 3226
Total loss for epoch 14: 6464.070658
validation loss after epoch 14 : 2319.149602
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(6881, 86)
  (lstm): LSTM(104, 34, num_layers=2, dropout=0.25, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:00.075768
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2639.484538
	Epoch 1....
Epoch has taken 0:00:23.327751
Number of used sentences in train = 359
Total loss for epoch 1: 1172.758708
	Epoch 2....
Epoch has taken 0:00:23.283550
Number of used sentences in train = 359
Total loss for epoch 2: 958.988629
	Epoch 3....
Epoch has taken 0:00:23.326318
Number of used sentences in train = 359
Total loss for epoch 3: 822.377011
	Epoch 4....
Epoch has taken 0:00:23.315587
Number of used sentences in train = 359
Total loss for epoch 4: 734.607858
	Epoch 5....
Epoch has taken 0:00:23.336446
Number of used sentences in train = 359
Total loss for epoch 5: 728.690067
	Epoch 6....
Epoch has taken 0:00:23.290135
Number of used sentences in train = 359
Total loss for epoch 6: 685.337167
	Epoch 7....
Epoch has taken 0:00:23.284504
Number of used sentences in train = 359
Total loss for epoch 7: 680.894558
	Epoch 8....
Epoch has taken 0:00:23.322822
Number of used sentences in train = 359
Total loss for epoch 8: 682.607331
	Epoch 9....
Epoch has taken 0:00:23.333070
Number of used sentences in train = 359
Total loss for epoch 9: 691.132446
	Epoch 10....
Epoch has taken 0:00:23.336187
Number of used sentences in train = 359
Total loss for epoch 10: 678.396799
	Epoch 11....
Epoch has taken 0:00:23.330559
Number of used sentences in train = 359
Total loss for epoch 11: 683.185861
	Epoch 12....
Epoch has taken 0:00:23.331988
Number of used sentences in train = 359
Total loss for epoch 12: 675.833169
	Epoch 13....
Epoch has taken 0:00:23.334850
Number of used sentences in train = 359
Total loss for epoch 13: 672.889460
	Epoch 14....
Epoch has taken 0:00:23.350557
Number of used sentences in train = 359
Total loss for epoch 14: 672.043915
Epoch has taken 0:00:23.314042

==================================================================================================
	Training time : 1:09:01.732606
==================================================================================================
	Identification : 0.133

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 41, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 69, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 74, 'lstmDropout': 0.26, 'denseActivation': 'tanh', 'wordDim': 58, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 69)
  (w_embeddings): Embedding(5898, 58)
  (lstm): LSTM(127, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13669.772128
validation loss after epoch 0 : 1118.102695
	Epoch 1....
Epoch has taken 0:03:13.526697
Number of used sentences in train = 2811
Total loss for epoch 1: 9758.113527
validation loss after epoch 1 : 1076.382677
	Epoch 2....
Epoch has taken 0:03:13.574693
Number of used sentences in train = 2811
Total loss for epoch 2: 8213.117899
validation loss after epoch 2 : 1012.803470
	Epoch 3....
Epoch has taken 0:02:46.860027
Number of used sentences in train = 2811
Total loss for epoch 3: 7266.645468
validation loss after epoch 3 : 1024.661772
	Epoch 4....
Epoch has taken 0:02:46.082142
Number of used sentences in train = 2811
Total loss for epoch 4: 6575.203982
validation loss after epoch 4 : 1038.442587
	Epoch 5....
Epoch has taken 0:02:47.242353
Number of used sentences in train = 2811
Total loss for epoch 5: 6033.430920
validation loss after epoch 5 : 1084.789092
	Epoch 6....
Epoch has taken 0:02:47.714612
Number of used sentences in train = 2811
Total loss for epoch 6: 5634.106454
validation loss after epoch 6 : 1119.924829
	Epoch 7....
Epoch has taken 0:03:13.635747
Number of used sentences in train = 2811
Total loss for epoch 7: 5366.432305
validation loss after epoch 7 : 1144.015654
	Epoch 8....
Epoch has taken 0:02:47.723722
Number of used sentences in train = 2811
Total loss for epoch 8: 5162.331819
validation loss after epoch 8 : 1178.262914
	Epoch 9....
Epoch has taken 0:02:47.170405
Number of used sentences in train = 2811
Total loss for epoch 9: 4972.517932
validation loss after epoch 9 : 1239.948873
	Epoch 10....
Epoch has taken 0:02:47.446812
Number of used sentences in train = 2811
Total loss for epoch 10: 4865.622665
validation loss after epoch 10 : 1265.238358
	Epoch 11....
Epoch has taken 0:03:13.493588
Number of used sentences in train = 2811
Total loss for epoch 11: 4762.882650
validation loss after epoch 11 : 1277.202342
	Epoch 12....
Epoch has taken 0:02:47.588851
Number of used sentences in train = 2811
Total loss for epoch 12: 4705.942980
validation loss after epoch 12 : 1307.026608
	Epoch 13....
Epoch has taken 0:02:45.962651
Number of used sentences in train = 2811
Total loss for epoch 13: 4654.744996
validation loss after epoch 13 : 1325.907142
	Epoch 14....
Epoch has taken 0:02:46.031812
Number of used sentences in train = 2811
Total loss for epoch 14: 4620.575957
validation loss after epoch 14 : 1338.861470
	TransitionClassifier(
  (p_embeddings): Embedding(18, 69)
  (w_embeddings): Embedding(5898, 58)
  (lstm): LSTM(127, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.412204
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1625.918772
	Epoch 1....
Epoch has taken 0:00:19.696845
Number of used sentences in train = 313
Total loss for epoch 1: 877.675294
	Epoch 2....
Epoch has taken 0:00:18.002362
Number of used sentences in train = 313
Total loss for epoch 2: 713.124013
	Epoch 3....
Epoch has taken 0:00:19.759121
Number of used sentences in train = 313
Total loss for epoch 3: 602.265575
	Epoch 4....
Epoch has taken 0:00:19.619706
Number of used sentences in train = 313
Total loss for epoch 4: 551.535191
	Epoch 5....
Epoch has taken 0:00:19.529511
Number of used sentences in train = 313
Total loss for epoch 5: 533.525461
	Epoch 6....
Epoch has taken 0:00:19.475169
Number of used sentences in train = 313
Total loss for epoch 6: 522.344342
	Epoch 7....
Epoch has taken 0:00:19.438175
Number of used sentences in train = 313
Total loss for epoch 7: 514.185157
	Epoch 8....
Epoch has taken 0:00:19.606821
Number of used sentences in train = 313
Total loss for epoch 8: 507.223730
	Epoch 9....
Epoch has taken 0:00:19.542903
Number of used sentences in train = 313
Total loss for epoch 9: 508.858742
	Epoch 10....
Epoch has taken 0:00:19.511037
Number of used sentences in train = 313
Total loss for epoch 10: 507.567867
	Epoch 11....
Epoch has taken 0:00:19.585588
Number of used sentences in train = 313
Total loss for epoch 11: 507.751888
	Epoch 12....
Epoch has taken 0:00:19.556874
Number of used sentences in train = 313
Total loss for epoch 12: 505.998389
	Epoch 13....
Epoch has taken 0:00:19.516266
Number of used sentences in train = 313
Total loss for epoch 13: 505.475932
	Epoch 14....
Epoch has taken 0:00:19.507908
Number of used sentences in train = 313
Total loss for epoch 14: 504.792439
Epoch has taken 0:00:19.601160

==================================================================================================
	Training time : 0:48:26.913622
==================================================================================================
	Identification : 0.458

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 69)
  (w_embeddings): Embedding(5633, 58)
  (lstm): LSTM(127, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10412.570661
validation loss after epoch 0 : 952.018621
	Epoch 1....
Epoch has taken 0:02:06.442256
Number of used sentences in train = 2074
Total loss for epoch 1: 6823.576798
validation loss after epoch 1 : 860.111866
	Epoch 2....
Epoch has taken 0:02:12.538851
Number of used sentences in train = 2074
Total loss for epoch 2: 5649.091386
validation loss after epoch 2 : 853.412263
	Epoch 3....
Epoch has taken 0:01:55.023294
Number of used sentences in train = 2074
Total loss for epoch 3: 4795.359360
validation loss after epoch 3 : 932.639031
	Epoch 4....
Epoch has taken 0:01:53.840971
Number of used sentences in train = 2074
Total loss for epoch 4: 4251.211990
validation loss after epoch 4 : 962.129555
	Epoch 5....
Epoch has taken 0:01:54.590226
Number of used sentences in train = 2074
Total loss for epoch 5: 3833.447642
validation loss after epoch 5 : 1035.867833
	Epoch 6....
Epoch has taken 0:01:54.865441
Number of used sentences in train = 2074
Total loss for epoch 6: 3572.642619
validation loss after epoch 6 : 1087.807339
	Epoch 7....
Epoch has taken 0:02:12.546662
Number of used sentences in train = 2074
Total loss for epoch 7: 3425.654616
validation loss after epoch 7 : 1060.593730
	Epoch 8....
Epoch has taken 0:01:53.773224
Number of used sentences in train = 2074
Total loss for epoch 8: 3314.738214
validation loss after epoch 8 : 1100.604910
	Epoch 9....
Epoch has taken 0:01:53.978370
Number of used sentences in train = 2074
Total loss for epoch 9: 3275.858698
validation loss after epoch 9 : 1140.566228
	Epoch 10....
Epoch has taken 0:01:54.594959
Number of used sentences in train = 2074
Total loss for epoch 10: 3248.801546
validation loss after epoch 10 : 1157.508086
	Epoch 11....
Epoch has taken 0:01:54.874136
Number of used sentences in train = 2074
Total loss for epoch 11: 3228.536816
validation loss after epoch 11 : 1192.249382
	Epoch 12....
Epoch has taken 0:02:12.615312
Number of used sentences in train = 2074
Total loss for epoch 12: 3210.024563
validation loss after epoch 12 : 1211.873498
	Epoch 13....
Epoch has taken 0:01:55.101253
Number of used sentences in train = 2074
Total loss for epoch 13: 3198.535936
validation loss after epoch 13 : 1224.728728
	Epoch 14....
Epoch has taken 0:01:53.893080
Number of used sentences in train = 2074
Total loss for epoch 14: 3191.402838
validation loss after epoch 14 : 1242.126187
	TransitionClassifier(
  (p_embeddings): Embedding(18, 69)
  (w_embeddings): Embedding(5633, 58)
  (lstm): LSTM(127, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.806096
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2679.298955
	Epoch 1....
Epoch has taken 0:00:11.576254
Number of used sentences in train = 231
Total loss for epoch 1: 657.873089
	Epoch 2....
Epoch has taken 0:00:11.592427
Number of used sentences in train = 231
Total loss for epoch 2: 488.612563
	Epoch 3....
Epoch has taken 0:00:11.558963
Number of used sentences in train = 231
Total loss for epoch 3: 428.154710
	Epoch 4....
Epoch has taken 0:00:11.584932
Number of used sentences in train = 231
Total loss for epoch 4: 389.879668
	Epoch 5....
Epoch has taken 0:00:11.583928
Number of used sentences in train = 231
Total loss for epoch 5: 368.707929
	Epoch 6....
Epoch has taken 0:00:11.588101
Number of used sentences in train = 231
Total loss for epoch 6: 361.586413
	Epoch 7....
Epoch has taken 0:00:11.575721
Number of used sentences in train = 231
Total loss for epoch 7: 354.972549
	Epoch 8....
Epoch has taken 0:00:11.586086
Number of used sentences in train = 231
Total loss for epoch 8: 353.380399
	Epoch 9....
Epoch has taken 0:00:11.675669
Number of used sentences in train = 231
Total loss for epoch 9: 352.651124
	Epoch 10....
Epoch has taken 0:00:11.676598
Number of used sentences in train = 231
Total loss for epoch 10: 351.944382
	Epoch 11....
Epoch has taken 0:00:11.683212
Number of used sentences in train = 231
Total loss for epoch 11: 349.983321
	Epoch 12....
Epoch has taken 0:00:11.665362
Number of used sentences in train = 231
Total loss for epoch 12: 349.016636
	Epoch 13....
Epoch has taken 0:00:11.688687
Number of used sentences in train = 231
Total loss for epoch 13: 348.651672
	Epoch 14....
Epoch has taken 0:00:11.680711
Number of used sentences in train = 231
Total loss for epoch 14: 348.073563
Epoch has taken 0:00:11.688789

==================================================================================================
	Training time : 0:32:37.231645
==================================================================================================
	Identification : 0.132

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 69)
  (w_embeddings): Embedding(6856, 58)
  (lstm): LSTM(127, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17352.339440
validation loss after epoch 0 : 1532.124390
	Epoch 1....
Epoch has taken 0:04:20.037097
Number of used sentences in train = 3226
Total loss for epoch 1: 12776.460778
validation loss after epoch 1 : 1509.055402
	Epoch 2....
Epoch has taken 0:04:11.104768
Number of used sentences in train = 3226
Total loss for epoch 2: 11201.146958
validation loss after epoch 2 : 1429.887790
	Epoch 3....
Epoch has taken 0:03:44.991715
Number of used sentences in train = 3226
Total loss for epoch 3: 10122.629463
validation loss after epoch 3 : 1494.775195
	Epoch 4....
Epoch has taken 0:03:43.479092
Number of used sentences in train = 3226
Total loss for epoch 4: 9272.216339
validation loss after epoch 4 : 1484.998237
	Epoch 5....
Epoch has taken 0:03:43.204333
Number of used sentences in train = 3226
Total loss for epoch 5: 8598.889735
validation loss after epoch 5 : 1544.035650
	Epoch 6....
Epoch has taken 0:03:45.433125
Number of used sentences in train = 3226
Total loss for epoch 6: 8041.505298
validation loss after epoch 6 : 1619.098578
	Epoch 7....
Epoch has taken 0:03:45.354682
Number of used sentences in train = 3226
Total loss for epoch 7: 7567.893368
validation loss after epoch 7 : 1739.283145
	Epoch 8....
Epoch has taken 0:03:45.082881
Number of used sentences in train = 3226
Total loss for epoch 8: 7167.022416
validation loss after epoch 8 : 1872.636687
	Epoch 9....
Epoch has taken 0:03:43.333625
Number of used sentences in train = 3226
Total loss for epoch 9: 6822.403599
validation loss after epoch 9 : 1948.315577
	Epoch 10....
Epoch has taken 0:03:43.104469
Number of used sentences in train = 3226
Total loss for epoch 10: 6573.035928
validation loss after epoch 10 : 2136.523036
	Epoch 11....
Epoch has taken 0:03:45.087227
Number of used sentences in train = 3226
Total loss for epoch 11: 6434.766092
validation loss after epoch 11 : 2203.020740
	Epoch 12....
Epoch has taken 0:03:45.066840
Number of used sentences in train = 3226
Total loss for epoch 12: 6314.524743
validation loss after epoch 12 : 2306.581550
	Epoch 13....
Epoch has taken 0:03:45.384067
Number of used sentences in train = 3226
Total loss for epoch 13: 6279.220647
validation loss after epoch 13 : 2353.677471
	Epoch 14....
Epoch has taken 0:03:43.255619
Number of used sentences in train = 3226
Total loss for epoch 14: 6244.748450
validation loss after epoch 14 : 2432.278217
	TransitionClassifier(
  (p_embeddings): Embedding(13, 69)
  (w_embeddings): Embedding(6856, 58)
  (lstm): LSTM(127, 74, bidirectional=True)
  (linear1): Linear(in_features=1184, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.055575
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2198.577865
	Epoch 1....
Epoch has taken 0:00:22.016578
Number of used sentences in train = 359
Total loss for epoch 1: 1233.414776
	Epoch 2....
Epoch has taken 0:00:22.008309
Number of used sentences in train = 359
Total loss for epoch 2: 989.992867
	Epoch 3....
Epoch has taken 0:00:21.990238
Number of used sentences in train = 359
Total loss for epoch 3: 833.940181
	Epoch 4....
Epoch has taken 0:00:21.956244
Number of used sentences in train = 359
Total loss for epoch 4: 744.266452
	Epoch 5....
Epoch has taken 0:00:21.987352
Number of used sentences in train = 359
Total loss for epoch 5: 709.014197
	Epoch 6....
Epoch has taken 0:00:21.959843
Number of used sentences in train = 359
Total loss for epoch 6: 694.497420
	Epoch 7....
Epoch has taken 0:00:25.290647
Number of used sentences in train = 359
Total loss for epoch 7: 691.059975
	Epoch 8....
Epoch has taken 0:00:25.289384
Number of used sentences in train = 359
Total loss for epoch 8: 689.108122
	Epoch 9....
Epoch has taken 0:00:25.284093
Number of used sentences in train = 359
Total loss for epoch 9: 688.128610
	Epoch 10....
Epoch has taken 0:00:25.277231
Number of used sentences in train = 359
Total loss for epoch 10: 686.626408
	Epoch 11....
Epoch has taken 0:00:25.275564
Number of used sentences in train = 359
Total loss for epoch 11: 685.504929
	Epoch 12....
Epoch has taken 0:00:25.283354
Number of used sentences in train = 359
Total loss for epoch 12: 682.726875
	Epoch 13....
Epoch has taken 0:00:25.294002
Number of used sentences in train = 359
Total loss for epoch 13: 679.244081
	Epoch 14....
Epoch has taken 0:00:25.274303
Number of used sentences in train = 359
Total loss for epoch 14: 678.429719
Epoch has taken 0:00:25.272080

==================================================================================================
	Training time : 1:03:07.088835
==================================================================================================
	Identification : 0.509

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 131, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 37, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 70, 'lstmDropout': 0.17, 'denseActivation': 'tanh', 'wordDim': 186, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(9325, 186)
  (lstm): LSTM(223, 70, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=131, bias=True)
  (linear2): Linear(in_features=131, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14069.278866
validation loss after epoch 0 : 1086.648047
	Epoch 1....
Epoch has taken 0:02:46.219979
Number of used sentences in train = 2811
Total loss for epoch 1: 8045.727580
validation loss after epoch 1 : 1045.976067
	Epoch 2....
Epoch has taken 0:02:46.283851
Number of used sentences in train = 2811
Total loss for epoch 2: 6178.789802
validation loss after epoch 2 : 1101.422324
	Epoch 3....
Epoch has taken 0:02:46.525800
Number of used sentences in train = 2811
Total loss for epoch 3: 5267.396767
validation loss after epoch 3 : 1248.288594
	Epoch 4....
Epoch has taken 0:02:47.937478
Number of used sentences in train = 2811
Total loss for epoch 4: 4851.451265
validation loss after epoch 4 : 1349.919355
	Epoch 5....
Epoch has taken 0:03:12.246407
Number of used sentences in train = 2811
Total loss for epoch 5: 4638.183061
validation loss after epoch 5 : 1427.550047
	Epoch 6....
Epoch has taken 0:02:48.842740
Number of used sentences in train = 2811
Total loss for epoch 6: 4557.749049
validation loss after epoch 6 : 1497.463378
	Epoch 7....
Epoch has taken 0:02:47.170027
Number of used sentences in train = 2811
Total loss for epoch 7: 4530.156603
validation loss after epoch 7 : 1522.893456
	Epoch 8....
Epoch has taken 0:02:47.245644
Number of used sentences in train = 2811
Total loss for epoch 8: 4510.048896
validation loss after epoch 8 : 1537.998723
	Epoch 9....
Epoch has taken 0:02:48.612743
Number of used sentences in train = 2811
Total loss for epoch 9: 4500.985736
validation loss after epoch 9 : 1572.054996
	Epoch 10....
Epoch has taken 0:02:48.892641
Number of used sentences in train = 2811
Total loss for epoch 10: 4494.162227
validation loss after epoch 10 : 1613.844484
	Epoch 11....
Epoch has taken 0:02:48.051279
Number of used sentences in train = 2811
Total loss for epoch 11: 4489.863077
validation loss after epoch 11 : 1624.877433
	Epoch 12....
Epoch has taken 0:02:46.583500
Number of used sentences in train = 2811
Total loss for epoch 12: 4486.417918
validation loss after epoch 12 : 1649.018826
	Epoch 13....
Epoch has taken 0:02:47.612723
Number of used sentences in train = 2811
Total loss for epoch 13: 4482.771449
validation loss after epoch 13 : 1651.378551
	Epoch 14....
Epoch has taken 0:02:48.251506
Number of used sentences in train = 2811
Total loss for epoch 14: 4480.517540
validation loss after epoch 14 : 1694.916282
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(9325, 186)
  (lstm): LSTM(223, 70, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=131, bias=True)
  (linear2): Linear(in_features=131, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:48.112281
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1799.720861
	Epoch 1....
Epoch has taken 0:00:17.616624
Number of used sentences in train = 313
Total loss for epoch 1: 749.866853
	Epoch 2....
Epoch has taken 0:00:17.649340
Number of used sentences in train = 313
Total loss for epoch 2: 572.199806
	Epoch 3....
Epoch has taken 0:00:17.679695
Number of used sentences in train = 313
Total loss for epoch 3: 527.395253
	Epoch 4....
Epoch has taken 0:00:17.681543
Number of used sentences in train = 313
Total loss for epoch 4: 514.421997
	Epoch 5....
Epoch has taken 0:00:17.649600
Number of used sentences in train = 313
Total loss for epoch 5: 512.651145
	Epoch 6....
Epoch has taken 0:00:17.640437
Number of used sentences in train = 313
Total loss for epoch 6: 509.734543
	Epoch 7....
Epoch has taken 0:00:19.386903
Number of used sentences in train = 313
Total loss for epoch 7: 508.441480
	Epoch 8....
Epoch has taken 0:00:19.696049
Number of used sentences in train = 313
Total loss for epoch 8: 508.063543
	Epoch 9....
Epoch has taken 0:00:19.686052
Number of used sentences in train = 313
Total loss for epoch 9: 507.320211
	Epoch 10....
Epoch has taken 0:00:19.667831
Number of used sentences in train = 313
Total loss for epoch 10: 506.551853
	Epoch 11....
Epoch has taken 0:00:19.675890
Number of used sentences in train = 313
Total loss for epoch 11: 505.735328
	Epoch 12....
Epoch has taken 0:00:19.678791
Number of used sentences in train = 313
Total loss for epoch 12: 505.436526
	Epoch 13....
Epoch has taken 0:00:19.686761
Number of used sentences in train = 313
Total loss for epoch 13: 503.957608
	Epoch 14....
Epoch has taken 0:00:19.698445
Number of used sentences in train = 313
Total loss for epoch 14: 503.944652
Epoch has taken 0:00:19.672461

==================================================================================================
	Training time : 0:47:01.883475
==================================================================================================
	Identification : 0.425

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(7105, 186)
  (lstm): LSTM(223, 70, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=131, bias=True)
  (linear2): Linear(in_features=131, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11241.690475
validation loss after epoch 0 : 962.563581
	Epoch 1....
Epoch has taken 0:02:07.028815
Number of used sentences in train = 2074
Total loss for epoch 1: 6079.861705
validation loss after epoch 1 : 926.973335
	Epoch 2....
Epoch has taken 0:02:07.067372
Number of used sentences in train = 2074
Total loss for epoch 2: 4474.717573
validation loss after epoch 2 : 1046.077039
	Epoch 3....
Epoch has taken 0:02:10.900163
Number of used sentences in train = 2074
Total loss for epoch 3: 3729.809475
validation loss after epoch 3 : 1140.094869
	Epoch 4....
Epoch has taken 0:02:05.906993
Number of used sentences in train = 2074
Total loss for epoch 4: 3446.107583
validation loss after epoch 4 : 1196.199190
	Epoch 5....
Epoch has taken 0:02:06.037993
Number of used sentences in train = 2074
Total loss for epoch 5: 3310.663962
validation loss after epoch 5 : 1276.218905
	Epoch 6....
Epoch has taken 0:02:06.999208
Number of used sentences in train = 2074
Total loss for epoch 6: 3246.629260
validation loss after epoch 6 : 1310.829093
	Epoch 7....
Epoch has taken 0:02:01.560262
Number of used sentences in train = 2074
Total loss for epoch 7: 3215.802125
validation loss after epoch 7 : 1325.571880
	Epoch 8....
Epoch has taken 0:01:54.799937
Number of used sentences in train = 2074
Total loss for epoch 8: 3193.805931
validation loss after epoch 8 : 1371.554524
	Epoch 9....
Epoch has taken 0:01:53.977479
Number of used sentences in train = 2074
Total loss for epoch 9: 3183.665115
validation loss after epoch 9 : 1400.340952
	Epoch 10....
Epoch has taken 0:01:54.531751
Number of used sentences in train = 2074
Total loss for epoch 10: 3175.685760
validation loss after epoch 10 : 1416.550438
	Epoch 11....
Epoch has taken 0:01:54.777892
Number of used sentences in train = 2074
Total loss for epoch 11: 3172.342972
validation loss after epoch 11 : 1441.435245
	Epoch 12....
Epoch has taken 0:01:54.639182
Number of used sentences in train = 2074
Total loss for epoch 12: 3167.487151
validation loss after epoch 12 : 1451.506802
	Epoch 13....
Epoch has taken 0:01:54.929320
Number of used sentences in train = 2074
Total loss for epoch 13: 3165.964179
validation loss after epoch 13 : 1464.579955
	Epoch 14....
Epoch has taken 0:01:53.846948
Number of used sentences in train = 2074
Total loss for epoch 14: 3164.066781
validation loss after epoch 14 : 1479.329935
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(7105, 186)
  (lstm): LSTM(223, 70, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=131, bias=True)
  (linear2): Linear(in_features=131, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.739355
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1747.745006
	Epoch 1....
Epoch has taken 0:00:11.719171
Number of used sentences in train = 231
Total loss for epoch 1: 710.019104
	Epoch 2....
Epoch has taken 0:00:11.701986
Number of used sentences in train = 231
Total loss for epoch 2: 428.598376
	Epoch 3....
Epoch has taken 0:00:11.727769
Number of used sentences in train = 231
Total loss for epoch 3: 364.330556
	Epoch 4....
Epoch has taken 0:00:11.720443
Number of used sentences in train = 231
Total loss for epoch 4: 354.595088
	Epoch 5....
Epoch has taken 0:00:11.710994
Number of used sentences in train = 231
Total loss for epoch 5: 351.048091
	Epoch 6....
Epoch has taken 0:00:11.702330
Number of used sentences in train = 231
Total loss for epoch 6: 349.125664
	Epoch 7....
Epoch has taken 0:00:13.490792
Number of used sentences in train = 231
Total loss for epoch 7: 347.932819
	Epoch 8....
Epoch has taken 0:00:13.486320
Number of used sentences in train = 231
Total loss for epoch 8: 347.322684
	Epoch 9....
Epoch has taken 0:00:13.486513
Number of used sentences in train = 231
Total loss for epoch 9: 346.889681
	Epoch 10....
Epoch has taken 0:00:13.488360
Number of used sentences in train = 231
Total loss for epoch 10: 346.559666
	Epoch 11....
Epoch has taken 0:00:12.321677
Number of used sentences in train = 231
Total loss for epoch 11: 346.199095
	Epoch 12....
Epoch has taken 0:00:11.743861
Number of used sentences in train = 231
Total loss for epoch 12: 346.060339
	Epoch 13....
Epoch has taken 0:00:11.745101
Number of used sentences in train = 231
Total loss for epoch 13: 345.814224
	Epoch 14....
Epoch has taken 0:00:11.743620
Number of used sentences in train = 231
Total loss for epoch 14: 345.571519
Epoch has taken 0:00:11.756996

==================================================================================================
	Training time : 0:33:05.642482
==================================================================================================
	Identification : 0.35

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 37)
  (w_embeddings): Embedding(17934, 186)
  (lstm): LSTM(223, 70, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=131, bias=True)
  (linear2): Linear(in_features=131, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18732.071846
validation loss after epoch 0 : 1629.239128
	Epoch 1....
Epoch has taken 0:03:41.602341
Number of used sentences in train = 3226
Total loss for epoch 1: 11445.034314
validation loss after epoch 1 : 1550.272850
	Epoch 2....
Epoch has taken 0:03:46.208704
Number of used sentences in train = 3226
Total loss for epoch 2: 8913.041234
validation loss after epoch 2 : 1741.840231
	Epoch 3....
Epoch has taken 0:04:06.278085
Number of used sentences in train = 3226
Total loss for epoch 3: 7503.672904
validation loss after epoch 3 : 1802.167975
	Epoch 4....
Epoch has taken 0:04:05.596625
Number of used sentences in train = 3226
Total loss for epoch 4: 6833.374471
validation loss after epoch 4 : 1927.465675
	Epoch 5....
Epoch has taken 0:04:18.393380
Number of used sentences in train = 3226
Total loss for epoch 5: 6504.662438
validation loss after epoch 5 : 2083.029567
	Epoch 6....
Epoch has taken 0:03:44.043966
Number of used sentences in train = 3226
Total loss for epoch 6: 6328.506727
validation loss after epoch 6 : 2249.778894
	Epoch 7....
Epoch has taken 0:03:42.120576
Number of used sentences in train = 3226
Total loss for epoch 7: 6271.333277
validation loss after epoch 7 : 2154.980998
	Epoch 8....
Epoch has taken 0:03:43.553103
Number of used sentences in train = 3226
Total loss for epoch 8: 6219.427384
validation loss after epoch 8 : 2291.954739
	Epoch 9....
Epoch has taken 0:03:43.852411
Number of used sentences in train = 3226
Total loss for epoch 9: 6188.711776
validation loss after epoch 9 : 2328.535825
	Epoch 10....
Epoch has taken 0:03:44.218296
Number of used sentences in train = 3226
Total loss for epoch 10: 6176.892386
validation loss after epoch 10 : 2350.599401
	Epoch 11....
Epoch has taken 0:03:41.897356
Number of used sentences in train = 3226
Total loss for epoch 11: 6164.973226
validation loss after epoch 11 : 2394.091961
	Epoch 12....
Epoch has taken 0:03:43.523682
Number of used sentences in train = 3226
Total loss for epoch 12: 6161.765032
validation loss after epoch 12 : 2418.799877
	Epoch 13....
Epoch has taken 0:03:52.728411
Number of used sentences in train = 3226
Total loss for epoch 13: 6152.223857
validation loss after epoch 13 : 2513.199595
	Epoch 14....
Epoch has taken 0:04:03.241628
Number of used sentences in train = 3226
Total loss for epoch 14: 6146.799835
validation loss after epoch 14 : 2464.538745
	TransitionClassifier(
  (p_embeddings): Embedding(13, 37)
  (w_embeddings): Embedding(17934, 186)
  (lstm): LSTM(223, 70, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=131, bias=True)
  (linear2): Linear(in_features=131, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:42.178557
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2731.645674
	Epoch 1....
Epoch has taken 0:00:21.862621
Number of used sentences in train = 359
Total loss for epoch 1: 1131.591599
	Epoch 2....
Epoch has taken 0:00:21.830129
Number of used sentences in train = 359
Total loss for epoch 2: 811.028770
	Epoch 3....
Epoch has taken 0:00:21.841908
Number of used sentences in train = 359
Total loss for epoch 3: 697.232990
	Epoch 4....
Epoch has taken 0:00:21.821910
Number of used sentences in train = 359
Total loss for epoch 4: 681.248816
	Epoch 5....
Epoch has taken 0:00:21.876625
Number of used sentences in train = 359
Total loss for epoch 5: 677.217171
	Epoch 6....
Epoch has taken 0:00:21.862184
Number of used sentences in train = 359
Total loss for epoch 6: 673.819428
	Epoch 7....
Epoch has taken 0:00:21.882982
Number of used sentences in train = 359
Total loss for epoch 7: 672.846984
	Epoch 8....
Epoch has taken 0:00:21.868184
Number of used sentences in train = 359
Total loss for epoch 8: 672.300142
	Epoch 9....
Epoch has taken 0:00:21.866572
Number of used sentences in train = 359
Total loss for epoch 9: 671.932659
	Epoch 10....
Epoch has taken 0:00:21.875452
Number of used sentences in train = 359
Total loss for epoch 10: 671.662162
	Epoch 11....
Epoch has taken 0:00:21.876133
Number of used sentences in train = 359
Total loss for epoch 11: 671.434917
	Epoch 12....
Epoch has taken 0:00:21.883066
Number of used sentences in train = 359
Total loss for epoch 12: 671.254068
	Epoch 13....
Epoch has taken 0:00:21.873373
Number of used sentences in train = 359
Total loss for epoch 13: 671.102749
	Epoch 14....
Epoch has taken 0:00:21.846259
Number of used sentences in train = 359
Total loss for epoch 14: 670.968260
Epoch has taken 0:00:21.853900

==================================================================================================
	Training time : 1:03:08.071919
==================================================================================================
	Identification : 0.313

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 30, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 37, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 43, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(9190, 63)
  (lstm): LSTM(100, 43, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14105.826009
validation loss after epoch 0 : 1143.649807
	Epoch 1....
Epoch has taken 0:02:59.097365
Number of used sentences in train = 2811
Total loss for epoch 1: 9597.648952
validation loss after epoch 1 : 1082.041093
	Epoch 2....
Epoch has taken 0:02:59.306020
Number of used sentences in train = 2811
Total loss for epoch 2: 7627.342921
validation loss after epoch 2 : 1056.317981
	Epoch 3....
Epoch has taken 0:02:58.829948
Number of used sentences in train = 2811
Total loss for epoch 3: 6403.919649
validation loss after epoch 3 : 1111.341033
	Epoch 4....
Epoch has taken 0:02:57.273604
Number of used sentences in train = 2811
Total loss for epoch 4: 5720.846377
validation loss after epoch 4 : 1170.776334
	Epoch 5....
Epoch has taken 0:02:59.035841
Number of used sentences in train = 2811
Total loss for epoch 5: 5280.383091
validation loss after epoch 5 : 1251.425014
	Epoch 6....
Epoch has taken 0:03:25.962796
Number of used sentences in train = 2811
Total loss for epoch 6: 5059.343589
validation loss after epoch 6 : 1330.004438
	Epoch 7....
Epoch has taken 0:02:58.668578
Number of used sentences in train = 2811
Total loss for epoch 7: 4908.481560
validation loss after epoch 7 : 1352.556082
	Epoch 8....
Epoch has taken 0:02:57.512230
Number of used sentences in train = 2811
Total loss for epoch 8: 4758.947688
validation loss after epoch 8 : 1403.778447
	Epoch 9....
Epoch has taken 0:03:24.331768
Number of used sentences in train = 2811
Total loss for epoch 9: 4713.384420
validation loss after epoch 9 : 1447.672725
	Epoch 10....
Epoch has taken 0:02:58.663349
Number of used sentences in train = 2811
Total loss for epoch 10: 4678.525403
validation loss after epoch 10 : 1518.534199
	Epoch 11....
Epoch has taken 0:02:57.344726
Number of used sentences in train = 2811
Total loss for epoch 11: 4625.677435
validation loss after epoch 11 : 1476.364324
	Epoch 12....
Epoch has taken 0:02:58.506871
Number of used sentences in train = 2811
Total loss for epoch 12: 4598.107016
validation loss after epoch 12 : 1515.121436
	Epoch 13....
Epoch has taken 0:02:58.933133
Number of used sentences in train = 2811
Total loss for epoch 13: 4555.333795
validation loss after epoch 13 : 1516.639701
	Epoch 14....
Epoch has taken 0:03:25.914113
Number of used sentences in train = 2811
Total loss for epoch 14: 4543.888015
validation loss after epoch 14 : 1521.421864
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(9190, 63)
  (lstm): LSTM(100, 43, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:58.818345
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1482.983281
	Epoch 1....
Epoch has taken 0:00:18.949348
Number of used sentences in train = 313
Total loss for epoch 1: 805.848511
	Epoch 2....
Epoch has taken 0:00:18.767670
Number of used sentences in train = 313
Total loss for epoch 2: 643.731037
	Epoch 3....
Epoch has taken 0:00:18.800506
Number of used sentences in train = 313
Total loss for epoch 3: 597.136776
	Epoch 4....
Epoch has taken 0:00:18.781382
Number of used sentences in train = 313
Total loss for epoch 4: 559.886798
	Epoch 5....
Epoch has taken 0:00:18.797072
Number of used sentences in train = 313
Total loss for epoch 5: 539.062591
	Epoch 6....
Epoch has taken 0:00:18.790307
Number of used sentences in train = 313
Total loss for epoch 6: 526.109434
	Epoch 7....
Epoch has taken 0:00:18.798057
Number of used sentences in train = 313
Total loss for epoch 7: 520.115579
	Epoch 8....
Epoch has taken 0:00:18.840471
Number of used sentences in train = 313
Total loss for epoch 8: 517.414022
	Epoch 9....
Epoch has taken 0:00:18.886378
Number of used sentences in train = 313
Total loss for epoch 9: 514.863582
	Epoch 10....
Epoch has taken 0:00:18.876963
Number of used sentences in train = 313
Total loss for epoch 10: 513.224293
	Epoch 11....
Epoch has taken 0:00:18.896110
Number of used sentences in train = 313
Total loss for epoch 11: 509.373941
	Epoch 12....
Epoch has taken 0:00:18.870147
Number of used sentences in train = 313
Total loss for epoch 12: 507.165921
	Epoch 13....
Epoch has taken 0:00:18.887827
Number of used sentences in train = 313
Total loss for epoch 13: 506.290710
	Epoch 14....
Epoch has taken 0:00:18.952018
Number of used sentences in train = 313
Total loss for epoch 14: 504.899975
Epoch has taken 0:00:18.963512

==================================================================================================
	Training time : 0:50:41.569015
==================================================================================================
	Identification : 0.37

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(7079, 63)
  (lstm): LSTM(100, 43, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11157.722314
validation loss after epoch 0 : 946.985249
	Epoch 1....
Epoch has taken 0:02:20.793058
Number of used sentences in train = 2074
Total loss for epoch 1: 6832.138547
validation loss after epoch 1 : 832.575162
	Epoch 2....
Epoch has taken 0:02:19.168425
Number of used sentences in train = 2074
Total loss for epoch 2: 5422.019070
validation loss after epoch 2 : 846.652713
	Epoch 3....
Epoch has taken 0:02:02.433877
Number of used sentences in train = 2074
Total loss for epoch 3: 4596.201926
validation loss after epoch 3 : 916.931623
	Epoch 4....
Epoch has taken 0:02:01.299939
Number of used sentences in train = 2074
Total loss for epoch 4: 4083.555089
validation loss after epoch 4 : 959.842640
	Epoch 5....
Epoch has taken 0:02:01.678494
Number of used sentences in train = 2074
Total loss for epoch 5: 3799.712138
validation loss after epoch 5 : 1051.753377
	Epoch 6....
Epoch has taken 0:02:02.017031
Number of used sentences in train = 2074
Total loss for epoch 6: 3625.692887
validation loss after epoch 6 : 1088.125694
	Epoch 7....
Epoch has taken 0:02:03.143714
Number of used sentences in train = 2074
Total loss for epoch 7: 3461.208608
validation loss after epoch 7 : 1052.574180
	Epoch 8....
Epoch has taken 0:02:03.101422
Number of used sentences in train = 2074
Total loss for epoch 8: 3402.678937
validation loss after epoch 8 : 1111.100468
	Epoch 9....
Epoch has taken 0:02:21.307640
Number of used sentences in train = 2074
Total loss for epoch 9: 3354.389808
validation loss after epoch 9 : 1131.169448
	Epoch 10....
Epoch has taken 0:02:03.037277
Number of used sentences in train = 2074
Total loss for epoch 10: 3308.970268
validation loss after epoch 10 : 1174.558892
	Epoch 11....
Epoch has taken 0:02:02.776470
Number of used sentences in train = 2074
Total loss for epoch 11: 3252.259904
validation loss after epoch 11 : 1179.759678
	Epoch 12....
Epoch has taken 0:02:02.063897
Number of used sentences in train = 2074
Total loss for epoch 12: 3242.561246
validation loss after epoch 12 : 1175.061539
	Epoch 13....
Epoch has taken 0:02:03.050660
Number of used sentences in train = 2074
Total loss for epoch 13: 3238.819529
validation loss after epoch 13 : 1254.475407
	Epoch 14....
Epoch has taken 0:02:02.850734
Number of used sentences in train = 2074
Total loss for epoch 14: 3221.819098
validation loss after epoch 14 : 1233.593783
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(7079, 63)
  (lstm): LSTM(100, 43, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:02.942955
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1332.186084
	Epoch 1....
Epoch has taken 0:00:12.530986
Number of used sentences in train = 231
Total loss for epoch 1: 683.064925
	Epoch 2....
Epoch has taken 0:00:12.511344
Number of used sentences in train = 231
Total loss for epoch 2: 487.219400
	Epoch 3....
Epoch has taken 0:00:12.517950
Number of used sentences in train = 231
Total loss for epoch 3: 404.022427
	Epoch 4....
Epoch has taken 0:00:12.519497
Number of used sentences in train = 231
Total loss for epoch 4: 375.447080
	Epoch 5....
Epoch has taken 0:00:12.512499
Number of used sentences in train = 231
Total loss for epoch 5: 358.686099
	Epoch 6....
Epoch has taken 0:00:12.537092
Number of used sentences in train = 231
Total loss for epoch 6: 352.747517
	Epoch 7....
Epoch has taken 0:00:12.521268
Number of used sentences in train = 231
Total loss for epoch 7: 349.422931
	Epoch 8....
Epoch has taken 0:00:12.511899
Number of used sentences in train = 231
Total loss for epoch 8: 350.069767
	Epoch 9....
Epoch has taken 0:00:12.529123
Number of used sentences in train = 231
Total loss for epoch 9: 352.897972
	Epoch 10....
Epoch has taken 0:00:12.515986
Number of used sentences in train = 231
Total loss for epoch 10: 348.261392
	Epoch 11....
Epoch has taken 0:00:12.522940
Number of used sentences in train = 231
Total loss for epoch 11: 346.484490
	Epoch 12....
Epoch has taken 0:00:12.509085
Number of used sentences in train = 231
Total loss for epoch 12: 346.286798
	Epoch 13....
Epoch has taken 0:00:12.522068
Number of used sentences in train = 231
Total loss for epoch 13: 346.188098
	Epoch 14....
Epoch has taken 0:00:12.498604
Number of used sentences in train = 231
Total loss for epoch 14: 345.949418
Epoch has taken 0:00:12.516112

==================================================================================================
	Training time : 0:34:39.791747
==================================================================================================
	Identification : 0.175

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 37)
  (w_embeddings): Embedding(18032, 63)
  (lstm): LSTM(100, 43, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 21901.332727
validation loss after epoch 0 : 1706.875898
	Epoch 1....
Epoch has taken 0:03:58.148068
Number of used sentences in train = 3226
Total loss for epoch 1: 13093.985934
validation loss after epoch 1 : 1623.860541
	Epoch 2....
Epoch has taken 0:03:58.425307
Number of used sentences in train = 3226
Total loss for epoch 2: 10466.412388
validation loss after epoch 2 : 1636.703239
	Epoch 3....
Epoch has taken 0:04:34.181169
Number of used sentences in train = 3226
Total loss for epoch 3: 8840.397283
validation loss after epoch 3 : 1837.002287
	Epoch 4....
Epoch has taken 0:03:58.381540
Number of used sentences in train = 3226
Total loss for epoch 4: 7773.684241
validation loss after epoch 4 : 2024.324744
	Epoch 5....
Epoch has taken 0:03:55.788638
Number of used sentences in train = 3226
Total loss for epoch 5: 7172.864751
validation loss after epoch 5 : 2168.264300
	Epoch 6....
Epoch has taken 0:03:58.620149
Number of used sentences in train = 3226
Total loss for epoch 6: 6743.171702
validation loss after epoch 6 : 2245.479662
	Epoch 7....
Epoch has taken 0:04:04.173110
Number of used sentences in train = 3226
Total loss for epoch 7: 6605.413989
validation loss after epoch 7 : 2384.711637
	Epoch 8....
Epoch has taken 0:03:56.460217
Number of used sentences in train = 3226
Total loss for epoch 8: 6453.097996
validation loss after epoch 8 : 2454.236047
	Epoch 9....
Epoch has taken 0:03:56.234747
Number of used sentences in train = 3226
Total loss for epoch 9: 6398.232383
validation loss after epoch 9 : 2514.540005
	Epoch 10....
Epoch has taken 0:04:01.420775
Number of used sentences in train = 3226
Total loss for epoch 10: 6339.483013
validation loss after epoch 10 : 2587.531231
	Epoch 11....
Epoch has taken 0:03:58.084766
Number of used sentences in train = 3226
Total loss for epoch 11: 6312.777585
validation loss after epoch 11 : 2484.725071
	Epoch 12....
Epoch has taken 0:04:29.349106
Number of used sentences in train = 3226
Total loss for epoch 12: 6282.346272
validation loss after epoch 12 : 2669.503324
	Epoch 13....
Epoch has taken 0:03:58.347058
Number of used sentences in train = 3226
Total loss for epoch 13: 6267.190636
validation loss after epoch 13 : 2545.869774
	Epoch 14....
Epoch has taken 0:03:55.976219
Number of used sentences in train = 3226
Total loss for epoch 14: 6243.443713
validation loss after epoch 14 : 2635.299521
	TransitionClassifier(
  (p_embeddings): Embedding(13, 37)
  (w_embeddings): Embedding(18032, 63)
  (lstm): LSTM(100, 43, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:56.735122
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2050.381785
	Epoch 1....
Epoch has taken 0:00:23.386169
Number of used sentences in train = 359
Total loss for epoch 1: 1165.689887
	Epoch 2....
Epoch has taken 0:00:23.373374
Number of used sentences in train = 359
Total loss for epoch 2: 865.820312
	Epoch 3....
Epoch has taken 0:00:23.379050
Number of used sentences in train = 359
Total loss for epoch 3: 761.545882
	Epoch 4....
Epoch has taken 0:00:23.384884
Number of used sentences in train = 359
Total loss for epoch 4: 720.939468
	Epoch 5....
Epoch has taken 0:00:24.042711
Number of used sentences in train = 359
Total loss for epoch 5: 701.856427
	Epoch 6....
Epoch has taken 0:00:26.789693
Number of used sentences in train = 359
Total loss for epoch 6: 700.579177
	Epoch 7....
Epoch has taken 0:00:26.793195
Number of used sentences in train = 359
Total loss for epoch 7: 678.207747
	Epoch 8....
Epoch has taken 0:00:26.805442
Number of used sentences in train = 359
Total loss for epoch 8: 675.989840
	Epoch 9....
Epoch has taken 0:00:26.797909
Number of used sentences in train = 359
Total loss for epoch 9: 680.394063
	Epoch 10....
Epoch has taken 0:00:26.784503
Number of used sentences in train = 359
Total loss for epoch 10: 672.145039
	Epoch 11....
Epoch has taken 0:00:26.760547
Number of used sentences in train = 359
Total loss for epoch 11: 671.899434
	Epoch 12....
Epoch has taken 0:00:26.769601
Number of used sentences in train = 359
Total loss for epoch 12: 671.328923
	Epoch 13....
Epoch has taken 0:00:26.780138
Number of used sentences in train = 359
Total loss for epoch 13: 677.374671
	Epoch 14....
Epoch has taken 0:00:26.791509
Number of used sentences in train = 359
Total loss for epoch 14: 671.337898
Epoch has taken 0:00:26.764156

==================================================================================================
	Training time : 1:07:06.403914
==================================================================================================
	Identification : 0.153

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 144, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 38, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 45, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 93, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(1882, 93)
  (lstm): LSTM(131, 45, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=144, bias=True)
  (linear2): Linear(in_features=144, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11088.607331
validation loss after epoch 0 : 845.327390
	Epoch 1....
Epoch has taken 0:03:00.023412
Number of used sentences in train = 2811
Total loss for epoch 1: 7206.051373
validation loss after epoch 1 : 817.953358
	Epoch 2....
Epoch has taken 0:03:00.054676
Number of used sentences in train = 2811
Total loss for epoch 2: 6375.714657
validation loss after epoch 2 : 816.080014
	Epoch 3....
Epoch has taken 0:03:16.325166
Number of used sentences in train = 2811
Total loss for epoch 3: 5835.821883
validation loss after epoch 3 : 851.341702
	Epoch 4....
Epoch has taken 0:03:13.557342
Number of used sentences in train = 2811
Total loss for epoch 4: 5466.374031
validation loss after epoch 4 : 890.769424
	Epoch 5....
Epoch has taken 0:03:15.770888
Number of used sentences in train = 2811
Total loss for epoch 5: 5174.806642
validation loss after epoch 5 : 930.005895
	Epoch 6....
Epoch has taken 0:02:59.197507
Number of used sentences in train = 2811
Total loss for epoch 6: 5018.705085
validation loss after epoch 6 : 966.276394
	Epoch 7....
Epoch has taken 0:02:59.574059
Number of used sentences in train = 2811
Total loss for epoch 7: 4891.027889
validation loss after epoch 7 : 965.901262
	Epoch 8....
Epoch has taken 0:03:26.522582
Number of used sentences in train = 2811
Total loss for epoch 8: 4822.431436
validation loss after epoch 8 : 998.893407
	Epoch 9....
Epoch has taken 0:02:58.331700
Number of used sentences in train = 2811
Total loss for epoch 9: 4764.161714
validation loss after epoch 9 : 964.891637
	Epoch 10....
Epoch has taken 0:02:58.610662
Number of used sentences in train = 2811
Total loss for epoch 10: 4685.825887
validation loss after epoch 10 : 1065.535115
	Epoch 11....
Epoch has taken 0:03:00.458625
Number of used sentences in train = 2811
Total loss for epoch 11: 4635.640651
validation loss after epoch 11 : 1084.662521
	Epoch 12....
Epoch has taken 0:03:27.284217
Number of used sentences in train = 2811
Total loss for epoch 12: 4625.294771
validation loss after epoch 12 : 1063.526372
	Epoch 13....
Epoch has taken 0:02:59.971798
Number of used sentences in train = 2811
Total loss for epoch 13: 4597.352960
validation loss after epoch 13 : 1102.445360
	Epoch 14....
Epoch has taken 0:02:58.815578
Number of used sentences in train = 2811
Total loss for epoch 14: 4596.010530
validation loss after epoch 14 : 1079.086259
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(1882, 93)
  (lstm): LSTM(131, 45, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=144, bias=True)
  (linear2): Linear(in_features=144, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:58.616641
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1521.785792
	Epoch 1....
Epoch has taken 0:00:19.103656
Number of used sentences in train = 313
Total loss for epoch 1: 703.778890
	Epoch 2....
Epoch has taken 0:00:19.125389
Number of used sentences in train = 313
Total loss for epoch 2: 606.314682
	Epoch 3....
Epoch has taken 0:00:19.125264
Number of used sentences in train = 313
Total loss for epoch 3: 557.860594
	Epoch 4....
Epoch has taken 0:00:19.130779
Number of used sentences in train = 313
Total loss for epoch 4: 531.116794
	Epoch 5....
Epoch has taken 0:00:19.122045
Number of used sentences in train = 313
Total loss for epoch 5: 516.455084
	Epoch 6....
Epoch has taken 0:00:19.116288
Number of used sentences in train = 313
Total loss for epoch 6: 506.890273
	Epoch 7....
Epoch has taken 0:00:19.102342
Number of used sentences in train = 313
Total loss for epoch 7: 505.658790
	Epoch 8....
Epoch has taken 0:00:19.132805
Number of used sentences in train = 313
Total loss for epoch 8: 504.095927
	Epoch 9....
Epoch has taken 0:00:19.127720
Number of used sentences in train = 313
Total loss for epoch 9: 504.001316
	Epoch 10....
Epoch has taken 0:00:19.098938
Number of used sentences in train = 313
Total loss for epoch 10: 505.119959
	Epoch 11....
Epoch has taken 0:00:19.120181
Number of used sentences in train = 313
Total loss for epoch 11: 503.020008
	Epoch 12....
Epoch has taken 0:00:19.102508
Number of used sentences in train = 313
Total loss for epoch 12: 502.477032
	Epoch 13....
Epoch has taken 0:00:19.093890
Number of used sentences in train = 313
Total loss for epoch 13: 502.645944
	Epoch 14....
Epoch has taken 0:00:19.133437
Number of used sentences in train = 313
Total loss for epoch 14: 503.558680
Epoch has taken 0:00:19.124059

==================================================================================================
	Training time : 0:51:20.381418
==================================================================================================
	Identification : 0.472

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(1680, 93)
  (lstm): LSTM(131, 45, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=144, bias=True)
  (linear2): Linear(in_features=144, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9546.505440
validation loss after epoch 0 : 654.353613
	Epoch 1....
Epoch has taken 0:02:03.215873
Number of used sentences in train = 2074
Total loss for epoch 1: 5145.519488
validation loss after epoch 1 : 577.045759
	Epoch 2....
Epoch has taken 0:02:03.297404
Number of used sentences in train = 2074
Total loss for epoch 2: 4403.989482
validation loss after epoch 2 : 599.092995
	Epoch 3....
Epoch has taken 0:02:02.409896
Number of used sentences in train = 2074
Total loss for epoch 3: 3946.920730
validation loss after epoch 3 : 628.979210
	Epoch 4....
Epoch has taken 0:02:03.279184
Number of used sentences in train = 2074
Total loss for epoch 4: 3695.068576
validation loss after epoch 4 : 676.456173
	Epoch 5....
Epoch has taken 0:02:20.251795
Number of used sentences in train = 2074
Total loss for epoch 5: 3478.709901
validation loss after epoch 5 : 696.507551
	Epoch 6....
Epoch has taken 0:02:03.612185
Number of used sentences in train = 2074
Total loss for epoch 6: 3394.131593
validation loss after epoch 6 : 702.430622
	Epoch 7....
Epoch has taken 0:02:05.151348
Number of used sentences in train = 2074
Total loss for epoch 7: 3302.119027
validation loss after epoch 7 : 739.454858
	Epoch 8....
Epoch has taken 0:02:09.777255
Number of used sentences in train = 2074
Total loss for epoch 8: 3269.810662
validation loss after epoch 8 : 776.294731
	Epoch 9....
Epoch has taken 0:02:05.622008
Number of used sentences in train = 2074
Total loss for epoch 9: 3266.062904
validation loss after epoch 9 : 770.060349
	Epoch 10....
Epoch has taken 0:02:06.659515
Number of used sentences in train = 2074
Total loss for epoch 10: 3246.331489
validation loss after epoch 10 : 764.309517
	Epoch 11....
Epoch has taken 0:02:12.290141
Number of used sentences in train = 2074
Total loss for epoch 11: 3236.393136
validation loss after epoch 11 : 778.921055
	Epoch 12....
Epoch has taken 0:02:13.071357
Number of used sentences in train = 2074
Total loss for epoch 12: 3211.247632
validation loss after epoch 12 : 780.939919
	Epoch 13....
Epoch has taken 0:02:09.316030
Number of used sentences in train = 2074
Total loss for epoch 13: 3195.586330
validation loss after epoch 13 : 796.415299
	Epoch 14....
Epoch has taken 0:02:11.063970
Number of used sentences in train = 2074
Total loss for epoch 14: 3194.311815
validation loss after epoch 14 : 770.345129
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(1680, 93)
  (lstm): LSTM(131, 45, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=144, bias=True)
  (linear2): Linear(in_features=144, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:12.595470
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1663.878309
	Epoch 1....
Epoch has taken 0:00:12.878128
Number of used sentences in train = 231
Total loss for epoch 1: 534.684848
	Epoch 2....
Epoch has taken 0:00:12.870038
Number of used sentences in train = 231
Total loss for epoch 2: 413.049491
	Epoch 3....
Epoch has taken 0:00:12.856347
Number of used sentences in train = 231
Total loss for epoch 3: 369.814633
	Epoch 4....
Epoch has taken 0:00:13.083964
Number of used sentences in train = 231
Total loss for epoch 4: 352.462922
	Epoch 5....
Epoch has taken 0:00:12.820503
Number of used sentences in train = 231
Total loss for epoch 5: 348.453293
	Epoch 6....
Epoch has taken 0:00:12.814536
Number of used sentences in train = 231
Total loss for epoch 6: 347.477685
	Epoch 7....
Epoch has taken 0:00:12.838510
Number of used sentences in train = 231
Total loss for epoch 7: 346.881688
	Epoch 8....
Epoch has taken 0:00:12.835434
Number of used sentences in train = 231
Total loss for epoch 8: 346.120063
	Epoch 9....
Epoch has taken 0:00:12.837722
Number of used sentences in train = 231
Total loss for epoch 9: 345.929431
	Epoch 10....
Epoch has taken 0:00:12.834388
Number of used sentences in train = 231
Total loss for epoch 10: 345.541176
	Epoch 11....
Epoch has taken 0:00:12.811328
Number of used sentences in train = 231
Total loss for epoch 11: 345.544442
	Epoch 12....
Epoch has taken 0:00:13.557306
Number of used sentences in train = 231
Total loss for epoch 12: 346.066576
	Epoch 13....
Epoch has taken 0:00:13.223724
Number of used sentences in train = 231
Total loss for epoch 13: 346.778805
	Epoch 14....
Epoch has taken 0:00:12.820617
Number of used sentences in train = 231
Total loss for epoch 14: 345.199838
Epoch has taken 0:00:12.803007

==================================================================================================
	Training time : 0:35:15.837934
==================================================================================================
	Identification : 0.401

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(3369, 93)
  (lstm): LSTM(131, 45, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=144, bias=True)
  (linear2): Linear(in_features=144, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12531.387010
validation loss after epoch 0 : 1040.137963
	Epoch 1....
Epoch has taken 0:04:18.081582
Number of used sentences in train = 3226
Total loss for epoch 1: 9330.575755
validation loss after epoch 1 : 1054.732754
	Epoch 2....
Epoch has taken 0:04:18.492707
Number of used sentences in train = 3226
Total loss for epoch 2: 8559.271986
validation loss after epoch 2 : 1036.743166
	Epoch 3....
Epoch has taken 0:04:19.042327
Number of used sentences in train = 3226
Total loss for epoch 3: 8010.033936
validation loss after epoch 3 : 1060.973187
	Epoch 4....
Epoch has taken 0:04:16.112052
Number of used sentences in train = 3226
Total loss for epoch 4: 7571.745734
validation loss after epoch 4 : 1100.255542
	Epoch 5....
Epoch has taken 0:04:16.468461
Number of used sentences in train = 3226
Total loss for epoch 5: 7321.558773
validation loss after epoch 5 : 1197.308018
	Epoch 6....
Epoch has taken 0:04:18.764017
Number of used sentences in train = 3226
Total loss for epoch 6: 7067.779564
validation loss after epoch 6 : 1222.131710
	Epoch 7....
Epoch has taken 0:04:56.119667
Number of used sentences in train = 3226
Total loss for epoch 7: 6933.793098
validation loss after epoch 7 : 1222.450960
	Epoch 8....
Epoch has taken 0:04:18.316874
Number of used sentences in train = 3226
Total loss for epoch 8: 6800.165360
validation loss after epoch 8 : 1322.644836
	Epoch 9....
Epoch has taken 0:04:17.889269
Number of used sentences in train = 3226
Total loss for epoch 9: 6669.399878
validation loss after epoch 9 : 1323.499469
	Epoch 10....
Epoch has taken 0:04:19.213136
Number of used sentences in train = 3226
Total loss for epoch 10: 6615.066221
validation loss after epoch 10 : 1420.070658
	Epoch 11....
Epoch has taken 0:04:14.049536
Number of used sentences in train = 3226
Total loss for epoch 11: 6510.577497
validation loss after epoch 11 : 1403.101506
	Epoch 12....
Epoch has taken 0:04:19.213756
Number of used sentences in train = 3226
Total loss for epoch 12: 6481.709584
validation loss after epoch 12 : 1473.812408
	Epoch 13....
Epoch has taken 0:04:16.850853
Number of used sentences in train = 3226
Total loss for epoch 13: 6420.222130
validation loss after epoch 13 : 1512.081577
	Epoch 14....
Epoch has taken 0:04:16.230704
Number of used sentences in train = 3226
Total loss for epoch 14: 6367.228972
validation loss after epoch 14 : 1518.865806
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(3369, 93)
  (lstm): LSTM(131, 45, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=720, out_features=144, bias=True)
  (linear2): Linear(in_features=144, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:57.821215
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 3460.623706
	Epoch 1....
Epoch has taken 0:00:25.625606
Number of used sentences in train = 359
Total loss for epoch 1: 967.196220
	Epoch 2....
Epoch has taken 0:00:25.508087
Number of used sentences in train = 359
Total loss for epoch 2: 839.378410
	Epoch 3....
Epoch has taken 0:00:25.531053
Number of used sentences in train = 359
Total loss for epoch 3: 774.948729
	Epoch 4....
Epoch has taken 0:00:25.628862
Number of used sentences in train = 359
Total loss for epoch 4: 732.332569
	Epoch 5....
Epoch has taken 0:00:25.611450
Number of used sentences in train = 359
Total loss for epoch 5: 721.269280
	Epoch 6....
Epoch has taken 0:00:25.377716
Number of used sentences in train = 359
Total loss for epoch 6: 705.057995
	Epoch 7....
Epoch has taken 0:00:25.330022
Number of used sentences in train = 359
Total loss for epoch 7: 703.433516
	Epoch 8....
Epoch has taken 0:00:25.171366
Number of used sentences in train = 359
Total loss for epoch 8: 687.876595
	Epoch 9....
Epoch has taken 0:00:25.255501
Number of used sentences in train = 359
Total loss for epoch 9: 682.315481
	Epoch 10....
Epoch has taken 0:00:25.364689
Number of used sentences in train = 359
Total loss for epoch 10: 684.126182
	Epoch 11....
Epoch has taken 0:00:25.112615
Number of used sentences in train = 359
Total loss for epoch 11: 681.529716
	Epoch 12....
Epoch has taken 0:00:25.159257
Number of used sentences in train = 359
Total loss for epoch 12: 676.030391
	Epoch 13....
Epoch has taken 0:00:25.137060
Number of used sentences in train = 359
Total loss for epoch 13: 674.025439
	Epoch 14....
Epoch has taken 0:00:25.158983
Number of used sentences in train = 359
Total loss for epoch 14: 672.663555
Epoch has taken 0:00:25.324303

==================================================================================================
	Training time : 1:12:03.634998
==================================================================================================
	Identification : 0.455

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 23, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 38, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 84, 'lstmDropout': 0.26, 'denseActivation': 'tanh', 'wordDim': 59, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(9285, 59)
  (lstm): LSTM(97, 84, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1344, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 18965.197361
validation loss after epoch 0 : 1336.131352
	Epoch 1....
Epoch has taken 0:03:12.736477
Number of used sentences in train = 2811
Total loss for epoch 1: 11337.420842
validation loss after epoch 1 : 1179.580568
	Epoch 2....
Epoch has taken 0:03:12.603818
Number of used sentences in train = 2811
Total loss for epoch 2: 9405.602397
validation loss after epoch 2 : 1097.496528
	Epoch 3....
Epoch has taken 0:03:12.794335
Number of used sentences in train = 2811
Total loss for epoch 3: 8118.197540
validation loss after epoch 3 : 1078.036748
	Epoch 4....
Epoch has taken 0:03:41.105347
Number of used sentences in train = 2811
Total loss for epoch 4: 7255.033623
validation loss after epoch 4 : 1100.260986
	Epoch 5....
Epoch has taken 0:03:13.213928
Number of used sentences in train = 2811
Total loss for epoch 5: 6534.569446
validation loss after epoch 5 : 1124.415522
	Epoch 6....
Epoch has taken 0:03:11.565744
Number of used sentences in train = 2811
Total loss for epoch 6: 6100.418533
validation loss after epoch 6 : 1187.554548
	Epoch 7....
Epoch has taken 0:03:11.597508
Number of used sentences in train = 2811
Total loss for epoch 7: 5811.465653
validation loss after epoch 7 : 1232.623224
	Epoch 8....
Epoch has taken 0:03:12.930094
Number of used sentences in train = 2811
Total loss for epoch 8: 5587.460445
validation loss after epoch 8 : 1294.352546
	Epoch 9....
Epoch has taken 0:03:11.355955
Number of used sentences in train = 2811
Total loss for epoch 9: 5436.852148
validation loss after epoch 9 : 1293.371895
	Epoch 10....
Epoch has taken 0:03:11.743233
Number of used sentences in train = 2811
Total loss for epoch 10: 5340.955628
validation loss after epoch 10 : 1348.999061
	Epoch 11....
Epoch has taken 0:03:13.747327
Number of used sentences in train = 2811
Total loss for epoch 11: 5173.456216
validation loss after epoch 11 : 1369.058614
	Epoch 12....
Epoch has taken 0:03:13.652099
Number of used sentences in train = 2811
Total loss for epoch 12: 5105.149298
validation loss after epoch 12 : 1384.269019
	Epoch 13....
Epoch has taken 0:03:42.420738
Number of used sentences in train = 2811
Total loss for epoch 13: 4990.660328
validation loss after epoch 13 : 1391.197392
	Epoch 14....
Epoch has taken 0:03:13.892125
Number of used sentences in train = 2811
Total loss for epoch 14: 4957.771763
validation loss after epoch 14 : 1367.094946
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(9285, 59)
  (lstm): LSTM(97, 84, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1344, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:13.463704
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1724.225563
	Epoch 1....
Epoch has taken 0:00:20.528216
Number of used sentences in train = 313
Total loss for epoch 1: 929.590973
	Epoch 2....
Epoch has taken 0:00:20.520447
Number of used sentences in train = 313
Total loss for epoch 2: 773.233369
	Epoch 3....
Epoch has taken 0:00:20.568838
Number of used sentences in train = 313
Total loss for epoch 3: 671.152272
	Epoch 4....
Epoch has taken 0:00:20.486286
Number of used sentences in train = 313
Total loss for epoch 4: 612.530996
	Epoch 5....
Epoch has taken 0:00:20.409914
Number of used sentences in train = 313
Total loss for epoch 5: 582.924203
	Epoch 6....
Epoch has taken 0:00:20.484762
Number of used sentences in train = 313
Total loss for epoch 6: 549.775599
	Epoch 7....
Epoch has taken 0:00:20.529604
Number of used sentences in train = 313
Total loss for epoch 7: 534.276226
	Epoch 8....
Epoch has taken 0:00:20.481944
Number of used sentences in train = 313
Total loss for epoch 8: 526.844429
	Epoch 9....
Epoch has taken 0:00:20.448808
Number of used sentences in train = 313
Total loss for epoch 9: 526.061118
	Epoch 10....
Epoch has taken 0:00:20.683204
Number of used sentences in train = 313
Total loss for epoch 10: 520.187073
	Epoch 11....
Epoch has taken 0:00:20.750183
Number of used sentences in train = 313
Total loss for epoch 11: 517.303873
	Epoch 12....
Epoch has taken 0:00:20.672930
Number of used sentences in train = 313
Total loss for epoch 12: 521.076905
	Epoch 13....
Epoch has taken 0:00:20.779266
Number of used sentences in train = 313
Total loss for epoch 13: 518.101444
	Epoch 14....
Epoch has taken 0:00:20.565405
Number of used sentences in train = 313
Total loss for epoch 14: 516.205558
Epoch has taken 0:00:20.566032

==================================================================================================
	Training time : 0:54:17.814692
==================================================================================================
	Identification : 0.464

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(7037, 59)
  (lstm): LSTM(97, 84, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1344, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 17625.306712
validation loss after epoch 0 : 1197.644949
	Epoch 1....
Epoch has taken 0:02:29.891207
Number of used sentences in train = 2074
Total loss for epoch 1: 8713.039472
validation loss after epoch 1 : 916.706651
	Epoch 2....
Epoch has taken 0:02:11.744912
Number of used sentences in train = 2074
Total loss for epoch 2: 6785.216297
validation loss after epoch 2 : 895.047506
	Epoch 3....
Epoch has taken 0:02:12.560886
Number of used sentences in train = 2074
Total loss for epoch 3: 5502.295291
validation loss after epoch 3 : 870.345526
	Epoch 4....
Epoch has taken 0:02:11.559957
Number of used sentences in train = 2074
Total loss for epoch 4: 4837.277507
validation loss after epoch 4 : 946.910601
	Epoch 5....
Epoch has taken 0:02:12.845755
Number of used sentences in train = 2074
Total loss for epoch 5: 4330.782531
validation loss after epoch 5 : 1034.640559
	Epoch 6....
Epoch has taken 0:02:30.282166
Number of used sentences in train = 2074
Total loss for epoch 6: 4094.225594
validation loss after epoch 6 : 1009.478777
	Epoch 7....
Epoch has taken 0:02:12.861663
Number of used sentences in train = 2074
Total loss for epoch 7: 3897.165076
validation loss after epoch 7 : 1098.888227
	Epoch 8....
Epoch has taken 0:02:10.655660
Number of used sentences in train = 2074
Total loss for epoch 8: 3753.471753
validation loss after epoch 8 : 1085.320886
	Epoch 9....
Epoch has taken 0:02:11.082155
Number of used sentences in train = 2074
Total loss for epoch 9: 3632.296099
validation loss after epoch 9 : 1191.743472
	Epoch 10....
Epoch has taken 0:02:12.860532
Number of used sentences in train = 2074
Total loss for epoch 10: 3583.488479
validation loss after epoch 10 : 1137.713532
	Epoch 11....
Epoch has taken 0:02:12.558613
Number of used sentences in train = 2074
Total loss for epoch 11: 3500.334392
validation loss after epoch 11 : 1155.970140
	Epoch 12....
Epoch has taken 0:02:12.503303
Number of used sentences in train = 2074
Total loss for epoch 12: 3457.308435
validation loss after epoch 12 : 1188.240872
	Epoch 13....
Epoch has taken 0:02:10.953356
Number of used sentences in train = 2074
Total loss for epoch 13: 3378.213224
validation loss after epoch 13 : 1296.729928
	Epoch 14....
Epoch has taken 0:02:11.541222
Number of used sentences in train = 2074
Total loss for epoch 14: 3362.680900
validation loss after epoch 14 : 1264.461465
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(7037, 59)
  (lstm): LSTM(97, 84, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1344, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:11.505298
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1726.837205
	Epoch 1....
Epoch has taken 0:00:13.738378
Number of used sentences in train = 231
Total loss for epoch 1: 869.688149
	Epoch 2....
Epoch has taken 0:00:13.649086
Number of used sentences in train = 231
Total loss for epoch 2: 606.632928
	Epoch 3....
Epoch has taken 0:00:13.393233
Number of used sentences in train = 231
Total loss for epoch 3: 502.486859
	Epoch 4....
Epoch has taken 0:00:13.558731
Number of used sentences in train = 231
Total loss for epoch 4: 436.344816
	Epoch 5....
Epoch has taken 0:00:13.687981
Number of used sentences in train = 231
Total loss for epoch 5: 390.677984
	Epoch 6....
Epoch has taken 0:00:13.611069
Number of used sentences in train = 231
Total loss for epoch 6: 382.529016
	Epoch 7....
Epoch has taken 0:00:13.736692
Number of used sentences in train = 231
Total loss for epoch 7: 373.599362
	Epoch 8....
Epoch has taken 0:00:13.498978
Number of used sentences in train = 231
Total loss for epoch 8: 368.915098
	Epoch 9....
Epoch has taken 0:00:13.690594
Number of used sentences in train = 231
Total loss for epoch 9: 366.238369
	Epoch 10....
Epoch has taken 0:00:13.741778
Number of used sentences in train = 231
Total loss for epoch 10: 360.055630
	Epoch 11....
Epoch has taken 0:00:13.540845
Number of used sentences in train = 231
Total loss for epoch 11: 361.452728
	Epoch 12....
Epoch has taken 0:00:13.820288
Number of used sentences in train = 231
Total loss for epoch 12: 358.178852
	Epoch 13....
Epoch has taken 0:00:13.466453
Number of used sentences in train = 231
Total loss for epoch 13: 359.869223
	Epoch 14....
Epoch has taken 0:00:13.734689
Number of used sentences in train = 231
Total loss for epoch 14: 363.456640
Epoch has taken 0:00:13.626406

==================================================================================================
	Training time : 0:37:00.251158
==================================================================================================
	Identification : 0.281

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(17981, 59)
  (lstm): LSTM(97, 84, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1344, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 23552.559584
validation loss after epoch 0 : 1764.567259
	Epoch 1....
Epoch has taken 0:04:18.324932
Number of used sentences in train = 3226
Total loss for epoch 1: 14408.314336
validation loss after epoch 1 : 1615.470171
	Epoch 2....
Epoch has taken 0:04:18.593664
Number of used sentences in train = 3226
Total loss for epoch 2: 12056.095597
validation loss after epoch 2 : 1705.523840
	Epoch 3....
Epoch has taken 0:04:16.162379
Number of used sentences in train = 3226
Total loss for epoch 3: 10391.088698
validation loss after epoch 3 : 1626.777987
	Epoch 4....
Epoch has taken 0:04:19.710579
Number of used sentences in train = 3226
Total loss for epoch 4: 9194.031342
validation loss after epoch 4 : 1707.601029
	Epoch 5....
Epoch has taken 0:04:57.472346
Number of used sentences in train = 3226
Total loss for epoch 5: 8321.071683
validation loss after epoch 5 : 1801.050503
	Epoch 6....
Epoch has taken 0:04:19.543230
Number of used sentences in train = 3226
Total loss for epoch 6: 7753.449667
validation loss after epoch 6 : 2040.212742
	Epoch 7....
Epoch has taken 0:04:17.098486
Number of used sentences in train = 3226
Total loss for epoch 7: 7334.000031
validation loss after epoch 7 : 2138.947182
	Epoch 8....
Epoch has taken 0:04:18.954734
Number of used sentences in train = 3226
Total loss for epoch 8: 7025.730816
validation loss after epoch 8 : 2159.867111
	Epoch 9....
Epoch has taken 0:04:22.608543
Number of used sentences in train = 3226
Total loss for epoch 9: 6750.587462
validation loss after epoch 9 : 2399.123940
	Epoch 10....
Epoch has taken 0:04:26.817580
Number of used sentences in train = 3226
Total loss for epoch 10: 6631.100059
validation loss after epoch 10 : 2350.077358
	Epoch 11....
Epoch has taken 0:04:24.533470
Number of used sentences in train = 3226
Total loss for epoch 11: 6543.433818
validation loss after epoch 11 : 2453.568774
	Epoch 12....
Epoch has taken 0:04:24.934579
Number of used sentences in train = 3226
Total loss for epoch 12: 6428.522540
validation loss after epoch 12 : 2528.674181
	Epoch 13....
Epoch has taken 0:04:25.556839
Number of used sentences in train = 3226
Total loss for epoch 13: 6390.013707
validation loss after epoch 13 : 2711.030777
	Epoch 14....
Epoch has taken 0:04:16.909256
Number of used sentences in train = 3226
Total loss for epoch 14: 6387.236883
validation loss after epoch 14 : 2582.723369
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(17981, 59)
  (lstm): LSTM(97, 84, num_layers=2, dropout=0.26, bidirectional=True)
  (linear1): Linear(in_features=1344, out_features=23, bias=True)
  (linear2): Linear(in_features=23, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:02.932680
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2634.509297
	Epoch 1....
Epoch has taken 0:00:26.076282
Number of used sentences in train = 359
Total loss for epoch 1: 1557.907113
	Epoch 2....
Epoch has taken 0:00:26.068471
Number of used sentences in train = 359
Total loss for epoch 2: 1176.547918
	Epoch 3....
Epoch has taken 0:00:26.057677
Number of used sentences in train = 359
Total loss for epoch 3: 985.757346
	Epoch 4....
Epoch has taken 0:00:26.050529
Number of used sentences in train = 359
Total loss for epoch 4: 884.001153
	Epoch 5....
Epoch has taken 0:00:25.302485
Number of used sentences in train = 359
Total loss for epoch 5: 805.723020
	Epoch 6....
Epoch has taken 0:00:23.508618
Number of used sentences in train = 359
Total loss for epoch 6: 767.213257
	Epoch 7....
Epoch has taken 0:00:23.496101
Number of used sentences in train = 359
Total loss for epoch 7: 730.023227
	Epoch 8....
Epoch has taken 0:00:23.511722
Number of used sentences in train = 359
Total loss for epoch 8: 722.158344
	Epoch 9....
Epoch has taken 0:00:23.490883
Number of used sentences in train = 359
Total loss for epoch 9: 711.722548
	Epoch 10....
Epoch has taken 0:00:23.522450
Number of used sentences in train = 359
Total loss for epoch 10: 696.465238
	Epoch 11....
Epoch has taken 0:00:23.566084
Number of used sentences in train = 359
Total loss for epoch 11: 695.335757
	Epoch 12....
Epoch has taken 0:00:23.579418
Number of used sentences in train = 359
Total loss for epoch 12: 696.244344
	Epoch 13....
Epoch has taken 0:00:23.524621
Number of used sentences in train = 359
Total loss for epoch 13: 699.349044
	Epoch 14....
Epoch has taken 0:00:23.545670
Number of used sentences in train = 359
Total loss for epoch 14: 690.761110
Epoch has taken 0:00:23.535615

==================================================================================================
	Training time : 1:11:35.674122
==================================================================================================
	Identification : 0.462

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 94, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 19, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 117, 'lstmDropout': 0.32, 'denseActivation': 'tanh', 'wordDim': 220, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5909, 220)
  (lstm): LSTM(239, 117, num_layers=2, dropout=0.32, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=94, bias=True)
  (linear2): Linear(in_features=94, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 19315.050420
validation loss after epoch 0 : 1374.495739
	Epoch 1....
Epoch has taken 0:03:00.656551
Number of used sentences in train = 2811
Total loss for epoch 1: 11433.646530
validation loss after epoch 1 : 1129.101954
	Epoch 2....
Epoch has taken 0:03:03.471600
Number of used sentences in train = 2811
Total loss for epoch 2: 8974.038649
validation loss after epoch 2 : 1099.461238
	Epoch 3....
Epoch has taken 0:03:00.858699
Number of used sentences in train = 2811
Total loss for epoch 3: 7471.587618
validation loss after epoch 3 : 1112.093482
	Epoch 4....
Epoch has taken 0:02:59.127893
Number of used sentences in train = 2811
Total loss for epoch 4: 6596.849064
validation loss after epoch 4 : 1125.833376
	Epoch 5....
Epoch has taken 0:02:59.635613
Number of used sentences in train = 2811
Total loss for epoch 5: 5953.301898
validation loss after epoch 5 : 1234.436783
	Epoch 6....
Epoch has taken 0:03:00.982571
Number of used sentences in train = 2811
Total loss for epoch 6: 5458.562077
validation loss after epoch 6 : 1207.766920
	Epoch 7....
Epoch has taken 0:03:00.723794
Number of used sentences in train = 2811
Total loss for epoch 7: 5198.557513
validation loss after epoch 7 : 1359.786995
	Epoch 8....
Epoch has taken 0:03:16.515211
Number of used sentences in train = 2811
Total loss for epoch 8: 5085.626788
validation loss after epoch 8 : 1316.711908
	Epoch 9....
Epoch has taken 0:02:59.539131
Number of used sentences in train = 2811
Total loss for epoch 9: 4927.567141
validation loss after epoch 9 : 1370.499495
	Epoch 10....
Epoch has taken 0:03:00.777717
Number of used sentences in train = 2811
Total loss for epoch 10: 4815.765091
validation loss after epoch 10 : 1503.790439
	Epoch 11....
Epoch has taken 0:03:00.657793
Number of used sentences in train = 2811
Total loss for epoch 11: 4733.347478
validation loss after epoch 11 : 1600.312710
	Epoch 12....
Epoch has taken 0:03:01.071376
Number of used sentences in train = 2811
Total loss for epoch 12: 4688.524658
validation loss after epoch 12 : 1484.901260
	Epoch 13....
Epoch has taken 0:02:59.091009
Number of used sentences in train = 2811
Total loss for epoch 13: 4677.424026
validation loss after epoch 13 : 1514.390649
	Epoch 14....
Epoch has taken 0:02:59.556517
Number of used sentences in train = 2811
Total loss for epoch 14: 4636.565704
validation loss after epoch 14 : 1583.229496
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5909, 220)
  (lstm): LSTM(239, 117, num_layers=2, dropout=0.32, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=94, bias=True)
  (linear2): Linear(in_features=94, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:00.835411
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1950.359864
	Epoch 1....
Epoch has taken 0:00:19.186923
Number of used sentences in train = 313
Total loss for epoch 1: 946.241662
	Epoch 2....
Epoch has taken 0:00:19.179876
Number of used sentences in train = 313
Total loss for epoch 2: 749.660278
	Epoch 3....
Epoch has taken 0:00:19.178017
Number of used sentences in train = 313
Total loss for epoch 3: 637.078299
	Epoch 4....
Epoch has taken 0:00:19.186768
Number of used sentences in train = 313
Total loss for epoch 4: 592.468499
	Epoch 5....
Epoch has taken 0:00:19.173416
Number of used sentences in train = 313
Total loss for epoch 5: 574.315672
	Epoch 6....
Epoch has taken 0:00:19.183224
Number of used sentences in train = 313
Total loss for epoch 6: 557.690195
	Epoch 7....
Epoch has taken 0:00:19.169495
Number of used sentences in train = 313
Total loss for epoch 7: 533.566043
	Epoch 8....
Epoch has taken 0:00:19.172221
Number of used sentences in train = 313
Total loss for epoch 8: 518.898347
	Epoch 9....
Epoch has taken 0:00:19.183955
Number of used sentences in train = 313
Total loss for epoch 9: 513.572045
	Epoch 10....
Epoch has taken 0:00:19.207445
Number of used sentences in train = 313
Total loss for epoch 10: 513.259348
	Epoch 11....
Epoch has taken 0:00:19.194038
Number of used sentences in train = 313
Total loss for epoch 11: 507.584085
	Epoch 12....
Epoch has taken 0:00:19.215639
Number of used sentences in train = 313
Total loss for epoch 12: 508.211185
	Epoch 13....
Epoch has taken 0:00:19.200044
Number of used sentences in train = 313
Total loss for epoch 13: 503.520875
	Epoch 14....
Epoch has taken 0:00:19.198639
Number of used sentences in train = 313
Total loss for epoch 14: 502.332083
Epoch has taken 0:00:19.207229

==================================================================================================
	Training time : 0:50:11.860854
==================================================================================================
	Identification : 0.413

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5660, 220)
  (lstm): LSTM(239, 117, num_layers=2, dropout=0.32, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=94, bias=True)
  (linear2): Linear(in_features=94, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 17371.355320
validation loss after epoch 0 : 1225.418332
	Epoch 1....
Epoch has taken 0:02:02.673813
Number of used sentences in train = 2074
Total loss for epoch 1: 8749.896414
validation loss after epoch 1 : 987.257255
	Epoch 2....
Epoch has taken 0:02:02.722771
Number of used sentences in train = 2074
Total loss for epoch 2: 6314.830872
validation loss after epoch 2 : 991.962889
	Epoch 3....
Epoch has taken 0:02:03.023445
Number of used sentences in train = 2074
Total loss for epoch 3: 5222.969008
validation loss after epoch 3 : 946.967431
	Epoch 4....
Epoch has taken 0:02:22.687731
Number of used sentences in train = 2074
Total loss for epoch 4: 4586.752600
validation loss after epoch 4 : 1114.999764
	Epoch 5....
Epoch has taken 0:02:04.165028
Number of used sentences in train = 2074
Total loss for epoch 5: 4187.935801
validation loss after epoch 5 : 1063.652335
	Epoch 6....
Epoch has taken 0:02:03.235261
Number of used sentences in train = 2074
Total loss for epoch 6: 3880.796242
validation loss after epoch 6 : 1185.082744
	Epoch 7....
Epoch has taken 0:02:03.156216
Number of used sentences in train = 2074
Total loss for epoch 7: 3760.027994
validation loss after epoch 7 : 1164.849225
	Epoch 8....
Epoch has taken 0:02:03.992287
Number of used sentences in train = 2074
Total loss for epoch 8: 3630.140617
validation loss after epoch 8 : 1106.930234
	Epoch 9....
Epoch has taken 0:02:04.114092
Number of used sentences in train = 2074
Total loss for epoch 9: 3502.194715
validation loss after epoch 9 : 1099.061429
	Epoch 10....
Epoch has taken 0:02:05.862446
Number of used sentences in train = 2074
Total loss for epoch 10: 3405.806122
validation loss after epoch 10 : 1213.246573
	Epoch 11....
Epoch has taken 0:02:04.253541
Number of used sentences in train = 2074
Total loss for epoch 11: 3381.682435
validation loss after epoch 11 : 1262.906554
	Epoch 12....
Epoch has taken 0:02:03.030410
Number of used sentences in train = 2074
Total loss for epoch 12: 3362.185253
validation loss after epoch 12 : 1305.395266
	Epoch 13....
Epoch has taken 0:02:03.565358
Number of used sentences in train = 2074
Total loss for epoch 13: 3332.090207
validation loss after epoch 13 : 1216.118848
	Epoch 14....
Epoch has taken 0:02:03.718772
Number of used sentences in train = 2074
Total loss for epoch 14: 3292.729186
validation loss after epoch 14 : 1194.610110
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5660, 220)
  (lstm): LSTM(239, 117, num_layers=2, dropout=0.32, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=94, bias=True)
  (linear2): Linear(in_features=94, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:03.456667
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1954.446327
	Epoch 1....
Epoch has taken 0:00:12.664390
Number of used sentences in train = 231
Total loss for epoch 1: 821.897749
	Epoch 2....
Epoch has taken 0:00:12.635047
Number of used sentences in train = 231
Total loss for epoch 2: 574.523616
	Epoch 3....
Epoch has taken 0:00:12.642542
Number of used sentences in train = 231
Total loss for epoch 3: 468.717872
	Epoch 4....
Epoch has taken 0:00:12.648838
Number of used sentences in train = 231
Total loss for epoch 4: 423.307636
	Epoch 5....
Epoch has taken 0:00:12.634435
Number of used sentences in train = 231
Total loss for epoch 5: 399.769817
	Epoch 6....
Epoch has taken 0:00:12.628165
Number of used sentences in train = 231
Total loss for epoch 6: 383.230392
	Epoch 7....
Epoch has taken 0:00:12.607333
Number of used sentences in train = 231
Total loss for epoch 7: 369.519029
	Epoch 8....
Epoch has taken 0:00:12.621162
Number of used sentences in train = 231
Total loss for epoch 8: 362.850390
	Epoch 9....
Epoch has taken 0:00:12.611917
Number of used sentences in train = 231
Total loss for epoch 9: 355.983477
	Epoch 10....
Epoch has taken 0:00:12.607546
Number of used sentences in train = 231
Total loss for epoch 10: 354.683887
	Epoch 11....
Epoch has taken 0:00:12.606285
Number of used sentences in train = 231
Total loss for epoch 11: 362.397423
	Epoch 12....
Epoch has taken 0:00:12.609147
Number of used sentences in train = 231
Total loss for epoch 12: 356.881342
	Epoch 13....
Epoch has taken 0:00:12.592081
Number of used sentences in train = 231
Total loss for epoch 13: 350.422730
	Epoch 14....
Epoch has taken 0:00:12.625749
Number of used sentences in train = 231
Total loss for epoch 14: 348.676501
Epoch has taken 0:00:12.496112

==================================================================================================
	Training time : 0:34:23.246470
==================================================================================================
	Identification : 0.224

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(6889, 220)
  (lstm): LSTM(239, 117, num_layers=2, dropout=0.32, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=94, bias=True)
  (linear2): Linear(in_features=94, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 25907.245907
validation loss after epoch 0 : 1639.351080
	Epoch 1....
Epoch has taken 0:03:56.907917
Number of used sentences in train = 3226
Total loss for epoch 1: 13182.663892
validation loss after epoch 1 : 1532.603224
	Epoch 2....
Epoch has taken 0:03:57.100568
Number of used sentences in train = 3226
Total loss for epoch 2: 10825.694734
validation loss after epoch 2 : 1576.348319
	Epoch 3....
Epoch has taken 0:03:57.500147
Number of used sentences in train = 3226
Total loss for epoch 3: 9197.709745
validation loss after epoch 3 : 1600.704470
	Epoch 4....
Epoch has taken 0:04:00.966020
Number of used sentences in train = 3226
Total loss for epoch 4: 8218.923160
validation loss after epoch 4 : 1741.941684
	Epoch 5....
Epoch has taken 0:03:59.494569
Number of used sentences in train = 3226
Total loss for epoch 5: 7578.932251
validation loss after epoch 5 : 1834.079701
	Epoch 6....
Epoch has taken 0:03:57.350549
Number of used sentences in train = 3226
Total loss for epoch 6: 7222.646313
validation loss after epoch 6 : 1973.500333
	Epoch 7....
Epoch has taken 0:03:58.142775
Number of used sentences in train = 3226
Total loss for epoch 7: 6961.139980
validation loss after epoch 7 : 2065.865777
	Epoch 8....
Epoch has taken 0:04:00.382007
Number of used sentences in train = 3226
Total loss for epoch 8: 6713.730102
validation loss after epoch 8 : 2165.907065
	Epoch 9....
Epoch has taken 0:04:00.604559
Number of used sentences in train = 3226
Total loss for epoch 9: 6553.405740
validation loss after epoch 9 : 2271.807297
	Epoch 10....
Epoch has taken 0:04:00.176175
Number of used sentences in train = 3226
Total loss for epoch 10: 6504.105140
validation loss after epoch 10 : 2268.487355
	Epoch 11....
Epoch has taken 0:03:58.645261
Number of used sentences in train = 3226
Total loss for epoch 11: 6439.701536
validation loss after epoch 11 : 2349.571198
	Epoch 12....
Epoch has taken 0:03:58.194865
Number of used sentences in train = 3226
Total loss for epoch 12: 6387.664917
validation loss after epoch 12 : 2482.985837
	Epoch 13....
Epoch has taken 0:04:00.344351
Number of used sentences in train = 3226
Total loss for epoch 13: 6401.768574
validation loss after epoch 13 : 2347.001375
	Epoch 14....
Epoch has taken 0:04:00.686920
Number of used sentences in train = 3226
Total loss for epoch 14: 6330.107841
validation loss after epoch 14 : 2482.252700
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(6889, 220)
  (lstm): LSTM(239, 117, num_layers=2, dropout=0.32, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=94, bias=True)
  (linear2): Linear(in_features=94, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:00.485182
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2369.993020
	Epoch 1....
Epoch has taken 0:00:23.301140
Number of used sentences in train = 359
Total loss for epoch 1: 1264.873027
	Epoch 2....
Epoch has taken 0:00:23.292340
Number of used sentences in train = 359
Total loss for epoch 2: 973.833512
	Epoch 3....
Epoch has taken 0:00:23.327400
Number of used sentences in train = 359
Total loss for epoch 3: 798.757352
	Epoch 4....
Epoch has taken 0:00:23.340012
Number of used sentences in train = 359
Total loss for epoch 4: 730.241170
	Epoch 5....
Epoch has taken 0:00:23.354584
Number of used sentences in train = 359
Total loss for epoch 5: 708.712397
	Epoch 6....
Epoch has taken 0:00:23.350280
Number of used sentences in train = 359
Total loss for epoch 6: 696.039690
	Epoch 7....
Epoch has taken 0:00:23.341915
Number of used sentences in train = 359
Total loss for epoch 7: 698.101920
	Epoch 8....
Epoch has taken 0:00:23.332774
Number of used sentences in train = 359
Total loss for epoch 8: 705.159425
	Epoch 9....
Epoch has taken 0:00:23.319766
Number of used sentences in train = 359
Total loss for epoch 9: 694.588486
	Epoch 10....
Epoch has taken 0:00:23.328490
Number of used sentences in train = 359
Total loss for epoch 10: 676.713974
	Epoch 11....
Epoch has taken 0:00:23.308283
Number of used sentences in train = 359
Total loss for epoch 11: 675.033390
	Epoch 12....
Epoch has taken 0:00:23.342476
Number of used sentences in train = 359
Total loss for epoch 12: 673.317509
	Epoch 13....
Epoch has taken 0:00:23.351949
Number of used sentences in train = 359
Total loss for epoch 13: 672.012926
	Epoch 14....
Epoch has taken 0:00:23.320243
Number of used sentences in train = 359
Total loss for epoch 14: 673.610709
Epoch has taken 0:00:23.349249

==================================================================================================
	Training time : 1:05:37.611629
==================================================================================================
	Identification : 0.51

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 82, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 55, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 29, 'lstmDropout': 0.28, 'denseActivation': 'tanh', 'wordDim': 229, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(1882, 229)
  (lstm): LSTM(284, 29, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=82, bias=True)
  (linear2): Linear(in_features=82, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11000.984860
validation loss after epoch 0 : 854.267209
	Epoch 1....
Epoch has taken 0:02:59.182498
Number of used sentences in train = 2811
Total loss for epoch 1: 7386.354550
validation loss after epoch 1 : 816.594826
	Epoch 2....
Epoch has taken 0:03:01.870500
Number of used sentences in train = 2811
Total loss for epoch 2: 6604.586245
validation loss after epoch 2 : 830.089131
	Epoch 3....
Epoch has taken 0:02:58.985340
Number of used sentences in train = 2811
Total loss for epoch 3: 6105.436580
validation loss after epoch 3 : 845.106974
	Epoch 4....
Epoch has taken 0:02:57.633832
Number of used sentences in train = 2811
Total loss for epoch 4: 5750.463818
validation loss after epoch 4 : 881.093849
	Epoch 5....
Epoch has taken 0:02:57.678844
Number of used sentences in train = 2811
Total loss for epoch 5: 5558.334606
validation loss after epoch 5 : 834.650548
	Epoch 6....
Epoch has taken 0:02:59.769653
Number of used sentences in train = 2811
Total loss for epoch 6: 5391.179235
validation loss after epoch 6 : 954.729324
	Epoch 7....
Epoch has taken 0:03:26.394186
Number of used sentences in train = 2811
Total loss for epoch 7: 5291.587859
validation loss after epoch 7 : 948.745569
	Epoch 8....
Epoch has taken 0:02:59.703670
Number of used sentences in train = 2811
Total loss for epoch 8: 5200.239420
validation loss after epoch 8 : 946.746185
	Epoch 9....
Epoch has taken 0:02:59.359357
Number of used sentences in train = 2811
Total loss for epoch 9: 5082.296316
validation loss after epoch 9 : 978.587931
	Epoch 10....
Epoch has taken 0:02:57.985929
Number of used sentences in train = 2811
Total loss for epoch 10: 5027.836024
validation loss after epoch 10 : 987.111660
	Epoch 11....
Epoch has taken 0:02:59.645273
Number of used sentences in train = 2811
Total loss for epoch 11: 4930.990864
validation loss after epoch 11 : 952.783013
	Epoch 12....
Epoch has taken 0:02:59.738017
Number of used sentences in train = 2811
Total loss for epoch 12: 4936.366103
validation loss after epoch 12 : 994.978356
	Epoch 13....
Epoch has taken 0:02:59.757233
Number of used sentences in train = 2811
Total loss for epoch 13: 4894.902449
validation loss after epoch 13 : 1002.004280
	Epoch 14....
Epoch has taken 0:02:59.808531
Number of used sentences in train = 2811
Total loss for epoch 14: 4859.586350
validation loss after epoch 14 : 1053.473892
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(1882, 229)
  (lstm): LSTM(284, 29, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=82, bias=True)
  (linear2): Linear(in_features=82, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.859200
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1306.502682
	Epoch 1....
Epoch has taken 0:00:18.859735
Number of used sentences in train = 313
Total loss for epoch 1: 731.627744
	Epoch 2....
Epoch has taken 0:00:18.839816
Number of used sentences in train = 313
Total loss for epoch 2: 640.584981
	Epoch 3....
Epoch has taken 0:00:18.818616
Number of used sentences in train = 313
Total loss for epoch 3: 578.265641
	Epoch 4....
Epoch has taken 0:00:18.853410
Number of used sentences in train = 313
Total loss for epoch 4: 561.681887
	Epoch 5....
Epoch has taken 0:00:18.834736
Number of used sentences in train = 313
Total loss for epoch 5: 552.704938
	Epoch 6....
Epoch has taken 0:00:18.851017
Number of used sentences in train = 313
Total loss for epoch 6: 525.452372
	Epoch 7....
Epoch has taken 0:00:18.872815
Number of used sentences in train = 313
Total loss for epoch 7: 526.060284
	Epoch 8....
Epoch has taken 0:00:18.821199
Number of used sentences in train = 313
Total loss for epoch 8: 515.443061
	Epoch 9....
Epoch has taken 0:00:18.841279
Number of used sentences in train = 313
Total loss for epoch 9: 515.145466
	Epoch 10....
Epoch has taken 0:00:18.846424
Number of used sentences in train = 313
Total loss for epoch 10: 514.083375
	Epoch 11....
Epoch has taken 0:00:18.832450
Number of used sentences in train = 313
Total loss for epoch 11: 516.379404
	Epoch 12....
Epoch has taken 0:00:18.844285
Number of used sentences in train = 313
Total loss for epoch 12: 507.978441
	Epoch 13....
Epoch has taken 0:00:18.845603
Number of used sentences in train = 313
Total loss for epoch 13: 507.357853
	Epoch 14....
Epoch has taken 0:00:18.835272
Number of used sentences in train = 313
Total loss for epoch 14: 505.757761
Epoch has taken 0:00:18.829495

==================================================================================================
	Training time : 0:49:58.499047
==================================================================================================
	Identification : 0.181

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(1680, 229)
  (lstm): LSTM(284, 29, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=82, bias=True)
  (linear2): Linear(in_features=82, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7833.019849
validation loss after epoch 0 : 709.598134
	Epoch 1....
Epoch has taken 0:02:03.161839
Number of used sentences in train = 2074
Total loss for epoch 1: 5241.266408
validation loss after epoch 1 : 694.311775
	Epoch 2....
Epoch has taken 0:02:03.119561
Number of used sentences in train = 2074
Total loss for epoch 2: 4578.956509
validation loss after epoch 2 : 649.141392
	Epoch 3....
Epoch has taken 0:02:21.844727
Number of used sentences in train = 2074
Total loss for epoch 3: 4233.272377
validation loss after epoch 3 : 642.120586
	Epoch 4....
Epoch has taken 0:02:03.253214
Number of used sentences in train = 2074
Total loss for epoch 4: 3969.777143
validation loss after epoch 4 : 672.543081
	Epoch 5....
Epoch has taken 0:02:02.290308
Number of used sentences in train = 2074
Total loss for epoch 5: 3790.025504
validation loss after epoch 5 : 729.069405
	Epoch 6....
Epoch has taken 0:02:02.318104
Number of used sentences in train = 2074
Total loss for epoch 6: 3649.593755
validation loss after epoch 6 : 729.215271
	Epoch 7....
Epoch has taken 0:02:03.254354
Number of used sentences in train = 2074
Total loss for epoch 7: 3532.983870
validation loss after epoch 7 : 755.757699
	Epoch 8....
Epoch has taken 0:02:21.609134
Number of used sentences in train = 2074
Total loss for epoch 8: 3485.353286
validation loss after epoch 8 : 778.463017
	Epoch 9....
Epoch has taken 0:02:02.888108
Number of used sentences in train = 2074
Total loss for epoch 9: 3396.404901
validation loss after epoch 9 : 781.735916
	Epoch 10....
Epoch has taken 0:02:02.056147
Number of used sentences in train = 2074
Total loss for epoch 10: 3412.653159
validation loss after epoch 10 : 861.377823
	Epoch 11....
Epoch has taken 0:02:01.853008
Number of used sentences in train = 2074
Total loss for epoch 11: 3387.064547
validation loss after epoch 11 : 836.375454
	Epoch 12....
Epoch has taken 0:02:02.024333
Number of used sentences in train = 2074
Total loss for epoch 12: 3364.782258
validation loss after epoch 12 : 827.907181
	Epoch 13....
Epoch has taken 0:02:17.749248
Number of used sentences in train = 2074
Total loss for epoch 13: 3326.668458
validation loss after epoch 13 : 823.242327
	Epoch 14....
Epoch has taken 0:02:02.994602
Number of used sentences in train = 2074
Total loss for epoch 14: 3310.347850
validation loss after epoch 14 : 888.920925
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(1680, 229)
  (lstm): LSTM(284, 29, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=82, bias=True)
  (linear2): Linear(in_features=82, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:02.211449
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1409.660805
	Epoch 1....
Epoch has taken 0:00:12.578145
Number of used sentences in train = 231
Total loss for epoch 1: 582.110660
	Epoch 2....
Epoch has taken 0:00:12.567974
Number of used sentences in train = 231
Total loss for epoch 2: 465.949161
	Epoch 3....
Epoch has taken 0:00:12.579101
Number of used sentences in train = 231
Total loss for epoch 3: 412.933118
	Epoch 4....
Epoch has taken 0:00:12.564881
Number of used sentences in train = 231
Total loss for epoch 4: 389.240742
	Epoch 5....
Epoch has taken 0:00:12.563796
Number of used sentences in train = 231
Total loss for epoch 5: 364.624198
	Epoch 6....
Epoch has taken 0:00:12.552693
Number of used sentences in train = 231
Total loss for epoch 6: 367.311133
	Epoch 7....
Epoch has taken 0:00:12.548482
Number of used sentences in train = 231
Total loss for epoch 7: 363.871098
	Epoch 8....
Epoch has taken 0:00:12.581037
Number of used sentences in train = 231
Total loss for epoch 8: 359.353679
	Epoch 9....
Epoch has taken 0:00:12.566965
Number of used sentences in train = 231
Total loss for epoch 9: 355.954217
	Epoch 10....
Epoch has taken 0:00:12.580271
Number of used sentences in train = 231
Total loss for epoch 10: 353.664088
	Epoch 11....
Epoch has taken 0:00:12.560516
Number of used sentences in train = 231
Total loss for epoch 11: 353.919333
	Epoch 12....
Epoch has taken 0:00:12.590715
Number of used sentences in train = 231
Total loss for epoch 12: 349.810313
	Epoch 13....
Epoch has taken 0:00:12.568322
Number of used sentences in train = 231
Total loss for epoch 13: 353.378547
	Epoch 14....
Epoch has taken 0:00:12.562545
Number of used sentences in train = 231
Total loss for epoch 14: 348.380259
Epoch has taken 0:00:12.538996

==================================================================================================
	Training time : 0:34:41.472497
==================================================================================================
	Identification : 0.282

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 55)
  (w_embeddings): Embedding(3369, 229)
  (lstm): LSTM(284, 29, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=82, bias=True)
  (linear2): Linear(in_features=82, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12356.261807
validation loss after epoch 0 : 1131.739664
	Epoch 1....
Epoch has taken 0:04:35.390023
Number of used sentences in train = 3226
Total loss for epoch 1: 9501.434202
validation loss after epoch 1 : 1081.993618
	Epoch 2....
Epoch has taken 0:04:02.672488
Number of used sentences in train = 3226
Total loss for epoch 2: 8772.686681
validation loss after epoch 2 : 1119.024225
	Epoch 3....
Epoch has taken 0:03:59.140684
Number of used sentences in train = 3226
Total loss for epoch 3: 8250.674761
validation loss after epoch 3 : 1106.859934
	Epoch 4....
Epoch has taken 0:03:57.126969
Number of used sentences in train = 3226
Total loss for epoch 4: 7946.466209
validation loss after epoch 4 : 1125.126164
	Epoch 5....
Epoch has taken 0:03:59.008166
Number of used sentences in train = 3226
Total loss for epoch 5: 7709.862565
validation loss after epoch 5 : 1163.470260
	Epoch 6....
Epoch has taken 0:03:59.525967
Number of used sentences in train = 3226
Total loss for epoch 6: 7524.926103
validation loss after epoch 6 : 1227.076349
	Epoch 7....
Epoch has taken 0:03:59.446123
Number of used sentences in train = 3226
Total loss for epoch 7: 7366.158128
validation loss after epoch 7 : 1241.564125
	Epoch 8....
Epoch has taken 0:03:57.638268
Number of used sentences in train = 3226
Total loss for epoch 8: 7224.140022
validation loss after epoch 8 : 1263.777381
	Epoch 9....
Epoch has taken 0:03:57.559147
Number of used sentences in train = 3226
Total loss for epoch 9: 7091.123851
validation loss after epoch 9 : 1335.590751
	Epoch 10....
Epoch has taken 0:03:59.800379
Number of used sentences in train = 3226
Total loss for epoch 10: 7098.114253
validation loss after epoch 10 : 1299.165708
	Epoch 11....
Epoch has taken 0:03:59.355760
Number of used sentences in train = 3226
Total loss for epoch 11: 7010.002311
validation loss after epoch 11 : 1346.790548
	Epoch 12....
Epoch has taken 0:03:59.646362
Number of used sentences in train = 3226
Total loss for epoch 12: 6981.713350
validation loss after epoch 12 : 1336.411295
	Epoch 13....
Epoch has taken 0:04:10.975355
Number of used sentences in train = 3226
Total loss for epoch 13: 6951.922277
validation loss after epoch 13 : 1327.964121
	Epoch 14....
Epoch has taken 0:03:59.024175
Number of used sentences in train = 3226
Total loss for epoch 14: 6842.193363
validation loss after epoch 14 : 1337.865331
	TransitionClassifier(
  (p_embeddings): Embedding(13, 55)
  (w_embeddings): Embedding(3369, 229)
  (lstm): LSTM(284, 29, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=82, bias=True)
  (linear2): Linear(in_features=82, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:23.956749
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1553.051717
	Epoch 1....
Epoch has taken 0:00:29.331779
Number of used sentences in train = 359
Total loss for epoch 1: 999.302101
	Epoch 2....
Epoch has taken 0:00:27.706379
Number of used sentences in train = 359
Total loss for epoch 2: 903.482858
	Epoch 3....
Epoch has taken 0:00:25.969638
Number of used sentences in train = 359
Total loss for epoch 3: 810.462397
	Epoch 4....
Epoch has taken 0:00:25.966118
Number of used sentences in train = 359
Total loss for epoch 4: 787.140212
	Epoch 5....
Epoch has taken 0:00:25.958580
Number of used sentences in train = 359
Total loss for epoch 5: 753.989614
	Epoch 6....
Epoch has taken 0:00:25.962682
Number of used sentences in train = 359
Total loss for epoch 6: 728.528733
	Epoch 7....
Epoch has taken 0:00:25.976074
Number of used sentences in train = 359
Total loss for epoch 7: 722.587667
	Epoch 8....
Epoch has taken 0:00:25.945265
Number of used sentences in train = 359
Total loss for epoch 8: 705.507437
	Epoch 9....
Epoch has taken 0:00:25.970181
Number of used sentences in train = 359
Total loss for epoch 9: 718.046551
	Epoch 10....
Epoch has taken 0:00:25.975357
Number of used sentences in train = 359
Total loss for epoch 10: 710.739682
	Epoch 11....
Epoch has taken 0:00:25.973890
Number of used sentences in train = 359
Total loss for epoch 11: 700.470053
	Epoch 12....
Epoch has taken 0:00:25.972770
Number of used sentences in train = 359
Total loss for epoch 12: 697.276173
	Epoch 13....
Epoch has taken 0:00:25.971543
Number of used sentences in train = 359
Total loss for epoch 13: 698.178982
	Epoch 14....
Epoch has taken 0:00:26.006955
Number of used sentences in train = 359
Total loss for epoch 14: 697.603308
Epoch has taken 0:00:25.974058

==================================================================================================
	Training time : 1:07:35.599314
==================================================================================================
	Identification : 0.259

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 68, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 66, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 27, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 94, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1882, 94)
  (lstm): LSTM(160, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=68, bias=True)
  (linear2): Linear(in_features=68, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10268.636384
validation loss after epoch 0 : 860.381201
	Epoch 1....
Epoch has taken 0:03:04.132434
Number of used sentences in train = 2811
Total loss for epoch 1: 7175.224371
validation loss after epoch 1 : 903.886867
	Epoch 2....
Epoch has taken 0:03:04.234922
Number of used sentences in train = 2811
Total loss for epoch 2: 6390.388563
validation loss after epoch 2 : 879.076268
	Epoch 3....
Epoch has taken 0:03:04.797616
Number of used sentences in train = 2811
Total loss for epoch 3: 5866.323079
validation loss after epoch 3 : 917.865613
	Epoch 4....
Epoch has taken 0:03:04.717057
Number of used sentences in train = 2811
Total loss for epoch 4: 5492.351404
validation loss after epoch 4 : 952.026609
	Epoch 5....
Epoch has taken 0:03:03.270965
Number of used sentences in train = 2811
Total loss for epoch 5: 5247.034923
validation loss after epoch 5 : 982.177182
	Epoch 6....
Epoch has taken 0:03:04.341583
Number of used sentences in train = 2811
Total loss for epoch 6: 5083.377406
validation loss after epoch 6 : 1040.385290
	Epoch 7....
Epoch has taken 0:03:04.533304
Number of used sentences in train = 2811
Total loss for epoch 7: 4937.668595
validation loss after epoch 7 : 1069.559014
	Epoch 8....
Epoch has taken 0:03:04.755875
Number of used sentences in train = 2811
Total loss for epoch 8: 4842.494528
validation loss after epoch 8 : 1117.382633
	Epoch 9....
Epoch has taken 0:03:04.725004
Number of used sentences in train = 2811
Total loss for epoch 9: 4752.303548
validation loss after epoch 9 : 1161.755254
	Epoch 10....
Epoch has taken 0:03:02.826818
Number of used sentences in train = 2811
Total loss for epoch 10: 4693.517185
validation loss after epoch 10 : 1193.028036
	Epoch 11....
Epoch has taken 0:02:58.953198
Number of used sentences in train = 2811
Total loss for epoch 11: 4649.841976
validation loss after epoch 11 : 1241.498474
	Epoch 12....
Epoch has taken 0:02:46.726614
Number of used sentences in train = 2811
Total loss for epoch 12: 4607.651129
validation loss after epoch 12 : 1235.607480
	Epoch 13....
Epoch has taken 0:02:47.051010
Number of used sentences in train = 2811
Total loss for epoch 13: 4580.139014
validation loss after epoch 13 : 1266.713381
	Epoch 14....
Epoch has taken 0:02:46.958771
Number of used sentences in train = 2811
Total loss for epoch 14: 4559.041418
validation loss after epoch 14 : 1285.237493
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1882, 94)
  (lstm): LSTM(160, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=68, bias=True)
  (linear2): Linear(in_features=68, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:58.452306
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1259.340546
	Epoch 1....
Epoch has taken 0:00:17.602388
Number of used sentences in train = 313
Total loss for epoch 1: 691.416481
	Epoch 2....
Epoch has taken 0:00:17.610996
Number of used sentences in train = 313
Total loss for epoch 2: 587.812686
	Epoch 3....
Epoch has taken 0:00:17.623604
Number of used sentences in train = 313
Total loss for epoch 3: 548.171614
	Epoch 4....
Epoch has taken 0:00:17.663126
Number of used sentences in train = 313
Total loss for epoch 4: 526.107158
	Epoch 5....
Epoch has taken 0:00:17.646605
Number of used sentences in train = 313
Total loss for epoch 5: 513.165104
	Epoch 6....
Epoch has taken 0:00:17.649398
Number of used sentences in train = 313
Total loss for epoch 6: 508.423897
	Epoch 7....
Epoch has taken 0:00:17.458003
Number of used sentences in train = 313
Total loss for epoch 7: 507.188992
	Epoch 8....
Epoch has taken 0:00:17.468751
Number of used sentences in train = 313
Total loss for epoch 8: 504.990557
	Epoch 9....
Epoch has taken 0:00:17.446957
Number of used sentences in train = 313
Total loss for epoch 9: 503.998054
	Epoch 10....
Epoch has taken 0:00:17.473658
Number of used sentences in train = 313
Total loss for epoch 10: 505.133018
	Epoch 11....
Epoch has taken 0:00:17.465509
Number of used sentences in train = 313
Total loss for epoch 11: 503.880486
	Epoch 12....
Epoch has taken 0:00:17.473691
Number of used sentences in train = 313
Total loss for epoch 12: 502.627464
	Epoch 13....
Epoch has taken 0:00:17.462863
Number of used sentences in train = 313
Total loss for epoch 13: 502.091411
	Epoch 14....
Epoch has taken 0:00:17.473926
Number of used sentences in train = 313
Total loss for epoch 14: 501.682586
Epoch has taken 0:00:17.457137

==================================================================================================
	Training time : 0:49:23.954409
==================================================================================================
	Identification : 0.358

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1680, 94)
  (lstm): LSTM(160, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=68, bias=True)
  (linear2): Linear(in_features=68, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7667.373502
validation loss after epoch 0 : 763.572667
	Epoch 1....
Epoch has taken 0:01:54.171465
Number of used sentences in train = 2074
Total loss for epoch 1: 5281.687083
validation loss after epoch 1 : 752.934439
	Epoch 2....
Epoch has taken 0:01:54.102395
Number of used sentences in train = 2074
Total loss for epoch 2: 4486.234428
validation loss after epoch 2 : 742.350319
	Epoch 3....
Epoch has taken 0:01:54.296043
Number of used sentences in train = 2074
Total loss for epoch 3: 3974.053576
validation loss after epoch 3 : 783.905478
	Epoch 4....
Epoch has taken 0:01:52.976252
Number of used sentences in train = 2074
Total loss for epoch 4: 3704.717921
validation loss after epoch 4 : 803.934314
	Epoch 5....
Epoch has taken 0:01:53.302124
Number of used sentences in train = 2074
Total loss for epoch 5: 3506.155355
validation loss after epoch 5 : 872.806406
	Epoch 6....
Epoch has taken 0:01:54.024376
Number of used sentences in train = 2074
Total loss for epoch 6: 3373.340679
validation loss after epoch 6 : 956.891393
	Epoch 7....
Epoch has taken 0:01:54.287407
Number of used sentences in train = 2074
Total loss for epoch 7: 3286.197645
validation loss after epoch 7 : 1004.654914
	Epoch 8....
Epoch has taken 0:02:11.880436
Number of used sentences in train = 2074
Total loss for epoch 8: 3246.765623
validation loss after epoch 8 : 1067.473501
	Epoch 9....
Epoch has taken 0:01:53.146717
Number of used sentences in train = 2074
Total loss for epoch 9: 3212.525108
validation loss after epoch 9 : 1106.804379
	Epoch 10....
Epoch has taken 0:01:53.380173
Number of used sentences in train = 2074
Total loss for epoch 10: 3198.664703
validation loss after epoch 10 : 1117.078651
	Epoch 11....
Epoch has taken 0:01:53.257910
Number of used sentences in train = 2074
Total loss for epoch 11: 3182.810327
validation loss after epoch 11 : 1156.436219
	Epoch 12....
Epoch has taken 0:01:54.120265
Number of used sentences in train = 2074
Total loss for epoch 12: 3174.281335
validation loss after epoch 12 : 1132.504941
	Epoch 13....
Epoch has taken 0:01:53.609244
Number of used sentences in train = 2074
Total loss for epoch 13: 3169.569728
validation loss after epoch 13 : 1171.996637
	Epoch 14....
Epoch has taken 0:01:53.816916
Number of used sentences in train = 2074
Total loss for epoch 14: 3166.451798
validation loss after epoch 14 : 1182.832264
	TransitionClassifier(
  (p_embeddings): Embedding(18, 66)
  (w_embeddings): Embedding(1680, 94)
  (lstm): LSTM(160, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=68, bias=True)
  (linear2): Linear(in_features=68, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.571855
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1612.587255
	Epoch 1....
Epoch has taken 0:00:11.533428
Number of used sentences in train = 231
Total loss for epoch 1: 604.862548
	Epoch 2....
Epoch has taken 0:00:11.526376
Number of used sentences in train = 231
Total loss for epoch 2: 493.282200
	Epoch 3....
Epoch has taken 0:00:11.529747
Number of used sentences in train = 231
Total loss for epoch 3: 432.225310
	Epoch 4....
Epoch has taken 0:00:11.527339
Number of used sentences in train = 231
Total loss for epoch 4: 400.358593
	Epoch 5....
Epoch has taken 0:00:11.526696
Number of used sentences in train = 231
Total loss for epoch 5: 372.578270
	Epoch 6....
Epoch has taken 0:00:11.503613
Number of used sentences in train = 231
Total loss for epoch 6: 357.668065
	Epoch 7....
Epoch has taken 0:00:11.508106
Number of used sentences in train = 231
Total loss for epoch 7: 352.635776
	Epoch 8....
Epoch has taken 0:00:11.537245
Number of used sentences in train = 231
Total loss for epoch 8: 350.828039
	Epoch 9....
Epoch has taken 0:00:11.522153
Number of used sentences in train = 231
Total loss for epoch 9: 348.962620
	Epoch 10....
Epoch has taken 0:00:11.522664
Number of used sentences in train = 231
Total loss for epoch 10: 348.473071
	Epoch 11....
Epoch has taken 0:00:11.504902
Number of used sentences in train = 231
Total loss for epoch 11: 347.934462
	Epoch 12....
Epoch has taken 0:00:11.528383
Number of used sentences in train = 231
Total loss for epoch 12: 346.922293
	Epoch 13....
Epoch has taken 0:00:11.513768
Number of used sentences in train = 231
Total loss for epoch 13: 346.593577
	Epoch 14....
Epoch has taken 0:00:11.517336
Number of used sentences in train = 231
Total loss for epoch 14: 346.281383
Epoch has taken 0:00:11.499702

==================================================================================================
	Training time : 0:31:37.083542
==================================================================================================
	Identification : 0.172

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 66)
  (w_embeddings): Embedding(3369, 94)
  (lstm): LSTM(160, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=68, bias=True)
  (linear2): Linear(in_features=68, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12647.812212
validation loss after epoch 0 : 1168.929579
	Epoch 1....
Epoch has taken 0:03:41.013411
Number of used sentences in train = 3226
Total loss for epoch 1: 9810.009845
validation loss after epoch 1 : 1111.020626
	Epoch 2....
Epoch has taken 0:04:12.502041
Number of used sentences in train = 3226
Total loss for epoch 2: 8884.104870
validation loss after epoch 2 : 1122.600880
	Epoch 3....
Epoch has taken 0:03:42.086254
Number of used sentences in train = 3226
Total loss for epoch 3: 8188.831921
validation loss after epoch 3 : 1147.954141
	Epoch 4....
Epoch has taken 0:03:40.345107
Number of used sentences in train = 3226
Total loss for epoch 4: 7722.470867
validation loss after epoch 4 : 1174.118670
	Epoch 5....
Epoch has taken 0:03:42.258055
Number of used sentences in train = 3226
Total loss for epoch 5: 7365.578158
validation loss after epoch 5 : 1277.634899
	Epoch 6....
Epoch has taken 0:03:42.232581
Number of used sentences in train = 3226
Total loss for epoch 6: 7141.589896
validation loss after epoch 6 : 1280.560235
	Epoch 7....
Epoch has taken 0:03:39.414483
Number of used sentences in train = 3226
Total loss for epoch 7: 6967.993993
validation loss after epoch 7 : 1344.267383
	Epoch 8....
Epoch has taken 0:03:39.196897
Number of used sentences in train = 3226
Total loss for epoch 8: 6840.052239
validation loss after epoch 8 : 1400.391451
	Epoch 9....
Epoch has taken 0:03:41.938609
Number of used sentences in train = 3226
Total loss for epoch 9: 6737.833517
validation loss after epoch 9 : 1416.594410
	Epoch 10....
Epoch has taken 0:04:13.273037
Number of used sentences in train = 3226
Total loss for epoch 10: 6647.689443
validation loss after epoch 10 : 1473.349181
	Epoch 11....
Epoch has taken 0:03:39.267880
Number of used sentences in train = 3226
Total loss for epoch 11: 6551.839050
validation loss after epoch 11 : 1693.790054
	Epoch 12....
Epoch has taken 0:03:39.743371
Number of used sentences in train = 3226
Total loss for epoch 12: 6505.461423
validation loss after epoch 12 : 1623.993840
	Epoch 13....
Epoch has taken 0:03:39.270199
Number of used sentences in train = 3226
Total loss for epoch 13: 6451.862869
validation loss after epoch 13 : 1628.431212
	Epoch 14....
Epoch has taken 0:03:41.208098
Number of used sentences in train = 3226
Total loss for epoch 14: 6419.205848
validation loss after epoch 14 : 1725.654590
	TransitionClassifier(
  (p_embeddings): Embedding(13, 66)
  (w_embeddings): Embedding(3369, 94)
  (lstm): LSTM(160, 27, bidirectional=True)
  (linear1): Linear(in_features=432, out_features=68, bias=True)
  (linear2): Linear(in_features=68, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:41.191912
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1747.506808
	Epoch 1....
Epoch has taken 0:00:21.623075
Number of used sentences in train = 359
Total loss for epoch 1: 944.916956
	Epoch 2....
Epoch has taken 0:00:21.648436
Number of used sentences in train = 359
Total loss for epoch 2: 811.182032
	Epoch 3....
Epoch has taken 0:00:21.647369
Number of used sentences in train = 359
Total loss for epoch 3: 754.274536
	Epoch 4....
Epoch has taken 0:00:21.648964
Number of used sentences in train = 359
Total loss for epoch 4: 727.710895
	Epoch 5....
Epoch has taken 0:00:21.655689
Number of used sentences in train = 359
Total loss for epoch 5: 700.711214
	Epoch 6....
Epoch has taken 0:00:21.607631
Number of used sentences in train = 359
Total loss for epoch 6: 692.390339
	Epoch 7....
Epoch has taken 0:00:21.616965
Number of used sentences in train = 359
Total loss for epoch 7: 684.688174
	Epoch 8....
Epoch has taken 0:00:21.639994
Number of used sentences in train = 359
Total loss for epoch 8: 680.607362
	Epoch 9....
Epoch has taken 0:00:21.630404
Number of used sentences in train = 359
Total loss for epoch 9: 677.722103
	Epoch 10....
Epoch has taken 0:00:21.635436
Number of used sentences in train = 359
Total loss for epoch 10: 676.591619
	Epoch 11....
Epoch has taken 0:00:21.628690
Number of used sentences in train = 359
Total loss for epoch 11: 674.637738
	Epoch 12....
Epoch has taken 0:00:21.642800
Number of used sentences in train = 359
Total loss for epoch 12: 673.315984
	Epoch 13....
Epoch has taken 0:00:21.627169
Number of used sentences in train = 359
Total loss for epoch 13: 673.123481
	Epoch 14....
Epoch has taken 0:00:21.620817
Number of used sentences in train = 359
Total loss for epoch 14: 673.183289
Epoch has taken 0:00:21.616441

==================================================================================================
	Training time : 1:01:40.106695
==================================================================================================
	Identification : 0.426

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 21, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 34, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 40, 'lstmDropout': 0.34, 'denseActivation': 'tanh', 'wordDim': 155, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(9355, 155)
  (lstm): LSTM(189, 40, bidirectional=True)
  (linear1): Linear(in_features=640, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13372.762201
validation loss after epoch 0 : 1216.295210
	Epoch 1....
Epoch has taken 0:02:45.013914
Number of used sentences in train = 2811
Total loss for epoch 1: 8731.456416
validation loss after epoch 1 : 1169.803250
	Epoch 2....
Epoch has taken 0:02:45.015313
Number of used sentences in train = 2811
Total loss for epoch 2: 6893.244590
validation loss after epoch 2 : 1198.342784
	Epoch 3....
Epoch has taken 0:02:46.346961
Number of used sentences in train = 2811
Total loss for epoch 3: 5860.923699
validation loss after epoch 3 : 1301.372551
	Epoch 4....
Epoch has taken 0:02:46.250630
Number of used sentences in train = 2811
Total loss for epoch 4: 5329.959627
validation loss after epoch 4 : 1368.598941
	Epoch 5....
Epoch has taken 0:02:46.433125
Number of used sentences in train = 2811
Total loss for epoch 5: 5035.811342
validation loss after epoch 5 : 1462.330663
	Epoch 6....
Epoch has taken 0:02:46.397717
Number of used sentences in train = 2811
Total loss for epoch 6: 4852.283614
validation loss after epoch 6 : 1510.635016
	Epoch 7....
Epoch has taken 0:02:44.714649
Number of used sentences in train = 2811
Total loss for epoch 7: 4756.652889
validation loss after epoch 7 : 1544.202059
	Epoch 8....
Epoch has taken 0:02:44.933760
Number of used sentences in train = 2811
Total loss for epoch 8: 4660.402911
validation loss after epoch 8 : 1569.981582
	Epoch 9....
Epoch has taken 0:02:46.586671
Number of used sentences in train = 2811
Total loss for epoch 9: 4613.108187
validation loss after epoch 9 : 1627.503344
	Epoch 10....
Epoch has taken 0:02:46.502991
Number of used sentences in train = 2811
Total loss for epoch 10: 4580.266317
validation loss after epoch 10 : 1647.282242
	Epoch 11....
Epoch has taken 0:02:46.268455
Number of used sentences in train = 2811
Total loss for epoch 11: 4556.943251
validation loss after epoch 11 : 1685.701923
	Epoch 12....
Epoch has taken 0:02:45.125201
Number of used sentences in train = 2811
Total loss for epoch 12: 4542.090327
validation loss after epoch 12 : 1700.222587
	Epoch 13....
Epoch has taken 0:02:44.748297
Number of used sentences in train = 2811
Total loss for epoch 13: 4532.876062
validation loss after epoch 13 : 1712.362682
	Epoch 14....
Epoch has taken 0:02:45.676412
Number of used sentences in train = 2811
Total loss for epoch 14: 4526.010394
validation loss after epoch 14 : 1742.958264
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(9355, 155)
  (lstm): LSTM(189, 40, bidirectional=True)
  (linear1): Linear(in_features=640, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.496885
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1571.220189
	Epoch 1....
Epoch has taken 0:00:17.583239
Number of used sentences in train = 313
Total loss for epoch 1: 866.393889
	Epoch 2....
Epoch has taken 0:00:17.543888
Number of used sentences in train = 313
Total loss for epoch 2: 641.516854
	Epoch 3....
Epoch has taken 0:00:17.553146
Number of used sentences in train = 313
Total loss for epoch 3: 567.364910
	Epoch 4....
Epoch has taken 0:00:17.568732
Number of used sentences in train = 313
Total loss for epoch 4: 537.422588
	Epoch 5....
Epoch has taken 0:00:17.563914
Number of used sentences in train = 313
Total loss for epoch 5: 525.844668
	Epoch 6....
Epoch has taken 0:00:17.536802
Number of used sentences in train = 313
Total loss for epoch 6: 522.677415
	Epoch 7....
Epoch has taken 0:00:17.557877
Number of used sentences in train = 313
Total loss for epoch 7: 514.660894
	Epoch 8....
Epoch has taken 0:00:17.559314
Number of used sentences in train = 313
Total loss for epoch 8: 512.997981
	Epoch 9....
Epoch has taken 0:00:17.553666
Number of used sentences in train = 313
Total loss for epoch 9: 510.950903
	Epoch 10....
Epoch has taken 0:00:17.569067
Number of used sentences in train = 313
Total loss for epoch 10: 509.837600
	Epoch 11....
Epoch has taken 0:00:17.552750
Number of used sentences in train = 313
Total loss for epoch 11: 507.468375
	Epoch 12....
Epoch has taken 0:00:17.544572
Number of used sentences in train = 313
Total loss for epoch 12: 509.053539
	Epoch 13....
Epoch has taken 0:00:17.541531
Number of used sentences in train = 313
Total loss for epoch 13: 508.144508
	Epoch 14....
Epoch has taken 0:00:17.566662
Number of used sentences in train = 313
Total loss for epoch 14: 507.167823
Epoch has taken 0:00:17.560639

==================================================================================================
	Training time : 0:45:50.386415
==================================================================================================
	Identification : 0.184

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(7105, 155)
  (lstm): LSTM(189, 40, bidirectional=True)
  (linear1): Linear(in_features=640, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10797.624462
validation loss after epoch 0 : 987.222311
	Epoch 1....
Epoch has taken 0:01:53.412471
Number of used sentences in train = 2074
Total loss for epoch 1: 6498.730797
validation loss after epoch 1 : 1035.243524
	Epoch 2....
Epoch has taken 0:01:53.436705
Number of used sentences in train = 2074
Total loss for epoch 2: 4863.507710
validation loss after epoch 2 : 1184.941851
	Epoch 3....
Epoch has taken 0:01:53.777513
Number of used sentences in train = 2074
Total loss for epoch 3: 4064.502767
validation loss after epoch 3 : 1237.738541
	Epoch 4....
Epoch has taken 0:01:53.766769
Number of used sentences in train = 2074
Total loss for epoch 4: 3660.861731
validation loss after epoch 4 : 1330.490697
	Epoch 5....
Epoch has taken 0:01:53.579386
Number of used sentences in train = 2074
Total loss for epoch 5: 3496.660337
validation loss after epoch 5 : 1436.882430
	Epoch 6....
Epoch has taken 0:01:52.831830
Number of used sentences in train = 2074
Total loss for epoch 6: 3396.950454
validation loss after epoch 6 : 1455.093405
	Epoch 7....
Epoch has taken 0:01:53.735981
Number of used sentences in train = 2074
Total loss for epoch 7: 3322.317340
validation loss after epoch 7 : 1493.587085
	Epoch 8....
Epoch has taken 0:02:11.510810
Number of used sentences in train = 2074
Total loss for epoch 8: 3262.971141
validation loss after epoch 8 : 1529.194989
	Epoch 9....
Epoch has taken 0:02:01.016935
Number of used sentences in train = 2074
Total loss for epoch 9: 3233.504004
validation loss after epoch 9 : 1551.573818
	Epoch 10....
Epoch has taken 0:01:53.847827
Number of used sentences in train = 2074
Total loss for epoch 10: 3208.147678
validation loss after epoch 10 : 1563.764159
	Epoch 11....
Epoch has taken 0:01:53.913478
Number of used sentences in train = 2074
Total loss for epoch 11: 3208.347167
validation loss after epoch 11 : 1601.767271
	Epoch 12....
Epoch has taken 0:01:54.149089
Number of used sentences in train = 2074
Total loss for epoch 12: 3197.788652
validation loss after epoch 12 : 1613.396739
	Epoch 13....
Epoch has taken 0:01:54.213158
Number of used sentences in train = 2074
Total loss for epoch 13: 3188.277475
validation loss after epoch 13 : 1639.092450
	Epoch 14....
Epoch has taken 0:02:05.990346
Number of used sentences in train = 2074
Total loss for epoch 14: 3181.930939
validation loss after epoch 14 : 1647.471736
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(7105, 155)
  (lstm): LSTM(189, 40, bidirectional=True)
  (linear1): Linear(in_features=640, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:04.295148
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1641.499608
	Epoch 1....
Epoch has taken 0:00:12.910773
Number of used sentences in train = 231
Total loss for epoch 1: 603.890378
	Epoch 2....
Epoch has taken 0:00:12.764381
Number of used sentences in train = 231
Total loss for epoch 2: 430.658358
	Epoch 3....
Epoch has taken 0:00:12.831021
Number of used sentences in train = 231
Total loss for epoch 3: 385.318915
	Epoch 4....
Epoch has taken 0:00:12.714256
Number of used sentences in train = 231
Total loss for epoch 4: 371.665672
	Epoch 5....
Epoch has taken 0:00:12.745496
Number of used sentences in train = 231
Total loss for epoch 5: 356.899833
	Epoch 6....
Epoch has taken 0:00:12.813049
Number of used sentences in train = 231
Total loss for epoch 6: 353.434578
	Epoch 7....
Epoch has taken 0:00:12.632491
Number of used sentences in train = 231
Total loss for epoch 7: 352.271456
	Epoch 8....
Epoch has taken 0:00:12.708691
Number of used sentences in train = 231
Total loss for epoch 8: 350.760901
	Epoch 9....
Epoch has taken 0:00:13.829280
Number of used sentences in train = 231
Total loss for epoch 9: 349.719979
	Epoch 10....
Epoch has taken 0:00:13.830536
Number of used sentences in train = 231
Total loss for epoch 10: 349.086011
	Epoch 11....
Epoch has taken 0:00:14.131216
Number of used sentences in train = 231
Total loss for epoch 11: 348.537152
	Epoch 12....
Epoch has taken 0:00:14.027983
Number of used sentences in train = 231
Total loss for epoch 12: 348.117016
	Epoch 13....
Epoch has taken 0:00:13.277227
Number of used sentences in train = 231
Total loss for epoch 13: 347.763782
	Epoch 14....
Epoch has taken 0:00:13.027779
Number of used sentences in train = 231
Total loss for epoch 14: 347.447294
Epoch has taken 0:00:12.982682

==================================================================================================
	Training time : 0:32:31.058297
==================================================================================================
	Identification : 0.168

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 34)
  (w_embeddings): Embedding(17854, 155)
  (lstm): LSTM(189, 40, bidirectional=True)
  (linear1): Linear(in_features=640, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17083.418725
validation loss after epoch 0 : 1602.596049
	Epoch 1....
Epoch has taken 0:03:58.378257
Number of used sentences in train = 3226
Total loss for epoch 1: 11484.379497
validation loss after epoch 1 : 1556.963068
	Epoch 2....
Epoch has taken 0:03:54.741916
Number of used sentences in train = 3226
Total loss for epoch 2: 8973.645297
validation loss after epoch 2 : 1707.115137
	Epoch 3....
Epoch has taken 0:03:59.282885
Number of used sentences in train = 3226
Total loss for epoch 3: 7623.495825
validation loss after epoch 3 : 1897.154834
	Epoch 4....
Epoch has taken 0:03:59.164839
Number of used sentences in train = 3226
Total loss for epoch 4: 6927.260485
validation loss after epoch 4 : 2092.813284
	Epoch 5....
Epoch has taken 0:03:56.905849
Number of used sentences in train = 3226
Total loss for epoch 5: 6575.713235
validation loss after epoch 5 : 2120.680493
	Epoch 6....
Epoch has taken 0:03:57.920161
Number of used sentences in train = 3226
Total loss for epoch 6: 6386.573858
validation loss after epoch 6 : 2260.781829
	Epoch 7....
Epoch has taken 0:03:58.896691
Number of used sentences in train = 3226
Total loss for epoch 7: 6302.932847
validation loss after epoch 7 : 2319.090392
	Epoch 8....
Epoch has taken 0:04:00.005820
Number of used sentences in train = 3226
Total loss for epoch 8: 6251.016201
validation loss after epoch 8 : 2439.076511
	Epoch 9....
Epoch has taken 0:03:59.113915
Number of used sentences in train = 3226
Total loss for epoch 9: 6217.259783
validation loss after epoch 9 : 2431.152139
	Epoch 10....
Epoch has taken 0:03:40.542158
Number of used sentences in train = 3226
Total loss for epoch 10: 6195.102768
validation loss after epoch 10 : 2551.744619
	Epoch 11....
Epoch has taken 0:03:47.808876
Number of used sentences in train = 3226
Total loss for epoch 11: 6192.337492
validation loss after epoch 11 : 2441.191242
	Epoch 12....
Epoch has taken 0:03:58.331432
Number of used sentences in train = 3226
Total loss for epoch 12: 6179.102771
validation loss after epoch 12 : 2513.417909
	Epoch 13....
Epoch has taken 0:03:58.395932
Number of used sentences in train = 3226
Total loss for epoch 13: 6165.553311
validation loss after epoch 13 : 2581.838524
	Epoch 14....
Epoch has taken 0:03:59.112531
Number of used sentences in train = 3226
Total loss for epoch 14: 6163.792125
validation loss after epoch 14 : 2555.254606
	TransitionClassifier(
  (p_embeddings): Embedding(13, 34)
  (w_embeddings): Embedding(17854, 155)
  (lstm): LSTM(189, 40, bidirectional=True)
  (linear1): Linear(in_features=640, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:40.712088
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2725.607594
	Epoch 1....
Epoch has taken 0:00:21.710744
Number of used sentences in train = 359
Total loss for epoch 1: 1049.412008
	Epoch 2....
Epoch has taken 0:00:21.664009
Number of used sentences in train = 359
Total loss for epoch 2: 793.510860
	Epoch 3....
Epoch has taken 0:00:22.036264
Number of used sentences in train = 359
Total loss for epoch 3: 710.861714
	Epoch 4....
Epoch has taken 0:00:21.699044
Number of used sentences in train = 359
Total loss for epoch 4: 688.029849
	Epoch 5....
Epoch has taken 0:00:21.669292
Number of used sentences in train = 359
Total loss for epoch 5: 683.118315
	Epoch 6....
Epoch has taken 0:00:21.698294
Number of used sentences in train = 359
Total loss for epoch 6: 679.883927
	Epoch 7....
Epoch has taken 0:00:21.676661
Number of used sentences in train = 359
Total loss for epoch 7: 676.939551
	Epoch 8....
Epoch has taken 0:00:21.815299
Number of used sentences in train = 359
Total loss for epoch 8: 674.383076
	Epoch 9....
Epoch has taken 0:00:21.658149
Number of used sentences in train = 359
Total loss for epoch 9: 673.235467
	Epoch 10....
Epoch has taken 0:00:21.669993
Number of used sentences in train = 359
Total loss for epoch 10: 672.841796
	Epoch 11....
Epoch has taken 0:00:21.684076
Number of used sentences in train = 359
Total loss for epoch 11: 672.547926
	Epoch 12....
Epoch has taken 0:00:22.205182
Number of used sentences in train = 359
Total loss for epoch 12: 672.313076
	Epoch 13....
Epoch has taken 0:00:21.681117
Number of used sentences in train = 359
Total loss for epoch 13: 672.115652
	Epoch 14....
Epoch has taken 0:00:21.645283
Number of used sentences in train = 359
Total loss for epoch 14: 671.949476
Epoch has taken 0:00:21.700429

==================================================================================================
	Training time : 1:04:16.234018
==================================================================================================
	Identification : 0.13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 112, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 16, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 30, 'lstmDropout': 0.13, 'denseActivation': 'tanh', 'wordDim': 186, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(5902, 186)
  (lstm): LSTM(202, 30, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13614.842796
validation loss after epoch 0 : 1089.956926
	Epoch 1....
Epoch has taken 0:02:59.059264
Number of used sentences in train = 2811
Total loss for epoch 1: 8327.134417
validation loss after epoch 1 : 1057.978114
	Epoch 2....
Epoch has taken 0:02:58.959080
Number of used sentences in train = 2811
Total loss for epoch 2: 6818.964191
validation loss after epoch 2 : 1125.651356
	Epoch 3....
Epoch has taken 0:02:57.363044
Number of used sentences in train = 2811
Total loss for epoch 3: 5956.507308
validation loss after epoch 3 : 1169.805417
	Epoch 4....
Epoch has taken 0:02:59.205254
Number of used sentences in train = 2811
Total loss for epoch 4: 5491.883282
validation loss after epoch 4 : 1220.488145
	Epoch 5....
Epoch has taken 0:02:59.118495
Number of used sentences in train = 2811
Total loss for epoch 5: 5191.773504
validation loss after epoch 5 : 1357.255938
	Epoch 6....
Epoch has taken 0:02:59.230058
Number of used sentences in train = 2811
Total loss for epoch 6: 4944.395981
validation loss after epoch 6 : 1394.179446
	Epoch 7....
Epoch has taken 0:02:59.088639
Number of used sentences in train = 2811
Total loss for epoch 7: 4806.894669
validation loss after epoch 7 : 1419.335633
	Epoch 8....
Epoch has taken 0:02:59.267180
Number of used sentences in train = 2811
Total loss for epoch 8: 4780.482391
validation loss after epoch 8 : 1436.019815
	Epoch 9....
Epoch has taken 0:03:08.687696
Number of used sentences in train = 2811
Total loss for epoch 9: 4695.346808
validation loss after epoch 9 : 1505.341391
	Epoch 10....
Epoch has taken 0:03:11.788020
Number of used sentences in train = 2811
Total loss for epoch 10: 4653.812680
validation loss after epoch 10 : 1532.803821
	Epoch 11....
Epoch has taken 0:03:11.917258
Number of used sentences in train = 2811
Total loss for epoch 11: 4626.299481
validation loss after epoch 11 : 1570.026091
	Epoch 12....
Epoch has taken 0:03:11.762716
Number of used sentences in train = 2811
Total loss for epoch 12: 4600.918082
validation loss after epoch 12 : 1576.319499
	Epoch 13....
Epoch has taken 0:03:09.822839
Number of used sentences in train = 2811
Total loss for epoch 13: 4567.261019
validation loss after epoch 13 : 1611.447395
	Epoch 14....
Epoch has taken 0:03:10.367049
Number of used sentences in train = 2811
Total loss for epoch 14: 4569.039713
validation loss after epoch 14 : 1629.155246
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(5902, 186)
  (lstm): LSTM(202, 30, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:11.523482
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1764.069177
	Epoch 1....
Epoch has taken 0:00:20.536512
Number of used sentences in train = 313
Total loss for epoch 1: 759.779134
	Epoch 2....
Epoch has taken 0:00:20.663952
Number of used sentences in train = 313
Total loss for epoch 2: 612.472708
	Epoch 3....
Epoch has taken 0:00:20.523073
Number of used sentences in train = 313
Total loss for epoch 3: 550.307205
	Epoch 4....
Epoch has taken 0:00:20.391285
Number of used sentences in train = 313
Total loss for epoch 4: 538.719091
	Epoch 5....
Epoch has taken 0:00:20.736947
Number of used sentences in train = 313
Total loss for epoch 5: 514.253135
	Epoch 6....
Epoch has taken 0:00:20.367153
Number of used sentences in train = 313
Total loss for epoch 6: 509.016395
	Epoch 7....
Epoch has taken 0:00:20.671763
Number of used sentences in train = 313
Total loss for epoch 7: 507.277897
	Epoch 8....
Epoch has taken 0:00:20.673198
Number of used sentences in train = 313
Total loss for epoch 8: 503.980885
	Epoch 9....
Epoch has taken 0:00:20.759697
Number of used sentences in train = 313
Total loss for epoch 9: 506.829878
	Epoch 10....
Epoch has taken 0:00:20.597113
Number of used sentences in train = 313
Total loss for epoch 10: 504.495706
	Epoch 11....
Epoch has taken 0:00:20.447513
Number of used sentences in train = 313
Total loss for epoch 11: 504.481553
	Epoch 12....
Epoch has taken 0:00:20.515191
Number of used sentences in train = 313
Total loss for epoch 12: 502.380981
	Epoch 13....
Epoch has taken 0:00:20.450179
Number of used sentences in train = 313
Total loss for epoch 13: 502.249021
	Epoch 14....
Epoch has taken 0:00:20.653471
Number of used sentences in train = 313
Total loss for epoch 14: 501.896337
Epoch has taken 0:00:20.443914

==================================================================================================
	Training time : 0:51:16.119232
==================================================================================================
	Identification : 0.346

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(5581, 186)
  (lstm): LSTM(202, 30, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10887.736801
validation loss after epoch 0 : 898.202015
	Epoch 1....
Epoch has taken 0:02:10.578268
Number of used sentences in train = 2074
Total loss for epoch 1: 6102.230856
validation loss after epoch 1 : 882.861690
	Epoch 2....
Epoch has taken 0:02:08.258692
Number of used sentences in train = 2074
Total loss for epoch 2: 4909.212443
validation loss after epoch 2 : 866.991174
	Epoch 3....
Epoch has taken 0:02:12.240999
Number of used sentences in train = 2074
Total loss for epoch 3: 4246.089544
validation loss after epoch 3 : 999.184818
	Epoch 4....
Epoch has taken 0:02:10.292629
Number of used sentences in train = 2074
Total loss for epoch 4: 3864.980954
validation loss after epoch 4 : 1027.159646
	Epoch 5....
Epoch has taken 0:02:10.889806
Number of used sentences in train = 2074
Total loss for epoch 5: 3636.219753
validation loss after epoch 5 : 1072.327914
	Epoch 6....
Epoch has taken 0:02:12.176392
Number of used sentences in train = 2074
Total loss for epoch 6: 3510.388561
validation loss after epoch 6 : 1127.493347
	Epoch 7....
Epoch has taken 0:02:11.920515
Number of used sentences in train = 2074
Total loss for epoch 7: 3419.809550
validation loss after epoch 7 : 1197.943282
	Epoch 8....
Epoch has taken 0:02:11.539847
Number of used sentences in train = 2074
Total loss for epoch 8: 3355.795342
validation loss after epoch 8 : 1215.712080
	Epoch 9....
Epoch has taken 0:02:11.696954
Number of used sentences in train = 2074
Total loss for epoch 9: 3296.083506
validation loss after epoch 9 : 1262.379427
	Epoch 10....
Epoch has taken 0:02:10.345248
Number of used sentences in train = 2074
Total loss for epoch 10: 3284.868107
validation loss after epoch 10 : 1326.999455
	Epoch 11....
Epoch has taken 0:02:10.334554
Number of used sentences in train = 2074
Total loss for epoch 11: 3246.991759
validation loss after epoch 11 : 1326.085154
	Epoch 12....
Epoch has taken 0:02:11.940035
Number of used sentences in train = 2074
Total loss for epoch 12: 3230.265974
validation loss after epoch 12 : 1335.692842
	Epoch 13....
Epoch has taken 0:02:11.933050
Number of used sentences in train = 2074
Total loss for epoch 13: 3208.788692
validation loss after epoch 13 : 1282.209841
	Epoch 14....
Epoch has taken 0:02:12.024715
Number of used sentences in train = 2074
Total loss for epoch 14: 3194.435109
validation loss after epoch 14 : 1323.456097
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(5581, 186)
  (lstm): LSTM(202, 30, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:10.493932
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1758.800750
	Epoch 1....
Epoch has taken 0:00:13.484235
Number of used sentences in train = 231
Total loss for epoch 1: 634.122585
	Epoch 2....
Epoch has taken 0:00:13.640700
Number of used sentences in train = 231
Total loss for epoch 2: 435.239947
	Epoch 3....
Epoch has taken 0:00:13.468496
Number of used sentences in train = 231
Total loss for epoch 3: 384.607971
	Epoch 4....
Epoch has taken 0:00:13.653425
Number of used sentences in train = 231
Total loss for epoch 4: 367.147006
	Epoch 5....
Epoch has taken 0:00:13.448137
Number of used sentences in train = 231
Total loss for epoch 5: 361.460260
	Epoch 6....
Epoch has taken 0:00:13.576970
Number of used sentences in train = 231
Total loss for epoch 6: 354.330981
	Epoch 7....
Epoch has taken 0:00:13.618731
Number of used sentences in train = 231
Total loss for epoch 7: 351.359502
	Epoch 8....
Epoch has taken 0:00:13.524547
Number of used sentences in train = 231
Total loss for epoch 8: 348.809740
	Epoch 9....
Epoch has taken 0:00:13.687846
Number of used sentences in train = 231
Total loss for epoch 9: 347.894814
	Epoch 10....
Epoch has taken 0:00:13.452750
Number of used sentences in train = 231
Total loss for epoch 10: 346.598577
	Epoch 11....
Epoch has taken 0:00:13.719411
Number of used sentences in train = 231
Total loss for epoch 11: 352.322070
	Epoch 12....
Epoch has taken 0:00:13.364914
Number of used sentences in train = 231
Total loss for epoch 12: 346.231379
	Epoch 13....
Epoch has taken 0:00:13.439106
Number of used sentences in train = 231
Total loss for epoch 13: 346.364331
	Epoch 14....
Epoch has taken 0:00:13.647473
Number of used sentences in train = 231
Total loss for epoch 14: 346.014235
Epoch has taken 0:00:13.562758

==================================================================================================
	Training time : 0:36:10.302600
==================================================================================================
	Identification : 0.312

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(6861, 186)
  (lstm): LSTM(202, 30, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18247.149347
validation loss after epoch 0 : 1543.497166
	Epoch 1....
Epoch has taken 0:04:55.688178
Number of used sentences in train = 3226
Total loss for epoch 1: 11552.219000
validation loss after epoch 1 : 1448.904315
	Epoch 2....
Epoch has taken 0:04:52.389801
Number of used sentences in train = 3226
Total loss for epoch 2: 9698.003550
validation loss after epoch 2 : 1545.192660
	Epoch 3....
Epoch has taken 0:04:18.035958
Number of used sentences in train = 3226
Total loss for epoch 3: 8680.492425
validation loss after epoch 3 : 1675.167277
	Epoch 4....
Epoch has taken 0:04:18.378026
Number of used sentences in train = 3226
Total loss for epoch 4: 7911.664513
validation loss after epoch 4 : 1732.357783
	Epoch 5....
Epoch has taken 0:04:31.546050
Number of used sentences in train = 3226
Total loss for epoch 5: 7445.459573
validation loss after epoch 5 : 1847.905655
	Epoch 6....
Epoch has taken 0:04:50.674588
Number of used sentences in train = 3226
Total loss for epoch 6: 7181.775960
validation loss after epoch 6 : 1807.108388
	Epoch 7....
Epoch has taken 0:04:48.231854
Number of used sentences in train = 3226
Total loss for epoch 7: 6924.750456
validation loss after epoch 7 : 2028.852696
	Epoch 8....
Epoch has taken 0:04:24.076830
Number of used sentences in train = 3226
Total loss for epoch 8: 6770.350572
validation loss after epoch 8 : 2107.935777
	Epoch 9....
Epoch has taken 0:04:06.799456
Number of used sentences in train = 3226
Total loss for epoch 9: 6630.187900
validation loss after epoch 9 : 2177.678183
	Epoch 10....
Epoch has taken 0:04:32.956477
Number of used sentences in train = 3226
Total loss for epoch 10: 6509.288084
validation loss after epoch 10 : 2214.791026
	Epoch 11....
Epoch has taken 0:04:14.929082
Number of used sentences in train = 3226
Total loss for epoch 11: 6497.176805
validation loss after epoch 11 : 2177.159956
	Epoch 12....
Epoch has taken 0:04:15.975287
Number of used sentences in train = 3226
Total loss for epoch 12: 6422.718476
validation loss after epoch 12 : 2357.656817
	Epoch 13....
Epoch has taken 0:04:17.710407
Number of used sentences in train = 3226
Total loss for epoch 13: 6344.859226
validation loss after epoch 13 : 2426.875699
	Epoch 14....
Epoch has taken 0:04:18.294486
Number of used sentences in train = 3226
Total loss for epoch 14: 6320.327004
validation loss after epoch 14 : 2428.235104
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(6861, 186)
  (lstm): LSTM(202, 30, num_layers=2, dropout=0.13, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:17.901114
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 3140.752513
	Epoch 1....
Epoch has taken 0:00:25.187604
Number of used sentences in train = 359
Total loss for epoch 1: 1100.272423
	Epoch 2....
Epoch has taken 0:00:25.170058
Number of used sentences in train = 359
Total loss for epoch 2: 862.726025
	Epoch 3....
Epoch has taken 0:00:25.273681
Number of used sentences in train = 359
Total loss for epoch 3: 757.622117
	Epoch 4....
Epoch has taken 0:00:25.151869
Number of used sentences in train = 359
Total loss for epoch 4: 703.501851
	Epoch 5....
Epoch has taken 0:00:25.114765
Number of used sentences in train = 359
Total loss for epoch 5: 688.633588
	Epoch 6....
Epoch has taken 0:00:25.183544
Number of used sentences in train = 359
Total loss for epoch 6: 686.456755
	Epoch 7....
Epoch has taken 0:00:25.202869
Number of used sentences in train = 359
Total loss for epoch 7: 681.257203
	Epoch 8....
Epoch has taken 0:00:25.241723
Number of used sentences in train = 359
Total loss for epoch 8: 676.471186
	Epoch 9....
Epoch has taken 0:00:25.129000
Number of used sentences in train = 359
Total loss for epoch 9: 677.179723
	Epoch 10....
Epoch has taken 0:00:25.120114
Number of used sentences in train = 359
Total loss for epoch 10: 679.760020
	Epoch 11....
Epoch has taken 0:00:25.415434
Number of used sentences in train = 359
Total loss for epoch 11: 679.190871
	Epoch 12....
Epoch has taken 0:00:25.388376
Number of used sentences in train = 359
Total loss for epoch 12: 673.249838
	Epoch 13....
Epoch has taken 0:00:25.393095
Number of used sentences in train = 359
Total loss for epoch 13: 672.064111
	Epoch 14....
Epoch has taken 0:00:25.516175
Number of used sentences in train = 359
Total loss for epoch 14: 672.172709
Epoch has taken 0:00:25.398840

==================================================================================================
	Training time : 1:13:23.137507
==================================================================================================
	Identification : 0.175

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 14, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 61, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 25, 'lstmDropout': 0.37, 'denseActivation': 'tanh', 'wordDim': 192, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 61)
  (w_embeddings): Embedding(9356, 192)
  (lstm): LSTM(253, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13338.610003
validation loss after epoch 0 : 1258.121639
	Epoch 1....
Epoch has taken 0:03:16.102338
Number of used sentences in train = 2811
Total loss for epoch 1: 9110.848937
validation loss after epoch 1 : 1233.291066
	Epoch 2....
Epoch has taken 0:02:53.974613
Number of used sentences in train = 2811
Total loss for epoch 2: 7167.329116
validation loss after epoch 2 : 1302.867874
	Epoch 3....
Epoch has taken 0:02:56.719447
Number of used sentences in train = 2811
Total loss for epoch 3: 6136.011876
validation loss after epoch 3 : 1338.841629
	Epoch 4....
Epoch has taken 0:02:58.835201
Number of used sentences in train = 2811
Total loss for epoch 4: 5588.580297
validation loss after epoch 4 : 1417.343448
	Epoch 5....
Epoch has taken 0:02:59.417049
Number of used sentences in train = 2811
Total loss for epoch 5: 5233.147768
validation loss after epoch 5 : 1500.477929
	Epoch 6....
Epoch has taken 0:03:25.204692
Number of used sentences in train = 2811
Total loss for epoch 6: 5042.828015
validation loss after epoch 6 : 1560.096769
	Epoch 7....
Epoch has taken 0:02:58.785098
Number of used sentences in train = 2811
Total loss for epoch 7: 4884.341789
validation loss after epoch 7 : 1622.787338
	Epoch 8....
Epoch has taken 0:02:54.741018
Number of used sentences in train = 2811
Total loss for epoch 8: 4774.883641
validation loss after epoch 8 : 1659.664046
	Epoch 9....
Epoch has taken 0:02:45.993664
Number of used sentences in train = 2811
Total loss for epoch 9: 4716.355322
validation loss after epoch 9 : 1697.765915
	Epoch 10....
Epoch has taken 0:02:46.762836
Number of used sentences in train = 2811
Total loss for epoch 10: 4662.248314
validation loss after epoch 10 : 1726.234018
	Epoch 11....
Epoch has taken 0:03:12.515124
Number of used sentences in train = 2811
Total loss for epoch 11: 4629.807056
validation loss after epoch 11 : 1770.491834
	Epoch 12....
Epoch has taken 0:02:46.351760
Number of used sentences in train = 2811
Total loss for epoch 12: 4598.736102
validation loss after epoch 12 : 1788.754704
	Epoch 13....
Epoch has taken 0:02:45.091599
Number of used sentences in train = 2811
Total loss for epoch 13: 4579.918473
validation loss after epoch 13 : 1823.122236
	Epoch 14....
Epoch has taken 0:02:45.936301
Number of used sentences in train = 2811
Total loss for epoch 14: 4566.135724
validation loss after epoch 14 : 1839.463018
	TransitionClassifier(
  (p_embeddings): Embedding(18, 61)
  (w_embeddings): Embedding(9356, 192)
  (lstm): LSTM(253, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.464961
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1605.082714
	Epoch 1....
Epoch has taken 0:00:17.579855
Number of used sentences in train = 313
Total loss for epoch 1: 835.116513
	Epoch 2....
Epoch has taken 0:00:17.600507
Number of used sentences in train = 313
Total loss for epoch 2: 633.431287
	Epoch 3....
Epoch has taken 0:00:17.575718
Number of used sentences in train = 313
Total loss for epoch 3: 564.191061
	Epoch 4....
Epoch has taken 0:00:17.619449
Number of used sentences in train = 313
Total loss for epoch 4: 534.152737
	Epoch 5....
Epoch has taken 0:00:17.609272
Number of used sentences in train = 313
Total loss for epoch 5: 524.358948
	Epoch 6....
Epoch has taken 0:00:17.595686
Number of used sentences in train = 313
Total loss for epoch 6: 514.351602
	Epoch 7....
Epoch has taken 0:00:17.615238
Number of used sentences in train = 313
Total loss for epoch 7: 510.936704
	Epoch 8....
Epoch has taken 0:00:17.591820
Number of used sentences in train = 313
Total loss for epoch 8: 508.844350
	Epoch 9....
Epoch has taken 0:00:17.622059
Number of used sentences in train = 313
Total loss for epoch 9: 505.292326
	Epoch 10....
Epoch has taken 0:00:17.608554
Number of used sentences in train = 313
Total loss for epoch 10: 503.925458
	Epoch 11....
Epoch has taken 0:00:17.619512
Number of used sentences in train = 313
Total loss for epoch 11: 503.416764
	Epoch 12....
Epoch has taken 0:00:17.649288
Number of used sentences in train = 313
Total loss for epoch 12: 503.014177
	Epoch 13....
Epoch has taken 0:00:17.609056
Number of used sentences in train = 313
Total loss for epoch 13: 502.622491
	Epoch 14....
Epoch has taken 0:00:17.621185
Number of used sentences in train = 313
Total loss for epoch 14: 502.298341
Epoch has taken 0:00:17.599644

==================================================================================================
	Training time : 0:48:37.543774
==================================================================================================
	Identification : 0.183

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 61)
  (w_embeddings): Embedding(7116, 192)
  (lstm): LSTM(253, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10573.710472
validation loss after epoch 0 : 1009.126102
	Epoch 1....
Epoch has taken 0:01:52.824610
Number of used sentences in train = 2074
Total loss for epoch 1: 6760.998715
validation loss after epoch 1 : 950.182238
	Epoch 2....
Epoch has taken 0:01:53.630896
Number of used sentences in train = 2074
Total loss for epoch 2: 5281.523596
validation loss after epoch 2 : 1050.785601
	Epoch 3....
Epoch has taken 0:01:53.782755
Number of used sentences in train = 2074
Total loss for epoch 3: 4439.506416
validation loss after epoch 3 : 1134.946106
	Epoch 4....
Epoch has taken 0:01:53.830161
Number of used sentences in train = 2074
Total loss for epoch 4: 3996.505959
validation loss after epoch 4 : 1149.134187
	Epoch 5....
Epoch has taken 0:01:52.537645
Number of used sentences in train = 2074
Total loss for epoch 5: 3719.542668
validation loss after epoch 5 : 1247.239742
	Epoch 6....
Epoch has taken 0:01:52.770163
Number of used sentences in train = 2074
Total loss for epoch 6: 3552.592152
validation loss after epoch 6 : 1285.423043
	Epoch 7....
Epoch has taken 0:01:53.925485
Number of used sentences in train = 2074
Total loss for epoch 7: 3464.785953
validation loss after epoch 7 : 1326.508390
	Epoch 8....
Epoch has taken 0:02:11.603205
Number of used sentences in train = 2074
Total loss for epoch 8: 3385.309275
validation loss after epoch 8 : 1359.976017
	Epoch 9....
Epoch has taken 0:01:52.531045
Number of used sentences in train = 2074
Total loss for epoch 9: 3325.455098
validation loss after epoch 9 : 1411.465300
	Epoch 10....
Epoch has taken 0:01:52.706200
Number of used sentences in train = 2074
Total loss for epoch 10: 3287.718470
validation loss after epoch 10 : 1446.398062
	Epoch 11....
Epoch has taken 0:01:53.831272
Number of used sentences in train = 2074
Total loss for epoch 11: 3266.576057
validation loss after epoch 11 : 1472.857664
	Epoch 12....
Epoch has taken 0:01:53.799617
Number of used sentences in train = 2074
Total loss for epoch 12: 3244.739546
validation loss after epoch 12 : 1475.790491
	Epoch 13....
Epoch has taken 0:02:11.514740
Number of used sentences in train = 2074
Total loss for epoch 13: 3225.906456
validation loss after epoch 13 : 1509.952564
	Epoch 14....
Epoch has taken 0:01:53.858481
Number of used sentences in train = 2074
Total loss for epoch 14: 3219.866667
validation loss after epoch 14 : 1524.669876
	TransitionClassifier(
  (p_embeddings): Embedding(18, 61)
  (w_embeddings): Embedding(7116, 192)
  (lstm): LSTM(253, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:52.947272
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1560.269596
	Epoch 1....
Epoch has taken 0:00:11.578968
Number of used sentences in train = 231
Total loss for epoch 1: 676.527546
	Epoch 2....
Epoch has taken 0:00:11.609668
Number of used sentences in train = 231
Total loss for epoch 2: 491.252981
	Epoch 3....
Epoch has taken 0:00:11.592021
Number of used sentences in train = 231
Total loss for epoch 3: 413.432254
	Epoch 4....
Epoch has taken 0:00:11.622169
Number of used sentences in train = 231
Total loss for epoch 4: 377.769937
	Epoch 5....
Epoch has taken 0:00:11.591245
Number of used sentences in train = 231
Total loss for epoch 5: 364.034068
	Epoch 6....
Epoch has taken 0:00:11.610406
Number of used sentences in train = 231
Total loss for epoch 6: 356.032546
	Epoch 7....
Epoch has taken 0:00:11.592313
Number of used sentences in train = 231
Total loss for epoch 7: 352.567351
	Epoch 8....
Epoch has taken 0:00:11.609628
Number of used sentences in train = 231
Total loss for epoch 8: 350.810401
	Epoch 9....
Epoch has taken 0:00:11.573875
Number of used sentences in train = 231
Total loss for epoch 9: 349.699396
	Epoch 10....
Epoch has taken 0:00:11.606992
Number of used sentences in train = 231
Total loss for epoch 10: 348.984205
	Epoch 11....
Epoch has taken 0:00:11.597495
Number of used sentences in train = 231
Total loss for epoch 11: 348.577057
	Epoch 12....
Epoch has taken 0:00:11.612339
Number of used sentences in train = 231
Total loss for epoch 12: 348.192116
	Epoch 13....
Epoch has taken 0:00:11.624165
Number of used sentences in train = 231
Total loss for epoch 13: 347.904866
	Epoch 14....
Epoch has taken 0:00:11.628403
Number of used sentences in train = 231
Total loss for epoch 14: 347.667834
Epoch has taken 0:00:11.611379

==================================================================================================
	Training time : 0:31:50.504889
==================================================================================================
	Identification : 0.161

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 61)
  (w_embeddings): Embedding(18012, 192)
  (lstm): LSTM(253, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18756.038031
validation loss after epoch 0 : 1759.455948
	Epoch 1....
Epoch has taken 0:03:41.657728
Number of used sentences in train = 3226
Total loss for epoch 1: 12797.837576
validation loss after epoch 1 : 1645.600701
	Epoch 2....
Epoch has taken 0:03:41.385970
Number of used sentences in train = 3226
Total loss for epoch 2: 10367.236590
validation loss after epoch 2 : 1679.874321
	Epoch 3....
Epoch has taken 0:03:44.109035
Number of used sentences in train = 3226
Total loss for epoch 3: 9002.158192
validation loss after epoch 3 : 1779.288607
	Epoch 4....
Epoch has taken 0:03:44.509557
Number of used sentences in train = 3226
Total loss for epoch 4: 8101.239833
validation loss after epoch 4 : 1884.813673
	Epoch 5....
Epoch has taken 0:03:42.130012
Number of used sentences in train = 3226
Total loss for epoch 5: 7425.343211
validation loss after epoch 5 : 2030.350629
	Epoch 6....
Epoch has taken 0:03:40.494543
Number of used sentences in train = 3226
Total loss for epoch 6: 7048.343169
validation loss after epoch 6 : 2064.871998
	Epoch 7....
Epoch has taken 0:03:42.029994
Number of used sentences in train = 3226
Total loss for epoch 7: 6785.810627
validation loss after epoch 7 : 2186.553753
	Epoch 8....
Epoch has taken 0:04:15.929067
Number of used sentences in train = 3226
Total loss for epoch 8: 6583.202979
validation loss after epoch 8 : 2277.584743
	Epoch 9....
Epoch has taken 0:03:41.322144
Number of used sentences in train = 3226
Total loss for epoch 9: 6463.603918
validation loss after epoch 9 : 2382.293475
	Epoch 10....
Epoch has taken 0:03:40.203598
Number of used sentences in train = 3226
Total loss for epoch 10: 6403.014851
validation loss after epoch 10 : 2388.797225
	Epoch 11....
Epoch has taken 0:03:41.640953
Number of used sentences in train = 3226
Total loss for epoch 11: 6361.415649
validation loss after epoch 11 : 2445.354186
	Epoch 12....
Epoch has taken 0:03:41.469705
Number of used sentences in train = 3226
Total loss for epoch 12: 6314.096081
validation loss after epoch 12 : 2499.100733
	Epoch 13....
Epoch has taken 0:03:41.254452
Number of used sentences in train = 3226
Total loss for epoch 13: 6285.237990
validation loss after epoch 13 : 2521.023073
	Epoch 14....
Epoch has taken 0:03:41.070534
Number of used sentences in train = 3226
Total loss for epoch 14: 6251.071633
validation loss after epoch 14 : 2599.644661
	TransitionClassifier(
  (p_embeddings): Embedding(13, 61)
  (w_embeddings): Embedding(18012, 192)
  (lstm): LSTM(253, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:40.275350
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2362.076484
	Epoch 1....
Epoch has taken 0:00:21.744739
Number of used sentences in train = 359
Total loss for epoch 1: 1213.817340
	Epoch 2....
Epoch has taken 0:00:21.741549
Number of used sentences in train = 359
Total loss for epoch 2: 909.770942
	Epoch 3....
Epoch has taken 0:00:21.774821
Number of used sentences in train = 359
Total loss for epoch 3: 773.162612
	Epoch 4....
Epoch has taken 0:00:21.743485
Number of used sentences in train = 359
Total loss for epoch 4: 725.938715
	Epoch 5....
Epoch has taken 0:00:21.750575
Number of used sentences in train = 359
Total loss for epoch 5: 716.496351
	Epoch 6....
Epoch has taken 0:00:21.757058
Number of used sentences in train = 359
Total loss for epoch 6: 699.946497
	Epoch 7....
Epoch has taken 0:00:21.744463
Number of used sentences in train = 359
Total loss for epoch 7: 691.720273
	Epoch 8....
Epoch has taken 0:00:21.730721
Number of used sentences in train = 359
Total loss for epoch 8: 686.444532
	Epoch 9....
Epoch has taken 0:00:21.745144
Number of used sentences in train = 359
Total loss for epoch 9: 682.008729
	Epoch 10....
Epoch has taken 0:00:21.722205
Number of used sentences in train = 359
Total loss for epoch 10: 678.565464
	Epoch 11....
Epoch has taken 0:00:21.741373
Number of used sentences in train = 359
Total loss for epoch 11: 676.455960
	Epoch 12....
Epoch has taken 0:00:21.747664
Number of used sentences in train = 359
Total loss for epoch 12: 675.679743
	Epoch 13....
Epoch has taken 0:00:21.726907
Number of used sentences in train = 359
Total loss for epoch 13: 674.909607
	Epoch 14....
Epoch has taken 0:00:21.723734
Number of used sentences in train = 359
Total loss for epoch 14: 674.360276
Epoch has taken 0:00:21.759050

==================================================================================================
	Training time : 1:01:26.343051
==================================================================================================
	Identification : 0.37

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 117, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 37, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 78, 'lstmDropout': 0.21, 'denseActivation': 'tanh', 'wordDim': 53, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(9349, 53)
  (lstm): LSTM(90, 78, bidirectional=True)
  (linear1): Linear(in_features=1248, out_features=117, bias=True)
  (linear2): Linear(in_features=117, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14884.326149
validation loss after epoch 0 : 1175.611533
	Epoch 1....
Epoch has taken 0:02:46.104334
Number of used sentences in train = 2811
Total loss for epoch 1: 9313.730502
validation loss after epoch 1 : 1087.866152
	Epoch 2....
Epoch has taken 0:02:46.241345
Number of used sentences in train = 2811
Total loss for epoch 2: 7481.316285
validation loss after epoch 2 : 1167.898476
	Epoch 3....
Epoch has taken 0:02:46.698811
Number of used sentences in train = 2811
Total loss for epoch 3: 6355.288294
validation loss after epoch 3 : 1112.141577
	Epoch 4....
Epoch has taken 0:02:46.445424
Number of used sentences in train = 2811
Total loss for epoch 4: 5606.736120
validation loss after epoch 4 : 1168.238372
	Epoch 5....
Epoch has taken 0:02:48.896645
Number of used sentences in train = 2811
Total loss for epoch 5: 5158.964113
validation loss after epoch 5 : 1263.692601
	Epoch 6....
Epoch has taken 0:02:48.424451
Number of used sentences in train = 2811
Total loss for epoch 6: 4883.170410
validation loss after epoch 6 : 1389.914764
	Epoch 7....
Epoch has taken 0:02:48.676889
Number of used sentences in train = 2811
Total loss for epoch 7: 4714.668567
validation loss after epoch 7 : 1419.148135
	Epoch 8....
Epoch has taken 0:02:46.597810
Number of used sentences in train = 2811
Total loss for epoch 8: 4623.035226
validation loss after epoch 8 : 1459.638130
	Epoch 9....
Epoch has taken 0:02:47.026899
Number of used sentences in train = 2811
Total loss for epoch 9: 4572.617035
validation loss after epoch 9 : 1491.352547
	Epoch 10....
Epoch has taken 0:02:59.652446
Number of used sentences in train = 2811
Total loss for epoch 10: 4539.559221
validation loss after epoch 10 : 1559.157349
	Epoch 11....
Epoch has taken 0:03:06.144473
Number of used sentences in train = 2811
Total loss for epoch 11: 4525.699166
validation loss after epoch 11 : 1551.741900
	Epoch 12....
Epoch has taken 0:03:06.116356
Number of used sentences in train = 2811
Total loss for epoch 12: 4509.644664
validation loss after epoch 12 : 1593.086629
	Epoch 13....
Epoch has taken 0:03:05.575823
Number of used sentences in train = 2811
Total loss for epoch 13: 4502.214266
validation loss after epoch 13 : 1624.138277
	Epoch 14....
Epoch has taken 0:02:50.440874
Number of used sentences in train = 2811
Total loss for epoch 14: 4496.472790
validation loss after epoch 14 : 1624.962055
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(9349, 53)
  (lstm): LSTM(90, 78, bidirectional=True)
  (linear1): Linear(in_features=1248, out_features=117, bias=True)
  (linear2): Linear(in_features=117, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.135290
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2212.129174
	Epoch 1....
Epoch has taken 0:00:17.691621
Number of used sentences in train = 313
Total loss for epoch 1: 789.685616
	Epoch 2....
Epoch has taken 0:00:17.705557
