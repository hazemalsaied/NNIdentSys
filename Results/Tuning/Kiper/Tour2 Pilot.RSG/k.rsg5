INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11516.642882
validation loss after epoch 0 : 870.156004
	Epoch 1....
Epoch has taken 0:02:48.874164
Number of used sentences in train = 2811
Total loss for epoch 1: 7625.484057
validation loss after epoch 1 : 836.592843
	Epoch 2....
Epoch has taken 0:02:51.516168
Number of used sentences in train = 2811
Total loss for epoch 2: 6861.480819
validation loss after epoch 2 : 799.198269
	Epoch 3....
Epoch has taken 0:02:51.801828
Number of used sentences in train = 2811
Total loss for epoch 3: 6386.207736
validation loss after epoch 3 : 810.271608
	Epoch 4....
Epoch has taken 0:02:51.916629
Number of used sentences in train = 2811
Total loss for epoch 4: 6082.850177
validation loss after epoch 4 : 805.193180
	Epoch 5....
Epoch has taken 0:02:51.807752
Number of used sentences in train = 2811
Total loss for epoch 5: 5790.993169
validation loss after epoch 5 : 830.015203
	Epoch 6....
Epoch has taken 0:02:51.762171
Number of used sentences in train = 2811
Total loss for epoch 6: 5568.581025
validation loss after epoch 6 : 858.487507
	Epoch 7....
Epoch has taken 0:02:51.674836
Number of used sentences in train = 2811
Total loss for epoch 7: 5381.460029
validation loss after epoch 7 : 885.863126
	Epoch 8....
Epoch has taken 0:02:49.949167
Number of used sentences in train = 2811
Total loss for epoch 8: 5223.274376
validation loss after epoch 8 : 925.069114
	Epoch 9....
Epoch has taken 0:02:51.666880
Number of used sentences in train = 2811
Total loss for epoch 9: 5088.302332
validation loss after epoch 9 : 884.737816
	Epoch 10....
Epoch has taken 0:02:51.862301
Number of used sentences in train = 2811
Total loss for epoch 10: 5029.031427
validation loss after epoch 10 : 918.996926
	Epoch 11....
Epoch has taken 0:02:51.198941
Number of used sentences in train = 2811
Total loss for epoch 11: 4922.858214
validation loss after epoch 11 : 941.225250
	Epoch 12....
Epoch has taken 0:02:50.902347
Number of used sentences in train = 2811
Total loss for epoch 12: 4872.880331
validation loss after epoch 12 : 962.155792
	Epoch 13....
Epoch has taken 0:02:51.966801
Number of used sentences in train = 2811
Total loss for epoch 13: 4820.930986
validation loss after epoch 13 : 977.179301
	Epoch 14....
Epoch has taken 0:02:51.937651
Number of used sentences in train = 2811
Total loss for epoch 14: 4788.646313
validation loss after epoch 14 : 987.988089
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.704831
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1307.187780
	Epoch 1....
Epoch has taken 0:00:18.213732
Number of used sentences in train = 313
Total loss for epoch 1: 785.149563
	Epoch 2....
Epoch has taken 0:00:18.204247
Number of used sentences in train = 313
Total loss for epoch 2: 679.559570
	Epoch 3....
Epoch has taken 0:00:18.218276
Number of used sentences in train = 313
Total loss for epoch 3: 615.347435
	Epoch 4....
Epoch has taken 0:00:18.212439
Number of used sentences in train = 313
Total loss for epoch 4: 591.094233
	Epoch 5....
Epoch has taken 0:00:18.210960
Number of used sentences in train = 313
Total loss for epoch 5: 573.812155
	Epoch 6....
Epoch has taken 0:00:18.217516
Number of used sentences in train = 313
Total loss for epoch 6: 566.726883
	Epoch 7....
Epoch has taken 0:00:18.470215
Number of used sentences in train = 313
Total loss for epoch 7: 557.023820
	Epoch 8....
Epoch has taken 0:00:18.206604
Number of used sentences in train = 313
Total loss for epoch 8: 552.637328
	Epoch 9....
Epoch has taken 0:00:18.222433
Number of used sentences in train = 313
Total loss for epoch 9: 547.721515
	Epoch 10....
Epoch has taken 0:00:18.239765
Number of used sentences in train = 313
Total loss for epoch 10: 546.569711
	Epoch 11....
Epoch has taken 0:00:18.213513
Number of used sentences in train = 313
Total loss for epoch 11: 541.432191
	Epoch 12....
Epoch has taken 0:00:18.231222
Number of used sentences in train = 313
Total loss for epoch 12: 540.064530
	Epoch 13....
Epoch has taken 0:00:18.230575
Number of used sentences in train = 313
Total loss for epoch 13: 543.703142
	Epoch 14....
Epoch has taken 0:00:18.213360
Number of used sentences in train = 313
Total loss for epoch 14: 549.610592
Epoch has taken 0:00:18.191719

==================================================================================================
	Training time : 0:47:29.859557
==================================================================================================
	Identification : 0.417

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9483.566756
validation loss after epoch 0 : 654.833909
	Epoch 1....
Epoch has taken 0:01:59.241582
Number of used sentences in train = 2074
Total loss for epoch 1: 5381.466573
validation loss after epoch 1 : 650.381577
	Epoch 2....
Epoch has taken 0:01:57.874074
Number of used sentences in train = 2074
Total loss for epoch 2: 4725.244909
validation loss after epoch 2 : 630.232702
	Epoch 3....
Epoch has taken 0:01:59.330517
Number of used sentences in train = 2074
Total loss for epoch 3: 4239.193058
validation loss after epoch 3 : 660.569420
	Epoch 4....
Epoch has taken 0:01:59.208716
Number of used sentences in train = 2074
Total loss for epoch 4: 3900.380597
validation loss after epoch 4 : 666.009353
	Epoch 5....
Epoch has taken 0:01:59.313773
Number of used sentences in train = 2074
Total loss for epoch 5: 3684.419699
validation loss after epoch 5 : 707.498701
	Epoch 6....
Epoch has taken 0:01:59.453442
Number of used sentences in train = 2074
Total loss for epoch 6: 3496.160044
validation loss after epoch 6 : 775.059514
	Epoch 7....
Epoch has taken 0:01:58.902569
Number of used sentences in train = 2074
Total loss for epoch 7: 3384.560502
validation loss after epoch 7 : 721.907809
	Epoch 8....
Epoch has taken 0:01:59.793456
Number of used sentences in train = 2074
Total loss for epoch 8: 3326.028897
validation loss after epoch 8 : 770.268828
	Epoch 9....
Epoch has taken 0:02:00.451268
Number of used sentences in train = 2074
Total loss for epoch 9: 3299.757787
validation loss after epoch 9 : 758.710927
	Epoch 10....
Epoch has taken 0:02:10.200639
Number of used sentences in train = 2074
Total loss for epoch 10: 3272.951834
validation loss after epoch 10 : 775.777453
	Epoch 11....
Epoch has taken 0:02:10.747365
Number of used sentences in train = 2074
Total loss for epoch 11: 3258.211090
validation loss after epoch 11 : 784.723701
	Epoch 12....
Epoch has taken 0:02:10.975288
Number of used sentences in train = 2074
Total loss for epoch 12: 3242.955455
validation loss after epoch 12 : 780.938734
	Epoch 13....
Epoch has taken 0:02:10.896398
Number of used sentences in train = 2074
Total loss for epoch 13: 3224.898465
validation loss after epoch 13 : 795.245364
	Epoch 14....
Epoch has taken 0:02:10.906910
Number of used sentences in train = 2074
Total loss for epoch 14: 3208.217678
validation loss after epoch 14 : 808.028841
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:10.801502
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1479.819466
	Epoch 1....
Epoch has taken 0:00:13.461280
Number of used sentences in train = 231
Total loss for epoch 1: 604.484587
	Epoch 2....
Epoch has taken 0:00:13.424825
Number of used sentences in train = 231
Total loss for epoch 2: 502.779872
	Epoch 3....
Epoch has taken 0:00:13.449052
Number of used sentences in train = 231
Total loss for epoch 3: 393.784270
	Epoch 4....
Epoch has taken 0:00:13.422591
Number of used sentences in train = 231
Total loss for epoch 4: 363.923214
	Epoch 5....
Epoch has taken 0:00:13.449049
Number of used sentences in train = 231
Total loss for epoch 5: 354.460499
	Epoch 6....
Epoch has taken 0:00:13.427154
Number of used sentences in train = 231
Total loss for epoch 6: 350.070197
	Epoch 7....
Epoch has taken 0:00:13.428302
Number of used sentences in train = 231
Total loss for epoch 7: 348.814038
	Epoch 8....
Epoch has taken 0:00:13.428498
Number of used sentences in train = 231
Total loss for epoch 8: 348.176302
	Epoch 9....
Epoch has taken 0:00:13.421610
Number of used sentences in train = 231
Total loss for epoch 9: 347.856295
	Epoch 10....
Epoch has taken 0:00:13.460656
Number of used sentences in train = 231
Total loss for epoch 10: 347.392049
	Epoch 11....
Epoch has taken 0:00:13.424724
Number of used sentences in train = 231
Total loss for epoch 11: 347.068073
	Epoch 12....
Epoch has taken 0:00:13.455685
Number of used sentences in train = 231
Total loss for epoch 12: 346.754337
	Epoch 13....
Epoch has taken 0:00:13.446397
Number of used sentences in train = 231
Total loss for epoch 13: 346.632392
	Epoch 14....
Epoch has taken 0:00:13.424295
Number of used sentences in train = 231
Total loss for epoch 14: 346.493361
Epoch has taken 0:00:13.445854

==================================================================================================
	Training time : 0:34:20.005677
==================================================================================================
	Identification : 0.383

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20941.887290
validation loss after epoch 0 : 1125.183122
	Epoch 1....
Epoch has taken 0:04:11.921816
Number of used sentences in train = 3226
Total loss for epoch 1: 9908.741483
validation loss after epoch 1 : 1077.553914
	Epoch 2....
Epoch has taken 0:04:13.019980
Number of used sentences in train = 3226
Total loss for epoch 2: 9071.885227
validation loss after epoch 2 : 1035.231926
	Epoch 3....
Epoch has taken 0:04:12.172690
Number of used sentences in train = 3226
Total loss for epoch 3: 8513.579493
validation loss after epoch 3 : 1041.584078
	Epoch 4....
Epoch has taken 0:04:14.525291
Number of used sentences in train = 3226
Total loss for epoch 4: 8093.194879
validation loss after epoch 4 : 1072.487437
	Epoch 5....
Epoch has taken 0:04:14.270124
Number of used sentences in train = 3226
Total loss for epoch 5: 7774.557379
validation loss after epoch 5 : 1102.083838
	Epoch 6....
Epoch has taken 0:03:53.800834
Number of used sentences in train = 3226
Total loss for epoch 6: 7492.110587
validation loss after epoch 6 : 1175.641186
	Epoch 7....
Epoch has taken 0:04:02.373092
Number of used sentences in train = 3226
Total loss for epoch 7: 7281.778668
validation loss after epoch 7 : 1309.567299
	Epoch 8....
Epoch has taken 0:04:13.882507
Number of used sentences in train = 3226
Total loss for epoch 8: 7122.528643
validation loss after epoch 8 : 1168.795651
	Epoch 9....
Epoch has taken 0:03:51.770148
Number of used sentences in train = 3226
Total loss for epoch 9: 6949.571895
validation loss after epoch 9 : 1278.815143
	Epoch 10....
Epoch has taken 0:03:51.287046
Number of used sentences in train = 3226
Total loss for epoch 10: 6829.921578
validation loss after epoch 10 : 1312.810788
	Epoch 11....
Epoch has taken 0:03:51.229805
Number of used sentences in train = 3226
Total loss for epoch 11: 6725.445338
validation loss after epoch 11 : 1312.692427
	Epoch 12....
Epoch has taken 0:03:51.504918
Number of used sentences in train = 3226
Total loss for epoch 12: 6641.432133
validation loss after epoch 12 : 1387.339068
	Epoch 13....
Epoch has taken 0:03:51.014482
Number of used sentences in train = 3226
Total loss for epoch 13: 6547.082190
validation loss after epoch 13 : 1371.881423
	Epoch 14....
Epoch has taken 0:03:50.542905
Number of used sentences in train = 3226
Total loss for epoch 14: 6515.589683
validation loss after epoch 14 : 1471.132691
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:51.296890
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1680.748902
	Epoch 1....
Epoch has taken 0:00:22.701002
Number of used sentences in train = 359
Total loss for epoch 1: 984.566505
	Epoch 2....
Epoch has taken 0:00:22.681824
Number of used sentences in train = 359
Total loss for epoch 2: 885.077110
	Epoch 3....
Epoch has taken 0:00:22.676930
Number of used sentences in train = 359
Total loss for epoch 3: 808.232161
	Epoch 4....
Epoch has taken 0:00:25.055385
Number of used sentences in train = 359
Total loss for epoch 4: 763.151025
	Epoch 5....
Epoch has taken 0:00:22.741154
Number of used sentences in train = 359
Total loss for epoch 5: 734.891357
	Epoch 6....
Epoch has taken 0:00:22.687344
Number of used sentences in train = 359
Total loss for epoch 6: 700.224436
	Epoch 7....
Epoch has taken 0:00:22.674738
Number of used sentences in train = 359
Total loss for epoch 7: 692.240186
	Epoch 8....
Epoch has taken 0:00:22.705808
Number of used sentences in train = 359
Total loss for epoch 8: 692.858652
	Epoch 9....
Epoch has taken 0:00:22.671335
Number of used sentences in train = 359
Total loss for epoch 9: 679.053235
	Epoch 10....
Epoch has taken 0:00:22.494636
Number of used sentences in train = 359
Total loss for epoch 10: 673.427022
	Epoch 11....
Epoch has taken 0:00:22.706022
Number of used sentences in train = 359
Total loss for epoch 11: 677.106072
	Epoch 12....
Epoch has taken 0:00:22.692804
Number of used sentences in train = 359
Total loss for epoch 12: 675.441532
	Epoch 13....
Epoch has taken 0:00:22.590784
Number of used sentences in train = 359
Total loss for epoch 13: 672.560776
	Epoch 14....
Epoch has taken 0:00:22.439841
Number of used sentences in train = 359
Total loss for epoch 14: 672.467520
Epoch has taken 0:00:22.643969

==================================================================================================
	Training time : 1:05:57.444334
==================================================================================================
	Identification : 0.234

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 32, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 15, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 39, 'lstmDropout': 0.35, 'denseActivation': 'tanh', 'wordDim': 92, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1882, 92)
  (lstm): LSTM(107, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 9693.423507
validation loss after epoch 0 : 850.046891
	Epoch 1....
Epoch has taken 0:02:42.673464
Number of used sentences in train = 2811
Total loss for epoch 1: 7047.582674
validation loss after epoch 1 : 866.695713
	Epoch 2....
Epoch has taken 0:02:42.040387
Number of used sentences in train = 2811
Total loss for epoch 2: 6122.404198
validation loss after epoch 2 : 894.352105
	Epoch 3....
Epoch has taken 0:02:42.668090
Number of used sentences in train = 2811
Total loss for epoch 3: 5574.466038
validation loss after epoch 3 : 885.783151
	Epoch 4....
Epoch has taken 0:02:41.066471
Number of used sentences in train = 2811
Total loss for epoch 4: 5231.246656
validation loss after epoch 4 : 947.394894
	Epoch 5....
Epoch has taken 0:02:43.508289
Number of used sentences in train = 2811
Total loss for epoch 5: 5010.618244
validation loss after epoch 5 : 979.136191
	Epoch 6....
Epoch has taken 0:02:43.266063
Number of used sentences in train = 2811
Total loss for epoch 6: 4866.486014
validation loss after epoch 6 : 1045.308833
	Epoch 7....
Epoch has taken 0:02:50.266776
Number of used sentences in train = 2811
Total loss for epoch 7: 4801.346616
validation loss after epoch 7 : 1060.151739
	Epoch 8....
Epoch has taken 0:02:58.826087
Number of used sentences in train = 2811
Total loss for epoch 8: 4711.627460
validation loss after epoch 8 : 1097.877181
	Epoch 9....
Epoch has taken 0:02:58.826285
Number of used sentences in train = 2811
Total loss for epoch 9: 4664.545478
validation loss after epoch 9 : 1104.835996
	Epoch 10....
Epoch has taken 0:02:58.408218
Number of used sentences in train = 2811
Total loss for epoch 10: 4668.605466
validation loss after epoch 10 : 1106.362973
	Epoch 11....
Epoch has taken 0:02:57.100325
Number of used sentences in train = 2811
Total loss for epoch 11: 4624.589033
validation loss after epoch 11 : 1133.757026
	Epoch 12....
Epoch has taken 0:02:58.623141
Number of used sentences in train = 2811
Total loss for epoch 12: 4596.556256
validation loss after epoch 12 : 1160.299059
	Epoch 13....
Epoch has taken 0:02:58.856258
Number of used sentences in train = 2811
Total loss for epoch 13: 4585.706788
validation loss after epoch 13 : 1160.310982
	Epoch 14....
Epoch has taken 0:02:58.811693
Number of used sentences in train = 2811
Total loss for epoch 14: 4572.964715
validation loss after epoch 14 : 1191.987646
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1882, 92)
  (lstm): LSTM(107, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:45.178063
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1362.694368
	Epoch 1....
Epoch has taken 0:00:17.225828
Number of used sentences in train = 313
Total loss for epoch 1: 688.077460
	Epoch 2....
Epoch has taken 0:00:17.223039
Number of used sentences in train = 313
Total loss for epoch 2: 579.065727
	Epoch 3....
Epoch has taken 0:00:17.211953
Number of used sentences in train = 313
Total loss for epoch 3: 553.305239
	Epoch 4....
Epoch has taken 0:00:17.212798
Number of used sentences in train = 313
Total loss for epoch 4: 530.579740
	Epoch 5....
Epoch has taken 0:00:17.213963
Number of used sentences in train = 313
Total loss for epoch 5: 519.574768
	Epoch 6....
Epoch has taken 0:00:17.221243
Number of used sentences in train = 313
Total loss for epoch 6: 514.090090
	Epoch 7....
Epoch has taken 0:00:17.211869
Number of used sentences in train = 313
Total loss for epoch 7: 509.941096
	Epoch 8....
Epoch has taken 0:00:17.227029
Number of used sentences in train = 313
Total loss for epoch 8: 508.978777
	Epoch 9....
Epoch has taken 0:00:17.208569
Number of used sentences in train = 313
Total loss for epoch 9: 508.358706
	Epoch 10....
Epoch has taken 0:00:17.215179
Number of used sentences in train = 313
Total loss for epoch 10: 508.017070
	Epoch 11....
Epoch has taken 0:00:17.232391
Number of used sentences in train = 313
Total loss for epoch 11: 506.377291
	Epoch 12....
Epoch has taken 0:00:17.239781
Number of used sentences in train = 313
Total loss for epoch 12: 506.422294
	Epoch 13....
Epoch has taken 0:00:17.251173
Number of used sentences in train = 313
Total loss for epoch 13: 505.557494
	Epoch 14....
Epoch has taken 0:00:17.224743
Number of used sentences in train = 313
Total loss for epoch 14: 505.520195
Epoch has taken 0:00:17.215808

==================================================================================================
	Training time : 0:46:58.952710
==================================================================================================
	Identification : 0.427

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1680, 92)
  (lstm): LSTM(107, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7911.557503
validation loss after epoch 0 : 701.095146
	Epoch 1....
Epoch has taken 0:01:51.597722
Number of used sentences in train = 2074
Total loss for epoch 1: 5194.025918
validation loss after epoch 1 : 676.133627
	Epoch 2....
Epoch has taken 0:01:57.120863
Number of used sentences in train = 2074
Total loss for epoch 2: 4378.193156
validation loss after epoch 2 : 665.127671
	Epoch 3....
Epoch has taken 0:02:02.672466
Number of used sentences in train = 2074
Total loss for epoch 3: 3940.286960
validation loss after epoch 3 : 718.817024
	Epoch 4....
Epoch has taken 0:02:02.625330
Number of used sentences in train = 2074
Total loss for epoch 4: 3621.269306
validation loss after epoch 4 : 737.190310
	Epoch 5....
Epoch has taken 0:02:02.583910
Number of used sentences in train = 2074
Total loss for epoch 5: 3458.357602
validation loss after epoch 5 : 760.180083
	Epoch 6....
Epoch has taken 0:02:02.535337
Number of used sentences in train = 2074
Total loss for epoch 6: 3357.993130
validation loss after epoch 6 : 800.375756
	Epoch 7....
Epoch has taken 0:02:02.612038
Number of used sentences in train = 2074
Total loss for epoch 7: 3277.118856
validation loss after epoch 7 : 838.264250
	Epoch 8....
Epoch has taken 0:02:02.513908
Number of used sentences in train = 2074
Total loss for epoch 8: 3238.151445
validation loss after epoch 8 : 850.167333
	Epoch 9....
Epoch has taken 0:02:02.578090
Number of used sentences in train = 2074
Total loss for epoch 9: 3218.066901
validation loss after epoch 9 : 872.321895
	Epoch 10....
Epoch has taken 0:01:54.745067
Number of used sentences in train = 2074
Total loss for epoch 10: 3203.552300
validation loss after epoch 10 : 882.856879
	Epoch 11....
Epoch has taken 0:01:48.589578
Number of used sentences in train = 2074
Total loss for epoch 11: 3199.747548
validation loss after epoch 11 : 905.544842
	Epoch 12....
Epoch has taken 0:01:49.361345
Number of used sentences in train = 2074
Total loss for epoch 12: 3189.303345
validation loss after epoch 12 : 912.051339
	Epoch 13....
Epoch has taken 0:01:49.042537
Number of used sentences in train = 2074
Total loss for epoch 13: 3179.781765
validation loss after epoch 13 : 924.352561
	Epoch 14....
Epoch has taken 0:01:50.721947
Number of used sentences in train = 2074
Total loss for epoch 14: 3174.431156
validation loss after epoch 14 : 933.048921
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1680, 92)
  (lstm): LSTM(107, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.298245
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1163.950207
	Epoch 1....
Epoch has taken 0:00:12.366817
Number of used sentences in train = 231
Total loss for epoch 1: 573.866822
	Epoch 2....
Epoch has taken 0:00:12.585234
Number of used sentences in train = 231
Total loss for epoch 2: 435.349841
	Epoch 3....
Epoch has taken 0:00:12.609121
Number of used sentences in train = 231
Total loss for epoch 3: 384.790162
	Epoch 4....
Epoch has taken 0:00:12.608685
Number of used sentences in train = 231
Total loss for epoch 4: 366.500671
	Epoch 5....
Epoch has taken 0:00:12.831412
Number of used sentences in train = 231
Total loss for epoch 5: 355.117658
	Epoch 6....
Epoch has taken 0:00:12.610288
Number of used sentences in train = 231
Total loss for epoch 6: 351.290969
	Epoch 7....
Epoch has taken 0:00:12.629718
Number of used sentences in train = 231
Total loss for epoch 7: 350.074247
	Epoch 8....
Epoch has taken 0:00:12.603086
Number of used sentences in train = 231
Total loss for epoch 8: 348.578618
	Epoch 9....
Epoch has taken 0:00:12.624116
Number of used sentences in train = 231
Total loss for epoch 9: 347.878293
	Epoch 10....
Epoch has taken 0:00:12.621234
Number of used sentences in train = 231
Total loss for epoch 10: 347.463351
	Epoch 11....
Epoch has taken 0:00:12.617567
Number of used sentences in train = 231
Total loss for epoch 11: 347.084584
	Epoch 12....
Epoch has taken 0:00:12.598630
Number of used sentences in train = 231
Total loss for epoch 12: 346.750389
	Epoch 13....
Epoch has taken 0:00:12.609163
Number of used sentences in train = 231
Total loss for epoch 13: 346.526927
	Epoch 14....
Epoch has taken 0:00:12.605083
Number of used sentences in train = 231
Total loss for epoch 14: 346.305110
Epoch has taken 0:00:12.611755

==================================================================================================
	Training time : 0:32:19.072586
==================================================================================================
	Identification : 0.207

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(3369, 92)
  (lstm): LSTM(107, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12184.127952
validation loss after epoch 0 : 1127.935912
	Epoch 1....
Epoch has taken 0:03:47.805572
Number of used sentences in train = 3226
Total loss for epoch 1: 9309.430516
validation loss after epoch 1 : 1102.809729
	Epoch 2....
Epoch has taken 0:03:37.419669
Number of used sentences in train = 3226
Total loss for epoch 2: 8435.693303
validation loss after epoch 2 : 1126.970423
	Epoch 3....
Epoch has taken 0:03:37.529350
Number of used sentences in train = 3226
Total loss for epoch 3: 7826.634990
validation loss after epoch 3 : 1227.029812
	Epoch 4....
Epoch has taken 0:03:37.530025
Number of used sentences in train = 3226
Total loss for epoch 4: 7455.914091
validation loss after epoch 4 : 1234.896988
	Epoch 5....
Epoch has taken 0:03:37.602605
Number of used sentences in train = 3226
Total loss for epoch 5: 7209.660926
validation loss after epoch 5 : 1314.891814
	Epoch 6....
Epoch has taken 0:03:36.138531
Number of used sentences in train = 3226
Total loss for epoch 6: 6995.782118
validation loss after epoch 6 : 1404.346137
	Epoch 7....
Epoch has taken 0:03:37.161515
Number of used sentences in train = 3226
Total loss for epoch 7: 6856.901300
validation loss after epoch 7 : 1374.005859
	Epoch 8....
Epoch has taken 0:03:36.515058
Number of used sentences in train = 3226
Total loss for epoch 8: 6724.117285
validation loss after epoch 8 : 1459.116151
	Epoch 9....
Epoch has taken 0:03:36.337885
Number of used sentences in train = 3226
Total loss for epoch 9: 6637.015791
validation loss after epoch 9 : 1511.423748
	Epoch 10....
Epoch has taken 0:03:40.379793
Number of used sentences in train = 3226
Total loss for epoch 10: 6515.741305
validation loss after epoch 10 : 1549.968282
	Epoch 11....
Epoch has taken 0:03:41.218651
Number of used sentences in train = 3226
Total loss for epoch 11: 6454.179164
validation loss after epoch 11 : 1587.645789
	Epoch 12....
Epoch has taken 0:03:59.701833
Number of used sentences in train = 3226
Total loss for epoch 12: 6401.983804
validation loss after epoch 12 : 1709.982800
	Epoch 13....
Epoch has taken 0:03:42.371771
Number of used sentences in train = 3226
Total loss for epoch 13: 6366.026738
validation loss after epoch 13 : 1631.850244
	Epoch 14....
Epoch has taken 0:03:37.729753
Number of used sentences in train = 3226
Total loss for epoch 14: 6309.084975
validation loss after epoch 14 : 1733.686360
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(3369, 92)
  (lstm): LSTM(107, 39, bidirectional=True)
  (linear1): Linear(in_features=624, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:37.809953
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1875.447306
	Epoch 1....
Epoch has taken 0:00:21.180609
Number of used sentences in train = 359
Total loss for epoch 1: 958.667344
	Epoch 2....
Epoch has taken 0:00:21.347901
Number of used sentences in train = 359
Total loss for epoch 2: 811.825965
	Epoch 3....
Epoch has taken 0:00:21.383973
Number of used sentences in train = 359
Total loss for epoch 3: 740.648168
	Epoch 4....
Epoch has taken 0:00:21.179754
Number of used sentences in train = 359
Total loss for epoch 4: 713.655305
	Epoch 5....
Epoch has taken 0:00:21.224737
Number of used sentences in train = 359
Total loss for epoch 5: 689.724235
	Epoch 6....
Epoch has taken 0:00:21.231926
Number of used sentences in train = 359
Total loss for epoch 6: 683.656257
	Epoch 7....
Epoch has taken 0:00:21.216787
Number of used sentences in train = 359
Total loss for epoch 7: 680.263482
	Epoch 8....
Epoch has taken 0:00:21.212640
Number of used sentences in train = 359
Total loss for epoch 8: 676.296777
	Epoch 9....
Epoch has taken 0:00:21.169887
Number of used sentences in train = 359
Total loss for epoch 9: 674.685969
	Epoch 10....
Epoch has taken 0:00:21.174482
Number of used sentences in train = 359
Total loss for epoch 10: 674.044584
	Epoch 11....
Epoch has taken 0:00:21.163117
Number of used sentences in train = 359
Total loss for epoch 11: 672.899907
	Epoch 12....
Epoch has taken 0:00:21.177524
Number of used sentences in train = 359
Total loss for epoch 12: 673.065492
	Epoch 13....
Epoch has taken 0:00:21.125527
Number of used sentences in train = 359
Total loss for epoch 13: 672.122189
	Epoch 14....
Epoch has taken 0:00:21.166829
Number of used sentences in train = 359
Total loss for epoch 14: 671.767796
Epoch has taken 0:00:21.177125

==================================================================================================
	Training time : 1:00:22.055131
==================================================================================================
	Identification : 0.196

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 32, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 52, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 22, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 76, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 52)
  (w_embeddings): Embedding(9325, 76)
  (lstm): LSTM(128, 22, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14184.114673
validation loss after epoch 0 : 1196.497161
	Epoch 1....
Epoch has taken 0:03:06.775848
Number of used sentences in train = 2811
Total loss for epoch 1: 9535.626146
validation loss after epoch 1 : 1092.999683
	Epoch 2....
Epoch has taken 0:03:04.081988
Number of used sentences in train = 2811
Total loss for epoch 2: 7865.968974
validation loss after epoch 2 : 1110.723479
	Epoch 3....
Epoch has taken 0:02:54.693786
Number of used sentences in train = 2811
Total loss for epoch 3: 6939.289128
validation loss after epoch 3 : 1098.486407
	Epoch 4....
Epoch has taken 0:02:54.130671
Number of used sentences in train = 2811
Total loss for epoch 4: 6200.707006
validation loss after epoch 4 : 1155.902068
	Epoch 5....
Epoch has taken 0:02:54.532924
Number of used sentences in train = 2811
Total loss for epoch 5: 5871.926359
validation loss after epoch 5 : 1254.946093
	Epoch 6....
Epoch has taken 0:02:54.701209
Number of used sentences in train = 2811
Total loss for epoch 6: 5535.179281
validation loss after epoch 6 : 1339.888831
	Epoch 7....
Epoch has taken 0:02:52.742518
Number of used sentences in train = 2811
Total loss for epoch 7: 5320.634554
validation loss after epoch 7 : 1358.942378
	Epoch 8....
Epoch has taken 0:02:54.696582
Number of used sentences in train = 2811
Total loss for epoch 8: 5159.773286
validation loss after epoch 8 : 1406.643858
	Epoch 9....
Epoch has taken 0:02:52.818308
Number of used sentences in train = 2811
Total loss for epoch 9: 5008.530776
validation loss after epoch 9 : 1442.593154
	Epoch 10....
Epoch has taken 0:02:54.592116
Number of used sentences in train = 2811
Total loss for epoch 10: 4962.806585
validation loss after epoch 10 : 1445.887803
	Epoch 11....
Epoch has taken 0:02:54.654928
Number of used sentences in train = 2811
Total loss for epoch 11: 4910.896286
validation loss after epoch 11 : 1463.627145
	Epoch 12....
Epoch has taken 0:02:54.384950
Number of used sentences in train = 2811
Total loss for epoch 12: 4839.975170
validation loss after epoch 12 : 1479.887462
	Epoch 13....
Epoch has taken 0:02:56.695546
Number of used sentences in train = 2811
Total loss for epoch 13: 4816.479039
validation loss after epoch 13 : 1515.155218
	Epoch 14....
Epoch has taken 0:03:05.416702
Number of used sentences in train = 2811
Total loss for epoch 14: 4745.092832
validation loss after epoch 14 : 1587.096414
	TransitionClassifier(
  (p_embeddings): Embedding(18, 52)
  (w_embeddings): Embedding(9325, 76)
  (lstm): LSTM(128, 22, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:55.513944
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1746.057319
	Epoch 1....
Epoch has taken 0:00:18.541827
Number of used sentences in train = 313
Total loss for epoch 1: 805.071576
	Epoch 2....
Epoch has taken 0:00:18.534172
Number of used sentences in train = 313
Total loss for epoch 2: 633.206365
	Epoch 3....
Epoch has taken 0:00:18.545411
Number of used sentences in train = 313
Total loss for epoch 3: 578.016051
	Epoch 4....
Epoch has taken 0:00:18.530640
Number of used sentences in train = 313
Total loss for epoch 4: 557.684083
	Epoch 5....
Epoch has taken 0:00:18.533186
Number of used sentences in train = 313
Total loss for epoch 5: 538.277403
	Epoch 6....
Epoch has taken 0:00:18.526749
Number of used sentences in train = 313
Total loss for epoch 6: 519.720834
	Epoch 7....
Epoch has taken 0:00:18.530138
Number of used sentences in train = 313
Total loss for epoch 7: 513.995499
	Epoch 8....
Epoch has taken 0:00:18.538206
Number of used sentences in train = 313
Total loss for epoch 8: 512.758773
	Epoch 9....
Epoch has taken 0:00:18.646418
Number of used sentences in train = 313
Total loss for epoch 9: 512.796612
	Epoch 10....
Epoch has taken 0:00:18.539831
Number of used sentences in train = 313
Total loss for epoch 10: 507.863007
	Epoch 11....
Epoch has taken 0:00:18.533356
Number of used sentences in train = 313
Total loss for epoch 11: 507.853540
	Epoch 12....
Epoch has taken 0:00:18.535466
Number of used sentences in train = 313
Total loss for epoch 12: 507.624051
	Epoch 13....
Epoch has taken 0:00:18.455905
Number of used sentences in train = 313
Total loss for epoch 13: 508.308779
	Epoch 14....
Epoch has taken 0:00:18.517086
Number of used sentences in train = 313
Total loss for epoch 14: 505.787337
Epoch has taken 0:00:18.500960

==================================================================================================
	Training time : 0:48:48.954993
==================================================================================================
	Identification : 0.434

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 52)
  (w_embeddings): Embedding(7043, 76)
  (lstm): LSTM(128, 22, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11037.786915
validation loss after epoch 0 : 967.738434
	Epoch 1....
Epoch has taken 0:01:59.546442
Number of used sentences in train = 2074
Total loss for epoch 1: 7083.919126
validation loss after epoch 1 : 906.473890
	Epoch 2....
Epoch has taken 0:01:59.585638
Number of used sentences in train = 2074
Total loss for epoch 2: 5723.162705
validation loss after epoch 2 : 911.882489
	Epoch 3....
Epoch has taken 0:01:59.687692
Number of used sentences in train = 2074
Total loss for epoch 3: 4839.859224
validation loss after epoch 3 : 1003.155219
	Epoch 4....
Epoch has taken 0:01:59.736046
Number of used sentences in train = 2074
Total loss for epoch 4: 4386.087996
validation loss after epoch 4 : 1126.970146
	Epoch 5....
Epoch has taken 0:01:59.584217
Number of used sentences in train = 2074
Total loss for epoch 5: 4041.227809
validation loss after epoch 5 : 1013.317548
	Epoch 6....
Epoch has taken 0:01:59.593070
Number of used sentences in train = 2074
Total loss for epoch 6: 3823.191115
validation loss after epoch 6 : 1310.412688
	Epoch 7....
Epoch has taken 0:01:59.561544
Number of used sentences in train = 2074
Total loss for epoch 7: 3662.319262
validation loss after epoch 7 : 1284.295354
	Epoch 8....
Epoch has taken 0:01:59.015952
Number of used sentences in train = 2074
Total loss for epoch 8: 3611.135326
validation loss after epoch 8 : 1376.908490
	Epoch 9....
Epoch has taken 0:01:59.679654
Number of used sentences in train = 2074
Total loss for epoch 9: 3489.838539
validation loss after epoch 9 : 1381.866608
	Epoch 10....
Epoch has taken 0:01:59.183624
Number of used sentences in train = 2074
Total loss for epoch 10: 3439.765428
validation loss after epoch 10 : 1548.457767
	Epoch 11....
Epoch has taken 0:01:59.554991
Number of used sentences in train = 2074
Total loss for epoch 11: 3414.325344
validation loss after epoch 11 : 1523.676014
	Epoch 12....
Epoch has taken 0:01:59.584011
Number of used sentences in train = 2074
Total loss for epoch 12: 3346.902312
validation loss after epoch 12 : 1392.975255
	Epoch 13....
Epoch has taken 0:01:59.695576
Number of used sentences in train = 2074
Total loss for epoch 13: 3290.482952
validation loss after epoch 13 : 1565.232890
	Epoch 14....
Epoch has taken 0:01:59.685634
Number of used sentences in train = 2074
Total loss for epoch 14: 3294.235860
validation loss after epoch 14 : 1476.932416
	TransitionClassifier(
  (p_embeddings): Embedding(18, 52)
  (w_embeddings): Embedding(7043, 76)
  (lstm): LSTM(128, 22, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:58.576283
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1355.126552
	Epoch 1....
Epoch has taken 0:00:12.235008
Number of used sentences in train = 231
Total loss for epoch 1: 746.408517
	Epoch 2....
Epoch has taken 0:00:12.233004
Number of used sentences in train = 231
Total loss for epoch 2: 491.337760
	Epoch 3....
Epoch has taken 0:00:12.198193
Number of used sentences in train = 231
Total loss for epoch 3: 425.381329
	Epoch 4....
Epoch has taken 0:00:12.231539
Number of used sentences in train = 231
Total loss for epoch 4: 397.690177
	Epoch 5....
Epoch has taken 0:00:12.064571
Number of used sentences in train = 231
Total loss for epoch 5: 379.245545
	Epoch 6....
Epoch has taken 0:00:12.044557
Number of used sentences in train = 231
Total loss for epoch 6: 371.460520
	Epoch 7....
Epoch has taken 0:00:12.235644
Number of used sentences in train = 231
Total loss for epoch 7: 361.708798
	Epoch 8....
Epoch has taken 0:00:12.227229
Number of used sentences in train = 231
Total loss for epoch 8: 370.139714
	Epoch 9....
Epoch has taken 0:00:12.217414
Number of used sentences in train = 231
Total loss for epoch 9: 352.130000
	Epoch 10....
Epoch has taken 0:00:12.246138
Number of used sentences in train = 231
Total loss for epoch 10: 360.651762
	Epoch 11....
Epoch has taken 0:00:12.242878
Number of used sentences in train = 231
Total loss for epoch 11: 361.298725
	Epoch 12....
Epoch has taken 0:00:12.232558
Number of used sentences in train = 231
Total loss for epoch 12: 349.139624
	Epoch 13....
Epoch has taken 0:00:12.256233
Number of used sentences in train = 231
Total loss for epoch 13: 352.297453
	Epoch 14....
Epoch has taken 0:00:12.245393
Number of used sentences in train = 231
Total loss for epoch 14: 347.748085
Epoch has taken 0:00:12.248007

==================================================================================================
	Training time : 0:32:55.772064
==================================================================================================
	Identification : 0.303

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 52)
  (w_embeddings): Embedding(17985, 76)
  (lstm): LSTM(128, 22, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18381.588053
validation loss after epoch 0 : 1626.721452
	Epoch 1....
Epoch has taken 0:03:56.596016
Number of used sentences in train = 3226
Total loss for epoch 1: 12761.942225
validation loss after epoch 1 : 1563.710694
	Epoch 2....
Epoch has taken 0:03:57.506372
Number of used sentences in train = 3226
Total loss for epoch 2: 10536.329451
validation loss after epoch 2 : 1588.215268
	Epoch 3....
Epoch has taken 0:03:55.342534
Number of used sentences in train = 3226
Total loss for epoch 3: 9112.229159
validation loss after epoch 3 : 1770.536398
	Epoch 4....
Epoch has taken 0:04:18.022617
Number of used sentences in train = 3226
Total loss for epoch 4: 8260.054245
validation loss after epoch 4 : 1842.769584
	Epoch 5....
Epoch has taken 0:04:20.362887
Number of used sentences in train = 3226
Total loss for epoch 5: 7619.495546
validation loss after epoch 5 : 1978.487949
	Epoch 6....
Epoch has taken 0:04:21.094995
Number of used sentences in train = 3226
Total loss for epoch 6: 7189.507349
validation loss after epoch 6 : 2229.341356
	Epoch 7....
Epoch has taken 0:04:20.179513
Number of used sentences in train = 3226
Total loss for epoch 7: 6966.334003
validation loss after epoch 7 : 2212.118769
	Epoch 8....
Epoch has taken 0:04:18.862479
Number of used sentences in train = 3226
Total loss for epoch 8: 6704.164957
validation loss after epoch 8 : 2344.361242
	Epoch 9....
Epoch has taken 0:04:18.141216
Number of used sentences in train = 3226
Total loss for epoch 9: 6674.481334
validation loss after epoch 9 : 2411.592314
	Epoch 10....
Epoch has taken 0:04:05.674289
Number of used sentences in train = 3226
Total loss for epoch 10: 6562.662354
validation loss after epoch 10 : 2460.326432
	Epoch 11....
Epoch has taken 0:03:54.554762
Number of used sentences in train = 3226
Total loss for epoch 11: 6489.680856
validation loss after epoch 11 : 2537.335073
	Epoch 12....
Epoch has taken 0:03:55.724215
Number of used sentences in train = 3226
Total loss for epoch 12: 6417.033994
validation loss after epoch 12 : 2531.308135
	Epoch 13....
Epoch has taken 0:03:57.884764
Number of used sentences in train = 3226
Total loss for epoch 13: 6416.493444
validation loss after epoch 13 : 2433.493450
	Epoch 14....
Epoch has taken 0:03:57.970664
Number of used sentences in train = 3226
Total loss for epoch 14: 6374.848919
validation loss after epoch 14 : 2579.124751
	TransitionClassifier(
  (p_embeddings): Embedding(13, 52)
  (w_embeddings): Embedding(17985, 76)
  (lstm): LSTM(128, 22, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=352, out_features=32, bias=True)
  (linear2): Linear(in_features=32, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:57.789396
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2574.466769
	Epoch 1....
Epoch has taken 0:00:23.311586
Number of used sentences in train = 359
Total loss for epoch 1: 1280.511756
	Epoch 2....
Epoch has taken 0:00:23.323828
Number of used sentences in train = 359
Total loss for epoch 2: 981.803836
	Epoch 3....
Epoch has taken 0:00:23.310652
Number of used sentences in train = 359
Total loss for epoch 3: 834.102971
	Epoch 4....
Epoch has taken 0:00:23.331371
Number of used sentences in train = 359
Total loss for epoch 4: 763.593002
	Epoch 5....
Epoch has taken 0:00:23.333688
Number of used sentences in train = 359
Total loss for epoch 5: 720.547931
	Epoch 6....
Epoch has taken 0:00:23.356160
Number of used sentences in train = 359
Total loss for epoch 6: 705.537852
	Epoch 7....
Epoch has taken 0:00:23.339272
Number of used sentences in train = 359
Total loss for epoch 7: 699.391792
	Epoch 8....
Epoch has taken 0:00:23.359586
Number of used sentences in train = 359
Total loss for epoch 8: 684.840412
	Epoch 9....
Epoch has taken 0:00:23.330569
Number of used sentences in train = 359
Total loss for epoch 9: 683.254973
	Epoch 10....
Epoch has taken 0:00:23.352161
Number of used sentences in train = 359
Total loss for epoch 10: 687.865218
	Epoch 11....
Epoch has taken 0:00:23.354949
Number of used sentences in train = 359
Total loss for epoch 11: 686.708819
	Epoch 12....
Epoch has taken 0:00:23.350733
Number of used sentences in train = 359
Total loss for epoch 12: 689.157874
	Epoch 13....
Epoch has taken 0:00:23.337905
Number of used sentences in train = 359
Total loss for epoch 13: 674.635969
	Epoch 14....
Epoch has taken 0:00:23.648733
Number of used sentences in train = 359
Total loss for epoch 14: 681.544015
Epoch has taken 0:00:25.666503

==================================================================================================
	Training time : 1:07:29.107186
==================================================================================================
	Identification : 0.284

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 12, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 37, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 75, 'lstmDropout': 0.4, 'denseActivation': 'tanh', 'wordDim': 50, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(9265, 50)
  (lstm): LSTM(87, 75, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 22634.322417
validation loss after epoch 0 : 1609.512276
	Epoch 1....
Epoch has taken 0:03:11.682022
Number of used sentences in train = 2811
Total loss for epoch 1: 13419.643486
validation loss after epoch 1 : 1279.756183
	Epoch 2....
Epoch has taken 0:03:14.892840
Number of used sentences in train = 2811
Total loss for epoch 2: 10801.566031
validation loss after epoch 2 : 1266.965521
	Epoch 3....
Epoch has taken 0:03:17.508341
Number of used sentences in train = 2811
Total loss for epoch 3: 9149.754716
validation loss after epoch 3 : 1117.813273
	Epoch 4....
Epoch has taken 0:03:10.707082
Number of used sentences in train = 2811
Total loss for epoch 4: 8179.453474
validation loss after epoch 4 : 1122.918528
	Epoch 5....
Epoch has taken 0:03:12.955903
Number of used sentences in train = 2811
Total loss for epoch 5: 7545.029157
validation loss after epoch 5 : 1073.640255
	Epoch 6....
Epoch has taken 0:03:17.930535
Number of used sentences in train = 2811
Total loss for epoch 6: 6928.944085
validation loss after epoch 6 : 1214.567505
	Epoch 7....
Epoch has taken 0:02:54.206650
Number of used sentences in train = 2811
Total loss for epoch 7: 6493.947458
validation loss after epoch 7 : 1164.938212
	Epoch 8....
Epoch has taken 0:02:55.876248
Number of used sentences in train = 2811
Total loss for epoch 8: 6197.926055
validation loss after epoch 8 : 1297.660812
	Epoch 9....
Epoch has taken 0:02:55.771126
Number of used sentences in train = 2811
Total loss for epoch 9: 5897.267866
validation loss after epoch 9 : 1311.946738
	Epoch 10....
Epoch has taken 0:02:55.791825
Number of used sentences in train = 2811
Total loss for epoch 10: 5742.999086
validation loss after epoch 10 : 1325.435446
	Epoch 11....
Epoch has taken 0:02:55.941825
Number of used sentences in train = 2811
Total loss for epoch 11: 5585.416431
validation loss after epoch 11 : 1399.226970
	Epoch 12....
Epoch has taken 0:02:55.703225
Number of used sentences in train = 2811
Total loss for epoch 12: 5536.490960
validation loss after epoch 12 : 1393.657127
	Epoch 13....
Epoch has taken 0:03:01.695626
Number of used sentences in train = 2811
Total loss for epoch 13: 5378.700725
validation loss after epoch 13 : 1389.335393
	Epoch 14....
Epoch has taken 0:03:12.816758
Number of used sentences in train = 2811
Total loss for epoch 14: 5335.256848
validation loss after epoch 14 : 1402.321796
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(9265, 50)
  (lstm): LSTM(87, 75, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:09.916481
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1836.469374
	Epoch 1....
Epoch has taken 0:00:18.108472
Number of used sentences in train = 313
Total loss for epoch 1: 1102.746671
	Epoch 2....
Epoch has taken 0:00:18.104749
Number of used sentences in train = 313
Total loss for epoch 2: 899.932996
	Epoch 3....
Epoch has taken 0:00:18.095300
Number of used sentences in train = 313
Total loss for epoch 3: 785.033673
	Epoch 4....
Epoch has taken 0:00:18.095746
Number of used sentences in train = 313
Total loss for epoch 4: 704.085743
	Epoch 5....
Epoch has taken 0:00:18.082191
Number of used sentences in train = 313
Total loss for epoch 5: 682.367251
	Epoch 6....
Epoch has taken 0:00:18.075110
Number of used sentences in train = 313
Total loss for epoch 6: 635.097548
	Epoch 7....
Epoch has taken 0:00:18.058002
Number of used sentences in train = 313
Total loss for epoch 7: 587.117843
	Epoch 8....
Epoch has taken 0:00:18.059370
Number of used sentences in train = 313
Total loss for epoch 8: 562.273944
	Epoch 9....
Epoch has taken 0:00:18.060460
Number of used sentences in train = 313
Total loss for epoch 9: 576.873760
	Epoch 10....
Epoch has taken 0:00:18.063947
Number of used sentences in train = 313
Total loss for epoch 10: 566.307174
	Epoch 11....
Epoch has taken 0:00:18.066841
Number of used sentences in train = 313
Total loss for epoch 11: 541.242771
	Epoch 12....
Epoch has taken 0:00:18.054298
Number of used sentences in train = 313
Total loss for epoch 12: 546.968625
	Epoch 13....
Epoch has taken 0:00:18.058771
Number of used sentences in train = 313
Total loss for epoch 13: 566.047323
	Epoch 14....
Epoch has taken 0:00:18.069080
Number of used sentences in train = 313
Total loss for epoch 14: 544.465281
Epoch has taken 0:00:18.054790

==================================================================================================
	Training time : 0:50:55.011192
==================================================================================================
	Identification : 0.469

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(6994, 50)
  (lstm): LSTM(87, 75, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 29381.143927
validation loss after epoch 0 : 2568.529876
	Epoch 1....
Epoch has taken 0:01:56.627939
Number of used sentences in train = 2074
Total loss for epoch 1: 15010.500910
validation loss after epoch 1 : 1107.361749
	Epoch 2....
Epoch has taken 0:02:00.024549
Number of used sentences in train = 2074
Total loss for epoch 2: 8520.947527
validation loss after epoch 2 : 935.769646
	Epoch 3....
Epoch has taken 0:01:58.971499
Number of used sentences in train = 2074
Total loss for epoch 3: 6798.548252
validation loss after epoch 3 : 889.705161
	Epoch 4....
Epoch has taken 0:01:56.977926
Number of used sentences in train = 2074
Total loss for epoch 4: 5803.066284
validation loss after epoch 4 : 852.297073
	Epoch 5....
Epoch has taken 0:01:59.583239
Number of used sentences in train = 2074
Total loss for epoch 5: 5270.953914
validation loss after epoch 5 : 845.990485
	Epoch 6....
Epoch has taken 0:01:56.738405
Number of used sentences in train = 2074
Total loss for epoch 6: 4810.378483
validation loss after epoch 6 : 992.736827
	Epoch 7....
Epoch has taken 0:01:59.139073
Number of used sentences in train = 2074
Total loss for epoch 7: 4523.491879
validation loss after epoch 7 : 1030.892988
	Epoch 8....
Epoch has taken 0:01:59.884899
Number of used sentences in train = 2074
Total loss for epoch 8: 4243.509654
validation loss after epoch 8 : 1048.695592
	Epoch 9....
Epoch has taken 0:01:56.860269
Number of used sentences in train = 2074
Total loss for epoch 9: 4128.380032
validation loss after epoch 9 : 1085.062283
	Epoch 10....
Epoch has taken 0:01:56.886627
Number of used sentences in train = 2074
Total loss for epoch 10: 4000.763711
validation loss after epoch 10 : 1081.665902
	Epoch 11....
Epoch has taken 0:01:56.846574
Number of used sentences in train = 2074
Total loss for epoch 11: 3848.549669
validation loss after epoch 11 : 1093.254379
	Epoch 12....
Epoch has taken 0:01:56.900093
Number of used sentences in train = 2074
Total loss for epoch 12: 3731.426592
validation loss after epoch 12 : 1122.615753
	Epoch 13....
Epoch has taken 0:01:56.772233
Number of used sentences in train = 2074
Total loss for epoch 13: 3688.627125
validation loss after epoch 13 : 1156.413974
	Epoch 14....
Epoch has taken 0:02:00.105662
Number of used sentences in train = 2074
Total loss for epoch 14: 3652.267480
validation loss after epoch 14 : 1136.025305
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(6994, 50)
  (lstm): LSTM(87, 75, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.484776
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1371.249324
	Epoch 1....
Epoch has taken 0:00:11.937962
Number of used sentences in train = 231
Total loss for epoch 1: 824.386470
	Epoch 2....
Epoch has taken 0:00:11.935488
Number of used sentences in train = 231
Total loss for epoch 2: 642.433442
	Epoch 3....
Epoch has taken 0:00:11.938775
Number of used sentences in train = 231
Total loss for epoch 3: 561.580647
	Epoch 4....
Epoch has taken 0:00:12.067771
Number of used sentences in train = 231
Total loss for epoch 4: 493.822423
	Epoch 5....
Epoch has taken 0:00:11.941841
Number of used sentences in train = 231
Total loss for epoch 5: 465.186785
	Epoch 6....
Epoch has taken 0:00:11.922090
Number of used sentences in train = 231
Total loss for epoch 6: 419.392621
	Epoch 7....
Epoch has taken 0:00:11.923420
Number of used sentences in train = 231
Total loss for epoch 7: 432.304616
	Epoch 8....
Epoch has taken 0:00:11.928794
Number of used sentences in train = 231
Total loss for epoch 8: 395.444548
	Epoch 9....
Epoch has taken 0:00:11.936523
Number of used sentences in train = 231
Total loss for epoch 9: 402.426627
	Epoch 10....
Epoch has taken 0:00:11.940992
Number of used sentences in train = 231
Total loss for epoch 10: 379.035254
	Epoch 11....
Epoch has taken 0:00:11.929756
Number of used sentences in train = 231
Total loss for epoch 11: 389.938770
	Epoch 12....
Epoch has taken 0:00:11.937831
Number of used sentences in train = 231
Total loss for epoch 12: 363.712699
	Epoch 13....
Epoch has taken 0:00:11.930010
Number of used sentences in train = 231
Total loss for epoch 13: 369.764011
	Epoch 14....
Epoch has taken 0:00:11.935881
Number of used sentences in train = 231
Total loss for epoch 14: 363.720361
Epoch has taken 0:00:11.934908

==================================================================================================
	Training time : 0:32:29.289850
==================================================================================================
	Identification : 0.303

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 37)
  (w_embeddings): Embedding(18147, 50)
  (lstm): LSTM(87, 75, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 23494.675341
validation loss after epoch 0 : 2001.354186
	Epoch 1....
Epoch has taken 0:03:48.346128
Number of used sentences in train = 3226
Total loss for epoch 1: 17009.110146
validation loss after epoch 1 : 1760.891655
	Epoch 2....
Epoch has taken 0:04:03.052992
Number of used sentences in train = 3226
Total loss for epoch 2: 14414.573437
validation loss after epoch 2 : 1637.278118
	Epoch 3....
Epoch has taken 0:03:50.620070
Number of used sentences in train = 3226
Total loss for epoch 3: 12320.784445
validation loss after epoch 3 : 1654.611153
	Epoch 4....
Epoch has taken 0:03:48.793655
Number of used sentences in train = 3226
Total loss for epoch 4: 11078.329766
validation loss after epoch 4 : 1638.268244
	Epoch 5....
Epoch has taken 0:03:49.533429
Number of used sentences in train = 3226
Total loss for epoch 5: 10025.631536
validation loss after epoch 5 : 1682.186061
	Epoch 6....
Epoch has taken 0:03:51.675421
Number of used sentences in train = 3226
Total loss for epoch 6: 9231.607430
validation loss after epoch 6 : 1824.512682
	Epoch 7....
Epoch has taken 0:03:46.874769
Number of used sentences in train = 3226
Total loss for epoch 7: 8626.265421
validation loss after epoch 7 : 1829.437273
	Epoch 8....
Epoch has taken 0:03:47.372735
Number of used sentences in train = 3226
Total loss for epoch 8: 8121.110146
validation loss after epoch 8 : 2008.148150
	Epoch 9....
Epoch has taken 0:03:46.776634
Number of used sentences in train = 3226
Total loss for epoch 9: 7737.782245
validation loss after epoch 9 : 1974.067498
	Epoch 10....
Epoch has taken 0:03:51.455371
Number of used sentences in train = 3226
Total loss for epoch 10: 7497.382568
validation loss after epoch 10 : 2066.571784
	Epoch 11....
Epoch has taken 0:03:56.817191
Number of used sentences in train = 3226
Total loss for epoch 11: 7331.062424
validation loss after epoch 11 : 2191.786607
	Epoch 12....
Epoch has taken 0:03:51.166598
Number of used sentences in train = 3226
Total loss for epoch 12: 7124.426819
validation loss after epoch 12 : 2183.132621
	Epoch 13....
Epoch has taken 0:03:46.826561
Number of used sentences in train = 3226
Total loss for epoch 13: 6939.278710
validation loss after epoch 13 : 2204.914025
	Epoch 14....
Epoch has taken 0:03:54.710391
Number of used sentences in train = 3226
Total loss for epoch 14: 6842.011121
validation loss after epoch 14 : 2334.565325
	TransitionClassifier(
  (p_embeddings): Embedding(13, 37)
  (w_embeddings): Embedding(18147, 50)
  (lstm): LSTM(87, 75, num_layers=2, dropout=0.4, bidirectional=True)
  (linear1): Linear(in_features=1200, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:54.328969
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2402.475989
	Epoch 1....
Epoch has taken 0:00:22.244431
Number of used sentences in train = 359
Total loss for epoch 1: 1702.831096
	Epoch 2....
Epoch has taken 0:00:22.224668
Number of used sentences in train = 359
Total loss for epoch 2: 1402.696383
	Epoch 3....
Epoch has taken 0:00:22.342641
Number of used sentences in train = 359
Total loss for epoch 3: 1238.853017
	Epoch 4....
Epoch has taken 0:00:22.217695
Number of used sentences in train = 359
Total loss for epoch 4: 1094.963093
	Epoch 5....
Epoch has taken 0:00:22.205006
Number of used sentences in train = 359
Total loss for epoch 5: 946.747265
	Epoch 6....
Epoch has taken 0:00:22.193279
Number of used sentences in train = 359
Total loss for epoch 6: 902.548585
	Epoch 7....
Epoch has taken 0:00:22.157742
Number of used sentences in train = 359
Total loss for epoch 7: 837.932325
	Epoch 8....
Epoch has taken 0:00:22.152346
Number of used sentences in train = 359
Total loss for epoch 8: 807.878871
	Epoch 9....
Epoch has taken 0:00:22.165276
Number of used sentences in train = 359
Total loss for epoch 9: 800.271486
	Epoch 10....
Epoch has taken 0:00:22.292003
Number of used sentences in train = 359
Total loss for epoch 10: 772.015823
	Epoch 11....
Epoch has taken 0:00:22.160834
Number of used sentences in train = 359
Total loss for epoch 11: 759.666833
	Epoch 12....
Epoch has taken 0:00:22.158594
Number of used sentences in train = 359
Total loss for epoch 12: 744.401389
	Epoch 13....
Epoch has taken 0:00:22.166257
Number of used sentences in train = 359
Total loss for epoch 13: 747.825715
	Epoch 14....
Epoch has taken 0:00:22.151258
Number of used sentences in train = 359
Total loss for epoch 14: 730.950662
Epoch has taken 0:00:22.172169

==================================================================================================
	Training time : 1:03:22.060940
==================================================================================================
	Identification : 0.453

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 141, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 27, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 56, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 53, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 27)
  (w_embeddings): Embedding(1177, 53)
  (lstm): LSTM(80, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11492.377757
validation loss after epoch 0 : 1038.096970
	Epoch 1....
Epoch has taken 0:02:39.654194
Number of used sentences in train = 2811
Total loss for epoch 1: 7960.970322
validation loss after epoch 1 : 896.010630
	Epoch 2....
Epoch has taken 0:02:40.367034
Number of used sentences in train = 2811
Total loss for epoch 2: 6986.521939
validation loss after epoch 2 : 895.237840
	Epoch 3....
Epoch has taken 0:02:39.713508
Number of used sentences in train = 2811
Total loss for epoch 3: 6368.749016
validation loss after epoch 3 : 912.008309
	Epoch 4....
Epoch has taken 0:02:39.605592
Number of used sentences in train = 2811
Total loss for epoch 4: 5874.647662
validation loss after epoch 4 : 918.179401
	Epoch 5....
Epoch has taken 0:02:39.668662
Number of used sentences in train = 2811
Total loss for epoch 5: 5488.042464
validation loss after epoch 5 : 963.068920
	Epoch 6....
Epoch has taken 0:02:39.798555
Number of used sentences in train = 2811
Total loss for epoch 6: 5233.541364
validation loss after epoch 6 : 1014.191745
	Epoch 7....
Epoch has taken 0:02:42.699009
Number of used sentences in train = 2811
Total loss for epoch 7: 5031.662846
validation loss after epoch 7 : 1095.220752
	Epoch 8....
Epoch has taken 0:02:39.659286
Number of used sentences in train = 2811
Total loss for epoch 8: 4875.596812
validation loss after epoch 8 : 1108.046539
	Epoch 9....
Epoch has taken 0:02:40.881936
Number of used sentences in train = 2811
Total loss for epoch 9: 4757.526764
validation loss after epoch 9 : 1168.309050
	Epoch 10....
Epoch has taken 0:02:40.769769
Number of used sentences in train = 2811
Total loss for epoch 10: 4663.673161
validation loss after epoch 10 : 1213.188548
	Epoch 11....
Epoch has taken 0:02:39.949339
Number of used sentences in train = 2811
Total loss for epoch 11: 4619.957265
validation loss after epoch 11 : 1230.247387
	Epoch 12....
Epoch has taken 0:02:40.003951
Number of used sentences in train = 2811
Total loss for epoch 12: 4585.311253
validation loss after epoch 12 : 1239.858313
	Epoch 13....
Epoch has taken 0:02:39.717032
Number of used sentences in train = 2811
Total loss for epoch 13: 4559.546425
validation loss after epoch 13 : 1278.695296
	Epoch 14....
Epoch has taken 0:02:39.665840
Number of used sentences in train = 2811
Total loss for epoch 14: 4538.034438
validation loss after epoch 14 : 1287.460259
	TransitionClassifier(
  (p_embeddings): Embedding(18, 27)
  (w_embeddings): Embedding(1177, 53)
  (lstm): LSTM(80, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:40.080483
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1709.514347
	Epoch 1....
Epoch has taken 0:00:16.872365
Number of used sentences in train = 313
Total loss for epoch 1: 755.479364
	Epoch 2....
Epoch has taken 0:00:16.895866
Number of used sentences in train = 313
Total loss for epoch 2: 614.606730
	Epoch 3....
Epoch has taken 0:00:17.174903
Number of used sentences in train = 313
Total loss for epoch 3: 557.634834
	Epoch 4....
Epoch has taken 0:00:16.893340
Number of used sentences in train = 313
Total loss for epoch 4: 536.701924
	Epoch 5....
Epoch has taken 0:00:16.864662
Number of used sentences in train = 313
Total loss for epoch 5: 528.267007
	Epoch 6....
Epoch has taken 0:00:16.885423
Number of used sentences in train = 313
Total loss for epoch 6: 515.885229
	Epoch 7....
Epoch has taken 0:00:16.869729
Number of used sentences in train = 313
Total loss for epoch 7: 510.424948
	Epoch 8....
Epoch has taken 0:00:16.882915
Number of used sentences in train = 313
Total loss for epoch 8: 507.566329
	Epoch 9....
Epoch has taken 0:00:16.874448
Number of used sentences in train = 313
Total loss for epoch 9: 506.963190
	Epoch 10....
Epoch has taken 0:00:16.878531
Number of used sentences in train = 313
Total loss for epoch 10: 505.633702
	Epoch 11....
Epoch has taken 0:00:16.874078
Number of used sentences in train = 313
Total loss for epoch 11: 504.183680
	Epoch 12....
Epoch has taken 0:00:16.877572
Number of used sentences in train = 313
Total loss for epoch 12: 505.028060
	Epoch 13....
Epoch has taken 0:00:16.867504
Number of used sentences in train = 313
Total loss for epoch 13: 504.209507
	Epoch 14....
Epoch has taken 0:00:16.868653
Number of used sentences in train = 313
Total loss for epoch 14: 504.986025
Epoch has taken 0:00:16.877855

==================================================================================================
	Training time : 0:44:16.192810
==================================================================================================
	Identification : 0.247

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 27)
  (w_embeddings): Embedding(1133, 53)
  (lstm): LSTM(80, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9623.627307
validation loss after epoch 0 : 666.215432
	Epoch 1....
Epoch has taken 0:01:49.192121
Number of used sentences in train = 2074
Total loss for epoch 1: 5413.815826
validation loss after epoch 1 : 635.091481
	Epoch 2....
Epoch has taken 0:01:49.325470
Number of used sentences in train = 2074
Total loss for epoch 2: 4616.782582
validation loss after epoch 2 : 656.479809
	Epoch 3....
Epoch has taken 0:01:49.430566
Number of used sentences in train = 2074
Total loss for epoch 3: 4102.939920
validation loss after epoch 3 : 617.438255
	Epoch 4....
Epoch has taken 0:01:49.337377
Number of used sentences in train = 2074
Total loss for epoch 4: 3728.062980
validation loss after epoch 4 : 644.465636
	Epoch 5....
Epoch has taken 0:01:50.282803
Number of used sentences in train = 2074
Total loss for epoch 5: 3494.820722
validation loss after epoch 5 : 675.987067
	Epoch 6....
Epoch has taken 0:01:49.332502
Number of used sentences in train = 2074
Total loss for epoch 6: 3343.686929
validation loss after epoch 6 : 718.485670
	Epoch 7....
Epoch has taken 0:01:49.327610
Number of used sentences in train = 2074
Total loss for epoch 7: 3255.618583
validation loss after epoch 7 : 716.055412
	Epoch 8....
Epoch has taken 0:01:49.296229
Number of used sentences in train = 2074
Total loss for epoch 8: 3213.585862
validation loss after epoch 8 : 775.200424
	Epoch 9....
Epoch has taken 0:01:49.322485
Number of used sentences in train = 2074
Total loss for epoch 9: 3188.782386
validation loss after epoch 9 : 779.954132
	Epoch 10....
Epoch has taken 0:01:49.313966
Number of used sentences in train = 2074
Total loss for epoch 10: 3177.672881
validation loss after epoch 10 : 803.777556
	Epoch 11....
Epoch has taken 0:01:49.305841
Number of used sentences in train = 2074
Total loss for epoch 11: 3172.328908
validation loss after epoch 11 : 815.110016
	Epoch 12....
Epoch has taken 0:01:49.335799
Number of used sentences in train = 2074
Total loss for epoch 12: 3171.288175
validation loss after epoch 12 : 813.427011
	Epoch 13....
Epoch has taken 0:01:49.758999
Number of used sentences in train = 2074
Total loss for epoch 13: 3164.571681
validation loss after epoch 13 : 823.591583
	Epoch 14....
Epoch has taken 0:01:51.825833
Number of used sentences in train = 2074
Total loss for epoch 14: 3161.183421
validation loss after epoch 14 : 836.215081
	TransitionClassifier(
  (p_embeddings): Embedding(18, 27)
  (w_embeddings): Embedding(1133, 53)
  (lstm): LSTM(80, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.165657
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2131.637784
	Epoch 1....
Epoch has taken 0:00:11.127263
Number of used sentences in train = 231
Total loss for epoch 1: 553.766035
	Epoch 2....
Epoch has taken 0:00:11.112481
Number of used sentences in train = 231
Total loss for epoch 2: 418.648667
	Epoch 3....
Epoch has taken 0:00:11.120085
Number of used sentences in train = 231
Total loss for epoch 3: 375.587768
	Epoch 4....
Epoch has taken 0:00:11.124691
Number of used sentences in train = 231
Total loss for epoch 4: 360.394329
	Epoch 5....
Epoch has taken 0:00:11.116377
Number of used sentences in train = 231
Total loss for epoch 5: 353.617454
	Epoch 6....
Epoch has taken 0:00:11.128274
Number of used sentences in train = 231
Total loss for epoch 6: 350.691984
	Epoch 7....
Epoch has taken 0:00:11.114393
Number of used sentences in train = 231
Total loss for epoch 7: 348.282208
	Epoch 8....
Epoch has taken 0:00:11.122988
Number of used sentences in train = 231
Total loss for epoch 8: 347.045952
	Epoch 9....
Epoch has taken 0:00:11.111669
Number of used sentences in train = 231
Total loss for epoch 9: 346.419297
	Epoch 10....
Epoch has taken 0:00:11.117086
Number of used sentences in train = 231
Total loss for epoch 10: 346.032878
	Epoch 11....
Epoch has taken 0:00:11.115082
Number of used sentences in train = 231
Total loss for epoch 11: 345.726954
	Epoch 12....
Epoch has taken 0:00:11.116435
Number of used sentences in train = 231
Total loss for epoch 12: 345.530921
	Epoch 13....
Epoch has taken 0:00:11.128860
Number of used sentences in train = 231
Total loss for epoch 13: 345.359347
	Epoch 14....
Epoch has taken 0:00:11.110344
Number of used sentences in train = 231
Total loss for epoch 14: 345.232837
Epoch has taken 0:00:11.123476

==================================================================================================
	Training time : 0:30:10.676846
==================================================================================================
	Identification : 0.244

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 27)
  (w_embeddings): Embedding(1202, 53)
  (lstm): LSTM(80, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14667.116974
validation loss after epoch 0 : 1277.610317
	Epoch 1....
Epoch has taken 0:03:33.360437
Number of used sentences in train = 3226
Total loss for epoch 1: 10858.920235
validation loss after epoch 1 : 1226.943528
	Epoch 2....
Epoch has taken 0:03:39.244917
Number of used sentences in train = 3226
Total loss for epoch 2: 9912.197994
validation loss after epoch 2 : 1263.658473
	Epoch 3....
Epoch has taken 0:03:36.554051
Number of used sentences in train = 3226
Total loss for epoch 3: 9243.526879
validation loss after epoch 3 : 1305.573564
	Epoch 4....
Epoch has taken 0:03:32.751973
Number of used sentences in train = 3226
Total loss for epoch 4: 8707.859005
validation loss after epoch 4 : 1316.768625
	Epoch 5....
Epoch has taken 0:03:32.920595
Number of used sentences in train = 3226
Total loss for epoch 5: 8294.846563
validation loss after epoch 5 : 1353.704286
	Epoch 6....
Epoch has taken 0:03:32.816735
Number of used sentences in train = 3226
Total loss for epoch 6: 7942.155685
validation loss after epoch 6 : 1403.605140
	Epoch 7....
Epoch has taken 0:04:02.532742
Number of used sentences in train = 3226
Total loss for epoch 7: 7613.267360
validation loss after epoch 7 : 1465.744060
	Epoch 8....
Epoch has taken 0:04:01.987885
Number of used sentences in train = 3226
Total loss for epoch 8: 7327.902722
validation loss after epoch 8 : 1530.324798
	Epoch 9....
Epoch has taken 0:03:56.438322
Number of used sentences in train = 3226
Total loss for epoch 9: 7123.511833
validation loss after epoch 9 : 1624.904970
	Epoch 10....
Epoch has taken 0:03:42.494963
Number of used sentences in train = 3226
Total loss for epoch 10: 6898.064848
validation loss after epoch 10 : 1693.707025
	Epoch 11....
Epoch has taken 0:04:03.664444
Number of used sentences in train = 3226
Total loss for epoch 11: 6741.985931
validation loss after epoch 11 : 1750.621889
	Epoch 12....
Epoch has taken 0:03:39.789027
Number of used sentences in train = 3226
Total loss for epoch 12: 6605.813970
validation loss after epoch 12 : 1807.663184
	Epoch 13....
Epoch has taken 0:03:36.750679
Number of used sentences in train = 3226
Total loss for epoch 13: 6532.162120
validation loss after epoch 13 : 1861.263891
	Epoch 14....
Epoch has taken 0:03:32.772351
Number of used sentences in train = 3226
Total loss for epoch 14: 6446.038433
validation loss after epoch 14 : 1913.769305
	TransitionClassifier(
  (p_embeddings): Embedding(13, 27)
  (w_embeddings): Embedding(1202, 53)
  (lstm): LSTM(80, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=141, bias=True)
  (linear2): Linear(in_features=141, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:38.514537
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2243.773910
	Epoch 1....
Epoch has taken 0:00:20.803235
Number of used sentences in train = 359
Total loss for epoch 1: 1134.606815
	Epoch 2....
Epoch has taken 0:00:24.350362
Number of used sentences in train = 359
Total loss for epoch 2: 935.700316
	Epoch 3....
Epoch has taken 0:00:20.785322
Number of used sentences in train = 359
Total loss for epoch 3: 826.522554
	Epoch 4....
Epoch has taken 0:00:20.787905
Number of used sentences in train = 359
Total loss for epoch 4: 744.295000
	Epoch 5....
Epoch has taken 0:00:20.775107
Number of used sentences in train = 359
Total loss for epoch 5: 721.342155
	Epoch 6....
Epoch has taken 0:00:20.849143
Number of used sentences in train = 359
Total loss for epoch 6: 695.637084
	Epoch 7....
Epoch has taken 0:00:20.775170
Number of used sentences in train = 359
Total loss for epoch 7: 678.154045
	Epoch 8....
Epoch has taken 0:00:20.788566
Number of used sentences in train = 359
Total loss for epoch 8: 677.487282
	Epoch 9....
Epoch has taken 0:00:20.785216
Number of used sentences in train = 359
Total loss for epoch 9: 675.864682
	Epoch 10....
Epoch has taken 0:00:20.790893
Number of used sentences in train = 359
Total loss for epoch 10: 672.313493
	Epoch 11....
Epoch has taken 0:00:20.777034
Number of used sentences in train = 359
Total loss for epoch 11: 671.567511
	Epoch 12....
Epoch has taken 0:00:20.767817
Number of used sentences in train = 359
Total loss for epoch 12: 671.269128
	Epoch 13....
Epoch has taken 0:00:20.775821
Number of used sentences in train = 359
Total loss for epoch 13: 671.055696
	Epoch 14....
Epoch has taken 0:00:20.775967
Number of used sentences in train = 359
Total loss for epoch 14: 670.862491
Epoch has taken 0:00:20.765633

==================================================================================================
	Training time : 1:00:58.594159
==================================================================================================
	Identification : 0.244

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 69, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 36, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 155, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 69)
  (w_embeddings): Embedding(9376, 155)
  (lstm): LSTM(224, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13476.839268
validation loss after epoch 0 : 1250.852560
	Epoch 1....
Epoch has taken 0:02:38.931355
Number of used sentences in train = 2811
Total loss for epoch 1: 9531.562613
validation loss after epoch 1 : 1203.109180
	Epoch 2....
Epoch has taken 0:02:39.158302
Number of used sentences in train = 2811
Total loss for epoch 2: 7641.535438
validation loss after epoch 2 : 1209.023854
	Epoch 3....
Epoch has taken 0:02:51.317739
Number of used sentences in train = 2811
Total loss for epoch 3: 6514.889520
validation loss after epoch 3 : 1273.108983
	Epoch 4....
Epoch has taken 0:02:40.861274
Number of used sentences in train = 2811
Total loss for epoch 4: 5857.497709
validation loss after epoch 4 : 1332.537177
	Epoch 5....
Epoch has taken 0:02:39.237416
Number of used sentences in train = 2811
Total loss for epoch 5: 5456.681233
validation loss after epoch 5 : 1383.737752
	Epoch 6....
Epoch has taken 0:02:41.120603
Number of used sentences in train = 2811
Total loss for epoch 6: 5213.760448
validation loss after epoch 6 : 1441.206607
	Epoch 7....
Epoch has taken 0:02:48.516994
Number of used sentences in train = 2811
Total loss for epoch 7: 5034.290955
validation loss after epoch 7 : 1524.628094
	Epoch 8....
Epoch has taken 0:02:43.363949
Number of used sentences in train = 2811
Total loss for epoch 8: 4923.061225
validation loss after epoch 8 : 1540.268082
	Epoch 9....
Epoch has taken 0:03:04.448303
Number of used sentences in train = 2811
Total loss for epoch 9: 4842.964449
validation loss after epoch 9 : 1554.214341
	Epoch 10....
Epoch has taken 0:02:39.089839
Number of used sentences in train = 2811
Total loss for epoch 10: 4766.442702
validation loss after epoch 10 : 1607.803229
	Epoch 11....
Epoch has taken 0:02:47.138405
Number of used sentences in train = 2811
Total loss for epoch 11: 4705.333508
validation loss after epoch 11 : 1604.499934
	Epoch 12....
Epoch has taken 0:02:56.101206
Number of used sentences in train = 2811
Total loss for epoch 12: 4666.117952
validation loss after epoch 12 : 1657.160160
	Epoch 13....
Epoch has taken 0:02:52.544318
Number of used sentences in train = 2811
Total loss for epoch 13: 4636.778110
validation loss after epoch 13 : 1684.143031
	Epoch 14....
Epoch has taken 0:02:39.009194
Number of used sentences in train = 2811
Total loss for epoch 14: 4610.063988
validation loss after epoch 14 : 1691.185441
	TransitionClassifier(
  (p_embeddings): Embedding(18, 69)
  (w_embeddings): Embedding(9376, 155)
  (lstm): LSTM(224, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:39.202053
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1975.771405
	Epoch 1....
Epoch has taken 0:00:16.815887
Number of used sentences in train = 313
Total loss for epoch 1: 896.909787
	Epoch 2....
Epoch has taken 0:00:16.816363
Number of used sentences in train = 313
Total loss for epoch 2: 675.311592
	Epoch 3....
Epoch has taken 0:00:16.812481
Number of used sentences in train = 313
Total loss for epoch 3: 605.847397
	Epoch 4....
Epoch has taken 0:00:16.824952
Number of used sentences in train = 313
Total loss for epoch 4: 578.528357
	Epoch 5....
Epoch has taken 0:00:16.821216
Number of used sentences in train = 313
Total loss for epoch 5: 564.629099
	Epoch 6....
Epoch has taken 0:00:16.810311
Number of used sentences in train = 313
Total loss for epoch 6: 558.153494
	Epoch 7....
Epoch has taken 0:00:17.232553
Number of used sentences in train = 313
Total loss for epoch 7: 554.848890
	Epoch 8....
Epoch has taken 0:00:17.150915
Number of used sentences in train = 313
Total loss for epoch 8: 552.894567
	Epoch 9....
Epoch has taken 0:00:16.801412
Number of used sentences in train = 313
Total loss for epoch 9: 535.046546
	Epoch 10....
Epoch has taken 0:00:16.798184
Number of used sentences in train = 313
Total loss for epoch 10: 524.505153
	Epoch 11....
Epoch has taken 0:00:16.825544
Number of used sentences in train = 313
Total loss for epoch 11: 521.749448
	Epoch 12....
Epoch has taken 0:00:17.117401
Number of used sentences in train = 313
Total loss for epoch 12: 518.448738
	Epoch 13....
Epoch has taken 0:00:16.814286
Number of used sentences in train = 313
Total loss for epoch 13: 516.904031
	Epoch 14....
Epoch has taken 0:00:16.818818
Number of used sentences in train = 313
Total loss for epoch 14: 516.772647
Epoch has taken 0:00:16.826887

==================================================================================================
	Training time : 0:45:33.850093
==================================================================================================
	Identification : 0.011

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 69)
  (w_embeddings): Embedding(7061, 155)
  (lstm): LSTM(224, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10656.675589
validation loss after epoch 0 : 1016.419591
	Epoch 1....
Epoch has taken 0:01:50.447941
Number of used sentences in train = 2074
Total loss for epoch 1: 6956.473507
validation loss after epoch 1 : 935.926753
	Epoch 2....
Epoch has taken 0:01:49.072803
Number of used sentences in train = 2074
Total loss for epoch 2: 5404.193485
validation loss after epoch 2 : 1032.575560
	Epoch 3....
Epoch has taken 0:01:50.204009
Number of used sentences in train = 2074
Total loss for epoch 3: 4525.055148
validation loss after epoch 3 : 1170.092805
	Epoch 4....
Epoch has taken 0:01:49.863515
Number of used sentences in train = 2074
Total loss for epoch 4: 4076.343750
validation loss after epoch 4 : 1280.256661
	Epoch 5....
Epoch has taken 0:01:49.061179
Number of used sentences in train = 2074
Total loss for epoch 5: 3786.763608
validation loss after epoch 5 : 1311.169758
	Epoch 6....
Epoch has taken 0:01:50.883349
Number of used sentences in train = 2074
Total loss for epoch 6: 3606.039932
validation loss after epoch 6 : 1347.953693
	Epoch 7....
Epoch has taken 0:01:48.978070
Number of used sentences in train = 2074
Total loss for epoch 7: 3524.616477
validation loss after epoch 7 : 1414.815777
	Epoch 8....
Epoch has taken 0:01:49.068374
Number of used sentences in train = 2074
Total loss for epoch 8: 3449.849930
validation loss after epoch 8 : 1434.251212
	Epoch 9....
Epoch has taken 0:01:49.065984
Number of used sentences in train = 2074
Total loss for epoch 9: 3410.567088
validation loss after epoch 9 : 1467.254274
	Epoch 10....
Epoch has taken 0:01:49.096328
Number of used sentences in train = 2074
Total loss for epoch 10: 3380.753737
validation loss after epoch 10 : 1464.809091
	Epoch 11....
Epoch has taken 0:01:50.002436
Number of used sentences in train = 2074
Total loss for epoch 11: 3361.149821
validation loss after epoch 11 : 1512.247679
	Epoch 12....
Epoch has taken 0:01:49.110671
Number of used sentences in train = 2074
Total loss for epoch 12: 3339.465263
validation loss after epoch 12 : 1516.022871
	Epoch 13....
Epoch has taken 0:01:48.915370
Number of used sentences in train = 2074
Total loss for epoch 13: 3326.009552
validation loss after epoch 13 : 1521.607777
	Epoch 14....
Epoch has taken 0:01:48.843226
Number of used sentences in train = 2074
Total loss for epoch 14: 3314.770581
validation loss after epoch 14 : 1521.927362
	TransitionClassifier(
  (p_embeddings): Embedding(18, 69)
  (w_embeddings): Embedding(7061, 155)
  (lstm): LSTM(224, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.901036
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1625.488633
	Epoch 1....
Epoch has taken 0:00:11.280696
Number of used sentences in train = 231
Total loss for epoch 1: 809.632029
	Epoch 2....
Epoch has taken 0:00:11.076681
Number of used sentences in train = 231
Total loss for epoch 2: 562.151725
	Epoch 3....
Epoch has taken 0:00:11.267613
Number of used sentences in train = 231
Total loss for epoch 3: 459.431949
	Epoch 4....
Epoch has taken 0:00:11.085908
Number of used sentences in train = 231
Total loss for epoch 4: 407.189387
	Epoch 5....
Epoch has taken 0:00:11.266651
Number of used sentences in train = 231
Total loss for epoch 5: 382.577803
	Epoch 6....
Epoch has taken 0:00:11.087044
Number of used sentences in train = 231
Total loss for epoch 6: 372.620420
	Epoch 7....
Epoch has taken 0:00:11.267116
Number of used sentences in train = 231
Total loss for epoch 7: 364.069097
	Epoch 8....
Epoch has taken 0:00:11.422457
Number of used sentences in train = 231
Total loss for epoch 8: 358.141980
	Epoch 9....
Epoch has taken 0:00:11.253110
Number of used sentences in train = 231
Total loss for epoch 9: 355.132480
	Epoch 10....
Epoch has taken 0:00:11.078255
Number of used sentences in train = 231
Total loss for epoch 10: 354.025412
	Epoch 11....
Epoch has taken 0:00:11.260468
Number of used sentences in train = 231
Total loss for epoch 11: 353.024051
	Epoch 12....
Epoch has taken 0:00:11.078316
Number of used sentences in train = 231
Total loss for epoch 12: 352.912342
	Epoch 13....
Epoch has taken 0:00:11.257332
Number of used sentences in train = 231
Total loss for epoch 13: 351.925457
	Epoch 14....
Epoch has taken 0:00:11.075612
Number of used sentences in train = 231
Total loss for epoch 14: 351.391819
Epoch has taken 0:00:11.076810

==================================================================================================
	Training time : 0:30:09.698102
==================================================================================================
	Identification : 0.353

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 69)
  (w_embeddings): Embedding(18044, 155)
  (lstm): LSTM(224, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20070.702074
validation loss after epoch 0 : 1704.685887
	Epoch 1....
Epoch has taken 0:03:32.141056
Number of used sentences in train = 3226
Total loss for epoch 1: 12636.203170
validation loss after epoch 1 : 1647.192153
	Epoch 2....
Epoch has taken 0:03:32.653011
Number of used sentences in train = 3226
Total loss for epoch 2: 10241.708758
validation loss after epoch 2 : 1701.770387
	Epoch 3....
Epoch has taken 0:03:35.359359
Number of used sentences in train = 3226
Total loss for epoch 3: 8757.424080
validation loss after epoch 3 : 1786.324932
	Epoch 4....
Epoch has taken 0:03:32.812274
Number of used sentences in train = 3226
Total loss for epoch 4: 7811.256488
validation loss after epoch 4 : 1927.881516
	Epoch 5....
Epoch has taken 0:03:33.347385
Number of used sentences in train = 3226
Total loss for epoch 5: 7211.177412
validation loss after epoch 5 : 2060.691276
	Epoch 6....
Epoch has taken 0:03:33.446759
Number of used sentences in train = 3226
Total loss for epoch 6: 6881.232973
validation loss after epoch 6 : 2146.133480
	Epoch 7....
Epoch has taken 0:03:32.787819
Number of used sentences in train = 3226
Total loss for epoch 7: 6627.436498
validation loss after epoch 7 : 2277.281071
	Epoch 8....
Epoch has taken 0:03:32.588956
Number of used sentences in train = 3226
Total loss for epoch 8: 6471.339696
validation loss after epoch 8 : 2334.847260
	Epoch 9....
Epoch has taken 0:03:37.077395
Number of used sentences in train = 3226
Total loss for epoch 9: 6395.523394
validation loss after epoch 9 : 2351.054826
	Epoch 10....
Epoch has taken 0:03:36.974624
Number of used sentences in train = 3226
Total loss for epoch 10: 6327.898198
validation loss after epoch 10 : 2450.764911
	Epoch 11....
Epoch has taken 0:03:32.957046
Number of used sentences in train = 3226
Total loss for epoch 11: 6313.954101
validation loss after epoch 11 : 2431.548693
	Epoch 12....
Epoch has taken 0:03:32.626440
Number of used sentences in train = 3226
Total loss for epoch 12: 6270.219493
validation loss after epoch 12 : 2492.767381
	Epoch 13....
Epoch has taken 0:03:32.811664
Number of used sentences in train = 3226
Total loss for epoch 13: 6256.523486
validation loss after epoch 13 : 2514.167184
	Epoch 14....
Epoch has taken 0:03:34.034643
Number of used sentences in train = 3226
Total loss for epoch 14: 6253.490979
validation loss after epoch 14 : 2519.828647
	TransitionClassifier(
  (p_embeddings): Embedding(13, 69)
  (w_embeddings): Embedding(18044, 155)
  (lstm): LSTM(224, 36, bidirectional=True)
  (linear1): Linear(in_features=576, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:36.908815
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2435.516044
	Epoch 1....
Epoch has taken 0:00:20.746342
Number of used sentences in train = 359
Total loss for epoch 1: 1221.049922
	Epoch 2....
Epoch has taken 0:00:20.743978
Number of used sentences in train = 359
Total loss for epoch 2: 901.982802
	Epoch 3....
Epoch has taken 0:00:20.721637
Number of used sentences in train = 359
Total loss for epoch 3: 752.270815
	Epoch 4....
Epoch has taken 0:00:20.723051
Number of used sentences in train = 359
Total loss for epoch 4: 697.819507
	Epoch 5....
Epoch has taken 0:00:20.712524
Number of used sentences in train = 359
Total loss for epoch 5: 686.023373
	Epoch 6....
Epoch has taken 0:00:20.927938
Number of used sentences in train = 359
Total loss for epoch 6: 681.253400
	Epoch 7....
Epoch has taken 0:00:20.727096
Number of used sentences in train = 359
Total loss for epoch 7: 679.227652
	Epoch 8....
Epoch has taken 0:00:20.766067
Number of used sentences in train = 359
Total loss for epoch 8: 678.620927
	Epoch 9....
Epoch has taken 0:00:20.846361
Number of used sentences in train = 359
Total loss for epoch 9: 676.385305
	Epoch 10....
Epoch has taken 0:00:20.733732
Number of used sentences in train = 359
Total loss for epoch 10: 674.662211
	Epoch 11....
Epoch has taken 0:00:20.740026
Number of used sentences in train = 359
Total loss for epoch 11: 673.965840
	Epoch 12....
Epoch has taken 0:00:20.730185
Number of used sentences in train = 359
Total loss for epoch 12: 673.626101
	Epoch 13....
Epoch has taken 0:00:20.757863
Number of used sentences in train = 359
Total loss for epoch 13: 673.346027
	Epoch 14....
Epoch has taken 0:00:20.760300
Number of used sentences in train = 359
Total loss for epoch 14: 673.124542
Epoch has taken 0:00:20.752335

==================================================================================================
	Training time : 0:58:40.618326
==================================================================================================
	Identification : 0.128

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 14, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 30, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 117, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 113, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1882, 113)
  (lstm): LSTM(143, 117, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 20442.945382
validation loss after epoch 0 : 1295.558668
	Epoch 1....
Epoch has taken 0:02:51.646934
Number of used sentences in train = 2811
Total loss for epoch 1: 10593.946752
validation loss after epoch 1 : 954.985878
	Epoch 2....
Epoch has taken 0:02:52.610072
Number of used sentences in train = 2811
Total loss for epoch 2: 9047.074468
validation loss after epoch 2 : 969.185581
	Epoch 3....
Epoch has taken 0:02:51.369723
Number of used sentences in train = 2811
Total loss for epoch 3: 7755.682537
validation loss after epoch 3 : 877.999091
	Epoch 4....
Epoch has taken 0:02:51.234017
Number of used sentences in train = 2811
Total loss for epoch 4: 7168.169516
validation loss after epoch 4 : 916.259442
	Epoch 5....
Epoch has taken 0:02:51.503424
Number of used sentences in train = 2811
Total loss for epoch 5: 6839.599364
validation loss after epoch 5 : 879.822775
	Epoch 6....
Epoch has taken 0:02:51.425679
Number of used sentences in train = 2811
Total loss for epoch 6: 6456.018181
validation loss after epoch 6 : 864.054317
	Epoch 7....
Epoch has taken 0:02:56.254039
Number of used sentences in train = 2811
Total loss for epoch 7: 6120.595792
validation loss after epoch 7 : 877.828094
	Epoch 8....
Epoch has taken 0:02:51.337950
Number of used sentences in train = 2811
Total loss for epoch 8: 5930.328936
validation loss after epoch 8 : 903.409350
	Epoch 9....
Epoch has taken 0:02:51.451002
Number of used sentences in train = 2811
Total loss for epoch 9: 5756.231924
validation loss after epoch 9 : 918.981216
	Epoch 10....
Epoch has taken 0:02:56.579499
Number of used sentences in train = 2811
Total loss for epoch 10: 5626.396939
validation loss after epoch 10 : 899.815646
	Epoch 11....
Epoch has taken 0:02:56.111571
Number of used sentences in train = 2811
Total loss for epoch 11: 5495.059646
validation loss after epoch 11 : 912.657774
	Epoch 12....
Epoch has taken 0:02:52.739342
Number of used sentences in train = 2811
Total loss for epoch 12: 5452.670415
validation loss after epoch 12 : 980.164671
	Epoch 13....
Epoch has taken 0:02:51.302721
Number of used sentences in train = 2811
Total loss for epoch 13: 5376.365920
validation loss after epoch 13 : 958.671699
	Epoch 14....
Epoch has taken 0:02:54.100901
Number of used sentences in train = 2811
Total loss for epoch 14: 5347.598080
validation loss after epoch 14 : 924.013151
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1882, 113)
  (lstm): LSTM(143, 117, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.584545
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1454.523974
	Epoch 1....
Epoch has taken 0:00:18.161447
Number of used sentences in train = 313
Total loss for epoch 1: 1033.798763
	Epoch 2....
Epoch has taken 0:00:18.133404
Number of used sentences in train = 313
Total loss for epoch 2: 871.041810
	Epoch 3....
Epoch has taken 0:00:18.141343
Number of used sentences in train = 313
Total loss for epoch 3: 770.590039
	Epoch 4....
Epoch has taken 0:00:18.140483
Number of used sentences in train = 313
Total loss for epoch 4: 715.218657
	Epoch 5....
Epoch has taken 0:00:18.142369
Number of used sentences in train = 313
Total loss for epoch 5: 677.434160
	Epoch 6....
Epoch has taken 0:00:18.138181
Number of used sentences in train = 313
Total loss for epoch 6: 646.443753
	Epoch 7....
Epoch has taken 0:00:18.132148
Number of used sentences in train = 313
Total loss for epoch 7: 622.342863
	Epoch 8....
Epoch has taken 0:00:18.128240
Number of used sentences in train = 313
Total loss for epoch 8: 587.335640
	Epoch 9....
Epoch has taken 0:00:18.123543
Number of used sentences in train = 313
Total loss for epoch 9: 577.276123
	Epoch 10....
Epoch has taken 0:00:18.136249
Number of used sentences in train = 313
Total loss for epoch 10: 571.172628
	Epoch 11....
Epoch has taken 0:00:18.134130
Number of used sentences in train = 313
Total loss for epoch 11: 567.153972
	Epoch 12....
Epoch has taken 0:00:18.123234
Number of used sentences in train = 313
Total loss for epoch 12: 566.423139
	Epoch 13....
Epoch has taken 0:00:18.119473
Number of used sentences in train = 313
Total loss for epoch 13: 552.152327
	Epoch 14....
Epoch has taken 0:00:18.123922
Number of used sentences in train = 313
Total loss for epoch 14: 532.487186
Epoch has taken 0:00:18.130968

==================================================================================================
	Training time : 0:47:44.776449
==================================================================================================
	Identification : 0.003

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1680, 113)
  (lstm): LSTM(143, 117, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 12686.284060
validation loss after epoch 0 : 830.155903
	Epoch 1....
Epoch has taken 0:01:57.122222
Number of used sentences in train = 2074
Total loss for epoch 1: 6574.182149
validation loss after epoch 1 : 712.260454
	Epoch 2....
Epoch has taken 0:01:57.006451
Number of used sentences in train = 2074
Total loss for epoch 2: 5512.962128
validation loss after epoch 2 : 676.773295
	Epoch 3....
Epoch has taken 0:01:59.937272
Number of used sentences in train = 2074
Total loss for epoch 3: 4897.184382
validation loss after epoch 3 : 676.151226
	Epoch 4....
Epoch has taken 0:01:57.333231
Number of used sentences in train = 2074
Total loss for epoch 4: 4459.193747
validation loss after epoch 4 : 702.897889
	Epoch 5....
Epoch has taken 0:01:59.316998
Number of used sentences in train = 2074
Total loss for epoch 5: 4241.526968
validation loss after epoch 5 : 712.409107
	Epoch 6....
Epoch has taken 0:01:57.266151
Number of used sentences in train = 2074
Total loss for epoch 6: 4037.841853
validation loss after epoch 6 : 653.976309
	Epoch 7....
Epoch has taken 0:01:58.907604
Number of used sentences in train = 2074
Total loss for epoch 7: 3831.356606
validation loss after epoch 7 : 706.043476
	Epoch 8....
Epoch has taken 0:01:57.608631
Number of used sentences in train = 2074
Total loss for epoch 8: 3712.965881
validation loss after epoch 8 : 765.378493
	Epoch 9....
Epoch has taken 0:01:57.871849
Number of used sentences in train = 2074
Total loss for epoch 9: 3626.011289
validation loss after epoch 9 : 836.513393
	Epoch 10....
Epoch has taken 0:01:57.369250
Number of used sentences in train = 2074
Total loss for epoch 10: 3586.285797
validation loss after epoch 10 : 762.086800
	Epoch 11....
Epoch has taken 0:01:57.398176
Number of used sentences in train = 2074
Total loss for epoch 11: 3550.847725
validation loss after epoch 11 : 753.177580
	Epoch 12....
Epoch has taken 0:01:57.400928
Number of used sentences in train = 2074
Total loss for epoch 12: 3469.829171
validation loss after epoch 12 : 811.506325
	Epoch 13....
Epoch has taken 0:01:57.414206
Number of used sentences in train = 2074
Total loss for epoch 13: 3446.778947
validation loss after epoch 13 : 815.425369
	Epoch 14....
Epoch has taken 0:01:57.335135
Number of used sentences in train = 2074
Total loss for epoch 14: 3433.507662
validation loss after epoch 14 : 790.615055
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1680, 113)
  (lstm): LSTM(143, 117, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:00.537486
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1303.203705
	Epoch 1....
Epoch has taken 0:00:12.372495
Number of used sentences in train = 231
Total loss for epoch 1: 814.312960
	Epoch 2....
Epoch has taken 0:00:12.430878
Number of used sentences in train = 231
Total loss for epoch 2: 652.669353
	Epoch 3....
Epoch has taken 0:00:12.325231
Number of used sentences in train = 231
Total loss for epoch 3: 574.238776
	Epoch 4....
Epoch has taken 0:00:12.328814
Number of used sentences in train = 231
Total loss for epoch 4: 502.418195
	Epoch 5....
Epoch has taken 0:00:12.330213
Number of used sentences in train = 231
Total loss for epoch 5: 483.308208
	Epoch 6....
Epoch has taken 0:00:12.552688
Number of used sentences in train = 231
Total loss for epoch 6: 479.454256
	Epoch 7....
Epoch has taken 0:00:12.678004
Number of used sentences in train = 231
Total loss for epoch 7: 447.646866
	Epoch 8....
Epoch has taken 0:00:12.519013
Number of used sentences in train = 231
Total loss for epoch 8: 403.286520
	Epoch 9....
Epoch has taken 0:00:12.335051
Number of used sentences in train = 231
Total loss for epoch 9: 374.587261
	Epoch 10....
Epoch has taken 0:00:12.319571
Number of used sentences in train = 231
Total loss for epoch 10: 368.775384
	Epoch 11....
Epoch has taken 0:00:12.446362
Number of used sentences in train = 231
Total loss for epoch 11: 357.196834
	Epoch 12....
Epoch has taken 0:00:12.338143
Number of used sentences in train = 231
Total loss for epoch 12: 355.504344
	Epoch 13....
Epoch has taken 0:00:12.354680
Number of used sentences in train = 231
Total loss for epoch 13: 357.111349
	Epoch 14....
Epoch has taken 0:00:12.550777
Number of used sentences in train = 231
Total loss for epoch 14: 354.150517
Epoch has taken 0:00:12.358293

==================================================================================================
	Training time : 0:32:36.409854
==================================================================================================
	Identification : 0.447

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(3369, 113)
  (lstm): LSTM(143, 117, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 39791.645570
validation loss after epoch 0 : 4021.500971
	Epoch 1....
Epoch has taken 0:04:14.753417
Number of used sentences in train = 3226
Total loss for epoch 1: 37319.053744
validation loss after epoch 1 : 3996.600652
	Epoch 2....
Epoch has taken 0:04:19.195004
Number of used sentences in train = 3226
Total loss for epoch 2: 37305.869267
validation loss after epoch 2 : 4019.457880
	Epoch 3....
Epoch has taken 0:04:19.715708
Number of used sentences in train = 3226
Total loss for epoch 3: 37254.585067
validation loss after epoch 3 : 3991.984293
	Epoch 4....
Epoch has taken 0:04:20.183498
Number of used sentences in train = 3226
Total loss for epoch 4: 20533.137445
validation loss after epoch 4 : 1286.672135
	Epoch 5....
Epoch has taken 0:04:18.573456
Number of used sentences in train = 3226
Total loss for epoch 5: 10156.585663
validation loss after epoch 5 : 1206.844969
	Epoch 6....
Epoch has taken 0:04:05.813930
Number of used sentences in train = 3226
Total loss for epoch 6: 9221.553253
validation loss after epoch 6 : 1159.468016
	Epoch 7....
Epoch has taken 0:03:56.903619
Number of used sentences in train = 3226
Total loss for epoch 7: 8666.839686
validation loss after epoch 7 : 1199.206017
	Epoch 8....
Epoch has taken 0:04:10.766895
Number of used sentences in train = 3226
Total loss for epoch 8: 8318.648772
validation loss after epoch 8 : 1173.485830
	Epoch 9....
Epoch has taken 0:04:02.295591
Number of used sentences in train = 3226
Total loss for epoch 9: 7906.460032
validation loss after epoch 9 : 1184.207022
	Epoch 10....
Epoch has taken 0:04:12.543147
Number of used sentences in train = 3226
Total loss for epoch 10: 7545.229248
validation loss after epoch 10 : 1297.828660
	Epoch 11....
Epoch has taken 0:04:20.302223
Number of used sentences in train = 3226
Total loss for epoch 11: 7374.673953
validation loss after epoch 11 : 1318.508567
	Epoch 12....
Epoch has taken 0:04:17.458736
Number of used sentences in train = 3226
Total loss for epoch 12: 7170.439971
validation loss after epoch 12 : 1370.196249
	Epoch 13....
Epoch has taken 0:04:19.057900
Number of used sentences in train = 3226
Total loss for epoch 13: 7092.811602
validation loss after epoch 13 : 1354.052432
	Epoch 14....
Epoch has taken 0:04:17.595214
Number of used sentences in train = 3226
Total loss for epoch 14: 6889.277895
validation loss after epoch 14 : 1370.420987
	TransitionClassifier(
  (p_embeddings): Embedding(13, 30)
  (w_embeddings): Embedding(3369, 113)
  (lstm): LSTM(143, 117, num_layers=2, dropout=0.23, bidirectional=True)
  (linear1): Linear(in_features=1872, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:19.249715
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1937.335268
	Epoch 1....
Epoch has taken 0:00:25.440521
Number of used sentences in train = 359
Total loss for epoch 1: 1179.166276
	Epoch 2....
Epoch has taken 0:00:25.926073
Number of used sentences in train = 359
Total loss for epoch 2: 1053.602175
	Epoch 3....
Epoch has taken 0:00:25.642405
Number of used sentences in train = 359
Total loss for epoch 3: 975.682008
	Epoch 4....
Epoch has taken 0:00:25.978496
Number of used sentences in train = 359
Total loss for epoch 4: 895.519497
	Epoch 5....
Epoch has taken 0:00:25.866554
Number of used sentences in train = 359
Total loss for epoch 5: 852.274599
	Epoch 6....
Epoch has taken 0:00:25.766761
Number of used sentences in train = 359
Total loss for epoch 6: 823.464077
	Epoch 7....
Epoch has taken 0:00:25.659576
Number of used sentences in train = 359
Total loss for epoch 7: 782.508452
	Epoch 8....
Epoch has taken 0:00:25.743365
Number of used sentences in train = 359
Total loss for epoch 8: 758.821421
	Epoch 9....
Epoch has taken 0:00:25.735587
Number of used sentences in train = 359
Total loss for epoch 9: 736.383909
	Epoch 10....
Epoch has taken 0:00:25.626845
Number of used sentences in train = 359
Total loss for epoch 10: 716.850421
	Epoch 11....
Epoch has taken 0:00:25.581249
Number of used sentences in train = 359
Total loss for epoch 11: 745.809461
	Epoch 12....
Epoch has taken 0:00:25.603086
Number of used sentences in train = 359
Total loss for epoch 12: 721.422411
	Epoch 13....
Epoch has taken 0:00:25.714566
Number of used sentences in train = 359
Total loss for epoch 13: 728.764004
	Epoch 14....
Epoch has taken 0:00:25.519704
Number of used sentences in train = 359
Total loss for epoch 14: 709.392727
Epoch has taken 0:00:25.728647

==================================================================================================
	Training time : 1:10:00.617520
==================================================================================================
	Identification : 0.473

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 15, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 55, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 77, 'lstmDropout': 0.17, 'denseActivation': 'tanh', 'wordDim': 94, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(9332, 94)
  (lstm): LSTM(149, 77, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1232, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 19382.349699
validation loss after epoch 0 : 1487.137513
	Epoch 1....
Epoch has taken 0:03:16.235234
Number of used sentences in train = 2811
Total loss for epoch 1: 11435.666605
validation loss after epoch 1 : 1117.891661
	Epoch 2....
Epoch has taken 0:03:15.068209
Number of used sentences in train = 2811
Total loss for epoch 2: 8882.023437
validation loss after epoch 2 : 1068.347512
	Epoch 3....
Epoch has taken 0:03:12.850288
Number of used sentences in train = 2811
Total loss for epoch 3: 7595.450093
validation loss after epoch 3 : 1095.969326
	Epoch 4....
Epoch has taken 0:03:01.514378
Number of used sentences in train = 2811
Total loss for epoch 4: 6758.699624
validation loss after epoch 4 : 1157.656185
	Epoch 5....
Epoch has taken 0:02:56.021367
Number of used sentences in train = 2811
Total loss for epoch 5: 6167.044553
validation loss after epoch 5 : 1126.754214
	Epoch 6....
Epoch has taken 0:02:56.130462
Number of used sentences in train = 2811
Total loss for epoch 6: 5814.364576
validation loss after epoch 6 : 1271.250510
	Epoch 7....
Epoch has taken 0:02:54.677460
Number of used sentences in train = 2811
Total loss for epoch 7: 5502.137033
validation loss after epoch 7 : 1262.628204
	Epoch 8....
Epoch has taken 0:02:55.226741
Number of used sentences in train = 2811
Total loss for epoch 8: 5305.246922
validation loss after epoch 8 : 1308.314959
	Epoch 9....
Epoch has taken 0:02:55.589094
Number of used sentences in train = 2811
Total loss for epoch 9: 5092.976557
validation loss after epoch 9 : 1366.140539
	Epoch 10....
Epoch has taken 0:02:56.410474
Number of used sentences in train = 2811
Total loss for epoch 10: 4999.449922
validation loss after epoch 10 : 1332.818354
	Epoch 11....
Epoch has taken 0:02:56.213673
Number of used sentences in train = 2811
Total loss for epoch 11: 4909.077765
validation loss after epoch 11 : 1500.302460
	Epoch 12....
Epoch has taken 0:02:54.365894
Number of used sentences in train = 2811
Total loss for epoch 12: 4843.028991
validation loss after epoch 12 : 1431.172418
	Epoch 13....
Epoch has taken 0:02:56.273118
Number of used sentences in train = 2811
Total loss for epoch 13: 4762.164808
validation loss after epoch 13 : 1453.803150
	Epoch 14....
Epoch has taken 0:02:56.357914
Number of used sentences in train = 2811
Total loss for epoch 14: 4761.636442
validation loss after epoch 14 : 1538.229485
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(9332, 94)
  (lstm): LSTM(149, 77, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1232, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.088124
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1911.451247
	Epoch 1....
Epoch has taken 0:00:18.719509
Number of used sentences in train = 313
Total loss for epoch 1: 967.932957
	Epoch 2....
Epoch has taken 0:00:18.871461
Number of used sentences in train = 313
Total loss for epoch 2: 771.715072
	Epoch 3....
Epoch has taken 0:00:18.550453
Number of used sentences in train = 313
Total loss for epoch 3: 685.311039
	Epoch 4....
Epoch has taken 0:00:18.624101
Number of used sentences in train = 313
Total loss for epoch 4: 610.730249
	Epoch 5....
Epoch has taken 0:00:18.728400
Number of used sentences in train = 313
Total loss for epoch 5: 586.774704
	Epoch 6....
Epoch has taken 0:00:19.045791
Number of used sentences in train = 313
Total loss for epoch 6: 562.994229
	Epoch 7....
Epoch has taken 0:00:18.833731
Number of used sentences in train = 313
Total loss for epoch 7: 556.444958
	Epoch 8....
Epoch has taken 0:00:18.988541
Number of used sentences in train = 313
Total loss for epoch 8: 547.131976
	Epoch 9....
Epoch has taken 0:00:18.994076
Number of used sentences in train = 313
Total loss for epoch 9: 536.940816
	Epoch 10....
Epoch has taken 0:00:18.761603
Number of used sentences in train = 313
Total loss for epoch 10: 528.990603
	Epoch 11....
Epoch has taken 0:00:19.196226
Number of used sentences in train = 313
Total loss for epoch 11: 528.111496
	Epoch 12....
Epoch has taken 0:00:18.930246
Number of used sentences in train = 313
Total loss for epoch 12: 526.260006
	Epoch 13....
Epoch has taken 0:00:18.855082
Number of used sentences in train = 313
Total loss for epoch 13: 523.876125
	Epoch 14....
Epoch has taken 0:00:18.917942
Number of used sentences in train = 313
Total loss for epoch 14: 522.811894
Epoch has taken 0:00:18.955790

==================================================================================================
	Training time : 0:49:43.510533
==================================================================================================
	Identification : 0.467

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(7118, 94)
  (lstm): LSTM(149, 77, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1232, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 21830.915017
validation loss after epoch 0 : 2115.532852
	Epoch 1....
Epoch has taken 0:02:02.029108
Number of used sentences in train = 2074
Total loss for epoch 1: 12043.915207
validation loss after epoch 1 : 1050.975009
	Epoch 2....
Epoch has taken 0:02:02.234858
Number of used sentences in train = 2074
Total loss for epoch 2: 7618.939364
validation loss after epoch 2 : 933.645395
	Epoch 3....
Epoch has taken 0:02:00.945254
Number of used sentences in train = 2074
Total loss for epoch 3: 6105.742554
validation loss after epoch 3 : 844.810007
	Epoch 4....
Epoch has taken 0:02:00.872446
Number of used sentences in train = 2074
Total loss for epoch 4: 5190.034986
validation loss after epoch 4 : 967.765555
	Epoch 5....
Epoch has taken 0:02:00.954669
Number of used sentences in train = 2074
Total loss for epoch 5: 4527.529660
validation loss after epoch 5 : 931.456303
	Epoch 6....
Epoch has taken 0:02:00.522428
Number of used sentences in train = 2074
Total loss for epoch 6: 4124.019588
validation loss after epoch 6 : 986.680350
	Epoch 7....
Epoch has taken 0:02:00.856232
Number of used sentences in train = 2074
Total loss for epoch 7: 3833.732896
validation loss after epoch 7 : 992.485563
	Epoch 8....
Epoch has taken 0:02:00.968909
Number of used sentences in train = 2074
Total loss for epoch 8: 3711.367251
validation loss after epoch 8 : 1030.683345
	Epoch 9....
Epoch has taken 0:02:03.937347
Number of used sentences in train = 2074
Total loss for epoch 9: 3610.131411
validation loss after epoch 9 : 1073.561540
	Epoch 10....
Epoch has taken 0:02:12.301908
Number of used sentences in train = 2074
Total loss for epoch 10: 3537.815503
validation loss after epoch 10 : 1111.864371
	Epoch 11....
Epoch has taken 0:02:11.235545
Number of used sentences in train = 2074
Total loss for epoch 11: 3457.931360
validation loss after epoch 11 : 1100.334620
	Epoch 12....
Epoch has taken 0:02:10.867142
Number of used sentences in train = 2074
Total loss for epoch 12: 3388.124062
validation loss after epoch 12 : 1131.396541
	Epoch 13....
Epoch has taken 0:02:12.239821
Number of used sentences in train = 2074
Total loss for epoch 13: 3357.575451
validation loss after epoch 13 : 1127.511124
	Epoch 14....
Epoch has taken 0:02:12.437591
Number of used sentences in train = 2074
Total loss for epoch 14: 3341.417896
validation loss after epoch 14 : 1138.659938
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(7118, 94)
  (lstm): LSTM(149, 77, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1232, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:06.143719
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1895.765788
	Epoch 1....
Epoch has taken 0:00:13.602753
Number of used sentences in train = 231
Total loss for epoch 1: 834.335452
	Epoch 2....
Epoch has taken 0:00:13.351104
Number of used sentences in train = 231
Total loss for epoch 2: 632.483234
	Epoch 3....
Epoch has taken 0:00:12.349631
Number of used sentences in train = 231
Total loss for epoch 3: 519.880483
	Epoch 4....
Epoch has taken 0:00:12.360252
Number of used sentences in train = 231
Total loss for epoch 4: 461.888747
	Epoch 5....
Epoch has taken 0:00:12.361117
Number of used sentences in train = 231
Total loss for epoch 5: 428.420028
	Epoch 6....
Epoch has taken 0:00:12.380384
Number of used sentences in train = 231
Total loss for epoch 6: 385.900871
	Epoch 7....
Epoch has taken 0:00:12.360648
Number of used sentences in train = 231
Total loss for epoch 7: 370.901596
	Epoch 8....
Epoch has taken 0:00:12.348829
Number of used sentences in train = 231
Total loss for epoch 8: 359.404636
	Epoch 9....
Epoch has taken 0:00:12.361956
Number of used sentences in train = 231
Total loss for epoch 9: 355.625602
	Epoch 10....
Epoch has taken 0:00:12.358099
Number of used sentences in train = 231
Total loss for epoch 10: 358.251896
	Epoch 11....
Epoch has taken 0:00:12.382727
Number of used sentences in train = 231
Total loss for epoch 11: 350.133615
	Epoch 12....
Epoch has taken 0:00:12.365552
Number of used sentences in train = 231
Total loss for epoch 12: 362.034878
	Epoch 13....
Epoch has taken 0:00:12.350932
Number of used sentences in train = 231
Total loss for epoch 13: 353.444153
	Epoch 14....
Epoch has taken 0:00:12.364322
Number of used sentences in train = 231
Total loss for epoch 14: 349.533124
Epoch has taken 0:00:12.361333

==================================================================================================
	Training time : 0:34:26.554787
==================================================================================================
	Identification : 0.094

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 55)
  (w_embeddings): Embedding(18021, 94)
  (lstm): LSTM(149, 77, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1232, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20868.791471
validation loss after epoch 0 : 1653.121939
	Epoch 1....
Epoch has taken 0:03:52.923495
Number of used sentences in train = 3226
Total loss for epoch 1: 13257.249734
validation loss after epoch 1 : 1572.890775
	Epoch 2....
Epoch has taken 0:03:52.425046
Number of used sentences in train = 3226
Total loss for epoch 2: 10728.712111
validation loss after epoch 2 : 1660.980466
	Epoch 3....
Epoch has taken 0:03:52.937205
Number of used sentences in train = 3226
Total loss for epoch 3: 9204.814346
validation loss after epoch 3 : 1613.833595
	Epoch 4....
Epoch has taken 0:03:53.004480
Number of used sentences in train = 3226
Total loss for epoch 4: 8178.856379
validation loss after epoch 4 : 1771.959351
	Epoch 5....
Epoch has taken 0:03:51.647717
Number of used sentences in train = 3226
Total loss for epoch 5: 7538.191526
validation loss after epoch 5 : 2030.520364
	Epoch 6....
Epoch has taken 0:03:53.081357
Number of used sentences in train = 3226
Total loss for epoch 6: 7153.976487
validation loss after epoch 6 : 2051.097406
	Epoch 7....
Epoch has taken 0:03:53.121413
Number of used sentences in train = 3226
Total loss for epoch 7: 6818.593360
validation loss after epoch 7 : 2250.785121
	Epoch 8....
Epoch has taken 0:03:52.962549
Number of used sentences in train = 3226
Total loss for epoch 8: 6624.183818
validation loss after epoch 8 : 2191.748394
	Epoch 9....
Epoch has taken 0:03:53.003366
Number of used sentences in train = 3226
Total loss for epoch 9: 6521.094599
validation loss after epoch 9 : 2382.505472
	Epoch 10....
Epoch has taken 0:03:52.504195
Number of used sentences in train = 3226
Total loss for epoch 10: 6402.861738
validation loss after epoch 10 : 2502.031246
	Epoch 11....
Epoch has taken 0:03:53.065328
Number of used sentences in train = 3226
Total loss for epoch 11: 6376.507216
validation loss after epoch 11 : 2453.281849
	Epoch 12....
Epoch has taken 0:03:52.769461
Number of used sentences in train = 3226
Total loss for epoch 12: 6325.986649
validation loss after epoch 12 : 2653.906821
	Epoch 13....
Epoch has taken 0:03:52.310130
Number of used sentences in train = 3226
Total loss for epoch 13: 6298.828882
validation loss after epoch 13 : 2625.647277
	Epoch 14....
Epoch has taken 0:03:52.062023
Number of used sentences in train = 3226
Total loss for epoch 14: 6276.670352
validation loss after epoch 14 : 2554.707179
	TransitionClassifier(
  (p_embeddings): Embedding(13, 55)
  (w_embeddings): Embedding(18021, 94)
  (lstm): LSTM(149, 77, num_layers=2, dropout=0.17, bidirectional=True)
  (linear1): Linear(in_features=1232, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:51.969146
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2207.431397
	Epoch 1....
Epoch has taken 0:00:22.935090
Number of used sentences in train = 359
Total loss for epoch 1: 1425.761764
	Epoch 2....
Epoch has taken 0:00:22.943278
Number of used sentences in train = 359
Total loss for epoch 2: 1099.105795
	Epoch 3....
Epoch has taken 0:00:22.957792
Number of used sentences in train = 359
Total loss for epoch 3: 894.069270
	Epoch 4....
Epoch has taken 0:00:22.953504
Number of used sentences in train = 359
Total loss for epoch 4: 770.686700
	Epoch 5....
Epoch has taken 0:00:22.935701
Number of used sentences in train = 359
Total loss for epoch 5: 741.363986
	Epoch 6....
Epoch has taken 0:00:22.935971
Number of used sentences in train = 359
Total loss for epoch 6: 708.314185
	Epoch 7....
Epoch has taken 0:00:22.921281
Number of used sentences in train = 359
Total loss for epoch 7: 690.378565
	Epoch 8....
Epoch has taken 0:00:22.861572
Number of used sentences in train = 359
Total loss for epoch 8: 679.989658
	Epoch 9....
Epoch has taken 0:00:22.941468
Number of used sentences in train = 359
Total loss for epoch 9: 689.276820
	Epoch 10....
Epoch has taken 0:00:22.959139
Number of used sentences in train = 359
Total loss for epoch 10: 675.908053
	Epoch 11....
Epoch has taken 0:00:22.944615
Number of used sentences in train = 359
Total loss for epoch 11: 675.362374
	Epoch 12....
Epoch has taken 0:00:22.963073
Number of used sentences in train = 359
Total loss for epoch 12: 675.274660
	Epoch 13....
Epoch has taken 0:00:22.935849
Number of used sentences in train = 359
Total loss for epoch 13: 674.054271
	Epoch 14....
Epoch has taken 0:00:22.949203
Number of used sentences in train = 359
Total loss for epoch 14: 676.490271
Epoch has taken 0:00:22.869749

==================================================================================================
	Training time : 1:03:54.483004
==================================================================================================
	Identification : 0.463

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 21, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 23, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 147, 'lstmDropout': 0.18, 'denseActivation': 'tanh', 'wordDim': 168, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 23)
  (w_embeddings): Embedding(9253, 168)
  (lstm): LSTM(191, 147, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15974.771288
validation loss after epoch 0 : 1178.850862
	Epoch 1....
Epoch has taken 0:02:56.104758
Number of used sentences in train = 2811
Total loss for epoch 1: 10437.779266
validation loss after epoch 1 : 1107.897202
	Epoch 2....
Epoch has taken 0:02:57.227882
Number of used sentences in train = 2811
Total loss for epoch 2: 8676.247528
validation loss after epoch 2 : 1125.139381
	Epoch 3....
Epoch has taken 0:03:02.907650
Number of used sentences in train = 2811
Total loss for epoch 3: 7395.580688
validation loss after epoch 3 : 1179.208429
	Epoch 4....
Epoch has taken 0:03:12.898778
Number of used sentences in train = 2811
Total loss for epoch 4: 6639.147509
validation loss after epoch 4 : 1184.754075
	Epoch 5....
Epoch has taken 0:03:06.564697
Number of used sentences in train = 2811
Total loss for epoch 5: 6168.620355
validation loss after epoch 5 : 1325.042763
	Epoch 6....
Epoch has taken 0:02:57.591736
Number of used sentences in train = 2811
Total loss for epoch 6: 5843.376322
validation loss after epoch 6 : 1296.315366
	Epoch 7....
Epoch has taken 0:02:58.383272
Number of used sentences in train = 2811
Total loss for epoch 7: 5561.967675
validation loss after epoch 7 : 1277.014035
	Epoch 8....
Epoch has taken 0:02:58.586043
Number of used sentences in train = 2811
Total loss for epoch 8: 5434.447528
validation loss after epoch 8 : 1338.395641
	Epoch 9....
Epoch has taken 0:02:58.204829
Number of used sentences in train = 2811
Total loss for epoch 9: 5279.072401
validation loss after epoch 9 : 1418.070884
	Epoch 10....
Epoch has taken 0:02:58.324502
Number of used sentences in train = 2811
Total loss for epoch 10: 5199.509838
validation loss after epoch 10 : 1381.437032
	Epoch 11....
Epoch has taken 0:02:57.848226
Number of used sentences in train = 2811
Total loss for epoch 11: 5113.774960
validation loss after epoch 11 : 1355.586295
	Epoch 12....
Epoch has taken 0:02:59.592131
Number of used sentences in train = 2811
Total loss for epoch 12: 5045.929794
validation loss after epoch 12 : 1449.466071
	Epoch 13....
Epoch has taken 0:02:59.111018
Number of used sentences in train = 2811
Total loss for epoch 13: 4991.790837
validation loss after epoch 13 : 1426.449260
	Epoch 14....
Epoch has taken 0:02:58.108020
Number of used sentences in train = 2811
Total loss for epoch 14: 4992.724429
validation loss after epoch 14 : 1390.854566
	TransitionClassifier(
  (p_embeddings): Embedding(18, 23)
  (w_embeddings): Embedding(9253, 168)
  (lstm): LSTM(191, 147, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.128079
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1869.776075
	Epoch 1....
Epoch has taken 0:00:18.751727
Number of used sentences in train = 313
Total loss for epoch 1: 1097.367380
	Epoch 2....
Epoch has taken 0:00:18.763262
Number of used sentences in train = 313
Total loss for epoch 2: 829.274813
	Epoch 3....
Epoch has taken 0:00:19.369693
Number of used sentences in train = 313
Total loss for epoch 3: 731.856922
	Epoch 4....
Epoch has taken 0:00:20.682583
Number of used sentences in train = 313
Total loss for epoch 4: 638.947037
	Epoch 5....
Epoch has taken 0:00:20.666051
Number of used sentences in train = 313
Total loss for epoch 5: 601.546163
	Epoch 6....
Epoch has taken 0:00:20.680670
Number of used sentences in train = 313
Total loss for epoch 6: 600.193684
	Epoch 7....
Epoch has taken 0:00:20.670683
Number of used sentences in train = 313
Total loss for epoch 7: 564.927311
	Epoch 8....
Epoch has taken 0:00:20.667165
Number of used sentences in train = 313
Total loss for epoch 8: 553.557625
	Epoch 9....
Epoch has taken 0:00:20.687064
Number of used sentences in train = 313
Total loss for epoch 9: 548.200391
	Epoch 10....
Epoch has taken 0:00:20.664719
Number of used sentences in train = 313
Total loss for epoch 10: 536.227830
	Epoch 11....
Epoch has taken 0:00:20.470068
Number of used sentences in train = 313
Total loss for epoch 11: 538.106191
	Epoch 12....
Epoch has taken 0:00:20.679885
Number of used sentences in train = 313
Total loss for epoch 12: 536.220435
	Epoch 13....
Epoch has taken 0:00:20.688288
Number of used sentences in train = 313
Total loss for epoch 13: 534.279290
	Epoch 14....
Epoch has taken 0:00:20.685660
Number of used sentences in train = 313
Total loss for epoch 14: 532.485894
Epoch has taken 0:00:20.665615

==================================================================================================
	Training time : 0:50:03.905713
==================================================================================================
	Identification : 0.064

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 23)
  (w_embeddings): Embedding(7014, 168)
  (lstm): LSTM(191, 147, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 16840.356495
validation loss after epoch 0 : 1189.559750
	Epoch 1....
Epoch has taken 0:02:11.509867
Number of used sentences in train = 2074
Total loss for epoch 1: 8584.302353
validation loss after epoch 1 : 1104.655093
	Epoch 2....
Epoch has taken 0:02:12.877852
Number of used sentences in train = 2074
Total loss for epoch 2: 6463.422025
validation loss after epoch 2 : 947.623073
	Epoch 3....
Epoch has taken 0:02:12.814937
Number of used sentences in train = 2074
Total loss for epoch 3: 5219.374197
validation loss after epoch 3 : 976.912408
	Epoch 4....
Epoch has taken 0:02:10.054443
Number of used sentences in train = 2074
Total loss for epoch 4: 4420.237181
validation loss after epoch 4 : 1083.138796
	Epoch 5....
Epoch has taken 0:02:02.341070
Number of used sentences in train = 2074
Total loss for epoch 5: 4011.199515
validation loss after epoch 5 : 1188.796476
	Epoch 6....
Epoch has taken 0:02:01.575936
Number of used sentences in train = 2074
Total loss for epoch 6: 3717.482632
validation loss after epoch 6 : 1135.644680
	Epoch 7....
Epoch has taken 0:02:01.338731
Number of used sentences in train = 2074
Total loss for epoch 7: 3577.364799
validation loss after epoch 7 : 1152.922028
	Epoch 8....
Epoch has taken 0:02:04.953074
Number of used sentences in train = 2074
Total loss for epoch 8: 3480.181639
validation loss after epoch 8 : 1255.179950
	Epoch 9....
Epoch has taken 0:02:10.174390
Number of used sentences in train = 2074
Total loss for epoch 9: 3384.636024
validation loss after epoch 9 : 1358.284337
	Epoch 10....
Epoch has taken 0:02:14.017472
Number of used sentences in train = 2074
Total loss for epoch 10: 3374.104448
validation loss after epoch 10 : 1250.435525
	Epoch 11....
Epoch has taken 0:02:07.237110
Number of used sentences in train = 2074
Total loss for epoch 11: 3353.953861
validation loss after epoch 11 : 1437.811828
	Epoch 12....
Epoch has taken 0:02:01.440752
Number of used sentences in train = 2074
Total loss for epoch 12: 3309.598505
validation loss after epoch 12 : 1403.498509
	Epoch 13....
Epoch has taken 0:02:01.287658
Number of used sentences in train = 2074
Total loss for epoch 13: 3287.089190
validation loss after epoch 13 : 1374.732558
	Epoch 14....
Epoch has taken 0:02:01.256597
Number of used sentences in train = 2074
Total loss for epoch 14: 3286.433471
validation loss after epoch 14 : 1359.601423
	TransitionClassifier(
  (p_embeddings): Embedding(18, 23)
  (w_embeddings): Embedding(7014, 168)
  (lstm): LSTM(191, 147, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.218374
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1785.310100
	Epoch 1....
Epoch has taken 0:00:12.377992
Number of used sentences in train = 231
Total loss for epoch 1: 874.857113
	Epoch 2....
Epoch has taken 0:00:12.383773
Number of used sentences in train = 231
Total loss for epoch 2: 635.182684
	Epoch 3....
Epoch has taken 0:00:12.364304
Number of used sentences in train = 231
Total loss for epoch 3: 508.120942
	Epoch 4....
Epoch has taken 0:00:12.359340
Number of used sentences in train = 231
Total loss for epoch 4: 436.165349
	Epoch 5....
Epoch has taken 0:00:12.383083
Number of used sentences in train = 231
Total loss for epoch 5: 419.412068
	Epoch 6....
Epoch has taken 0:00:12.375019
Number of used sentences in train = 231
Total loss for epoch 6: 399.319461
	Epoch 7....
Epoch has taken 0:00:12.370423
Number of used sentences in train = 231
Total loss for epoch 7: 392.244125
	Epoch 8....
Epoch has taken 0:00:12.365962
Number of used sentences in train = 231
Total loss for epoch 8: 379.480772
	Epoch 9....
Epoch has taken 0:00:12.363089
Number of used sentences in train = 231
Total loss for epoch 9: 368.401636
	Epoch 10....
Epoch has taken 0:00:12.383069
Number of used sentences in train = 231
Total loss for epoch 10: 362.256945
	Epoch 11....
Epoch has taken 0:00:12.384537
Number of used sentences in train = 231
Total loss for epoch 11: 354.465817
	Epoch 12....
Epoch has taken 0:00:12.368486
Number of used sentences in train = 231
Total loss for epoch 12: 356.807332
	Epoch 13....
Epoch has taken 0:00:12.366566
Number of used sentences in train = 231
Total loss for epoch 13: 351.691186
	Epoch 14....
Epoch has taken 0:00:12.367625
Number of used sentences in train = 231
Total loss for epoch 14: 351.448241
Epoch has taken 0:00:12.391055

==================================================================================================
	Training time : 0:34:40.061814
==================================================================================================
	Identification : 0.064

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 23)
  (w_embeddings): Embedding(18132, 168)
  (lstm): LSTM(191, 147, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 38712.375912
validation loss after epoch 0 : 3995.712591
	Epoch 1....
Epoch has taken 0:03:54.042633
Number of used sentences in train = 3226
Total loss for epoch 1: 37278.081241
validation loss after epoch 1 : 3982.869563
	Epoch 2....
Epoch has taken 0:03:53.236707
Number of used sentences in train = 3226
Total loss for epoch 2: 33969.688441
validation loss after epoch 2 : 2242.380957
	Epoch 3....
Epoch has taken 0:03:53.070032
Number of used sentences in train = 3226
Total loss for epoch 3: 16733.807448
validation loss after epoch 3 : 1797.848013
	Epoch 4....
Epoch has taken 0:03:54.674857
Number of used sentences in train = 3226
Total loss for epoch 4: 12566.110467
validation loss after epoch 4 : 1625.238256
	Epoch 5....
Epoch has taken 0:03:54.526113
Number of used sentences in train = 3226
Total loss for epoch 5: 10451.466515
validation loss after epoch 5 : 1657.877918
	Epoch 6....
Epoch has taken 0:03:53.725113
Number of used sentences in train = 3226
Total loss for epoch 6: 8899.650641
validation loss after epoch 6 : 1783.241539
	Epoch 7....
Epoch has taken 0:03:54.627977
Number of used sentences in train = 3226
Total loss for epoch 7: 8009.043930
validation loss after epoch 7 : 1909.419648
	Epoch 8....
Epoch has taken 0:03:54.566238
Number of used sentences in train = 3226
Total loss for epoch 8: 7423.993439
validation loss after epoch 8 : 2137.663778
	Epoch 9....
Epoch has taken 0:03:54.849537
Number of used sentences in train = 3226
Total loss for epoch 9: 7024.553187
validation loss after epoch 9 : 2221.238030
	Epoch 10....
Epoch has taken 0:03:53.636700
Number of used sentences in train = 3226
Total loss for epoch 10: 6697.096786
validation loss after epoch 10 : 2239.813981
	Epoch 11....
Epoch has taken 0:03:53.118212
Number of used sentences in train = 3226
Total loss for epoch 11: 6591.291712
validation loss after epoch 11 : 2335.968641
	Epoch 12....
Epoch has taken 0:03:53.870504
Number of used sentences in train = 3226
Total loss for epoch 12: 6469.646670
validation loss after epoch 12 : 2161.687821
	Epoch 13....
Epoch has taken 0:03:53.925786
Number of used sentences in train = 3226
Total loss for epoch 13: 6338.276590
validation loss after epoch 13 : 2415.095949
	Epoch 14....
Epoch has taken 0:03:54.570131
Number of used sentences in train = 3226
Total loss for epoch 14: 6304.389886
validation loss after epoch 14 : 2542.054330
	TransitionClassifier(
  (p_embeddings): Embedding(13, 23)
  (w_embeddings): Embedding(18132, 168)
  (lstm): LSTM(191, 147, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=2352, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:54.226061
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2400.892257
	Epoch 1....
Epoch has taken 0:00:23.026124
Number of used sentences in train = 359
Total loss for epoch 1: 1528.217502
	Epoch 2....
Epoch has taken 0:00:23.045600
Number of used sentences in train = 359
Total loss for epoch 2: 1186.927089
	Epoch 3....
Epoch has taken 0:00:22.987473
Number of used sentences in train = 359
Total loss for epoch 3: 1052.568310
	Epoch 4....
Epoch has taken 0:00:23.035809
Number of used sentences in train = 359
Total loss for epoch 4: 874.518861
	Epoch 5....
Epoch has taken 0:00:23.039165
Number of used sentences in train = 359
Total loss for epoch 5: 828.152856
	Epoch 6....
Epoch has taken 0:00:23.030626
Number of used sentences in train = 359
Total loss for epoch 6: 783.266379
	Epoch 7....
Epoch has taken 0:00:23.054049
Number of used sentences in train = 359
Total loss for epoch 7: 758.163313
	Epoch 8....
Epoch has taken 0:00:23.055042
Number of used sentences in train = 359
Total loss for epoch 8: 718.593325
	Epoch 9....
Epoch has taken 0:00:23.064302
Number of used sentences in train = 359
Total loss for epoch 9: 706.770592
	Epoch 10....
Epoch has taken 0:00:23.023129
Number of used sentences in train = 359
Total loss for epoch 10: 702.197613
	Epoch 11....
Epoch has taken 0:00:23.040028
Number of used sentences in train = 359
Total loss for epoch 11: 712.103199
	Epoch 12....
Epoch has taken 0:00:23.039158
Number of used sentences in train = 359
Total loss for epoch 12: 696.895535
	Epoch 13....
Epoch has taken 0:00:23.033227
Number of used sentences in train = 359
Total loss for epoch 13: 693.573810
	Epoch 14....
Epoch has taken 0:00:23.032066
Number of used sentences in train = 359
Total loss for epoch 14: 691.848770
Epoch has taken 0:00:23.042013

==================================================================================================
	Training time : 1:04:16.932468
==================================================================================================
	Identification : 0.44

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 76, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 29, 'lstmDropout': 0.19, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5860, 63)
  (lstm): LSTM(80, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12681.235798
validation loss after epoch 0 : 1103.945119
	Epoch 1....
Epoch has taken 0:02:42.553044
Number of used sentences in train = 2811
Total loss for epoch 1: 8431.295711
validation loss after epoch 1 : 1093.533339
	Epoch 2....
Epoch has taken 0:02:44.106533
Number of used sentences in train = 2811
Total loss for epoch 2: 7026.248753
validation loss after epoch 2 : 1050.457497
	Epoch 3....
Epoch has taken 0:02:44.391842
Number of used sentences in train = 2811
Total loss for epoch 3: 6143.380807
validation loss after epoch 3 : 1133.085543
	Epoch 4....
Epoch has taken 0:02:44.492320
Number of used sentences in train = 2811
Total loss for epoch 4: 5545.279141
validation loss after epoch 4 : 1176.255203
	Epoch 5....
Epoch has taken 0:02:44.547150
Number of used sentences in train = 2811
Total loss for epoch 5: 5170.926690
validation loss after epoch 5 : 1272.062657
	Epoch 6....
Epoch has taken 0:02:44.587598
Number of used sentences in train = 2811
Total loss for epoch 6: 4920.961487
validation loss after epoch 6 : 1317.252113
	Epoch 7....
Epoch has taken 0:02:44.401603
Number of used sentences in train = 2811
Total loss for epoch 7: 4734.775573
validation loss after epoch 7 : 1397.514371
	Epoch 8....
Epoch has taken 0:02:40.556331
Number of used sentences in train = 2811
Total loss for epoch 8: 4637.685043
validation loss after epoch 8 : 1443.750925
	Epoch 9....
Epoch has taken 0:02:40.736106
Number of used sentences in train = 2811
Total loss for epoch 9: 4572.519603
validation loss after epoch 9 : 1502.557561
	Epoch 10....
Epoch has taken 0:02:42.239344
Number of used sentences in train = 2811
Total loss for epoch 10: 4542.711035
validation loss after epoch 10 : 1520.876936
	Epoch 11....
Epoch has taken 0:02:39.770457
Number of used sentences in train = 2811
Total loss for epoch 11: 4523.223194
validation loss after epoch 11 : 1576.183964
	Epoch 12....
Epoch has taken 0:02:41.949990
Number of used sentences in train = 2811
Total loss for epoch 12: 4511.566639
validation loss after epoch 12 : 1592.450542
	Epoch 13....
Epoch has taken 0:02:39.624320
Number of used sentences in train = 2811
Total loss for epoch 13: 4500.845308
validation loss after epoch 13 : 1618.922887
	Epoch 14....
Epoch has taken 0:02:42.982547
Number of used sentences in train = 2811
Total loss for epoch 14: 4496.271015
validation loss after epoch 14 : 1642.581857
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5860, 63)
  (lstm): LSTM(80, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:39.408637
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1568.501063
	Epoch 1....
Epoch has taken 0:00:16.841140
Number of used sentences in train = 313
Total loss for epoch 1: 794.779129
	Epoch 2....
Epoch has taken 0:00:16.912383
Number of used sentences in train = 313
Total loss for epoch 2: 634.680088
	Epoch 3....
Epoch has taken 0:00:17.305560
Number of used sentences in train = 313
Total loss for epoch 3: 559.946422
	Epoch 4....
Epoch has taken 0:00:16.855638
Number of used sentences in train = 313
Total loss for epoch 4: 528.802600
	Epoch 5....
Epoch has taken 0:00:16.851659
Number of used sentences in train = 313
Total loss for epoch 5: 516.836159
	Epoch 6....
Epoch has taken 0:00:16.860822
Number of used sentences in train = 313
Total loss for epoch 6: 511.454314
	Epoch 7....
Epoch has taken 0:00:16.871893
Number of used sentences in train = 313
Total loss for epoch 7: 508.516947
	Epoch 8....
Epoch has taken 0:00:16.869055
Number of used sentences in train = 313
Total loss for epoch 8: 507.004324
	Epoch 9....
Epoch has taken 0:00:16.863844
Number of used sentences in train = 313
Total loss for epoch 9: 505.428057
	Epoch 10....
Epoch has taken 0:00:16.861141
Number of used sentences in train = 313
Total loss for epoch 10: 504.416432
	Epoch 11....
Epoch has taken 0:00:16.868823
Number of used sentences in train = 313
Total loss for epoch 11: 503.875730
	Epoch 12....
Epoch has taken 0:00:16.862221
Number of used sentences in train = 313
Total loss for epoch 12: 504.937438
	Epoch 13....
Epoch has taken 0:00:16.870759
Number of used sentences in train = 313
Total loss for epoch 13: 504.128032
	Epoch 14....
Epoch has taken 0:00:16.879718
Number of used sentences in train = 313
Total loss for epoch 14: 502.870077
Epoch has taken 0:00:16.864361

==================================================================================================
	Training time : 0:44:50.292910
==================================================================================================
	Identification : 0.294

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5660, 63)
  (lstm): LSTM(80, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10052.163044
validation loss after epoch 0 : 882.932411
	Epoch 1....
Epoch has taken 0:01:49.115207
Number of used sentences in train = 2074
Total loss for epoch 1: 6090.701279
validation loss after epoch 1 : 816.161447
	Epoch 2....
Epoch has taken 0:01:51.502697
Number of used sentences in train = 2074
Total loss for epoch 2: 4864.667363
validation loss after epoch 2 : 864.714071
	Epoch 3....
Epoch has taken 0:01:49.098487
Number of used sentences in train = 2074
Total loss for epoch 3: 4088.203855
validation loss after epoch 3 : 967.239902
	Epoch 4....
Epoch has taken 0:01:51.452863
Number of used sentences in train = 2074
Total loss for epoch 4: 3650.472061
validation loss after epoch 4 : 1023.246615
	Epoch 5....
Epoch has taken 0:01:49.430644
Number of used sentences in train = 2074
Total loss for epoch 5: 3407.652533
validation loss after epoch 5 : 1119.804528
	Epoch 6....
Epoch has taken 0:01:49.186490
Number of used sentences in train = 2074
Total loss for epoch 6: 3269.714211
validation loss after epoch 6 : 1154.542970
	Epoch 7....
Epoch has taken 0:01:49.195167
Number of used sentences in train = 2074
Total loss for epoch 7: 3213.410406
validation loss after epoch 7 : 1205.033430
	Epoch 8....
Epoch has taken 0:01:49.610711
Number of used sentences in train = 2074
Total loss for epoch 8: 3195.475114
validation loss after epoch 8 : 1214.896049
	Epoch 9....
Epoch has taken 0:01:49.284010
Number of used sentences in train = 2074
Total loss for epoch 9: 3182.014228
validation loss after epoch 9 : 1246.947219
	Epoch 10....
Epoch has taken 0:01:49.276205
Number of used sentences in train = 2074
Total loss for epoch 10: 3176.817570
validation loss after epoch 10 : 1268.672989
	Epoch 11....
Epoch has taken 0:01:50.029341
Number of used sentences in train = 2074
Total loss for epoch 11: 3170.422154
validation loss after epoch 11 : 1287.731121
	Epoch 12....
Epoch has taken 0:01:49.498275
Number of used sentences in train = 2074
Total loss for epoch 12: 3166.354767
validation loss after epoch 12 : 1311.194660
	Epoch 13....
Epoch has taken 0:01:50.139835
Number of used sentences in train = 2074
Total loss for epoch 13: 3163.053512
validation loss after epoch 13 : 1309.608279
	Epoch 14....
Epoch has taken 0:01:49.038674
Number of used sentences in train = 2074
Total loss for epoch 14: 3160.898933
validation loss after epoch 14 : 1318.462558
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5660, 63)
  (lstm): LSTM(80, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.352734
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1525.201419
	Epoch 1....
Epoch has taken 0:00:11.142776
Number of used sentences in train = 231
Total loss for epoch 1: 619.213095
	Epoch 2....
Epoch has taken 0:00:11.135370
Number of used sentences in train = 231
Total loss for epoch 2: 461.444713
	Epoch 3....
Epoch has taken 0:00:11.140364
Number of used sentences in train = 231
Total loss for epoch 3: 406.624300
	Epoch 4....
Epoch has taken 0:00:11.125679
Number of used sentences in train = 231
Total loss for epoch 4: 370.823703
	Epoch 5....
Epoch has taken 0:00:11.128338
Number of used sentences in train = 231
Total loss for epoch 5: 358.246261
	Epoch 6....
Epoch has taken 0:00:11.162937
Number of used sentences in train = 231
Total loss for epoch 6: 355.748450
	Epoch 7....
Epoch has taken 0:00:11.144341
Number of used sentences in train = 231
Total loss for epoch 7: 350.648031
	Epoch 8....
Epoch has taken 0:00:11.140018
Number of used sentences in train = 231
Total loss for epoch 8: 347.690977
	Epoch 9....
Epoch has taken 0:00:11.151284
Number of used sentences in train = 231
Total loss for epoch 9: 347.006241
	Epoch 10....
Epoch has taken 0:00:11.129791
Number of used sentences in train = 231
Total loss for epoch 10: 346.490317
	Epoch 11....
Epoch has taken 0:00:11.134877
Number of used sentences in train = 231
Total loss for epoch 11: 346.165521
	Epoch 12....
Epoch has taken 0:00:11.134538
Number of used sentences in train = 231
Total loss for epoch 12: 345.916318
	Epoch 13....
Epoch has taken 0:00:11.125307
Number of used sentences in train = 231
Total loss for epoch 13: 345.694509
	Epoch 14....
Epoch has taken 0:00:11.130136
Number of used sentences in train = 231
Total loss for epoch 14: 345.546088
Epoch has taken 0:00:11.133047

==================================================================================================
	Training time : 0:30:14.609241
==================================================================================================
	Identification : 0.18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(6839, 63)
  (lstm): LSTM(80, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16323.633181
validation loss after epoch 0 : 1516.970716
	Epoch 1....
Epoch has taken 0:03:32.671412
Number of used sentences in train = 3226
Total loss for epoch 1: 11727.540036
validation loss after epoch 1 : 1444.999134
	Epoch 2....
Epoch has taken 0:03:37.153806
Number of used sentences in train = 3226
Total loss for epoch 2: 9947.208890
validation loss after epoch 2 : 1490.524350
	Epoch 3....
Epoch has taken 0:03:37.197414
Number of used sentences in train = 3226
Total loss for epoch 3: 8779.919545
validation loss after epoch 3 : 1543.129561
	Epoch 4....
Epoch has taken 0:03:32.862002
Number of used sentences in train = 3226
Total loss for epoch 4: 7910.289749
validation loss after epoch 4 : 1648.082954
	Epoch 5....
Epoch has taken 0:03:32.960576
Number of used sentences in train = 3226
Total loss for epoch 5: 7295.039151
validation loss after epoch 5 : 1921.116227
	Epoch 6....
Epoch has taken 0:03:32.846404
Number of used sentences in train = 3226
Total loss for epoch 6: 6897.087829
validation loss after epoch 6 : 1945.881202
	Epoch 7....
Epoch has taken 0:03:32.722183
Number of used sentences in train = 3226
Total loss for epoch 7: 6621.880656
validation loss after epoch 7 : 2136.570873
	Epoch 8....
Epoch has taken 0:03:35.054662
Number of used sentences in train = 3226
Total loss for epoch 8: 6458.256431
validation loss after epoch 8 : 2223.488054
	Epoch 9....
Epoch has taken 0:03:36.560184
Number of used sentences in train = 3226
Total loss for epoch 9: 6370.905701
validation loss after epoch 9 : 2350.026725
	Epoch 10....
Epoch has taken 0:03:32.482809
Number of used sentences in train = 3226
Total loss for epoch 10: 6305.224181
validation loss after epoch 10 : 2350.802279
	Epoch 11....
Epoch has taken 0:03:32.539802
Number of used sentences in train = 3226
Total loss for epoch 11: 6268.281224
validation loss after epoch 11 : 2395.153500
	Epoch 12....
Epoch has taken 0:03:32.506051
Number of used sentences in train = 3226
Total loss for epoch 12: 6260.995978
validation loss after epoch 12 : 2401.843955
	Epoch 13....
Epoch has taken 0:03:39.355880
Number of used sentences in train = 3226
Total loss for epoch 13: 6223.342577
validation loss after epoch 13 : 2518.913616
	Epoch 14....
Epoch has taken 0:03:32.713529
Number of used sentences in train = 3226
Total loss for epoch 14: 6215.258593
validation loss after epoch 14 : 2516.419676
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(6839, 63)
  (lstm): LSTM(80, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=76, bias=True)
  (linear2): Linear(in_features=76, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:33.096741
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2225.931560
	Epoch 1....
Epoch has taken 0:00:20.990233
Number of used sentences in train = 359
Total loss for epoch 1: 1153.310255
	Epoch 2....
Epoch has taken 0:00:21.100023
Number of used sentences in train = 359
Total loss for epoch 2: 854.067638
	Epoch 3....
Epoch has taken 0:00:21.339755
Number of used sentences in train = 359
Total loss for epoch 3: 730.408874
	Epoch 4....
Epoch has taken 0:00:21.733403
Number of used sentences in train = 359
Total loss for epoch 4: 693.479991
	Epoch 5....
Epoch has taken 0:00:23.389972
Number of used sentences in train = 359
Total loss for epoch 5: 685.726436
	Epoch 6....
Epoch has taken 0:00:21.334653
Number of used sentences in train = 359
Total loss for epoch 6: 678.890202
	Epoch 7....
Epoch has taken 0:00:21.532938
Number of used sentences in train = 359
Total loss for epoch 7: 674.571840
	Epoch 8....
Epoch has taken 0:00:21.707639
Number of used sentences in train = 359
Total loss for epoch 8: 672.765110
	Epoch 9....
Epoch has taken 0:00:21.674336
Number of used sentences in train = 359
Total loss for epoch 9: 672.175813
	Epoch 10....
Epoch has taken 0:00:21.497703
Number of used sentences in train = 359
Total loss for epoch 10: 671.804982
	Epoch 11....
Epoch has taken 0:00:21.839965
Number of used sentences in train = 359
Total loss for epoch 11: 671.589391
	Epoch 12....
Epoch has taken 0:00:23.633913
Number of used sentences in train = 359
Total loss for epoch 12: 671.311200
	Epoch 13....
Epoch has taken 0:00:23.601082
Number of used sentences in train = 359
Total loss for epoch 13: 671.138655
	Epoch 14....
Epoch has taken 0:00:23.726519
Number of used sentences in train = 359
Total loss for epoch 14: 670.999779
Epoch has taken 0:00:23.964263

==================================================================================================
	Training time : 0:59:06.442352
==================================================================================================
	Identification : 0.21

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 27, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 112, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 50, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 27)
  (w_embeddings): Embedding(1177, 50)
  (lstm): LSTM(77, 112, bidirectional=True)
  (linear1): Linear(in_features=1792, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13493.192111
validation loss after epoch 0 : 1022.401527
	Epoch 1....
Epoch has taken 0:03:02.260295
Number of used sentences in train = 2811
Total loss for epoch 1: 9074.738044
validation loss after epoch 1 : 1005.370312
	Epoch 2....
Epoch has taken 0:02:57.909970
Number of used sentences in train = 2811
Total loss for epoch 2: 8182.856316
validation loss after epoch 2 : 953.333967
	Epoch 3....
Epoch has taken 0:03:01.366729
Number of used sentences in train = 2811
Total loss for epoch 3: 7565.853808
validation loss after epoch 3 : 976.529330
	Epoch 4....
Epoch has taken 0:03:02.220749
Number of used sentences in train = 2811
Total loss for epoch 4: 7022.824768
validation loss after epoch 4 : 962.871384
	Epoch 5....
Epoch has taken 0:02:59.877036
Number of used sentences in train = 2811
Total loss for epoch 5: 6664.170291
validation loss after epoch 5 : 957.316327
	Epoch 6....
Epoch has taken 0:02:48.659535
Number of used sentences in train = 2811
Total loss for epoch 6: 6401.011740
validation loss after epoch 6 : 971.054513
	Epoch 7....
Epoch has taken 0:02:44.557776
Number of used sentences in train = 2811
Total loss for epoch 7: 6125.433130
validation loss after epoch 7 : 981.187807
	Epoch 8....
Epoch has taken 0:02:43.152150
Number of used sentences in train = 2811
Total loss for epoch 8: 5917.738177
validation loss after epoch 8 : 994.326887
	Epoch 9....
Epoch has taken 0:02:44.661146
Number of used sentences in train = 2811
Total loss for epoch 9: 5725.492067
validation loss after epoch 9 : 995.240155
	Epoch 10....
Epoch has taken 0:02:44.454519
Number of used sentences in train = 2811
Total loss for epoch 10: 5599.230567
validation loss after epoch 10 : 993.970320
	Epoch 11....
Epoch has taken 0:02:42.535305
Number of used sentences in train = 2811
Total loss for epoch 11: 5468.760266
validation loss after epoch 11 : 997.345669
	Epoch 12....
Epoch has taken 0:02:42.837448
Number of used sentences in train = 2811
Total loss for epoch 12: 5368.877607
validation loss after epoch 12 : 1040.789782
	Epoch 13....
Epoch has taken 0:02:44.718245
Number of used sentences in train = 2811
Total loss for epoch 13: 5324.663452
validation loss after epoch 13 : 1018.062655
	Epoch 14....
Epoch has taken 0:02:57.054412
Number of used sentences in train = 2811
Total loss for epoch 14: 5250.304937
validation loss after epoch 14 : 1039.213107
	TransitionClassifier(
  (p_embeddings): Embedding(18, 27)
  (w_embeddings): Embedding(1177, 50)
  (lstm): LSTM(77, 112, bidirectional=True)
  (linear1): Linear(in_features=1792, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:54.721871
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1655.832200
	Epoch 1....
Epoch has taken 0:00:17.391678
Number of used sentences in train = 313
Total loss for epoch 1: 1039.081819
	Epoch 2....
Epoch has taken 0:00:17.407071
Number of used sentences in train = 313
Total loss for epoch 2: 857.972979
	Epoch 3....
Epoch has taken 0:00:17.404616
Number of used sentences in train = 313
Total loss for epoch 3: 747.769922
	Epoch 4....
Epoch has taken 0:00:17.401366
Number of used sentences in train = 313
Total loss for epoch 4: 675.177647
	Epoch 5....
Epoch has taken 0:00:17.378440
Number of used sentences in train = 313
Total loss for epoch 5: 624.897417
	Epoch 6....
Epoch has taken 0:00:17.397019
Number of used sentences in train = 313
Total loss for epoch 6: 612.281889
	Epoch 7....
Epoch has taken 0:00:17.412926
Number of used sentences in train = 313
Total loss for epoch 7: 594.700918
	Epoch 8....
Epoch has taken 0:00:17.403399
Number of used sentences in train = 313
Total loss for epoch 8: 585.927502
	Epoch 9....
Epoch has taken 0:00:17.408931
Number of used sentences in train = 313
Total loss for epoch 9: 579.561184
	Epoch 10....
Epoch has taken 0:00:17.410083
Number of used sentences in train = 313
Total loss for epoch 10: 572.646352
	Epoch 11....
Epoch has taken 0:00:17.405450
Number of used sentences in train = 313
Total loss for epoch 11: 572.338845
	Epoch 12....
Epoch has taken 0:00:17.403861
Number of used sentences in train = 313
Total loss for epoch 12: 568.597257
	Epoch 13....
Epoch has taken 0:00:17.399647
Number of used sentences in train = 313
Total loss for epoch 13: 567.332219
	Epoch 14....
Epoch has taken 0:00:17.404247
Number of used sentences in train = 313
Total loss for epoch 14: 566.686485
Epoch has taken 0:00:17.400275

==================================================================================================
	Training time : 0:47:12.513656
==================================================================================================
	Identification : 0.496

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 27)
  (w_embeddings): Embedding(1133, 50)
  (lstm): LSTM(77, 112, bidirectional=True)
  (linear1): Linear(in_features=1792, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10159.647435
validation loss after epoch 0 : 752.272491
	Epoch 1....
Epoch has taken 0:01:53.010120
Number of used sentences in train = 2074
Total loss for epoch 1: 5964.890321
validation loss after epoch 1 : 663.604161
	Epoch 2....
Epoch has taken 0:01:53.054664
Number of used sentences in train = 2074
Total loss for epoch 2: 5102.652253
validation loss after epoch 2 : 644.616185
	Epoch 3....
Epoch has taken 0:01:52.989147
Number of used sentences in train = 2074
Total loss for epoch 3: 4583.115607
validation loss after epoch 3 : 685.936403
	Epoch 4....
Epoch has taken 0:01:52.900038
Number of used sentences in train = 2074
Total loss for epoch 4: 4226.276696
validation loss after epoch 4 : 660.239278
	Epoch 5....
Epoch has taken 0:01:52.943244
Number of used sentences in train = 2074
Total loss for epoch 5: 3966.650993
validation loss after epoch 5 : 680.746892
	Epoch 6....
Epoch has taken 0:01:52.948456
Number of used sentences in train = 2074
Total loss for epoch 6: 3746.953044
validation loss after epoch 6 : 712.461489
	Epoch 7....
Epoch has taken 0:01:52.846942
Number of used sentences in train = 2074
Total loss for epoch 7: 3620.595308
validation loss after epoch 7 : 735.303117
	Epoch 8....
Epoch has taken 0:01:51.867529
Number of used sentences in train = 2074
Total loss for epoch 8: 3520.971175
validation loss after epoch 8 : 770.069719
	Epoch 9....
Epoch has taken 0:01:52.940144
Number of used sentences in train = 2074
Total loss for epoch 9: 3479.544353
validation loss after epoch 9 : 753.774866
	Epoch 10....
Epoch has taken 0:01:53.060440
Number of used sentences in train = 2074
Total loss for epoch 10: 3468.806040
validation loss after epoch 10 : 758.048609
	Epoch 11....
Epoch has taken 0:01:50.651038
Number of used sentences in train = 2074
Total loss for epoch 11: 3426.084055
validation loss after epoch 11 : 763.936289
	Epoch 12....
Epoch has taken 0:01:49.534001
Number of used sentences in train = 2074
Total loss for epoch 12: 3384.402057
validation loss after epoch 12 : 771.108667
	Epoch 13....
Epoch has taken 0:01:49.473070
Number of used sentences in train = 2074
Total loss for epoch 13: 3348.435384
validation loss after epoch 13 : 774.102448
	Epoch 14....
Epoch has taken 0:01:49.597097
Number of used sentences in train = 2074
Total loss for epoch 14: 3322.204976
validation loss after epoch 14 : 775.860248
	TransitionClassifier(
  (p_embeddings): Embedding(18, 27)
  (w_embeddings): Embedding(1133, 50)
  (lstm): LSTM(77, 112, bidirectional=True)
  (linear1): Linear(in_features=1792, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.537319
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1148.276212
	Epoch 1....
Epoch has taken 0:00:11.155748
Number of used sentences in train = 231
Total loss for epoch 1: 746.421220
	Epoch 2....
Epoch has taken 0:00:11.158751
Number of used sentences in train = 231
Total loss for epoch 2: 569.832337
	Epoch 3....
Epoch has taken 0:00:11.150432
Number of used sentences in train = 231
Total loss for epoch 3: 478.763720
	Epoch 4....
Epoch has taken 0:00:11.174371
Number of used sentences in train = 231
Total loss for epoch 4: 432.720294
	Epoch 5....
Epoch has taken 0:00:11.158468
Number of used sentences in train = 231
Total loss for epoch 5: 394.003263
	Epoch 6....
Epoch has taken 0:00:11.163391
Number of used sentences in train = 231
Total loss for epoch 6: 380.711764
	Epoch 7....
Epoch has taken 0:00:11.149036
Number of used sentences in train = 231
Total loss for epoch 7: 366.117754
	Epoch 8....
Epoch has taken 0:00:11.160693
Number of used sentences in train = 231
Total loss for epoch 8: 363.194368
	Epoch 9....
Epoch has taken 0:00:11.149132
Number of used sentences in train = 231
Total loss for epoch 9: 360.893037
	Epoch 10....
Epoch has taken 0:00:11.152602
Number of used sentences in train = 231
Total loss for epoch 10: 360.475973
	Epoch 11....
Epoch has taken 0:00:11.160620
Number of used sentences in train = 231
Total loss for epoch 11: 359.031108
	Epoch 12....
Epoch has taken 0:00:11.156247
Number of used sentences in train = 231
Total loss for epoch 12: 357.954995
	Epoch 13....
Epoch has taken 0:00:11.150423
Number of used sentences in train = 231
Total loss for epoch 13: 356.986216
	Epoch 14....
Epoch has taken 0:00:11.166854
Number of used sentences in train = 231
Total loss for epoch 14: 356.490262
Epoch has taken 0:00:11.164670

==================================================================================================
	Training time : 0:30:45.060396
==================================================================================================
	Identification : 0.121

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 27)
  (w_embeddings): Embedding(1202, 50)
  (lstm): LSTM(77, 112, bidirectional=True)
  (linear1): Linear(in_features=1792, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19808.973075
validation loss after epoch 0 : 1559.189332
	Epoch 1....
Epoch has taken 0:03:37.010352
Number of used sentences in train = 3226
Total loss for epoch 1: 12075.651546
validation loss after epoch 1 : 1360.252646
	Epoch 2....
Epoch has taken 0:03:41.649831
Number of used sentences in train = 3226
Total loss for epoch 2: 10726.073073
validation loss after epoch 2 : 1365.260578
	Epoch 3....
Epoch has taken 0:03:41.531627
Number of used sentences in train = 3226
Total loss for epoch 3: 9802.211091
validation loss after epoch 3 : 1310.757711
	Epoch 4....
Epoch has taken 0:03:42.249251
Number of used sentences in train = 3226
Total loss for epoch 4: 9148.540053
validation loss after epoch 4 : 1356.363779
	Epoch 5....
Epoch has taken 0:03:37.799780
Number of used sentences in train = 3226
Total loss for epoch 5: 8643.122483
validation loss after epoch 5 : 1419.662654
	Epoch 6....
Epoch has taken 0:03:36.459898
Number of used sentences in train = 3226
Total loss for epoch 6: 8256.633260
validation loss after epoch 6 : 1447.960945
	Epoch 7....
Epoch has taken 0:03:41.967563
Number of used sentences in train = 3226
Total loss for epoch 7: 7946.577543
validation loss after epoch 7 : 1506.401458
	Epoch 8....
Epoch has taken 0:03:37.587737
Number of used sentences in train = 3226
Total loss for epoch 8: 7628.604908
validation loss after epoch 8 : 1499.114936
	Epoch 9....
Epoch has taken 0:03:38.488859
Number of used sentences in train = 3226
Total loss for epoch 9: 7384.576566
validation loss after epoch 9 : 1593.380461
	Epoch 10....
Epoch has taken 0:03:36.158047
Number of used sentences in train = 3226
Total loss for epoch 10: 7184.821939
validation loss after epoch 10 : 1587.771311
	Epoch 11....
Epoch has taken 0:03:37.080772
Number of used sentences in train = 3226
Total loss for epoch 11: 6994.852611
validation loss after epoch 11 : 1708.245162
	Epoch 12....
Epoch has taken 0:03:36.155916
Number of used sentences in train = 3226
Total loss for epoch 12: 6833.435409
validation loss after epoch 12 : 1742.713314
	Epoch 13....
Epoch has taken 0:03:37.827156
Number of used sentences in train = 3226
Total loss for epoch 13: 6671.540277
validation loss after epoch 13 : 1820.508288
	Epoch 14....
Epoch has taken 0:03:42.165876
Number of used sentences in train = 3226
Total loss for epoch 14: 6574.866158
validation loss after epoch 14 : 1836.654752
	TransitionClassifier(
  (p_embeddings): Embedding(13, 27)
  (w_embeddings): Embedding(1202, 50)
  (lstm): LSTM(77, 112, bidirectional=True)
  (linear1): Linear(in_features=1792, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:38.343120
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 3104.937710
	Epoch 1....
Epoch has taken 0:00:21.120049
Number of used sentences in train = 359
Total loss for epoch 1: 1358.874648
	Epoch 2....
Epoch has taken 0:00:21.259438
Number of used sentences in train = 359
Total loss for epoch 2: 1182.023983
	Epoch 3....
Epoch has taken 0:00:21.813073
Number of used sentences in train = 359
Total loss for epoch 3: 1010.248411
	Epoch 4....
Epoch has taken 0:00:21.824335
Number of used sentences in train = 359
Total loss for epoch 4: 930.101948
	Epoch 5....
Epoch has taken 0:00:21.119249
Number of used sentences in train = 359
Total loss for epoch 5: 874.550626
	Epoch 6....
Epoch has taken 0:00:21.105969
Number of used sentences in train = 359
Total loss for epoch 6: 822.060117
	Epoch 7....
Epoch has taken 0:00:21.130241
Number of used sentences in train = 359
Total loss for epoch 7: 787.152656
	Epoch 8....
Epoch has taken 0:00:21.096618
Number of used sentences in train = 359
Total loss for epoch 8: 759.171126
	Epoch 9....
Epoch has taken 0:00:21.126249
Number of used sentences in train = 359
Total loss for epoch 9: 739.524063
	Epoch 10....
Epoch has taken 0:00:21.162269
Number of used sentences in train = 359
Total loss for epoch 10: 727.511006
	Epoch 11....
Epoch has taken 0:00:21.820959
Number of used sentences in train = 359
Total loss for epoch 11: 716.040206
	Epoch 12....
Epoch has taken 0:00:21.803260
Number of used sentences in train = 359
Total loss for epoch 12: 705.994449
	Epoch 13....
Epoch has taken 0:00:21.802851
Number of used sentences in train = 359
Total loss for epoch 13: 702.635163
	Epoch 14....
Epoch has taken 0:00:21.115976
Number of used sentences in train = 359
Total loss for epoch 14: 699.091146
Epoch has taken 0:00:21.298783

==================================================================================================
	Training time : 1:00:03.750149
==================================================================================================
	Identification : 0.043

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 15, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 35, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 21, 'lstmDropout': 0.13, 'denseActivation': 'tanh', 'wordDim': 161, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(9278, 161)
  (lstm): LSTM(196, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13319.658912
validation loss after epoch 0 : 1182.772526
	Epoch 1....
Epoch has taken 0:02:44.158610
Number of used sentences in train = 2811
Total loss for epoch 1: 8770.956955
validation loss after epoch 1 : 1159.521337
	Epoch 2....
Epoch has taken 0:02:44.229622
Number of used sentences in train = 2811
Total loss for epoch 2: 6785.550775
validation loss after epoch 2 : 1225.724679
	Epoch 3....
Epoch has taken 0:02:44.162632
Number of used sentences in train = 2811
Total loss for epoch 3: 5873.845384
validation loss after epoch 3 : 1298.339274
	Epoch 4....
Epoch has taken 0:02:44.451068
Number of used sentences in train = 2811
Total loss for epoch 4: 5387.620872
validation loss after epoch 4 : 1377.255003
	Epoch 5....
Epoch has taken 0:02:44.288204
Number of used sentences in train = 2811
Total loss for epoch 5: 5121.368822
validation loss after epoch 5 : 1472.860461
	Epoch 6....
Epoch has taken 0:02:42.369194
Number of used sentences in train = 2811
Total loss for epoch 6: 4940.446799
validation loss after epoch 6 : 1500.989859
	Epoch 7....
Epoch has taken 0:02:44.188578
Number of used sentences in train = 2811
Total loss for epoch 7: 4803.534256
validation loss after epoch 7 : 1576.948312
	Epoch 8....
Epoch has taken 0:02:44.238831
Number of used sentences in train = 2811
Total loss for epoch 8: 4716.513135
validation loss after epoch 8 : 1602.295645
	Epoch 9....
Epoch has taken 0:02:46.303563
Number of used sentences in train = 2811
Total loss for epoch 9: 4657.036757
validation loss after epoch 9 : 1616.452629
	Epoch 10....
Epoch has taken 0:02:45.785372
Number of used sentences in train = 2811
Total loss for epoch 10: 4611.219967
validation loss after epoch 10 : 1668.754248
	Epoch 11....
Epoch has taken 0:02:44.161215
Number of used sentences in train = 2811
Total loss for epoch 11: 4588.098337
validation loss after epoch 11 : 1694.809851
	Epoch 12....
Epoch has taken 0:02:40.793960
Number of used sentences in train = 2811
Total loss for epoch 12: 4570.374712
validation loss after epoch 12 : 1718.249557
	Epoch 13....
Epoch has taken 0:02:40.142428
Number of used sentences in train = 2811
Total loss for epoch 13: 4549.261207
validation loss after epoch 13 : 1759.719907
	Epoch 14....
Epoch has taken 0:02:40.239304
Number of used sentences in train = 2811
Total loss for epoch 14: 4543.649496
validation loss after epoch 14 : 1790.980313
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(9278, 161)
  (lstm): LSTM(196, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:39.586199
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1526.107944
	Epoch 1....
Epoch has taken 0:00:16.878133
Number of used sentences in train = 313
Total loss for epoch 1: 796.888546
	Epoch 2....
Epoch has taken 0:00:16.867584
Number of used sentences in train = 313
Total loss for epoch 2: 621.993641
	Epoch 3....
Epoch has taken 0:00:16.869463
Number of used sentences in train = 313
Total loss for epoch 3: 561.226941
	Epoch 4....
Epoch has taken 0:00:16.867406
Number of used sentences in train = 313
Total loss for epoch 4: 536.477468
	Epoch 5....
Epoch has taken 0:00:16.862046
Number of used sentences in train = 313
Total loss for epoch 5: 526.403573
	Epoch 6....
Epoch has taken 0:00:16.849973
Number of used sentences in train = 313
Total loss for epoch 6: 517.023821
	Epoch 7....
Epoch has taken 0:00:16.852201
Number of used sentences in train = 313
Total loss for epoch 7: 512.931775
	Epoch 8....
Epoch has taken 0:00:16.865201
Number of used sentences in train = 313
Total loss for epoch 8: 510.414249
	Epoch 9....
Epoch has taken 0:00:16.877874
Number of used sentences in train = 313
Total loss for epoch 9: 507.236523
	Epoch 10....
Epoch has taken 0:00:16.865630
Number of used sentences in train = 313
Total loss for epoch 10: 504.327241
	Epoch 11....
Epoch has taken 0:00:16.869797
Number of used sentences in train = 313
Total loss for epoch 11: 503.548052
	Epoch 12....
Epoch has taken 0:00:16.880992
Number of used sentences in train = 313
Total loss for epoch 12: 503.052116
	Epoch 13....
Epoch has taken 0:00:16.882082
Number of used sentences in train = 313
Total loss for epoch 13: 502.628866
	Epoch 14....
Epoch has taken 0:00:16.871696
Number of used sentences in train = 313
Total loss for epoch 14: 502.300896
Epoch has taken 0:00:16.863112

==================================================================================================
	Training time : 0:45:02.644809
==================================================================================================
	Identification : 0.214

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(7081, 161)
  (lstm): LSTM(196, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10493.095445
validation loss after epoch 0 : 1110.343915
	Epoch 1....
Epoch has taken 0:01:49.029233
Number of used sentences in train = 2074
Total loss for epoch 1: 6543.220120
validation loss after epoch 1 : 1013.357337
	Epoch 2....
Epoch has taken 0:01:49.232204
Number of used sentences in train = 2074
Total loss for epoch 2: 4988.617518
validation loss after epoch 2 : 1161.814694
	Epoch 3....
Epoch has taken 0:01:51.009625
Number of used sentences in train = 2074
Total loss for epoch 3: 4187.068358
validation loss after epoch 3 : 1220.274416
	Epoch 4....
Epoch has taken 0:01:49.260583
Number of used sentences in train = 2074
Total loss for epoch 4: 3797.774717
validation loss after epoch 4 : 1272.994142
	Epoch 5....
Epoch has taken 0:01:49.218773
Number of used sentences in train = 2074
Total loss for epoch 5: 3564.774934
validation loss after epoch 5 : 1356.043242
	Epoch 6....
Epoch has taken 0:01:49.246544
Number of used sentences in train = 2074
Total loss for epoch 6: 3452.099864
validation loss after epoch 6 : 1419.521599
	Epoch 7....
Epoch has taken 0:01:49.208399
Number of used sentences in train = 2074
Total loss for epoch 7: 3381.529530
validation loss after epoch 7 : 1459.833341
	Epoch 8....
Epoch has taken 0:01:51.516144
Number of used sentences in train = 2074
Total loss for epoch 8: 3332.293078
validation loss after epoch 8 : 1511.162290
	Epoch 9....
Epoch has taken 0:01:50.784324
Number of used sentences in train = 2074
Total loss for epoch 9: 3295.632044
validation loss after epoch 9 : 1537.061254
	Epoch 10....
Epoch has taken 0:01:49.111002
Number of used sentences in train = 2074
Total loss for epoch 10: 3274.682833
validation loss after epoch 10 : 1567.359215
	Epoch 11....
Epoch has taken 0:01:49.100080
Number of used sentences in train = 2074
Total loss for epoch 11: 3244.087860
validation loss after epoch 11 : 1576.832105
	Epoch 12....
Epoch has taken 0:01:49.115067
Number of used sentences in train = 2074
Total loss for epoch 12: 3227.781045
validation loss after epoch 12 : 1612.148652
	Epoch 13....
Epoch has taken 0:01:49.151355
Number of used sentences in train = 2074
Total loss for epoch 13: 3214.749058
validation loss after epoch 13 : 1617.466709
	Epoch 14....
Epoch has taken 0:01:49.465636
Number of used sentences in train = 2074
Total loss for epoch 14: 3205.265177
validation loss after epoch 14 : 1633.140875
	TransitionClassifier(
  (p_embeddings): Embedding(18, 35)
  (w_embeddings): Embedding(7081, 161)
  (lstm): LSTM(196, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.663870
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1473.976265
	Epoch 1....
Epoch has taken 0:00:11.131581
Number of used sentences in train = 231
Total loss for epoch 1: 630.886149
	Epoch 2....
Epoch has taken 0:00:11.121663
Number of used sentences in train = 231
Total loss for epoch 2: 473.494723
	Epoch 3....
Epoch has taken 0:00:11.112283
Number of used sentences in train = 231
Total loss for epoch 3: 402.299347
	Epoch 4....
Epoch has taken 0:00:11.215994
Number of used sentences in train = 231
Total loss for epoch 4: 376.543932
	Epoch 5....
Epoch has taken 0:00:11.508589
Number of used sentences in train = 231
Total loss for epoch 5: 359.415029
	Epoch 6....
Epoch has taken 0:00:11.111871
Number of used sentences in train = 231
Total loss for epoch 6: 355.862350
	Epoch 7....
Epoch has taken 0:00:11.153635
Number of used sentences in train = 231
Total loss for epoch 7: 351.987365
	Epoch 8....
Epoch has taken 0:00:11.109934
Number of used sentences in train = 231
Total loss for epoch 8: 351.026260
	Epoch 9....
Epoch has taken 0:00:11.110941
Number of used sentences in train = 231
Total loss for epoch 9: 350.250472
	Epoch 10....
Epoch has taken 0:00:11.115508
Number of used sentences in train = 231
Total loss for epoch 10: 349.743100
	Epoch 11....
Epoch has taken 0:00:11.115488
Number of used sentences in train = 231
Total loss for epoch 11: 348.929743
	Epoch 12....
Epoch has taken 0:00:11.126528
Number of used sentences in train = 231
Total loss for epoch 12: 348.499973
	Epoch 13....
Epoch has taken 0:00:11.114951
Number of used sentences in train = 231
Total loss for epoch 13: 348.181508
	Epoch 14....
Epoch has taken 0:00:11.122855
Number of used sentences in train = 231
Total loss for epoch 14: 347.898259
Epoch has taken 0:00:11.115789

==================================================================================================
	Training time : 0:30:11.749422
==================================================================================================
	Identification : 0.179

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(17973, 161)
  (lstm): LSTM(196, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18475.391406
validation loss after epoch 0 : 1770.582584
	Epoch 1....
Epoch has taken 0:03:32.943601
Number of used sentences in train = 3226
Total loss for epoch 1: 11805.026034
validation loss after epoch 1 : 1697.250060
	Epoch 2....
Epoch has taken 0:04:02.353902
Number of used sentences in train = 3226
Total loss for epoch 2: 9554.399495
validation loss after epoch 2 : 1726.316596
	Epoch 3....
Epoch has taken 0:03:33.180288
Number of used sentences in train = 3226
Total loss for epoch 3: 8276.997256
validation loss after epoch 3 : 1840.908191
	Epoch 4....
Epoch has taken 0:03:33.877176
Number of used sentences in train = 3226
Total loss for epoch 4: 7473.479899
validation loss after epoch 4 : 1971.679704
	Epoch 5....
Epoch has taken 0:03:33.005400
Number of used sentences in train = 3226
Total loss for epoch 5: 7030.880996
validation loss after epoch 5 : 2123.433644
	Epoch 6....
Epoch has taken 0:03:33.106369
Number of used sentences in train = 3226
Total loss for epoch 6: 6728.085906
validation loss after epoch 6 : 2231.826870
	Epoch 7....
Epoch has taken 0:03:33.148888
Number of used sentences in train = 3226
Total loss for epoch 7: 6552.375803
validation loss after epoch 7 : 2322.287235
	Epoch 8....
Epoch has taken 0:03:33.806273
Number of used sentences in train = 3226
Total loss for epoch 8: 6441.823506
validation loss after epoch 8 : 2367.298679
	Epoch 9....
Epoch has taken 0:03:33.615717
Number of used sentences in train = 3226
Total loss for epoch 9: 6358.521585
validation loss after epoch 9 : 2445.933146
	Epoch 10....
Epoch has taken 0:03:39.789507
Number of used sentences in train = 3226
Total loss for epoch 10: 6302.782137
validation loss after epoch 10 : 2501.279484
	Epoch 11....
Epoch has taken 0:03:34.597008
Number of used sentences in train = 3226
Total loss for epoch 11: 6281.843590
validation loss after epoch 11 : 2551.041747
	Epoch 12....
Epoch has taken 0:03:33.977496
Number of used sentences in train = 3226
Total loss for epoch 12: 6251.496329
validation loss after epoch 12 : 2624.240140
	Epoch 13....
Epoch has taken 0:03:34.482188
Number of used sentences in train = 3226
Total loss for epoch 13: 6229.901632
validation loss after epoch 13 : 2610.357310
	Epoch 14....
Epoch has taken 0:03:33.719468
Number of used sentences in train = 3226
Total loss for epoch 14: 6218.664386
validation loss after epoch 14 : 2665.008631
	TransitionClassifier(
  (p_embeddings): Embedding(13, 35)
  (w_embeddings): Embedding(17973, 161)
  (lstm): LSTM(196, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=15, bias=True)
  (linear2): Linear(in_features=15, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:33.301727
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2366.789319
	Epoch 1....
Epoch has taken 0:00:20.759669
Number of used sentences in train = 359
Total loss for epoch 1: 1116.298181
	Epoch 2....
Epoch has taken 0:00:20.782747
Number of used sentences in train = 359
Total loss for epoch 2: 855.077775
	Epoch 3....
Epoch has taken 0:00:20.768955
Number of used sentences in train = 359
Total loss for epoch 3: 747.454367
	Epoch 4....
Epoch has taken 0:00:20.766040
Number of used sentences in train = 359
Total loss for epoch 4: 721.224207
	Epoch 5....
Epoch has taken 0:00:20.765995
Number of used sentences in train = 359
Total loss for epoch 5: 706.548289
	Epoch 6....
Epoch has taken 0:00:20.755001
Number of used sentences in train = 359
Total loss for epoch 6: 695.956641
	Epoch 7....
Epoch has taken 0:00:20.761363
Number of used sentences in train = 359
Total loss for epoch 7: 688.748896
	Epoch 8....
Epoch has taken 0:00:20.755374
Number of used sentences in train = 359
Total loss for epoch 8: 685.496632
	Epoch 9....
Epoch has taken 0:00:20.819714
Number of used sentences in train = 359
Total loss for epoch 9: 684.212939
	Epoch 10....
Epoch has taken 0:00:20.763233
Number of used sentences in train = 359
Total loss for epoch 10: 680.372990
	Epoch 11....
Epoch has taken 0:00:21.110275
Number of used sentences in train = 359
Total loss for epoch 11: 676.139671
	Epoch 12....
Epoch has taken 0:00:20.948750
Number of used sentences in train = 359
Total loss for epoch 12: 674.498929
	Epoch 13....
Epoch has taken 0:00:20.758902
Number of used sentences in train = 359
Total loss for epoch 13: 673.696342
	Epoch 14....
Epoch has taken 0:00:20.769332
Number of used sentences in train = 359
Total loss for epoch 14: 673.225223
Epoch has taken 0:00:20.758641

==================================================================================================
	Training time : 0:59:11.647264
==================================================================================================
	Identification : 0.211

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 148, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 48, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 24, 'lstmDropout': 0.3, 'denseActivation': 'tanh', 'wordDim': 148, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(9297, 148)
  (lstm): LSTM(196, 24, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=148, bias=True)
  (linear2): Linear(in_features=148, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15380.367247
validation loss after epoch 0 : 1228.645322
	Epoch 1....
Epoch has taken 0:02:51.505359
Number of used sentences in train = 2811
Total loss for epoch 1: 9580.404822
validation loss after epoch 1 : 1159.649344
	Epoch 2....
Epoch has taken 0:02:51.684027
Number of used sentences in train = 2811
Total loss for epoch 2: 7742.866392
validation loss after epoch 2 : 1174.380419
	Epoch 3....
Epoch has taken 0:02:56.251928
Number of used sentences in train = 2811
Total loss for epoch 3: 6879.962156
validation loss after epoch 3 : 1183.393911
	Epoch 4....
Epoch has taken 0:02:51.451380
Number of used sentences in train = 2811
Total loss for epoch 4: 6255.238713
validation loss after epoch 4 : 1278.612263
	Epoch 5....
Epoch has taken 0:02:51.748522
Number of used sentences in train = 2811
Total loss for epoch 5: 5775.677629
validation loss after epoch 5 : 1316.303166
	Epoch 6....
Epoch has taken 0:02:51.712040
Number of used sentences in train = 2811
Total loss for epoch 6: 5517.802457
validation loss after epoch 6 : 1426.604264
	Epoch 7....
Epoch has taken 0:02:51.617479
Number of used sentences in train = 2811
Total loss for epoch 7: 5347.655259
validation loss after epoch 7 : 1342.654490
	Epoch 8....
Epoch has taken 0:02:51.756266
Number of used sentences in train = 2811
Total loss for epoch 8: 5203.881369
validation loss after epoch 8 : 1479.854696
	Epoch 9....
Epoch has taken 0:02:51.548923
Number of used sentences in train = 2811
Total loss for epoch 9: 5109.369006
validation loss after epoch 9 : 1528.963979
	Epoch 10....
Epoch has taken 0:02:56.568124
Number of used sentences in train = 2811
Total loss for epoch 10: 5020.196723
validation loss after epoch 10 : 1517.977062
	Epoch 11....
Epoch has taken 0:02:53.275787
Number of used sentences in train = 2811
Total loss for epoch 11: 4928.242203
validation loss after epoch 11 : 1605.231846
	Epoch 12....
Epoch has taken 0:02:52.212572
Number of used sentences in train = 2811
Total loss for epoch 12: 4864.365937
validation loss after epoch 12 : 1619.271622
	Epoch 13....
Epoch has taken 0:02:56.536212
Number of used sentences in train = 2811
Total loss for epoch 13: 4892.717319
validation loss after epoch 13 : 1584.548798
	Epoch 14....
Epoch has taken 0:02:51.755244
Number of used sentences in train = 2811
Total loss for epoch 14: 4764.460232
validation loss after epoch 14 : 1611.080848
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(9297, 148)
  (lstm): LSTM(196, 24, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=148, bias=True)
  (linear2): Linear(in_features=148, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.791055
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2244.365751
	Epoch 1....
Epoch has taken 0:00:18.166671
Number of used sentences in train = 313
Total loss for epoch 1: 909.019583
	Epoch 2....
Epoch has taken 0:00:18.177953
Number of used sentences in train = 313
Total loss for epoch 2: 669.903892
	Epoch 3....
Epoch has taken 0:00:18.161421
Number of used sentences in train = 313
Total loss for epoch 3: 601.251651
	Epoch 4....
Epoch has taken 0:00:18.444206
Number of used sentences in train = 313
Total loss for epoch 4: 563.535492
	Epoch 5....
Epoch has taken 0:00:18.161259
Number of used sentences in train = 313
Total loss for epoch 5: 540.996635
	Epoch 6....
Epoch has taken 0:00:18.148924
Number of used sentences in train = 313
Total loss for epoch 6: 546.570966
	Epoch 7....
Epoch has taken 0:00:18.150421
Number of used sentences in train = 313
Total loss for epoch 7: 531.412141
	Epoch 8....
Epoch has taken 0:00:18.153109
Number of used sentences in train = 313
Total loss for epoch 8: 521.847236
	Epoch 9....
Epoch has taken 0:00:18.153306
Number of used sentences in train = 313
Total loss for epoch 9: 523.083638
	Epoch 10....
Epoch has taken 0:00:18.707612
Number of used sentences in train = 313
Total loss for epoch 10: 517.838349
	Epoch 11....
Epoch has taken 0:00:18.180269
Number of used sentences in train = 313
Total loss for epoch 11: 518.358178
	Epoch 12....
Epoch has taken 0:00:18.166818
Number of used sentences in train = 313
Total loss for epoch 12: 514.771614
	Epoch 13....
Epoch has taken 0:00:18.154080
Number of used sentences in train = 313
Total loss for epoch 13: 507.655120
	Epoch 14....
Epoch has taken 0:00:18.164716
Number of used sentences in train = 313
Total loss for epoch 14: 513.857393
Epoch has taken 0:00:18.301072

==================================================================================================
	Training time : 0:47:45.325624
==================================================================================================
	Identification : 0.151

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(7064, 148)
  (lstm): LSTM(196, 24, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=148, bias=True)
  (linear2): Linear(in_features=148, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 13158.193480
validation loss after epoch 0 : 1161.885237
	Epoch 1....
Epoch has taken 0:01:57.149867
Number of used sentences in train = 2074
Total loss for epoch 1: 7395.793441
validation loss after epoch 1 : 1085.100408
	Epoch 2....
Epoch has taken 0:01:57.264956
Number of used sentences in train = 2074
Total loss for epoch 2: 5893.932576
validation loss after epoch 2 : 961.920817
	Epoch 3....
Epoch has taken 0:01:57.544314
Number of used sentences in train = 2074
Total loss for epoch 3: 5141.535116
validation loss after epoch 3 : 1075.021907
	Epoch 4....
Epoch has taken 0:01:58.246184
Number of used sentences in train = 2074
Total loss for epoch 4: 4657.143555
validation loss after epoch 4 : 1087.401376
	Epoch 5....
Epoch has taken 0:01:57.523865
Number of used sentences in train = 2074
Total loss for epoch 5: 4268.333627
validation loss after epoch 5 : 1121.961842
	Epoch 6....
Epoch has taken 0:02:00.578496
Number of used sentences in train = 2074
Total loss for epoch 6: 4024.556388
validation loss after epoch 6 : 1244.501516
	Epoch 7....
Epoch has taken 0:01:57.429127
Number of used sentences in train = 2074
Total loss for epoch 7: 3845.390028
validation loss after epoch 7 : 1262.143361
	Epoch 8....
Epoch has taken 0:01:57.377774
Number of used sentences in train = 2074
Total loss for epoch 8: 3727.989551
validation loss after epoch 8 : 1239.326462
	Epoch 9....
Epoch has taken 0:02:00.868297
Number of used sentences in train = 2074
Total loss for epoch 9: 3626.295631
validation loss after epoch 9 : 1238.640843
	Epoch 10....
Epoch has taken 0:01:57.445073
Number of used sentences in train = 2074
Total loss for epoch 10: 3568.377571
validation loss after epoch 10 : 1384.467908
	Epoch 11....
Epoch has taken 0:01:57.340149
Number of used sentences in train = 2074
Total loss for epoch 11: 3594.586034
validation loss after epoch 11 : 1315.985138
	Epoch 12....
Epoch has taken 0:01:57.353186
Number of used sentences in train = 2074
Total loss for epoch 12: 3483.141477
validation loss after epoch 12 : 1411.688985
	Epoch 13....
Epoch has taken 0:01:57.283109
Number of used sentences in train = 2074
Total loss for epoch 13: 3478.289122
validation loss after epoch 13 : 1320.472672
	Epoch 14....
Epoch has taken 0:01:57.333313
Number of used sentences in train = 2074
Total loss for epoch 14: 3440.302213
validation loss after epoch 14 : 1293.317825
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(7064, 148)
  (lstm): LSTM(196, 24, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=148, bias=True)
  (linear2): Linear(in_features=148, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.312169
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2841.333947
	Epoch 1....
Epoch has taken 0:00:11.978814
Number of used sentences in train = 231
Total loss for epoch 1: 716.302204
	Epoch 2....
Epoch has taken 0:00:11.979829
Number of used sentences in train = 231
Total loss for epoch 2: 549.263140
	Epoch 3....
Epoch has taken 0:00:11.975601
Number of used sentences in train = 231
Total loss for epoch 3: 455.731812
	Epoch 4....
Epoch has taken 0:00:11.976982
Number of used sentences in train = 231
Total loss for epoch 4: 404.827063
	Epoch 5....
Epoch has taken 0:00:11.961940
Number of used sentences in train = 231
Total loss for epoch 5: 391.204717
	Epoch 6....
Epoch has taken 0:00:11.960471
Number of used sentences in train = 231
Total loss for epoch 6: 379.672625
	Epoch 7....
Epoch has taken 0:00:11.963152
Number of used sentences in train = 231
Total loss for epoch 7: 373.630748
	Epoch 8....
Epoch has taken 0:00:11.980659
Number of used sentences in train = 231
Total loss for epoch 8: 363.439500
	Epoch 9....
Epoch has taken 0:00:11.983169
Number of used sentences in train = 231
Total loss for epoch 9: 366.187320
	Epoch 10....
Epoch has taken 0:00:11.966361
Number of used sentences in train = 231
Total loss for epoch 10: 359.557354
	Epoch 11....
Epoch has taken 0:00:11.980306
Number of used sentences in train = 231
Total loss for epoch 11: 359.546959
	Epoch 12....
Epoch has taken 0:00:11.974519
Number of used sentences in train = 231
Total loss for epoch 12: 354.751191
	Epoch 13....
Epoch has taken 0:00:11.961517
Number of used sentences in train = 231
Total loss for epoch 13: 356.334993
	Epoch 14....
Epoch has taken 0:00:11.975586
Number of used sentences in train = 231
Total loss for epoch 14: 352.422965
Epoch has taken 0:00:11.973409

==================================================================================================
	Training time : 0:32:27.991503
==================================================================================================
	Identification : 0.289

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 48)
  (w_embeddings): Embedding(17986, 148)
  (lstm): LSTM(196, 24, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=148, bias=True)
  (linear2): Linear(in_features=148, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18924.897414
validation loss after epoch 0 : 1614.950166
	Epoch 1....
Epoch has taken 0:03:47.257211
Number of used sentences in train = 3226
Total loss for epoch 1: 12574.699946
validation loss after epoch 1 : 1606.525587
	Epoch 2....
Epoch has taken 0:03:48.175274
Number of used sentences in train = 3226
Total loss for epoch 2: 10421.397524
validation loss after epoch 2 : 1615.893817
	Epoch 3....
Epoch has taken 0:03:52.961629
Number of used sentences in train = 3226
Total loss for epoch 3: 9048.797683
validation loss after epoch 3 : 1750.772820
	Epoch 4....
Epoch has taken 0:03:50.669289
Number of used sentences in train = 3226
Total loss for epoch 4: 8300.450333
validation loss after epoch 4 : 1862.429174
	Epoch 5....
Epoch has taken 0:03:47.612492
Number of used sentences in train = 3226
Total loss for epoch 5: 7768.582997
validation loss after epoch 5 : 1985.002676
	Epoch 6....
Epoch has taken 0:03:50.549055
Number of used sentences in train = 3226
Total loss for epoch 6: 7443.215942
validation loss after epoch 6 : 2086.897787
	Epoch 7....
Epoch has taken 0:03:47.409071
Number of used sentences in train = 3226
Total loss for epoch 7: 7214.589872
validation loss after epoch 7 : 2133.563395
	Epoch 8....
Epoch has taken 0:03:48.638883
Number of used sentences in train = 3226
Total loss for epoch 8: 6951.308106
validation loss after epoch 8 : 2160.499523
	Epoch 9....
Epoch has taken 0:03:47.725926
Number of used sentences in train = 3226
Total loss for epoch 9: 6912.754429
validation loss after epoch 9 : 2371.189982
	Epoch 10....
Epoch has taken 0:03:47.679967
Number of used sentences in train = 3226
Total loss for epoch 10: 6783.003253
validation loss after epoch 10 : 2345.864071
	Epoch 11....
Epoch has taken 0:03:47.740467
Number of used sentences in train = 3226
Total loss for epoch 11: 6797.457462
validation loss after epoch 11 : 2433.896712
	Epoch 12....
Epoch has taken 0:03:47.900090
Number of used sentences in train = 3226
Total loss for epoch 12: 6631.037267
validation loss after epoch 12 : 2611.695041
	Epoch 13....
Epoch has taken 0:03:51.458720
Number of used sentences in train = 3226
Total loss for epoch 13: 6584.528979
validation loss after epoch 13 : 2582.975251
	Epoch 14....
Epoch has taken 0:03:47.599298
Number of used sentences in train = 3226
Total loss for epoch 14: 6601.668140
validation loss after epoch 14 : 2683.452335
	TransitionClassifier(
  (p_embeddings): Embedding(13, 48)
  (w_embeddings): Embedding(17986, 148)
  (lstm): LSTM(196, 24, num_layers=2, dropout=0.3, bidirectional=True)
  (linear1): Linear(in_features=384, out_features=148, bias=True)
  (linear2): Linear(in_features=148, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:47.461442
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2561.149606
	Epoch 1....
Epoch has taken 0:00:22.231026
Number of used sentences in train = 359
Total loss for epoch 1: 1227.991498
	Epoch 2....
Epoch has taken 0:00:22.206504
Number of used sentences in train = 359
Total loss for epoch 2: 956.630913
	Epoch 3....
Epoch has taken 0:00:22.224821
Number of used sentences in train = 359
Total loss for epoch 3: 805.580366
	Epoch 4....
Epoch has taken 0:00:22.233571
Number of used sentences in train = 359
Total loss for epoch 4: 763.294896
	Epoch 5....
Epoch has taken 0:00:22.198868
Number of used sentences in train = 359
Total loss for epoch 5: 709.966732
	Epoch 6....
Epoch has taken 0:00:22.220132
Number of used sentences in train = 359
Total loss for epoch 6: 708.239071
	Epoch 7....
Epoch has taken 0:00:22.223992
Number of used sentences in train = 359
Total loss for epoch 7: 710.911281
	Epoch 8....
Epoch has taken 0:00:22.213151
Number of used sentences in train = 359
Total loss for epoch 8: 701.532442
	Epoch 9....
Epoch has taken 0:00:22.205691
Number of used sentences in train = 359
Total loss for epoch 9: 690.148266
	Epoch 10....
Epoch has taken 0:00:22.221779
Number of used sentences in train = 359
Total loss for epoch 10: 689.588394
	Epoch 11....
Epoch has taken 0:00:22.282215
Number of used sentences in train = 359
Total loss for epoch 11: 682.633491
	Epoch 12....
Epoch has taken 0:00:22.824361
Number of used sentences in train = 359
Total loss for epoch 12: 683.302225
	Epoch 13....
Epoch has taken 0:00:22.225626
Number of used sentences in train = 359
Total loss for epoch 13: 693.120201
	Epoch 14....
Epoch has taken 0:00:22.219806
Number of used sentences in train = 359
Total loss for epoch 14: 684.160711
Epoch has taken 0:00:22.230648

==================================================================================================
	Training time : 1:02:45.508541
==================================================================================================
	Identification : 0.327

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 31, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 50, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 56, 'lstmDropout': 0.14, 'denseActivation': 'tanh', 'wordDim': 194, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 50)
  (w_embeddings): Embedding(5880, 194)
  (lstm): LSTM(244, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12683.381307
validation loss after epoch 0 : 1147.822424
	Epoch 1....
Epoch has taken 0:02:39.961633
Number of used sentences in train = 2811
Total loss for epoch 1: 8163.846250
validation loss after epoch 1 : 1088.101366
	Epoch 2....
Epoch has taken 0:02:44.524954
Number of used sentences in train = 2811
Total loss for epoch 2: 6503.673133
validation loss after epoch 2 : 1099.627751
	Epoch 3....
Epoch has taken 0:02:44.836189
Number of used sentences in train = 2811
Total loss for epoch 3: 5657.752231
validation loss after epoch 3 : 1196.607857
	Epoch 4....
Epoch has taken 0:02:43.053011
Number of used sentences in train = 2811
Total loss for epoch 4: 5201.434523
validation loss after epoch 4 : 1285.608819
	Epoch 5....
Epoch has taken 0:02:43.415854
Number of used sentences in train = 2811
Total loss for epoch 5: 4956.569320
validation loss after epoch 5 : 1350.154895
	Epoch 6....
Epoch has taken 0:02:44.702060
Number of used sentences in train = 2811
Total loss for epoch 6: 4797.829474
validation loss after epoch 6 : 1411.181132
	Epoch 7....
Epoch has taken 0:02:44.983558
Number of used sentences in train = 2811
Total loss for epoch 7: 4705.676821
validation loss after epoch 7 : 1452.678177
	Epoch 8....
Epoch has taken 0:02:44.665000
Number of used sentences in train = 2811
Total loss for epoch 8: 4649.474778
validation loss after epoch 8 : 1483.038062
	Epoch 9....
Epoch has taken 0:02:43.850309
Number of used sentences in train = 2811
Total loss for epoch 9: 4602.446539
validation loss after epoch 9 : 1513.008014
	Epoch 10....
Epoch has taken 0:02:45.893666
Number of used sentences in train = 2811
Total loss for epoch 10: 4578.775592
validation loss after epoch 10 : 1537.445423
	Epoch 11....
Epoch has taken 0:02:44.853149
Number of used sentences in train = 2811
Total loss for epoch 11: 4555.355081
validation loss after epoch 11 : 1558.445257
	Epoch 12....
Epoch has taken 0:02:44.941038
Number of used sentences in train = 2811
Total loss for epoch 12: 4544.963010
validation loss after epoch 12 : 1581.155995
	Epoch 13....
Epoch has taken 0:02:43.945848
Number of used sentences in train = 2811
Total loss for epoch 13: 4536.633849
validation loss after epoch 13 : 1610.747054
	Epoch 14....
Epoch has taken 0:02:39.830345
Number of used sentences in train = 2811
Total loss for epoch 14: 4532.291191
validation loss after epoch 14 : 1628.164045
	TransitionClassifier(
  (p_embeddings): Embedding(18, 50)
  (w_embeddings): Embedding(5880, 194)
  (lstm): LSTM(244, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.019190
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1483.238034
	Epoch 1....
Epoch has taken 0:00:16.902338
Number of used sentences in train = 313
Total loss for epoch 1: 853.122169
	Epoch 2....
Epoch has taken 0:00:16.898668
Number of used sentences in train = 313
Total loss for epoch 2: 646.885026
	Epoch 3....
Epoch has taken 0:00:16.925211
Number of used sentences in train = 313
Total loss for epoch 3: 572.699022
	Epoch 4....
Epoch has taken 0:00:16.910338
Number of used sentences in train = 313
Total loss for epoch 4: 542.259003
	Epoch 5....
Epoch has taken 0:00:16.914810
Number of used sentences in train = 313
Total loss for epoch 5: 524.519787
	Epoch 6....
Epoch has taken 0:00:16.899541
Number of used sentences in train = 313
Total loss for epoch 6: 517.565879
	Epoch 7....
Epoch has taken 0:00:16.913752
Number of used sentences in train = 313
Total loss for epoch 7: 515.158953
	Epoch 8....
Epoch has taken 0:00:16.903426
Number of used sentences in train = 313
Total loss for epoch 8: 514.287877
	Epoch 9....
Epoch has taken 0:00:16.906769
Number of used sentences in train = 313
Total loss for epoch 9: 512.797966
	Epoch 10....
Epoch has taken 0:00:16.862995
Number of used sentences in train = 313
Total loss for epoch 10: 511.315081
	Epoch 11....
Epoch has taken 0:00:16.884363
Number of used sentences in train = 313
Total loss for epoch 11: 512.240696
	Epoch 12....
Epoch has taken 0:00:16.875080
Number of used sentences in train = 313
Total loss for epoch 12: 512.059771
	Epoch 13....
Epoch has taken 0:00:16.899541
Number of used sentences in train = 313
Total loss for epoch 13: 510.932565
	Epoch 14....
Epoch has taken 0:00:16.883637
Number of used sentences in train = 313
Total loss for epoch 14: 509.698108
Epoch has taken 0:00:16.886989

==================================================================================================
	Training time : 0:45:17.455991
==================================================================================================
	Identification : 0.271

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 50)
  (w_embeddings): Embedding(5634, 194)
  (lstm): LSTM(244, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10163.546725
validation loss after epoch 0 : 1005.522078
	Epoch 1....
Epoch has taken 0:01:49.195254
Number of used sentences in train = 2074
Total loss for epoch 1: 6451.031948
validation loss after epoch 1 : 898.039148
	Epoch 2....
Epoch has taken 0:01:52.352088
Number of used sentences in train = 2074
Total loss for epoch 2: 5008.558543
validation loss after epoch 2 : 957.310869
	Epoch 3....
Epoch has taken 0:01:50.783603
Number of used sentences in train = 2074
Total loss for epoch 3: 4158.411188
validation loss after epoch 3 : 1057.696880
	Epoch 4....
Epoch has taken 0:01:49.390828
Number of used sentences in train = 2074
Total loss for epoch 4: 3720.262075
validation loss after epoch 4 : 1124.870906
	Epoch 5....
Epoch has taken 0:01:49.393687
Number of used sentences in train = 2074
Total loss for epoch 5: 3490.922742
validation loss after epoch 5 : 1215.063775
	Epoch 6....
Epoch has taken 0:01:49.330810
Number of used sentences in train = 2074
Total loss for epoch 6: 3382.657592
validation loss after epoch 6 : 1235.549847
	Epoch 7....
Epoch has taken 0:01:49.409413
Number of used sentences in train = 2074
Total loss for epoch 7: 3308.535589
validation loss after epoch 7 : 1269.068548
	Epoch 8....
Epoch has taken 0:01:49.388572
Number of used sentences in train = 2074
Total loss for epoch 8: 3272.465011
validation loss after epoch 8 : 1301.406928
	Epoch 9....
Epoch has taken 0:01:51.783806
Number of used sentences in train = 2074
Total loss for epoch 9: 3242.854349
validation loss after epoch 9 : 1323.427600
	Epoch 10....
Epoch has taken 0:01:49.412016
Number of used sentences in train = 2074
Total loss for epoch 10: 3224.668646
validation loss after epoch 10 : 1342.575311
	Epoch 11....
Epoch has taken 0:01:49.411886
Number of used sentences in train = 2074
Total loss for epoch 11: 3211.513897
validation loss after epoch 11 : 1344.898947
	Epoch 12....
Epoch has taken 0:01:49.299849
Number of used sentences in train = 2074
Total loss for epoch 12: 3203.549408
validation loss after epoch 12 : 1371.353002
	Epoch 13....
Epoch has taken 0:01:49.561342
Number of used sentences in train = 2074
Total loss for epoch 13: 3197.223834
validation loss after epoch 13 : 1384.675679
	Epoch 14....
Epoch has taken 0:01:49.166179
Number of used sentences in train = 2074
Total loss for epoch 14: 3193.302760
validation loss after epoch 14 : 1393.617526
	TransitionClassifier(
  (p_embeddings): Embedding(18, 50)
  (w_embeddings): Embedding(5634, 194)
  (lstm): LSTM(244, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.252273
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1442.185259
	Epoch 1....
Epoch has taken 0:00:11.119261
Number of used sentences in train = 231
Total loss for epoch 1: 670.849555
	Epoch 2....
Epoch has taken 0:00:11.099218
Number of used sentences in train = 231
Total loss for epoch 2: 460.758552
	Epoch 3....
Epoch has taken 0:00:11.108935
Number of used sentences in train = 231
Total loss for epoch 3: 394.664828
	Epoch 4....
Epoch has taken 0:00:11.093845
Number of used sentences in train = 231
Total loss for epoch 4: 373.745507
	Epoch 5....
Epoch has taken 0:00:11.115757
Number of used sentences in train = 231
Total loss for epoch 5: 365.927657
	Epoch 6....
Epoch has taken 0:00:11.102914
Number of used sentences in train = 231
Total loss for epoch 6: 355.566523
	Epoch 7....
Epoch has taken 0:00:11.118802
Number of used sentences in train = 231
Total loss for epoch 7: 352.810556
	Epoch 8....
Epoch has taken 0:00:11.131115
Number of used sentences in train = 231
Total loss for epoch 8: 350.493550
	Epoch 9....
Epoch has taken 0:00:11.513644
Number of used sentences in train = 231
Total loss for epoch 9: 349.352474
	Epoch 10....
Epoch has taken 0:00:11.495706
Number of used sentences in train = 231
Total loss for epoch 10: 348.697899
	Epoch 11....
Epoch has taken 0:00:11.510360
Number of used sentences in train = 231
Total loss for epoch 11: 348.161132
	Epoch 12....
Epoch has taken 0:00:11.505580
Number of used sentences in train = 231
Total loss for epoch 12: 347.748487
	Epoch 13....
Epoch has taken 0:00:11.533221
Number of used sentences in train = 231
Total loss for epoch 13: 347.253319
	Epoch 14....
Epoch has taken 0:00:11.512186
Number of used sentences in train = 231
Total loss for epoch 14: 346.872533
Epoch has taken 0:00:11.501507

==================================================================================================
	Training time : 0:30:16.942767
==================================================================================================
	Identification : 0.221

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 50)
  (w_embeddings): Embedding(6813, 194)
  (lstm): LSTM(244, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15944.129816
validation loss after epoch 0 : 1517.988943
	Epoch 1....
Epoch has taken 0:03:37.410097
Number of used sentences in train = 3226
Total loss for epoch 1: 11472.129723
validation loss after epoch 1 : 1539.848246
	Epoch 2....
Epoch has taken 0:03:37.319355
Number of used sentences in train = 3226
Total loss for epoch 2: 9646.415282
validation loss after epoch 2 : 1623.452572
	Epoch 3....
Epoch has taken 0:03:32.531354
Number of used sentences in train = 3226
Total loss for epoch 3: 8440.849497
validation loss after epoch 3 : 1699.766320
	Epoch 4....
Epoch has taken 0:03:35.550298
Number of used sentences in train = 3226
Total loss for epoch 4: 7691.107051
validation loss after epoch 4 : 1797.829823
	Epoch 5....
Epoch has taken 0:03:32.492546
Number of used sentences in train = 3226
Total loss for epoch 5: 7125.347594
validation loss after epoch 5 : 1962.967544
	Epoch 6....
Epoch has taken 0:03:32.906135
Number of used sentences in train = 3226
Total loss for epoch 6: 6791.995702
validation loss after epoch 6 : 2138.115262
	Epoch 7....
Epoch has taken 0:03:34.958599
Number of used sentences in train = 3226
Total loss for epoch 7: 6593.962629
validation loss after epoch 7 : 2262.797666
	Epoch 8....
Epoch has taken 0:03:37.272205
Number of used sentences in train = 3226
Total loss for epoch 8: 6422.967408
validation loss after epoch 8 : 2282.388847
	Epoch 9....
Epoch has taken 0:03:36.708100
Number of used sentences in train = 3226
Total loss for epoch 9: 6342.820993
validation loss after epoch 9 : 2439.097121
	Epoch 10....
Epoch has taken 0:03:32.433356
Number of used sentences in train = 3226
Total loss for epoch 10: 6272.243730
validation loss after epoch 10 : 2499.693091
	Epoch 11....
Epoch has taken 0:03:37.809314
Number of used sentences in train = 3226
Total loss for epoch 11: 6250.761244
validation loss after epoch 11 : 2554.278857
	Epoch 12....
Epoch has taken 0:03:37.756247
Number of used sentences in train = 3226
Total loss for epoch 12: 6221.410454
validation loss after epoch 12 : 2605.342029
	Epoch 13....
Epoch has taken 0:03:32.836320
Number of used sentences in train = 3226
Total loss for epoch 13: 6203.856621
validation loss after epoch 13 : 2633.026196
	Epoch 14....
Epoch has taken 0:03:32.794101
Number of used sentences in train = 3226
Total loss for epoch 14: 6190.933447
validation loss after epoch 14 : 2669.250084
	TransitionClassifier(
  (p_embeddings): Embedding(13, 50)
  (w_embeddings): Embedding(6813, 194)
  (lstm): LSTM(244, 56, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:34.129261
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2046.847421
	Epoch 1....
Epoch has taken 0:00:20.757415
Number of used sentences in train = 359
Total loss for epoch 1: 1100.355450
	Epoch 2....
Epoch has taken 0:00:20.799655
Number of used sentences in train = 359
Total loss for epoch 2: 840.089462
	Epoch 3....
Epoch has taken 0:00:20.738354
Number of used sentences in train = 359
Total loss for epoch 3: 750.214063
	Epoch 4....
Epoch has taken 0:00:20.752655
Number of used sentences in train = 359
Total loss for epoch 4: 714.022055
	Epoch 5....
Epoch has taken 0:00:20.758574
Number of used sentences in train = 359
Total loss for epoch 5: 699.260170
	Epoch 6....
Epoch has taken 0:00:20.761941
Number of used sentences in train = 359
Total loss for epoch 6: 689.554439
	Epoch 7....
Epoch has taken 0:00:20.794616
Number of used sentences in train = 359
Total loss for epoch 7: 685.629173
	Epoch 8....
Epoch has taken 0:00:20.751737
Number of used sentences in train = 359
Total loss for epoch 8: 679.499195
	Epoch 9....
Epoch has taken 0:00:20.752109
Number of used sentences in train = 359
Total loss for epoch 9: 675.650156
	Epoch 10....
Epoch has taken 0:00:20.734400
Number of used sentences in train = 359
Total loss for epoch 10: 673.914181
	Epoch 11....
Epoch has taken 0:00:20.750757
Number of used sentences in train = 359
Total loss for epoch 11: 673.046310
	Epoch 12....
Epoch has taken 0:00:20.991875
Number of used sentences in train = 359
Total loss for epoch 12: 672.419585
	Epoch 13....
Epoch has taken 0:00:20.748577
Number of used sentences in train = 359
Total loss for epoch 13: 671.949521
	Epoch 14....
Epoch has taken 0:00:20.747522
Number of used sentences in train = 359
Total loss for epoch 14: 671.667447
Epoch has taken 0:00:20.732587

==================================================================================================
	Training time : 0:58:57.142163
==================================================================================================
	Identification : 0.446

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 19, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 113, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 222, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5850, 222)
  (lstm): LSTM(239, 113, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 16900.308750
validation loss after epoch 0 : 1267.117333
	Epoch 1....
Epoch has taken 0:02:51.800448
Number of used sentences in train = 2811
Total loss for epoch 1: 10144.191333
validation loss after epoch 1 : 1151.972789
	Epoch 2....
Epoch has taken 0:02:51.840325
Number of used sentences in train = 2811
Total loss for epoch 2: 7976.561146
validation loss after epoch 2 : 1114.513471
	Epoch 3....
Epoch has taken 0:02:51.940773
Number of used sentences in train = 2811
Total loss for epoch 3: 6742.686039
validation loss after epoch 3 : 1168.024857
	Epoch 4....
Epoch has taken 0:02:54.999374
Number of used sentences in train = 2811
Total loss for epoch 4: 6043.315958
validation loss after epoch 4 : 1222.105951
	Epoch 5....
Epoch has taken 0:02:51.981914
Number of used sentences in train = 2811
Total loss for epoch 5: 5543.943131
validation loss after epoch 5 : 1273.675741
	Epoch 6....
Epoch has taken 0:02:52.499543
Number of used sentences in train = 2811
Total loss for epoch 6: 5326.515706
validation loss after epoch 6 : 1313.732210
	Epoch 7....
Epoch has taken 0:02:52.124198
Number of used sentences in train = 2811
Total loss for epoch 7: 5110.850482
validation loss after epoch 7 : 1384.130347
	Epoch 8....
Epoch has taken 0:02:54.325058
Number of used sentences in train = 2811
Total loss for epoch 8: 4942.994082
validation loss after epoch 8 : 1397.657398
	Epoch 9....
Epoch has taken 0:02:51.695757
Number of used sentences in train = 2811
Total loss for epoch 9: 4814.001739
validation loss after epoch 9 : 1489.838103
	Epoch 10....
Epoch has taken 0:02:51.948824
Number of used sentences in train = 2811
Total loss for epoch 10: 4769.216306
validation loss after epoch 10 : 1516.478547
	Epoch 11....
Epoch has taken 0:02:51.741263
Number of used sentences in train = 2811
Total loss for epoch 11: 4707.303772
validation loss after epoch 11 : 1575.399334
	Epoch 12....
Epoch has taken 0:02:51.808727
Number of used sentences in train = 2811
Total loss for epoch 12: 4664.416110
validation loss after epoch 12 : 1572.940449
	Epoch 13....
Epoch has taken 0:02:52.056116
Number of used sentences in train = 2811
Total loss for epoch 13: 4663.271186
validation loss after epoch 13 : 1677.632943
	Epoch 14....
Epoch has taken 0:02:52.100110
Number of used sentences in train = 2811
Total loss for epoch 14: 4668.920055
validation loss after epoch 14 : 1638.787478
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5850, 222)
  (lstm): LSTM(239, 113, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.939652
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1530.393196
	Epoch 1....
Epoch has taken 0:00:18.550042
Number of used sentences in train = 313
Total loss for epoch 1: 944.554650
	Epoch 2....
Epoch has taken 0:00:18.165936
Number of used sentences in train = 313
Total loss for epoch 2: 765.378759
	Epoch 3....
Epoch has taken 0:00:18.168953
Number of used sentences in train = 313
Total loss for epoch 3: 657.369846
	Epoch 4....
Epoch has taken 0:00:19.230464
Number of used sentences in train = 313
Total loss for epoch 4: 607.605832
	Epoch 5....
Epoch has taken 0:00:20.138129
Number of used sentences in train = 313
Total loss for epoch 5: 607.798674
	Epoch 6....
Epoch has taken 0:00:18.170910
Number of used sentences in train = 313
Total loss for epoch 6: 597.194836
	Epoch 7....
Epoch has taken 0:00:18.166949
Number of used sentences in train = 313
Total loss for epoch 7: 566.251650
	Epoch 8....
Epoch has taken 0:00:18.155177
Number of used sentences in train = 313
Total loss for epoch 8: 551.385190
	Epoch 9....
Epoch has taken 0:00:18.174644
Number of used sentences in train = 313
Total loss for epoch 9: 546.317194
	Epoch 10....
Epoch has taken 0:00:18.170467
Number of used sentences in train = 313
Total loss for epoch 10: 542.953982
	Epoch 11....
Epoch has taken 0:00:18.171967
Number of used sentences in train = 313
Total loss for epoch 11: 542.536366
	Epoch 12....
Epoch has taken 0:00:18.166771
Number of used sentences in train = 313
Total loss for epoch 12: 538.471952
	Epoch 13....
Epoch has taken 0:00:18.169651
Number of used sentences in train = 313
Total loss for epoch 13: 536.237386
	Epoch 14....
Epoch has taken 0:00:18.169296
Number of used sentences in train = 313
Total loss for epoch 14: 534.095665
Epoch has taken 0:00:18.170385

==================================================================================================
	Training time : 0:47:41.260574
==================================================================================================
	Identification : 0.486

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5620, 222)
  (lstm): LSTM(239, 113, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 14686.467421
validation loss after epoch 0 : 1200.763366
	Epoch 1....
Epoch has taken 0:01:57.734433
Number of used sentences in train = 2074
Total loss for epoch 1: 8601.131849
validation loss after epoch 1 : 937.030991
	Epoch 2....
Epoch has taken 0:01:57.840285
Number of used sentences in train = 2074
Total loss for epoch 2: 6448.521548
validation loss after epoch 2 : 947.516372
	Epoch 3....
Epoch has taken 0:01:57.791424
Number of used sentences in train = 2074
Total loss for epoch 3: 5166.226575
validation loss after epoch 3 : 922.236573
	Epoch 4....
Epoch has taken 0:01:57.881432
Number of used sentences in train = 2074
Total loss for epoch 4: 4492.631023
validation loss after epoch 4 : 958.933947
	Epoch 5....
Epoch has taken 0:01:57.820703
Number of used sentences in train = 2074
Total loss for epoch 5: 4072.035031
validation loss after epoch 5 : 994.929018
	Epoch 6....
Epoch has taken 0:01:59.679823
Number of used sentences in train = 2074
Total loss for epoch 6: 3856.132837
validation loss after epoch 6 : 1056.963078
	Epoch 7....
Epoch has taken 0:01:58.746561
Number of used sentences in train = 2074
Total loss for epoch 7: 3777.642794
validation loss after epoch 7 : 1019.930147
	Epoch 8....
Epoch has taken 0:01:59.062978
Number of used sentences in train = 2074
Total loss for epoch 8: 3575.877805
validation loss after epoch 8 : 1109.149484
	Epoch 9....
Epoch has taken 0:02:01.589313
Number of used sentences in train = 2074
Total loss for epoch 9: 3525.964576
validation loss after epoch 9 : 1126.359804
	Epoch 10....
Epoch has taken 0:01:57.870802
Number of used sentences in train = 2074
Total loss for epoch 10: 3449.516627
validation loss after epoch 10 : 1102.061080
	Epoch 11....
Epoch has taken 0:01:57.844288
Number of used sentences in train = 2074
Total loss for epoch 11: 3409.312803
validation loss after epoch 11 : 1152.018768
	Epoch 12....
Epoch has taken 0:01:57.907347
Number of used sentences in train = 2074
Total loss for epoch 12: 3356.804861
validation loss after epoch 12 : 1322.851380
	Epoch 13....
Epoch has taken 0:01:58.604848
Number of used sentences in train = 2074
Total loss for epoch 13: 3336.259980
validation loss after epoch 13 : 1314.805671
	Epoch 14....
Epoch has taken 0:02:02.056469
Number of used sentences in train = 2074
Total loss for epoch 14: 3299.501436
validation loss after epoch 14 : 1265.806893
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(5620, 222)
  (lstm): LSTM(239, 113, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.969678
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1476.295323
	Epoch 1....
Epoch has taken 0:00:12.001911
Number of used sentences in train = 231
Total loss for epoch 1: 856.406438
	Epoch 2....
Epoch has taken 0:00:11.995588
Number of used sentences in train = 231
Total loss for epoch 2: 594.534248
	Epoch 3....
Epoch has taken 0:00:12.004413
Number of used sentences in train = 231
Total loss for epoch 3: 460.695208
	Epoch 4....
Epoch has taken 0:00:11.993408
Number of used sentences in train = 231
Total loss for epoch 4: 415.139335
	Epoch 5....
Epoch has taken 0:00:11.997740
Number of used sentences in train = 231
Total loss for epoch 5: 388.231788
	Epoch 6....
Epoch has taken 0:00:11.995301
Number of used sentences in train = 231
Total loss for epoch 6: 383.201505
	Epoch 7....
Epoch has taken 0:00:12.007146
Number of used sentences in train = 231
Total loss for epoch 7: 368.957181
	Epoch 8....
Epoch has taken 0:00:12.001998
Number of used sentences in train = 231
Total loss for epoch 8: 365.270854
	Epoch 9....
Epoch has taken 0:00:12.002625
Number of used sentences in train = 231
Total loss for epoch 9: 358.433306
	Epoch 10....
Epoch has taken 0:00:11.998148
Number of used sentences in train = 231
Total loss for epoch 10: 356.813257
	Epoch 11....
Epoch has taken 0:00:11.998786
Number of used sentences in train = 231
Total loss for epoch 11: 355.620329
	Epoch 12....
Epoch has taken 0:00:12.422932
Number of used sentences in train = 231
Total loss for epoch 12: 355.035901
	Epoch 13....
Epoch has taken 0:00:12.419487
Number of used sentences in train = 231
Total loss for epoch 13: 354.454419
	Epoch 14....
Epoch has taken 0:00:12.426023
Number of used sentences in train = 231
Total loss for epoch 14: 351.818828
Epoch has taken 0:00:12.435711

==================================================================================================
	Training time : 0:32:42.458400
==================================================================================================
	Identification : 0.199

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(6883, 222)
  (lstm): LSTM(239, 113, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18665.495380
validation loss after epoch 0 : 1651.985853
	Epoch 1....
Epoch has taken 0:03:49.781037
Number of used sentences in train = 3226
Total loss for epoch 1: 12659.396761
validation loss after epoch 1 : 1526.431645
	Epoch 2....
Epoch has taken 0:03:50.175891
Number of used sentences in train = 3226
Total loss for epoch 2: 10458.214119
validation loss after epoch 2 : 1599.965927
	Epoch 3....
Epoch has taken 0:03:49.699382
Number of used sentences in train = 3226
Total loss for epoch 3: 8885.207611
validation loss after epoch 3 : 1649.399706
	Epoch 4....
Epoch has taken 0:03:49.215300
Number of used sentences in train = 3226
Total loss for epoch 4: 7954.382867
validation loss after epoch 4 : 1733.151347
	Epoch 5....
Epoch has taken 0:03:49.220328
Number of used sentences in train = 3226
Total loss for epoch 5: 7322.792651
validation loss after epoch 5 : 2005.989717
	Epoch 6....
Epoch has taken 0:03:49.211108
Number of used sentences in train = 3226
Total loss for epoch 6: 6944.426403
validation loss after epoch 6 : 2017.618950
	Epoch 7....
Epoch has taken 0:03:52.665163
Number of used sentences in train = 3226
Total loss for epoch 7: 6692.643986
validation loss after epoch 7 : 2130.039987
	Epoch 8....
Epoch has taken 0:03:51.497585
Number of used sentences in train = 3226
Total loss for epoch 8: 6539.944568
validation loss after epoch 8 : 2101.721050
	Epoch 9....
Epoch has taken 0:03:49.959821
Number of used sentences in train = 3226
Total loss for epoch 9: 6449.817294
validation loss after epoch 9 : 2322.572121
	Epoch 10....
Epoch has taken 0:03:49.432715
Number of used sentences in train = 3226
Total loss for epoch 10: 6381.210395
validation loss after epoch 10 : 2371.244971
	Epoch 11....
Epoch has taken 0:03:51.423337
Number of used sentences in train = 3226
Total loss for epoch 11: 6325.973472
validation loss after epoch 11 : 2465.428175
	Epoch 12....
Epoch has taken 0:03:49.541509
Number of used sentences in train = 3226
Total loss for epoch 12: 6317.990491
validation loss after epoch 12 : 2387.368242
	Epoch 13....
Epoch has taken 0:03:52.236104
Number of used sentences in train = 3226
Total loss for epoch 13: 6305.144324
validation loss after epoch 13 : 2257.503462
	Epoch 14....
Epoch has taken 0:03:49.114554
Number of used sentences in train = 3226
Total loss for epoch 14: 6258.676450
validation loss after epoch 14 : 2471.329699
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(6883, 222)
  (lstm): LSTM(239, 113, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=19, bias=True)
  (linear2): Linear(in_features=19, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:49.214781
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2205.972424
	Epoch 1....
Epoch has taken 0:00:22.364931
Number of used sentences in train = 359
Total loss for epoch 1: 1306.913017
	Epoch 2....
Epoch has taken 0:00:22.637333
Number of used sentences in train = 359
Total loss for epoch 2: 1031.102166
	Epoch 3....
Epoch has taken 0:00:23.027073
Number of used sentences in train = 359
Total loss for epoch 3: 899.569654
	Epoch 4....
Epoch has taken 0:00:23.039866
Number of used sentences in train = 359
Total loss for epoch 4: 776.383847
	Epoch 5....
Epoch has taken 0:00:23.044028
Number of used sentences in train = 359
Total loss for epoch 5: 736.374966
	Epoch 6....
Epoch has taken 0:00:23.035830
Number of used sentences in train = 359
Total loss for epoch 6: 720.259049
	Epoch 7....
Epoch has taken 0:00:23.050536
Number of used sentences in train = 359
Total loss for epoch 7: 706.219354
	Epoch 8....
Epoch has taken 0:00:23.037054
Number of used sentences in train = 359
Total loss for epoch 8: 698.876680
	Epoch 9....
Epoch has taken 0:00:23.031410
Number of used sentences in train = 359
Total loss for epoch 9: 690.966597
	Epoch 10....
Epoch has taken 0:00:23.050042
Number of used sentences in train = 359
Total loss for epoch 10: 690.138260
	Epoch 11....
Epoch has taken 0:00:23.055645
Number of used sentences in train = 359
Total loss for epoch 11: 690.871330
	Epoch 12....
Epoch has taken 0:00:23.064399
Number of used sentences in train = 359
Total loss for epoch 12: 687.758469
	Epoch 13....
Epoch has taken 0:00:23.031583
Number of used sentences in train = 359
Total loss for epoch 13: 686.835156
	Epoch 14....
Epoch has taken 0:00:23.025269
Number of used sentences in train = 359
Total loss for epoch 14: 686.425835
Epoch has taken 0:00:23.046021

==================================================================================================
	Training time : 1:03:17.601811
==================================================================================================
	Identification : 0.482

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 26, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 103, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 61, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1177, 61)
  (lstm): LSTM(128, 103, bidirectional=True)
  (linear1): Linear(in_features=1648, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12755.111068
validation loss after epoch 0 : 1022.705272
	Epoch 1....
Epoch has taken 0:02:45.226845
Number of used sentences in train = 2811
Total loss for epoch 1: 8896.956622
validation loss after epoch 1 : 953.155803
	Epoch 2....
Epoch has taken 0:02:44.784174
Number of used sentences in train = 2811
Total loss for epoch 2: 8100.527087
validation loss after epoch 2 : 939.374912
	Epoch 3....
Epoch has taken 0:02:53.171417
Number of used sentences in train = 2811
Total loss for epoch 3: 7426.807488
validation loss after epoch 3 : 919.077769
	Epoch 4....
Epoch has taken 0:02:45.418928
Number of used sentences in train = 2811
Total loss for epoch 4: 6977.882721
validation loss after epoch 4 : 940.028736
	Epoch 5....
Epoch has taken 0:02:45.319244
Number of used sentences in train = 2811
Total loss for epoch 5: 6560.517008
validation loss after epoch 5 : 932.421415
	Epoch 6....
Epoch has taken 0:02:54.820843
Number of used sentences in train = 2811
Total loss for epoch 6: 6259.519611
validation loss after epoch 6 : 974.607515
	Epoch 7....
Epoch has taken 0:02:46.126445
Number of used sentences in train = 2811
Total loss for epoch 7: 6006.591373
validation loss after epoch 7 : 982.585105
	Epoch 8....
Epoch has taken 0:02:45.698406
Number of used sentences in train = 2811
Total loss for epoch 8: 5813.954371
validation loss after epoch 8 : 953.640299
	Epoch 9....
Epoch has taken 0:02:46.304848
Number of used sentences in train = 2811
Total loss for epoch 9: 5592.110214
validation loss after epoch 9 : 999.787471
	Epoch 10....
Epoch has taken 0:02:57.556653
Number of used sentences in train = 2811
Total loss for epoch 10: 5412.823261
validation loss after epoch 10 : 981.337421
	Epoch 11....
Epoch has taken 0:02:45.818934
Number of used sentences in train = 2811
Total loss for epoch 11: 5276.314434
validation loss after epoch 11 : 1017.056873
	Epoch 12....
Epoch has taken 0:02:46.113988
Number of used sentences in train = 2811
Total loss for epoch 12: 5132.998815
validation loss after epoch 12 : 1060.048984
	Epoch 13....
Epoch has taken 0:02:43.754127
Number of used sentences in train = 2811
Total loss for epoch 13: 5060.854933
validation loss after epoch 13 : 1065.041538
	Epoch 14....
Epoch has taken 0:02:45.148403
Number of used sentences in train = 2811
Total loss for epoch 14: 4968.654192
validation loss after epoch 14 : 1071.802070
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1177, 61)
  (lstm): LSTM(128, 103, bidirectional=True)
  (linear1): Linear(in_features=1648, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.496846
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1415.658494
	Epoch 1....
Epoch has taken 0:00:17.408094
Number of used sentences in train = 313
Total loss for epoch 1: 943.787100
	Epoch 2....
Epoch has taken 0:00:17.417140
Number of used sentences in train = 313
Total loss for epoch 2: 811.904645
	Epoch 3....
Epoch has taken 0:00:17.424392
Number of used sentences in train = 313
Total loss for epoch 3: 697.025661
	Epoch 4....
Epoch has taken 0:00:17.405136
Number of used sentences in train = 313
Total loss for epoch 4: 638.403710
	Epoch 5....
Epoch has taken 0:00:17.403544
Number of used sentences in train = 313
Total loss for epoch 5: 596.284019
	Epoch 6....
Epoch has taken 0:00:17.426561
Number of used sentences in train = 313
Total loss for epoch 6: 577.319149
	Epoch 7....
Epoch has taken 0:00:17.440378
Number of used sentences in train = 313
Total loss for epoch 7: 570.445856
	Epoch 8....
Epoch has taken 0:00:17.454297
Number of used sentences in train = 313
Total loss for epoch 8: 564.894640
	Epoch 9....
Epoch has taken 0:00:17.470122
Number of used sentences in train = 313
Total loss for epoch 9: 560.590278
	Epoch 10....
Epoch has taken 0:00:17.459219
Number of used sentences in train = 313
Total loss for epoch 10: 557.077232
	Epoch 11....
Epoch has taken 0:00:17.457606
Number of used sentences in train = 313
Total loss for epoch 11: 555.317264
	Epoch 12....
Epoch has taken 0:00:17.462949
Number of used sentences in train = 313
Total loss for epoch 12: 553.030763
	Epoch 13....
Epoch has taken 0:00:17.455368
Number of used sentences in train = 313
Total loss for epoch 13: 548.160052
	Epoch 14....
Epoch has taken 0:00:17.459165
Number of used sentences in train = 313
Total loss for epoch 14: 546.652643
Epoch has taken 0:00:17.459634

==================================================================================================
	Training time : 0:46:23.859373
==================================================================================================
	Identification : 0.527

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1133, 61)
  (lstm): LSTM(128, 103, bidirectional=True)
  (linear1): Linear(in_features=1648, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8446.961939
validation loss after epoch 0 : 729.272288
	Epoch 1....
Epoch has taken 0:01:53.082130
Number of used sentences in train = 2074
Total loss for epoch 1: 5898.675195
validation loss after epoch 1 : 718.172870
	Epoch 2....
Epoch has taken 0:01:52.927642
Number of used sentences in train = 2074
Total loss for epoch 2: 5141.776604
validation loss after epoch 2 : 687.162439
	Epoch 3....
Epoch has taken 0:01:52.902717
Number of used sentences in train = 2074
Total loss for epoch 3: 4554.591428
validation loss after epoch 3 : 693.939302
	Epoch 4....
Epoch has taken 0:01:52.937666
Number of used sentences in train = 2074
Total loss for epoch 4: 4206.677525
validation loss after epoch 4 : 747.398592
	Epoch 5....
Epoch has taken 0:02:01.415024
Number of used sentences in train = 2074
Total loss for epoch 5: 3917.307592
validation loss after epoch 5 : 726.712584
	Epoch 6....
Epoch has taken 0:01:52.998576
Number of used sentences in train = 2074
Total loss for epoch 6: 3715.293135
validation loss after epoch 6 : 798.761930
	Epoch 7....
Epoch has taken 0:02:05.186844
Number of used sentences in train = 2074
Total loss for epoch 7: 3541.308781
validation loss after epoch 7 : 800.192562
	Epoch 8....
Epoch has taken 0:01:51.280068
Number of used sentences in train = 2074
Total loss for epoch 8: 3404.203912
validation loss after epoch 8 : 784.211219
	Epoch 9....
Epoch has taken 0:01:49.789373
Number of used sentences in train = 2074
Total loss for epoch 9: 3345.082125
validation loss after epoch 9 : 852.000644
	Epoch 10....
Epoch has taken 0:01:49.920769
Number of used sentences in train = 2074
Total loss for epoch 10: 3296.785409
validation loss after epoch 10 : 874.256963
	Epoch 11....
Epoch has taken 0:01:49.865343
Number of used sentences in train = 2074
Total loss for epoch 11: 3262.709196
validation loss after epoch 11 : 907.516452
	Epoch 12....
Epoch has taken 0:02:04.195222
Number of used sentences in train = 2074
Total loss for epoch 12: 3244.597164
validation loss after epoch 12 : 937.332776
	Epoch 13....
Epoch has taken 0:01:49.774928
Number of used sentences in train = 2074
Total loss for epoch 13: 3232.300665
validation loss after epoch 13 : 942.634156
	Epoch 14....
Epoch has taken 0:01:49.769297
Number of used sentences in train = 2074
Total loss for epoch 14: 3223.038394
validation loss after epoch 14 : 955.274444
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1133, 61)
  (lstm): LSTM(128, 103, bidirectional=True)
  (linear1): Linear(in_features=1648, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.751799
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1266.735276
	Epoch 1....
Epoch has taken 0:00:11.151596
Number of used sentences in train = 231
Total loss for epoch 1: 696.759324
	Epoch 2....
Epoch has taken 0:00:11.152748
Number of used sentences in train = 231
Total loss for epoch 2: 525.200862
	Epoch 3....
Epoch has taken 0:00:11.147877
Number of used sentences in train = 231
Total loss for epoch 3: 446.229543
	Epoch 4....
Epoch has taken 0:00:11.161630
Number of used sentences in train = 231
Total loss for epoch 4: 401.591946
	Epoch 5....
Epoch has taken 0:00:11.157774
Number of used sentences in train = 231
Total loss for epoch 5: 373.067526
	Epoch 6....
Epoch has taken 0:00:11.135539
Number of used sentences in train = 231
Total loss for epoch 6: 364.278242
	Epoch 7....
Epoch has taken 0:00:11.142174
Number of used sentences in train = 231
Total loss for epoch 7: 361.833806
	Epoch 8....
Epoch has taken 0:00:11.139137
Number of used sentences in train = 231
Total loss for epoch 8: 358.025875
	Epoch 9....
Epoch has taken 0:00:11.140216
Number of used sentences in train = 231
Total loss for epoch 9: 357.302794
	Epoch 10....
Epoch has taken 0:00:11.145414
Number of used sentences in train = 231
Total loss for epoch 10: 355.676477
	Epoch 11....
Epoch has taken 0:00:11.153191
Number of used sentences in train = 231
Total loss for epoch 11: 355.205409
	Epoch 12....
Epoch has taken 0:00:11.142774
Number of used sentences in train = 231
Total loss for epoch 12: 354.347109
	Epoch 13....
Epoch has taken 0:00:11.152401
Number of used sentences in train = 231
Total loss for epoch 13: 353.766153
	Epoch 14....
Epoch has taken 0:00:11.144351
Number of used sentences in train = 231
Total loss for epoch 14: 353.214098
Epoch has taken 0:00:11.142699

==================================================================================================
	Training time : 0:31:13.341713
==================================================================================================
	Identification : 0.197

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(1202, 61)
  (lstm): LSTM(128, 103, bidirectional=True)
  (linear1): Linear(in_features=1648, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17097.126280
validation loss after epoch 0 : 1389.662200
	Epoch 1....
Epoch has taken 0:03:50.816293
Number of used sentences in train = 3226
Total loss for epoch 1: 11873.610177
validation loss after epoch 1 : 1300.770525
	Epoch 2....
Epoch has taken 0:03:35.791113
Number of used sentences in train = 3226
Total loss for epoch 2: 10894.341809
validation loss after epoch 2 : 1288.724223
	Epoch 3....
Epoch has taken 0:03:39.402280
Number of used sentences in train = 3226
Total loss for epoch 3: 10139.856262
validation loss after epoch 3 : 1246.514479
	Epoch 4....
Epoch has taken 0:03:41.478203
Number of used sentences in train = 3226
Total loss for epoch 4: 9569.298323
validation loss after epoch 4 : 1291.899352
	Epoch 5....
Epoch has taken 0:03:38.999260
Number of used sentences in train = 3226
Total loss for epoch 5: 9192.302457
validation loss after epoch 5 : 1268.714574
	Epoch 6....
Epoch has taken 0:03:42.407158
Number of used sentences in train = 3226
Total loss for epoch 6: 8808.794366
validation loss after epoch 6 : 1303.748864
	Epoch 7....
Epoch has taken 0:03:36.299426
Number of used sentences in train = 3226
Total loss for epoch 7: 8503.373182
validation loss after epoch 7 : 1333.128333
	Epoch 8....
Epoch has taken 0:03:36.325060
Number of used sentences in train = 3226
Total loss for epoch 8: 8241.378878
validation loss after epoch 8 : 1387.585000
	Epoch 9....
Epoch has taken 0:03:37.797723
Number of used sentences in train = 3226
Total loss for epoch 9: 8010.358792
validation loss after epoch 9 : 1414.643253
	Epoch 10....
Epoch has taken 0:03:39.087322
Number of used sentences in train = 3226
Total loss for epoch 10: 7768.503376
validation loss after epoch 10 : 1421.080862
	Epoch 11....
Epoch has taken 0:03:36.435437
Number of used sentences in train = 3226
Total loss for epoch 11: 7568.796018
validation loss after epoch 11 : 1485.105962
	Epoch 12....
Epoch has taken 0:04:05.856419
Number of used sentences in train = 3226
Total loss for epoch 12: 7406.251417
validation loss after epoch 12 : 1512.056978
	Epoch 13....
Epoch has taken 0:03:36.557330
Number of used sentences in train = 3226
Total loss for epoch 13: 7260.170072
validation loss after epoch 13 : 1520.193633
	Epoch 14....
Epoch has taken 0:03:57.884861
Number of used sentences in train = 3226
Total loss for epoch 14: 7107.145909
validation loss after epoch 14 : 1580.803162
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(1202, 61)
  (lstm): LSTM(128, 103, bidirectional=True)
  (linear1): Linear(in_features=1648, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:37.944328
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1703.627173
	Epoch 1....
Epoch has taken 0:00:21.106379
Number of used sentences in train = 359
Total loss for epoch 1: 1210.911480
	Epoch 2....
Epoch has taken 0:00:21.093926
Number of used sentences in train = 359
Total loss for epoch 2: 1083.842471
	Epoch 3....
Epoch has taken 0:00:21.101109
Number of used sentences in train = 359
Total loss for epoch 3: 980.801517
	Epoch 4....
Epoch has taken 0:00:21.106324
Number of used sentences in train = 359
Total loss for epoch 4: 851.215815
	Epoch 5....
Epoch has taken 0:00:21.090401
Number of used sentences in train = 359
Total loss for epoch 5: 788.781268
	Epoch 6....
Epoch has taken 0:00:21.093314
Number of used sentences in train = 359
Total loss for epoch 6: 742.702499
	Epoch 7....
Epoch has taken 0:00:21.093061
Number of used sentences in train = 359
Total loss for epoch 7: 728.148926
	Epoch 8....
Epoch has taken 0:00:21.088245
Number of used sentences in train = 359
Total loss for epoch 8: 708.510537
	Epoch 9....
Epoch has taken 0:00:21.091834
Number of used sentences in train = 359
Total loss for epoch 9: 695.901937
	Epoch 10....
Epoch has taken 0:00:21.090853
Number of used sentences in train = 359
Total loss for epoch 10: 685.147283
	Epoch 11....
Epoch has taken 0:00:21.107706
Number of used sentences in train = 359
Total loss for epoch 11: 679.929713
	Epoch 12....
Epoch has taken 0:00:21.095416
Number of used sentences in train = 359
Total loss for epoch 12: 676.676958
	Epoch 13....
Epoch has taken 0:00:21.112790
Number of used sentences in train = 359
Total loss for epoch 13: 674.033116
	Epoch 14....
Epoch has taken 0:00:21.101119
Number of used sentences in train = 359
Total loss for epoch 14: 672.889039
Epoch has taken 0:00:21.093652

==================================================================================================
	Training time : 1:00:50.232513
==================================================================================================
	Identification : 0.084

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 33, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 49, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 101, 'lstmDropout': 0.2, 'denseActivation': 'tanh', 'wordDim': 102, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 49)
  (w_embeddings): Embedding(5906, 102)
  (lstm): LSTM(151, 101, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=1616, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 16955.172555
validation loss after epoch 0 : 1218.113951
	Epoch 1....
Epoch has taken 0:02:52.798145
Number of used sentences in train = 2811
Total loss for epoch 1: 9733.054189
validation loss after epoch 1 : 1066.911471
	Epoch 2....
Epoch has taken 0:02:52.362723
Number of used sentences in train = 2811
Total loss for epoch 2: 7976.009752
validation loss after epoch 2 : 1054.455503
	Epoch 3....
Epoch has taken 0:02:54.657571
Number of used sentences in train = 2811
Total loss for epoch 3: 6968.428723
validation loss after epoch 3 : 1046.829636
	Epoch 4....
Epoch has taken 0:02:58.523523
Number of used sentences in train = 2811
Total loss for epoch 4: 6236.931842
validation loss after epoch 4 : 1061.305365
	Epoch 5....
Epoch has taken 0:02:58.035317
Number of used sentences in train = 2811
Total loss for epoch 5: 5792.155840
validation loss after epoch 5 : 1140.804702
	Epoch 6....
Epoch has taken 0:02:59.112167
Number of used sentences in train = 2811
Total loss for epoch 6: 5343.854701
validation loss after epoch 6 : 1196.486058
	Epoch 7....
Epoch has taken 0:02:59.097659
Number of used sentences in train = 2811
Total loss for epoch 7: 5127.664901
validation loss after epoch 7 : 1279.708835
	Epoch 8....
Epoch has taken 0:02:57.033784
Number of used sentences in train = 2811
Total loss for epoch 8: 4989.994840
validation loss after epoch 8 : 1270.026057
	Epoch 9....
Epoch has taken 0:02:57.952924
Number of used sentences in train = 2811
Total loss for epoch 9: 4863.348396
validation loss after epoch 9 : 1314.497818
	Epoch 10....
Epoch has taken 0:02:57.368864
Number of used sentences in train = 2811
Total loss for epoch 10: 4751.914416
validation loss after epoch 10 : 1373.682644
	Epoch 11....
Epoch has taken 0:02:59.024841
Number of used sentences in train = 2811
Total loss for epoch 11: 4744.143608
validation loss after epoch 11 : 1392.523489
	Epoch 12....
Epoch has taken 0:02:59.156842
Number of used sentences in train = 2811
Total loss for epoch 12: 4665.927724
validation loss after epoch 12 : 1399.204807
	Epoch 13....
Epoch has taken 0:02:57.746825
Number of used sentences in train = 2811
Total loss for epoch 13: 4683.530707
validation loss after epoch 13 : 1445.660378
	Epoch 14....
Epoch has taken 0:02:58.461344
Number of used sentences in train = 2811
Total loss for epoch 14: 4619.853359
validation loss after epoch 14 : 1512.739856
	TransitionClassifier(
  (p_embeddings): Embedding(18, 49)
  (w_embeddings): Embedding(5906, 102)
  (lstm): LSTM(151, 101, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=1616, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:57.341423
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1855.446455
	Epoch 1....
Epoch has taken 0:00:18.848618
Number of used sentences in train = 313
Total loss for epoch 1: 878.350078
	Epoch 2....
Epoch has taken 0:00:18.841780
Number of used sentences in train = 313
Total loss for epoch 2: 714.001399
	Epoch 3....
Epoch has taken 0:00:18.853235
Number of used sentences in train = 313
Total loss for epoch 3: 616.115144
	Epoch 4....
Epoch has taken 0:00:18.835938
Number of used sentences in train = 313
Total loss for epoch 4: 591.436849
	Epoch 5....
Epoch has taken 0:00:18.854435
Number of used sentences in train = 313
Total loss for epoch 5: 570.092775
	Epoch 6....
Epoch has taken 0:00:18.839714
Number of used sentences in train = 313
Total loss for epoch 6: 544.836183
	Epoch 7....
Epoch has taken 0:00:18.842522
Number of used sentences in train = 313
Total loss for epoch 7: 539.731303
	Epoch 8....
Epoch has taken 0:00:18.847466
Number of used sentences in train = 313
Total loss for epoch 8: 519.574972
	Epoch 9....
Epoch has taken 0:00:18.838810
Number of used sentences in train = 313
Total loss for epoch 9: 517.418561
	Epoch 10....
Epoch has taken 0:00:18.754790
Number of used sentences in train = 313
Total loss for epoch 10: 529.610657
	Epoch 11....
Epoch has taken 0:00:18.624896
Number of used sentences in train = 313
Total loss for epoch 11: 520.219337
	Epoch 12....
Epoch has taken 0:00:18.581022
Number of used sentences in train = 313
Total loss for epoch 12: 516.068887
	Epoch 13....
Epoch has taken 0:00:18.854415
Number of used sentences in train = 313
Total loss for epoch 13: 514.694625
	Epoch 14....
Epoch has taken 0:00:18.822889
Number of used sentences in train = 313
Total loss for epoch 14: 513.736936
Epoch has taken 0:00:18.830837

==================================================================================================
	Training time : 0:49:01.258968
==================================================================================================
	Identification : 0.354

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 49)
  (w_embeddings): Embedding(5645, 102)
  (lstm): LSTM(151, 101, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=1616, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 14531.296152
validation loss after epoch 0 : 1055.272725
	Epoch 1....
Epoch has taken 0:02:01.989869
Number of used sentences in train = 2074
Total loss for epoch 1: 7336.269520
validation loss after epoch 1 : 801.608114
	Epoch 2....
Epoch has taken 0:02:03.250565
Number of used sentences in train = 2074
Total loss for epoch 2: 5916.222240
validation loss after epoch 2 : 799.842884
	Epoch 3....
Epoch has taken 0:02:02.229725
Number of used sentences in train = 2074
Total loss for epoch 3: 5033.803167
validation loss after epoch 3 : 873.856577
	Epoch 4....
Epoch has taken 0:02:08.732425
Number of used sentences in train = 2074
Total loss for epoch 4: 4487.293755
validation loss after epoch 4 : 925.481816
	Epoch 5....
Epoch has taken 0:02:15.138957
Number of used sentences in train = 2074
Total loss for epoch 5: 4136.975335
validation loss after epoch 5 : 937.251083
	Epoch 6....
Epoch has taken 0:02:12.002154
Number of used sentences in train = 2074
Total loss for epoch 6: 3927.135034
validation loss after epoch 6 : 1047.344051
	Epoch 7....
Epoch has taken 0:02:13.955829
Number of used sentences in train = 2074
Total loss for epoch 7: 3704.473187
validation loss after epoch 7 : 989.361140
	Epoch 8....
Epoch has taken 0:02:04.795395
Number of used sentences in train = 2074
Total loss for epoch 8: 3553.625093
validation loss after epoch 8 : 1060.012863
	Epoch 9....
Epoch has taken 0:02:02.588607
Number of used sentences in train = 2074
Total loss for epoch 9: 3490.046927
validation loss after epoch 9 : 1055.157455
	Epoch 10....
Epoch has taken 0:02:02.236517
Number of used sentences in train = 2074
Total loss for epoch 10: 3397.189390
validation loss after epoch 10 : 1055.571699
	Epoch 11....
Epoch has taken 0:01:58.549954
Number of used sentences in train = 2074
Total loss for epoch 11: 3339.008457
validation loss after epoch 11 : 1077.230027
	Epoch 12....
Epoch has taken 0:02:01.040910
Number of used sentences in train = 2074
Total loss for epoch 12: 3289.149092
validation loss after epoch 12 : 1102.392586
	Epoch 13....
Epoch has taken 0:01:59.097936
Number of used sentences in train = 2074
Total loss for epoch 13: 3304.228830
validation loss after epoch 13 : 1108.758363
	Epoch 14....
Epoch has taken 0:02:02.163641
Number of used sentences in train = 2074
Total loss for epoch 14: 3277.365247
validation loss after epoch 14 : 1097.411804
	TransitionClassifier(
  (p_embeddings): Embedding(18, 49)
  (w_embeddings): Embedding(5645, 102)
  (lstm): LSTM(151, 101, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=1616, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:00.751961
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1616.952734
	Epoch 1....
Epoch has taken 0:00:12.451101
Number of used sentences in train = 231
Total loss for epoch 1: 822.925645
	Epoch 2....
Epoch has taken 0:00:12.447072
Number of used sentences in train = 231
Total loss for epoch 2: 609.535221
	Epoch 3....
Epoch has taken 0:00:12.434763
Number of used sentences in train = 231
Total loss for epoch 3: 469.693260
	Epoch 4....
Epoch has taken 0:00:12.452231
Number of used sentences in train = 231
Total loss for epoch 4: 421.343520
	Epoch 5....
Epoch has taken 0:00:12.444781
Number of used sentences in train = 231
Total loss for epoch 5: 380.786375
	Epoch 6....
Epoch has taken 0:00:12.441563
Number of used sentences in train = 231
Total loss for epoch 6: 363.857848
	Epoch 7....
Epoch has taken 0:00:12.430821
Number of used sentences in train = 231
Total loss for epoch 7: 356.140214
	Epoch 8....
Epoch has taken 0:00:12.466110
Number of used sentences in train = 231
Total loss for epoch 8: 353.463187
	Epoch 9....
Epoch has taken 0:00:12.446452
Number of used sentences in train = 231
Total loss for epoch 9: 357.710197
	Epoch 10....
Epoch has taken 0:00:12.445218
Number of used sentences in train = 231
Total loss for epoch 10: 352.254877
	Epoch 11....
Epoch has taken 0:00:12.454020
Number of used sentences in train = 231
Total loss for epoch 11: 349.751636
	Epoch 12....
Epoch has taken 0:00:12.452929
Number of used sentences in train = 231
Total loss for epoch 12: 350.143265
	Epoch 13....
Epoch has taken 0:00:12.444002
Number of used sentences in train = 231
Total loss for epoch 13: 348.304507
	Epoch 14....
Epoch has taken 0:00:12.449832
Number of used sentences in train = 231
Total loss for epoch 14: 348.506335
Epoch has taken 0:00:12.445062

==================================================================================================
	Training time : 0:34:15.576371
==================================================================================================
	Identification : 0.361

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 49)
  (w_embeddings): Embedding(6870, 102)
  (lstm): LSTM(151, 101, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=1616, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20989.025043
validation loss after epoch 0 : 1542.031636
	Epoch 1....
Epoch has taken 0:03:52.915778
Number of used sentences in train = 3226
Total loss for epoch 1: 13030.326312
validation loss after epoch 1 : 1470.646359
	Epoch 2....
Epoch has taken 0:03:54.383271
Number of used sentences in train = 3226
Total loss for epoch 2: 11004.938727
validation loss after epoch 2 : 1505.830234
	Epoch 3....
Epoch has taken 0:03:53.215125
Number of used sentences in train = 3226
Total loss for epoch 3: 9706.666080
validation loss after epoch 3 : 1518.387572
	Epoch 4....
Epoch has taken 0:03:52.711337
Number of used sentences in train = 3226
Total loss for epoch 4: 8870.740018
validation loss after epoch 4 : 1656.811449
	Epoch 5....
Epoch has taken 0:03:48.811964
Number of used sentences in train = 3226
Total loss for epoch 5: 8152.010050
validation loss after epoch 5 : 1814.704569
	Epoch 6....
Epoch has taken 0:03:52.326406
Number of used sentences in train = 3226
Total loss for epoch 6: 7644.489948
validation loss after epoch 6 : 2032.847280
	Epoch 7....
Epoch has taken 0:03:52.826432
Number of used sentences in train = 3226
Total loss for epoch 7: 7268.893043
validation loss after epoch 7 : 2113.199950
	Epoch 8....
Epoch has taken 0:03:48.972036
Number of used sentences in train = 3226
Total loss for epoch 8: 6990.451624
validation loss after epoch 8 : 2250.204539
	Epoch 9....
Epoch has taken 0:03:51.708536
Number of used sentences in train = 3226
Total loss for epoch 9: 6791.658722
validation loss after epoch 9 : 2349.305038
	Epoch 10....
Epoch has taken 0:03:51.901911
Number of used sentences in train = 3226
Total loss for epoch 10: 6683.710452
validation loss after epoch 10 : 2350.981218
	Epoch 11....
Epoch has taken 0:03:49.029192
Number of used sentences in train = 3226
Total loss for epoch 11: 6604.883321
validation loss after epoch 11 : 2193.787145
	Epoch 12....
Epoch has taken 0:03:51.754758
Number of used sentences in train = 3226
Total loss for epoch 12: 6494.412265
validation loss after epoch 12 : 2394.776309
	Epoch 13....
Epoch has taken 0:03:49.032665
Number of used sentences in train = 3226
Total loss for epoch 13: 6387.978904
validation loss after epoch 13 : 2361.366847
	Epoch 14....
Epoch has taken 0:03:50.095380
Number of used sentences in train = 3226
Total loss for epoch 14: 6367.243278
validation loss after epoch 14 : 2564.157179
	TransitionClassifier(
  (p_embeddings): Embedding(13, 49)
  (w_embeddings): Embedding(6870, 102)
  (lstm): LSTM(151, 101, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=1616, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:50.736626
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2251.428932
	Epoch 1....
Epoch has taken 0:00:22.295293
Number of used sentences in train = 359
Total loss for epoch 1: 1280.217737
	Epoch 2....
Epoch has taken 0:00:22.305102
Number of used sentences in train = 359
Total loss for epoch 2: 1057.167249
	Epoch 3....
Epoch has taken 0:00:22.295291
Number of used sentences in train = 359
Total loss for epoch 3: 880.891335
	Epoch 4....
Epoch has taken 0:00:22.293213
Number of used sentences in train = 359
Total loss for epoch 4: 796.765666
	Epoch 5....
Epoch has taken 0:00:22.308162
Number of used sentences in train = 359
Total loss for epoch 5: 742.334814
	Epoch 6....
Epoch has taken 0:00:22.395523
Number of used sentences in train = 359
Total loss for epoch 6: 706.687149
	Epoch 7....
Epoch has taken 0:00:22.308811
Number of used sentences in train = 359
Total loss for epoch 7: 696.993969
	Epoch 8....
Epoch has taken 0:00:22.301436
Number of used sentences in train = 359
Total loss for epoch 8: 682.179913
	Epoch 9....
Epoch has taken 0:00:22.296022
Number of used sentences in train = 359
Total loss for epoch 9: 675.358419
	Epoch 10....
Epoch has taken 0:00:22.291863
Number of used sentences in train = 359
Total loss for epoch 10: 681.036560
	Epoch 11....
Epoch has taken 0:00:22.300019
Number of used sentences in train = 359
Total loss for epoch 11: 672.706443
	Epoch 12....
Epoch has taken 0:00:22.299033
Number of used sentences in train = 359
Total loss for epoch 12: 679.952921
	Epoch 13....
Epoch has taken 0:00:22.302554
Number of used sentences in train = 359
Total loss for epoch 13: 685.877071
	Epoch 14....
Epoch has taken 0:00:22.294708
Number of used sentences in train = 359
Total loss for epoch 14: 675.222229
Epoch has taken 0:00:22.718946

==================================================================================================
	Training time : 1:03:26.088308
==================================================================================================
	Identification : 0.068

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 78, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 55, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 29, 'lstmDropout': 0.24, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(1177, 63)
  (lstm): LSTM(118, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=78, bias=True)
  (linear2): Linear(in_features=78, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10743.277632
validation loss after epoch 0 : 922.046286
	Epoch 1....
Epoch has taken 0:02:39.845377
Number of used sentences in train = 2811
Total loss for epoch 1: 7808.790967
validation loss after epoch 1 : 871.236054
	Epoch 2....
Epoch has taken 0:02:39.999076
Number of used sentences in train = 2811
Total loss for epoch 2: 6977.340358
validation loss after epoch 2 : 881.029883
	Epoch 3....
Epoch has taken 0:02:45.007534
Number of used sentences in train = 2811
Total loss for epoch 3: 6373.586444
validation loss after epoch 3 : 928.272533
	Epoch 4....
Epoch has taken 0:02:41.049268
Number of used sentences in train = 2811
Total loss for epoch 4: 5977.728137
validation loss after epoch 4 : 955.254802
	Epoch 5....
Epoch has taken 0:02:40.112349
Number of used sentences in train = 2811
Total loss for epoch 5: 5659.061126
validation loss after epoch 5 : 986.643683
	Epoch 6....
Epoch has taken 0:02:42.380971
Number of used sentences in train = 2811
Total loss for epoch 6: 5429.874801
validation loss after epoch 6 : 1007.638555
	Epoch 7....
Epoch has taken 0:02:39.740496
Number of used sentences in train = 2811
Total loss for epoch 7: 5232.865845
validation loss after epoch 7 : 1039.364397
	Epoch 8....
Epoch has taken 0:02:40.089597
Number of used sentences in train = 2811
Total loss for epoch 8: 5078.650992
validation loss after epoch 8 : 1071.077573
	Epoch 9....
Epoch has taken 0:02:40.376049
Number of used sentences in train = 2811
Total loss for epoch 9: 4957.367174
validation loss after epoch 9 : 1134.949880
	Epoch 10....
Epoch has taken 0:02:43.517530
Number of used sentences in train = 2811
Total loss for epoch 10: 4858.529465
validation loss after epoch 10 : 1142.577475
	Epoch 11....
Epoch has taken 0:02:39.922306
Number of used sentences in train = 2811
Total loss for epoch 11: 4786.022538
validation loss after epoch 11 : 1213.468148
	Epoch 12....
Epoch has taken 0:02:44.125961
Number of used sentences in train = 2811
Total loss for epoch 12: 4711.118241
validation loss after epoch 12 : 1267.885434
	Epoch 13....
Epoch has taken 0:02:41.978945
Number of used sentences in train = 2811
Total loss for epoch 13: 4671.302193
validation loss after epoch 13 : 1270.824841
	Epoch 14....
Epoch has taken 0:02:41.771538
Number of used sentences in train = 2811
Total loss for epoch 14: 4626.452096
validation loss after epoch 14 : 1304.497184
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(1177, 63)
  (lstm): LSTM(118, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=78, bias=True)
  (linear2): Linear(in_features=78, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:44.324769
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1609.063230
	Epoch 1....
Epoch has taken 0:00:16.910309
Number of used sentences in train = 313
Total loss for epoch 1: 753.233229
	Epoch 2....
Epoch has taken 0:00:16.897500
Number of used sentences in train = 313
Total loss for epoch 2: 624.088560
	Epoch 3....
Epoch has taken 0:00:16.902307
Number of used sentences in train = 313
Total loss for epoch 3: 563.117033
	Epoch 4....
Epoch has taken 0:00:16.886600
Number of used sentences in train = 313
Total loss for epoch 4: 539.070897
	Epoch 5....
Epoch has taken 0:00:16.910843
Number of used sentences in train = 313
Total loss for epoch 5: 527.473226
	Epoch 6....
Epoch has taken 0:00:16.891105
Number of used sentences in train = 313
Total loss for epoch 6: 519.901207
	Epoch 7....
Epoch has taken 0:00:16.904329
Number of used sentences in train = 313
Total loss for epoch 7: 512.976224
	Epoch 8....
Epoch has taken 0:00:16.848705
Number of used sentences in train = 313
Total loss for epoch 8: 507.743515
	Epoch 9....
Epoch has taken 0:00:17.041509
Number of used sentences in train = 313
Total loss for epoch 9: 504.825856
	Epoch 10....
Epoch has taken 0:00:17.461000
Number of used sentences in train = 313
Total loss for epoch 10: 503.760021
	Epoch 11....
Epoch has taken 0:00:16.914072
Number of used sentences in train = 313
Total loss for epoch 11: 503.130126
	Epoch 12....
Epoch has taken 0:00:16.912235
Number of used sentences in train = 313
Total loss for epoch 12: 502.586324
	Epoch 13....
Epoch has taken 0:00:17.297467
Number of used sentences in train = 313
Total loss for epoch 13: 502.209830
	Epoch 14....
Epoch has taken 0:00:16.902437
Number of used sentences in train = 313
Total loss for epoch 14: 501.894990
Epoch has taken 0:00:16.913923

==================================================================================================
	Training time : 0:44:39.334384
==================================================================================================
	Identification : 0.504

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(1133, 63)
  (lstm): LSTM(118, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=78, bias=True)
  (linear2): Linear(in_features=78, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9097.986321
validation loss after epoch 0 : 769.635017
	Epoch 1....
Epoch has taken 0:01:49.401565
Number of used sentences in train = 2074
Total loss for epoch 1: 5773.891245
validation loss after epoch 1 : 706.283167
	Epoch 2....
Epoch has taken 0:01:49.437831
Number of used sentences in train = 2074
Total loss for epoch 2: 4956.962640
validation loss after epoch 2 : 711.909642
	Epoch 3....
Epoch has taken 0:01:50.198314
Number of used sentences in train = 2074
Total loss for epoch 3: 4480.185279
validation loss after epoch 3 : 726.946406
	Epoch 4....
Epoch has taken 0:01:49.330204
Number of used sentences in train = 2074
Total loss for epoch 4: 4110.011713
validation loss after epoch 4 : 786.317791
	Epoch 5....
Epoch has taken 0:01:49.398809
Number of used sentences in train = 2074
Total loss for epoch 5: 3892.708864
validation loss after epoch 5 : 744.214133
	Epoch 6....
Epoch has taken 0:01:49.449176
Number of used sentences in train = 2074
Total loss for epoch 6: 3702.414735
validation loss after epoch 6 : 796.504483
	Epoch 7....
Epoch has taken 0:01:49.441774
Number of used sentences in train = 2074
Total loss for epoch 7: 3580.000338
validation loss after epoch 7 : 819.107699
	Epoch 8....
Epoch has taken 0:01:49.366655
Number of used sentences in train = 2074
Total loss for epoch 8: 3460.253136
validation loss after epoch 8 : 841.319266
	Epoch 9....
Epoch has taken 0:01:54.036498
Number of used sentences in train = 2074
Total loss for epoch 9: 3387.750033
validation loss after epoch 9 : 835.005423
	Epoch 10....
Epoch has taken 0:01:49.417050
Number of used sentences in train = 2074
Total loss for epoch 10: 3325.248258
validation loss after epoch 10 : 879.491698
	Epoch 11....
Epoch has taken 0:01:49.517785
Number of used sentences in train = 2074
Total loss for epoch 11: 3290.277821
validation loss after epoch 11 : 905.465376
	Epoch 12....
Epoch has taken 0:01:49.491725
Number of used sentences in train = 2074
Total loss for epoch 12: 3263.009960
validation loss after epoch 12 : 901.638749
	Epoch 13....
Epoch has taken 0:01:51.698980
Number of used sentences in train = 2074
Total loss for epoch 13: 3234.967881
validation loss after epoch 13 : 930.657622
	Epoch 14....
Epoch has taken 0:01:49.359239
Number of used sentences in train = 2074
Total loss for epoch 14: 3214.211702
validation loss after epoch 14 : 946.298847
	TransitionClassifier(
  (p_embeddings): Embedding(18, 55)
  (w_embeddings): Embedding(1133, 63)
  (lstm): LSTM(118, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=78, bias=True)
  (linear2): Linear(in_features=78, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.313727
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1293.888716
	Epoch 1....
Epoch has taken 0:00:11.496995
Number of used sentences in train = 231
Total loss for epoch 1: 571.342910
	Epoch 2....
Epoch has taken 0:00:11.490662
Number of used sentences in train = 231
Total loss for epoch 2: 456.632490
	Epoch 3....
Epoch has taken 0:00:11.504168
Number of used sentences in train = 231
Total loss for epoch 3: 395.665598
	Epoch 4....
Epoch has taken 0:00:11.486392
Number of used sentences in train = 231
Total loss for epoch 4: 373.182335
	Epoch 5....
Epoch has taken 0:00:11.395645
Number of used sentences in train = 231
Total loss for epoch 5: 363.445208
	Epoch 6....
Epoch has taken 0:00:11.292566
Number of used sentences in train = 231
Total loss for epoch 6: 356.901457
	Epoch 7....
Epoch has taken 0:00:11.483357
Number of used sentences in train = 231
Total loss for epoch 7: 353.697508
	Epoch 8....
Epoch has taken 0:00:11.494113
Number of used sentences in train = 231
Total loss for epoch 8: 351.113795
	Epoch 9....
Epoch has taken 0:00:11.483901
Number of used sentences in train = 231
Total loss for epoch 9: 348.077875
	Epoch 10....
Epoch has taken 0:00:11.496863
Number of used sentences in train = 231
Total loss for epoch 10: 347.369074
	Epoch 11....
Epoch has taken 0:00:11.380170
Number of used sentences in train = 231
Total loss for epoch 11: 346.772004
	Epoch 12....
Epoch has taken 0:00:11.294802
Number of used sentences in train = 231
Total loss for epoch 12: 346.475276
	Epoch 13....
Epoch has taken 0:00:11.492647
Number of used sentences in train = 231
Total loss for epoch 13: 346.167844
	Epoch 14....
Epoch has taken 0:00:11.486959
Number of used sentences in train = 231
Total loss for epoch 14: 345.944712
Epoch has taken 0:00:11.494227

==================================================================================================
	Training time : 0:30:22.964261
==================================================================================================
	Identification : 0.393

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 55)
  (w_embeddings): Embedding(1202, 63)
  (lstm): LSTM(118, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=78, bias=True)
  (linear2): Linear(in_features=78, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15159.651377
validation loss after epoch 0 : 1324.088523
	Epoch 1....
Epoch has taken 0:03:37.844234
Number of used sentences in train = 3226
Total loss for epoch 1: 11504.466860
validation loss after epoch 1 : 1327.323195
	Epoch 2....
Epoch has taken 0:03:36.665574
Number of used sentences in train = 3226
Total loss for epoch 2: 10595.775524
validation loss after epoch 2 : 1298.730839
	Epoch 3....
Epoch has taken 0:03:37.508816
Number of used sentences in train = 3226
Total loss for epoch 3: 9976.588772
validation loss after epoch 3 : 1341.009822
	Epoch 4....
Epoch has taken 0:03:37.834524
Number of used sentences in train = 3226
Total loss for epoch 4: 9550.329464
validation loss after epoch 4 : 1386.861821
	Epoch 5....
Epoch has taken 0:03:37.723107
Number of used sentences in train = 3226
Total loss for epoch 5: 9153.523490
validation loss after epoch 5 : 1384.406010
	Epoch 6....
Epoch has taken 0:03:36.602820
Number of used sentences in train = 3226
Total loss for epoch 6: 8802.597337
validation loss after epoch 6 : 1478.554012
	Epoch 7....
Epoch has taken 0:03:38.121584
Number of used sentences in train = 3226
Total loss for epoch 7: 8538.720686
validation loss after epoch 7 : 1470.217394
	Epoch 8....
Epoch has taken 0:03:37.846542
Number of used sentences in train = 3226
Total loss for epoch 8: 8297.773133
validation loss after epoch 8 : 1502.600723
	Epoch 9....
Epoch has taken 0:03:36.534057
Number of used sentences in train = 3226
Total loss for epoch 9: 8072.930678
validation loss after epoch 9 : 1620.091815
	Epoch 10....
Epoch has taken 0:03:37.781291
Number of used sentences in train = 3226
Total loss for epoch 10: 7873.154573
validation loss after epoch 10 : 1650.508339
	Epoch 11....
Epoch has taken 0:03:38.554345
Number of used sentences in train = 3226
Total loss for epoch 11: 7723.015989
validation loss after epoch 11 : 1715.144931
	Epoch 12....
Epoch has taken 0:03:36.942294
Number of used sentences in train = 3226
Total loss for epoch 12: 7558.616421
validation loss after epoch 12 : 1733.732731
	Epoch 13....
Epoch has taken 0:03:38.221730
Number of used sentences in train = 3226
Total loss for epoch 13: 7393.678607
validation loss after epoch 13 : 1792.898855
	Epoch 14....
Epoch has taken 0:03:37.835108
Number of used sentences in train = 3226
Total loss for epoch 14: 7276.120024
validation loss after epoch 14 : 1865.298643
	TransitionClassifier(
  (p_embeddings): Embedding(13, 55)
  (w_embeddings): Embedding(1202, 63)
  (lstm): LSTM(118, 29, bidirectional=True)
  (linear1): Linear(in_features=464, out_features=78, bias=True)
  (linear2): Linear(in_features=78, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:37.199575
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2460.976276
	Epoch 1....
Epoch has taken 0:00:21.370147
Number of used sentences in train = 359
Total loss for epoch 1: 1117.954773
	Epoch 2....
Epoch has taken 0:00:21.358943
Number of used sentences in train = 359
Total loss for epoch 2: 932.963932
	Epoch 3....
Epoch has taken 0:00:21.364806
Number of used sentences in train = 359
Total loss for epoch 3: 820.536402
	Epoch 4....
Epoch has taken 0:00:21.361163
Number of used sentences in train = 359
Total loss for epoch 4: 757.672925
	Epoch 5....
Epoch has taken 0:00:21.364648
Number of used sentences in train = 359
Total loss for epoch 5: 721.559007
	Epoch 6....
Epoch has taken 0:00:21.366912
Number of used sentences in train = 359
Total loss for epoch 6: 703.889712
	Epoch 7....
Epoch has taken 0:00:21.374346
Number of used sentences in train = 359
Total loss for epoch 7: 688.222799
	Epoch 8....
Epoch has taken 0:00:21.370452
Number of used sentences in train = 359
Total loss for epoch 8: 680.057533
	Epoch 9....
Epoch has taken 0:00:21.380130
Number of used sentences in train = 359
Total loss for epoch 9: 677.301290
	Epoch 10....
Epoch has taken 0:00:21.372815
Number of used sentences in train = 359
Total loss for epoch 10: 675.510273
	Epoch 11....
Epoch has taken 0:00:21.347240
Number of used sentences in train = 359
Total loss for epoch 11: 674.498364
	Epoch 12....
Epoch has taken 0:00:21.354210
Number of used sentences in train = 359
Total loss for epoch 12: 673.760759
	Epoch 13....
Epoch has taken 0:00:21.340781
Number of used sentences in train = 359
Total loss for epoch 13: 673.338853
	Epoch 14....
Epoch has taken 0:00:21.347791
Number of used sentences in train = 359
Total loss for epoch 14: 672.844692
Epoch has taken 0:00:21.340239

==================================================================================================
	Training time : 0:59:44.296568
==================================================================================================
	Identification : 0.257

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 30, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 37, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 82, 'lstmDropout': 0.17, 'denseActivation': 'tanh', 'wordDim': 204, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(5850, 204)
  (lstm): LSTM(241, 82, bidirectional=True)
  (linear1): Linear(in_features=1312, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13732.821809
validation loss after epoch 0 : 1165.972859
	Epoch 1....
Epoch has taken 0:02:42.759466
Number of used sentences in train = 2811
Total loss for epoch 1: 8694.849878
validation loss after epoch 1 : 1129.216457
	Epoch 2....
Epoch has taken 0:02:45.252149
Number of used sentences in train = 2811
Total loss for epoch 2: 6923.247832
validation loss after epoch 2 : 1129.014876
	Epoch 3....
Epoch has taken 0:02:45.391412
Number of used sentences in train = 2811
Total loss for epoch 3: 5960.771222
validation loss after epoch 3 : 1222.943839
	Epoch 4....
Epoch has taken 0:02:45.490467
Number of used sentences in train = 2811
Total loss for epoch 4: 5453.992611
validation loss after epoch 4 : 1282.050359
	Epoch 5....
Epoch has taken 0:02:45.523649
Number of used sentences in train = 2811
Total loss for epoch 5: 5142.699052
validation loss after epoch 5 : 1348.508138
	Epoch 6....
Epoch has taken 0:02:45.102776
Number of used sentences in train = 2811
Total loss for epoch 6: 4954.359744
validation loss after epoch 6 : 1413.737567
	Epoch 7....
Epoch has taken 0:02:45.266500
Number of used sentences in train = 2811
Total loss for epoch 7: 4838.386818
validation loss after epoch 7 : 1455.214772
	Epoch 8....
Epoch has taken 0:02:43.069123
Number of used sentences in train = 2811
Total loss for epoch 8: 4754.350601
validation loss after epoch 8 : 1487.301560
	Epoch 9....
Epoch has taken 0:02:45.437741
Number of used sentences in train = 2811
Total loss for epoch 9: 4694.338071
validation loss after epoch 9 : 1504.839399
	Epoch 10....
Epoch has taken 0:02:45.254642
Number of used sentences in train = 2811
Total loss for epoch 10: 4644.238286
validation loss after epoch 10 : 1518.139944
	Epoch 11....
Epoch has taken 0:02:45.231337
Number of used sentences in train = 2811
Total loss for epoch 11: 4614.542161
validation loss after epoch 11 : 1556.032217
	Epoch 12....
Epoch has taken 0:02:59.614857
Number of used sentences in train = 2811
Total loss for epoch 12: 4585.821726
validation loss after epoch 12 : 1579.425261
	Epoch 13....
Epoch has taken 0:03:01.712418
Number of used sentences in train = 2811
Total loss for epoch 13: 4571.670930
validation loss after epoch 13 : 1594.400815
	Epoch 14....
Epoch has taken 0:03:01.645991
Number of used sentences in train = 2811
Total loss for epoch 14: 4560.125080
validation loss after epoch 14 : 1607.225586
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(5850, 204)
  (lstm): LSTM(241, 82, bidirectional=True)
  (linear1): Linear(in_features=1312, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:59.232587
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1605.804039
	Epoch 1....
Epoch has taken 0:00:17.540914
Number of used sentences in train = 313
Total loss for epoch 1: 793.554174
	Epoch 2....
Epoch has taken 0:00:17.531171
Number of used sentences in train = 313
Total loss for epoch 2: 615.997843
	Epoch 3....
Epoch has taken 0:00:17.541780
Number of used sentences in train = 313
Total loss for epoch 3: 555.436371
	Epoch 4....
Epoch has taken 0:00:17.546364
Number of used sentences in train = 313
Total loss for epoch 4: 541.226464
	Epoch 5....
Epoch has taken 0:00:17.540975
Number of used sentences in train = 313
Total loss for epoch 5: 526.244246
	Epoch 6....
Epoch has taken 0:00:17.534056
Number of used sentences in train = 313
Total loss for epoch 6: 520.129754
	Epoch 7....
Epoch has taken 0:00:17.543247
Number of used sentences in train = 313
Total loss for epoch 7: 516.559692
	Epoch 8....
Epoch has taken 0:00:17.538398
Number of used sentences in train = 313
Total loss for epoch 8: 514.614726
	Epoch 9....
Epoch has taken 0:00:17.561570
Number of used sentences in train = 313
Total loss for epoch 9: 511.983981
	Epoch 10....
Epoch has taken 0:00:17.543860
Number of used sentences in train = 313
Total loss for epoch 10: 509.888459
	Epoch 11....
Epoch has taken 0:00:17.541394
Number of used sentences in train = 313
Total loss for epoch 11: 509.257009
	Epoch 12....
Epoch has taken 0:00:17.547069
Number of used sentences in train = 313
Total loss for epoch 12: 508.503510
	Epoch 13....
Epoch has taken 0:00:17.549622
Number of used sentences in train = 313
Total loss for epoch 13: 506.731927
	Epoch 14....
Epoch has taken 0:00:17.537359
Number of used sentences in train = 313
Total loss for epoch 14: 506.281496
Epoch has taken 0:00:17.530855

==================================================================================================
	Training time : 0:46:39.628280
==================================================================================================
	Identification : 0.438

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 37)
  (w_embeddings): Embedding(5662, 204)
  (lstm): LSTM(241, 82, bidirectional=True)
  (linear1): Linear(in_features=1312, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
