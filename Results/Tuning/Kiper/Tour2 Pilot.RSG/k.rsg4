INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11516.642882
validation loss after epoch 0 : 870.156004
	Epoch 1....
Epoch has taken 0:02:51.115785
Number of used sentences in train = 2811
Total loss for epoch 1: 7625.484057
validation loss after epoch 1 : 836.592843
	Epoch 2....
Epoch has taken 0:02:53.496995
Number of used sentences in train = 2811
Total loss for epoch 2: 6861.480819
validation loss after epoch 2 : 799.198269
	Epoch 3....
Epoch has taken 0:02:52.295132
Number of used sentences in train = 2811
Total loss for epoch 3: 6386.207736
validation loss after epoch 3 : 810.271608
	Epoch 4....
Epoch has taken 0:02:54.286331
Number of used sentences in train = 2811
Total loss for epoch 4: 6082.850177
validation loss after epoch 4 : 805.193180
	Epoch 5....
Epoch has taken 0:02:52.400470
Number of used sentences in train = 2811
Total loss for epoch 5: 5790.993169
validation loss after epoch 5 : 830.015203
	Epoch 6....
Epoch has taken 0:02:54.304879
Number of used sentences in train = 2811
Total loss for epoch 6: 5568.581025
validation loss after epoch 6 : 858.487507
	Epoch 7....
Epoch has taken 0:02:54.238028
Number of used sentences in train = 2811
Total loss for epoch 7: 5381.460029
validation loss after epoch 7 : 885.863126
	Epoch 8....
Epoch has taken 0:02:53.005502
Number of used sentences in train = 2811
Total loss for epoch 8: 5223.274376
validation loss after epoch 8 : 925.069114
	Epoch 9....
Epoch has taken 0:02:52.247974
Number of used sentences in train = 2811
Total loss for epoch 9: 5088.302332
validation loss after epoch 9 : 884.737816
	Epoch 10....
Epoch has taken 0:02:53.371636
Number of used sentences in train = 2811
Total loss for epoch 10: 5029.031427
validation loss after epoch 10 : 918.996926
	Epoch 11....
Epoch has taken 0:02:53.479636
Number of used sentences in train = 2811
Total loss for epoch 11: 4922.858214
validation loss after epoch 11 : 941.225250
	Epoch 12....
Epoch has taken 0:02:52.647065
Number of used sentences in train = 2811
Total loss for epoch 12: 4872.880331
validation loss after epoch 12 : 962.155792
	Epoch 13....
Epoch has taken 0:02:54.200615
Number of used sentences in train = 2811
Total loss for epoch 13: 4820.930986
validation loss after epoch 13 : 977.179301
	Epoch 14....
Epoch has taken 0:02:54.113499
Number of used sentences in train = 2811
Total loss for epoch 14: 4788.646313
validation loss after epoch 14 : 987.988089
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:53.443321
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1307.187780
	Epoch 1....
Epoch has taken 0:00:18.424690
Number of used sentences in train = 313
Total loss for epoch 1: 785.149563
	Epoch 2....
Epoch has taken 0:00:18.423409
Number of used sentences in train = 313
Total loss for epoch 2: 679.559570
	Epoch 3....
Epoch has taken 0:00:18.421791
Number of used sentences in train = 313
Total loss for epoch 3: 615.347435
	Epoch 4....
Epoch has taken 0:00:18.430932
Number of used sentences in train = 313
Total loss for epoch 4: 591.094233
	Epoch 5....
Epoch has taken 0:00:18.688098
Number of used sentences in train = 313
Total loss for epoch 5: 573.812155
	Epoch 6....
Epoch has taken 0:00:18.451024
Number of used sentences in train = 313
Total loss for epoch 6: 566.726883
	Epoch 7....
Epoch has taken 0:00:18.437390
Number of used sentences in train = 313
Total loss for epoch 7: 557.023820
	Epoch 8....
Epoch has taken 0:00:18.432712
Number of used sentences in train = 313
Total loss for epoch 8: 552.637328
	Epoch 9....
Epoch has taken 0:00:18.427363
Number of used sentences in train = 313
Total loss for epoch 9: 547.721515
	Epoch 10....
Epoch has taken 0:00:18.437389
Number of used sentences in train = 313
Total loss for epoch 10: 546.569711
	Epoch 11....
Epoch has taken 0:00:18.446964
Number of used sentences in train = 313
Total loss for epoch 11: 541.432191
	Epoch 12....
Epoch has taken 0:00:18.468424
Number of used sentences in train = 313
Total loss for epoch 12: 540.064530
	Epoch 13....
Epoch has taken 0:00:18.431717
Number of used sentences in train = 313
Total loss for epoch 13: 543.703142
	Epoch 14....
Epoch has taken 0:00:18.456063
Number of used sentences in train = 313
Total loss for epoch 14: 549.610592
Epoch has taken 0:00:18.453361

==================================================================================================
	Training time : 0:48:01.936210
==================================================================================================
	Identification : 0.417

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9483.566756
validation loss after epoch 0 : 654.833909
	Epoch 1....
Epoch has taken 0:02:00.852236
Number of used sentences in train = 2074
Total loss for epoch 1: 5381.466573
validation loss after epoch 1 : 650.381577
	Epoch 2....
Epoch has taken 0:02:00.917326
Number of used sentences in train = 2074
Total loss for epoch 2: 4725.244909
validation loss after epoch 2 : 630.232702
	Epoch 3....
Epoch has taken 0:01:59.954450
Number of used sentences in train = 2074
Total loss for epoch 3: 4239.193058
validation loss after epoch 3 : 660.569420
	Epoch 4....
Epoch has taken 0:02:00.276676
Number of used sentences in train = 2074
Total loss for epoch 4: 3900.380597
validation loss after epoch 4 : 666.009353
	Epoch 5....
Epoch has taken 0:02:00.762112
Number of used sentences in train = 2074
Total loss for epoch 5: 3684.419699
validation loss after epoch 5 : 707.498701
	Epoch 6....
Epoch has taken 0:02:00.811013
Number of used sentences in train = 2074
Total loss for epoch 6: 3496.160044
validation loss after epoch 6 : 775.059514
	Epoch 7....
Epoch has taken 0:02:00.671816
Number of used sentences in train = 2074
Total loss for epoch 7: 3384.560502
validation loss after epoch 7 : 721.907809
	Epoch 8....
Epoch has taken 0:02:00.794745
Number of used sentences in train = 2074
Total loss for epoch 8: 3326.028897
validation loss after epoch 8 : 770.268828
	Epoch 9....
Epoch has taken 0:02:00.739530
Number of used sentences in train = 2074
Total loss for epoch 9: 3299.757787
validation loss after epoch 9 : 758.710927
	Epoch 10....
Epoch has taken 0:02:00.731962
Number of used sentences in train = 2074
Total loss for epoch 10: 3272.951834
validation loss after epoch 10 : 775.777453
	Epoch 11....
Epoch has taken 0:02:00.763886
Number of used sentences in train = 2074
Total loss for epoch 11: 3258.211090
validation loss after epoch 11 : 784.723701
	Epoch 12....
Epoch has taken 0:02:00.889030
Number of used sentences in train = 2074
Total loss for epoch 12: 3242.955455
validation loss after epoch 12 : 780.938734
	Epoch 13....
Epoch has taken 0:02:00.829319
Number of used sentences in train = 2074
Total loss for epoch 13: 3224.898465
validation loss after epoch 13 : 795.245364
	Epoch 14....
Epoch has taken 0:02:00.879455
Number of used sentences in train = 2074
Total loss for epoch 14: 3208.217678
validation loss after epoch 14 : 808.028841
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:00.807737
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1479.819466
	Epoch 1....
Epoch has taken 0:00:12.349037
Number of used sentences in train = 231
Total loss for epoch 1: 604.484587
	Epoch 2....
Epoch has taken 0:00:12.344478
Number of used sentences in train = 231
Total loss for epoch 2: 502.779872
	Epoch 3....
Epoch has taken 0:00:12.342505
Number of used sentences in train = 231
Total loss for epoch 3: 393.784270
	Epoch 4....
Epoch has taken 0:00:12.339536
Number of used sentences in train = 231
Total loss for epoch 4: 363.923214
	Epoch 5....
Epoch has taken 0:00:12.346830
Number of used sentences in train = 231
Total loss for epoch 5: 354.460499
	Epoch 6....
Epoch has taken 0:00:12.351828
Number of used sentences in train = 231
Total loss for epoch 6: 350.070197
	Epoch 7....
Epoch has taken 0:00:12.343317
Number of used sentences in train = 231
Total loss for epoch 7: 348.814038
	Epoch 8....
Epoch has taken 0:00:12.347946
Number of used sentences in train = 231
Total loss for epoch 8: 348.176302
	Epoch 9....
Epoch has taken 0:00:12.356129
Number of used sentences in train = 231
Total loss for epoch 9: 347.856295
	Epoch 10....
Epoch has taken 0:00:12.341857
Number of used sentences in train = 231
Total loss for epoch 10: 347.392049
	Epoch 11....
Epoch has taken 0:00:12.338395
Number of used sentences in train = 231
Total loss for epoch 11: 347.068073
	Epoch 12....
Epoch has taken 0:00:12.362539
Number of used sentences in train = 231
Total loss for epoch 12: 346.754337
	Epoch 13....
Epoch has taken 0:00:12.353003
Number of used sentences in train = 231
Total loss for epoch 13: 346.632392
	Epoch 14....
Epoch has taken 0:00:12.352004
Number of used sentences in train = 231
Total loss for epoch 14: 346.493361
Epoch has taken 0:00:12.352012

==================================================================================================
	Training time : 0:33:16.237614
==================================================================================================
	Identification : 0.383

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20941.887290
validation loss after epoch 0 : 1125.183122
	Epoch 1....
Epoch has taken 0:03:53.584566
Number of used sentences in train = 3226
Total loss for epoch 1: 9908.741483
validation loss after epoch 1 : 1077.553914
	Epoch 2....
Epoch has taken 0:03:58.917585
Number of used sentences in train = 3226
Total loss for epoch 2: 9071.885227
validation loss after epoch 2 : 1035.231926
	Epoch 3....
Epoch has taken 0:04:16.573637
Number of used sentences in train = 3226
Total loss for epoch 3: 8513.579493
validation loss after epoch 3 : 1041.584078
	Epoch 4....
Epoch has taken 0:04:16.469574
Number of used sentences in train = 3226
Total loss for epoch 4: 8093.194879
validation loss after epoch 4 : 1072.487437
	Epoch 5....
Epoch has taken 0:04:16.802078
Number of used sentences in train = 3226
Total loss for epoch 5: 7774.557379
validation loss after epoch 5 : 1102.083838
	Epoch 6....
Epoch has taken 0:04:08.822287
Number of used sentences in train = 3226
Total loss for epoch 6: 7492.110587
validation loss after epoch 6 : 1175.641186
	Epoch 7....
Epoch has taken 0:03:55.739708
Number of used sentences in train = 3226
Total loss for epoch 7: 7281.778668
validation loss after epoch 7 : 1309.567299
	Epoch 8....
Epoch has taken 0:03:54.389112
Number of used sentences in train = 3226
Total loss for epoch 8: 7122.528643
validation loss after epoch 8 : 1168.795651
	Epoch 9....
Epoch has taken 0:03:54.204416
Number of used sentences in train = 3226
Total loss for epoch 9: 6949.571895
validation loss after epoch 9 : 1278.815143
	Epoch 10....
Epoch has taken 0:03:54.305451
Number of used sentences in train = 3226
Total loss for epoch 10: 6829.921578
validation loss after epoch 10 : 1312.810788
	Epoch 11....
Epoch has taken 0:03:55.630999
Number of used sentences in train = 3226
Total loss for epoch 11: 6725.445338
validation loss after epoch 11 : 1312.692427
	Epoch 12....
Epoch has taken 0:04:05.047010
Number of used sentences in train = 3226
Total loss for epoch 12: 6641.432133
validation loss after epoch 12 : 1387.339068
	Epoch 13....
Epoch has taken 0:04:08.125096
Number of used sentences in train = 3226
Total loss for epoch 13: 6547.082190
validation loss after epoch 13 : 1371.881423
	Epoch 14....
Epoch has taken 0:04:02.022166
Number of used sentences in train = 3226
Total loss for epoch 14: 6515.589683
validation loss after epoch 14 : 1471.132691
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:54.123511
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1680.748902
	Epoch 1....
Epoch has taken 0:00:23.313095
Number of used sentences in train = 359
Total loss for epoch 1: 984.566505
	Epoch 2....
Epoch has taken 0:00:25.267952
Number of used sentences in train = 359
Total loss for epoch 2: 885.077110
	Epoch 3....
Epoch has taken 0:00:25.268500
Number of used sentences in train = 359
Total loss for epoch 3: 808.232161
	Epoch 4....
Epoch has taken 0:00:25.238998
Number of used sentences in train = 359
Total loss for epoch 4: 763.151025
	Epoch 5....
Epoch has taken 0:00:25.264797
Number of used sentences in train = 359
Total loss for epoch 5: 734.891357
	Epoch 6....
Epoch has taken 0:00:25.250781
Number of used sentences in train = 359
Total loss for epoch 6: 700.224436
	Epoch 7....
Epoch has taken 0:00:25.251128
Number of used sentences in train = 359
Total loss for epoch 7: 692.240186
	Epoch 8....
Epoch has taken 0:00:25.257609
Number of used sentences in train = 359
Total loss for epoch 8: 692.858652
	Epoch 9....
Epoch has taken 0:00:25.250610
Number of used sentences in train = 359
Total loss for epoch 9: 679.053235
	Epoch 10....
Epoch has taken 0:00:25.271116
Number of used sentences in train = 359
Total loss for epoch 10: 673.427022
	Epoch 11....
Epoch has taken 0:00:25.254932
Number of used sentences in train = 359
Total loss for epoch 11: 677.106072
	Epoch 12....
Epoch has taken 0:00:25.258472
Number of used sentences in train = 359
Total loss for epoch 12: 675.441532
	Epoch 13....
Epoch has taken 0:00:25.267610
Number of used sentences in train = 359
Total loss for epoch 13: 672.560776
	Epoch 14....
Epoch has taken 0:00:25.259602
Number of used sentences in train = 359
Total loss for epoch 14: 672.467520
Epoch has taken 0:00:25.253230

==================================================================================================
	Training time : 1:06:52.354003
==================================================================================================
	Identification : 0.234

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 50, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 48, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 62, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 85, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(9265, 85)
  (lstm): LSTM(133, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12955.402491
validation loss after epoch 0 : 1125.345218
	Epoch 1....
Epoch has taken 0:03:00.667651
Number of used sentences in train = 2811
Total loss for epoch 1: 8886.226555
validation loss after epoch 1 : 1079.608241
	Epoch 2....
Epoch has taken 0:02:54.321685
Number of used sentences in train = 2811
Total loss for epoch 2: 7128.306231
validation loss after epoch 2 : 1124.430416
	Epoch 3....
Epoch has taken 0:02:55.288885
Number of used sentences in train = 2811
Total loss for epoch 3: 6016.547939
validation loss after epoch 3 : 1185.732170
	Epoch 4....
Epoch has taken 0:03:00.430773
Number of used sentences in train = 2811
Total loss for epoch 4: 5419.988597
validation loss after epoch 4 : 1281.545597
	Epoch 5....
Epoch has taken 0:02:56.905481
Number of used sentences in train = 2811
Total loss for epoch 5: 5092.578861
validation loss after epoch 5 : 1354.062144
	Epoch 6....
Epoch has taken 0:02:45.080932
Number of used sentences in train = 2811
Total loss for epoch 6: 4868.821751
validation loss after epoch 6 : 1418.109939
	Epoch 7....
Epoch has taken 0:02:46.845880
Number of used sentences in train = 2811
Total loss for epoch 7: 4747.873086
validation loss after epoch 7 : 1485.229585
	Epoch 8....
Epoch has taken 0:02:45.177156
Number of used sentences in train = 2811
Total loss for epoch 8: 4663.864506
validation loss after epoch 8 : 1506.840439
	Epoch 9....
Epoch has taken 0:02:58.615730
Number of used sentences in train = 2811
Total loss for epoch 9: 4603.720834
validation loss after epoch 9 : 1545.429708
	Epoch 10....
Epoch has taken 0:03:01.045879
Number of used sentences in train = 2811
Total loss for epoch 10: 4576.111537
validation loss after epoch 10 : 1599.690309
	Epoch 11....
Epoch has taken 0:03:01.158629
Number of used sentences in train = 2811
Total loss for epoch 11: 4566.000586
validation loss after epoch 11 : 1614.974523
	Epoch 12....
Epoch has taken 0:02:54.418723
Number of used sentences in train = 2811
Total loss for epoch 12: 4550.583139
validation loss after epoch 12 : 1655.829955
	Epoch 13....
Epoch has taken 0:02:57.806609
Number of used sentences in train = 2811
Total loss for epoch 13: 4539.454581
validation loss after epoch 13 : 1679.471873
	Epoch 14....
Epoch has taken 0:02:45.189766
Number of used sentences in train = 2811
Total loss for epoch 14: 4533.323105
validation loss after epoch 14 : 1703.113627
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(9265, 85)
  (lstm): LSTM(133, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.952005
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2841.149843
	Epoch 1....
Epoch has taken 0:00:17.464064
Number of used sentences in train = 313
Total loss for epoch 1: 761.087578
	Epoch 2....
Epoch has taken 0:00:17.461939
Number of used sentences in train = 313
Total loss for epoch 2: 596.683298
	Epoch 3....
Epoch has taken 0:00:17.683452
Number of used sentences in train = 313
Total loss for epoch 3: 550.461414
	Epoch 4....
Epoch has taken 0:00:17.453266
Number of used sentences in train = 313
Total loss for epoch 4: 533.314067
	Epoch 5....
Epoch has taken 0:00:17.455216
Number of used sentences in train = 313
Total loss for epoch 5: 527.024167
	Epoch 6....
Epoch has taken 0:00:17.458288
Number of used sentences in train = 313
Total loss for epoch 6: 521.806370
	Epoch 7....
Epoch has taken 0:00:17.454480
Number of used sentences in train = 313
Total loss for epoch 7: 518.311842
	Epoch 8....
Epoch has taken 0:00:17.460926
Number of used sentences in train = 313
Total loss for epoch 8: 515.552777
	Epoch 9....
Epoch has taken 0:00:17.096024
Number of used sentences in train = 313
Total loss for epoch 9: 515.000823
	Epoch 10....
Epoch has taken 0:00:16.919910
Number of used sentences in train = 313
Total loss for epoch 10: 513.367702
	Epoch 11....
Epoch has taken 0:00:16.904869
Number of used sentences in train = 313
Total loss for epoch 11: 512.551338
	Epoch 12....
Epoch has taken 0:00:16.916592
Number of used sentences in train = 313
Total loss for epoch 12: 511.603767
	Epoch 13....
Epoch has taken 0:00:16.906268
Number of used sentences in train = 313
Total loss for epoch 13: 510.973666
	Epoch 14....
Epoch has taken 0:00:16.896484
Number of used sentences in train = 313
Total loss for epoch 14: 509.735367
Epoch has taken 0:00:16.908481

==================================================================================================
	Training time : 0:47:58.854595
==================================================================================================
	Identification : 0.459

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(7068, 85)
  (lstm): LSTM(133, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10150.345147
validation loss after epoch 0 : 932.989731
	Epoch 1....
Epoch has taken 0:01:51.813262
Number of used sentences in train = 2074
Total loss for epoch 1: 6580.734204
validation loss after epoch 1 : 981.599038
	Epoch 2....
Epoch has taken 0:01:52.023638
Number of used sentences in train = 2074
Total loss for epoch 2: 5087.109822
validation loss after epoch 2 : 958.574742
	Epoch 3....
Epoch has taken 0:01:51.114805
Number of used sentences in train = 2074
Total loss for epoch 3: 4130.277662
validation loss after epoch 3 : 1024.114569
	Epoch 4....
Epoch has taken 0:01:53.140945
Number of used sentences in train = 2074
Total loss for epoch 4: 3682.448981
validation loss after epoch 4 : 1134.275713
	Epoch 5....
Epoch has taken 0:01:54.808372
Number of used sentences in train = 2074
Total loss for epoch 5: 3436.660609
validation loss after epoch 5 : 1305.875665
	Epoch 6....
Epoch has taken 0:02:04.157711
Number of used sentences in train = 2074
Total loss for epoch 6: 3292.341117
validation loss after epoch 6 : 1297.412065
	Epoch 7....
Epoch has taken 0:02:03.607146
Number of used sentences in train = 2074
Total loss for epoch 7: 3245.333231
validation loss after epoch 7 : 1341.363979
	Epoch 8....
Epoch has taken 0:02:04.112028
Number of used sentences in train = 2074
Total loss for epoch 8: 3217.891018
validation loss after epoch 8 : 1360.995068
	Epoch 9....
Epoch has taken 0:02:03.429466
Number of used sentences in train = 2074
Total loss for epoch 9: 3195.388054
validation loss after epoch 9 : 1385.044667
	Epoch 10....
Epoch has taken 0:02:02.814003
Number of used sentences in train = 2074
Total loss for epoch 10: 3183.947406
validation loss after epoch 10 : 1409.791809
	Epoch 11....
Epoch has taken 0:02:02.424278
Number of used sentences in train = 2074
Total loss for epoch 11: 3177.738101
validation loss after epoch 11 : 1425.299103
	Epoch 12....
Epoch has taken 0:01:52.966405
Number of used sentences in train = 2074
Total loss for epoch 12: 3172.954009
validation loss after epoch 12 : 1447.979748
	Epoch 13....
Epoch has taken 0:01:52.959469
Number of used sentences in train = 2074
Total loss for epoch 13: 3170.341640
validation loss after epoch 13 : 1460.818575
	Epoch 14....
Epoch has taken 0:01:52.887773
Number of used sentences in train = 2074
Total loss for epoch 14: 3168.681764
validation loss after epoch 14 : 1474.300842
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(7068, 85)
  (lstm): LSTM(133, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:52.995441
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1566.834753
	Epoch 1....
Epoch has taken 0:00:11.562987
Number of used sentences in train = 231
Total loss for epoch 1: 582.240968
	Epoch 2....
Epoch has taken 0:00:11.549862
Number of used sentences in train = 231
Total loss for epoch 2: 421.561726
	Epoch 3....
Epoch has taken 0:00:11.554665
Number of used sentences in train = 231
Total loss for epoch 3: 384.017666
	Epoch 4....
Epoch has taken 0:00:11.556163
Number of used sentences in train = 231
Total loss for epoch 4: 363.618667
	Epoch 5....
Epoch has taken 0:00:11.567935
Number of used sentences in train = 231
Total loss for epoch 5: 356.909661
	Epoch 6....
Epoch has taken 0:00:11.569131
Number of used sentences in train = 231
Total loss for epoch 6: 351.199552
	Epoch 7....
Epoch has taken 0:00:11.565797
Number of used sentences in train = 231
Total loss for epoch 7: 349.020706
	Epoch 8....
Epoch has taken 0:00:11.569870
Number of used sentences in train = 231
Total loss for epoch 8: 347.846061
	Epoch 9....
Epoch has taken 0:00:11.562379
Number of used sentences in train = 231
Total loss for epoch 9: 347.248335
	Epoch 10....
Epoch has taken 0:00:11.577341
Number of used sentences in train = 231
Total loss for epoch 10: 346.815582
	Epoch 11....
Epoch has taken 0:00:11.584063
Number of used sentences in train = 231
Total loss for epoch 11: 346.448769
	Epoch 12....
Epoch has taken 0:00:11.569220
Number of used sentences in train = 231
Total loss for epoch 12: 346.210264
	Epoch 13....
Epoch has taken 0:00:11.564287
Number of used sentences in train = 231
Total loss for epoch 13: 346.019506
	Epoch 14....
Epoch has taken 0:00:11.588541
Number of used sentences in train = 231
Total loss for epoch 14: 345.854769
Epoch has taken 0:00:11.587727

==================================================================================================
	Training time : 0:32:09.129518
==================================================================================================
	Identification : 0.207

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 48)
  (w_embeddings): Embedding(17997, 85)
  (lstm): LSTM(133, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17701.321012
validation loss after epoch 0 : 1614.578080
	Epoch 1....
Epoch has taken 0:03:38.598763
Number of used sentences in train = 3226
Total loss for epoch 1: 12475.882304
validation loss after epoch 1 : 1520.913392
	Epoch 2....
Epoch has taken 0:03:37.550855
Number of used sentences in train = 3226
Total loss for epoch 2: 9945.197893
validation loss after epoch 2 : 1596.799616
	Epoch 3....
Epoch has taken 0:03:39.755871
Number of used sentences in train = 3226
Total loss for epoch 3: 8353.643571
validation loss after epoch 3 : 1842.694353
	Epoch 4....
Epoch has taken 0:03:39.514452
Number of used sentences in train = 3226
Total loss for epoch 4: 7411.205312
validation loss after epoch 4 : 1816.670824
	Epoch 5....
Epoch has taken 0:03:38.215788
Number of used sentences in train = 3226
Total loss for epoch 5: 6853.981824
validation loss after epoch 5 : 2045.939601
	Epoch 6....
Epoch has taken 0:03:38.484851
Number of used sentences in train = 3226
Total loss for epoch 6: 6548.272568
validation loss after epoch 6 : 2221.260685
	Epoch 7....
Epoch has taken 0:03:39.778977
Number of used sentences in train = 3226
Total loss for epoch 7: 6392.815369
validation loss after epoch 7 : 2356.185094
	Epoch 8....
Epoch has taken 0:03:39.075441
Number of used sentences in train = 3226
Total loss for epoch 8: 6299.321204
validation loss after epoch 8 : 2354.217545
	Epoch 9....
Epoch has taken 0:03:40.001200
Number of used sentences in train = 3226
Total loss for epoch 9: 6251.895446
validation loss after epoch 9 : 2399.476985
	Epoch 10....
Epoch has taken 0:03:42.690593
Number of used sentences in train = 3226
Total loss for epoch 10: 6217.729492
validation loss after epoch 10 : 2438.737958
	Epoch 11....
Epoch has taken 0:04:01.788202
Number of used sentences in train = 3226
Total loss for epoch 11: 6192.106605
validation loss after epoch 11 : 2462.119293
	Epoch 12....
Epoch has taken 0:03:41.908123
Number of used sentences in train = 3226
Total loss for epoch 12: 6176.314566
validation loss after epoch 12 : 2571.814065
	Epoch 13....
Epoch has taken 0:03:59.954960
Number of used sentences in train = 3226
Total loss for epoch 13: 6169.226495
validation loss after epoch 13 : 2522.819822
	Epoch 14....
Epoch has taken 0:04:01.667843
Number of used sentences in train = 3226
Total loss for epoch 14: 6160.794646
validation loss after epoch 14 : 2583.038702
	TransitionClassifier(
  (p_embeddings): Embedding(13, 48)
  (w_embeddings): Embedding(17997, 85)
  (lstm): LSTM(133, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=50, bias=True)
  (linear2): Linear(in_features=50, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:01.735637
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2414.713398
	Epoch 1....
Epoch has taken 0:00:23.829600
Number of used sentences in train = 359
Total loss for epoch 1: 1209.315506
	Epoch 2....
Epoch has taken 0:00:23.817657
Number of used sentences in train = 359
Total loss for epoch 2: 880.408911
	Epoch 3....
Epoch has taken 0:00:23.837741
Number of used sentences in train = 359
Total loss for epoch 3: 752.720376
	Epoch 4....
Epoch has taken 0:00:23.663036
Number of used sentences in train = 359
Total loss for epoch 4: 707.829646
	Epoch 5....
Epoch has taken 0:00:21.588715
Number of used sentences in train = 359
Total loss for epoch 5: 692.680337
	Epoch 6....
Epoch has taken 0:00:21.580727
Number of used sentences in train = 359
Total loss for epoch 6: 676.935196
	Epoch 7....
Epoch has taken 0:00:21.581668
Number of used sentences in train = 359
Total loss for epoch 7: 674.263432
	Epoch 8....
Epoch has taken 0:00:21.566906
Number of used sentences in train = 359
Total loss for epoch 8: 673.266737
	Epoch 9....
Epoch has taken 0:00:21.570760
Number of used sentences in train = 359
Total loss for epoch 9: 672.661195
	Epoch 10....
Epoch has taken 0:00:21.566790
Number of used sentences in train = 359
Total loss for epoch 10: 672.246319
	Epoch 11....
Epoch has taken 0:00:21.596521
Number of used sentences in train = 359
Total loss for epoch 11: 671.928392
	Epoch 12....
Epoch has taken 0:00:21.574183
Number of used sentences in train = 359
Total loss for epoch 12: 671.669819
	Epoch 13....
Epoch has taken 0:00:21.571246
Number of used sentences in train = 359
Total loss for epoch 13: 671.471982
	Epoch 14....
Epoch has taken 0:00:21.580690
Number of used sentences in train = 359
Total loss for epoch 14: 671.306868
Epoch has taken 0:00:21.588895

==================================================================================================
	Training time : 1:01:53.939562
==================================================================================================
	Identification : 0.384

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 24, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 60, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 148, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 51, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 60)
  (w_embeddings): Embedding(1882, 51)
  (lstm): LSTM(111, 148, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2368, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 27025.600262
validation loss after epoch 0 : 2739.094740
	Epoch 1....
Epoch has taken 0:02:58.678705
Number of used sentences in train = 2811
Total loss for epoch 1: 17734.461725
validation loss after epoch 1 : 1079.811471
	Epoch 2....
Epoch has taken 0:02:56.678355
Number of used sentences in train = 2811
Total loss for epoch 2: 10211.590594
validation loss after epoch 2 : 928.935452
	Epoch 3....
Epoch has taken 0:02:56.866870
Number of used sentences in train = 2811
Total loss for epoch 3: 8789.065588
validation loss after epoch 3 : 893.068114
	Epoch 4....
Epoch has taken 0:02:56.639039
Number of used sentences in train = 2811
Total loss for epoch 4: 7956.631462
validation loss after epoch 4 : 951.025706
	Epoch 5....
Epoch has taken 0:02:57.583014
Number of used sentences in train = 2811
Total loss for epoch 5: 7624.852289
validation loss after epoch 5 : 842.516935
	Epoch 6....
Epoch has taken 0:02:58.294960
Number of used sentences in train = 2811
Total loss for epoch 6: 7243.453732
validation loss after epoch 6 : 837.838016
	Epoch 7....
Epoch has taken 0:02:58.620553
Number of used sentences in train = 2811
Total loss for epoch 7: 7034.262553
validation loss after epoch 7 : 881.546504
	Epoch 8....
Epoch has taken 0:02:59.677636
Number of used sentences in train = 2811
Total loss for epoch 8: 6739.468308
validation loss after epoch 8 : 874.469323
	Epoch 9....
Epoch has taken 0:02:58.733371
Number of used sentences in train = 2811
Total loss for epoch 9: 6552.095993
validation loss after epoch 9 : 840.282054
	Epoch 10....
Epoch has taken 0:03:08.582975
Number of used sentences in train = 2811
Total loss for epoch 10: 6336.859761
validation loss after epoch 10 : 870.441342
	Epoch 11....
Epoch has taken 0:02:58.319705
Number of used sentences in train = 2811
Total loss for epoch 11: 6067.893674
validation loss after epoch 11 : 889.350821
	Epoch 12....
Epoch has taken 0:02:58.299295
Number of used sentences in train = 2811
Total loss for epoch 12: 5926.981412
validation loss after epoch 12 : 891.457710
	Epoch 13....
Epoch has taken 0:02:57.955495
Number of used sentences in train = 2811
Total loss for epoch 13: 5758.110620
validation loss after epoch 13 : 893.209069
	Epoch 14....
Epoch has taken 0:02:59.345435
Number of used sentences in train = 2811
Total loss for epoch 14: 5675.180262
validation loss after epoch 14 : 857.287970
	TransitionClassifier(
  (p_embeddings): Embedding(18, 60)
  (w_embeddings): Embedding(1882, 51)
  (lstm): LSTM(111, 148, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2368, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:58.187670
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2817.869565
	Epoch 1....
Epoch has taken 0:00:18.893301
Number of used sentences in train = 313
Total loss for epoch 1: 1344.559481
	Epoch 2....
Epoch has taken 0:00:18.893490
Number of used sentences in train = 313
Total loss for epoch 2: 1021.341652
	Epoch 3....
Epoch has taken 0:00:18.896517
Number of used sentences in train = 313
Total loss for epoch 3: 943.832957
	Epoch 4....
Epoch has taken 0:00:18.887545
Number of used sentences in train = 313
Total loss for epoch 4: 837.158096
	Epoch 5....
Epoch has taken 0:00:19.026131
Number of used sentences in train = 313
Total loss for epoch 5: 785.778950
	Epoch 6....
Epoch has taken 0:00:20.148464
Number of used sentences in train = 313
Total loss for epoch 6: 734.000361
	Epoch 7....
Epoch has taken 0:00:21.073952
Number of used sentences in train = 313
Total loss for epoch 7: 770.052004
	Epoch 8....
Epoch has taken 0:00:18.897368
Number of used sentences in train = 313
Total loss for epoch 8: 809.943149
	Epoch 9....
Epoch has taken 0:00:18.897767
Number of used sentences in train = 313
Total loss for epoch 9: 688.354885
	Epoch 10....
Epoch has taken 0:00:18.908818
Number of used sentences in train = 313
Total loss for epoch 10: 652.949880
	Epoch 11....
Epoch has taken 0:00:18.917429
Number of used sentences in train = 313
Total loss for epoch 11: 614.146691
	Epoch 12....
Epoch has taken 0:00:18.913627
Number of used sentences in train = 313
Total loss for epoch 12: 589.417859
	Epoch 13....
Epoch has taken 0:00:18.934260
Number of used sentences in train = 313
Total loss for epoch 13: 586.795708
	Epoch 14....
Epoch has taken 0:00:18.925229
Number of used sentences in train = 313
Total loss for epoch 14: 573.274604
Epoch has taken 0:00:18.925129

==================================================================================================
	Training time : 0:49:30.114636
==================================================================================================
	Identification : 0.489

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 60)
  (w_embeddings): Embedding(1680, 51)
  (lstm): LSTM(111, 148, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2368, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 22026.696589
validation loss after epoch 0 : 2375.365100
	Epoch 1....
Epoch has taken 0:02:02.132873
Number of used sentences in train = 2074
Total loss for epoch 1: 16401.766662
validation loss after epoch 1 : 1644.024240
	Epoch 2....
Epoch has taken 0:02:02.145114
Number of used sentences in train = 2074
Total loss for epoch 2: 11976.263209
validation loss after epoch 2 : 1203.443699
	Epoch 3....
Epoch has taken 0:02:02.147371
Number of used sentences in train = 2074
Total loss for epoch 3: 8000.532239
validation loss after epoch 3 : 853.982073
	Epoch 4....
Epoch has taken 0:02:02.188369
Number of used sentences in train = 2074
Total loss for epoch 4: 6320.679080
validation loss after epoch 4 : 772.116122
	Epoch 5....
Epoch has taken 0:02:02.591610
Number of used sentences in train = 2074
Total loss for epoch 5: 5506.121745
validation loss after epoch 5 : 687.515040
	Epoch 6....
Epoch has taken 0:02:06.329172
Number of used sentences in train = 2074
Total loss for epoch 6: 5029.036381
validation loss after epoch 6 : 699.776175
	Epoch 7....
Epoch has taken 0:02:13.596386
Number of used sentences in train = 2074
Total loss for epoch 7: 4659.372225
validation loss after epoch 7 : 711.995713
	Epoch 8....
Epoch has taken 0:02:13.692802
Number of used sentences in train = 2074
Total loss for epoch 8: 4421.871836
validation loss after epoch 8 : 709.999878
	Epoch 9....
Epoch has taken 0:02:10.395655
Number of used sentences in train = 2074
Total loss for epoch 9: 4223.453680
validation loss after epoch 9 : 728.730701
	Epoch 10....
Epoch has taken 0:02:02.432636
Number of used sentences in train = 2074
Total loss for epoch 10: 4047.424362
validation loss after epoch 10 : 713.706270
	Epoch 11....
Epoch has taken 0:02:02.217883
Number of used sentences in train = 2074
Total loss for epoch 11: 3844.468317
validation loss after epoch 11 : 748.800430
	Epoch 12....
Epoch has taken 0:02:08.373704
Number of used sentences in train = 2074
Total loss for epoch 12: 3773.128440
validation loss after epoch 12 : 836.168938
	Epoch 13....
Epoch has taken 0:02:02.282829
Number of used sentences in train = 2074
Total loss for epoch 13: 3679.336399
validation loss after epoch 13 : 755.470676
	Epoch 14....
Epoch has taken 0:02:02.185943
Number of used sentences in train = 2074
Total loss for epoch 14: 3625.980119
validation loss after epoch 14 : 798.810843
	TransitionClassifier(
  (p_embeddings): Embedding(18, 60)
  (w_embeddings): Embedding(1680, 51)
  (lstm): LSTM(111, 148, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2368, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:08.614743
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1574.029396
	Epoch 1....
Epoch has taken 0:00:12.524104
Number of used sentences in train = 231
Total loss for epoch 1: 1107.772836
	Epoch 2....
Epoch has taken 0:00:12.515007
Number of used sentences in train = 231
Total loss for epoch 2: 871.192624
	Epoch 3....
Epoch has taken 0:00:12.505859
Number of used sentences in train = 231
Total loss for epoch 3: 661.820796
	Epoch 4....
Epoch has taken 0:00:12.503257
Number of used sentences in train = 231
Total loss for epoch 4: 584.876285
	Epoch 5....
Epoch has taken 0:00:12.510977
Number of used sentences in train = 231
Total loss for epoch 5: 536.277542
	Epoch 6....
Epoch has taken 0:00:12.507342
Number of used sentences in train = 231
Total loss for epoch 6: 452.518602
	Epoch 7....
Epoch has taken 0:00:12.509233
Number of used sentences in train = 231
Total loss for epoch 7: 422.584073
	Epoch 8....
Epoch has taken 0:00:12.509488
Number of used sentences in train = 231
Total loss for epoch 8: 401.753054
	Epoch 9....
Epoch has taken 0:00:12.518287
Number of used sentences in train = 231
Total loss for epoch 9: 376.915122
	Epoch 10....
Epoch has taken 0:00:12.502659
Number of used sentences in train = 231
Total loss for epoch 10: 371.699111
	Epoch 11....
Epoch has taken 0:00:12.513324
Number of used sentences in train = 231
Total loss for epoch 11: 361.829779
	Epoch 12....
Epoch has taken 0:00:12.508672
Number of used sentences in train = 231
Total loss for epoch 12: 363.551990
	Epoch 13....
Epoch has taken 0:00:12.524488
Number of used sentences in train = 231
Total loss for epoch 13: 358.907773
	Epoch 14....
Epoch has taken 0:00:12.503512
Number of used sentences in train = 231
Total loss for epoch 14: 364.886287
Epoch has taken 0:00:12.519509

==================================================================================================
	Training time : 0:34:29.349879
==================================================================================================
	Identification : 0.149

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 60)
  (w_embeddings): Embedding(3369, 51)
  (lstm): LSTM(111, 148, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2368, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 54053.436021
validation loss after epoch 0 : 5587.359984
	Epoch 1....
Epoch has taken 0:03:59.233671
Number of used sentences in train = 3226
Total loss for epoch 1: 52117.841665
validation loss after epoch 1 : 5610.945207
	Epoch 2....
Epoch has taken 0:04:04.551601
Number of used sentences in train = 3226
Total loss for epoch 2: 51947.490638
validation loss after epoch 2 : 5553.193590
	Epoch 3....
Epoch has taken 0:04:18.279938
Number of used sentences in train = 3226
Total loss for epoch 3: 48375.388937
validation loss after epoch 3 : 4158.746522
	Epoch 4....
Epoch has taken 0:04:18.478359
Number of used sentences in train = 3226
Total loss for epoch 4: 38578.035918
validation loss after epoch 4 : 4134.616444
	Epoch 5....
Epoch has taken 0:04:18.643318
Number of used sentences in train = 3226
Total loss for epoch 5: 37883.903279
validation loss after epoch 5 : 4032.011666
	Epoch 6....
Epoch has taken 0:04:08.010669
Number of used sentences in train = 3226
Total loss for epoch 6: 37454.285618
validation loss after epoch 6 : 4020.094039
	Epoch 7....
Epoch has taken 0:03:54.867262
Number of used sentences in train = 3226
Total loss for epoch 7: 37314.737470
validation loss after epoch 7 : 4002.858485
	Epoch 8....
Epoch has taken 0:03:56.109863
Number of used sentences in train = 3226
Total loss for epoch 8: 37256.231237
validation loss after epoch 8 : 3990.732057
	Epoch 9....
Epoch has taken 0:03:55.933089
Number of used sentences in train = 3226
Total loss for epoch 9: 37190.331996
validation loss after epoch 9 : 3989.241790
	Epoch 10....
Epoch has taken 0:03:55.815143
Number of used sentences in train = 3226
Total loss for epoch 10: 37167.494008
validation loss after epoch 10 : 3997.500104
	Epoch 11....
Epoch has taken 0:03:55.482228
Number of used sentences in train = 3226
Total loss for epoch 11: 37138.231721
validation loss after epoch 11 : 3990.775699
	Epoch 12....
Epoch has taken 0:03:55.953309
Number of used sentences in train = 3226
Total loss for epoch 12: 37112.638604
validation loss after epoch 12 : 3989.403115
	Epoch 13....
Epoch has taken 0:03:56.037752
Number of used sentences in train = 3226
Total loss for epoch 13: 37093.906530
validation loss after epoch 13 : 3978.175710
	Epoch 14....
Epoch has taken 0:03:56.000070
Number of used sentences in train = 3226
Total loss for epoch 14: 37089.407526
validation loss after epoch 14 : 3979.998977
	TransitionClassifier(
  (p_embeddings): Embedding(13, 60)
  (w_embeddings): Embedding(3369, 51)
  (lstm): LSTM(111, 148, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=2368, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:56.119006
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 4902.434529
	Epoch 1....
Epoch has taken 0:00:23.206397
Number of used sentences in train = 359
Total loss for epoch 1: 4146.279411
	Epoch 2....
Epoch has taken 0:00:23.220739
Number of used sentences in train = 359
Total loss for epoch 2: 4037.566875
	Epoch 3....
Epoch has taken 0:00:23.641392
Number of used sentences in train = 359
Total loss for epoch 3: 4159.851733
	Epoch 4....
Epoch has taken 0:00:25.531583
Number of used sentences in train = 359
Total loss for epoch 4: 4019.307481
	Epoch 5....
Epoch has taken 0:00:24.406066
Number of used sentences in train = 359
Total loss for epoch 5: 3913.555755
	Epoch 6....
Epoch has taken 0:00:23.356180
Number of used sentences in train = 359
Total loss for epoch 6: 2303.070345
	Epoch 7....
Epoch has taken 0:00:25.264035
Number of used sentences in train = 359
Total loss for epoch 7: 1745.749427
	Epoch 8....
Epoch has taken 0:00:25.537178
Number of used sentences in train = 359
Total loss for epoch 8: 1417.116485
	Epoch 9....
Epoch has taken 0:00:25.533968
Number of used sentences in train = 359
Total loss for epoch 9: 1241.341952
	Epoch 10....
Epoch has taken 0:00:25.531377
Number of used sentences in train = 359
Total loss for epoch 10: 1225.378451
	Epoch 11....
Epoch has taken 0:00:25.531134
Number of used sentences in train = 359
Total loss for epoch 11: 1107.732612
	Epoch 12....
Epoch has taken 0:00:25.523646
Number of used sentences in train = 359
Total loss for epoch 12: 1037.218591
	Epoch 13....
Epoch has taken 0:00:25.514554
Number of used sentences in train = 359
Total loss for epoch 13: 997.318909
	Epoch 14....
Epoch has taken 0:00:25.540786
Number of used sentences in train = 359
Total loss for epoch 14: 962.099724
Epoch has taken 0:00:25.526242

==================================================================================================
	Training time : 1:06:43.059735
==================================================================================================
	Identification : 0.056

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 108, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 36, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 28, 'lstmDropout': 0.22, 'denseActivation': 'tanh', 'wordDim': 73, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 36)
  (w_embeddings): Embedding(5926, 73)
  (lstm): LSTM(109, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=108, bias=True)
  (linear2): Linear(in_features=108, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13109.591447
validation loss after epoch 0 : 1142.570124
	Epoch 1....
Epoch has taken 0:03:00.935003
Number of used sentences in train = 2811
Total loss for epoch 1: 8441.565570
validation loss after epoch 1 : 1104.543490
	Epoch 2....
Epoch has taken 0:02:58.915267
Number of used sentences in train = 2811
Total loss for epoch 2: 6899.522010
validation loss after epoch 2 : 1208.408873
	Epoch 3....
Epoch has taken 0:02:59.455141
Number of used sentences in train = 2811
Total loss for epoch 3: 6018.564557
validation loss after epoch 3 : 1239.899864
	Epoch 4....
Epoch has taken 0:03:01.172239
Number of used sentences in train = 2811
Total loss for epoch 4: 5457.022088
validation loss after epoch 4 : 1389.798048
	Epoch 5....
Epoch has taken 0:03:00.938634
Number of used sentences in train = 2811
Total loss for epoch 5: 5094.126801
validation loss after epoch 5 : 1444.965249
	Epoch 6....
Epoch has taken 0:02:59.718591
Number of used sentences in train = 2811
Total loss for epoch 6: 4863.596401
validation loss after epoch 6 : 1547.698509
	Epoch 7....
Epoch has taken 0:02:59.735160
Number of used sentences in train = 2811
Total loss for epoch 7: 4731.545056
validation loss after epoch 7 : 1606.017197
	Epoch 8....
Epoch has taken 0:02:51.394218
Number of used sentences in train = 2811
Total loss for epoch 8: 4634.108708
validation loss after epoch 8 : 1702.112435
	Epoch 9....
Epoch has taken 0:02:48.912776
Number of used sentences in train = 2811
Total loss for epoch 9: 4583.444285
validation loss after epoch 9 : 1779.756847
	Epoch 10....
Epoch has taken 0:02:54.895877
Number of used sentences in train = 2811
Total loss for epoch 10: 4544.750732
validation loss after epoch 10 : 1806.541814
	Epoch 11....
Epoch has taken 0:02:46.319920
Number of used sentences in train = 2811
Total loss for epoch 11: 4526.188810
validation loss after epoch 11 : 1852.009840
	Epoch 12....
Epoch has taken 0:02:44.438934
Number of used sentences in train = 2811
Total loss for epoch 12: 4509.555584
validation loss after epoch 12 : 1901.087480
	Epoch 13....
Epoch has taken 0:02:44.995986
Number of used sentences in train = 2811
Total loss for epoch 13: 4497.562574
validation loss after epoch 13 : 1918.420363
	Epoch 14....
Epoch has taken 0:02:45.038248
Number of used sentences in train = 2811
Total loss for epoch 14: 4492.957360
validation loss after epoch 14 : 1959.234089
	TransitionClassifier(
  (p_embeddings): Embedding(18, 36)
  (w_embeddings): Embedding(5926, 73)
  (lstm): LSTM(109, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=108, bias=True)
  (linear2): Linear(in_features=108, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:43.108897
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1893.264551
	Epoch 1....
Epoch has taken 0:00:17.441084
Number of used sentences in train = 313
Total loss for epoch 1: 829.520126
	Epoch 2....
Epoch has taken 0:00:17.422525
Number of used sentences in train = 313
Total loss for epoch 2: 629.709237
	Epoch 3....
Epoch has taken 0:00:17.419600
Number of used sentences in train = 313
Total loss for epoch 3: 551.967518
	Epoch 4....
Epoch has taken 0:00:17.422041
Number of used sentences in train = 313
Total loss for epoch 4: 527.900458
	Epoch 5....
Epoch has taken 0:00:17.424546
Number of used sentences in train = 313
Total loss for epoch 5: 515.310713
	Epoch 6....
Epoch has taken 0:00:17.433583
Number of used sentences in train = 313
Total loss for epoch 6: 509.013137
	Epoch 7....
Epoch has taken 0:00:17.437929
Number of used sentences in train = 313
Total loss for epoch 7: 506.281090
	Epoch 8....
Epoch has taken 0:00:17.425414
Number of used sentences in train = 313
Total loss for epoch 8: 504.726315
	Epoch 9....
Epoch has taken 0:00:17.421765
Number of used sentences in train = 313
Total loss for epoch 9: 504.239291
	Epoch 10....
Epoch has taken 0:00:17.422197
Number of used sentences in train = 313
Total loss for epoch 10: 502.476310
	Epoch 11....
Epoch has taken 0:00:17.451990
Number of used sentences in train = 313
Total loss for epoch 11: 501.590798
	Epoch 12....
Epoch has taken 0:00:17.446960
Number of used sentences in train = 313
Total loss for epoch 12: 501.214126
	Epoch 13....
Epoch has taken 0:00:17.170021
Number of used sentences in train = 313
Total loss for epoch 13: 500.963458
	Epoch 14....
Epoch has taken 0:00:17.426431
Number of used sentences in train = 313
Total loss for epoch 14: 500.775451
Epoch has taken 0:00:17.423426

==================================================================================================
	Training time : 0:47:41.658284
==================================================================================================
	Identification : 0.382

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 36)
  (w_embeddings): Embedding(5626, 73)
  (lstm): LSTM(109, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=108, bias=True)
  (linear2): Linear(in_features=108, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10378.699362
validation loss after epoch 0 : 988.891636
	Epoch 1....
Epoch has taken 0:01:53.204337
Number of used sentences in train = 2074
Total loss for epoch 1: 6233.177799
validation loss after epoch 1 : 978.983766
	Epoch 2....
Epoch has taken 0:01:52.699219
Number of used sentences in train = 2074
Total loss for epoch 2: 4987.247508
validation loss after epoch 2 : 1046.401997
	Epoch 3....
Epoch has taken 0:01:51.851963
Number of used sentences in train = 2074
Total loss for epoch 3: 4217.718960
validation loss after epoch 3 : 1244.177609
	Epoch 4....
Epoch has taken 0:01:53.450580
Number of used sentences in train = 2074
Total loss for epoch 4: 3772.968901
validation loss after epoch 4 : 1309.114963
	Epoch 5....
Epoch has taken 0:01:53.260072
Number of used sentences in train = 2074
Total loss for epoch 5: 3516.661328
validation loss after epoch 5 : 1410.867037
	Epoch 6....
Epoch has taken 0:01:53.370188
Number of used sentences in train = 2074
Total loss for epoch 6: 3361.104416
validation loss after epoch 6 : 1460.402805
	Epoch 7....
Epoch has taken 0:01:53.421997
Number of used sentences in train = 2074
Total loss for epoch 7: 3257.060819
validation loss after epoch 7 : 1547.592159
	Epoch 8....
Epoch has taken 0:01:54.370898
Number of used sentences in train = 2074
Total loss for epoch 8: 3215.248158
validation loss after epoch 8 : 1578.204553
	Epoch 9....
Epoch has taken 0:02:03.169421
Number of used sentences in train = 2074
Total loss for epoch 9: 3192.837094
validation loss after epoch 9 : 1599.655918
	Epoch 10....
Epoch has taken 0:01:57.187979
Number of used sentences in train = 2074
Total loss for epoch 10: 3182.842324
validation loss after epoch 10 : 1621.292401
	Epoch 11....
Epoch has taken 0:02:03.588919
Number of used sentences in train = 2074
Total loss for epoch 11: 3176.843920
validation loss after epoch 11 : 1653.833513
	Epoch 12....
Epoch has taken 0:02:04.304420
Number of used sentences in train = 2074
Total loss for epoch 12: 3170.790769
validation loss after epoch 12 : 1674.026208
	Epoch 13....
Epoch has taken 0:02:04.263832
Number of used sentences in train = 2074
Total loss for epoch 13: 3167.757399
validation loss after epoch 13 : 1702.283623
	Epoch 14....
Epoch has taken 0:01:57.029827
Number of used sentences in train = 2074
Total loss for epoch 14: 3165.181084
validation loss after epoch 14 : 1713.732747
	TransitionClassifier(
  (p_embeddings): Embedding(18, 36)
  (w_embeddings): Embedding(5626, 73)
  (lstm): LSTM(109, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=108, bias=True)
  (linear2): Linear(in_features=108, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.474252
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1997.296950
	Epoch 1....
Epoch has taken 0:00:11.596002
Number of used sentences in train = 231
Total loss for epoch 1: 674.798897
	Epoch 2....
Epoch has taken 0:00:11.573585
Number of used sentences in train = 231
Total loss for epoch 2: 466.419472
	Epoch 3....
Epoch has taken 0:00:11.575840
Number of used sentences in train = 231
Total loss for epoch 3: 391.613805
	Epoch 4....
Epoch has taken 0:00:11.862121
Number of used sentences in train = 231
Total loss for epoch 4: 361.595113
	Epoch 5....
Epoch has taken 0:00:11.576080
Number of used sentences in train = 231
Total loss for epoch 5: 354.510311
	Epoch 6....
Epoch has taken 0:00:11.558175
Number of used sentences in train = 231
Total loss for epoch 6: 351.201151
	Epoch 7....
Epoch has taken 0:00:11.605740
Number of used sentences in train = 231
Total loss for epoch 7: 349.509404
	Epoch 8....
Epoch has taken 0:00:11.571135
Number of used sentences in train = 231
Total loss for epoch 8: 348.517672
	Epoch 9....
Epoch has taken 0:00:11.575034
Number of used sentences in train = 231
Total loss for epoch 9: 347.728190
	Epoch 10....
Epoch has taken 0:00:11.580675
Number of used sentences in train = 231
Total loss for epoch 10: 347.150429
	Epoch 11....
Epoch has taken 0:00:11.590005
Number of used sentences in train = 231
Total loss for epoch 11: 346.785813
	Epoch 12....
Epoch has taken 0:00:11.583619
Number of used sentences in train = 231
Total loss for epoch 12: 346.480792
	Epoch 13....
Epoch has taken 0:00:11.584009
Number of used sentences in train = 231
Total loss for epoch 13: 346.241757
	Epoch 14....
Epoch has taken 0:00:11.563955
Number of used sentences in train = 231
Total loss for epoch 14: 346.015862
Epoch has taken 0:00:11.586077

==================================================================================================
	Training time : 0:32:02.969441
==================================================================================================
	Identification : 0.176

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 36)
  (w_embeddings): Embedding(6886, 73)
  (lstm): LSTM(109, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=108, bias=True)
  (linear2): Linear(in_features=108, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16416.563576
validation loss after epoch 0 : 1514.874155
	Epoch 1....
Epoch has taken 0:03:39.899936
Number of used sentences in train = 3226
Total loss for epoch 1: 12131.830488
validation loss after epoch 1 : 1519.611863
	Epoch 2....
Epoch has taken 0:03:39.910088
Number of used sentences in train = 3226
Total loss for epoch 2: 10489.308869
validation loss after epoch 2 : 1516.628826
	Epoch 3....
Epoch has taken 0:03:40.330905
Number of used sentences in train = 3226
Total loss for epoch 3: 9302.957224
validation loss after epoch 3 : 1691.209094
	Epoch 4....
Epoch has taken 0:03:42.347822
Number of used sentences in train = 3226
Total loss for epoch 4: 8485.130931
validation loss after epoch 4 : 1688.041213
	Epoch 5....
Epoch has taken 0:03:40.119461
Number of used sentences in train = 3226
Total loss for epoch 5: 7801.067750
validation loss after epoch 5 : 1846.318851
	Epoch 6....
Epoch has taken 0:03:39.517955
Number of used sentences in train = 3226
Total loss for epoch 6: 7305.040114
validation loss after epoch 6 : 2001.014709
	Epoch 7....
Epoch has taken 0:04:05.476394
Number of used sentences in train = 3226
Total loss for epoch 7: 6948.214168
validation loss after epoch 7 : 2157.520665
	Epoch 8....
Epoch has taken 0:03:39.724458
Number of used sentences in train = 3226
Total loss for epoch 8: 6693.443358
validation loss after epoch 8 : 2280.944995
	Epoch 9....
Epoch has taken 0:03:40.406800
Number of used sentences in train = 3226
Total loss for epoch 9: 6496.236474
validation loss after epoch 9 : 2444.702034
	Epoch 10....
Epoch has taken 0:03:42.150797
Number of used sentences in train = 3226
Total loss for epoch 10: 6414.991170
validation loss after epoch 10 : 2510.493885
	Epoch 11....
Epoch has taken 0:03:53.656703
Number of used sentences in train = 3226
Total loss for epoch 11: 6352.516724
validation loss after epoch 11 : 2554.100946
	Epoch 12....
Epoch has taken 0:03:42.137286
Number of used sentences in train = 3226
Total loss for epoch 12: 6315.671778
validation loss after epoch 12 : 2646.127104
	Epoch 13....
Epoch has taken 0:03:42.319320
Number of used sentences in train = 3226
Total loss for epoch 13: 6268.807548
validation loss after epoch 13 : 2680.059676
	Epoch 14....
Epoch has taken 0:03:42.145139
Number of used sentences in train = 3226
Total loss for epoch 14: 6248.436964
validation loss after epoch 14 : 2725.458351
	TransitionClassifier(
  (p_embeddings): Embedding(13, 36)
  (w_embeddings): Embedding(6886, 73)
  (lstm): LSTM(109, 28, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=108, bias=True)
  (linear2): Linear(in_features=108, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:40.366491
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2260.401211
	Epoch 1....
Epoch has taken 0:00:21.512642
Number of used sentences in train = 359
Total loss for epoch 1: 1130.285086
	Epoch 2....
Epoch has taken 0:00:21.507055
Number of used sentences in train = 359
Total loss for epoch 2: 898.862254
	Epoch 3....
Epoch has taken 0:00:21.511692
Number of used sentences in train = 359
Total loss for epoch 3: 765.519745
	Epoch 4....
Epoch has taken 0:00:21.523126
Number of used sentences in train = 359
Total loss for epoch 4: 708.970641
	Epoch 5....
Epoch has taken 0:00:21.518488
Number of used sentences in train = 359
Total loss for epoch 5: 683.712187
	Epoch 6....
Epoch has taken 0:00:21.509901
Number of used sentences in train = 359
Total loss for epoch 6: 676.014943
	Epoch 7....
Epoch has taken 0:00:21.518972
Number of used sentences in train = 359
Total loss for epoch 7: 673.630090
	Epoch 8....
Epoch has taken 0:00:21.513148
Number of used sentences in train = 359
Total loss for epoch 8: 672.601351
	Epoch 9....
Epoch has taken 0:00:21.492726
Number of used sentences in train = 359
Total loss for epoch 9: 672.108978
	Epoch 10....
Epoch has taken 0:00:21.509725
Number of used sentences in train = 359
Total loss for epoch 10: 671.777766
	Epoch 11....
Epoch has taken 0:00:21.509268
Number of used sentences in train = 359
Total loss for epoch 11: 671.465008
	Epoch 12....
Epoch has taken 0:00:21.514540
Number of used sentences in train = 359
Total loss for epoch 12: 671.255908
	Epoch 13....
Epoch has taken 0:00:21.515932
Number of used sentences in train = 359
Total loss for epoch 13: 671.089225
	Epoch 14....
Epoch has taken 0:00:21.527000
Number of used sentences in train = 359
Total loss for epoch 14: 670.956519
Epoch has taken 0:00:21.523022

==================================================================================================
	Training time : 1:01:13.899611
==================================================================================================
	Identification : 0.06

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 106, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 113, 'lstmDropout': 0.17, 'denseActivation': 'tanh', 'wordDim': 147, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 147)
  (lstm): LSTM(214, 113, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11895.464731
validation loss after epoch 0 : 895.441368
	Epoch 1....
Epoch has taken 0:02:46.430891
Number of used sentences in train = 2811
Total loss for epoch 1: 8123.285360
validation loss after epoch 1 : 863.846199
	Epoch 2....
Epoch has taken 0:02:46.565648
Number of used sentences in train = 2811
Total loss for epoch 2: 7256.854047
validation loss after epoch 2 : 823.147035
	Epoch 3....
Epoch has taken 0:02:46.499645
Number of used sentences in train = 2811
Total loss for epoch 3: 6698.466252
validation loss after epoch 3 : 874.016420
	Epoch 4....
Epoch has taken 0:02:46.478858
Number of used sentences in train = 2811
Total loss for epoch 4: 6124.795117
validation loss after epoch 4 : 890.470754
	Epoch 5....
Epoch has taken 0:02:55.658060
Number of used sentences in train = 2811
Total loss for epoch 5: 5823.232135
validation loss after epoch 5 : 881.610143
	Epoch 6....
Epoch has taken 0:02:46.401730
Number of used sentences in train = 2811
Total loss for epoch 6: 5501.083513
validation loss after epoch 6 : 886.450949
	Epoch 7....
Epoch has taken 0:02:46.473938
Number of used sentences in train = 2811
Total loss for epoch 7: 5266.969930
validation loss after epoch 7 : 929.930044
	Epoch 8....
Epoch has taken 0:02:44.419481
Number of used sentences in train = 2811
Total loss for epoch 8: 5125.869233
validation loss after epoch 8 : 959.660598
	Epoch 9....
Epoch has taken 0:02:46.465394
Number of used sentences in train = 2811
Total loss for epoch 9: 4986.669232
validation loss after epoch 9 : 974.861651
	Epoch 10....
Epoch has taken 0:02:46.437338
Number of used sentences in train = 2811
Total loss for epoch 10: 4909.861992
validation loss after epoch 10 : 1014.285890
	Epoch 11....
Epoch has taken 0:02:46.342243
Number of used sentences in train = 2811
Total loss for epoch 11: 4841.240597
validation loss after epoch 11 : 1028.274916
	Epoch 12....
Epoch has taken 0:02:45.646178
Number of used sentences in train = 2811
Total loss for epoch 12: 4772.937068
validation loss after epoch 12 : 1028.876971
	Epoch 13....
Epoch has taken 0:02:46.373138
Number of used sentences in train = 2811
Total loss for epoch 13: 4724.381107
validation loss after epoch 13 : 1053.572271
	Epoch 14....
Epoch has taken 0:02:46.425261
Number of used sentences in train = 2811
Total loss for epoch 14: 4692.506971
validation loss after epoch 14 : 1098.381906
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 147)
  (lstm): LSTM(214, 113, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.379113
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2013.784125
	Epoch 1....
Epoch has taken 0:00:17.596563
Number of used sentences in train = 313
Total loss for epoch 1: 758.635263
	Epoch 2....
Epoch has taken 0:00:17.612020
Number of used sentences in train = 313
Total loss for epoch 2: 632.099356
	Epoch 3....
Epoch has taken 0:00:17.393314
Number of used sentences in train = 313
Total loss for epoch 3: 572.378118
	Epoch 4....
Epoch has taken 0:00:17.609391
Number of used sentences in train = 313
Total loss for epoch 4: 543.288909
	Epoch 5....
Epoch has taken 0:00:17.599949
Number of used sentences in train = 313
Total loss for epoch 5: 531.913771
	Epoch 6....
Epoch has taken 0:00:17.608165
Number of used sentences in train = 313
Total loss for epoch 6: 528.839100
	Epoch 7....
Epoch has taken 0:00:17.596052
Number of used sentences in train = 313
Total loss for epoch 7: 519.825344
	Epoch 8....
Epoch has taken 0:00:17.608716
Number of used sentences in train = 313
Total loss for epoch 8: 516.392999
	Epoch 9....
Epoch has taken 0:00:17.602408
Number of used sentences in train = 313
Total loss for epoch 9: 516.663486
	Epoch 10....
Epoch has taken 0:00:17.590430
Number of used sentences in train = 313
Total loss for epoch 10: 511.974575
	Epoch 11....
Epoch has taken 0:00:17.590385
Number of used sentences in train = 313
Total loss for epoch 11: 508.654593
	Epoch 12....
Epoch has taken 0:00:17.598722
Number of used sentences in train = 313
Total loss for epoch 12: 503.905277
	Epoch 13....
Epoch has taken 0:00:17.606487
Number of used sentences in train = 313
Total loss for epoch 13: 503.154325
	Epoch 14....
Epoch has taken 0:00:17.601516
Number of used sentences in train = 313
Total loss for epoch 14: 502.735099
Epoch has taken 0:00:17.598386

==================================================================================================
	Training time : 0:46:07.313753
==================================================================================================
	Identification : 0.064

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 147)
  (lstm): LSTM(214, 113, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8964.904224
validation loss after epoch 0 : 781.571139
	Epoch 1....
Epoch has taken 0:01:54.035690
Number of used sentences in train = 2074
Total loss for epoch 1: 5546.421882
validation loss after epoch 1 : 737.493580
	Epoch 2....
Epoch has taken 0:01:52.976730
Number of used sentences in train = 2074
Total loss for epoch 2: 4639.306952
validation loss after epoch 2 : 726.873037
	Epoch 3....
Epoch has taken 0:01:54.129905
Number of used sentences in train = 2074
Total loss for epoch 3: 4037.417429
validation loss after epoch 3 : 755.081050
	Epoch 4....
Epoch has taken 0:02:03.323648
Number of used sentences in train = 2074
Total loss for epoch 4: 3680.173633
validation loss after epoch 4 : 784.888905
	Epoch 5....
Epoch has taken 0:02:03.887851
Number of used sentences in train = 2074
Total loss for epoch 5: 3413.663487
validation loss after epoch 5 : 860.359998
	Epoch 6....
Epoch has taken 0:01:54.248934
Number of used sentences in train = 2074
Total loss for epoch 6: 3290.588756
validation loss after epoch 6 : 858.886786
	Epoch 7....
Epoch has taken 0:01:52.684127
Number of used sentences in train = 2074
Total loss for epoch 7: 3232.552991
validation loss after epoch 7 : 904.235359
	Epoch 8....
Epoch has taken 0:01:54.248108
Number of used sentences in train = 2074
Total loss for epoch 8: 3205.855026
validation loss after epoch 8 : 925.492360
	Epoch 9....
Epoch has taken 0:01:54.220075
Number of used sentences in train = 2074
Total loss for epoch 9: 3187.400372
validation loss after epoch 9 : 950.651290
	Epoch 10....
Epoch has taken 0:01:52.855272
Number of used sentences in train = 2074
Total loss for epoch 10: 3178.420483
validation loss after epoch 10 : 959.782861
	Epoch 11....
Epoch has taken 0:01:52.869992
Number of used sentences in train = 2074
Total loss for epoch 11: 3171.250563
validation loss after epoch 11 : 976.491012
	Epoch 12....
Epoch has taken 0:01:54.128055
Number of used sentences in train = 2074
Total loss for epoch 12: 3167.374693
validation loss after epoch 12 : 987.119928
	Epoch 13....
Epoch has taken 0:01:54.117806
Number of used sentences in train = 2074
Total loss for epoch 13: 3163.494182
validation loss after epoch 13 : 999.644881
	Epoch 14....
Epoch has taken 0:01:54.069414
Number of used sentences in train = 2074
Total loss for epoch 14: 3161.334344
validation loss after epoch 14 : 1010.221908
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 147)
  (lstm): LSTM(214, 113, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.093197
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1291.181187
	Epoch 1....
Epoch has taken 0:00:11.671495
Number of used sentences in train = 231
Total loss for epoch 1: 593.667596
	Epoch 2....
Epoch has taken 0:00:11.674226
Number of used sentences in train = 231
Total loss for epoch 2: 454.656720
	Epoch 3....
Epoch has taken 0:00:11.667134
Number of used sentences in train = 231
Total loss for epoch 3: 395.460169
	Epoch 4....
Epoch has taken 0:00:11.642733
Number of used sentences in train = 231
Total loss for epoch 4: 374.573534
	Epoch 5....
Epoch has taken 0:00:11.664403
Number of used sentences in train = 231
Total loss for epoch 5: 363.420259
	Epoch 6....
Epoch has taken 0:00:11.658803
Number of used sentences in train = 231
Total loss for epoch 6: 357.883816
	Epoch 7....
Epoch has taken 0:00:11.657294
Number of used sentences in train = 231
Total loss for epoch 7: 355.513717
	Epoch 8....
Epoch has taken 0:00:12.926980
Number of used sentences in train = 231
Total loss for epoch 8: 351.837187
	Epoch 9....
Epoch has taken 0:00:12.986910
Number of used sentences in train = 231
Total loss for epoch 9: 349.461777
	Epoch 10....
Epoch has taken 0:00:12.992672
Number of used sentences in train = 231
Total loss for epoch 10: 348.350083
	Epoch 11....
Epoch has taken 0:00:12.983311
Number of used sentences in train = 231
Total loss for epoch 11: 348.814301
	Epoch 12....
Epoch has taken 0:00:12.991716
Number of used sentences in train = 231
Total loss for epoch 12: 347.320937
	Epoch 13....
Epoch has taken 0:00:12.988426
Number of used sentences in train = 231
Total loss for epoch 13: 347.035364
	Epoch 14....
Epoch has taken 0:00:12.987381
Number of used sentences in train = 231
Total loss for epoch 14: 346.812962
Epoch has taken 0:00:12.984121

==================================================================================================
	Training time : 0:31:51.707630
==================================================================================================
	Identification : 0.349

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 147)
  (lstm): LSTM(214, 113, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20927.256606
validation loss after epoch 0 : 1606.430738
	Epoch 1....
Epoch has taken 0:03:45.654849
Number of used sentences in train = 3226
Total loss for epoch 1: 11713.143968
validation loss after epoch 1 : 1229.472034
	Epoch 2....
Epoch has taken 0:03:50.428716
Number of used sentences in train = 3226
Total loss for epoch 2: 10389.349617
validation loss after epoch 2 : 1175.321633
	Epoch 3....
Epoch has taken 0:03:42.527202
Number of used sentences in train = 3226
Total loss for epoch 3: 9248.246352
validation loss after epoch 3 : 1157.374804
	Epoch 4....
Epoch has taken 0:03:42.350786
Number of used sentences in train = 3226
Total loss for epoch 4: 8511.321194
validation loss after epoch 4 : 1162.106638
	Epoch 5....
Epoch has taken 0:03:42.854283
Number of used sentences in train = 3226
Total loss for epoch 5: 8001.954814
validation loss after epoch 5 : 1225.052485
	Epoch 6....
Epoch has taken 0:03:43.073353
Number of used sentences in train = 3226
Total loss for epoch 6: 7677.934264
validation loss after epoch 6 : 1248.614951
	Epoch 7....
Epoch has taken 0:03:42.564932
Number of used sentences in train = 3226
Total loss for epoch 7: 7329.032373
validation loss after epoch 7 : 1296.306056
	Epoch 8....
Epoch has taken 0:04:00.186146
Number of used sentences in train = 3226
Total loss for epoch 8: 7085.261606
validation loss after epoch 8 : 1306.199171
	Epoch 9....
Epoch has taken 0:04:02.357757
Number of used sentences in train = 3226
Total loss for epoch 9: 6929.958058
validation loss after epoch 9 : 1342.407546
	Epoch 10....
Epoch has taken 0:04:04.231453
Number of used sentences in train = 3226
Total loss for epoch 10: 6771.947612
validation loss after epoch 10 : 1562.801466
	Epoch 11....
Epoch has taken 0:03:42.682042
Number of used sentences in train = 3226
Total loss for epoch 11: 6651.470643
validation loss after epoch 11 : 1405.321675
	Epoch 12....
Epoch has taken 0:03:37.211155
Number of used sentences in train = 3226
Total loss for epoch 12: 6578.740660
validation loss after epoch 12 : 1466.179256
	Epoch 13....
Epoch has taken 0:03:37.151355
Number of used sentences in train = 3226
Total loss for epoch 13: 6492.430079
validation loss after epoch 13 : 1522.666227
	Epoch 14....
Epoch has taken 0:03:38.987713
Number of used sentences in train = 3226
Total loss for epoch 14: 6458.894131
validation loss after epoch 14 : 1504.847135
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 147)
  (lstm): LSTM(214, 113, bidirectional=True)
  (linear1): Linear(in_features=1808, out_features=106, bias=True)
  (linear2): Linear(in_features=106, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:42.595770
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2161.758984
	Epoch 1....
Epoch has taken 0:00:21.183975
Number of used sentences in train = 359
Total loss for epoch 1: 1116.168967
	Epoch 2....
Epoch has taken 0:00:21.196919
Number of used sentences in train = 359
Total loss for epoch 2: 961.998565
	Epoch 3....
Epoch has taken 0:00:21.202581
Number of used sentences in train = 359
Total loss for epoch 3: 836.489833
	Epoch 4....
Epoch has taken 0:00:21.181907
Number of used sentences in train = 359
Total loss for epoch 4: 789.817471
	Epoch 5....
Epoch has taken 0:00:21.184233
Number of used sentences in train = 359
Total loss for epoch 5: 759.101012
	Epoch 6....
Epoch has taken 0:00:21.179131
Number of used sentences in train = 359
Total loss for epoch 6: 736.398793
	Epoch 7....
Epoch has taken 0:00:21.173781
Number of used sentences in train = 359
Total loss for epoch 7: 715.436787
	Epoch 8....
Epoch has taken 0:00:21.169549
Number of used sentences in train = 359
Total loss for epoch 8: 703.801928
	Epoch 9....
Epoch has taken 0:00:21.190914
Number of used sentences in train = 359
Total loss for epoch 9: 692.292198
	Epoch 10....
Epoch has taken 0:00:21.188853
Number of used sentences in train = 359
Total loss for epoch 10: 687.030557
	Epoch 11....
Epoch has taken 0:00:21.668560
Number of used sentences in train = 359
Total loss for epoch 11: 681.355044
	Epoch 12....
Epoch has taken 0:00:21.193869
Number of used sentences in train = 359
Total loss for epoch 12: 677.658456
	Epoch 13....
Epoch has taken 0:00:21.203163
Number of used sentences in train = 359
Total loss for epoch 13: 676.361448
	Epoch 14....
Epoch has taken 0:00:21.215005
Number of used sentences in train = 359
Total loss for epoch 14: 675.994310
Epoch has taken 0:00:21.211775

==================================================================================================
	Training time : 1:01:53.873141
==================================================================================================
	Identification : 0.363

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 11, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 34, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 25, 'lstmDropout': 0.29, 'denseActivation': 'tanh', 'wordDim': 188, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(9349, 188)
  (lstm): LSTM(222, 25, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15290.578605
validation loss after epoch 0 : 1268.958310
	Epoch 1....
Epoch has taken 0:02:53.796889
Number of used sentences in train = 2811
Total loss for epoch 1: 10018.842483
validation loss after epoch 1 : 1175.700289
	Epoch 2....
Epoch has taken 0:02:57.100150
Number of used sentences in train = 2811
Total loss for epoch 2: 8026.860238
validation loss after epoch 2 : 1121.013441
	Epoch 3....
Epoch has taken 0:02:52.949035
Number of used sentences in train = 2811
Total loss for epoch 3: 6952.656575
validation loss after epoch 3 : 1178.567576
	Epoch 4....
Epoch has taken 0:02:56.489661
Number of used sentences in train = 2811
Total loss for epoch 4: 6351.686712
validation loss after epoch 4 : 1282.070095
	Epoch 5....
Epoch has taken 0:02:52.953197
Number of used sentences in train = 2811
Total loss for epoch 5: 6081.887954
validation loss after epoch 5 : 1287.400545
	Epoch 6....
Epoch has taken 0:02:57.974767
Number of used sentences in train = 2811
Total loss for epoch 6: 5773.696573
validation loss after epoch 6 : 1314.062598
	Epoch 7....
Epoch has taken 0:02:53.026305
Number of used sentences in train = 2811
Total loss for epoch 7: 5642.186437
validation loss after epoch 7 : 1322.481854
	Epoch 8....
Epoch has taken 0:02:53.410157
Number of used sentences in train = 2811
Total loss for epoch 8: 5438.624886
validation loss after epoch 8 : 1347.095083
	Epoch 9....
Epoch has taken 0:02:54.401356
Number of used sentences in train = 2811
Total loss for epoch 9: 5357.157020
validation loss after epoch 9 : 1344.723812
	Epoch 10....
Epoch has taken 0:02:57.046412
Number of used sentences in train = 2811
Total loss for epoch 10: 5182.032169
validation loss after epoch 10 : 1426.999952
	Epoch 11....
Epoch has taken 0:02:56.344777
Number of used sentences in train = 2811
Total loss for epoch 11: 5138.600274
validation loss after epoch 11 : 1437.352579
	Epoch 12....
Epoch has taken 0:02:52.958323
Number of used sentences in train = 2811
Total loss for epoch 12: 5075.687573
validation loss after epoch 12 : 1488.146029
	Epoch 13....
Epoch has taken 0:02:52.999652
Number of used sentences in train = 2811
Total loss for epoch 13: 4995.552474
validation loss after epoch 13 : 1451.402182
	Epoch 14....
Epoch has taken 0:03:17.381721
Number of used sentences in train = 2811
Total loss for epoch 14: 4957.705523
validation loss after epoch 14 : 1497.967534
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(9349, 188)
  (lstm): LSTM(222, 25, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:16.530375
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1651.936787
	Epoch 1....
Epoch has taken 0:00:18.300575
Number of used sentences in train = 313
Total loss for epoch 1: 884.660240
	Epoch 2....
Epoch has taken 0:00:18.306749
Number of used sentences in train = 313
Total loss for epoch 2: 733.139141
	Epoch 3....
Epoch has taken 0:00:18.307709
Number of used sentences in train = 313
Total loss for epoch 3: 644.844564
	Epoch 4....
Epoch has taken 0:00:18.306387
Number of used sentences in train = 313
Total loss for epoch 4: 584.279178
	Epoch 5....
Epoch has taken 0:00:18.306269
Number of used sentences in train = 313
Total loss for epoch 5: 562.646507
	Epoch 6....
Epoch has taken 0:00:20.647795
Number of used sentences in train = 313
Total loss for epoch 6: 551.004381
	Epoch 7....
Epoch has taken 0:00:18.306822
Number of used sentences in train = 313
Total loss for epoch 7: 542.977797
	Epoch 8....
Epoch has taken 0:00:18.293361
Number of used sentences in train = 313
Total loss for epoch 8: 536.232914
	Epoch 9....
Epoch has taken 0:00:18.287486
Number of used sentences in train = 313
Total loss for epoch 9: 524.637417
	Epoch 10....
Epoch has taken 0:00:18.303501
Number of used sentences in train = 313
Total loss for epoch 10: 527.408698
	Epoch 11....
Epoch has taken 0:00:18.297896
Number of used sentences in train = 313
Total loss for epoch 11: 534.925782
	Epoch 12....
Epoch has taken 0:00:18.290684
Number of used sentences in train = 313
Total loss for epoch 12: 516.508783
	Epoch 13....
Epoch has taken 0:00:18.298599
Number of used sentences in train = 313
Total loss for epoch 13: 515.176558
	Epoch 14....
Epoch has taken 0:00:18.291976
Number of used sentences in train = 313
Total loss for epoch 14: 513.152272
Epoch has taken 0:00:18.290605

==================================================================================================
	Training time : 0:49:02.721679
==================================================================================================
	Identification : 0.198

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(7078, 188)
  (lstm): LSTM(222, 25, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 12345.569259
validation loss after epoch 0 : 988.952336
	Epoch 1....
Epoch has taken 0:01:58.265419
Number of used sentences in train = 2074
Total loss for epoch 1: 7519.520754
validation loss after epoch 1 : 920.697101
	Epoch 2....
Epoch has taken 0:01:58.361320
Number of used sentences in train = 2074
Total loss for epoch 2: 6051.038678
validation loss after epoch 2 : 900.548696
	Epoch 3....
Epoch has taken 0:02:00.829876
Number of used sentences in train = 2074
Total loss for epoch 3: 5317.529560
validation loss after epoch 3 : 921.000880
	Epoch 4....
Epoch has taken 0:01:59.980448
Number of used sentences in train = 2074
Total loss for epoch 4: 4774.093681
validation loss after epoch 4 : 928.334871
	Epoch 5....
Epoch has taken 0:01:59.324286
Number of used sentences in train = 2074
Total loss for epoch 5: 4531.810344
validation loss after epoch 5 : 1008.482822
	Epoch 6....
Epoch has taken 0:01:58.361100
Number of used sentences in train = 2074
Total loss for epoch 6: 4289.236099
validation loss after epoch 6 : 993.610281
	Epoch 7....
Epoch has taken 0:01:58.279014
Number of used sentences in train = 2074
Total loss for epoch 7: 4153.362643
validation loss after epoch 7 : 1083.578490
	Epoch 8....
Epoch has taken 0:01:58.260152
Number of used sentences in train = 2074
Total loss for epoch 8: 3993.348242
validation loss after epoch 8 : 1099.928496
	Epoch 9....
Epoch has taken 0:01:58.349048
Number of used sentences in train = 2074
Total loss for epoch 9: 3930.702631
validation loss after epoch 9 : 1093.353893
	Epoch 10....
Epoch has taken 0:01:58.639825
Number of used sentences in train = 2074
Total loss for epoch 10: 3850.912945
validation loss after epoch 10 : 1118.289032
	Epoch 11....
Epoch has taken 0:01:58.519448
Number of used sentences in train = 2074
Total loss for epoch 11: 3772.254781
validation loss after epoch 11 : 1131.950187
	Epoch 12....
Epoch has taken 0:01:58.500885
Number of used sentences in train = 2074
Total loss for epoch 12: 3718.109560
validation loss after epoch 12 : 1055.224213
	Epoch 13....
Epoch has taken 0:02:02.090019
Number of used sentences in train = 2074
Total loss for epoch 13: 3650.320460
validation loss after epoch 13 : 1126.556305
	Epoch 14....
Epoch has taken 0:01:58.677086
Number of used sentences in train = 2074
Total loss for epoch 14: 3631.749815
validation loss after epoch 14 : 1130.438259
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(7078, 188)
  (lstm): LSTM(222, 25, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:58.602563
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1314.034717
	Epoch 1....
Epoch has taken 0:00:12.091649
Number of used sentences in train = 231
Total loss for epoch 1: 692.206341
	Epoch 2....
Epoch has taken 0:00:12.097629
Number of used sentences in train = 231
Total loss for epoch 2: 522.356311
	Epoch 3....
Epoch has taken 0:00:12.099346
Number of used sentences in train = 231
Total loss for epoch 3: 453.167104
	Epoch 4....
Epoch has taken 0:00:12.077029
Number of used sentences in train = 231
Total loss for epoch 4: 430.489881
	Epoch 5....
Epoch has taken 0:00:12.109357
Number of used sentences in train = 231
Total loss for epoch 5: 403.409878
	Epoch 6....
Epoch has taken 0:00:12.109528
Number of used sentences in train = 231
Total loss for epoch 6: 416.328097
	Epoch 7....
Epoch has taken 0:00:12.082333
Number of used sentences in train = 231
Total loss for epoch 7: 378.071775
	Epoch 8....
Epoch has taken 0:00:12.101059
Number of used sentences in train = 231
Total loss for epoch 8: 389.737455
	Epoch 9....
Epoch has taken 0:00:12.087806
Number of used sentences in train = 231
Total loss for epoch 9: 378.438386
	Epoch 10....
Epoch has taken 0:00:12.049970
Number of used sentences in train = 231
Total loss for epoch 10: 372.689836
	Epoch 11....
Epoch has taken 0:00:12.089050
Number of used sentences in train = 231
Total loss for epoch 11: 365.518227
	Epoch 12....
Epoch has taken 0:00:12.060174
Number of used sentences in train = 231
Total loss for epoch 12: 370.686573
	Epoch 13....
Epoch has taken 0:00:12.102219
Number of used sentences in train = 231
Total loss for epoch 13: 366.606895
	Epoch 14....
Epoch has taken 0:00:12.058345
Number of used sentences in train = 231
Total loss for epoch 14: 358.507161
Epoch has taken 0:00:12.082953

==================================================================================================
	Training time : 0:32:46.691663
==================================================================================================
	Identification : 0.141

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 34)
  (w_embeddings): Embedding(18110, 188)
  (lstm): LSTM(222, 25, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18650.104794
validation loss after epoch 0 : 1637.333207
	Epoch 1....
Epoch has taken 0:03:51.828951
Number of used sentences in train = 3226
Total loss for epoch 1: 12732.987803
validation loss after epoch 1 : 1622.878256
	Epoch 2....
Epoch has taken 0:03:50.962738
Number of used sentences in train = 3226
Total loss for epoch 2: 10631.979785
validation loss after epoch 2 : 1561.630470
	Epoch 3....
Epoch has taken 0:03:50.618777
Number of used sentences in train = 3226
Total loss for epoch 3: 9563.821971
validation loss after epoch 3 : 1715.989017
	Epoch 4....
Epoch has taken 0:03:55.865735
Number of used sentences in train = 3226
Total loss for epoch 4: 8828.662511
validation loss after epoch 4 : 1778.633740
	Epoch 5....
Epoch has taken 0:03:54.673873
Number of used sentences in train = 3226
Total loss for epoch 5: 8215.167921
validation loss after epoch 5 : 1844.538788
	Epoch 6....
Epoch has taken 0:04:11.760505
Number of used sentences in train = 3226
Total loss for epoch 6: 7874.081365
validation loss after epoch 6 : 1920.516521
	Epoch 7....
Epoch has taken 0:04:16.899391
Number of used sentences in train = 3226
Total loss for epoch 7: 7553.906321
validation loss after epoch 7 : 1965.195730
	Epoch 8....
Epoch has taken 0:03:57.480177
Number of used sentences in train = 3226
Total loss for epoch 8: 7328.510075
validation loss after epoch 8 : 2018.777863
	Epoch 9....
Epoch has taken 0:03:56.984775
Number of used sentences in train = 3226
Total loss for epoch 9: 7179.912031
validation loss after epoch 9 : 2031.026392
	Epoch 10....
Epoch has taken 0:03:55.041802
Number of used sentences in train = 3226
Total loss for epoch 10: 7003.302448
validation loss after epoch 10 : 2137.382647
	Epoch 11....
Epoch has taken 0:03:50.241103
Number of used sentences in train = 3226
Total loss for epoch 11: 6922.937359
validation loss after epoch 11 : 2232.360757
	Epoch 12....
Epoch has taken 0:03:50.057240
Number of used sentences in train = 3226
Total loss for epoch 12: 6805.920615
validation loss after epoch 12 : 2238.908765
	Epoch 13....
Epoch has taken 0:03:50.443116
Number of used sentences in train = 3226
Total loss for epoch 13: 6719.161016
validation loss after epoch 13 : 2262.489391
	Epoch 14....
Epoch has taken 0:03:54.710537
Number of used sentences in train = 3226
Total loss for epoch 14: 6722.771546
validation loss after epoch 14 : 2254.422875
	TransitionClassifier(
  (p_embeddings): Embedding(13, 34)
  (w_embeddings): Embedding(18110, 188)
  (lstm): LSTM(222, 25, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:50.249775
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2207.578911
	Epoch 1....
Epoch has taken 0:00:22.474086
Number of used sentences in train = 359
Total loss for epoch 1: 1198.149782
	Epoch 2....
Epoch has taken 0:00:22.467637
Number of used sentences in train = 359
Total loss for epoch 2: 946.109856
	Epoch 3....
Epoch has taken 0:00:22.462882
Number of used sentences in train = 359
Total loss for epoch 3: 833.221247
	Epoch 4....
Epoch has taken 0:00:22.479137
Number of used sentences in train = 359
Total loss for epoch 4: 773.494545
	Epoch 5....
Epoch has taken 0:00:22.477935
Number of used sentences in train = 359
Total loss for epoch 5: 726.119422
	Epoch 6....
Epoch has taken 0:00:22.486625
Number of used sentences in train = 359
Total loss for epoch 6: 734.291456
	Epoch 7....
Epoch has taken 0:00:22.474967
Number of used sentences in train = 359
Total loss for epoch 7: 715.380634
	Epoch 8....
Epoch has taken 0:00:22.472467
Number of used sentences in train = 359
Total loss for epoch 8: 709.549856
	Epoch 9....
Epoch has taken 0:00:22.472783
Number of used sentences in train = 359
Total loss for epoch 9: 700.293590
	Epoch 10....
Epoch has taken 0:00:22.473032
Number of used sentences in train = 359
Total loss for epoch 10: 693.050187
	Epoch 11....
Epoch has taken 0:00:22.473022
Number of used sentences in train = 359
Total loss for epoch 11: 681.313288
	Epoch 12....
Epoch has taken 0:00:22.469281
Number of used sentences in train = 359
Total loss for epoch 12: 692.998616
	Epoch 13....
Epoch has taken 0:00:22.475401
Number of used sentences in train = 359
Total loss for epoch 13: 687.282377
	Epoch 14....
Epoch has taken 0:00:22.446822
Number of used sentences in train = 359
Total loss for epoch 14: 685.193596
Epoch has taken 0:00:22.453479

==================================================================================================
	Training time : 1:04:35.600010
==================================================================================================
	Identification : 0.425

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 48, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 79, 'lstmDropout': 0.19, 'denseActivation': 'tanh', 'wordDim': 173, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(9237, 173)
  (lstm): LSTM(221, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14122.986896
validation loss after epoch 0 : 1225.212118
	Epoch 1....
Epoch has taken 0:02:41.306693
Number of used sentences in train = 2811
Total loss for epoch 1: 9240.053300
validation loss after epoch 1 : 1164.846132
	Epoch 2....
Epoch has taken 0:02:42.399225
Number of used sentences in train = 2811
Total loss for epoch 2: 7317.584644
validation loss after epoch 2 : 1211.888826
	Epoch 3....
Epoch has taken 0:02:41.342130
Number of used sentences in train = 2811
Total loss for epoch 3: 6196.815197
validation loss after epoch 3 : 1276.558020
	Epoch 4....
Epoch has taken 0:03:02.582972
Number of used sentences in train = 2811
Total loss for epoch 4: 5640.460688
validation loss after epoch 4 : 1363.502789
	Epoch 5....
Epoch has taken 0:02:41.652872
Number of used sentences in train = 2811
Total loss for epoch 5: 5373.426917
validation loss after epoch 5 : 1393.415586
	Epoch 6....
Epoch has taken 0:02:41.544488
Number of used sentences in train = 2811
Total loss for epoch 6: 5225.452601
validation loss after epoch 6 : 1447.782413
	Epoch 7....
Epoch has taken 0:02:43.984394
Number of used sentences in train = 2811
Total loss for epoch 7: 5108.665238
validation loss after epoch 7 : 1461.029361
	Epoch 8....
Epoch has taken 0:02:49.429931
Number of used sentences in train = 2811
Total loss for epoch 8: 5011.231774
validation loss after epoch 8 : 1526.936440
	Epoch 9....
Epoch has taken 0:02:41.368597
Number of used sentences in train = 2811
Total loss for epoch 9: 4949.702795
validation loss after epoch 9 : 1531.875777
	Epoch 10....
Epoch has taken 0:02:44.781459
Number of used sentences in train = 2811
Total loss for epoch 10: 4900.919412
validation loss after epoch 10 : 1559.590767
	Epoch 11....
Epoch has taken 0:02:41.395716
Number of used sentences in train = 2811
Total loss for epoch 11: 4863.528038
validation loss after epoch 11 : 1574.263208
	Epoch 12....
Epoch has taken 0:02:41.521573
Number of used sentences in train = 2811
Total loss for epoch 12: 4821.233651
validation loss after epoch 12 : 1587.016930
	Epoch 13....
Epoch has taken 0:02:41.568558
Number of used sentences in train = 2811
Total loss for epoch 13: 4799.090771
validation loss after epoch 13 : 1608.515318
	Epoch 14....
Epoch has taken 0:02:41.503806
Number of used sentences in train = 2811
Total loss for epoch 14: 4775.575587
validation loss after epoch 14 : 1640.486471
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(9237, 173)
  (lstm): LSTM(221, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:41.358607
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1653.945928
	Epoch 1....
Epoch has taken 0:00:17.006863
Number of used sentences in train = 313
Total loss for epoch 1: 965.082095
	Epoch 2....
Epoch has taken 0:00:16.997335
Number of used sentences in train = 313
Total loss for epoch 2: 767.519702
	Epoch 3....
Epoch has taken 0:00:17.001347
Number of used sentences in train = 313
Total loss for epoch 3: 695.320767
	Epoch 4....
Epoch has taken 0:00:17.005904
Number of used sentences in train = 313
Total loss for epoch 4: 644.081582
	Epoch 5....
Epoch has taken 0:00:16.998887
Number of used sentences in train = 313
Total loss for epoch 5: 621.515524
	Epoch 6....
Epoch has taken 0:00:17.004003
Number of used sentences in train = 313
Total loss for epoch 6: 607.014728
	Epoch 7....
Epoch has taken 0:00:17.007864
Number of used sentences in train = 313
Total loss for epoch 7: 599.048377
	Epoch 8....
Epoch has taken 0:00:17.004090
Number of used sentences in train = 313
Total loss for epoch 8: 588.422080
	Epoch 9....
Epoch has taken 0:00:17.003817
Number of used sentences in train = 313
Total loss for epoch 9: 585.170410
	Epoch 10....
Epoch has taken 0:00:16.993790
Number of used sentences in train = 313
Total loss for epoch 10: 580.365013
	Epoch 11....
Epoch has taken 0:00:17.008576
Number of used sentences in train = 313
Total loss for epoch 11: 576.005101
	Epoch 12....
Epoch has taken 0:00:17.000204
Number of used sentences in train = 313
Total loss for epoch 12: 555.808454
	Epoch 13....
Epoch has taken 0:00:17.011103
Number of used sentences in train = 313
Total loss for epoch 13: 554.002335
	Epoch 14....
Epoch has taken 0:00:16.990740
Number of used sentences in train = 313
Total loss for epoch 14: 550.167054
Epoch has taken 0:00:16.994860

==================================================================================================
	Training time : 0:45:13.288776
==================================================================================================
	Identification : 0.048

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(7037, 173)
  (lstm): LSTM(221, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 12009.510523
validation loss after epoch 0 : 1024.475826
	Epoch 1....
Epoch has taken 0:01:50.693106
Number of used sentences in train = 2074
Total loss for epoch 1: 7088.279788
validation loss after epoch 1 : 1005.783156
	Epoch 2....
Epoch has taken 0:01:50.863659
Number of used sentences in train = 2074
Total loss for epoch 2: 5359.789045
validation loss after epoch 2 : 1117.850960
	Epoch 3....
Epoch has taken 0:01:50.979883
Number of used sentences in train = 2074
Total loss for epoch 3: 4501.893984
validation loss after epoch 3 : 1185.272218
	Epoch 4....
Epoch has taken 0:02:00.754276
Number of used sentences in train = 2074
Total loss for epoch 4: 4061.961134
validation loss after epoch 4 : 1275.130982
	Epoch 5....
Epoch has taken 0:01:50.816885
Number of used sentences in train = 2074
Total loss for epoch 5: 3835.461347
validation loss after epoch 5 : 1372.495412
	Epoch 6....
Epoch has taken 0:01:50.799784
Number of used sentences in train = 2074
Total loss for epoch 6: 3698.303754
validation loss after epoch 6 : 1422.918301
	Epoch 7....
Epoch has taken 0:01:50.886648
Number of used sentences in train = 2074
Total loss for epoch 7: 3621.631170
validation loss after epoch 7 : 1478.156172
	Epoch 8....
Epoch has taken 0:01:50.938056
Number of used sentences in train = 2074
Total loss for epoch 8: 3544.553626
validation loss after epoch 8 : 1540.176748
	Epoch 9....
Epoch has taken 0:02:04.417074
Number of used sentences in train = 2074
Total loss for epoch 9: 3481.926958
validation loss after epoch 9 : 1610.969674
	Epoch 10....
Epoch has taken 0:01:50.894305
Number of used sentences in train = 2074
Total loss for epoch 10: 3433.236032
validation loss after epoch 10 : 1615.242911
	Epoch 11....
Epoch has taken 0:01:50.934272
Number of used sentences in train = 2074
Total loss for epoch 11: 3393.271198
validation loss after epoch 11 : 1666.256828
	Epoch 12....
Epoch has taken 0:01:54.729988
Number of used sentences in train = 2074
Total loss for epoch 12: 3364.326206
validation loss after epoch 12 : 1675.247224
	Epoch 13....
Epoch has taken 0:01:54.251344
Number of used sentences in train = 2074
Total loss for epoch 13: 3337.722652
validation loss after epoch 13 : 1721.538373
	Epoch 14....
Epoch has taken 0:01:54.422077
Number of used sentences in train = 2074
Total loss for epoch 14: 3319.813689
validation loss after epoch 14 : 1732.320641
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(7037, 173)
  (lstm): LSTM(221, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.563770
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1580.401637
	Epoch 1....
Epoch has taken 0:00:11.693625
Number of used sentences in train = 231
Total loss for epoch 1: 715.820604
	Epoch 2....
Epoch has taken 0:00:11.681962
Number of used sentences in train = 231
Total loss for epoch 2: 499.437760
	Epoch 3....
Epoch has taken 0:00:11.684499
Number of used sentences in train = 231
Total loss for epoch 3: 424.176570
	Epoch 4....
Epoch has taken 0:00:11.677141
Number of used sentences in train = 231
Total loss for epoch 4: 390.807037
	Epoch 5....
Epoch has taken 0:00:11.684809
Number of used sentences in train = 231
Total loss for epoch 5: 373.918010
	Epoch 6....
Epoch has taken 0:00:11.478128
Number of used sentences in train = 231
Total loss for epoch 6: 364.515616
	Epoch 7....
Epoch has taken 0:00:11.475701
Number of used sentences in train = 231
Total loss for epoch 7: 357.315622
	Epoch 8....
Epoch has taken 0:00:11.468142
Number of used sentences in train = 231
Total loss for epoch 8: 356.205334
	Epoch 9....
Epoch has taken 0:00:11.476261
Number of used sentences in train = 231
Total loss for epoch 9: 353.596059
	Epoch 10....
Epoch has taken 0:00:11.486091
Number of used sentences in train = 231
Total loss for epoch 10: 352.621506
	Epoch 11....
Epoch has taken 0:00:11.482298
Number of used sentences in train = 231
Total loss for epoch 11: 352.074058
	Epoch 12....
Epoch has taken 0:00:11.479785
Number of used sentences in train = 231
Total loss for epoch 12: 351.590868
	Epoch 13....
Epoch has taken 0:00:11.711011
Number of used sentences in train = 231
Total loss for epoch 13: 351.181481
	Epoch 14....
Epoch has taken 0:00:11.700090
Number of used sentences in train = 231
Total loss for epoch 14: 350.809621
Epoch has taken 0:00:11.697645

==================================================================================================
	Training time : 0:31:14.174851
==================================================================================================
	Identification : 0.109

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 48)
  (w_embeddings): Embedding(17910, 173)
  (lstm): LSTM(221, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19655.627474
validation loss after epoch 0 : 1756.476435
	Epoch 1....
Epoch has taken 0:03:41.779337
Number of used sentences in train = 3226
Total loss for epoch 1: 12567.039373
validation loss after epoch 1 : 1694.901757
	Epoch 2....
Epoch has taken 0:03:41.343890
Number of used sentences in train = 3226
Total loss for epoch 2: 9771.013680
validation loss after epoch 2 : 1687.447011
	Epoch 3....
Epoch has taken 0:03:41.996725
Number of used sentences in train = 3226
Total loss for epoch 3: 8314.993490
validation loss after epoch 3 : 1894.834344
	Epoch 4....
Epoch has taken 0:03:41.441760
Number of used sentences in train = 3226
Total loss for epoch 4: 7475.744883
validation loss after epoch 4 : 2027.938524
	Epoch 5....
Epoch has taken 0:03:41.959933
Number of used sentences in train = 3226
Total loss for epoch 5: 6959.924458
validation loss after epoch 5 : 2166.308584
	Epoch 6....
Epoch has taken 0:03:41.883549
Number of used sentences in train = 3226
Total loss for epoch 6: 6698.399073
validation loss after epoch 6 : 2263.415420
	Epoch 7....
Epoch has taken 0:03:37.981442
Number of used sentences in train = 3226
Total loss for epoch 7: 6519.151540
validation loss after epoch 7 : 2327.042312
	Epoch 8....
Epoch has taken 0:03:36.611461
Number of used sentences in train = 3226
Total loss for epoch 8: 6414.457444
validation loss after epoch 8 : 2417.667267
	Epoch 9....
Epoch has taken 0:03:41.006983
Number of used sentences in train = 3226
Total loss for epoch 9: 6343.908340
validation loss after epoch 9 : 2508.417748
	Epoch 10....
Epoch has taken 0:03:39.206084
Number of used sentences in train = 3226
Total loss for epoch 10: 6304.994621
validation loss after epoch 10 : 2569.687978
	Epoch 11....
Epoch has taken 0:03:36.757850
Number of used sentences in train = 3226
Total loss for epoch 11: 6276.003578
validation loss after epoch 11 : 2590.214342
	Epoch 12....
Epoch has taken 0:03:36.503478
Number of used sentences in train = 3226
Total loss for epoch 12: 6257.875250
validation loss after epoch 12 : 2612.341966
	Epoch 13....
Epoch has taken 0:03:36.689774
Number of used sentences in train = 3226
Total loss for epoch 13: 6237.230878
validation loss after epoch 13 : 2625.844512
	Epoch 14....
Epoch has taken 0:03:40.495694
Number of used sentences in train = 3226
Total loss for epoch 14: 6219.601762
validation loss after epoch 14 : 2659.123987
	TransitionClassifier(
  (p_embeddings): Embedding(13, 48)
  (w_embeddings): Embedding(17910, 173)
  (lstm): LSTM(221, 79, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:36.832168
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2455.806428
	Epoch 1....
Epoch has taken 0:00:21.135586
Number of used sentences in train = 359
Total loss for epoch 1: 1159.858317
	Epoch 2....
Epoch has taken 0:00:21.182897
Number of used sentences in train = 359
Total loss for epoch 2: 857.519067
	Epoch 3....
Epoch has taken 0:00:21.187841
Number of used sentences in train = 359
Total loss for epoch 3: 740.940845
	Epoch 4....
Epoch has taken 0:00:21.164123
Number of used sentences in train = 359
Total loss for epoch 4: 700.455352
	Epoch 5....
Epoch has taken 0:00:21.149045
Number of used sentences in train = 359
Total loss for epoch 5: 687.055931
	Epoch 6....
Epoch has taken 0:00:21.148141
Number of used sentences in train = 359
Total loss for epoch 6: 683.612483
	Epoch 7....
Epoch has taken 0:00:21.183336
Number of used sentences in train = 359
Total loss for epoch 7: 681.829696
	Epoch 8....
Epoch has taken 0:00:21.189313
Number of used sentences in train = 359
Total loss for epoch 8: 679.140383
	Epoch 9....
Epoch has taken 0:00:21.196957
Number of used sentences in train = 359
Total loss for epoch 9: 678.043047
	Epoch 10....
Epoch has taken 0:00:21.161515
Number of used sentences in train = 359
Total loss for epoch 10: 677.417389
	Epoch 11....
Epoch has taken 0:00:21.254192
Number of used sentences in train = 359
Total loss for epoch 11: 676.894592
	Epoch 12....
Epoch has taken 0:00:21.137656
Number of used sentences in train = 359
Total loss for epoch 12: 676.356444
	Epoch 13....
Epoch has taken 0:00:21.146367
Number of used sentences in train = 359
Total loss for epoch 13: 673.800713
	Epoch 14....
Epoch has taken 0:00:21.146337
Number of used sentences in train = 359
Total loss for epoch 14: 673.532644
Epoch has taken 0:00:21.137489

==================================================================================================
	Training time : 1:00:10.713657
==================================================================================================
	Identification : 0.13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 71, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 44, 'lstmDropout': 0.39, 'denseActivation': 'tanh', 'wordDim': 138, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 138)
  (lstm): LSTM(155, 44, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=71, bias=True)
  (linear2): Linear(in_features=71, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11971.464600
validation loss after epoch 0 : 966.549479
	Epoch 1....
Epoch has taken 0:02:52.585901
Number of used sentences in train = 2811
Total loss for epoch 1: 8383.335033
validation loss after epoch 1 : 927.812595
	Epoch 2....
Epoch has taken 0:02:52.619692
Number of used sentences in train = 2811
Total loss for epoch 2: 7456.388569
validation loss after epoch 2 : 906.275846
	Epoch 3....
Epoch has taken 0:02:52.609934
Number of used sentences in train = 2811
Total loss for epoch 3: 6979.490821
validation loss after epoch 3 : 923.992407
	Epoch 4....
Epoch has taken 0:02:55.925005
Number of used sentences in train = 2811
Total loss for epoch 4: 6579.849315
validation loss after epoch 4 : 890.442501
	Epoch 5....
Epoch has taken 0:02:53.023727
Number of used sentences in train = 2811
Total loss for epoch 5: 6233.566428
validation loss after epoch 5 : 907.901449
	Epoch 6....
Epoch has taken 0:02:56.343039
Number of used sentences in train = 2811
Total loss for epoch 6: 6014.839846
validation loss after epoch 6 : 930.824073
	Epoch 7....
Epoch has taken 0:02:52.833878
Number of used sentences in train = 2811
Total loss for epoch 7: 5816.087500
validation loss after epoch 7 : 936.468399
	Epoch 8....
Epoch has taken 0:02:52.936476
Number of used sentences in train = 2811
Total loss for epoch 8: 5685.492826
validation loss after epoch 8 : 935.004809
	Epoch 9....
Epoch has taken 0:02:52.592317
Number of used sentences in train = 2811
Total loss for epoch 9: 5549.064278
validation loss after epoch 9 : 943.039105
	Epoch 10....
Epoch has taken 0:02:57.519942
Number of used sentences in train = 2811
Total loss for epoch 10: 5432.434122
validation loss after epoch 10 : 955.855919
	Epoch 11....
Epoch has taken 0:02:52.732784
Number of used sentences in train = 2811
Total loss for epoch 11: 5317.629806
validation loss after epoch 11 : 947.776457
	Epoch 12....
Epoch has taken 0:02:53.192544
Number of used sentences in train = 2811
Total loss for epoch 12: 5223.628014
validation loss after epoch 12 : 998.424054
	Epoch 13....
Epoch has taken 0:02:52.886125
Number of used sentences in train = 2811
Total loss for epoch 13: 5174.001166
validation loss after epoch 13 : 1007.645169
	Epoch 14....
Epoch has taken 0:02:54.485316
Number of used sentences in train = 2811
Total loss for epoch 14: 5130.366769
validation loss after epoch 14 : 1031.783765
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 138)
  (lstm): LSTM(155, 44, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=71, bias=True)
  (linear2): Linear(in_features=71, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.914658
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1776.413203
	Epoch 1....
Epoch has taken 0:00:18.256563
Number of used sentences in train = 313
Total loss for epoch 1: 794.876821
	Epoch 2....
Epoch has taken 0:00:18.244836
Number of used sentences in train = 313
Total loss for epoch 2: 715.171929
	Epoch 3....
Epoch has taken 0:00:18.248891
Number of used sentences in train = 313
Total loss for epoch 3: 630.575533
	Epoch 4....
Epoch has taken 0:00:18.257679
Number of used sentences in train = 313
Total loss for epoch 4: 579.613806
	Epoch 5....
Epoch has taken 0:00:18.252449
Number of used sentences in train = 313
Total loss for epoch 5: 576.966038
	Epoch 6....
Epoch has taken 0:00:18.252426
Number of used sentences in train = 313
Total loss for epoch 6: 543.567218
	Epoch 7....
Epoch has taken 0:00:18.260463
Number of used sentences in train = 313
Total loss for epoch 7: 533.273566
	Epoch 8....
Epoch has taken 0:00:18.255183
Number of used sentences in train = 313
Total loss for epoch 8: 529.627492
	Epoch 9....
Epoch has taken 0:00:18.256171
Number of used sentences in train = 313
Total loss for epoch 9: 534.259862
	Epoch 10....
Epoch has taken 0:00:18.248884
Number of used sentences in train = 313
Total loss for epoch 10: 516.838905
	Epoch 11....
Epoch has taken 0:00:18.253277
Number of used sentences in train = 313
Total loss for epoch 11: 523.079900
	Epoch 12....
Epoch has taken 0:00:18.257484
Number of used sentences in train = 313
Total loss for epoch 12: 511.343179
	Epoch 13....
Epoch has taken 0:00:18.252328
Number of used sentences in train = 313
Total loss for epoch 13: 511.881096
	Epoch 14....
Epoch has taken 0:00:18.275292
Number of used sentences in train = 313
Total loss for epoch 14: 509.186524
Epoch has taken 0:00:18.288272

==================================================================================================
	Training time : 0:48:03.554199
==================================================================================================
	Identification : 0.274

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 138)
  (lstm): LSTM(155, 44, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=71, bias=True)
  (linear2): Linear(in_features=71, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9023.819087
validation loss after epoch 0 : 720.152911
	Epoch 1....
Epoch has taken 0:01:58.267722
Number of used sentences in train = 2074
Total loss for epoch 1: 5637.306733
validation loss after epoch 1 : 636.401999
	Epoch 2....
Epoch has taken 0:01:58.291776
Number of used sentences in train = 2074
Total loss for epoch 2: 4877.400332
validation loss after epoch 2 : 702.123698
	Epoch 3....
Epoch has taken 0:01:58.295626
Number of used sentences in train = 2074
Total loss for epoch 3: 4419.852048
validation loss after epoch 3 : 693.331087
	Epoch 4....
Epoch has taken 0:01:58.298612
Number of used sentences in train = 2074
Total loss for epoch 4: 4150.703456
validation loss after epoch 4 : 680.490069
	Epoch 5....
Epoch has taken 0:01:58.280589
Number of used sentences in train = 2074
Total loss for epoch 5: 3894.062758
validation loss after epoch 5 : 723.579860
	Epoch 6....
Epoch has taken 0:01:58.293498
Number of used sentences in train = 2074
Total loss for epoch 6: 3760.470375
validation loss after epoch 6 : 717.198767
	Epoch 7....
Epoch has taken 0:01:58.250828
Number of used sentences in train = 2074
Total loss for epoch 7: 3662.873687
validation loss after epoch 7 : 665.812388
	Epoch 8....
Epoch has taken 0:01:58.256401
Number of used sentences in train = 2074
Total loss for epoch 8: 3605.630305
validation loss after epoch 8 : 762.810391
	Epoch 9....
Epoch has taken 0:01:58.314208
Number of used sentences in train = 2074
Total loss for epoch 9: 3550.518422
validation loss after epoch 9 : 716.599367
	Epoch 10....
Epoch has taken 0:01:58.336838
Number of used sentences in train = 2074
Total loss for epoch 10: 3473.215372
validation loss after epoch 10 : 768.106858
	Epoch 11....
Epoch has taken 0:02:01.650203
Number of used sentences in train = 2074
Total loss for epoch 11: 3446.542843
validation loss after epoch 11 : 799.171219
	Epoch 12....
Epoch has taken 0:01:58.484478
Number of used sentences in train = 2074
Total loss for epoch 12: 3404.695658
validation loss after epoch 12 : 793.938072
	Epoch 13....
Epoch has taken 0:01:58.509977
Number of used sentences in train = 2074
Total loss for epoch 13: 3373.403469
validation loss after epoch 13 : 746.778333
	Epoch 14....
Epoch has taken 0:02:00.348474
Number of used sentences in train = 2074
Total loss for epoch 14: 3337.170497
validation loss after epoch 14 : 730.968608
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 138)
  (lstm): LSTM(155, 44, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=71, bias=True)
  (linear2): Linear(in_features=71, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:02.175930
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1432.558628
	Epoch 1....
Epoch has taken 0:00:13.746857
Number of used sentences in train = 231
Total loss for epoch 1: 555.207091
	Epoch 2....
Epoch has taken 0:00:13.755974
Number of used sentences in train = 231
Total loss for epoch 2: 454.517384
	Epoch 3....
Epoch has taken 0:00:13.749622
Number of used sentences in train = 231
Total loss for epoch 3: 417.023770
	Epoch 4....
Epoch has taken 0:00:13.744396
Number of used sentences in train = 231
Total loss for epoch 4: 398.298436
	Epoch 5....
Epoch has taken 0:00:13.751524
Number of used sentences in train = 231
Total loss for epoch 5: 380.642255
	Epoch 6....
Epoch has taken 0:00:13.753630
Number of used sentences in train = 231
Total loss for epoch 6: 371.097525
	Epoch 7....
Epoch has taken 0:00:13.743298
Number of used sentences in train = 231
Total loss for epoch 7: 368.098069
	Epoch 8....
Epoch has taken 0:00:13.759551
Number of used sentences in train = 231
Total loss for epoch 8: 368.768349
	Epoch 9....
Epoch has taken 0:00:13.756402
Number of used sentences in train = 231
Total loss for epoch 9: 353.206662
	Epoch 10....
Epoch has taken 0:00:13.748419
Number of used sentences in train = 231
Total loss for epoch 10: 350.669315
	Epoch 11....
Epoch has taken 0:00:13.744723
Number of used sentences in train = 231
Total loss for epoch 11: 354.016140
	Epoch 12....
Epoch has taken 0:00:13.746817
Number of used sentences in train = 231
Total loss for epoch 12: 355.924841
	Epoch 13....
Epoch has taken 0:00:13.762774
Number of used sentences in train = 231
Total loss for epoch 13: 354.908758
	Epoch 14....
Epoch has taken 0:00:13.747987
Number of used sentences in train = 231
Total loss for epoch 14: 347.470791
Epoch has taken 0:00:13.749393

==================================================================================================
	Training time : 0:33:10.651374
==================================================================================================
	Identification : 0.374

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 138)
  (lstm): LSTM(155, 44, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=71, bias=True)
  (linear2): Linear(in_features=71, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17098.160871
validation loss after epoch 0 : 1352.606709
	Epoch 1....
Epoch has taken 0:04:17.515943
Number of used sentences in train = 3226
Total loss for epoch 1: 11425.208446
validation loss after epoch 1 : 1283.491771
	Epoch 2....
Epoch has taken 0:04:11.686972
Number of used sentences in train = 3226
Total loss for epoch 2: 10553.955411
validation loss after epoch 2 : 1279.887386
	Epoch 3....
Epoch has taken 0:03:54.798595
Number of used sentences in train = 3226
Total loss for epoch 3: 9949.677344
validation loss after epoch 3 : 1267.854618
	Epoch 4....
Epoch has taken 0:03:54.740905
Number of used sentences in train = 3226
Total loss for epoch 4: 9541.735333
validation loss after epoch 4 : 1263.074477
	Epoch 5....
Epoch has taken 0:03:55.282522
Number of used sentences in train = 3226
Total loss for epoch 5: 9194.276608
validation loss after epoch 5 : 1250.417926
	Epoch 6....
Epoch has taken 0:03:54.724437
Number of used sentences in train = 3226
Total loss for epoch 6: 8951.547839
validation loss after epoch 6 : 1314.699906
	Epoch 7....
Epoch has taken 0:03:54.959650
Number of used sentences in train = 3226
Total loss for epoch 7: 8733.518246
validation loss after epoch 7 : 1300.237804
	Epoch 8....
Epoch has taken 0:03:54.771265
Number of used sentences in train = 3226
Total loss for epoch 8: 8649.080039
validation loss after epoch 8 : 1315.954236
	Epoch 9....
Epoch has taken 0:03:57.673004
Number of used sentences in train = 3226
Total loss for epoch 9: 8449.745981
validation loss after epoch 9 : 1373.479134
	Epoch 10....
Epoch has taken 0:04:18.032019
Number of used sentences in train = 3226
Total loss for epoch 10: 8285.350551
validation loss after epoch 10 : 1344.876570
	Epoch 11....
Epoch has taken 0:03:55.469388
Number of used sentences in train = 3226
Total loss for epoch 11: 8239.331448
validation loss after epoch 11 : 1421.206958
	Epoch 12....
Epoch has taken 0:03:54.899050
Number of used sentences in train = 3226
Total loss for epoch 12: 8095.674357
validation loss after epoch 12 : 1407.639773
	Epoch 13....
Epoch has taken 0:03:55.013522
Number of used sentences in train = 3226
Total loss for epoch 13: 7999.382954
validation loss after epoch 13 : 1414.075454
	Epoch 14....
Epoch has taken 0:03:54.799247
Number of used sentences in train = 3226
Total loss for epoch 14: 7923.165471
validation loss after epoch 14 : 1446.526764
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 138)
  (lstm): LSTM(155, 44, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=704, out_features=71, bias=True)
  (linear2): Linear(in_features=71, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:55.106528
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2472.912220
	Epoch 1....
Epoch has taken 0:00:23.013674
Number of used sentences in train = 359
Total loss for epoch 1: 1169.351310
	Epoch 2....
Epoch has taken 0:00:23.250936
Number of used sentences in train = 359
Total loss for epoch 2: 1040.129257
	Epoch 3....
Epoch has taken 0:00:23.034908
Number of used sentences in train = 359
Total loss for epoch 3: 947.772894
	Epoch 4....
Epoch has taken 0:00:23.039687
Number of used sentences in train = 359
Total loss for epoch 4: 895.160903
	Epoch 5....
Epoch has taken 0:00:23.044711
Number of used sentences in train = 359
Total loss for epoch 5: 882.118755
	Epoch 6....
Epoch has taken 0:00:23.040586
Number of used sentences in train = 359
Total loss for epoch 6: 822.999732
	Epoch 7....
Epoch has taken 0:00:23.024945
Number of used sentences in train = 359
Total loss for epoch 7: 799.394224
	Epoch 8....
Epoch has taken 0:00:23.038058
Number of used sentences in train = 359
Total loss for epoch 8: 782.580631
	Epoch 9....
Epoch has taken 0:00:23.047397
Number of used sentences in train = 359
Total loss for epoch 9: 775.066943
	Epoch 10....
Epoch has taken 0:00:23.067271
Number of used sentences in train = 359
Total loss for epoch 10: 774.185777
	Epoch 11....
Epoch has taken 0:00:23.067527
Number of used sentences in train = 359
Total loss for epoch 11: 751.222731
	Epoch 12....
Epoch has taken 0:00:23.059261
Number of used sentences in train = 359
Total loss for epoch 12: 750.645931
	Epoch 13....
Epoch has taken 0:00:23.028963
Number of used sentences in train = 359
Total loss for epoch 13: 721.232152
	Epoch 14....
Epoch has taken 0:00:23.017602
Number of used sentences in train = 359
Total loss for epoch 14: 722.186209
Epoch has taken 0:00:23.005276

==================================================================================================
	Training time : 1:05:35.907712
==================================================================================================
	Identification : 0.433

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 20, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 44, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 67, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 185, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5788, 185)
  (lstm): LSTM(229, 67, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12618.484726
validation loss after epoch 0 : 1151.117241
	Epoch 1....
Epoch has taken 0:02:46.360270
Number of used sentences in train = 2811
Total loss for epoch 1: 8600.382852
validation loss after epoch 1 : 1112.508843
	Epoch 2....
Epoch has taken 0:02:46.251540
Number of used sentences in train = 2811
Total loss for epoch 2: 7052.404449
validation loss after epoch 2 : 1131.651720
	Epoch 3....
Epoch has taken 0:02:51.966591
Number of used sentences in train = 2811
Total loss for epoch 3: 6082.440239
validation loss after epoch 3 : 1181.321133
	Epoch 4....
Epoch has taken 0:03:00.132679
Number of used sentences in train = 2811
Total loss for epoch 4: 5477.959988
validation loss after epoch 4 : 1279.903744
	Epoch 5....
Epoch has taken 0:02:45.245711
Number of used sentences in train = 2811
Total loss for epoch 5: 5164.169345
validation loss after epoch 5 : 1338.066056
	Epoch 6....
Epoch has taken 0:02:46.417507
Number of used sentences in train = 2811
Total loss for epoch 6: 4978.966323
validation loss after epoch 6 : 1407.099655
	Epoch 7....
Epoch has taken 0:02:46.502631
Number of used sentences in train = 2811
Total loss for epoch 7: 4850.137148
validation loss after epoch 7 : 1460.936358
	Epoch 8....
Epoch has taken 0:02:47.024334
Number of used sentences in train = 2811
Total loss for epoch 8: 4767.342986
validation loss after epoch 8 : 1510.129611
	Epoch 9....
Epoch has taken 0:02:53.137743
Number of used sentences in train = 2811
Total loss for epoch 9: 4709.286889
validation loss after epoch 9 : 1530.428597
	Epoch 10....
Epoch has taken 0:02:48.066381
Number of used sentences in train = 2811
Total loss for epoch 10: 4678.167435
validation loss after epoch 10 : 1561.400256
	Epoch 11....
Epoch has taken 0:02:48.920942
Number of used sentences in train = 2811
Total loss for epoch 11: 4659.977443
validation loss after epoch 11 : 1584.082756
	Epoch 12....
Epoch has taken 0:02:47.735471
Number of used sentences in train = 2811
Total loss for epoch 12: 4634.721464
validation loss after epoch 12 : 1588.802996
	Epoch 13....
Epoch has taken 0:02:47.711736
Number of used sentences in train = 2811
Total loss for epoch 13: 4604.619498
validation loss after epoch 13 : 1608.519353
	Epoch 14....
Epoch has taken 0:02:46.308466
Number of used sentences in train = 2811
Total loss for epoch 14: 4588.309307
validation loss after epoch 14 : 1632.709279
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5788, 185)
  (lstm): LSTM(229, 67, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:44.253672
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1580.442222
	Epoch 1....
Epoch has taken 0:00:17.623004
Number of used sentences in train = 313
Total loss for epoch 1: 838.740545
	Epoch 2....
Epoch has taken 0:00:17.625508
Number of used sentences in train = 313
Total loss for epoch 2: 663.345055
	Epoch 3....
Epoch has taken 0:00:17.628906
Number of used sentences in train = 313
Total loss for epoch 3: 601.793154
	Epoch 4....
Epoch has taken 0:00:17.624191
Number of used sentences in train = 313
Total loss for epoch 4: 565.174721
	Epoch 5....
Epoch has taken 0:00:17.430297
Number of used sentences in train = 313
Total loss for epoch 5: 549.253522
	Epoch 6....
Epoch has taken 0:00:17.640861
Number of used sentences in train = 313
Total loss for epoch 6: 538.911565
	Epoch 7....
Epoch has taken 0:00:17.644638
Number of used sentences in train = 313
Total loss for epoch 7: 534.682525
	Epoch 8....
Epoch has taken 0:00:17.638273
Number of used sentences in train = 313
Total loss for epoch 8: 532.974133
	Epoch 9....
Epoch has taken 0:00:17.636310
Number of used sentences in train = 313
Total loss for epoch 9: 529.843859
	Epoch 10....
Epoch has taken 0:00:17.638395
Number of used sentences in train = 313
Total loss for epoch 10: 527.591181
	Epoch 11....
Epoch has taken 0:00:17.648778
Number of used sentences in train = 313
Total loss for epoch 11: 526.684978
	Epoch 12....
Epoch has taken 0:00:17.644177
Number of used sentences in train = 313
Total loss for epoch 12: 525.022861
	Epoch 13....
Epoch has taken 0:00:17.634632
Number of used sentences in train = 313
Total loss for epoch 13: 523.368353
	Epoch 14....
Epoch has taken 0:00:17.651208
Number of used sentences in train = 313
Total loss for epoch 14: 522.253785
Epoch has taken 0:00:17.668671

==================================================================================================
	Training time : 0:46:30.921323
==================================================================================================
	Identification : 0.116

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5620, 185)
  (lstm): LSTM(229, 67, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10078.794139
validation loss after epoch 0 : 990.990988
	Epoch 1....
Epoch has taken 0:01:53.601602
Number of used sentences in train = 2074
Total loss for epoch 1: 6180.774009
validation loss after epoch 1 : 952.260438
	Epoch 2....
Epoch has taken 0:01:55.046347
Number of used sentences in train = 2074
Total loss for epoch 2: 4887.709692
validation loss after epoch 2 : 1011.109393
	Epoch 3....
Epoch has taken 0:02:02.900284
Number of used sentences in train = 2074
Total loss for epoch 3: 4159.640348
validation loss after epoch 3 : 1000.584374
	Epoch 4....
Epoch has taken 0:01:53.870956
Number of used sentences in train = 2074
Total loss for epoch 4: 3739.053431
validation loss after epoch 4 : 1073.155341
	Epoch 5....
Epoch has taken 0:01:59.375233
Number of used sentences in train = 2074
Total loss for epoch 5: 3531.434693
validation loss after epoch 5 : 1153.145982
	Epoch 6....
Epoch has taken 0:01:53.921951
Number of used sentences in train = 2074
Total loss for epoch 6: 3405.737136
validation loss after epoch 6 : 1181.277232
	Epoch 7....
Epoch has taken 0:01:53.795407
Number of used sentences in train = 2074
Total loss for epoch 7: 3328.732584
validation loss after epoch 7 : 1236.044488
	Epoch 8....
Epoch has taken 0:01:53.706569
Number of used sentences in train = 2074
Total loss for epoch 8: 3282.452796
validation loss after epoch 8 : 1233.247539
	Epoch 9....
Epoch has taken 0:01:53.790926
Number of used sentences in train = 2074
Total loss for epoch 9: 3243.208622
validation loss after epoch 9 : 1271.587293
	Epoch 10....
Epoch has taken 0:01:53.964294
Number of used sentences in train = 2074
Total loss for epoch 10: 3225.941763
validation loss after epoch 10 : 1284.773250
	Epoch 11....
Epoch has taken 0:01:53.788464
Number of used sentences in train = 2074
Total loss for epoch 11: 3212.605326
validation loss after epoch 11 : 1309.606153
	Epoch 12....
Epoch has taken 0:01:53.794642
Number of used sentences in train = 2074
Total loss for epoch 12: 3204.263735
validation loss after epoch 12 : 1326.064759
	Epoch 13....
Epoch has taken 0:02:03.307401
Number of used sentences in train = 2074
Total loss for epoch 13: 3201.642203
validation loss after epoch 13 : 1346.513662
	Epoch 14....
Epoch has taken 0:02:00.831590
Number of used sentences in train = 2074
Total loss for epoch 14: 3191.694190
validation loss after epoch 14 : 1354.080377
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5620, 185)
  (lstm): LSTM(229, 67, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:00.588791
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1634.526148
	Epoch 1....
Epoch has taken 0:00:11.610650
Number of used sentences in train = 231
Total loss for epoch 1: 615.457609
	Epoch 2....
Epoch has taken 0:00:11.619270
Number of used sentences in train = 231
Total loss for epoch 2: 468.860958
	Epoch 3....
Epoch has taken 0:00:11.602099
Number of used sentences in train = 231
Total loss for epoch 3: 421.260880
	Epoch 4....
Epoch has taken 0:00:11.606321
Number of used sentences in train = 231
Total loss for epoch 4: 396.850182
	Epoch 5....
Epoch has taken 0:00:11.602982
Number of used sentences in train = 231
Total loss for epoch 5: 377.596679
	Epoch 6....
Epoch has taken 0:00:11.590998
Number of used sentences in train = 231
Total loss for epoch 6: 364.620316
	Epoch 7....
Epoch has taken 0:00:11.615973
Number of used sentences in train = 231
Total loss for epoch 7: 356.945989
	Epoch 8....
Epoch has taken 0:00:11.620451
Number of used sentences in train = 231
Total loss for epoch 8: 355.334382
	Epoch 9....
Epoch has taken 0:00:11.894413
Number of used sentences in train = 231
Total loss for epoch 9: 351.541656
	Epoch 10....
Epoch has taken 0:00:12.940711
Number of used sentences in train = 231
Total loss for epoch 10: 349.822651
	Epoch 11....
Epoch has taken 0:00:12.943782
Number of used sentences in train = 231
Total loss for epoch 11: 349.162316
	Epoch 12....
Epoch has taken 0:00:12.941902
Number of used sentences in train = 231
Total loss for epoch 12: 348.641416
	Epoch 13....
Epoch has taken 0:00:12.939891
Number of used sentences in train = 231
Total loss for epoch 13: 348.287080
	Epoch 14....
Epoch has taken 0:00:12.943074
Number of used sentences in train = 231
Total loss for epoch 14: 347.997599
Epoch has taken 0:00:12.947952

==================================================================================================
	Training time : 0:32:09.049291
==================================================================================================
	Identification : 0.45

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 44)
  (w_embeddings): Embedding(6810, 185)
  (lstm): LSTM(229, 67, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16372.538838
validation loss after epoch 0 : 1534.078802
	Epoch 1....
Epoch has taken 0:03:56.297473
Number of used sentences in train = 3226
Total loss for epoch 1: 12050.013518
validation loss after epoch 1 : 1495.880470
	Epoch 2....
Epoch has taken 0:03:39.232878
Number of used sentences in train = 3226
Total loss for epoch 2: 10096.744939
validation loss after epoch 2 : 1592.403681
	Epoch 3....
Epoch has taken 0:03:43.371114
Number of used sentences in train = 3226
Total loss for epoch 3: 8807.058874
validation loss after epoch 3 : 1696.085340
	Epoch 4....
Epoch has taken 0:03:36.838832
Number of used sentences in train = 3226
Total loss for epoch 4: 7881.325298
validation loss after epoch 4 : 1834.260313
	Epoch 5....
Epoch has taken 0:03:40.423025
Number of used sentences in train = 3226
Total loss for epoch 5: 7273.227669
validation loss after epoch 5 : 1949.000418
	Epoch 6....
Epoch has taken 0:03:39.447226
Number of used sentences in train = 3226
Total loss for epoch 6: 6861.214379
validation loss after epoch 6 : 2019.194801
	Epoch 7....
Epoch has taken 0:03:38.792538
Number of used sentences in train = 3226
Total loss for epoch 7: 6602.584633
validation loss after epoch 7 : 2175.855528
	Epoch 8....
Epoch has taken 0:03:46.846924
Number of used sentences in train = 3226
Total loss for epoch 8: 6444.735347
validation loss after epoch 8 : 2275.966956
	Epoch 9....
Epoch has taken 0:03:42.591484
Number of used sentences in train = 3226
Total loss for epoch 9: 6356.203478
validation loss after epoch 9 : 2368.132655
	Epoch 10....
Epoch has taken 0:03:44.146611
Number of used sentences in train = 3226
Total loss for epoch 10: 6281.797733
validation loss after epoch 10 : 2402.698441
	Epoch 11....
Epoch has taken 0:03:57.596843
Number of used sentences in train = 3226
Total loss for epoch 11: 6238.729321
validation loss after epoch 11 : 2404.924834
	Epoch 12....
Epoch has taken 0:04:04.991689
Number of used sentences in train = 3226
Total loss for epoch 12: 6221.274772
validation loss after epoch 12 : 2458.815413
	Epoch 13....
Epoch has taken 0:04:04.400745
Number of used sentences in train = 3226
Total loss for epoch 13: 6197.929804
validation loss after epoch 13 : 2507.259296
	Epoch 14....
Epoch has taken 0:03:54.103030
Number of used sentences in train = 3226
Total loss for epoch 14: 6181.254805
validation loss after epoch 14 : 2516.058084
	TransitionClassifier(
  (p_embeddings): Embedding(13, 44)
  (w_embeddings): Embedding(6810, 185)
  (lstm): LSTM(229, 67, bidirectional=True)
  (linear1): Linear(in_features=1072, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:42.666349
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2502.510965
	Epoch 1....
Epoch has taken 0:00:21.787874
Number of used sentences in train = 359
Total loss for epoch 1: 1184.904417
	Epoch 2....
Epoch has taken 0:00:21.778177
Number of used sentences in train = 359
Total loss for epoch 2: 882.952282
	Epoch 3....
Epoch has taken 0:00:21.783379
Number of used sentences in train = 359
Total loss for epoch 3: 761.741111
	Epoch 4....
Epoch has taken 0:00:21.735122
Number of used sentences in train = 359
Total loss for epoch 4: 713.954562
	Epoch 5....
Epoch has taken 0:00:21.159160
Number of used sentences in train = 359
Total loss for epoch 5: 699.515354
	Epoch 6....
Epoch has taken 0:00:21.151403
Number of used sentences in train = 359
Total loss for epoch 6: 688.295738
	Epoch 7....
Epoch has taken 0:00:21.172054
Number of used sentences in train = 359
Total loss for epoch 7: 684.939000
	Epoch 8....
Epoch has taken 0:00:21.169408
Number of used sentences in train = 359
Total loss for epoch 8: 681.884193
	Epoch 9....
Epoch has taken 0:00:21.140629
Number of used sentences in train = 359
Total loss for epoch 9: 679.466139
	Epoch 10....
Epoch has taken 0:00:21.141066
Number of used sentences in train = 359
Total loss for epoch 10: 677.115396
	Epoch 11....
Epoch has taken 0:00:21.139407
Number of used sentences in train = 359
Total loss for epoch 11: 674.029374
	Epoch 12....
Epoch has taken 0:00:21.129887
Number of used sentences in train = 359
Total loss for epoch 12: 673.112166
	Epoch 13....
Epoch has taken 0:00:21.146312
Number of used sentences in train = 359
Total loss for epoch 13: 672.782912
	Epoch 14....
Epoch has taken 0:00:21.158224
Number of used sentences in train = 359
Total loss for epoch 14: 672.549413
Epoch has taken 0:00:21.178017

==================================================================================================
	Training time : 1:02:12.175441
==================================================================================================
	Identification : 0.183

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 21, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 73, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 94, 'lstmDropout': 0.25, 'denseActivation': 'tanh', 'wordDim': 189, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 73)
  (w_embeddings): Embedding(5917, 189)
  (lstm): LSTM(262, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14332.870479
validation loss after epoch 0 : 1236.940802
	Epoch 1....
Epoch has taken 0:02:41.566546
Number of used sentences in train = 2811
Total loss for epoch 1: 9473.816415
validation loss after epoch 1 : 1186.667173
	Epoch 2....
Epoch has taken 0:02:41.837937
Number of used sentences in train = 2811
Total loss for epoch 2: 7763.699205
validation loss after epoch 2 : 1182.410444
	Epoch 3....
Epoch has taken 0:02:41.893726
Number of used sentences in train = 2811
Total loss for epoch 3: 6666.106114
validation loss after epoch 3 : 1224.289907
	Epoch 4....
Epoch has taken 0:02:43.336975
Number of used sentences in train = 2811
Total loss for epoch 4: 5987.711824
validation loss after epoch 4 : 1285.124075
	Epoch 5....
Epoch has taken 0:02:41.702155
Number of used sentences in train = 2811
Total loss for epoch 5: 5573.652591
validation loss after epoch 5 : 1342.948365
	Epoch 6....
Epoch has taken 0:02:41.777879
Number of used sentences in train = 2811
Total loss for epoch 6: 5359.929717
validation loss after epoch 6 : 1361.186697
	Epoch 7....
Epoch has taken 0:02:46.400173
Number of used sentences in train = 2811
Total loss for epoch 7: 5203.158521
validation loss after epoch 7 : 1414.108922
	Epoch 8....
Epoch has taken 0:02:41.862560
Number of used sentences in train = 2811
Total loss for epoch 8: 5096.328649
validation loss after epoch 8 : 1472.492855
	Epoch 9....
Epoch has taken 0:02:42.433888
Number of used sentences in train = 2811
Total loss for epoch 9: 4975.957081
validation loss after epoch 9 : 1506.990068
	Epoch 10....
Epoch has taken 0:02:41.921719
Number of used sentences in train = 2811
Total loss for epoch 10: 4889.470486
validation loss after epoch 10 : 1516.062683
	Epoch 11....
Epoch has taken 0:02:42.520011
Number of used sentences in train = 2811
Total loss for epoch 11: 4842.485555
validation loss after epoch 11 : 1560.068811
	Epoch 12....
Epoch has taken 0:02:42.096424
Number of used sentences in train = 2811
Total loss for epoch 12: 4798.403635
validation loss after epoch 12 : 1569.133406
	Epoch 13....
Epoch has taken 0:02:41.979003
Number of used sentences in train = 2811
Total loss for epoch 13: 4751.141145
validation loss after epoch 13 : 1621.892452
	Epoch 14....
Epoch has taken 0:02:43.134445
Number of used sentences in train = 2811
Total loss for epoch 14: 4720.143582
validation loss after epoch 14 : 1590.166156
	TransitionClassifier(
  (p_embeddings): Embedding(18, 73)
  (w_embeddings): Embedding(5917, 189)
  (lstm): LSTM(262, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:41.795922
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1684.639506
	Epoch 1....
Epoch has taken 0:00:17.121190
Number of used sentences in train = 313
Total loss for epoch 1: 947.468201
	Epoch 2....
Epoch has taken 0:00:17.130972
Number of used sentences in train = 313
Total loss for epoch 2: 730.870274
	Epoch 3....
Epoch has taken 0:00:17.113395
Number of used sentences in train = 313
Total loss for epoch 3: 637.467035
	Epoch 4....
Epoch has taken 0:00:17.118419
Number of used sentences in train = 313
Total loss for epoch 4: 601.877715
	Epoch 5....
Epoch has taken 0:00:17.115081
Number of used sentences in train = 313
Total loss for epoch 5: 595.208093
	Epoch 6....
Epoch has taken 0:00:17.126330
Number of used sentences in train = 313
Total loss for epoch 6: 581.795591
	Epoch 7....
Epoch has taken 0:00:17.108146
Number of used sentences in train = 313
Total loss for epoch 7: 573.998232
	Epoch 8....
Epoch has taken 0:00:17.118605
Number of used sentences in train = 313
Total loss for epoch 8: 566.604863
	Epoch 9....
Epoch has taken 0:00:17.113272
Number of used sentences in train = 313
Total loss for epoch 9: 559.990296
	Epoch 10....
Epoch has taken 0:00:17.106681
Number of used sentences in train = 313
Total loss for epoch 10: 553.148646
	Epoch 11....
Epoch has taken 0:00:17.116945
Number of used sentences in train = 313
Total loss for epoch 11: 545.207547
	Epoch 12....
Epoch has taken 0:00:18.713366
Number of used sentences in train = 313
Total loss for epoch 12: 539.694792
	Epoch 13....
Epoch has taken 0:00:19.513096
Number of used sentences in train = 313
Total loss for epoch 13: 537.067518
	Epoch 14....
Epoch has taken 0:00:19.509799
Number of used sentences in train = 313
Total loss for epoch 14: 536.275994
Epoch has taken 0:00:18.966612

==================================================================================================
	Training time : 0:45:01.760116
==================================================================================================
	Identification : 0.486

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 73)
  (w_embeddings): Embedding(5638, 189)
  (lstm): LSTM(262, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10227.194594
validation loss after epoch 0 : 992.781759
	Epoch 1....
Epoch has taken 0:01:50.736279
Number of used sentences in train = 2074
Total loss for epoch 1: 6474.657014
validation loss after epoch 1 : 943.845808
	Epoch 2....
Epoch has taken 0:01:50.695364
Number of used sentences in train = 2074
Total loss for epoch 2: 5329.258689
validation loss after epoch 2 : 1007.659383
	Epoch 3....
Epoch has taken 0:01:50.673152
Number of used sentences in train = 2074
Total loss for epoch 3: 4400.503048
validation loss after epoch 3 : 1149.218606
	Epoch 4....
Epoch has taken 0:01:50.737471
Number of used sentences in train = 2074
Total loss for epoch 4: 3921.020278
validation loss after epoch 4 : 1116.041101
	Epoch 5....
Epoch has taken 0:01:50.858282
Number of used sentences in train = 2074
Total loss for epoch 5: 3622.764309
validation loss after epoch 5 : 1154.680936
	Epoch 6....
Epoch has taken 0:01:53.595325
Number of used sentences in train = 2074
Total loss for epoch 6: 3462.130328
validation loss after epoch 6 : 1173.009282
	Epoch 7....
Epoch has taken 0:01:50.738608
Number of used sentences in train = 2074
Total loss for epoch 7: 3392.249603
validation loss after epoch 7 : 1216.471413
	Epoch 8....
Epoch has taken 0:01:50.653177
Number of used sentences in train = 2074
Total loss for epoch 8: 3350.979041
validation loss after epoch 8 : 1238.575387
	Epoch 9....
Epoch has taken 0:01:50.701876
Number of used sentences in train = 2074
Total loss for epoch 9: 3327.787977
validation loss after epoch 9 : 1268.610243
	Epoch 10....
Epoch has taken 0:01:50.787464
Number of used sentences in train = 2074
Total loss for epoch 10: 3310.844998
validation loss after epoch 10 : 1284.263731
	Epoch 11....
Epoch has taken 0:01:50.705574
Number of used sentences in train = 2074
Total loss for epoch 11: 3301.036481
validation loss after epoch 11 : 1294.176619
	Epoch 12....
Epoch has taken 0:01:50.709962
Number of used sentences in train = 2074
Total loss for epoch 12: 3291.627456
validation loss after epoch 12 : 1312.011399
	Epoch 13....
Epoch has taken 0:01:54.063322
Number of used sentences in train = 2074
Total loss for epoch 13: 3284.546172
validation loss after epoch 13 : 1330.844918
	Epoch 14....
Epoch has taken 0:01:53.442410
Number of used sentences in train = 2074
Total loss for epoch 14: 3277.145692
validation loss after epoch 14 : 1349.347310
	TransitionClassifier(
  (p_embeddings): Embedding(18, 73)
  (w_embeddings): Embedding(5638, 189)
  (lstm): LSTM(262, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.612883
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1505.144740
	Epoch 1....
Epoch has taken 0:00:11.298756
Number of used sentences in train = 231
Total loss for epoch 1: 689.646559
	Epoch 2....
Epoch has taken 0:00:11.302646
Number of used sentences in train = 231
Total loss for epoch 2: 493.547004
	Epoch 3....
Epoch has taken 0:00:11.300661
Number of used sentences in train = 231
Total loss for epoch 3: 434.945376
	Epoch 4....
Epoch has taken 0:00:11.295421
Number of used sentences in train = 231
Total loss for epoch 4: 404.595991
	Epoch 5....
Epoch has taken 0:00:11.304274
Number of used sentences in train = 231
Total loss for epoch 5: 396.193186
	Epoch 6....
Epoch has taken 0:00:11.301772
Number of used sentences in train = 231
Total loss for epoch 6: 388.359954
	Epoch 7....
Epoch has taken 0:00:11.285961
Number of used sentences in train = 231
Total loss for epoch 7: 381.349969
	Epoch 8....
Epoch has taken 0:00:12.767673
Number of used sentences in train = 231
Total loss for epoch 8: 375.765735
	Epoch 9....
Epoch has taken 0:00:13.234927
Number of used sentences in train = 231
Total loss for epoch 9: 372.619749
	Epoch 10....
Epoch has taken 0:00:11.306196
Number of used sentences in train = 231
Total loss for epoch 10: 370.669016
	Epoch 11....
Epoch has taken 0:00:11.297669
Number of used sentences in train = 231
Total loss for epoch 11: 369.579777
	Epoch 12....
Epoch has taken 0:00:11.309173
Number of used sentences in train = 231
Total loss for epoch 12: 368.456954
	Epoch 13....
Epoch has taken 0:00:11.296497
Number of used sentences in train = 231
Total loss for epoch 13: 367.710464
	Epoch 14....
Epoch has taken 0:00:11.297736
Number of used sentences in train = 231
Total loss for epoch 14: 366.716282
Epoch has taken 0:00:11.307430

==================================================================================================
	Training time : 0:30:53.997262
==================================================================================================
	Identification : 0.217

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 73)
  (w_embeddings): Embedding(6911, 189)
  (lstm): LSTM(262, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 22762.270378
validation loss after epoch 0 : 1737.595898
	Epoch 1....
Epoch has taken 0:03:39.758998
Number of used sentences in train = 3226
Total loss for epoch 1: 14276.064972
validation loss after epoch 1 : 1605.608920
	Epoch 2....
Epoch has taken 0:03:38.399420
Number of used sentences in train = 3226
Total loss for epoch 2: 12187.314154
validation loss after epoch 2 : 1568.082854
	Epoch 3....
Epoch has taken 0:03:38.207616
Number of used sentences in train = 3226
Total loss for epoch 3: 10704.502429
validation loss after epoch 3 : 1568.208060
	Epoch 4....
Epoch has taken 0:03:38.566328
Number of used sentences in train = 3226
Total loss for epoch 4: 9708.472411
validation loss after epoch 4 : 1625.174572
	Epoch 5....
Epoch has taken 0:03:38.216080
Number of used sentences in train = 3226
Total loss for epoch 5: 8846.801269
validation loss after epoch 5 : 1681.232792
	Epoch 6....
Epoch has taken 0:04:09.989763
Number of used sentences in train = 3226
Total loss for epoch 6: 8193.355052
validation loss after epoch 6 : 1778.497709
	Epoch 7....
Epoch has taken 0:03:38.413605
Number of used sentences in train = 3226
Total loss for epoch 7: 7688.495171
validation loss after epoch 7 : 1787.135763
	Epoch 8....
Epoch has taken 0:03:38.371861
Number of used sentences in train = 3226
Total loss for epoch 8: 7286.649802
validation loss after epoch 8 : 1849.784458
	Epoch 9....
Epoch has taken 0:03:49.594480
Number of used sentences in train = 3226
Total loss for epoch 9: 7004.950675
validation loss after epoch 9 : 1944.981328
	Epoch 10....
Epoch has taken 0:03:38.496993
Number of used sentences in train = 3226
Total loss for epoch 10: 6798.741778
validation loss after epoch 10 : 1978.989372
	Epoch 11....
Epoch has taken 0:03:40.015474
Number of used sentences in train = 3226
Total loss for epoch 11: 6649.269193
validation loss after epoch 11 : 2040.535439
	Epoch 12....
Epoch has taken 0:03:44.196308
Number of used sentences in train = 3226
Total loss for epoch 12: 6540.549336
validation loss after epoch 12 : 2177.812998
	Epoch 13....
Epoch has taken 0:03:44.666951
Number of used sentences in train = 3226
Total loss for epoch 13: 6438.010304
validation loss after epoch 13 : 2187.887541
	Epoch 14....
Epoch has taken 0:03:43.077556
Number of used sentences in train = 3226
Total loss for epoch 14: 6368.119804
validation loss after epoch 14 : 2218.240507
	TransitionClassifier(
  (p_embeddings): Embedding(13, 73)
  (w_embeddings): Embedding(6911, 189)
  (lstm): LSTM(262, 94, bidirectional=True)
  (linear1): Linear(in_features=1504, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:38.555779
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2438.564577
	Epoch 1....
Epoch has taken 0:00:21.289740
Number of used sentences in train = 359
Total loss for epoch 1: 1429.941439
	Epoch 2....
Epoch has taken 0:00:21.288324
Number of used sentences in train = 359
Total loss for epoch 2: 1131.236231
	Epoch 3....
Epoch has taken 0:00:21.314988
Number of used sentences in train = 359
Total loss for epoch 3: 1011.713309
	Epoch 4....
Epoch has taken 0:00:22.031834
Number of used sentences in train = 359
Total loss for epoch 4: 923.763921
	Epoch 5....
Epoch has taken 0:00:22.083701
Number of used sentences in train = 359
Total loss for epoch 5: 852.398858
	Epoch 6....
Epoch has taken 0:00:21.292483
Number of used sentences in train = 359
Total loss for epoch 6: 796.130415
	Epoch 7....
Epoch has taken 0:00:21.307789
Number of used sentences in train = 359
Total loss for epoch 7: 761.804460
	Epoch 8....
Epoch has taken 0:00:21.321633
Number of used sentences in train = 359
Total loss for epoch 8: 739.839942
	Epoch 9....
Epoch has taken 0:00:21.305467
Number of used sentences in train = 359
Total loss for epoch 9: 719.773232
	Epoch 10....
Epoch has taken 0:00:21.752121
Number of used sentences in train = 359
Total loss for epoch 10: 711.960090
	Epoch 11....
Epoch has taken 0:00:21.301854
Number of used sentences in train = 359
Total loss for epoch 11: 702.296863
	Epoch 12....
Epoch has taken 0:00:21.307772
Number of used sentences in train = 359
Total loss for epoch 12: 696.503403
	Epoch 13....
Epoch has taken 0:00:21.295610
Number of used sentences in train = 359
Total loss for epoch 13: 692.251398
	Epoch 14....
Epoch has taken 0:00:21.322157
Number of used sentences in train = 359
Total loss for epoch 14: 689.833151
Epoch has taken 0:00:21.333596

==================================================================================================
	Training time : 1:01:00.742862
==================================================================================================
	Identification : 0.036

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 27, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 28, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 21, 'lstmDropout': 0.39, 'denseActivation': 'tanh', 'wordDim': 126, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(5856, 126)
  (lstm): LSTM(154, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12883.916174
validation loss after epoch 0 : 1170.184211
	Epoch 1....
Epoch has taken 0:02:43.558673
Number of used sentences in train = 2811
Total loss for epoch 1: 8627.099221
validation loss after epoch 1 : 1141.830544
	Epoch 2....
Epoch has taken 0:02:42.875649
Number of used sentences in train = 2811
Total loss for epoch 2: 7072.900564
validation loss after epoch 2 : 1168.178908
	Epoch 3....
Epoch has taken 0:02:43.446841
Number of used sentences in train = 2811
Total loss for epoch 3: 6208.517721
validation loss after epoch 3 : 1303.838508
	Epoch 4....
Epoch has taken 0:02:43.671947
Number of used sentences in train = 2811
Total loss for epoch 4: 5640.946841
validation loss after epoch 4 : 1391.660304
	Epoch 5....
Epoch has taken 0:02:41.515237
Number of used sentences in train = 2811
Total loss for epoch 5: 5343.493300
validation loss after epoch 5 : 1450.569957
	Epoch 6....
Epoch has taken 0:02:45.018535
Number of used sentences in train = 2811
Total loss for epoch 6: 5102.206114
validation loss after epoch 6 : 1486.267336
	Epoch 7....
Epoch has taken 0:02:45.581308
Number of used sentences in train = 2811
Total loss for epoch 7: 4949.110571
validation loss after epoch 7 : 1530.656203
	Epoch 8....
Epoch has taken 0:02:41.541326
Number of used sentences in train = 2811
Total loss for epoch 8: 4827.469853
validation loss after epoch 8 : 1617.323167
	Epoch 9....
Epoch has taken 0:02:46.241624
Number of used sentences in train = 2811
Total loss for epoch 9: 4751.584167
validation loss after epoch 9 : 1651.806292
	Epoch 10....
Epoch has taken 0:02:41.447165
Number of used sentences in train = 2811
Total loss for epoch 10: 4687.546863
validation loss after epoch 10 : 1683.502838
	Epoch 11....
Epoch has taken 0:02:41.601567
Number of used sentences in train = 2811
Total loss for epoch 11: 4655.258137
validation loss after epoch 11 : 1707.003699
	Epoch 12....
Epoch has taken 0:02:46.441578
Number of used sentences in train = 2811
Total loss for epoch 12: 4614.116907
validation loss after epoch 12 : 1757.238849
	Epoch 13....
Epoch has taken 0:02:41.801068
Number of used sentences in train = 2811
Total loss for epoch 13: 4593.191835
validation loss after epoch 13 : 1797.461528
	Epoch 14....
Epoch has taken 0:02:42.713900
Number of used sentences in train = 2811
Total loss for epoch 14: 4563.572930
validation loss after epoch 14 : 1809.824721
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(5856, 126)
  (lstm): LSTM(154, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:41.781906
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1544.410026
	Epoch 1....
Epoch has taken 0:00:17.084344
Number of used sentences in train = 313
Total loss for epoch 1: 827.772513
	Epoch 2....
Epoch has taken 0:00:17.083368
Number of used sentences in train = 313
Total loss for epoch 2: 657.010744
	Epoch 3....
Epoch has taken 0:00:17.313061
Number of used sentences in train = 313
Total loss for epoch 3: 582.562319
	Epoch 4....
Epoch has taken 0:00:17.053330
Number of used sentences in train = 313
Total loss for epoch 4: 543.938391
	Epoch 5....
Epoch has taken 0:00:17.070367
Number of used sentences in train = 313
Total loss for epoch 5: 527.450997
	Epoch 6....
Epoch has taken 0:00:17.617023
Number of used sentences in train = 313
Total loss for epoch 6: 519.475140
	Epoch 7....
Epoch has taken 0:00:17.709137
Number of used sentences in train = 313
Total loss for epoch 7: 516.073883
	Epoch 8....
Epoch has taken 0:00:17.077757
Number of used sentences in train = 313
Total loss for epoch 8: 513.713879
	Epoch 9....
Epoch has taken 0:00:17.076600
Number of used sentences in train = 313
Total loss for epoch 9: 512.599919
	Epoch 10....
Epoch has taken 0:00:17.080094
Number of used sentences in train = 313
Total loss for epoch 10: 512.361263
	Epoch 11....
Epoch has taken 0:00:17.077940
Number of used sentences in train = 313
Total loss for epoch 11: 510.460172
	Epoch 12....
Epoch has taken 0:00:17.067323
Number of used sentences in train = 313
Total loss for epoch 12: 509.867495
	Epoch 13....
Epoch has taken 0:00:17.084034
Number of used sentences in train = 313
Total loss for epoch 13: 509.445160
	Epoch 14....
Epoch has taken 0:00:17.088261
Number of used sentences in train = 313
Total loss for epoch 14: 509.217743
Epoch has taken 0:00:17.074298

==================================================================================================
	Training time : 0:45:07.305733
==================================================================================================
	Identification : 0.218

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(5637, 126)
  (lstm): LSTM(154, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10038.526869
validation loss after epoch 0 : 990.277972
	Epoch 1....
Epoch has taken 0:01:50.213412
Number of used sentences in train = 2074
Total loss for epoch 1: 6439.842154
validation loss after epoch 1 : 968.677055
	Epoch 2....
Epoch has taken 0:01:50.146464
Number of used sentences in train = 2074
Total loss for epoch 2: 5167.481694
validation loss after epoch 2 : 992.505837
	Epoch 3....
Epoch has taken 0:01:50.323337
Number of used sentences in train = 2074
Total loss for epoch 3: 4430.061336
validation loss after epoch 3 : 1006.468690
	Epoch 4....
Epoch has taken 0:01:51.976292
Number of used sentences in train = 2074
Total loss for epoch 4: 3992.548659
validation loss after epoch 4 : 1159.336978
	Epoch 5....
Epoch has taken 0:01:50.195850
Number of used sentences in train = 2074
Total loss for epoch 5: 3691.405449
validation loss after epoch 5 : 1210.533842
	Epoch 6....
Epoch has taken 0:01:50.148903
Number of used sentences in train = 2074
Total loss for epoch 6: 3485.118839
validation loss after epoch 6 : 1238.698771
	Epoch 7....
Epoch has taken 0:01:50.223238
Number of used sentences in train = 2074
Total loss for epoch 7: 3352.637359
validation loss after epoch 7 : 1305.786791
	Epoch 8....
Epoch has taken 0:01:50.146214
Number of used sentences in train = 2074
Total loss for epoch 8: 3290.485604
validation loss after epoch 8 : 1329.539634
	Epoch 9....
Epoch has taken 0:01:50.320921
Number of used sentences in train = 2074
Total loss for epoch 9: 3250.540498
validation loss after epoch 9 : 1365.964365
	Epoch 10....
Epoch has taken 0:01:50.371474
Number of used sentences in train = 2074
Total loss for epoch 10: 3228.001451
validation loss after epoch 10 : 1391.238193
	Epoch 11....
Epoch has taken 0:01:50.348868
Number of used sentences in train = 2074
Total loss for epoch 11: 3211.724648
validation loss after epoch 11 : 1412.854155
	Epoch 12....
Epoch has taken 0:01:50.360262
Number of used sentences in train = 2074
Total loss for epoch 12: 3201.375267
validation loss after epoch 12 : 1430.900490
	Epoch 13....
Epoch has taken 0:01:50.299514
Number of used sentences in train = 2074
Total loss for epoch 13: 3193.015498
validation loss after epoch 13 : 1449.748223
	Epoch 14....
Epoch has taken 0:01:50.974723
Number of used sentences in train = 2074
Total loss for epoch 14: 3186.974376
validation loss after epoch 14 : 1470.294241
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(5637, 126)
  (lstm): LSTM(154, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.946812
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1589.344641
	Epoch 1....
Epoch has taken 0:00:11.222025
Number of used sentences in train = 231
Total loss for epoch 1: 647.429989
	Epoch 2....
Epoch has taken 0:00:11.236137
Number of used sentences in train = 231
Total loss for epoch 2: 491.592592
	Epoch 3....
Epoch has taken 0:00:11.237259
Number of used sentences in train = 231
Total loss for epoch 3: 405.174206
	Epoch 4....
Epoch has taken 0:00:11.244108
Number of used sentences in train = 231
Total loss for epoch 4: 375.348803
	Epoch 5....
Epoch has taken 0:00:11.230589
Number of used sentences in train = 231
Total loss for epoch 5: 366.172865
	Epoch 6....
Epoch has taken 0:00:11.234225
Number of used sentences in train = 231
Total loss for epoch 6: 360.780862
	Epoch 7....
Epoch has taken 0:00:11.224154
Number of used sentences in train = 231
Total loss for epoch 7: 358.693625
	Epoch 8....
Epoch has taken 0:00:11.234170
Number of used sentences in train = 231
Total loss for epoch 8: 357.394729
	Epoch 9....
Epoch has taken 0:00:11.231868
Number of used sentences in train = 231
Total loss for epoch 9: 356.586890
	Epoch 10....
Epoch has taken 0:00:11.223153
Number of used sentences in train = 231
Total loss for epoch 10: 355.974060
	Epoch 11....
Epoch has taken 0:00:11.220555
Number of used sentences in train = 231
Total loss for epoch 11: 355.453232
	Epoch 12....
Epoch has taken 0:00:11.227350
Number of used sentences in train = 231
Total loss for epoch 12: 355.010709
	Epoch 13....
Epoch has taken 0:00:11.231513
Number of used sentences in train = 231
Total loss for epoch 13: 354.594966
	Epoch 14....
Epoch has taken 0:00:11.216229
Number of used sentences in train = 231
Total loss for epoch 14: 354.133011
Epoch has taken 0:00:11.228190

==================================================================================================
	Training time : 0:30:26.776744
==================================================================================================
	Identification : 0.367

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(6816, 126)
  (lstm): LSTM(154, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16013.512398
validation loss after epoch 0 : 1522.634652
	Epoch 1....
Epoch has taken 0:03:35.732942
Number of used sentences in train = 3226
Total loss for epoch 1: 12394.294218
validation loss after epoch 1 : 1508.968374
	Epoch 2....
Epoch has taken 0:03:34.904920
Number of used sentences in train = 3226
Total loss for epoch 2: 10869.717516
validation loss after epoch 2 : 1658.800434
	Epoch 3....
Epoch has taken 0:03:35.043938
Number of used sentences in train = 3226
Total loss for epoch 3: 9746.504480
validation loss after epoch 3 : 1628.855848
	Epoch 4....
Epoch has taken 0:03:44.630486
Number of used sentences in train = 3226
Total loss for epoch 4: 8904.297763
validation loss after epoch 4 : 1732.652814
	Epoch 5....
Epoch has taken 0:03:48.452552
Number of used sentences in train = 3226
Total loss for epoch 5: 8260.364505
validation loss after epoch 5 : 1853.454313
	Epoch 6....
Epoch has taken 0:03:41.876700
Number of used sentences in train = 3226
Total loss for epoch 6: 7732.302643
validation loss after epoch 6 : 2004.766937
	Epoch 7....
Epoch has taken 0:03:35.439416
Number of used sentences in train = 3226
Total loss for epoch 7: 7382.488157
validation loss after epoch 7 : 2073.138672
	Epoch 8....
Epoch has taken 0:03:34.837204
Number of used sentences in train = 3226
Total loss for epoch 8: 7049.978175
validation loss after epoch 8 : 2225.870473
	Epoch 9....
Epoch has taken 0:03:35.097054
Number of used sentences in train = 3226
Total loss for epoch 9: 6831.047185
validation loss after epoch 9 : 2313.781179
	Epoch 10....
Epoch has taken 0:03:35.269487
Number of used sentences in train = 3226
Total loss for epoch 10: 6645.729726
validation loss after epoch 10 : 2412.841406
	Epoch 11....
Epoch has taken 0:03:35.210808
Number of used sentences in train = 3226
Total loss for epoch 11: 6530.006896
validation loss after epoch 11 : 2576.337505
	Epoch 12....
Epoch has taken 0:03:35.297783
Number of used sentences in train = 3226
Total loss for epoch 12: 6473.016329
validation loss after epoch 12 : 2628.029491
	Epoch 13....
Epoch has taken 0:03:34.884510
Number of used sentences in train = 3226
Total loss for epoch 13: 6395.355532
validation loss after epoch 13 : 2688.612170
	Epoch 14....
Epoch has taken 0:03:36.386901
Number of used sentences in train = 3226
Total loss for epoch 14: 6323.718158
validation loss after epoch 14 : 2762.052533
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(6816, 126)
  (lstm): LSTM(154, 21, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=27, bias=True)
  (linear2): Linear(in_features=27, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:36.521412
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1955.215820
	Epoch 1....
Epoch has taken 0:00:20.936456
Number of used sentences in train = 359
Total loss for epoch 1: 1144.572291
	Epoch 2....
Epoch has taken 0:00:21.474931
Number of used sentences in train = 359
Total loss for epoch 2: 880.210928
	Epoch 3....
Epoch has taken 0:00:21.695348
Number of used sentences in train = 359
Total loss for epoch 3: 763.842385
	Epoch 4....
Epoch has taken 0:00:20.908964
Number of used sentences in train = 359
Total loss for epoch 4: 707.989653
	Epoch 5....
Epoch has taken 0:00:20.932730
Number of used sentences in train = 359
Total loss for epoch 5: 689.327211
	Epoch 6....
Epoch has taken 0:00:20.936929
Number of used sentences in train = 359
Total loss for epoch 6: 681.780105
	Epoch 7....
Epoch has taken 0:00:20.930431
Number of used sentences in train = 359
Total loss for epoch 7: 678.679542
	Epoch 8....
Epoch has taken 0:00:20.932956
Number of used sentences in train = 359
Total loss for epoch 8: 676.438819
	Epoch 9....
Epoch has taken 0:00:20.917902
Number of used sentences in train = 359
Total loss for epoch 9: 675.159806
	Epoch 10....
Epoch has taken 0:00:20.921874
Number of used sentences in train = 359
Total loss for epoch 10: 674.198866
	Epoch 11....
Epoch has taken 0:00:20.923740
Number of used sentences in train = 359
Total loss for epoch 11: 673.503058
	Epoch 12....
Epoch has taken 0:00:20.944438
Number of used sentences in train = 359
Total loss for epoch 12: 673.038664
	Epoch 13....
Epoch has taken 0:00:20.917650
Number of used sentences in train = 359
Total loss for epoch 13: 672.629341
	Epoch 14....
Epoch has taken 0:00:20.927646
Number of used sentences in train = 359
Total loss for epoch 14: 672.345263
Epoch has taken 0:00:20.938490

==================================================================================================
	Training time : 0:59:35.479902
==================================================================================================
	Identification : 0.238

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 73, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 58, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 31, 'lstmDropout': 0.24, 'denseActivation': 'tanh', 'wordDim': 148, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 58)
  (w_embeddings): Embedding(9273, 148)
  (lstm): LSTM(206, 31, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13275.908306
validation loss after epoch 0 : 1137.206055
	Epoch 1....
Epoch has taken 0:02:41.724730
Number of used sentences in train = 2811
Total loss for epoch 1: 8646.633576
validation loss after epoch 1 : 1104.771497
	Epoch 2....
Epoch has taken 0:02:46.730128
Number of used sentences in train = 2811
Total loss for epoch 2: 6722.103137
validation loss after epoch 2 : 1179.572521
	Epoch 3....
Epoch has taken 0:02:46.555883
Number of used sentences in train = 2811
Total loss for epoch 3: 5754.150775
validation loss after epoch 3 : 1283.130756
	Epoch 4....
Epoch has taken 0:02:46.560465
Number of used sentences in train = 2811
Total loss for epoch 4: 5210.331415
validation loss after epoch 4 : 1412.433138
	Epoch 5....
Epoch has taken 0:02:46.486161
Number of used sentences in train = 2811
Total loss for epoch 5: 4926.069612
validation loss after epoch 5 : 1519.546049
	Epoch 6....
Epoch has taken 0:02:46.591131
Number of used sentences in train = 2811
Total loss for epoch 6: 4764.811924
validation loss after epoch 6 : 1579.997311
	Epoch 7....
Epoch has taken 0:02:46.655925
Number of used sentences in train = 2811
Total loss for epoch 7: 4671.095281
validation loss after epoch 7 : 1649.952447
	Epoch 8....
Epoch has taken 0:02:47.228747
Number of used sentences in train = 2811
Total loss for epoch 8: 4589.750814
validation loss after epoch 8 : 1740.879899
	Epoch 9....
Epoch has taken 0:02:44.658892
Number of used sentences in train = 2811
Total loss for epoch 9: 4551.847481
validation loss after epoch 9 : 1741.634263
	Epoch 10....
Epoch has taken 0:02:46.656592
Number of used sentences in train = 2811
Total loss for epoch 10: 4529.429184
validation loss after epoch 10 : 1790.819419
	Epoch 11....
Epoch has taken 0:02:58.506719
Number of used sentences in train = 2811
Total loss for epoch 11: 4518.326895
validation loss after epoch 11 : 1828.416400
	Epoch 12....
Epoch has taken 0:03:01.265239
Number of used sentences in train = 2811
Total loss for epoch 12: 4503.268229
validation loss after epoch 12 : 1837.524518
	Epoch 13....
Epoch has taken 0:03:03.027354
Number of used sentences in train = 2811
Total loss for epoch 13: 4494.190261
validation loss after epoch 13 : 1875.690825
	Epoch 14....
Epoch has taken 0:03:03.137864
Number of used sentences in train = 2811
Total loss for epoch 14: 4488.905831
validation loss after epoch 14 : 1905.454782
	TransitionClassifier(
  (p_embeddings): Embedding(18, 58)
  (w_embeddings): Embedding(9273, 148)
  (lstm): LSTM(206, 31, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:03.020235
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1755.030495
	Epoch 1....
Epoch has taken 0:00:19.487954
Number of used sentences in train = 313
Total loss for epoch 1: 814.825210
	Epoch 2....
Epoch has taken 0:00:19.520982
Number of used sentences in train = 313
Total loss for epoch 2: 601.168537
	Epoch 3....
Epoch has taken 0:00:19.519698
Number of used sentences in train = 313
Total loss for epoch 3: 540.094080
	Epoch 4....
Epoch has taken 0:00:19.489932
Number of used sentences in train = 313
Total loss for epoch 4: 522.207306
	Epoch 5....
Epoch has taken 0:00:19.504557
Number of used sentences in train = 313
Total loss for epoch 5: 517.070923
	Epoch 6....
Epoch has taken 0:00:19.514504
Number of used sentences in train = 313
Total loss for epoch 6: 509.047099
	Epoch 7....
Epoch has taken 0:00:19.485854
Number of used sentences in train = 313
Total loss for epoch 7: 506.318685
	Epoch 8....
Epoch has taken 0:00:19.506461
Number of used sentences in train = 313
Total loss for epoch 8: 506.416342
	Epoch 9....
Epoch has taken 0:00:19.505109
Number of used sentences in train = 313
Total loss for epoch 9: 503.683638
	Epoch 10....
Epoch has taken 0:00:19.492257
Number of used sentences in train = 313
Total loss for epoch 10: 502.612531
	Epoch 11....
Epoch has taken 0:00:19.321203
Number of used sentences in train = 313
Total loss for epoch 11: 501.912599
	Epoch 12....
Epoch has taken 0:00:17.821644
Number of used sentences in train = 313
Total loss for epoch 12: 501.376941
	Epoch 13....
Epoch has taken 0:00:17.898270
Number of used sentences in train = 313
Total loss for epoch 13: 501.022061
	Epoch 14....
Epoch has taken 0:00:17.861468
Number of used sentences in train = 313
Total loss for epoch 14: 500.811791
Epoch has taken 0:00:17.860834

==================================================================================================
	Training time : 0:47:35.117618
==================================================================================================
	Identification : 0.349

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 58)
  (w_embeddings): Embedding(7036, 148)
  (lstm): LSTM(206, 31, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10836.184510
validation loss after epoch 0 : 1106.200485
	Epoch 1....
Epoch has taken 0:01:54.981296
Number of used sentences in train = 2074
Total loss for epoch 1: 6620.566244
validation loss after epoch 1 : 1091.896058
	Epoch 2....
Epoch has taken 0:01:59.463951
Number of used sentences in train = 2074
Total loss for epoch 2: 5007.561653
validation loss after epoch 2 : 1144.600071
	Epoch 3....
Epoch has taken 0:02:05.143973
Number of used sentences in train = 2074
Total loss for epoch 3: 4154.800325
validation loss after epoch 3 : 1272.709825
	Epoch 4....
Epoch has taken 0:02:05.090239
Number of used sentences in train = 2074
Total loss for epoch 4: 3738.285167
validation loss after epoch 4 : 1355.834805
	Epoch 5....
Epoch has taken 0:02:04.590346
Number of used sentences in train = 2074
Total loss for epoch 5: 3497.205587
validation loss after epoch 5 : 1417.322672
	Epoch 6....
Epoch has taken 0:02:05.209770
Number of used sentences in train = 2074
Total loss for epoch 6: 3374.941535
validation loss after epoch 6 : 1478.538899
	Epoch 7....
Epoch has taken 0:02:05.206449
Number of used sentences in train = 2074
Total loss for epoch 7: 3284.003460
validation loss after epoch 7 : 1461.570120
	Epoch 8....
Epoch has taken 0:02:05.033218
Number of used sentences in train = 2074
Total loss for epoch 8: 3233.514534
validation loss after epoch 8 : 1539.766399
	Epoch 9....
Epoch has taken 0:01:53.220907
Number of used sentences in train = 2074
Total loss for epoch 9: 3206.589615
validation loss after epoch 9 : 1598.171288
	Epoch 10....
Epoch has taken 0:01:50.658954
Number of used sentences in train = 2074
Total loss for epoch 10: 3192.418987
validation loss after epoch 10 : 1626.559609
	Epoch 11....
Epoch has taken 0:01:50.640475
Number of used sentences in train = 2074
Total loss for epoch 11: 3183.636450
validation loss after epoch 11 : 1663.514183
	Epoch 12....
Epoch has taken 0:01:50.713273
Number of used sentences in train = 2074
Total loss for epoch 12: 3177.999495
validation loss after epoch 12 : 1677.505961
	Epoch 13....
Epoch has taken 0:01:50.728555
Number of used sentences in train = 2074
Total loss for epoch 13: 3171.348468
validation loss after epoch 13 : 1700.164564
	Epoch 14....
Epoch has taken 0:01:57.831348
Number of used sentences in train = 2074
Total loss for epoch 14: 3167.258914
validation loss after epoch 14 : 1731.784178
	TransitionClassifier(
  (p_embeddings): Embedding(18, 58)
  (w_embeddings): Embedding(7036, 148)
  (lstm): LSTM(206, 31, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.605589
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2106.302431
	Epoch 1....
Epoch has taken 0:00:11.250795
Number of used sentences in train = 231
Total loss for epoch 1: 696.593767
	Epoch 2....
Epoch has taken 0:00:11.261393
Number of used sentences in train = 231
Total loss for epoch 2: 502.251723
	Epoch 3....
Epoch has taken 0:00:13.137974
Number of used sentences in train = 231
Total loss for epoch 3: 402.856879
	Epoch 4....
Epoch has taken 0:00:11.260034
Number of used sentences in train = 231
Total loss for epoch 4: 364.820303
	Epoch 5....
Epoch has taken 0:00:11.251773
Number of used sentences in train = 231
Total loss for epoch 5: 355.039725
	Epoch 6....
Epoch has taken 0:00:11.252189
Number of used sentences in train = 231
Total loss for epoch 6: 352.411760
	Epoch 7....
Epoch has taken 0:00:11.252937
Number of used sentences in train = 231
Total loss for epoch 7: 350.186586
	Epoch 8....
Epoch has taken 0:00:11.252098
Number of used sentences in train = 231
Total loss for epoch 8: 349.245677
	Epoch 9....
Epoch has taken 0:00:11.253319
Number of used sentences in train = 231
Total loss for epoch 9: 348.484559
	Epoch 10....
Epoch has taken 0:00:11.253572
Number of used sentences in train = 231
Total loss for epoch 10: 348.312794
	Epoch 11....
Epoch has taken 0:00:11.267183
Number of used sentences in train = 231
Total loss for epoch 11: 347.320365
	Epoch 12....
Epoch has taken 0:00:11.254346
Number of used sentences in train = 231
Total loss for epoch 12: 346.939156
	Epoch 13....
Epoch has taken 0:00:11.261454
Number of used sentences in train = 231
Total loss for epoch 13: 346.940365
	Epoch 14....
Epoch has taken 0:00:11.264078
Number of used sentences in train = 231
Total loss for epoch 14: 346.431746
Epoch has taken 0:00:11.252026

==================================================================================================
	Training time : 0:32:20.194717
==================================================================================================
	Identification : 0.227

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 58)
  (w_embeddings): Embedding(18046, 148)
  (lstm): LSTM(206, 31, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17954.746659
validation loss after epoch 0 : 1624.257271
	Epoch 1....
Epoch has taken 0:03:45.864125
Number of used sentences in train = 3226
Total loss for epoch 1: 11628.456614
validation loss after epoch 1 : 1584.971107
	Epoch 2....
Epoch has taken 0:03:59.108966
Number of used sentences in train = 3226
Total loss for epoch 2: 9158.812930
validation loss after epoch 2 : 1770.669129
	Epoch 3....
Epoch has taken 0:03:35.961909
Number of used sentences in train = 3226
Total loss for epoch 3: 7891.980165
validation loss after epoch 3 : 1847.941696
	Epoch 4....
Epoch has taken 0:03:35.684491
Number of used sentences in train = 3226
Total loss for epoch 4: 7152.509729
validation loss after epoch 4 : 2069.791775
	Epoch 5....
Epoch has taken 0:03:35.828739
Number of used sentences in train = 3226
Total loss for epoch 5: 6767.463628
validation loss after epoch 5 : 2235.925488
	Epoch 6....
Epoch has taken 0:03:35.642627
Number of used sentences in train = 3226
Total loss for epoch 6: 6544.671245
validation loss after epoch 6 : 2317.548032
	Epoch 7....
Epoch has taken 0:03:35.557845
Number of used sentences in train = 3226
Total loss for epoch 7: 6376.864840
validation loss after epoch 7 : 2463.839360
	Epoch 8....
Epoch has taken 0:03:35.871159
Number of used sentences in train = 3226
Total loss for epoch 8: 6298.827117
validation loss after epoch 8 : 2515.755992
	Epoch 9....
Epoch has taken 0:03:39.418108
Number of used sentences in train = 3226
Total loss for epoch 9: 6251.148581
validation loss after epoch 9 : 2611.621375
	Epoch 10....
Epoch has taken 0:03:38.445037
Number of used sentences in train = 3226
Total loss for epoch 10: 6233.719024
validation loss after epoch 10 : 2625.338353
	Epoch 11....
Epoch has taken 0:03:39.791160
Number of used sentences in train = 3226
Total loss for epoch 11: 6217.641007
validation loss after epoch 11 : 2632.596597
	Epoch 12....
Epoch has taken 0:03:41.063527
Number of used sentences in train = 3226
Total loss for epoch 12: 6191.623137
validation loss after epoch 12 : 2705.998099
	Epoch 13....
Epoch has taken 0:03:37.467795
Number of used sentences in train = 3226
Total loss for epoch 13: 6195.170455
validation loss after epoch 13 : 2691.222015
	Epoch 14....
Epoch has taken 0:03:37.301426
Number of used sentences in train = 3226
Total loss for epoch 14: 6183.268565
validation loss after epoch 14 : 2785.005192
	TransitionClassifier(
  (p_embeddings): Embedding(13, 58)
  (w_embeddings): Embedding(18046, 148)
  (lstm): LSTM(206, 31, bidirectional=True)
  (linear1): Linear(in_features=496, out_features=73, bias=True)
  (linear2): Linear(in_features=73, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:35.645352
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2488.596442
	Epoch 1....
Epoch has taken 0:00:21.104368
Number of used sentences in train = 359
Total loss for epoch 1: 1161.517093
	Epoch 2....
Epoch has taken 0:00:21.091611
Number of used sentences in train = 359
Total loss for epoch 2: 865.234496
	Epoch 3....
Epoch has taken 0:00:21.094584
Number of used sentences in train = 359
Total loss for epoch 3: 751.850214
	Epoch 4....
Epoch has taken 0:00:21.078948
Number of used sentences in train = 359
Total loss for epoch 4: 698.948689
	Epoch 5....
Epoch has taken 0:00:21.100658
Number of used sentences in train = 359
Total loss for epoch 5: 681.239277
	Epoch 6....
Epoch has taken 0:00:21.084469
Number of used sentences in train = 359
Total loss for epoch 6: 676.371950
	Epoch 7....
Epoch has taken 0:00:21.083845
Number of used sentences in train = 359
Total loss for epoch 7: 674.482447
	Epoch 8....
Epoch has taken 0:00:21.077561
Number of used sentences in train = 359
Total loss for epoch 8: 673.330402
	Epoch 9....
Epoch has taken 0:00:21.084573
Number of used sentences in train = 359
Total loss for epoch 9: 672.703303
	Epoch 10....
Epoch has taken 0:00:21.081042
Number of used sentences in train = 359
Total loss for epoch 10: 672.221037
	Epoch 11....
Epoch has taken 0:00:21.085682
Number of used sentences in train = 359
Total loss for epoch 11: 671.876053
	Epoch 12....
Epoch has taken 0:00:21.097331
Number of used sentences in train = 359
Total loss for epoch 12: 671.599854
	Epoch 13....
Epoch has taken 0:00:21.094319
Number of used sentences in train = 359
Total loss for epoch 13: 671.389608
	Epoch 14....
Epoch has taken 0:00:21.069637
Number of used sentences in train = 359
Total loss for epoch 14: 671.211467
Epoch has taken 0:00:21.062855

==================================================================================================
	Training time : 1:00:05.639292
==================================================================================================
	Identification : 0.239

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 17, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 28, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 34, 'lstmDropout': 0.18, 'denseActivation': 'tanh', 'wordDim': 107, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(9340, 107)
  (lstm): LSTM(135, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13305.204183
validation loss after epoch 0 : 1220.255024
	Epoch 1....
Epoch has taken 0:02:45.878864
Number of used sentences in train = 2811
Total loss for epoch 1: 9042.864872
validation loss after epoch 1 : 1249.990839
	Epoch 2....
Epoch has taken 0:02:41.898533
Number of used sentences in train = 2811
Total loss for epoch 2: 6997.334124
validation loss after epoch 2 : 1234.398116
	Epoch 3....
Epoch has taken 0:02:42.141802
Number of used sentences in train = 2811
Total loss for epoch 3: 5980.384757
validation loss after epoch 3 : 1344.476194
	Epoch 4....
Epoch has taken 0:02:41.760989
Number of used sentences in train = 2811
Total loss for epoch 4: 5428.485619
validation loss after epoch 4 : 1441.743169
	Epoch 5....
Epoch has taken 0:02:41.694393
Number of used sentences in train = 2811
Total loss for epoch 5: 5130.630212
validation loss after epoch 5 : 1494.511663
	Epoch 6....
Epoch has taken 0:02:41.826371
Number of used sentences in train = 2811
Total loss for epoch 6: 4953.929538
validation loss after epoch 6 : 1521.547265
	Epoch 7....
Epoch has taken 0:02:41.856502
Number of used sentences in train = 2811
Total loss for epoch 7: 4831.772557
validation loss after epoch 7 : 1602.066036
	Epoch 8....
Epoch has taken 0:02:42.873560
Number of used sentences in train = 2811
Total loss for epoch 8: 4732.586605
validation loss after epoch 8 : 1627.190880
	Epoch 9....
Epoch has taken 0:02:42.007991
Number of used sentences in train = 2811
Total loss for epoch 9: 4685.390663
validation loss after epoch 9 : 1670.806995
	Epoch 10....
Epoch has taken 0:02:43.502030
Number of used sentences in train = 2811
Total loss for epoch 10: 4649.332612
validation loss after epoch 10 : 1716.804206
	Epoch 11....
Epoch has taken 0:02:41.611658
Number of used sentences in train = 2811
Total loss for epoch 11: 4612.402106
validation loss after epoch 11 : 1731.997278
	Epoch 12....
Epoch has taken 0:02:44.412480
Number of used sentences in train = 2811
Total loss for epoch 12: 4582.395993
validation loss after epoch 12 : 1751.865508
	Epoch 13....
Epoch has taken 0:02:44.919440
Number of used sentences in train = 2811
Total loss for epoch 13: 4557.879997
validation loss after epoch 13 : 1790.217976
	Epoch 14....
Epoch has taken 0:02:45.617336
Number of used sentences in train = 2811
Total loss for epoch 14: 4549.322224
validation loss after epoch 14 : 1824.293012
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(9340, 107)
  (lstm): LSTM(135, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:41.867650
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1845.591618
	Epoch 1....
Epoch has taken 0:00:17.086657
Number of used sentences in train = 313
Total loss for epoch 1: 841.299012
	Epoch 2....
Epoch has taken 0:00:17.073235
Number of used sentences in train = 313
Total loss for epoch 2: 666.436302
	Epoch 3....
Epoch has taken 0:00:17.070008
Number of used sentences in train = 313
Total loss for epoch 3: 598.689915
	Epoch 4....
Epoch has taken 0:00:17.096045
Number of used sentences in train = 313
Total loss for epoch 4: 557.155718
	Epoch 5....
Epoch has taken 0:00:17.077113
Number of used sentences in train = 313
Total loss for epoch 5: 546.815982
	Epoch 6....
Epoch has taken 0:00:17.073803
Number of used sentences in train = 313
Total loss for epoch 6: 542.937776
	Epoch 7....
Epoch has taken 0:00:17.100145
Number of used sentences in train = 313
Total loss for epoch 7: 542.232832
	Epoch 8....
Epoch has taken 0:00:17.167307
Number of used sentences in train = 313
Total loss for epoch 8: 537.784351
	Epoch 9....
Epoch has taken 0:00:17.101297
Number of used sentences in train = 313
Total loss for epoch 9: 534.188138
	Epoch 10....
Epoch has taken 0:00:17.089795
Number of used sentences in train = 313
Total loss for epoch 10: 534.697612
	Epoch 11....
Epoch has taken 0:00:17.100331
Number of used sentences in train = 313
Total loss for epoch 11: 533.837099
	Epoch 12....
Epoch has taken 0:00:17.079712
Number of used sentences in train = 313
Total loss for epoch 12: 533.278172
	Epoch 13....
Epoch has taken 0:00:17.091391
Number of used sentences in train = 313
Total loss for epoch 13: 530.973416
	Epoch 14....
Epoch has taken 0:00:17.076764
Number of used sentences in train = 313
Total loss for epoch 14: 531.339424
Epoch has taken 0:00:17.085398

==================================================================================================
	Training time : 0:45:00.754970
==================================================================================================
	Identification : 0.448

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(7016, 107)
  (lstm): LSTM(135, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10613.045948
validation loss after epoch 0 : 952.746600
	Epoch 1....
Epoch has taken 0:01:50.144017
Number of used sentences in train = 2074
Total loss for epoch 1: 6513.341888
validation loss after epoch 1 : 956.233234
	Epoch 2....
Epoch has taken 0:01:50.175423
Number of used sentences in train = 2074
Total loss for epoch 2: 4999.964276
validation loss after epoch 2 : 1022.717305
	Epoch 3....
Epoch has taken 0:01:50.414809
Number of used sentences in train = 2074
Total loss for epoch 3: 4218.448752
validation loss after epoch 3 : 1079.007225
	Epoch 4....
Epoch has taken 0:01:50.301017
Number of used sentences in train = 2074
Total loss for epoch 4: 3762.658491
validation loss after epoch 4 : 1168.542882
	Epoch 5....
Epoch has taken 0:01:50.356586
Number of used sentences in train = 2074
Total loss for epoch 5: 3519.282864
validation loss after epoch 5 : 1200.131029
	Epoch 6....
Epoch has taken 0:01:50.428172
Number of used sentences in train = 2074
Total loss for epoch 6: 3377.086930
validation loss after epoch 6 : 1248.013904
	Epoch 7....
Epoch has taken 0:01:51.315680
Number of used sentences in train = 2074
Total loss for epoch 7: 3304.839716
validation loss after epoch 7 : 1283.730464
	Epoch 8....
Epoch has taken 0:01:50.320098
Number of used sentences in train = 2074
Total loss for epoch 8: 3260.349525
validation loss after epoch 8 : 1310.261794
	Epoch 9....
Epoch has taken 0:01:50.949114
Number of used sentences in train = 2074
Total loss for epoch 9: 3241.865362
validation loss after epoch 9 : 1332.425582
	Epoch 10....
Epoch has taken 0:01:50.562151
Number of used sentences in train = 2074
Total loss for epoch 10: 3224.869252
validation loss after epoch 10 : 1347.073151
	Epoch 11....
Epoch has taken 0:01:50.397197
Number of used sentences in train = 2074
Total loss for epoch 11: 3209.775444
validation loss after epoch 11 : 1357.314411
	Epoch 12....
Epoch has taken 0:01:51.077859
Number of used sentences in train = 2074
Total loss for epoch 12: 3211.283093
validation loss after epoch 12 : 1379.046583
	Epoch 13....
Epoch has taken 0:01:50.304237
Number of used sentences in train = 2074
Total loss for epoch 13: 3207.539220
validation loss after epoch 13 : 1373.428518
	Epoch 14....
Epoch has taken 0:01:50.394861
Number of used sentences in train = 2074
Total loss for epoch 14: 3193.997356
validation loss after epoch 14 : 1390.492403
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(7016, 107)
  (lstm): LSTM(135, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.552549
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1411.331750
	Epoch 1....
Epoch has taken 0:00:11.213157
Number of used sentences in train = 231
Total loss for epoch 1: 633.683118
	Epoch 2....
Epoch has taken 0:00:11.255381
Number of used sentences in train = 231
Total loss for epoch 2: 462.545271
	Epoch 3....
Epoch has taken 0:00:11.228069
Number of used sentences in train = 231
Total loss for epoch 3: 407.377353
	Epoch 4....
Epoch has taken 0:00:11.234016
Number of used sentences in train = 231
Total loss for epoch 4: 383.472456
	Epoch 5....
Epoch has taken 0:00:11.260201
Number of used sentences in train = 231
Total loss for epoch 5: 368.757718
	Epoch 6....
Epoch has taken 0:00:11.222145
Number of used sentences in train = 231
Total loss for epoch 6: 366.044617
	Epoch 7....
Epoch has taken 0:00:11.222675
Number of used sentences in train = 231
Total loss for epoch 7: 358.043519
	Epoch 8....
Epoch has taken 0:00:11.236022
Number of used sentences in train = 231
Total loss for epoch 8: 350.603886
	Epoch 9....
Epoch has taken 0:00:11.217053
Number of used sentences in train = 231
Total loss for epoch 9: 349.618205
	Epoch 10....
Epoch has taken 0:00:11.252515
Number of used sentences in train = 231
Total loss for epoch 10: 348.504893
	Epoch 11....
Epoch has taken 0:00:11.229750
Number of used sentences in train = 231
Total loss for epoch 11: 347.927526
	Epoch 12....
Epoch has taken 0:00:11.277543
Number of used sentences in train = 231
Total loss for epoch 12: 347.551901
	Epoch 13....
Epoch has taken 0:00:11.255737
Number of used sentences in train = 231
Total loss for epoch 13: 347.195467
	Epoch 14....
Epoch has taken 0:00:11.212617
Number of used sentences in train = 231
Total loss for epoch 14: 346.945837
Epoch has taken 0:00:11.259335

==================================================================================================
	Training time : 0:30:26.610870
==================================================================================================
	Identification : 0.185

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(17972, 107)
  (lstm): LSTM(135, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17964.553171
validation loss after epoch 0 : 1644.478441
	Epoch 1....
Epoch has taken 0:03:40.892666
Number of used sentences in train = 3226
Total loss for epoch 1: 11935.950678
validation loss after epoch 1 : 1660.375498
	Epoch 2....
Epoch has taken 0:03:36.970503
Number of used sentences in train = 3226
Total loss for epoch 2: 9384.068578
validation loss after epoch 2 : 1742.589958
	Epoch 3....
Epoch has taken 0:03:36.517157
Number of used sentences in train = 3226
Total loss for epoch 3: 7961.814968
validation loss after epoch 3 : 1897.706191
	Epoch 4....
Epoch has taken 0:03:41.468650
Number of used sentences in train = 3226
Total loss for epoch 4: 7156.662698
validation loss after epoch 4 : 2004.606452
	Epoch 5....
Epoch has taken 0:03:38.737449
Number of used sentences in train = 3226
Total loss for epoch 5: 6702.672121
validation loss after epoch 5 : 2162.767834
	Epoch 6....
Epoch has taken 0:03:38.784437
Number of used sentences in train = 3226
Total loss for epoch 6: 6511.620429
validation loss after epoch 6 : 2203.414050
	Epoch 7....
Epoch has taken 0:03:38.869444
Number of used sentences in train = 3226
Total loss for epoch 7: 6375.521943
validation loss after epoch 7 : 2334.718779
	Epoch 8....
Epoch has taken 0:03:36.300287
Number of used sentences in train = 3226
Total loss for epoch 8: 6307.342845
validation loss after epoch 8 : 2348.701332
	Epoch 9....
Epoch has taken 0:03:39.162029
Number of used sentences in train = 3226
Total loss for epoch 9: 6281.141705
validation loss after epoch 9 : 2338.957324
	Epoch 10....
Epoch has taken 0:03:36.405416
Number of used sentences in train = 3226
Total loss for epoch 10: 6246.355092
validation loss after epoch 10 : 2422.715125
	Epoch 11....
Epoch has taken 0:03:38.691078
Number of used sentences in train = 3226
Total loss for epoch 11: 6223.971558
validation loss after epoch 11 : 2448.308157
	Epoch 12....
Epoch has taken 0:03:36.461200
Number of used sentences in train = 3226
Total loss for epoch 12: 6206.606925
validation loss after epoch 12 : 2469.963169
	Epoch 13....
Epoch has taken 0:03:36.636797
Number of used sentences in train = 3226
Total loss for epoch 13: 6200.755982
validation loss after epoch 13 : 2526.315259
	Epoch 14....
Epoch has taken 0:03:36.437620
Number of used sentences in train = 3226
Total loss for epoch 14: 6188.629531
validation loss after epoch 14 : 2540.307703
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(17972, 107)
  (lstm): LSTM(135, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=17, bias=True)
  (linear2): Linear(in_features=17, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:55.862481
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1995.055501
	Epoch 1....
Epoch has taken 0:00:21.769195
Number of used sentences in train = 359
Total loss for epoch 1: 1109.268511
	Epoch 2....
Epoch has taken 0:00:21.750988
Number of used sentences in train = 359
Total loss for epoch 2: 808.689101
	Epoch 3....
Epoch has taken 0:00:21.734009
Number of used sentences in train = 359
Total loss for epoch 3: 731.205788
	Epoch 4....
Epoch has taken 0:00:21.759296
Number of used sentences in train = 359
Total loss for epoch 4: 694.570906
	Epoch 5....
Epoch has taken 0:00:21.757990
Number of used sentences in train = 359
Total loss for epoch 5: 681.509769
	Epoch 6....
Epoch has taken 0:00:21.734935
Number of used sentences in train = 359
Total loss for epoch 6: 676.583245
	Epoch 7....
Epoch has taken 0:00:21.787504
Number of used sentences in train = 359
Total loss for epoch 7: 673.655788
	Epoch 8....
Epoch has taken 0:00:21.761397
Number of used sentences in train = 359
Total loss for epoch 8: 672.929588
	Epoch 9....
Epoch has taken 0:00:21.768251
Number of used sentences in train = 359
Total loss for epoch 9: 672.515106
	Epoch 10....
Epoch has taken 0:00:21.758921
Number of used sentences in train = 359
Total loss for epoch 10: 672.209452
	Epoch 11....
Epoch has taken 0:00:21.729634
Number of used sentences in train = 359
Total loss for epoch 11: 671.969146
	Epoch 12....
Epoch has taken 0:00:21.480487
Number of used sentences in train = 359
Total loss for epoch 12: 671.773850
	Epoch 13....
Epoch has taken 0:00:21.770844
Number of used sentences in train = 359
Total loss for epoch 13: 671.610692
	Epoch 14....
Epoch has taken 0:00:21.806159
Number of used sentences in train = 359
Total loss for epoch 14: 671.471384
Epoch has taken 0:00:21.769767

==================================================================================================
	Training time : 1:00:15.025720
==================================================================================================
	Identification : 0.434

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 79, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 28, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 47, 'lstmDropout': 0.19, 'denseActivation': 'tanh', 'wordDim': 50, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(1177, 50)
  (lstm): LSTM(78, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=79, bias=True)
  (linear2): Linear(in_features=79, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11217.070856
validation loss after epoch 0 : 889.621886
	Epoch 1....
Epoch has taken 0:02:53.903796
Number of used sentences in train = 2811
Total loss for epoch 1: 7563.317867
validation loss after epoch 1 : 913.649291
	Epoch 2....
Epoch has taken 0:02:46.462772
Number of used sentences in train = 2811
Total loss for epoch 2: 6637.928296
validation loss after epoch 2 : 857.030379
	Epoch 3....
Epoch has taken 0:02:46.478410
Number of used sentences in train = 2811
Total loss for epoch 3: 6049.264928
validation loss after epoch 3 : 861.662277
	Epoch 4....
Epoch has taken 0:02:47.347924
Number of used sentences in train = 2811
Total loss for epoch 4: 5630.045541
validation loss after epoch 4 : 906.660797
	Epoch 5....
Epoch has taken 0:03:01.184412
Number of used sentences in train = 2811
Total loss for epoch 5: 5293.593105
validation loss after epoch 5 : 971.812359
	Epoch 6....
Epoch has taken 0:03:01.983833
Number of used sentences in train = 2811
Total loss for epoch 6: 5106.648841
validation loss after epoch 6 : 981.811281
	Epoch 7....
Epoch has taken 0:03:01.521637
Number of used sentences in train = 2811
Total loss for epoch 7: 4927.393760
validation loss after epoch 7 : 1054.784343
	Epoch 8....
Epoch has taken 0:02:53.437042
Number of used sentences in train = 2811
Total loss for epoch 8: 4785.375288
validation loss after epoch 8 : 1067.888249
	Epoch 9....
Epoch has taken 0:02:46.456434
Number of used sentences in train = 2811
Total loss for epoch 9: 4696.143622
validation loss after epoch 9 : 1115.603970
	Epoch 10....
Epoch has taken 0:02:45.546645
Number of used sentences in train = 2811
Total loss for epoch 10: 4622.621951
validation loss after epoch 10 : 1138.740621
	Epoch 11....
Epoch has taken 0:02:46.498506
Number of used sentences in train = 2811
Total loss for epoch 11: 4593.163486
validation loss after epoch 11 : 1163.770739
	Epoch 12....
Epoch has taken 0:02:45.443115
Number of used sentences in train = 2811
Total loss for epoch 12: 4565.243387
validation loss after epoch 12 : 1191.959405
	Epoch 13....
Epoch has taken 0:02:46.497484
Number of used sentences in train = 2811
Total loss for epoch 13: 4549.351415
validation loss after epoch 13 : 1195.755090
	Epoch 14....
Epoch has taken 0:02:46.473524
Number of used sentences in train = 2811
Total loss for epoch 14: 4525.489237
validation loss after epoch 14 : 1222.655730
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(1177, 50)
  (lstm): LSTM(78, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=79, bias=True)
  (linear2): Linear(in_features=79, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:46.480446
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1955.152404
	Epoch 1....
Epoch has taken 0:00:17.637584
Number of used sentences in train = 313
Total loss for epoch 1: 743.993181
	Epoch 2....
Epoch has taken 0:00:17.636226
Number of used sentences in train = 313
Total loss for epoch 2: 630.232839
	Epoch 3....
Epoch has taken 0:00:17.636709
Number of used sentences in train = 313
Total loss for epoch 3: 565.214685
	Epoch 4....
Epoch has taken 0:00:17.655313
Number of used sentences in train = 313
Total loss for epoch 4: 541.581500
	Epoch 5....
Epoch has taken 0:00:17.402570
Number of used sentences in train = 313
Total loss for epoch 5: 529.394976
	Epoch 6....
Epoch has taken 0:00:17.360102
Number of used sentences in train = 313
Total loss for epoch 6: 523.262976
	Epoch 7....
Epoch has taken 0:00:17.648318
Number of used sentences in train = 313
Total loss for epoch 7: 518.936626
	Epoch 8....
Epoch has taken 0:00:17.635128
Number of used sentences in train = 313
Total loss for epoch 8: 515.147249
	Epoch 9....
Epoch has taken 0:00:17.651875
Number of used sentences in train = 313
Total loss for epoch 9: 512.973721
	Epoch 10....
Epoch has taken 0:00:17.637108
Number of used sentences in train = 313
Total loss for epoch 10: 510.500085
	Epoch 11....
Epoch has taken 0:00:17.661720
Number of used sentences in train = 313
Total loss for epoch 11: 509.360645
	Epoch 12....
Epoch has taken 0:00:17.626506
Number of used sentences in train = 313
Total loss for epoch 12: 507.368093
	Epoch 13....
Epoch has taken 0:00:17.675925
Number of used sentences in train = 313
Total loss for epoch 13: 507.441886
	Epoch 14....
Epoch has taken 0:00:17.643750
Number of used sentences in train = 313
Total loss for epoch 14: 508.476844
Epoch has taken 0:00:17.633917

==================================================================================================
	Training time : 0:47:00.353535
==================================================================================================
	Identification : 0.484

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(1133, 50)
  (lstm): LSTM(78, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=79, bias=True)
  (linear2): Linear(in_features=79, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8424.384814
validation loss after epoch 0 : 713.750518
	Epoch 1....
Epoch has taken 0:01:53.871438
Number of used sentences in train = 2074
Total loss for epoch 1: 5227.425568
validation loss after epoch 1 : 683.579843
	Epoch 2....
Epoch has taken 0:01:52.375125
Number of used sentences in train = 2074
Total loss for epoch 2: 4448.168256
validation loss after epoch 2 : 675.262122
	Epoch 3....
Epoch has taken 0:01:54.131653
Number of used sentences in train = 2074
Total loss for epoch 3: 3992.155222
validation loss after epoch 3 : 697.925916
	Epoch 4....
Epoch has taken 0:01:54.060244
Number of used sentences in train = 2074
Total loss for epoch 4: 3628.209118
validation loss after epoch 4 : 788.560570
	Epoch 5....
Epoch has taken 0:01:54.037071
Number of used sentences in train = 2074
Total loss for epoch 5: 3427.314880
validation loss after epoch 5 : 789.021757
	Epoch 6....
Epoch has taken 0:01:53.925233
Number of used sentences in train = 2074
Total loss for epoch 6: 3284.943347
validation loss after epoch 6 : 827.958030
	Epoch 7....
Epoch has taken 0:01:54.018266
Number of used sentences in train = 2074
Total loss for epoch 7: 3231.245451
validation loss after epoch 7 : 866.970699
	Epoch 8....
Epoch has taken 0:02:02.746857
Number of used sentences in train = 2074
Total loss for epoch 8: 3202.828094
validation loss after epoch 8 : 905.765973
	Epoch 9....
Epoch has taken 0:01:54.068181
Number of used sentences in train = 2074
Total loss for epoch 9: 3187.701880
validation loss after epoch 9 : 915.685591
	Epoch 10....
Epoch has taken 0:01:53.945554
Number of used sentences in train = 2074
Total loss for epoch 10: 3179.290417
validation loss after epoch 10 : 933.643962
	Epoch 11....
Epoch has taken 0:01:53.933838
Number of used sentences in train = 2074
Total loss for epoch 11: 3175.109267
validation loss after epoch 11 : 948.785607
	Epoch 12....
Epoch has taken 0:02:02.756649
Number of used sentences in train = 2074
Total loss for epoch 12: 3172.020214
validation loss after epoch 12 : 952.575430
	Epoch 13....
Epoch has taken 0:02:05.564156
Number of used sentences in train = 2074
Total loss for epoch 13: 3174.043390
validation loss after epoch 13 : 957.777319
	Epoch 14....
Epoch has taken 0:01:53.953030
Number of used sentences in train = 2074
Total loss for epoch 14: 3171.066164
validation loss after epoch 14 : 961.504224
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(1133, 50)
  (lstm): LSTM(78, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=79, bias=True)
  (linear2): Linear(in_features=79, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.865727
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1315.934159
	Epoch 1....
Epoch has taken 0:00:11.606254
Number of used sentences in train = 231
Total loss for epoch 1: 520.926765
	Epoch 2....
Epoch has taken 0:00:11.611731
Number of used sentences in train = 231
Total loss for epoch 2: 403.123248
	Epoch 3....
Epoch has taken 0:00:11.825131
Number of used sentences in train = 231
Total loss for epoch 3: 364.741505
	Epoch 4....
Epoch has taken 0:00:11.608140
Number of used sentences in train = 231
Total loss for epoch 4: 354.161488
	Epoch 5....
Epoch has taken 0:00:11.638624
Number of used sentences in train = 231
Total loss for epoch 5: 351.197864
	Epoch 6....
Epoch has taken 0:00:11.593232
Number of used sentences in train = 231
Total loss for epoch 6: 349.476113
	Epoch 7....
Epoch has taken 0:00:11.618255
Number of used sentences in train = 231
Total loss for epoch 7: 348.357599
	Epoch 8....
Epoch has taken 0:00:11.604906
Number of used sentences in train = 231
Total loss for epoch 8: 347.738450
	Epoch 9....
Epoch has taken 0:00:11.611333
Number of used sentences in train = 231
Total loss for epoch 9: 347.199201
	Epoch 10....
Epoch has taken 0:00:11.607986
Number of used sentences in train = 231
Total loss for epoch 10: 346.800164
	Epoch 11....
Epoch has taken 0:00:11.615682
Number of used sentences in train = 231
Total loss for epoch 11: 346.501371
	Epoch 12....
Epoch has taken 0:00:11.607071
Number of used sentences in train = 231
Total loss for epoch 12: 346.250391
	Epoch 13....
Epoch has taken 0:00:11.594421
Number of used sentences in train = 231
Total loss for epoch 13: 346.021033
	Epoch 14....
Epoch has taken 0:00:11.609060
Number of used sentences in train = 231
Total loss for epoch 14: 345.706623
Epoch has taken 0:00:11.606886

==================================================================================================
	Training time : 0:31:51.944630
==================================================================================================
	Identification : 0.301

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(1202, 50)
  (lstm): LSTM(78, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=79, bias=True)
  (linear2): Linear(in_features=79, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15107.611646
validation loss after epoch 0 : 1352.591679
	Epoch 1....
Epoch has taken 0:03:39.973853
Number of used sentences in train = 3226
Total loss for epoch 1: 11557.513614
validation loss after epoch 1 : 1287.584875
	Epoch 2....
Epoch has taken 0:04:01.573259
Number of used sentences in train = 3226
Total loss for epoch 2: 10573.623580
validation loss after epoch 2 : 1275.280014
	Epoch 3....
Epoch has taken 0:03:41.980917
Number of used sentences in train = 3226
Total loss for epoch 3: 9785.707900
validation loss after epoch 3 : 1269.927188
	Epoch 4....
Epoch has taken 0:03:58.907528
Number of used sentences in train = 3226
Total loss for epoch 4: 9253.642795
validation loss after epoch 4 : 1319.091990
	Epoch 5....
Epoch has taken 0:03:40.165835
Number of used sentences in train = 3226
Total loss for epoch 5: 8775.726450
validation loss after epoch 5 : 1299.484807
	Epoch 6....
Epoch has taken 0:03:40.095150
Number of used sentences in train = 3226
Total loss for epoch 6: 8441.435501
validation loss after epoch 6 : 1317.106228
	Epoch 7....
Epoch has taken 0:03:59.602633
Number of used sentences in train = 3226
Total loss for epoch 7: 8084.031657
validation loss after epoch 7 : 1385.660319
	Epoch 8....
Epoch has taken 0:03:40.136601
Number of used sentences in train = 3226
Total loss for epoch 8: 7820.659168
validation loss after epoch 8 : 1419.422191
	Epoch 9....
Epoch has taken 0:03:56.675579
Number of used sentences in train = 3226
Total loss for epoch 9: 7573.078074
validation loss after epoch 9 : 1493.046666
	Epoch 10....
Epoch has taken 0:04:06.073631
Number of used sentences in train = 3226
Total loss for epoch 10: 7414.411537
validation loss after epoch 10 : 1555.304963
	Epoch 11....
Epoch has taken 0:03:50.726741
Number of used sentences in train = 3226
Total loss for epoch 11: 7212.765974
validation loss after epoch 11 : 1598.747713
	Epoch 12....
Epoch has taken 0:03:40.091728
Number of used sentences in train = 3226
Total loss for epoch 12: 7020.423992
validation loss after epoch 12 : 1655.884080
	Epoch 13....
Epoch has taken 0:03:40.274634
Number of used sentences in train = 3226
Total loss for epoch 13: 6886.137927
validation loss after epoch 13 : 1748.309602
	Epoch 14....
Epoch has taken 0:03:40.202609
Number of used sentences in train = 3226
Total loss for epoch 14: 6776.594131
validation loss after epoch 14 : 1753.139586
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(1202, 50)
  (lstm): LSTM(78, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=79, bias=True)
  (linear2): Linear(in_features=79, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:41.815896
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2403.025373
	Epoch 1....
Epoch has taken 0:00:21.551718
Number of used sentences in train = 359
Total loss for epoch 1: 1140.508552
	Epoch 2....
Epoch has taken 0:00:21.568582
Number of used sentences in train = 359
Total loss for epoch 2: 975.282759
	Epoch 3....
Epoch has taken 0:00:21.541247
Number of used sentences in train = 359
Total loss for epoch 3: 888.243374
	Epoch 4....
Epoch has taken 0:00:21.568943
Number of used sentences in train = 359
Total loss for epoch 4: 799.285492
	Epoch 5....
Epoch has taken 0:00:21.585645
Number of used sentences in train = 359
Total loss for epoch 5: 753.059065
	Epoch 6....
Epoch has taken 0:00:21.569597
Number of used sentences in train = 359
Total loss for epoch 6: 732.778405
	Epoch 7....
Epoch has taken 0:00:21.578510
Number of used sentences in train = 359
Total loss for epoch 7: 701.785862
	Epoch 8....
Epoch has taken 0:00:21.555326
Number of used sentences in train = 359
Total loss for epoch 8: 694.130822
	Epoch 9....
Epoch has taken 0:00:21.585440
Number of used sentences in train = 359
Total loss for epoch 9: 687.377923
	Epoch 10....
Epoch has taken 0:00:21.578212
Number of used sentences in train = 359
Total loss for epoch 10: 681.806507
	Epoch 11....
Epoch has taken 0:00:21.579932
Number of used sentences in train = 359
Total loss for epoch 11: 676.272036
	Epoch 12....
Epoch has taken 0:00:21.598535
Number of used sentences in train = 359
Total loss for epoch 12: 675.065117
	Epoch 13....
Epoch has taken 0:00:21.589768
Number of used sentences in train = 359
Total loss for epoch 13: 673.013392
	Epoch 14....
Epoch has taken 0:00:21.568142
Number of used sentences in train = 359
Total loss for epoch 14: 672.006608
Epoch has taken 0:00:21.600478

==================================================================================================
	Training time : 1:02:22.558760
==================================================================================================
	Identification : 0.47

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 33, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 26, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 58, 'lstmDropout': 0.18, 'denseActivation': 'tanh', 'wordDim': 114, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1882, 114)
  (lstm): LSTM(140, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10715.568991
validation loss after epoch 0 : 873.309219
	Epoch 1....
Epoch has taken 0:02:46.780919
Number of used sentences in train = 2811
Total loss for epoch 1: 7689.292026
validation loss after epoch 1 : 864.342508
	Epoch 2....
Epoch has taken 0:02:46.861700
Number of used sentences in train = 2811
Total loss for epoch 2: 6630.143171
validation loss after epoch 2 : 876.438183
	Epoch 3....
Epoch has taken 0:02:46.612084
Number of used sentences in train = 2811
Total loss for epoch 3: 5940.690245
validation loss after epoch 3 : 867.775481
	Epoch 4....
Epoch has taken 0:02:46.414583
Number of used sentences in train = 2811
Total loss for epoch 4: 5551.615462
validation loss after epoch 4 : 921.312452
	Epoch 5....
Epoch has taken 0:02:44.695710
Number of used sentences in train = 2811
Total loss for epoch 5: 5277.804354
validation loss after epoch 5 : 921.993726
	Epoch 6....
Epoch has taken 0:02:46.875899
Number of used sentences in train = 2811
Total loss for epoch 6: 5071.960263
validation loss after epoch 6 : 956.780985
	Epoch 7....
Epoch has taken 0:02:45.763469
Number of used sentences in train = 2811
Total loss for epoch 7: 4954.933386
validation loss after epoch 7 : 983.292075
	Epoch 8....
Epoch has taken 0:02:45.002740
Number of used sentences in train = 2811
Total loss for epoch 8: 4885.498082
validation loss after epoch 8 : 1027.973961
	Epoch 9....
Epoch has taken 0:02:46.730661
Number of used sentences in train = 2811
Total loss for epoch 9: 4789.188628
validation loss after epoch 9 : 1043.454399
	Epoch 10....
Epoch has taken 0:02:46.673732
Number of used sentences in train = 2811
Total loss for epoch 10: 4729.208069
validation loss after epoch 10 : 1072.618541
	Epoch 11....
Epoch has taken 0:02:46.728571
Number of used sentences in train = 2811
Total loss for epoch 11: 4677.671815
validation loss after epoch 11 : 1076.665892
	Epoch 12....
Epoch has taken 0:02:44.936255
Number of used sentences in train = 2811
Total loss for epoch 12: 4639.529992
validation loss after epoch 12 : 1111.726891
	Epoch 13....
Epoch has taken 0:02:45.491411
Number of used sentences in train = 2811
Total loss for epoch 13: 4620.690917
validation loss after epoch 13 : 1101.506945
	Epoch 14....
Epoch has taken 0:02:59.371871
Number of used sentences in train = 2811
Total loss for epoch 14: 4597.314789
validation loss after epoch 14 : 1115.464877
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1882, 114)
  (lstm): LSTM(140, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.011040
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1583.339740
	Epoch 1....
Epoch has taken 0:00:19.343835
Number of used sentences in train = 313
Total loss for epoch 1: 746.005372
	Epoch 2....
Epoch has taken 0:00:19.350142
Number of used sentences in train = 313
Total loss for epoch 2: 616.908842
	Epoch 3....
Epoch has taken 0:00:19.482636
Number of used sentences in train = 313
Total loss for epoch 3: 559.467038
	Epoch 4....
Epoch has taken 0:00:19.484075
Number of used sentences in train = 313
Total loss for epoch 4: 544.347864
	Epoch 5....
Epoch has taken 0:00:19.491188
Number of used sentences in train = 313
Total loss for epoch 5: 530.392945
	Epoch 6....
Epoch has taken 0:00:19.502548
Number of used sentences in train = 313
Total loss for epoch 6: 523.281689
	Epoch 7....
Epoch has taken 0:00:19.478517
Number of used sentences in train = 313
Total loss for epoch 7: 519.999816
	Epoch 8....
Epoch has taken 0:00:19.510295
Number of used sentences in train = 313
Total loss for epoch 8: 518.341817
	Epoch 9....
Epoch has taken 0:00:19.497777
Number of used sentences in train = 313
Total loss for epoch 9: 516.554785
	Epoch 10....
Epoch has taken 0:00:19.503756
Number of used sentences in train = 313
Total loss for epoch 10: 514.745924
	Epoch 11....
Epoch has taken 0:00:19.489387
Number of used sentences in train = 313
Total loss for epoch 11: 514.268260
	Epoch 12....
Epoch has taken 0:00:18.178595
Number of used sentences in train = 313
Total loss for epoch 12: 513.586808
	Epoch 13....
Epoch has taken 0:00:17.967090
Number of used sentences in train = 313
Total loss for epoch 13: 513.233777
	Epoch 14....
Epoch has taken 0:00:17.779364
Number of used sentences in train = 313
Total loss for epoch 14: 511.578834
Epoch has taken 0:00:17.688117

==================================================================================================
	Training time : 0:46:36.198259
==================================================================================================
	Identification : 0.129

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1680, 114)
  (lstm): LSTM(140, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7773.133907
validation loss after epoch 0 : 688.533826
	Epoch 1....
Epoch has taken 0:01:55.395530
Number of used sentences in train = 2074
Total loss for epoch 1: 4956.728037
validation loss after epoch 1 : 745.041526
	Epoch 2....
Epoch has taken 0:01:54.589301
Number of used sentences in train = 2074
Total loss for epoch 2: 4109.332380
validation loss after epoch 2 : 769.957085
	Epoch 3....
Epoch has taken 0:01:56.001455
Number of used sentences in train = 2074
Total loss for epoch 3: 3695.967951
validation loss after epoch 3 : 833.588126
	Epoch 4....
Epoch has taken 0:01:55.543217
Number of used sentences in train = 2074
Total loss for epoch 4: 3442.922525
validation loss after epoch 4 : 868.465221
	Epoch 5....
Epoch has taken 0:01:55.861517
Number of used sentences in train = 2074
Total loss for epoch 5: 3310.781351
validation loss after epoch 5 : 909.704656
	Epoch 6....
Epoch has taken 0:01:55.145925
Number of used sentences in train = 2074
Total loss for epoch 6: 3248.441990
validation loss after epoch 6 : 965.460284
	Epoch 7....
Epoch has taken 0:01:55.409098
Number of used sentences in train = 2074
Total loss for epoch 7: 3225.341961
validation loss after epoch 7 : 982.113872
	Epoch 8....
Epoch has taken 0:01:55.084546
Number of used sentences in train = 2074
Total loss for epoch 8: 3205.908940
validation loss after epoch 8 : 1004.413595
	Epoch 9....
Epoch has taken 0:01:55.216836
Number of used sentences in train = 2074
Total loss for epoch 9: 3194.645696
validation loss after epoch 9 : 1030.625251
	Epoch 10....
Epoch has taken 0:01:55.176422
Number of used sentences in train = 2074
Total loss for epoch 10: 3187.765410
validation loss after epoch 10 : 1042.600158
	Epoch 11....
Epoch has taken 0:01:55.369092
Number of used sentences in train = 2074
Total loss for epoch 11: 3181.896411
validation loss after epoch 11 : 1052.751183
	Epoch 12....
Epoch has taken 0:01:55.455921
Number of used sentences in train = 2074
Total loss for epoch 12: 3176.784027
validation loss after epoch 12 : 1067.922053
	Epoch 13....
Epoch has taken 0:02:00.979905
Number of used sentences in train = 2074
Total loss for epoch 13: 3173.697882
validation loss after epoch 13 : 1076.555974
	Epoch 14....
Epoch has taken 0:01:55.026611
Number of used sentences in train = 2074
Total loss for epoch 14: 3169.910412
validation loss after epoch 14 : 1082.507838
	TransitionClassifier(
  (p_embeddings): Embedding(18, 26)
  (w_embeddings): Embedding(1680, 114)
  (lstm): LSTM(140, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.631928
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1106.703904
	Epoch 1....
Epoch has taken 0:00:11.698832
Number of used sentences in train = 231
Total loss for epoch 1: 547.022354
	Epoch 2....
Epoch has taken 0:00:11.773051
Number of used sentences in train = 231
Total loss for epoch 2: 420.719152
	Epoch 3....
Epoch has taken 0:00:11.886902
Number of used sentences in train = 231
Total loss for epoch 3: 375.491253
	Epoch 4....
Epoch has taken 0:00:11.751529
Number of used sentences in train = 231
Total loss for epoch 4: 361.588144
	Epoch 5....
Epoch has taken 0:00:11.667890
Number of used sentences in train = 231
Total loss for epoch 5: 357.482970
	Epoch 6....
Epoch has taken 0:00:11.836563
Number of used sentences in train = 231
Total loss for epoch 6: 355.807798
	Epoch 7....
Epoch has taken 0:00:12.051433
Number of used sentences in train = 231
Total loss for epoch 7: 354.421037
	Epoch 8....
Epoch has taken 0:00:11.805249
Number of used sentences in train = 231
Total loss for epoch 8: 353.618222
	Epoch 9....
Epoch has taken 0:00:11.790227
Number of used sentences in train = 231
Total loss for epoch 9: 352.901441
	Epoch 10....
Epoch has taken 0:00:11.781727
Number of used sentences in train = 231
Total loss for epoch 10: 352.455000
	Epoch 11....
Epoch has taken 0:00:11.706283
Number of used sentences in train = 231
Total loss for epoch 11: 351.884141
	Epoch 12....
Epoch has taken 0:00:11.669694
Number of used sentences in train = 231
Total loss for epoch 12: 351.127798
	Epoch 13....
Epoch has taken 0:00:11.866497
Number of used sentences in train = 231
Total loss for epoch 13: 350.237787
	Epoch 14....
Epoch has taken 0:00:11.707027
Number of used sentences in train = 231
Total loss for epoch 14: 349.935161
Epoch has taken 0:00:11.981679

==================================================================================================
	Training time : 0:31:52.197101
==================================================================================================
	Identification : 0.182

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(3369, 114)
  (lstm): LSTM(140, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 13995.573884
validation loss after epoch 0 : 1128.131481
	Epoch 1....
Epoch has taken 0:03:45.984308
Number of used sentences in train = 3226
Total loss for epoch 1: 9589.267192
validation loss after epoch 1 : 1079.924151
	Epoch 2....
Epoch has taken 0:03:40.735869
Number of used sentences in train = 3226
Total loss for epoch 2: 8462.214205
validation loss after epoch 2 : 1117.764035
	Epoch 3....
Epoch has taken 0:03:41.900160
Number of used sentences in train = 3226
Total loss for epoch 3: 7779.881326
validation loss after epoch 3 : 1150.337199
	Epoch 4....
Epoch has taken 0:03:35.553637
Number of used sentences in train = 3226
Total loss for epoch 4: 7335.836884
validation loss after epoch 4 : 1183.929089
	Epoch 5....
Epoch has taken 0:03:35.382121
Number of used sentences in train = 3226
Total loss for epoch 5: 6975.016221
validation loss after epoch 5 : 1249.379168
	Epoch 6....
Epoch has taken 0:03:35.176136
Number of used sentences in train = 3226
Total loss for epoch 6: 6769.878531
validation loss after epoch 6 : 1321.375772
	Epoch 7....
Epoch has taken 0:03:35.532106
Number of used sentences in train = 3226
Total loss for epoch 7: 6597.747397
validation loss after epoch 7 : 1367.409826
	Epoch 8....
Epoch has taken 0:03:35.108479
Number of used sentences in train = 3226
Total loss for epoch 8: 6508.282701
validation loss after epoch 8 : 1384.662778
	Epoch 9....
Epoch has taken 0:03:35.266327
Number of used sentences in train = 3226
Total loss for epoch 9: 6418.879786
validation loss after epoch 9 : 1463.264860
	Epoch 10....
Epoch has taken 0:03:35.304460
Number of used sentences in train = 3226
Total loss for epoch 10: 6357.912250
validation loss after epoch 10 : 1462.528133
	Epoch 11....
Epoch has taken 0:03:35.526708
Number of used sentences in train = 3226
Total loss for epoch 11: 6312.826519
validation loss after epoch 11 : 1513.530633
	Epoch 12....
Epoch has taken 0:03:35.266594
Number of used sentences in train = 3226
Total loss for epoch 12: 6287.161705
validation loss after epoch 12 : 1607.279652
	Epoch 13....
Epoch has taken 0:03:38.670580
Number of used sentences in train = 3226
Total loss for epoch 13: 6267.912536
validation loss after epoch 13 : 1565.724455
	Epoch 14....
Epoch has taken 0:03:39.489026
Number of used sentences in train = 3226
Total loss for epoch 14: 6222.756251
validation loss after epoch 14 : 1626.315042
	TransitionClassifier(
  (p_embeddings): Embedding(13, 26)
  (w_embeddings): Embedding(3369, 114)
  (lstm): LSTM(140, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=33, bias=True)
  (linear2): Linear(in_features=33, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:39.185446
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1659.147803
	Epoch 1....
Epoch has taken 0:00:21.016284
Number of used sentences in train = 359
Total loss for epoch 1: 943.889846
	Epoch 2....
Epoch has taken 0:00:20.999602
Number of used sentences in train = 359
Total loss for epoch 2: 785.422716
	Epoch 3....
Epoch has taken 0:00:21.002979
Number of used sentences in train = 359
Total loss for epoch 3: 737.852917
	Epoch 4....
Epoch has taken 0:00:21.005592
Number of used sentences in train = 359
Total loss for epoch 4: 698.677374
	Epoch 5....
Epoch has taken 0:00:21.022325
Number of used sentences in train = 359
Total loss for epoch 5: 686.074871
	Epoch 6....
Epoch has taken 0:00:21.531352
Number of used sentences in train = 359
Total loss for epoch 6: 680.588973
	Epoch 7....
Epoch has taken 0:00:21.004866
Number of used sentences in train = 359
Total loss for epoch 7: 674.478842
	Epoch 8....
Epoch has taken 0:00:21.003799
Number of used sentences in train = 359
Total loss for epoch 8: 672.568377
	Epoch 9....
Epoch has taken 0:00:21.018981
Number of used sentences in train = 359
Total loss for epoch 9: 671.910166
	Epoch 10....
Epoch has taken 0:00:21.027201
Number of used sentences in train = 359
Total loss for epoch 10: 671.535577
	Epoch 11....
Epoch has taken 0:00:21.001836
Number of used sentences in train = 359
Total loss for epoch 11: 671.271530
	Epoch 12....
Epoch has taken 0:00:21.004175
Number of used sentences in train = 359
Total loss for epoch 12: 671.070169
	Epoch 13....
Epoch has taken 0:00:21.020949
Number of used sentences in train = 359
Total loss for epoch 13: 670.900650
	Epoch 14....
Epoch has taken 0:00:21.020503
Number of used sentences in train = 359
Total loss for epoch 14: 670.768337
Epoch has taken 0:00:21.015273

==================================================================================================
	Training time : 0:59:40.444457
==================================================================================================
	Identification : 0.355

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 142, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 70, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 62, 'lstmDropout': 0.35, 'denseActivation': 'tanh', 'wordDim': 229, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(9292, 229)
  (lstm): LSTM(299, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=142, bias=True)
  (linear2): Linear(in_features=142, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15442.916580
validation loss after epoch 0 : 1208.398732
	Epoch 1....
Epoch has taken 0:02:47.916250
Number of used sentences in train = 2811
Total loss for epoch 1: 8750.281021
validation loss after epoch 1 : 1155.618645
	Epoch 2....
Epoch has taken 0:02:43.246091
Number of used sentences in train = 2811
Total loss for epoch 2: 6569.705402
validation loss after epoch 2 : 1223.717637
	Epoch 3....
Epoch has taken 0:02:48.878947
Number of used sentences in train = 2811
Total loss for epoch 3: 5656.149959
validation loss after epoch 3 : 1367.936768
	Epoch 4....
Epoch has taken 0:02:48.554240
Number of used sentences in train = 2811
Total loss for epoch 4: 5178.306410
validation loss after epoch 4 : 1410.030761
	Epoch 5....
Epoch has taken 0:02:47.577712
Number of used sentences in train = 2811
Total loss for epoch 5: 4899.356282
validation loss after epoch 5 : 1451.888193
	Epoch 6....
Epoch has taken 0:02:47.136112
Number of used sentences in train = 2811
Total loss for epoch 6: 4722.363318
validation loss after epoch 6 : 1520.192467
	Epoch 7....
Epoch has taken 0:02:44.390308
Number of used sentences in train = 2811
Total loss for epoch 7: 4622.157044
validation loss after epoch 7 : 1613.774935
	Epoch 8....
Epoch has taken 0:02:42.681013
Number of used sentences in train = 2811
Total loss for epoch 8: 4565.503973
validation loss after epoch 8 : 1625.295229
	Epoch 9....
Epoch has taken 0:02:42.538738
Number of used sentences in train = 2811
Total loss for epoch 9: 4530.173531
validation loss after epoch 9 : 1666.593358
	Epoch 10....
Epoch has taken 0:02:44.660920
Number of used sentences in train = 2811
Total loss for epoch 10: 4512.582316
validation loss after epoch 10 : 1722.249545
	Epoch 11....
Epoch has taken 0:02:47.280313
Number of used sentences in train = 2811
Total loss for epoch 11: 4507.624924
validation loss after epoch 11 : 1742.855960
	Epoch 12....
Epoch has taken 0:02:47.075496
Number of used sentences in train = 2811
Total loss for epoch 12: 4497.965357
validation loss after epoch 12 : 1766.514166
	Epoch 13....
Epoch has taken 0:02:44.925052
Number of used sentences in train = 2811
Total loss for epoch 13: 4492.048893
validation loss after epoch 13 : 1790.780620
	Epoch 14....
Epoch has taken 0:02:54.836896
Number of used sentences in train = 2811
Total loss for epoch 14: 4485.578077
validation loss after epoch 14 : 1814.601402
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(9292, 229)
  (lstm): LSTM(299, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=142, bias=True)
  (linear2): Linear(in_features=142, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:03.503835
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2436.740793
	Epoch 1....
Epoch has taken 0:00:19.584663
Number of used sentences in train = 313
Total loss for epoch 1: 744.322698
	Epoch 2....
Epoch has taken 0:00:19.602061
Number of used sentences in train = 313
Total loss for epoch 2: 573.577779
	Epoch 3....
Epoch has taken 0:00:19.591133
Number of used sentences in train = 313
Total loss for epoch 3: 536.167439
	Epoch 4....
Epoch has taken 0:00:19.595633
Number of used sentences in train = 313
Total loss for epoch 4: 516.924903
	Epoch 5....
Epoch has taken 0:00:19.591269
Number of used sentences in train = 313
Total loss for epoch 5: 510.454777
	Epoch 6....
Epoch has taken 0:00:19.612571
Number of used sentences in train = 313
Total loss for epoch 6: 506.970299
	Epoch 7....
Epoch has taken 0:00:19.596080
Number of used sentences in train = 313
Total loss for epoch 7: 508.232298
	Epoch 8....
Epoch has taken 0:00:19.609103
Number of used sentences in train = 313
Total loss for epoch 8: 507.166170
	Epoch 9....
Epoch has taken 0:00:19.606784
Number of used sentences in train = 313
Total loss for epoch 9: 505.923291
	Epoch 10....
Epoch has taken 0:00:19.588227
Number of used sentences in train = 313
Total loss for epoch 10: 504.723415
	Epoch 11....
Epoch has taken 0:00:19.605201
Number of used sentences in train = 313
Total loss for epoch 11: 504.097609
	Epoch 12....
Epoch has taken 0:00:19.587418
Number of used sentences in train = 313
Total loss for epoch 12: 504.204900
	Epoch 13....
Epoch has taken 0:00:19.599695
Number of used sentences in train = 313
Total loss for epoch 13: 502.855948
	Epoch 14....
Epoch has taken 0:00:19.583354
Number of used sentences in train = 313
Total loss for epoch 14: 502.434541
Epoch has taken 0:00:19.594669

==================================================================================================
	Training time : 0:46:49.680168
==================================================================================================
	Identification : 0.446

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(7076, 229)
  (lstm): LSTM(299, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=142, bias=True)
  (linear2): Linear(in_features=142, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11137.629819
validation loss after epoch 0 : 1043.645249
	Epoch 1....
Epoch has taken 0:02:05.460358
Number of used sentences in train = 2074
Total loss for epoch 1: 6234.301388
validation loss after epoch 1 : 1044.730003
	Epoch 2....
Epoch has taken 0:02:05.425379
Number of used sentences in train = 2074
Total loss for epoch 2: 4525.920722
validation loss after epoch 2 : 1217.944509
	Epoch 3....
Epoch has taken 0:02:05.598796
Number of used sentences in train = 2074
Total loss for epoch 3: 3736.679695
validation loss after epoch 3 : 1312.087736
	Epoch 4....
Epoch has taken 0:02:05.665610
Number of used sentences in train = 2074
Total loss for epoch 4: 3407.533320
validation loss after epoch 4 : 1429.152698
	Epoch 5....
Epoch has taken 0:02:05.513947
Number of used sentences in train = 2074
Total loss for epoch 5: 3287.864858
validation loss after epoch 5 : 1495.081704
	Epoch 6....
Epoch has taken 0:02:05.599810
Number of used sentences in train = 2074
Total loss for epoch 6: 3219.352287
validation loss after epoch 6 : 1555.433776
	Epoch 7....
Epoch has taken 0:02:05.049288
Number of used sentences in train = 2074
Total loss for epoch 7: 3197.257186
validation loss after epoch 7 : 1596.245737
	Epoch 8....
Epoch has taken 0:02:05.472172
Number of used sentences in train = 2074
Total loss for epoch 8: 3180.943148
validation loss after epoch 8 : 1660.698701
	Epoch 9....
Epoch has taken 0:02:03.616528
Number of used sentences in train = 2074
Total loss for epoch 9: 3167.970439
validation loss after epoch 9 : 1672.805842
	Epoch 10....
Epoch has taken 0:02:00.326524
Number of used sentences in train = 2074
Total loss for epoch 10: 3163.454777
validation loss after epoch 10 : 1703.166214
	Epoch 11....
Epoch has taken 0:02:04.787210
Number of used sentences in train = 2074
Total loss for epoch 11: 3160.718790
validation loss after epoch 11 : 1726.970056
	Epoch 12....
Epoch has taken 0:02:05.642637
Number of used sentences in train = 2074
Total loss for epoch 12: 3158.834884
validation loss after epoch 12 : 1746.519364
	Epoch 13....
Epoch has taken 0:02:05.765195
Number of used sentences in train = 2074
Total loss for epoch 13: 3157.410175
validation loss after epoch 13 : 1765.373588
	Epoch 14....
Epoch has taken 0:02:05.640323
Number of used sentences in train = 2074
Total loss for epoch 14: 3156.283947
validation loss after epoch 14 : 1784.387436
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(7076, 229)
  (lstm): LSTM(299, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=142, bias=True)
  (linear2): Linear(in_features=142, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:54.063347
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 3030.530340
	Epoch 1....
Epoch has taken 0:00:11.694544
Number of used sentences in train = 231
Total loss for epoch 1: 629.721158
	Epoch 2....
Epoch has taken 0:00:11.687532
Number of used sentences in train = 231
Total loss for epoch 2: 413.460813
	Epoch 3....
Epoch has taken 0:00:11.742123
Number of used sentences in train = 231
Total loss for epoch 3: 374.880041
	Epoch 4....
Epoch has taken 0:00:11.706801
Number of used sentences in train = 231
Total loss for epoch 4: 361.145709
	Epoch 5....
Epoch has taken 0:00:11.654773
Number of used sentences in train = 231
Total loss for epoch 5: 354.383278
	Epoch 6....
Epoch has taken 0:00:11.600392
Number of used sentences in train = 231
Total loss for epoch 6: 351.302205
	Epoch 7....
Epoch has taken 0:00:11.581704
Number of used sentences in train = 231
Total loss for epoch 7: 349.857748
	Epoch 8....
Epoch has taken 0:00:11.470154
Number of used sentences in train = 231
Total loss for epoch 8: 348.414075
	Epoch 9....
Epoch has taken 0:00:11.513747
Number of used sentences in train = 231
Total loss for epoch 9: 347.631468
	Epoch 10....
Epoch has taken 0:00:11.675227
Number of used sentences in train = 231
Total loss for epoch 10: 347.103437
	Epoch 11....
Epoch has taken 0:00:11.684327
Number of used sentences in train = 231
Total loss for epoch 11: 346.762668
	Epoch 12....
Epoch has taken 0:00:11.607068
Number of used sentences in train = 231
Total loss for epoch 12: 346.385916
	Epoch 13....
Epoch has taken 0:00:11.624551
Number of used sentences in train = 231
Total loss for epoch 13: 346.133776
	Epoch 14....
Epoch has taken 0:00:11.582433
Number of used sentences in train = 231
Total loss for epoch 14: 345.975535
Epoch has taken 0:00:11.688654

==================================================================================================
	Training time : 0:33:58.497031
==================================================================================================
	Identification : 0.367

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 70)
  (w_embeddings): Embedding(18020, 229)
  (lstm): LSTM(299, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=142, bias=True)
  (linear2): Linear(in_features=142, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18084.110709
validation loss after epoch 0 : 1706.846516
	Epoch 1....
Epoch has taken 0:03:43.613734
Number of used sentences in train = 3226
Total loss for epoch 1: 11257.735574
validation loss after epoch 1 : 1603.044489
	Epoch 2....
Epoch has taken 0:03:48.893907
Number of used sentences in train = 3226
Total loss for epoch 2: 8518.908779
validation loss after epoch 2 : 1795.861153
	Epoch 3....
Epoch has taken 0:03:45.707240
Number of used sentences in train = 3226
Total loss for epoch 3: 7152.963571
validation loss after epoch 3 : 2011.477769
	Epoch 4....
Epoch has taken 0:03:45.591125
Number of used sentences in train = 3226
Total loss for epoch 4: 6607.523249
validation loss after epoch 4 : 2177.423704
	Epoch 5....
Epoch has taken 0:03:46.336643
Number of used sentences in train = 3226
Total loss for epoch 5: 6378.110656
validation loss after epoch 5 : 2248.018953
	Epoch 6....
Epoch has taken 0:03:46.843191
Number of used sentences in train = 3226
Total loss for epoch 6: 6263.738497
validation loss after epoch 6 : 2420.530572
	Epoch 7....
Epoch has taken 0:03:49.909673
Number of used sentences in train = 3226
Total loss for epoch 7: 6207.012058
validation loss after epoch 7 : 2498.982771
	Epoch 8....
Epoch has taken 0:03:46.322695
Number of used sentences in train = 3226
Total loss for epoch 8: 6193.176528
validation loss after epoch 8 : 2515.396302
	Epoch 9....
Epoch has taken 0:03:45.297984
Number of used sentences in train = 3226
Total loss for epoch 9: 6169.545822
validation loss after epoch 9 : 2501.741333
	Epoch 10....
Epoch has taken 0:03:53.394078
Number of used sentences in train = 3226
Total loss for epoch 10: 6166.161795
validation loss after epoch 10 : 2560.889071
	Epoch 11....
Epoch has taken 0:03:48.417640
Number of used sentences in train = 3226
Total loss for epoch 11: 6161.690135
validation loss after epoch 11 : 2593.235658
	Epoch 12....
Epoch has taken 0:03:49.676150
Number of used sentences in train = 3226
Total loss for epoch 12: 6153.358445
validation loss after epoch 12 : 2643.106534
	Epoch 13....
Epoch has taken 0:04:11.192619
Number of used sentences in train = 3226
Total loss for epoch 13: 6150.517230
validation loss after epoch 13 : 2668.167805
	Epoch 14....
Epoch has taken 0:04:01.742050
Number of used sentences in train = 3226
Total loss for epoch 14: 6147.052325
validation loss after epoch 14 : 2681.602786
	TransitionClassifier(
  (p_embeddings): Embedding(13, 70)
  (w_embeddings): Embedding(18020, 229)
  (lstm): LSTM(299, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=142, bias=True)
  (linear2): Linear(in_features=142, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:05.955424
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2591.774012
	Epoch 1....
Epoch has taken 0:00:23.891336
Number of used sentences in train = 359
Total loss for epoch 1: 1032.697108
	Epoch 2....
Epoch has taken 0:00:24.163737
Number of used sentences in train = 359
Total loss for epoch 2: 776.210408
	Epoch 3....
Epoch has taken 0:00:24.083035
Number of used sentences in train = 359
Total loss for epoch 3: 702.078550
	Epoch 4....
Epoch has taken 0:00:24.087765
Number of used sentences in train = 359
Total loss for epoch 4: 680.272482
	Epoch 5....
Epoch has taken 0:00:24.054699
Number of used sentences in train = 359
Total loss for epoch 5: 675.240207
	Epoch 6....
Epoch has taken 0:00:24.061237
Number of used sentences in train = 359
Total loss for epoch 6: 673.359530
	Epoch 7....
Epoch has taken 0:00:24.088573
Number of used sentences in train = 359
Total loss for epoch 7: 672.481171
	Epoch 8....
Epoch has taken 0:00:24.069385
Number of used sentences in train = 359
Total loss for epoch 8: 671.965854
	Epoch 9....
Epoch has taken 0:00:24.078967
Number of used sentences in train = 359
Total loss for epoch 9: 671.596752
	Epoch 10....
Epoch has taken 0:00:24.076738
Number of used sentences in train = 359
Total loss for epoch 10: 671.350659
	Epoch 11....
Epoch has taken 0:00:24.083804
Number of used sentences in train = 359
Total loss for epoch 11: 671.151685
	Epoch 12....
Epoch has taken 0:00:24.060075
Number of used sentences in train = 359
Total loss for epoch 12: 670.978872
	Epoch 13....
Epoch has taken 0:00:24.077815
Number of used sentences in train = 359
Total loss for epoch 13: 670.838585
	Epoch 14....
Epoch has taken 0:00:24.075936
Number of used sentences in train = 359
Total loss for epoch 14: 670.725776
Epoch has taken 0:00:24.066986

==================================================================================================
	Training time : 1:03:50.631450
==================================================================================================
	Identification : 0.266

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 22, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 33, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 34, 'lstmDropout': 0.28, 'denseActivation': 'tanh', 'wordDim': 72, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(9305, 72)
  (lstm): LSTM(105, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13371.219741
validation loss after epoch 0 : 1200.502006
	Epoch 1....
Epoch has taken 0:03:01.490540
Number of used sentences in train = 2811
Total loss for epoch 1: 9255.126588
validation loss after epoch 1 : 1119.185316
	Epoch 2....
Epoch has taken 0:02:46.554067
Number of used sentences in train = 2811
Total loss for epoch 2: 7370.322817
validation loss after epoch 2 : 1148.054705
	Epoch 3....
Epoch has taken 0:02:44.770450
Number of used sentences in train = 2811
Total loss for epoch 3: 6286.685575
validation loss after epoch 3 : 1172.773067
	Epoch 4....
Epoch has taken 0:02:45.162311
Number of used sentences in train = 2811
Total loss for epoch 4: 5636.020041
validation loss after epoch 4 : 1262.059472
	Epoch 5....
Epoch has taken 0:02:46.565097
Number of used sentences in train = 2811
Total loss for epoch 5: 5249.847981
validation loss after epoch 5 : 1290.527198
	Epoch 6....
Epoch has taken 0:02:46.498185
Number of used sentences in train = 2811
Total loss for epoch 6: 4992.405806
validation loss after epoch 6 : 1354.030887
	Epoch 7....
Epoch has taken 0:02:46.463375
Number of used sentences in train = 2811
Total loss for epoch 7: 4836.429432
validation loss after epoch 7 : 1399.755309
	Epoch 8....
Epoch has taken 0:02:42.426400
Number of used sentences in train = 2811
Total loss for epoch 8: 4732.010292
validation loss after epoch 8 : 1434.663093
	Epoch 9....
Epoch has taken 0:02:41.708070
Number of used sentences in train = 2811
Total loss for epoch 9: 4655.353631
validation loss after epoch 9 : 1462.654765
	Epoch 10....
Epoch has taken 0:02:45.778460
Number of used sentences in train = 2811
Total loss for epoch 10: 4613.416643
validation loss after epoch 10 : 1493.681755
	Epoch 11....
Epoch has taken 0:02:41.783892
Number of used sentences in train = 2811
Total loss for epoch 11: 4591.592224
validation loss after epoch 11 : 1515.047326
	Epoch 12....
Epoch has taken 0:02:41.753481
Number of used sentences in train = 2811
Total loss for epoch 12: 4574.624124
validation loss after epoch 12 : 1546.170405
	Epoch 13....
Epoch has taken 0:02:41.832391
Number of used sentences in train = 2811
Total loss for epoch 13: 4575.790771
validation loss after epoch 13 : 1561.408684
	Epoch 14....
Epoch has taken 0:02:41.621959
Number of used sentences in train = 2811
Total loss for epoch 14: 4558.231617
validation loss after epoch 14 : 1583.804901
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(9305, 72)
  (lstm): LSTM(105, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:42.941248
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1726.230009
	Epoch 1....
Epoch has taken 0:00:17.110277
Number of used sentences in train = 313
Total loss for epoch 1: 796.255338
	Epoch 2....
Epoch has taken 0:00:17.105147
Number of used sentences in train = 313
Total loss for epoch 2: 617.380327
	Epoch 3....
Epoch has taken 0:00:17.094438
Number of used sentences in train = 313
Total loss for epoch 3: 556.182463
	Epoch 4....
Epoch has taken 0:00:17.097520
Number of used sentences in train = 313
Total loss for epoch 4: 530.141358
	Epoch 5....
Epoch has taken 0:00:17.115219
Number of used sentences in train = 313
Total loss for epoch 5: 517.261630
	Epoch 6....
Epoch has taken 0:00:17.103519
Number of used sentences in train = 313
Total loss for epoch 6: 513.181418
	Epoch 7....
Epoch has taken 0:00:17.106664
Number of used sentences in train = 313
Total loss for epoch 7: 511.372431
	Epoch 8....
Epoch has taken 0:00:17.099639
Number of used sentences in train = 313
Total loss for epoch 8: 510.317799
	Epoch 9....
Epoch has taken 0:00:17.101766
Number of used sentences in train = 313
Total loss for epoch 9: 509.553374
	Epoch 10....
Epoch has taken 0:00:17.096974
Number of used sentences in train = 313
Total loss for epoch 10: 508.883901
	Epoch 11....
Epoch has taken 0:00:17.100973
Number of used sentences in train = 313
Total loss for epoch 11: 507.743687
	Epoch 12....
Epoch has taken 0:00:17.462363
Number of used sentences in train = 313
Total loss for epoch 12: 506.269658
	Epoch 13....
Epoch has taken 0:00:17.145941
Number of used sentences in train = 313
Total loss for epoch 13: 505.757149
	Epoch 14....
Epoch has taken 0:00:17.106658
Number of used sentences in train = 313
Total loss for epoch 14: 505.323732
Epoch has taken 0:00:17.104319

==================================================================================================
	Training time : 0:45:34.810222
==================================================================================================
	Identification : 0.465

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(7054, 72)
  (lstm): LSTM(105, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10778.349195
validation loss after epoch 0 : 1042.252063
	Epoch 1....
Epoch has taken 0:01:51.825972
Number of used sentences in train = 2074
Total loss for epoch 1: 6783.761514
validation loss after epoch 1 : 1087.274735
	Epoch 2....
Epoch has taken 0:01:51.861743
Number of used sentences in train = 2074
Total loss for epoch 2: 5252.705765
validation loss after epoch 2 : 1128.727814
	Epoch 3....
Epoch has taken 0:01:51.996677
Number of used sentences in train = 2074
Total loss for epoch 3: 4422.447367
validation loss after epoch 3 : 1231.633867
	Epoch 4....
Epoch has taken 0:01:51.867373
Number of used sentences in train = 2074
Total loss for epoch 4: 3858.159739
validation loss after epoch 4 : 1341.088667
	Epoch 5....
Epoch has taken 0:01:51.777776
Number of used sentences in train = 2074
Total loss for epoch 5: 3580.198077
validation loss after epoch 5 : 1433.474252
	Epoch 6....
Epoch has taken 0:01:51.858405
Number of used sentences in train = 2074
Total loss for epoch 6: 3434.595471
validation loss after epoch 6 : 1481.813309
	Epoch 7....
Epoch has taken 0:01:52.558988
Number of used sentences in train = 2074
Total loss for epoch 7: 3334.409977
validation loss after epoch 7 : 1570.606953
	Epoch 8....
Epoch has taken 0:01:51.943344
Number of used sentences in train = 2074
Total loss for epoch 8: 3293.411780
validation loss after epoch 8 : 1635.486367
	Epoch 9....
Epoch has taken 0:01:51.924383
Number of used sentences in train = 2074
Total loss for epoch 9: 3263.918266
validation loss after epoch 9 : 1667.247013
	Epoch 10....
Epoch has taken 0:01:52.116257
Number of used sentences in train = 2074
Total loss for epoch 10: 3235.803685
validation loss after epoch 10 : 1688.971845
	Epoch 11....
Epoch has taken 0:01:54.317405
Number of used sentences in train = 2074
Total loss for epoch 11: 3211.926706
validation loss after epoch 11 : 1697.833183
	Epoch 12....
Epoch has taken 0:01:55.199433
Number of used sentences in train = 2074
Total loss for epoch 12: 3193.391209
validation loss after epoch 12 : 1722.264627
	Epoch 13....
Epoch has taken 0:01:55.515357
Number of used sentences in train = 2074
Total loss for epoch 13: 3184.969261
validation loss after epoch 13 : 1768.515075
	Epoch 14....
Epoch has taken 0:01:53.485737
Number of used sentences in train = 2074
Total loss for epoch 14: 3181.167527
validation loss after epoch 14 : 1771.782531
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(7054, 72)
  (lstm): LSTM(105, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:53.019250
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1503.874789
	Epoch 1....
Epoch has taken 0:00:11.692676
Number of used sentences in train = 231
Total loss for epoch 1: 689.457184
	Epoch 2....
Epoch has taken 0:00:11.682166
Number of used sentences in train = 231
Total loss for epoch 2: 479.387869
	Epoch 3....
Epoch has taken 0:00:11.695281
Number of used sentences in train = 231
Total loss for epoch 3: 409.629655
	Epoch 4....
Epoch has taken 0:00:11.685353
Number of used sentences in train = 231
Total loss for epoch 4: 382.517163
	Epoch 5....
Epoch has taken 0:00:11.682437
Number of used sentences in train = 231
Total loss for epoch 5: 370.307554
	Epoch 6....
Epoch has taken 0:00:11.680797
Number of used sentences in train = 231
Total loss for epoch 6: 368.635883
	Epoch 7....
Epoch has taken 0:00:11.681272
Number of used sentences in train = 231
Total loss for epoch 7: 360.274429
	Epoch 8....
Epoch has taken 0:00:11.690432
Number of used sentences in train = 231
Total loss for epoch 8: 355.659466
	Epoch 9....
Epoch has taken 0:00:11.693803
Number of used sentences in train = 231
Total loss for epoch 9: 352.375629
	Epoch 10....
Epoch has taken 0:00:11.690998
Number of used sentences in train = 231
Total loss for epoch 10: 350.427248
	Epoch 11....
Epoch has taken 0:00:11.692357
Number of used sentences in train = 231
Total loss for epoch 11: 349.256379
	Epoch 12....
Epoch has taken 0:00:11.695614
Number of used sentences in train = 231
Total loss for epoch 12: 348.865981
	Epoch 13....
Epoch has taken 0:00:11.685419
Number of used sentences in train = 231
Total loss for epoch 13: 348.538675
	Epoch 14....
Epoch has taken 0:00:11.675782
Number of used sentences in train = 231
Total loss for epoch 14: 348.241541
Epoch has taken 0:00:11.688341

==================================================================================================
	Training time : 0:31:06.931386
==================================================================================================
	Identification : 0.35

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 33)
  (w_embeddings): Embedding(18063, 72)
  (lstm): LSTM(105, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19652.131595
validation loss after epoch 0 : 1641.535898
	Epoch 1....
Epoch has taken 0:03:41.947414
Number of used sentences in train = 3226
Total loss for epoch 1: 12768.011282
validation loss after epoch 1 : 1575.098816
	Epoch 2....
Epoch has taken 0:03:40.555394
Number of used sentences in train = 3226
Total loss for epoch 2: 10373.770583
validation loss after epoch 2 : 1653.108020
	Epoch 3....
Epoch has taken 0:03:42.073712
Number of used sentences in train = 3226
Total loss for epoch 3: 8916.907669
validation loss after epoch 3 : 1777.258856
	Epoch 4....
Epoch has taken 0:03:41.881627
Number of used sentences in train = 3226
Total loss for epoch 4: 7890.326159
validation loss after epoch 4 : 1943.426314
	Epoch 5....
Epoch has taken 0:03:41.768563
Number of used sentences in train = 3226
Total loss for epoch 5: 7261.668254
validation loss after epoch 5 : 2087.593167
	Epoch 6....
Epoch has taken 0:03:42.011169
Number of used sentences in train = 3226
Total loss for epoch 6: 6851.892179
validation loss after epoch 6 : 2245.559687
	Epoch 7....
Epoch has taken 0:03:41.853563
Number of used sentences in train = 3226
Total loss for epoch 7: 6611.968857
validation loss after epoch 7 : 2357.239480
	Epoch 8....
Epoch has taken 0:03:41.308956
Number of used sentences in train = 3226
Total loss for epoch 8: 6455.753868
validation loss after epoch 8 : 2407.978000
	Epoch 9....
Epoch has taken 0:03:42.116892
Number of used sentences in train = 3226
Total loss for epoch 9: 6379.109482
validation loss after epoch 9 : 2467.009836
	Epoch 10....
Epoch has taken 0:03:42.038389
Number of used sentences in train = 3226
Total loss for epoch 10: 6319.301999
validation loss after epoch 10 : 2557.942633
	Epoch 11....
Epoch has taken 0:03:41.095956
Number of used sentences in train = 3226
Total loss for epoch 11: 6280.113791
validation loss after epoch 11 : 2580.243223
	Epoch 12....
Epoch has taken 0:03:42.207265
Number of used sentences in train = 3226
Total loss for epoch 12: 6258.773498
validation loss after epoch 12 : 2597.738596
	Epoch 13....
Epoch has taken 0:03:41.793976
Number of used sentences in train = 3226
Total loss for epoch 13: 6230.640284
validation loss after epoch 13 : 2575.892787
	Epoch 14....
Epoch has taken 0:03:41.249886
Number of used sentences in train = 3226
Total loss for epoch 14: 6218.633840
validation loss after epoch 14 : 2671.751759
	TransitionClassifier(
  (p_embeddings): Embedding(13, 33)
  (w_embeddings): Embedding(18063, 72)
  (lstm): LSTM(105, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=22, bias=True)
  (linear2): Linear(in_features=22, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:44.793372
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2073.312485
	Epoch 1....
Epoch has taken 0:00:21.700933
Number of used sentences in train = 359
Total loss for epoch 1: 1162.475855
	Epoch 2....
Epoch has taken 0:00:21.683844
Number of used sentences in train = 359
Total loss for epoch 2: 875.849503
	Epoch 3....
Epoch has taken 0:00:21.710561
Number of used sentences in train = 359
Total loss for epoch 3: 746.016895
	Epoch 4....
Epoch has taken 0:00:21.696179
Number of used sentences in train = 359
Total loss for epoch 4: 702.840530
	Epoch 5....
Epoch has taken 0:00:21.682688
Number of used sentences in train = 359
Total loss for epoch 5: 689.211608
	Epoch 6....
Epoch has taken 0:00:21.700478
Number of used sentences in train = 359
Total loss for epoch 6: 684.093488
	Epoch 7....
Epoch has taken 0:00:21.712865
Number of used sentences in train = 359
Total loss for epoch 7: 676.279102
	Epoch 8....
Epoch has taken 0:00:21.723856
Number of used sentences in train = 359
Total loss for epoch 8: 674.676835
	Epoch 9....
Epoch has taken 0:00:21.723696
Number of used sentences in train = 359
Total loss for epoch 9: 673.760573
	Epoch 10....
Epoch has taken 0:00:21.713658
Number of used sentences in train = 359
Total loss for epoch 10: 673.119089
	Epoch 11....
Epoch has taken 0:00:21.707744
Number of used sentences in train = 359
Total loss for epoch 11: 672.708157
	Epoch 12....
Epoch has taken 0:00:21.516559
Number of used sentences in train = 359
Total loss for epoch 12: 672.765435
	Epoch 13....
Epoch has taken 0:00:21.714180
Number of used sentences in train = 359
Total loss for epoch 13: 672.211056
	Epoch 14....
Epoch has taken 0:00:21.703594
Number of used sentences in train = 359
Total loss for epoch 14: 671.942479
Epoch has taken 0:00:21.701505

==================================================================================================
	Training time : 1:00:54.768696
==================================================================================================
	Identification : 0.42

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 41, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 46, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 125, 'lstmDropout': 0.12, 'denseActivation': 'tanh', 'wordDim': 102, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 46)
  (w_embeddings): Embedding(5865, 102)
  (lstm): LSTM(148, 125, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13841.482021
validation loss after epoch 0 : 1148.745705
	Epoch 1....
Epoch has taken 0:02:47.259560
Number of used sentences in train = 2811
Total loss for epoch 1: 9323.669721
validation loss after epoch 1 : 1102.430579
	Epoch 2....
Epoch has taken 0:02:47.305020
Number of used sentences in train = 2811
Total loss for epoch 2: 7747.798578
validation loss after epoch 2 : 1090.706625
	Epoch 3....
Epoch has taken 0:02:47.177442
Number of used sentences in train = 2811
Total loss for epoch 3: 6707.243313
validation loss after epoch 3 : 1159.255528
	Epoch 4....
Epoch has taken 0:02:47.160415
Number of used sentences in train = 2811
Total loss for epoch 4: 5977.460811
validation loss after epoch 4 : 1181.668000
	Epoch 5....
Epoch has taken 0:02:47.399922
Number of used sentences in train = 2811
Total loss for epoch 5: 5531.779532
validation loss after epoch 5 : 1217.723628
	Epoch 6....
Epoch has taken 0:02:47.159077
Number of used sentences in train = 2811
Total loss for epoch 6: 5205.450724
validation loss after epoch 6 : 1294.988159
	Epoch 7....
Epoch has taken 0:02:47.161072
Number of used sentences in train = 2811
Total loss for epoch 7: 5032.136705
validation loss after epoch 7 : 1319.423245
	Epoch 8....
Epoch has taken 0:02:47.526162
Number of used sentences in train = 2811
Total loss for epoch 8: 4920.124183
validation loss after epoch 8 : 1351.478791
	Epoch 9....
Epoch has taken 0:02:47.110571
Number of used sentences in train = 2811
Total loss for epoch 9: 4852.153467
validation loss after epoch 9 : 1371.793016
	Epoch 10....
Epoch has taken 0:02:47.144079
Number of used sentences in train = 2811
Total loss for epoch 10: 4796.594424
validation loss after epoch 10 : 1378.410955
	Epoch 11....
Epoch has taken 0:02:47.188675
Number of used sentences in train = 2811
Total loss for epoch 11: 4750.990641
validation loss after epoch 11 : 1442.046090
	Epoch 12....
Epoch has taken 0:02:47.222911
Number of used sentences in train = 2811
Total loss for epoch 12: 4708.576681
validation loss after epoch 12 : 1449.129090
	Epoch 13....
Epoch has taken 0:02:47.294633
Number of used sentences in train = 2811
Total loss for epoch 13: 4672.242793
validation loss after epoch 13 : 1475.311819
	Epoch 14....
Epoch has taken 0:02:47.234303
Number of used sentences in train = 2811
Total loss for epoch 14: 4639.378592
validation loss after epoch 14 : 1456.029670
	TransitionClassifier(
  (p_embeddings): Embedding(18, 46)
  (w_embeddings): Embedding(5865, 102)
  (lstm): LSTM(148, 125, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:47.156554
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1647.269099
	Epoch 1....
Epoch has taken 0:00:17.748291
Number of used sentences in train = 313
Total loss for epoch 1: 1033.278212
	Epoch 2....
Epoch has taken 0:00:17.747287
Number of used sentences in train = 313
Total loss for epoch 2: 822.941428
	Epoch 3....
Epoch has taken 0:00:17.766890
Number of used sentences in train = 313
Total loss for epoch 3: 684.345803
	Epoch 4....
Epoch has taken 0:00:17.783907
Number of used sentences in train = 313
Total loss for epoch 4: 589.863987
	Epoch 5....
Epoch has taken 0:00:17.533724
Number of used sentences in train = 313
Total loss for epoch 5: 557.265992
	Epoch 6....
Epoch has taken 0:00:17.803106
Number of used sentences in train = 313
Total loss for epoch 6: 539.738383
	Epoch 7....
Epoch has taken 0:00:17.766354
Number of used sentences in train = 313
Total loss for epoch 7: 530.022932
	Epoch 8....
Epoch has taken 0:00:17.769098
Number of used sentences in train = 313
Total loss for epoch 8: 519.676781
	Epoch 9....
Epoch has taken 0:00:17.767001
Number of used sentences in train = 313
Total loss for epoch 9: 514.850579
	Epoch 10....
Epoch has taken 0:00:17.781105
Number of used sentences in train = 313
Total loss for epoch 10: 518.871348
	Epoch 11....
Epoch has taken 0:00:17.325755
Number of used sentences in train = 313
Total loss for epoch 11: 516.085453
	Epoch 12....
Epoch has taken 0:00:17.218333
Number of used sentences in train = 313
Total loss for epoch 12: 528.039824
	Epoch 13....
Epoch has taken 0:00:17.217610
Number of used sentences in train = 313
Total loss for epoch 13: 512.936320
	Epoch 14....
Epoch has taken 0:00:17.225083
Number of used sentences in train = 313
Total loss for epoch 14: 510.448214
Epoch has taken 0:00:17.553833

==================================================================================================
	Training time : 0:46:13.011661
==================================================================================================
	Identification : 0.147

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 46)
  (w_embeddings): Embedding(5638, 102)
  (lstm): LSTM(148, 125, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10283.192270
validation loss after epoch 0 : 973.558784
	Epoch 1....
Epoch has taken 0:01:54.338792
Number of used sentences in train = 2074
Total loss for epoch 1: 6620.704720
validation loss after epoch 1 : 829.998263
	Epoch 2....
Epoch has taken 0:01:51.384104
Number of used sentences in train = 2074
Total loss for epoch 2: 5409.018978
validation loss after epoch 2 : 886.891152
	Epoch 3....
Epoch has taken 0:01:51.219774
Number of used sentences in train = 2074
Total loss for epoch 3: 4600.164023
validation loss after epoch 3 : 911.966469
	Epoch 4....
Epoch has taken 0:01:52.387057
Number of used sentences in train = 2074
Total loss for epoch 4: 4032.074835
validation loss after epoch 4 : 968.777041
	Epoch 5....
Epoch has taken 0:01:51.271452
Number of used sentences in train = 2074
Total loss for epoch 5: 3674.579072
validation loss after epoch 5 : 1052.369217
	Epoch 6....
Epoch has taken 0:01:51.237556
Number of used sentences in train = 2074
Total loss for epoch 6: 3490.880520
validation loss after epoch 6 : 1081.413234
	Epoch 7....
Epoch has taken 0:01:51.225855
Number of used sentences in train = 2074
Total loss for epoch 7: 3375.597077
validation loss after epoch 7 : 1098.051169
	Epoch 8....
Epoch has taken 0:01:51.218446
Number of used sentences in train = 2074
Total loss for epoch 8: 3304.532067
validation loss after epoch 8 : 1145.404320
	Epoch 9....
Epoch has taken 0:01:51.188637
Number of used sentences in train = 2074
Total loss for epoch 9: 3263.118167
validation loss after epoch 9 : 1168.174280
	Epoch 10....
Epoch has taken 0:01:51.190371
Number of used sentences in train = 2074
Total loss for epoch 10: 3234.410709
validation loss after epoch 10 : 1189.371255
	Epoch 11....
Epoch has taken 0:01:51.195171
Number of used sentences in train = 2074
Total loss for epoch 11: 3220.766344
validation loss after epoch 11 : 1206.517638
	Epoch 12....
Epoch has taken 0:01:51.127482
Number of used sentences in train = 2074
Total loss for epoch 12: 3211.636838
validation loss after epoch 12 : 1224.633653
	Epoch 13....
Epoch has taken 0:01:51.135100
Number of used sentences in train = 2074
Total loss for epoch 13: 3204.253278
validation loss after epoch 13 : 1228.913293
	Epoch 14....
Epoch has taken 0:01:51.116780
Number of used sentences in train = 2074
Total loss for epoch 14: 3196.234496
validation loss after epoch 14 : 1245.496404
	TransitionClassifier(
  (p_embeddings): Embedding(18, 46)
  (w_embeddings): Embedding(5638, 102)
  (lstm): LSTM(148, 125, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.204945
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1450.806804
	Epoch 1....
Epoch has taken 0:00:11.344643
Number of used sentences in train = 231
Total loss for epoch 1: 702.736695
	Epoch 2....
Epoch has taken 0:00:11.351016
Number of used sentences in train = 231
Total loss for epoch 2: 522.255361
	Epoch 3....
Epoch has taken 0:00:11.341854
Number of used sentences in train = 231
Total loss for epoch 3: 420.173743
	Epoch 4....
Epoch has taken 0:00:11.354610
Number of used sentences in train = 231
Total loss for epoch 4: 387.513106
	Epoch 5....
Epoch has taken 0:00:11.336232
Number of used sentences in train = 231
Total loss for epoch 5: 368.743869
	Epoch 6....
Epoch has taken 0:00:11.333110
Number of used sentences in train = 231
Total loss for epoch 6: 360.410187
	Epoch 7....
Epoch has taken 0:00:11.343221
Number of used sentences in train = 231
Total loss for epoch 7: 357.570381
	Epoch 8....
Epoch has taken 0:00:11.338409
Number of used sentences in train = 231
Total loss for epoch 8: 353.945451
	Epoch 9....
Epoch has taken 0:00:11.349884
Number of used sentences in train = 231
Total loss for epoch 9: 351.874037
	Epoch 10....
Epoch has taken 0:00:11.341258
Number of used sentences in train = 231
Total loss for epoch 10: 350.434560
	Epoch 11....
Epoch has taken 0:00:11.348845
Number of used sentences in train = 231
Total loss for epoch 11: 349.012860
	Epoch 12....
Epoch has taken 0:00:11.347809
Number of used sentences in train = 231
Total loss for epoch 12: 348.426320
	Epoch 13....
Epoch has taken 0:00:11.352738
Number of used sentences in train = 231
Total loss for epoch 13: 347.976475
	Epoch 14....
Epoch has taken 0:00:11.356292
Number of used sentences in train = 231
Total loss for epoch 14: 347.481990
Epoch has taken 0:00:11.358921

==================================================================================================
	Training time : 0:30:42.983478
==================================================================================================
	Identification : 0.396

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 46)
  (w_embeddings): Embedding(6855, 102)
  (lstm): LSTM(148, 125, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17371.384273
validation loss after epoch 0 : 1555.752591
	Epoch 1....
Epoch has taken 0:03:42.841130
Number of used sentences in train = 3226
Total loss for epoch 1: 12698.346518
validation loss after epoch 1 : 1495.892626
	Epoch 2....
Epoch has taken 0:03:36.915780
Number of used sentences in train = 3226
Total loss for epoch 2: 10779.561033
validation loss after epoch 2 : 1442.970966
	Epoch 3....
Epoch has taken 0:03:37.188744
Number of used sentences in train = 3226
Total loss for epoch 3: 9466.406624
validation loss after epoch 3 : 1479.271906
	Epoch 4....
Epoch has taken 0:03:40.556044
Number of used sentences in train = 3226
Total loss for epoch 4: 8404.059024
validation loss after epoch 4 : 1694.508931
	Epoch 5....
Epoch has taken 0:03:37.108171
Number of used sentences in train = 3226
Total loss for epoch 5: 7614.884111
validation loss after epoch 5 : 1716.462726
	Epoch 6....
Epoch has taken 0:03:36.925802
Number of used sentences in train = 3226
Total loss for epoch 6: 7042.058115
validation loss after epoch 6 : 1781.314703
	Epoch 7....
Epoch has taken 0:03:42.820801
Number of used sentences in train = 3226
Total loss for epoch 7: 6698.737774
validation loss after epoch 7 : 1938.544003
	Epoch 8....
Epoch has taken 0:03:36.946072
Number of used sentences in train = 3226
Total loss for epoch 8: 6471.545445
validation loss after epoch 8 : 1980.767858
	Epoch 9....
Epoch has taken 0:03:41.760300
Number of used sentences in train = 3226
Total loss for epoch 9: 6359.551718
validation loss after epoch 9 : 2028.014178
	Epoch 10....
Epoch has taken 0:03:39.738186
Number of used sentences in train = 3226
Total loss for epoch 10: 6262.245451
validation loss after epoch 10 : 2147.035767
	Epoch 11....
Epoch has taken 0:03:40.024194
Number of used sentences in train = 3226
Total loss for epoch 11: 6215.680213
validation loss after epoch 11 : 2211.369434
	Epoch 12....
Epoch has taken 0:03:42.723823
Number of used sentences in train = 3226
Total loss for epoch 12: 6182.542398
validation loss after epoch 12 : 2227.443441
	Epoch 13....
Epoch has taken 0:03:43.218902
Number of used sentences in train = 3226
Total loss for epoch 13: 6173.479506
validation loss after epoch 13 : 2272.372402
	Epoch 14....
Epoch has taken 0:03:54.941388
Number of used sentences in train = 3226
Total loss for epoch 14: 6162.566054
validation loss after epoch 14 : 2295.079800
	TransitionClassifier(
  (p_embeddings): Embedding(13, 46)
  (w_embeddings): Embedding(6855, 102)
  (lstm): LSTM(148, 125, bidirectional=True)
  (linear1): Linear(in_features=2000, out_features=41, bias=True)
  (linear2): Linear(in_features=41, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:57.831849
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2548.244465
	Epoch 1....
Epoch has taken 0:00:24.101732
Number of used sentences in train = 359
Total loss for epoch 1: 1226.545674
	Epoch 2....
Epoch has taken 0:00:24.080423
Number of used sentences in train = 359
Total loss for epoch 2: 996.613999
	Epoch 3....
Epoch has taken 0:00:24.037986
Number of used sentences in train = 359
Total loss for epoch 3: 829.408397
	Epoch 4....
Epoch has taken 0:00:24.118254
Number of used sentences in train = 359
Total loss for epoch 4: 739.982779
	Epoch 5....
Epoch has taken 0:00:24.069439
Number of used sentences in train = 359
Total loss for epoch 5: 702.956539
	Epoch 6....
Epoch has taken 0:00:21.840952
Number of used sentences in train = 359
Total loss for epoch 6: 689.637236
	Epoch 7....
Epoch has taken 0:00:21.760821
Number of used sentences in train = 359
Total loss for epoch 7: 685.226845
	Epoch 8....
Epoch has taken 0:00:21.737586
Number of used sentences in train = 359
Total loss for epoch 8: 683.953897
	Epoch 9....
Epoch has taken 0:00:21.743046
Number of used sentences in train = 359
Total loss for epoch 9: 682.906699
	Epoch 10....
Epoch has taken 0:00:21.767919
Number of used sentences in train = 359
Total loss for epoch 10: 682.336945
	Epoch 11....
Epoch has taken 0:00:21.726122
Number of used sentences in train = 359
Total loss for epoch 11: 681.910357
	Epoch 12....
Epoch has taken 0:00:21.752410
Number of used sentences in train = 359
Total loss for epoch 12: 681.569217
	Epoch 13....
Epoch has taken 0:00:21.739747
Number of used sentences in train = 359
Total loss for epoch 13: 681.281732
	Epoch 14....
Epoch has taken 0:00:21.755227
Number of used sentences in train = 359
Total loss for epoch 14: 681.028718
Epoch has taken 0:00:21.753768

==================================================================================================
	Training time : 1:01:10.187843
==================================================================================================
	Identification : 0.484

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 20, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 20, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 33, 'lstmDropout': 0.26, 'denseActivation': 'tanh', 'wordDim': 185, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(9269, 185)
  (lstm): LSTM(205, 33, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13419.268710
validation loss after epoch 0 : 1200.668587
	Epoch 1....
Epoch has taken 0:02:46.406383
Number of used sentences in train = 2811
Total loss for epoch 1: 8422.025004
validation loss after epoch 1 : 1216.151310
	Epoch 2....
Epoch has taken 0:02:46.410966
Number of used sentences in train = 2811
Total loss for epoch 2: 6509.607317
validation loss after epoch 2 : 1358.261133
	Epoch 3....
Epoch has taken 0:02:46.000763
Number of used sentences in train = 2811
Total loss for epoch 3: 5658.846177
validation loss after epoch 3 : 1412.689157
	Epoch 4....
Epoch has taken 0:02:46.387068
Number of used sentences in train = 2811
Total loss for epoch 4: 5201.505469
validation loss after epoch 4 : 1511.727442
	Epoch 5....
Epoch has taken 0:02:46.480576
Number of used sentences in train = 2811
Total loss for epoch 5: 4932.644714
validation loss after epoch 5 : 1571.042165
	Epoch 6....
Epoch has taken 0:02:56.961890
Number of used sentences in train = 2811
Total loss for epoch 6: 4796.353173
validation loss after epoch 6 : 1622.861520
	Epoch 7....
Epoch has taken 0:03:01.085027
Number of used sentences in train = 2811
Total loss for epoch 7: 4707.183154
validation loss after epoch 7 : 1664.805703
	Epoch 8....
Epoch has taken 0:03:02.675628
Number of used sentences in train = 2811
Total loss for epoch 8: 4655.222974
validation loss after epoch 8 : 1704.563902
	Epoch 9....
Epoch has taken 0:02:45.031540
Number of used sentences in train = 2811
Total loss for epoch 9: 4616.082561
validation loss after epoch 9 : 1737.653764
	Epoch 10....
Epoch has taken 0:02:46.531157
Number of used sentences in train = 2811
Total loss for epoch 10: 4583.774030
validation loss after epoch 10 : 1748.351942
	Epoch 11....
Epoch has taken 0:02:46.435582
Number of used sentences in train = 2811
Total loss for epoch 11: 4569.118872
validation loss after epoch 11 : 1792.378007
	Epoch 12....
Epoch has taken 0:02:45.419126
Number of used sentences in train = 2811
Total loss for epoch 12: 4562.794032
validation loss after epoch 12 : 1815.700338
	Epoch 13....
Epoch has taken 0:02:54.308502
Number of used sentences in train = 2811
Total loss for epoch 13: 4551.408576
validation loss after epoch 13 : 1826.286068
	Epoch 14....
Epoch has taken 0:02:57.941852
Number of used sentences in train = 2811
Total loss for epoch 14: 4548.329877
validation loss after epoch 14 : 1851.833054
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(9269, 185)
  (lstm): LSTM(205, 33, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:00.733633
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1528.933720
	Epoch 1....
Epoch has taken 0:00:19.565000
Number of used sentences in train = 313
Total loss for epoch 1: 804.666570
	Epoch 2....
Epoch has taken 0:00:18.445996
Number of used sentences in train = 313
Total loss for epoch 2: 633.432280
	Epoch 3....
Epoch has taken 0:00:17.710664
Number of used sentences in train = 313
Total loss for epoch 3: 580.629523
	Epoch 4....
Epoch has taken 0:00:17.720103
Number of used sentences in train = 313
Total loss for epoch 4: 541.750857
	Epoch 5....
Epoch has taken 0:00:17.716534
Number of used sentences in train = 313
Total loss for epoch 5: 528.768719
	Epoch 6....
Epoch has taken 0:00:17.716184
Number of used sentences in train = 313
Total loss for epoch 6: 518.930357
	Epoch 7....
Epoch has taken 0:00:17.698088
Number of used sentences in train = 313
Total loss for epoch 7: 507.743024
	Epoch 8....
Epoch has taken 0:00:17.719334
Number of used sentences in train = 313
Total loss for epoch 8: 506.324874
	Epoch 9....
Epoch has taken 0:00:17.689045
Number of used sentences in train = 313
Total loss for epoch 9: 505.053457
	Epoch 10....
Epoch has taken 0:00:17.707426
Number of used sentences in train = 313
Total loss for epoch 10: 503.907279
	Epoch 11....
Epoch has taken 0:00:17.692829
Number of used sentences in train = 313
Total loss for epoch 11: 503.139925
	Epoch 12....
Epoch has taken 0:00:17.700851
Number of used sentences in train = 313
Total loss for epoch 12: 502.392065
	Epoch 13....
Epoch has taken 0:00:17.708894
Number of used sentences in train = 313
Total loss for epoch 13: 502.037188
	Epoch 14....
Epoch has taken 0:00:17.709246
Number of used sentences in train = 313
Total loss for epoch 14: 501.849860
Epoch has taken 0:00:17.692858

==================================================================================================
	Training time : 0:47:17.525604
==================================================================================================
	Identification : 0.343

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(7056, 185)
  (lstm): LSTM(205, 33, bidirectional=True)
  (linear1): Linear(in_features=528, out_features=20, bias=True)
  (linear2): Linear(in_features=20, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10641.596483
validation loss after epoch 0 : 964.928440
	Epoch 1....
Epoch has taken 0:01:53.907718
Number of used sentences in train = 2074
Total loss for epoch 1: 6009.994383
validation loss after epoch 1 : 998.768677
	Epoch 2....
Epoch has taken 0:01:55.505564
Number of used sentences in train = 2074
Total loss for epoch 2: 4489.453990
validation loss after epoch 2 : 1077.190311
	Epoch 3....
Epoch has taken 0:01:57.332699
Number of used sentences in train = 2074
Total loss for epoch 3: 3859.296934
validation loss after epoch 3 : 1201.197334
	Epoch 4....
Epoch has taken 0:01:56.605859
