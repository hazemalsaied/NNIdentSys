INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11516.642882
validation loss after epoch 0 : 870.156004
	Epoch 1....
Epoch has taken 0:03:06.663434
Number of used sentences in train = 2811
Total loss for epoch 1: 7625.484057
validation loss after epoch 1 : 836.592843
	Epoch 2....
Epoch has taken 0:03:07.823515
Number of used sentences in train = 2811
Total loss for epoch 2: 6861.480819
validation loss after epoch 2 : 799.198269
	Epoch 3....
Epoch has taken 0:02:56.890172
Number of used sentences in train = 2811
Total loss for epoch 3: 6386.207736
validation loss after epoch 3 : 810.271608
	Epoch 4....
Epoch has taken 0:02:51.648505
Number of used sentences in train = 2811
Total loss for epoch 4: 6082.850177
validation loss after epoch 4 : 805.193180
	Epoch 5....
Epoch has taken 0:02:51.611155
Number of used sentences in train = 2811
Total loss for epoch 5: 5790.993169
validation loss after epoch 5 : 830.015203
	Epoch 6....
Epoch has taken 0:02:51.778944
Number of used sentences in train = 2811
Total loss for epoch 6: 5568.581025
validation loss after epoch 6 : 858.487507
	Epoch 7....
Epoch has taken 0:02:51.869015
Number of used sentences in train = 2811
Total loss for epoch 7: 5381.460029
validation loss after epoch 7 : 885.863126
	Epoch 8....
Epoch has taken 0:02:51.824199
Number of used sentences in train = 2811
Total loss for epoch 8: 5223.274376
validation loss after epoch 8 : 925.069114
	Epoch 9....
Epoch has taken 0:02:51.925462
Number of used sentences in train = 2811
Total loss for epoch 9: 5088.302332
validation loss after epoch 9 : 884.737816
	Epoch 10....
Epoch has taken 0:02:51.882396
Number of used sentences in train = 2811
Total loss for epoch 10: 5029.031427
validation loss after epoch 10 : 918.996926
	Epoch 11....
Epoch has taken 0:02:51.831167
Number of used sentences in train = 2811
Total loss for epoch 11: 4922.858214
validation loss after epoch 11 : 941.225250
	Epoch 12....
Epoch has taken 0:02:52.002990
Number of used sentences in train = 2811
Total loss for epoch 12: 4872.880331
validation loss after epoch 12 : 962.155792
	Epoch 13....
Epoch has taken 0:02:51.916580
Number of used sentences in train = 2811
Total loss for epoch 13: 4820.930986
validation loss after epoch 13 : 977.179301
	Epoch 14....
Epoch has taken 0:02:52.210525
Number of used sentences in train = 2811
Total loss for epoch 14: 4788.646313
validation loss after epoch 14 : 987.988089
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.169327
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1307.187780
	Epoch 1....
Epoch has taken 0:00:18.204431
Number of used sentences in train = 313
Total loss for epoch 1: 785.149563
	Epoch 2....
Epoch has taken 0:00:18.200301
Number of used sentences in train = 313
Total loss for epoch 2: 679.559570
	Epoch 3....
Epoch has taken 0:00:18.196604
Number of used sentences in train = 313
Total loss for epoch 3: 615.347435
	Epoch 4....
Epoch has taken 0:00:18.206745
Number of used sentences in train = 313
Total loss for epoch 4: 591.094233
	Epoch 5....
Epoch has taken 0:00:18.202661
Number of used sentences in train = 313
Total loss for epoch 5: 573.812155
	Epoch 6....
Epoch has taken 0:00:18.203530
Number of used sentences in train = 313
Total loss for epoch 6: 566.726883
	Epoch 7....
Epoch has taken 0:00:18.207311
Number of used sentences in train = 313
Total loss for epoch 7: 557.023820
	Epoch 8....
Epoch has taken 0:00:18.207683
Number of used sentences in train = 313
Total loss for epoch 8: 552.637328
	Epoch 9....
Epoch has taken 0:00:18.214420
Number of used sentences in train = 313
Total loss for epoch 9: 547.721515
	Epoch 10....
Epoch has taken 0:00:18.205971
Number of used sentences in train = 313
Total loss for epoch 10: 546.569711
	Epoch 11....
Epoch has taken 0:00:18.202576
Number of used sentences in train = 313
Total loss for epoch 11: 541.432191
	Epoch 12....
Epoch has taken 0:00:18.216452
Number of used sentences in train = 313
Total loss for epoch 12: 540.064530
	Epoch 13....
Epoch has taken 0:00:18.212158
Number of used sentences in train = 313
Total loss for epoch 13: 543.703142
	Epoch 14....
Epoch has taken 0:00:18.213981
Number of used sentences in train = 313
Total loss for epoch 14: 549.610592
Epoch has taken 0:00:18.215419

==================================================================================================
	Training time : 0:48:12.640981
==================================================================================================
	Identification : 0.417

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9483.566756
validation loss after epoch 0 : 654.833909
	Epoch 1....
Epoch has taken 0:01:59.358173
Number of used sentences in train = 2074
Total loss for epoch 1: 5381.466573
validation loss after epoch 1 : 650.381577
	Epoch 2....
Epoch has taken 0:01:56.564188
Number of used sentences in train = 2074
Total loss for epoch 2: 4725.244909
validation loss after epoch 2 : 630.232702
	Epoch 3....
Epoch has taken 0:01:56.025206
Number of used sentences in train = 2074
Total loss for epoch 3: 4239.193058
validation loss after epoch 3 : 660.569420
	Epoch 4....
Epoch has taken 0:01:56.024220
Number of used sentences in train = 2074
Total loss for epoch 4: 3900.380597
validation loss after epoch 4 : 666.009353
	Epoch 5....
Epoch has taken 0:01:56.061164
Number of used sentences in train = 2074
Total loss for epoch 5: 3684.419699
validation loss after epoch 5 : 707.498701
	Epoch 6....
Epoch has taken 0:01:56.056959
Number of used sentences in train = 2074
Total loss for epoch 6: 3496.160044
validation loss after epoch 6 : 775.059514
	Epoch 7....
Epoch has taken 0:01:56.062007
Number of used sentences in train = 2074
Total loss for epoch 7: 3384.560502
validation loss after epoch 7 : 721.907809
	Epoch 8....
Epoch has taken 0:01:56.072893
Number of used sentences in train = 2074
Total loss for epoch 8: 3326.028897
validation loss after epoch 8 : 770.268828
	Epoch 9....
Epoch has taken 0:01:56.084280
Number of used sentences in train = 2074
Total loss for epoch 9: 3299.757787
validation loss after epoch 9 : 758.710927
	Epoch 10....
Epoch has taken 0:01:56.113004
Number of used sentences in train = 2074
Total loss for epoch 10: 3272.951834
validation loss after epoch 10 : 775.777453
	Epoch 11....
Epoch has taken 0:01:56.018097
Number of used sentences in train = 2074
Total loss for epoch 11: 3258.211090
validation loss after epoch 11 : 784.723701
	Epoch 12....
Epoch has taken 0:01:56.028976
Number of used sentences in train = 2074
Total loss for epoch 12: 3242.955455
validation loss after epoch 12 : 780.938734
	Epoch 13....
Epoch has taken 0:01:56.071647
Number of used sentences in train = 2074
Total loss for epoch 13: 3224.898465
validation loss after epoch 13 : 795.245364
	Epoch 14....
Epoch has taken 0:01:56.047764
Number of used sentences in train = 2074
Total loss for epoch 14: 3208.217678
validation loss after epoch 14 : 808.028841
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.100023
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1479.819466
	Epoch 1....
Epoch has taken 0:00:11.817002
Number of used sentences in train = 231
Total loss for epoch 1: 604.484587
	Epoch 2....
Epoch has taken 0:00:11.811422
Number of used sentences in train = 231
Total loss for epoch 2: 502.779872
	Epoch 3....
Epoch has taken 0:00:11.806897
Number of used sentences in train = 231
Total loss for epoch 3: 393.784270
	Epoch 4....
Epoch has taken 0:00:11.807258
Number of used sentences in train = 231
Total loss for epoch 4: 363.923214
	Epoch 5....
Epoch has taken 0:00:11.807173
Number of used sentences in train = 231
Total loss for epoch 5: 354.460499
	Epoch 6....
Epoch has taken 0:00:11.805878
Number of used sentences in train = 231
Total loss for epoch 6: 350.070197
	Epoch 7....
Epoch has taken 0:00:11.809065
Number of used sentences in train = 231
Total loss for epoch 7: 348.814038
	Epoch 8....
Epoch has taken 0:00:11.803846
Number of used sentences in train = 231
Total loss for epoch 8: 348.176302
	Epoch 9....
Epoch has taken 0:00:11.798674
Number of used sentences in train = 231
Total loss for epoch 9: 347.856295
	Epoch 10....
Epoch has taken 0:00:11.832091
Number of used sentences in train = 231
Total loss for epoch 10: 347.392049
	Epoch 11....
Epoch has taken 0:00:11.806572
Number of used sentences in train = 231
Total loss for epoch 11: 347.068073
	Epoch 12....
Epoch has taken 0:00:11.811489
Number of used sentences in train = 231
Total loss for epoch 12: 346.754337
	Epoch 13....
Epoch has taken 0:00:11.810521
Number of used sentences in train = 231
Total loss for epoch 13: 346.632392
	Epoch 14....
Epoch has taken 0:00:11.806939
Number of used sentences in train = 231
Total loss for epoch 14: 346.493361
Epoch has taken 0:00:11.803203

==================================================================================================
	Training time : 0:32:02.157388
==================================================================================================
	Identification : 0.383

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20941.887290
validation loss after epoch 0 : 1125.183122
	Epoch 1....
Epoch has taken 0:03:45.288104
Number of used sentences in train = 3226
Total loss for epoch 1: 9908.741483
validation loss after epoch 1 : 1077.553914
	Epoch 2....
Epoch has taken 0:03:45.328375
Number of used sentences in train = 3226
Total loss for epoch 2: 9071.885227
validation loss after epoch 2 : 1035.231926
	Epoch 3....
Epoch has taken 0:03:45.515482
Number of used sentences in train = 3226
Total loss for epoch 3: 8513.579493
validation loss after epoch 3 : 1041.584078
	Epoch 4....
Epoch has taken 0:03:45.353362
Number of used sentences in train = 3226
Total loss for epoch 4: 8093.194879
validation loss after epoch 4 : 1072.487437
	Epoch 5....
Epoch has taken 0:03:45.268154
Number of used sentences in train = 3226
Total loss for epoch 5: 7774.557379
validation loss after epoch 5 : 1102.083838
	Epoch 6....
Epoch has taken 0:03:45.307357
Number of used sentences in train = 3226
Total loss for epoch 6: 7492.110587
validation loss after epoch 6 : 1175.641186
	Epoch 7....
Epoch has taken 0:03:45.259324
Number of used sentences in train = 3226
Total loss for epoch 7: 7281.778668
validation loss after epoch 7 : 1309.567299
	Epoch 8....
Epoch has taken 0:03:45.318714
Number of used sentences in train = 3226
Total loss for epoch 8: 7122.528643
validation loss after epoch 8 : 1168.795651
	Epoch 9....
Epoch has taken 0:03:45.416903
Number of used sentences in train = 3226
Total loss for epoch 9: 6949.571895
validation loss after epoch 9 : 1278.815143
	Epoch 10....
Epoch has taken 0:03:45.533075
Number of used sentences in train = 3226
Total loss for epoch 10: 6829.921578
validation loss after epoch 10 : 1312.810788
	Epoch 11....
Epoch has taken 0:03:45.413476
Number of used sentences in train = 3226
Total loss for epoch 11: 6725.445338
validation loss after epoch 11 : 1312.692427
	Epoch 12....
Epoch has taken 0:03:45.497389
Number of used sentences in train = 3226
Total loss for epoch 12: 6641.432133
validation loss after epoch 12 : 1387.339068
	Epoch 13....
Epoch has taken 0:03:45.551295
Number of used sentences in train = 3226
Total loss for epoch 13: 6547.082190
validation loss after epoch 13 : 1371.881423
	Epoch 14....
Epoch has taken 0:03:45.594701
Number of used sentences in train = 3226
Total loss for epoch 14: 6515.589683
validation loss after epoch 14 : 1471.132691
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:45.581602
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1680.748902
	Epoch 1....
Epoch has taken 0:00:22.025360
Number of used sentences in train = 359
Total loss for epoch 1: 984.566505
	Epoch 2....
Epoch has taken 0:00:22.011081
Number of used sentences in train = 359
Total loss for epoch 2: 885.077110
	Epoch 3....
Epoch has taken 0:00:22.011935
Number of used sentences in train = 359
Total loss for epoch 3: 808.232161
	Epoch 4....
Epoch has taken 0:00:22.010746
Number of used sentences in train = 359
Total loss for epoch 4: 763.151025
	Epoch 5....
Epoch has taken 0:00:21.986654
Number of used sentences in train = 359
Total loss for epoch 5: 734.891357
	Epoch 6....
Epoch has taken 0:00:21.994066
Number of used sentences in train = 359
Total loss for epoch 6: 700.224436
	Epoch 7....
Epoch has taken 0:00:21.984526
Number of used sentences in train = 359
Total loss for epoch 7: 692.240186
	Epoch 8....
Epoch has taken 0:00:21.973995
Number of used sentences in train = 359
Total loss for epoch 8: 692.858652
	Epoch 9....
Epoch has taken 0:00:21.967701
Number of used sentences in train = 359
Total loss for epoch 9: 679.053235
	Epoch 10....
Epoch has taken 0:00:21.973802
Number of used sentences in train = 359
Total loss for epoch 10: 673.427022
	Epoch 11....
Epoch has taken 0:00:21.967130
Number of used sentences in train = 359
Total loss for epoch 11: 677.106072
	Epoch 12....
Epoch has taken 0:00:21.960478
Number of used sentences in train = 359
Total loss for epoch 12: 675.441532
	Epoch 13....
Epoch has taken 0:00:21.969024
Number of used sentences in train = 359
Total loss for epoch 13: 672.560776
	Epoch 14....
Epoch has taken 0:00:21.967964
Number of used sentences in train = 359
Total loss for epoch 14: 672.467520
Epoch has taken 0:00:21.971038

==================================================================================================
	Training time : 1:01:51.657749
==================================================================================================
	Identification : 0.234

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 14, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 38, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 30, 'lstmDropout': 0.34, 'denseActivation': 'tanh', 'wordDim': 174, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(9265, 174)
  (lstm): LSTM(212, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13592.248228
validation loss after epoch 0 : 1199.595142
	Epoch 1....
Epoch has taken 0:02:37.934854
Number of used sentences in train = 2811
Total loss for epoch 1: 9041.132939
validation loss after epoch 1 : 1205.142819
	Epoch 2....
Epoch has taken 0:02:38.007757
Number of used sentences in train = 2811
Total loss for epoch 2: 7100.635433
validation loss after epoch 2 : 1225.117867
	Epoch 3....
Epoch has taken 0:02:38.165328
Number of used sentences in train = 2811
Total loss for epoch 3: 6055.213218
validation loss after epoch 3 : 1330.308199
	Epoch 4....
Epoch has taken 0:02:38.158197
Number of used sentences in train = 2811
Total loss for epoch 4: 5490.131982
validation loss after epoch 4 : 1387.801859
	Epoch 5....
Epoch has taken 0:02:38.097666
Number of used sentences in train = 2811
Total loss for epoch 5: 5162.833678
validation loss after epoch 5 : 1460.816106
	Epoch 6....
Epoch has taken 0:02:38.095515
Number of used sentences in train = 2811
Total loss for epoch 6: 4959.435227
validation loss after epoch 6 : 1510.613525
	Epoch 7....
Epoch has taken 0:02:37.999578
Number of used sentences in train = 2811
Total loss for epoch 7: 4832.586845
validation loss after epoch 7 : 1554.367752
	Epoch 8....
Epoch has taken 0:02:37.906065
Number of used sentences in train = 2811
Total loss for epoch 8: 4755.273908
validation loss after epoch 8 : 1612.690221
	Epoch 9....
Epoch has taken 0:02:38.112667
Number of used sentences in train = 2811
Total loss for epoch 9: 4697.544821
validation loss after epoch 9 : 1638.300082
	Epoch 10....
Epoch has taken 0:02:38.143502
Number of used sentences in train = 2811
Total loss for epoch 10: 4657.976592
validation loss after epoch 10 : 1664.639446
	Epoch 11....
Epoch has taken 0:02:38.232098
Number of used sentences in train = 2811
Total loss for epoch 11: 4626.672227
validation loss after epoch 11 : 1685.416665
	Epoch 12....
Epoch has taken 0:02:38.089847
Number of used sentences in train = 2811
Total loss for epoch 12: 4603.999469
validation loss after epoch 12 : 1699.332489
	Epoch 13....
Epoch has taken 0:02:38.155540
Number of used sentences in train = 2811
Total loss for epoch 13: 4596.244666
validation loss after epoch 13 : 1714.632865
	Epoch 14....
Epoch has taken 0:02:38.097900
Number of used sentences in train = 2811
Total loss for epoch 14: 4587.262129
validation loss after epoch 14 : 1734.393300
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(9265, 174)
  (lstm): LSTM(212, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:38.141301
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1561.182421
	Epoch 1....
Epoch has taken 0:00:16.685401
Number of used sentences in train = 313
Total loss for epoch 1: 886.191635
	Epoch 2....
Epoch has taken 0:00:16.664863
Number of used sentences in train = 313
Total loss for epoch 2: 668.706912
	Epoch 3....
Epoch has taken 0:00:16.677751
Number of used sentences in train = 313
Total loss for epoch 3: 597.714494
	Epoch 4....
Epoch has taken 0:00:16.687864
Number of used sentences in train = 313
Total loss for epoch 4: 557.596469
	Epoch 5....
Epoch has taken 0:00:16.689879
Number of used sentences in train = 313
Total loss for epoch 5: 540.990990
	Epoch 6....
Epoch has taken 0:00:16.691424
Number of used sentences in train = 313
Total loss for epoch 6: 529.365777
	Epoch 7....
Epoch has taken 0:00:16.690427
Number of used sentences in train = 313
Total loss for epoch 7: 521.557225
	Epoch 8....
Epoch has taken 0:00:16.679417
Number of used sentences in train = 313
Total loss for epoch 8: 519.051430
	Epoch 9....
Epoch has taken 0:00:16.689881
Number of used sentences in train = 313
Total loss for epoch 9: 518.183897
	Epoch 10....
Epoch has taken 0:00:16.680900
Number of used sentences in train = 313
Total loss for epoch 10: 517.096828
	Epoch 11....
Epoch has taken 0:00:16.671232
Number of used sentences in train = 313
Total loss for epoch 11: 515.072830
	Epoch 12....
Epoch has taken 0:00:16.659108
Number of used sentences in train = 313
Total loss for epoch 12: 511.038481
	Epoch 13....
Epoch has taken 0:00:16.657609
Number of used sentences in train = 313
Total loss for epoch 13: 509.627178
	Epoch 14....
Epoch has taken 0:00:16.667173
Number of used sentences in train = 313
Total loss for epoch 14: 508.837228
Epoch has taken 0:00:16.665929

==================================================================================================
	Training time : 0:43:42.006074
==================================================================================================
	Identification : 0.489

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(7069, 174)
  (lstm): LSTM(212, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10681.404576
validation loss after epoch 0 : 1024.954627
	Epoch 1....
Epoch has taken 0:01:48.019659
Number of used sentences in train = 2074
Total loss for epoch 1: 6565.491707
validation loss after epoch 1 : 1026.560784
	Epoch 2....
Epoch has taken 0:01:48.048133
Number of used sentences in train = 2074
Total loss for epoch 2: 5014.626854
validation loss after epoch 2 : 1114.056322
	Epoch 3....
Epoch has taken 0:01:48.243998
Number of used sentences in train = 2074
Total loss for epoch 3: 4234.621579
validation loss after epoch 3 : 1165.307814
	Epoch 4....
Epoch has taken 0:01:48.198765
Number of used sentences in train = 2074
Total loss for epoch 4: 3798.867274
validation loss after epoch 4 : 1221.838911
	Epoch 5....
Epoch has taken 0:01:48.202527
Number of used sentences in train = 2074
Total loss for epoch 5: 3579.579236
validation loss after epoch 5 : 1294.745561
	Epoch 6....
Epoch has taken 0:01:48.241861
Number of used sentences in train = 2074
Total loss for epoch 6: 3440.043690
validation loss after epoch 6 : 1378.281884
	Epoch 7....
Epoch has taken 0:01:48.132266
Number of used sentences in train = 2074
Total loss for epoch 7: 3350.588282
validation loss after epoch 7 : 1447.550098
	Epoch 8....
Epoch has taken 0:01:48.186065
Number of used sentences in train = 2074
Total loss for epoch 8: 3303.314396
validation loss after epoch 8 : 1446.655541
	Epoch 9....
Epoch has taken 0:01:48.218843
Number of used sentences in train = 2074
Total loss for epoch 9: 3277.149074
validation loss after epoch 9 : 1495.382373
	Epoch 10....
Epoch has taken 0:01:48.246626
Number of used sentences in train = 2074
Total loss for epoch 10: 3251.365839
validation loss after epoch 10 : 1510.995573
	Epoch 11....
Epoch has taken 0:01:48.179747
Number of used sentences in train = 2074
Total loss for epoch 11: 3235.546369
validation loss after epoch 11 : 1526.640412
	Epoch 12....
Epoch has taken 0:01:48.137187
Number of used sentences in train = 2074
Total loss for epoch 12: 3213.213505
validation loss after epoch 12 : 1536.466282
	Epoch 13....
Epoch has taken 0:01:48.128897
Number of used sentences in train = 2074
Total loss for epoch 13: 3195.784250
validation loss after epoch 13 : 1562.271868
	Epoch 14....
Epoch has taken 0:01:48.134675
Number of used sentences in train = 2074
Total loss for epoch 14: 3183.571761
validation loss after epoch 14 : 1597.523426
	TransitionClassifier(
  (p_embeddings): Embedding(18, 38)
  (w_embeddings): Embedding(7069, 174)
  (lstm): LSTM(212, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.146509
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1556.630703
	Epoch 1....
Epoch has taken 0:00:10.996332
Number of used sentences in train = 231
Total loss for epoch 1: 630.429103
	Epoch 2....
Epoch has taken 0:00:10.995447
Number of used sentences in train = 231
Total loss for epoch 2: 443.366062
	Epoch 3....
Epoch has taken 0:00:10.991391
Number of used sentences in train = 231
Total loss for epoch 3: 389.385867
	Epoch 4....
Epoch has taken 0:00:10.988152
Number of used sentences in train = 231
Total loss for epoch 4: 375.344812
	Epoch 5....
Epoch has taken 0:00:10.985600
Number of used sentences in train = 231
Total loss for epoch 5: 367.533683
	Epoch 6....
Epoch has taken 0:00:10.981137
Number of used sentences in train = 231
Total loss for epoch 6: 361.594014
	Epoch 7....
Epoch has taken 0:00:10.984212
Number of used sentences in train = 231
Total loss for epoch 7: 358.173951
	Epoch 8....
Epoch has taken 0:00:10.985122
Number of used sentences in train = 231
Total loss for epoch 8: 352.652347
	Epoch 9....
Epoch has taken 0:00:10.987555
Number of used sentences in train = 231
Total loss for epoch 9: 350.349748
	Epoch 10....
Epoch has taken 0:00:10.985378
Number of used sentences in train = 231
Total loss for epoch 10: 349.536246
	Epoch 11....
Epoch has taken 0:00:10.984185
Number of used sentences in train = 231
Total loss for epoch 11: 348.781721
	Epoch 12....
Epoch has taken 0:00:10.992059
Number of used sentences in train = 231
Total loss for epoch 12: 348.558972
	Epoch 13....
Epoch has taken 0:00:10.984410
Number of used sentences in train = 231
Total loss for epoch 13: 348.278589
	Epoch 14....
Epoch has taken 0:00:10.986154
Number of used sentences in train = 231
Total loss for epoch 14: 347.976666
Epoch has taken 0:00:10.989393

==================================================================================================
	Training time : 0:29:47.625591
==================================================================================================
	Identification : 0.173

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(17996, 174)
  (lstm): LSTM(212, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19028.289696
validation loss after epoch 0 : 1676.913482
	Epoch 1....
Epoch has taken 0:03:31.476818
Number of used sentences in train = 3226
Total loss for epoch 1: 12208.723729
validation loss after epoch 1 : 1594.289639
	Epoch 2....
Epoch has taken 0:03:31.515728
Number of used sentences in train = 3226
Total loss for epoch 2: 9611.251247
validation loss after epoch 2 : 1717.661179
	Epoch 3....
Epoch has taken 0:03:31.683632
Number of used sentences in train = 3226
Total loss for epoch 3: 8214.751154
validation loss after epoch 3 : 1876.693455
	Epoch 4....
Epoch has taken 0:03:31.881923
Number of used sentences in train = 3226
Total loss for epoch 4: 7448.849741
validation loss after epoch 4 : 1985.020164
	Epoch 5....
Epoch has taken 0:03:31.910295
Number of used sentences in train = 3226
Total loss for epoch 5: 6950.243933
validation loss after epoch 5 : 2111.783250
	Epoch 6....
Epoch has taken 0:03:31.872773
Number of used sentences in train = 3226
Total loss for epoch 6: 6686.742219
validation loss after epoch 6 : 2239.064083
	Epoch 7....
Epoch has taken 0:03:31.835307
Number of used sentences in train = 3226
Total loss for epoch 7: 6514.871076
validation loss after epoch 7 : 2297.243958
	Epoch 8....
Epoch has taken 0:03:31.525389
Number of used sentences in train = 3226
Total loss for epoch 8: 6395.008389
validation loss after epoch 8 : 2410.014825
	Epoch 9....
Epoch has taken 0:03:31.836054
Number of used sentences in train = 3226
Total loss for epoch 9: 6339.746007
validation loss after epoch 9 : 2438.226177
	Epoch 10....
Epoch has taken 0:03:31.562153
Number of used sentences in train = 3226
Total loss for epoch 10: 6310.975661
validation loss after epoch 10 : 2492.397170
	Epoch 11....
Epoch has taken 0:03:31.642785
Number of used sentences in train = 3226
Total loss for epoch 11: 6273.231395
validation loss after epoch 11 : 2535.853188
	Epoch 12....
Epoch has taken 0:03:31.679211
Number of used sentences in train = 3226
Total loss for epoch 12: 6244.087961
validation loss after epoch 12 : 2598.789076
	Epoch 13....
Epoch has taken 0:03:31.851850
Number of used sentences in train = 3226
Total loss for epoch 13: 6228.486080
validation loss after epoch 13 : 2609.736115
	Epoch 14....
Epoch has taken 0:03:31.890031
Number of used sentences in train = 3226
Total loss for epoch 14: 6218.312560
validation loss after epoch 14 : 2715.098007
	TransitionClassifier(
  (p_embeddings): Embedding(13, 38)
  (w_embeddings): Embedding(17996, 174)
  (lstm): LSTM(212, 30, bidirectional=True)
  (linear1): Linear(in_features=480, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:31.779517
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2482.444876
	Epoch 1....
Epoch has taken 0:00:20.601712
Number of used sentences in train = 359
Total loss for epoch 1: 1091.143395
	Epoch 2....
Epoch has taken 0:00:20.601700
Number of used sentences in train = 359
Total loss for epoch 2: 828.498899
	Epoch 3....
Epoch has taken 0:00:20.605146
Number of used sentences in train = 359
Total loss for epoch 3: 736.575438
	Epoch 4....
Epoch has taken 0:00:20.611951
Number of used sentences in train = 359
Total loss for epoch 4: 700.787362
	Epoch 5....
Epoch has taken 0:00:20.602909
Number of used sentences in train = 359
Total loss for epoch 5: 684.588738
	Epoch 6....
Epoch has taken 0:00:20.587289
Number of used sentences in train = 359
Total loss for epoch 6: 680.838253
	Epoch 7....
Epoch has taken 0:00:20.610292
Number of used sentences in train = 359
Total loss for epoch 7: 676.546148
	Epoch 8....
Epoch has taken 0:00:20.604858
Number of used sentences in train = 359
Total loss for epoch 8: 674.200927
	Epoch 9....
Epoch has taken 0:00:20.600886
Number of used sentences in train = 359
Total loss for epoch 9: 673.222101
	Epoch 10....
Epoch has taken 0:00:20.605510
Number of used sentences in train = 359
Total loss for epoch 10: 672.696462
	Epoch 11....
Epoch has taken 0:00:20.600824
Number of used sentences in train = 359
Total loss for epoch 11: 672.400571
	Epoch 12....
Epoch has taken 0:00:20.610902
Number of used sentences in train = 359
Total loss for epoch 12: 672.164513
	Epoch 13....
Epoch has taken 0:00:20.616712
Number of used sentences in train = 359
Total loss for epoch 13: 671.975704
	Epoch 14....
Epoch has taken 0:00:20.599294
Number of used sentences in train = 359
Total loss for epoch 14: 671.829765
Epoch has taken 0:00:20.587953

==================================================================================================
	Training time : 0:58:05.691466
==================================================================================================
	Identification : 0.13

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 44, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 54, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 85, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 76, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 54)
  (w_embeddings): Embedding(9295, 76)
  (lstm): LSTM(130, 85, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14226.249861
validation loss after epoch 0 : 1178.581412
	Epoch 1....
Epoch has taken 0:02:38.728159
Number of used sentences in train = 2811
Total loss for epoch 1: 9412.141540
validation loss after epoch 1 : 1102.677833
	Epoch 2....
Epoch has taken 0:02:38.688961
Number of used sentences in train = 2811
Total loss for epoch 2: 7722.168433
validation loss after epoch 2 : 1084.211227
	Epoch 3....
Epoch has taken 0:02:38.962005
Number of used sentences in train = 2811
Total loss for epoch 3: 6526.804950
validation loss after epoch 3 : 1194.055188
	Epoch 4....
Epoch has taken 0:02:39.017391
Number of used sentences in train = 2811
Total loss for epoch 4: 5815.477172
validation loss after epoch 4 : 1245.650072
	Epoch 5....
Epoch has taken 0:02:38.969967
Number of used sentences in train = 2811
Total loss for epoch 5: 5357.156833
validation loss after epoch 5 : 1351.945069
	Epoch 6....
Epoch has taken 0:02:39.005572
Number of used sentences in train = 2811
Total loss for epoch 6: 5058.066197
validation loss after epoch 6 : 1438.732136
	Epoch 7....
Epoch has taken 0:02:38.985510
Number of used sentences in train = 2811
Total loss for epoch 7: 4865.395321
validation loss after epoch 7 : 1486.940218
	Epoch 8....
Epoch has taken 0:02:39.044707
Number of used sentences in train = 2811
Total loss for epoch 8: 4749.619334
validation loss after epoch 8 : 1553.893107
	Epoch 9....
Epoch has taken 0:02:39.015328
Number of used sentences in train = 2811
Total loss for epoch 9: 4691.219908
validation loss after epoch 9 : 1584.143962
	Epoch 10....
Epoch has taken 0:02:38.995294
Number of used sentences in train = 2811
Total loss for epoch 10: 4645.859152
validation loss after epoch 10 : 1651.779023
	Epoch 11....
Epoch has taken 0:02:38.779907
Number of used sentences in train = 2811
Total loss for epoch 11: 4610.120308
validation loss after epoch 11 : 1681.446807
	Epoch 12....
Epoch has taken 0:02:39.002424
Number of used sentences in train = 2811
Total loss for epoch 12: 4582.417096
validation loss after epoch 12 : 1711.353321
	Epoch 13....
Epoch has taken 0:02:39.153246
Number of used sentences in train = 2811
Total loss for epoch 13: 4553.710093
validation loss after epoch 13 : 1748.059436
	Epoch 14....
Epoch has taken 0:02:39.083659
Number of used sentences in train = 2811
Total loss for epoch 14: 4531.342077
validation loss after epoch 14 : 1775.783952
	TransitionClassifier(
  (p_embeddings): Embedding(18, 54)
  (w_embeddings): Embedding(9295, 76)
  (lstm): LSTM(130, 85, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:39.099193
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1572.652487
	Epoch 1....
Epoch has taken 0:00:16.794245
Number of used sentences in train = 313
Total loss for epoch 1: 964.774380
	Epoch 2....
Epoch has taken 0:00:16.787106
Number of used sentences in train = 313
Total loss for epoch 2: 731.794431
	Epoch 3....
Epoch has taken 0:00:16.791639
Number of used sentences in train = 313
Total loss for epoch 3: 619.307759
	Epoch 4....
Epoch has taken 0:00:16.780977
Number of used sentences in train = 313
Total loss for epoch 4: 563.134224
	Epoch 5....
Epoch has taken 0:00:16.789394
Number of used sentences in train = 313
Total loss for epoch 5: 543.587339
	Epoch 6....
Epoch has taken 0:00:16.776165
Number of used sentences in train = 313
Total loss for epoch 6: 534.826271
	Epoch 7....
Epoch has taken 0:00:16.789398
Number of used sentences in train = 313
Total loss for epoch 7: 522.763881
	Epoch 8....
Epoch has taken 0:00:16.779765
Number of used sentences in train = 313
Total loss for epoch 8: 518.442484
	Epoch 9....
Epoch has taken 0:00:16.790714
Number of used sentences in train = 313
Total loss for epoch 9: 511.775932
	Epoch 10....
Epoch has taken 0:00:16.785346
Number of used sentences in train = 313
Total loss for epoch 10: 512.380958
	Epoch 11....
Epoch has taken 0:00:16.784505
Number of used sentences in train = 313
Total loss for epoch 11: 509.880738
	Epoch 12....
Epoch has taken 0:00:16.790099
Number of used sentences in train = 313
Total loss for epoch 12: 508.690351
	Epoch 13....
Epoch has taken 0:00:16.790575
Number of used sentences in train = 313
Total loss for epoch 13: 507.871956
	Epoch 14....
Epoch has taken 0:00:16.792866
Number of used sentences in train = 313
Total loss for epoch 14: 507.553613
Epoch has taken 0:00:16.784649

==================================================================================================
	Training time : 0:43:56.844217
==================================================================================================
	Identification : 0.455

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 54)
  (w_embeddings): Embedding(7116, 76)
  (lstm): LSTM(130, 85, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10554.905963
validation loss after epoch 0 : 956.217489
	Epoch 1....
Epoch has taken 0:01:48.742999
Number of used sentences in train = 2074
Total loss for epoch 1: 6985.826258
validation loss after epoch 1 : 987.167100
	Epoch 2....
Epoch has taken 0:01:48.799093
Number of used sentences in train = 2074
Total loss for epoch 2: 5512.203061
validation loss after epoch 2 : 978.057372
	Epoch 3....
Epoch has taken 0:01:48.801699
Number of used sentences in train = 2074
Total loss for epoch 3: 4507.227253
validation loss after epoch 3 : 1111.826755
	Epoch 4....
Epoch has taken 0:01:48.896962
Number of used sentences in train = 2074
Total loss for epoch 4: 3912.335566
validation loss after epoch 4 : 1189.162135
	Epoch 5....
Epoch has taken 0:01:48.789597
Number of used sentences in train = 2074
Total loss for epoch 5: 3607.643909
validation loss after epoch 5 : 1243.898383
	Epoch 6....
Epoch has taken 0:01:48.716218
Number of used sentences in train = 2074
Total loss for epoch 6: 3459.643732
validation loss after epoch 6 : 1294.664316
	Epoch 7....
Epoch has taken 0:01:48.739221
Number of used sentences in train = 2074
Total loss for epoch 7: 3347.711081
validation loss after epoch 7 : 1351.846744
	Epoch 8....
Epoch has taken 0:01:48.705569
Number of used sentences in train = 2074
Total loss for epoch 8: 3281.952111
validation loss after epoch 8 : 1382.716167
	Epoch 9....
Epoch has taken 0:01:48.795709
Number of used sentences in train = 2074
Total loss for epoch 9: 3243.582249
validation loss after epoch 9 : 1456.305512
	Epoch 10....
Epoch has taken 0:01:48.781485
Number of used sentences in train = 2074
Total loss for epoch 10: 3219.365343
validation loss after epoch 10 : 1472.750176
	Epoch 11....
Epoch has taken 0:01:48.898985
Number of used sentences in train = 2074
Total loss for epoch 11: 3202.658129
validation loss after epoch 11 : 1507.387238
	Epoch 12....
Epoch has taken 0:01:48.875027
Number of used sentences in train = 2074
Total loss for epoch 12: 3190.064197
validation loss after epoch 12 : 1525.157909
	Epoch 13....
Epoch has taken 0:01:48.910364
Number of used sentences in train = 2074
Total loss for epoch 13: 3183.540612
validation loss after epoch 13 : 1550.959255
	Epoch 14....
Epoch has taken 0:01:48.936416
Number of used sentences in train = 2074
Total loss for epoch 14: 3179.348333
validation loss after epoch 14 : 1561.260962
	TransitionClassifier(
  (p_embeddings): Embedding(18, 54)
  (w_embeddings): Embedding(7116, 76)
  (lstm): LSTM(130, 85, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.829832
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1603.998125
	Epoch 1....
Epoch has taken 0:00:11.057075
Number of used sentences in train = 231
Total loss for epoch 1: 710.955157
	Epoch 2....
Epoch has taken 0:00:11.061820
Number of used sentences in train = 231
Total loss for epoch 2: 523.311224
	Epoch 3....
Epoch has taken 0:00:11.064581
Number of used sentences in train = 231
Total loss for epoch 3: 429.971968
	Epoch 4....
Epoch has taken 0:00:11.065762
Number of used sentences in train = 231
Total loss for epoch 4: 373.055621
	Epoch 5....
Epoch has taken 0:00:11.061537
Number of used sentences in train = 231
Total loss for epoch 5: 356.936095
	Epoch 6....
Epoch has taken 0:00:11.055016
Number of used sentences in train = 231
Total loss for epoch 6: 354.642028
	Epoch 7....
Epoch has taken 0:00:11.059549
Number of used sentences in train = 231
Total loss for epoch 7: 351.115648
	Epoch 8....
Epoch has taken 0:00:11.054600
Number of used sentences in train = 231
Total loss for epoch 8: 350.008815
	Epoch 9....
Epoch has taken 0:00:11.062163
Number of used sentences in train = 231
Total loss for epoch 9: 349.090982
	Epoch 10....
Epoch has taken 0:00:11.048514
Number of used sentences in train = 231
Total loss for epoch 10: 348.400247
	Epoch 11....
Epoch has taken 0:00:11.052982
Number of used sentences in train = 231
Total loss for epoch 11: 347.540502
	Epoch 12....
Epoch has taken 0:00:11.054651
Number of used sentences in train = 231
Total loss for epoch 12: 347.200594
	Epoch 13....
Epoch has taken 0:00:11.062592
Number of used sentences in train = 231
Total loss for epoch 13: 346.849358
	Epoch 14....
Epoch has taken 0:00:11.050995
Number of used sentences in train = 231
Total loss for epoch 14: 346.675407
Epoch has taken 0:00:11.059271

==================================================================================================
	Training time : 0:29:58.430379
==================================================================================================
	Identification : 0.187

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 54)
  (w_embeddings): Embedding(18076, 76)
  (lstm): LSTM(130, 85, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19395.277798
validation loss after epoch 0 : 1703.427155
	Epoch 1....
Epoch has taken 0:03:36.314133
Number of used sentences in train = 3226
Total loss for epoch 1: 12727.771121
validation loss after epoch 1 : 1558.190798
	Epoch 2....
Epoch has taken 0:03:37.629221
Number of used sentences in train = 3226
Total loss for epoch 2: 10648.761155
validation loss after epoch 2 : 1625.332480
	Epoch 3....
Epoch has taken 0:03:32.192359
Number of used sentences in train = 3226
Total loss for epoch 3: 9292.700911
validation loss after epoch 3 : 1629.512175
	Epoch 4....
Epoch has taken 0:03:31.976611
Number of used sentences in train = 3226
Total loss for epoch 4: 8268.234578
validation loss after epoch 4 : 1794.826432
	Epoch 5....
Epoch has taken 0:03:37.123158
Number of used sentences in train = 3226
Total loss for epoch 5: 7546.010749
validation loss after epoch 5 : 1967.844846
	Epoch 6....
Epoch has taken 0:03:32.054027
Number of used sentences in train = 3226
Total loss for epoch 6: 7103.013810
validation loss after epoch 6 : 2148.698565
	Epoch 7....
Epoch has taken 0:03:32.839142
Number of used sentences in train = 3226
Total loss for epoch 7: 6757.733120
validation loss after epoch 7 : 2115.781719
	Epoch 8....
Epoch has taken 0:03:32.847857
Number of used sentences in train = 3226
Total loss for epoch 8: 6585.096970
validation loss after epoch 8 : 2134.512512
	Epoch 9....
Epoch has taken 0:03:32.136550
Number of used sentences in train = 3226
Total loss for epoch 9: 6438.774693
validation loss after epoch 9 : 2299.741870
	Epoch 10....
Epoch has taken 0:03:36.583144
Number of used sentences in train = 3226
Total loss for epoch 10: 6346.618459
validation loss after epoch 10 : 2423.631552
	Epoch 11....
Epoch has taken 0:03:35.546509
Number of used sentences in train = 3226
Total loss for epoch 11: 6288.437078
validation loss after epoch 11 : 2424.628752
	Epoch 12....
Epoch has taken 0:03:32.061686
Number of used sentences in train = 3226
Total loss for epoch 12: 6251.732439
validation loss after epoch 12 : 2461.597896
	Epoch 13....
Epoch has taken 0:03:32.047270
Number of used sentences in train = 3226
Total loss for epoch 13: 6214.399976
validation loss after epoch 13 : 2567.181875
	Epoch 14....
Epoch has taken 0:03:34.765261
Number of used sentences in train = 3226
Total loss for epoch 14: 6199.169718
validation loss after epoch 14 : 2598.430623
	TransitionClassifier(
  (p_embeddings): Embedding(13, 54)
  (w_embeddings): Embedding(18076, 76)
  (lstm): LSTM(130, 85, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:32.051575
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2228.787891
	Epoch 1....
Epoch has taken 0:00:20.666427
Number of used sentences in train = 359
Total loss for epoch 1: 1203.894289
	Epoch 2....
Epoch has taken 0:00:20.671044
Number of used sentences in train = 359
Total loss for epoch 2: 918.849735
	Epoch 3....
Epoch has taken 0:00:20.671035
Number of used sentences in train = 359
Total loss for epoch 3: 785.232288
	Epoch 4....
Epoch has taken 0:00:20.678491
Number of used sentences in train = 359
Total loss for epoch 4: 715.091977
	Epoch 5....
Epoch has taken 0:00:20.819021
Number of used sentences in train = 359
Total loss for epoch 5: 690.306699
	Epoch 6....
Epoch has taken 0:00:20.669746
Number of used sentences in train = 359
Total loss for epoch 6: 679.396118
	Epoch 7....
Epoch has taken 0:00:20.664283
Number of used sentences in train = 359
Total loss for epoch 7: 675.932344
	Epoch 8....
Epoch has taken 0:00:20.658576
Number of used sentences in train = 359
Total loss for epoch 8: 673.858636
	Epoch 9....
Epoch has taken 0:00:20.678516
Number of used sentences in train = 359
Total loss for epoch 9: 673.171074
	Epoch 10....
Epoch has taken 0:00:20.936988
Number of used sentences in train = 359
Total loss for epoch 10: 672.604889
	Epoch 11....
Epoch has taken 0:00:20.665458
Number of used sentences in train = 359
Total loss for epoch 11: 672.274663
	Epoch 12....
Epoch has taken 0:00:20.661562
Number of used sentences in train = 359
Total loss for epoch 12: 671.964482
	Epoch 13....
Epoch has taken 0:00:20.648493
Number of used sentences in train = 359
Total loss for epoch 13: 671.641915
	Epoch 14....
Epoch has taken 0:00:20.652140
Number of used sentences in train = 359
Total loss for epoch 14: 671.411068
Epoch has taken 0:00:20.683758

==================================================================================================
	Training time : 0:58:39.271578
==================================================================================================
	Identification : 0.427

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 26, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 44, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 122, 'lstmDropout': 0.36, 'denseActivation': 'tanh', 'wordDim': 159, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5816, 159)
  (lstm): LSTM(203, 122, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 16018.005287
validation loss after epoch 0 : 1260.635282
	Epoch 1....
Epoch has taken 0:02:39.012157
Number of used sentences in train = 2811
Total loss for epoch 1: 10304.329167
validation loss after epoch 1 : 1137.838008
	Epoch 2....
Epoch has taken 0:02:39.019584
Number of used sentences in train = 2811
Total loss for epoch 2: 8434.895555
validation loss after epoch 2 : 1100.953743
	Epoch 3....
Epoch has taken 0:02:39.077144
Number of used sentences in train = 2811
Total loss for epoch 3: 7263.173618
validation loss after epoch 3 : 1151.937542
	Epoch 4....
Epoch has taken 0:02:39.149177
Number of used sentences in train = 2811
Total loss for epoch 4: 6424.447997
validation loss after epoch 4 : 1192.155845
	Epoch 5....
Epoch has taken 0:02:39.031779
Number of used sentences in train = 2811
Total loss for epoch 5: 5908.474265
validation loss after epoch 5 : 1247.797764
	Epoch 6....
Epoch has taken 0:02:39.185026
Number of used sentences in train = 2811
Total loss for epoch 6: 5608.084912
validation loss after epoch 6 : 1258.448782
	Epoch 7....
Epoch has taken 0:02:40.295925
Number of used sentences in train = 2811
Total loss for epoch 7: 5364.566293
validation loss after epoch 7 : 1345.163772
	Epoch 8....
Epoch has taken 0:02:41.946638
Number of used sentences in train = 2811
Total loss for epoch 8: 5201.816813
validation loss after epoch 8 : 1367.055284
	Epoch 9....
Epoch has taken 0:02:41.719527
Number of used sentences in train = 2811
Total loss for epoch 9: 5098.190728
validation loss after epoch 9 : 1392.210696
	Epoch 10....
Epoch has taken 0:02:56.567974
Number of used sentences in train = 2811
Total loss for epoch 10: 5025.934912
validation loss after epoch 10 : 1409.401351
	Epoch 11....
Epoch has taken 0:02:59.005868
Number of used sentences in train = 2811
Total loss for epoch 11: 4954.317470
validation loss after epoch 11 : 1443.904439
	Epoch 12....
Epoch has taken 0:02:57.881080
Number of used sentences in train = 2811
Total loss for epoch 12: 4906.060146
validation loss after epoch 12 : 1459.791598
	Epoch 13....
Epoch has taken 0:02:59.999079
Number of used sentences in train = 2811
Total loss for epoch 13: 4868.879910
validation loss after epoch 13 : 1484.200302
	Epoch 14....
Epoch has taken 0:02:43.958391
Number of used sentences in train = 2811
Total loss for epoch 14: 4826.084412
validation loss after epoch 14 : 1498.926871
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5816, 159)
  (lstm): LSTM(203, 122, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:44.029186
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1514.010446
	Epoch 1....
Epoch has taken 0:00:17.398669
Number of used sentences in train = 313
Total loss for epoch 1: 975.934755
	Epoch 2....
Epoch has taken 0:00:17.398306
Number of used sentences in train = 313
Total loss for epoch 2: 758.164377
	Epoch 3....
Epoch has taken 0:00:17.403439
Number of used sentences in train = 313
Total loss for epoch 3: 658.971392
	Epoch 4....
Epoch has taken 0:00:17.401016
Number of used sentences in train = 313
Total loss for epoch 4: 606.304643
	Epoch 5....
Epoch has taken 0:00:17.402648
Number of used sentences in train = 313
Total loss for epoch 5: 577.673798
	Epoch 6....
Epoch has taken 0:00:17.415693
Number of used sentences in train = 313
Total loss for epoch 6: 565.071461
	Epoch 7....
Epoch has taken 0:00:17.394688
Number of used sentences in train = 313
Total loss for epoch 7: 555.216850
	Epoch 8....
Epoch has taken 0:00:17.399155
Number of used sentences in train = 313
Total loss for epoch 8: 545.166146
	Epoch 9....
Epoch has taken 0:00:17.399167
Number of used sentences in train = 313
Total loss for epoch 9: 541.220407
	Epoch 10....
Epoch has taken 0:00:17.393780
Number of used sentences in train = 313
Total loss for epoch 10: 538.901321
	Epoch 11....
Epoch has taken 0:00:17.395985
Number of used sentences in train = 313
Total loss for epoch 11: 536.272678
	Epoch 12....
Epoch has taken 0:00:17.405247
Number of used sentences in train = 313
Total loss for epoch 12: 534.430954
	Epoch 13....
Epoch has taken 0:00:17.421751
Number of used sentences in train = 313
Total loss for epoch 13: 533.453777
	Epoch 14....
Epoch has taken 0:00:17.427231
Number of used sentences in train = 313
Total loss for epoch 14: 532.211702
Epoch has taken 0:00:17.411770

==================================================================================================
	Training time : 0:45:41.453394
==================================================================================================
	Identification : 0.511

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5589, 159)
  (lstm): LSTM(203, 122, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11372.751093
validation loss after epoch 0 : 1094.811723
	Epoch 1....
Epoch has taken 0:01:52.330097
Number of used sentences in train = 2074
Total loss for epoch 1: 7222.092230
validation loss after epoch 1 : 983.481634
	Epoch 2....
Epoch has taken 0:01:50.829146
Number of used sentences in train = 2074
Total loss for epoch 2: 5885.373775
validation loss after epoch 2 : 1014.098841
	Epoch 3....
Epoch has taken 0:01:52.513152
Number of used sentences in train = 2074
Total loss for epoch 3: 4924.634759
validation loss after epoch 3 : 971.536011
	Epoch 4....
Epoch has taken 0:01:52.410237
Number of used sentences in train = 2074
Total loss for epoch 4: 4328.683324
validation loss after epoch 4 : 1093.934177
	Epoch 5....
Epoch has taken 0:01:52.435654
Number of used sentences in train = 2074
Total loss for epoch 5: 3920.631715
validation loss after epoch 5 : 1089.493039
	Epoch 6....
Epoch has taken 0:01:56.390499
Number of used sentences in train = 2074
Total loss for epoch 6: 3714.721010
validation loss after epoch 6 : 1192.901292
	Epoch 7....
Epoch has taken 0:02:03.611718
Number of used sentences in train = 2074
Total loss for epoch 7: 3598.079354
validation loss after epoch 7 : 1222.443029
	Epoch 8....
Epoch has taken 0:02:02.398830
Number of used sentences in train = 2074
Total loss for epoch 8: 3493.062772
validation loss after epoch 8 : 1228.895285
	Epoch 9....
Epoch has taken 0:02:03.572011
Number of used sentences in train = 2074
Total loss for epoch 9: 3431.195951
validation loss after epoch 9 : 1260.599548
	Epoch 10....
Epoch has taken 0:02:03.538918
Number of used sentences in train = 2074
Total loss for epoch 10: 3397.514447
validation loss after epoch 10 : 1300.240236
	Epoch 11....
Epoch has taken 0:02:03.578189
Number of used sentences in train = 2074
Total loss for epoch 11: 3368.541869
validation loss after epoch 11 : 1338.135742
	Epoch 12....
Epoch has taken 0:02:02.734057
Number of used sentences in train = 2074
Total loss for epoch 12: 3347.444183
validation loss after epoch 12 : 1347.261955
	Epoch 13....
Epoch has taken 0:02:03.683655
Number of used sentences in train = 2074
Total loss for epoch 13: 3330.317441
validation loss after epoch 13 : 1350.598580
	Epoch 14....
Epoch has taken 0:01:52.735292
Number of used sentences in train = 2074
Total loss for epoch 14: 3317.770364
validation loss after epoch 14 : 1371.243430
	TransitionClassifier(
  (p_embeddings): Embedding(18, 44)
  (w_embeddings): Embedding(5589, 159)
  (lstm): LSTM(203, 122, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:52.419261
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1484.586516
	Epoch 1....
Epoch has taken 0:00:11.460014
Number of used sentences in train = 231
Total loss for epoch 1: 799.171591
	Epoch 2....
Epoch has taken 0:00:11.456747
Number of used sentences in train = 231
Total loss for epoch 2: 563.543699
	Epoch 3....
Epoch has taken 0:00:11.465868
Number of used sentences in train = 231
Total loss for epoch 3: 467.585064
	Epoch 4....
Epoch has taken 0:00:11.462675
Number of used sentences in train = 231
Total loss for epoch 4: 429.541506
	Epoch 5....
Epoch has taken 0:00:11.465382
Number of used sentences in train = 231
Total loss for epoch 5: 400.222574
	Epoch 6....
Epoch has taken 0:00:11.459293
Number of used sentences in train = 231
Total loss for epoch 6: 384.256189
	Epoch 7....
Epoch has taken 0:00:11.334609
Number of used sentences in train = 231
Total loss for epoch 7: 376.158872
	Epoch 8....
Epoch has taken 0:00:11.245586
Number of used sentences in train = 231
Total loss for epoch 8: 372.910732
	Epoch 9....
Epoch has taken 0:00:11.250246
Number of used sentences in train = 231
Total loss for epoch 9: 370.858816
	Epoch 10....
Epoch has taken 0:00:11.257792
Number of used sentences in train = 231
Total loss for epoch 10: 369.866886
	Epoch 11....
Epoch has taken 0:00:11.249673
Number of used sentences in train = 231
Total loss for epoch 11: 367.200717
	Epoch 12....
Epoch has taken 0:00:11.249231
Number of used sentences in train = 231
Total loss for epoch 12: 366.521640
	Epoch 13....
Epoch has taken 0:00:11.258346
Number of used sentences in train = 231
Total loss for epoch 13: 365.933942
	Epoch 14....
Epoch has taken 0:00:11.250276
Number of used sentences in train = 231
Total loss for epoch 14: 365.443084
Epoch has taken 0:00:11.249092

==================================================================================================
	Training time : 0:32:15.639654
==================================================================================================
	Identification : 0.016

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 44)
  (w_embeddings): Embedding(6826, 159)
  (lstm): LSTM(203, 122, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20378.159657
validation loss after epoch 0 : 1702.725810
	Epoch 1....
Epoch has taken 0:03:39.012313
Number of used sentences in train = 3226
Total loss for epoch 1: 13489.438153
validation loss after epoch 1 : 1555.770113
	Epoch 2....
Epoch has taken 0:03:37.009597
Number of used sentences in train = 3226
Total loss for epoch 2: 11669.832164
validation loss after epoch 2 : 1561.622667
	Epoch 3....
Epoch has taken 0:03:37.942579
Number of used sentences in train = 3226
Total loss for epoch 3: 10322.839837
validation loss after epoch 3 : 1523.108717
	Epoch 4....
Epoch has taken 0:03:39.261753
Number of used sentences in train = 3226
Total loss for epoch 4: 9144.630361
validation loss after epoch 4 : 1634.492410
	Epoch 5....
Epoch has taken 0:03:38.640733
Number of used sentences in train = 3226
Total loss for epoch 5: 8403.326383
validation loss after epoch 5 : 1607.151035
	Epoch 6....
Epoch has taken 0:03:39.377980
Number of used sentences in train = 3226
Total loss for epoch 6: 7746.724634
validation loss after epoch 6 : 1758.026850
	Epoch 7....
Epoch has taken 0:03:38.103819
Number of used sentences in train = 3226
Total loss for epoch 7: 7247.916775
validation loss after epoch 7 : 1846.867021
	Epoch 8....
Epoch has taken 0:03:39.103724
Number of used sentences in train = 3226
Total loss for epoch 8: 6874.360410
validation loss after epoch 8 : 2013.819202
	Epoch 9....
Epoch has taken 0:03:37.677226
Number of used sentences in train = 3226
Total loss for epoch 9: 6660.598895
validation loss after epoch 9 : 2052.460900
	Epoch 10....
Epoch has taken 0:03:38.620520
Number of used sentences in train = 3226
Total loss for epoch 10: 6502.033667
validation loss after epoch 10 : 2187.263833
	Epoch 11....
Epoch has taken 0:03:37.960463
Number of used sentences in train = 3226
Total loss for epoch 11: 6417.388104
validation loss after epoch 11 : 2244.211819
	Epoch 12....
Epoch has taken 0:03:36.949738
Number of used sentences in train = 3226
Total loss for epoch 12: 6371.904639
validation loss after epoch 12 : 2112.852685
	Epoch 13....
Epoch has taken 0:03:37.833543
Number of used sentences in train = 3226
Total loss for epoch 13: 6341.030158
validation loss after epoch 13 : 2312.265566
	Epoch 14....
Epoch has taken 0:03:38.957160
Number of used sentences in train = 3226
Total loss for epoch 14: 6306.360256
validation loss after epoch 14 : 2284.226345
	TransitionClassifier(
  (p_embeddings): Embedding(13, 44)
  (w_embeddings): Embedding(6826, 159)
  (lstm): LSTM(203, 122, bidirectional=True)
  (linear1): Linear(in_features=1952, out_features=26, bias=True)
  (linear2): Linear(in_features=26, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:38.917785
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2251.120417
	Epoch 1....
Epoch has taken 0:00:21.462521
Number of used sentences in train = 359
Total loss for epoch 1: 1385.463764
	Epoch 2....
Epoch has taken 0:00:23.662883
Number of used sentences in train = 359
Total loss for epoch 2: 1128.460082
	Epoch 3....
Epoch has taken 0:00:21.424296
Number of used sentences in train = 359
Total loss for epoch 3: 920.327638
	Epoch 4....
Epoch has taken 0:00:21.471649
Number of used sentences in train = 359
Total loss for epoch 4: 822.585173
	Epoch 5....
Epoch has taken 0:00:21.472855
Number of used sentences in train = 359
Total loss for epoch 5: 762.382293
	Epoch 6....
Epoch has taken 0:00:21.489023
Number of used sentences in train = 359
Total loss for epoch 6: 726.437004
	Epoch 7....
Epoch has taken 0:00:21.496551
Number of used sentences in train = 359
Total loss for epoch 7: 700.636848
	Epoch 8....
Epoch has taken 0:00:21.475408
Number of used sentences in train = 359
Total loss for epoch 8: 687.777534
	Epoch 9....
Epoch has taken 0:00:21.473667
Number of used sentences in train = 359
Total loss for epoch 9: 679.373773
	Epoch 10....
Epoch has taken 0:00:21.477253
Number of used sentences in train = 359
Total loss for epoch 10: 676.904484
	Epoch 11....
Epoch has taken 0:00:21.484165
Number of used sentences in train = 359
Total loss for epoch 11: 675.727616
	Epoch 12....
Epoch has taken 0:00:21.802412
Number of used sentences in train = 359
Total loss for epoch 12: 674.925577
	Epoch 13....
Epoch has taken 0:00:21.476604
Number of used sentences in train = 359
Total loss for epoch 13: 674.263413
	Epoch 14....
Epoch has taken 0:00:21.487976
Number of used sentences in train = 359
Total loss for epoch 14: 673.686751
Epoch has taken 0:00:21.484191

==================================================================================================
	Training time : 1:00:00.664258
==================================================================================================
	Identification : 0.093

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 44, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 63, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 65, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 64, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1882, 64)
  (lstm): LSTM(127, 65, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10627.959182
validation loss after epoch 0 : 884.125221
	Epoch 1....
Epoch has taken 0:02:55.992357
Number of used sentences in train = 2811
Total loss for epoch 1: 7487.843747
validation loss after epoch 1 : 819.876135
	Epoch 2....
Epoch has taken 0:02:53.820764
Number of used sentences in train = 2811
Total loss for epoch 2: 6693.824006
validation loss after epoch 2 : 808.913177
	Epoch 3....
Epoch has taken 0:02:55.877763
Number of used sentences in train = 2811
Total loss for epoch 3: 6177.589223
validation loss after epoch 3 : 833.422975
	Epoch 4....
Epoch has taken 0:02:55.920352
Number of used sentences in train = 2811
Total loss for epoch 4: 5806.739442
validation loss after epoch 4 : 864.679125
	Epoch 5....
Epoch has taken 0:02:54.434756
Number of used sentences in train = 2811
Total loss for epoch 5: 5489.327258
validation loss after epoch 5 : 846.755538
	Epoch 6....
Epoch has taken 0:02:56.048684
Number of used sentences in train = 2811
Total loss for epoch 6: 5234.664884
validation loss after epoch 6 : 879.732452
	Epoch 7....
Epoch has taken 0:02:55.097673
Number of used sentences in train = 2811
Total loss for epoch 7: 5108.246843
validation loss after epoch 7 : 908.775441
	Epoch 8....
Epoch has taken 0:03:13.283010
Number of used sentences in train = 2811
Total loss for epoch 8: 4983.629803
validation loss after epoch 8 : 927.631078
	Epoch 9....
Epoch has taken 0:03:03.971891
Number of used sentences in train = 2811
Total loss for epoch 9: 4831.243947
validation loss after epoch 9 : 995.981235
	Epoch 10....
Epoch has taken 0:03:00.163701
Number of used sentences in train = 2811
Total loss for epoch 10: 4789.486694
validation loss after epoch 10 : 1010.052770
	Epoch 11....
Epoch has taken 0:02:55.738063
Number of used sentences in train = 2811
Total loss for epoch 11: 4747.774061
validation loss after epoch 11 : 1014.103092
	Epoch 12....
Epoch has taken 0:02:56.005812
Number of used sentences in train = 2811
Total loss for epoch 12: 4685.659461
validation loss after epoch 12 : 1041.277037
	Epoch 13....
Epoch has taken 0:02:55.896104
Number of used sentences in train = 2811
Total loss for epoch 13: 4685.412473
validation loss after epoch 13 : 1037.844432
	Epoch 14....
Epoch has taken 0:02:56.037823
Number of used sentences in train = 2811
Total loss for epoch 14: 4640.860765
validation loss after epoch 14 : 1092.765200
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1882, 64)
  (lstm): LSTM(127, 65, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.272930
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1164.700285
	Epoch 1....
Epoch has taken 0:00:18.679133
Number of used sentences in train = 313
Total loss for epoch 1: 787.486737
	Epoch 2....
Epoch has taken 0:00:18.684032
Number of used sentences in train = 313
Total loss for epoch 2: 654.439657
	Epoch 3....
Epoch has taken 0:00:18.674310
Number of used sentences in train = 313
Total loss for epoch 3: 600.605634
	Epoch 4....
Epoch has taken 0:00:18.684903
Number of used sentences in train = 313
Total loss for epoch 4: 567.151157
	Epoch 5....
Epoch has taken 0:00:18.664262
Number of used sentences in train = 313
Total loss for epoch 5: 551.535678
	Epoch 6....
Epoch has taken 0:00:18.673620
Number of used sentences in train = 313
Total loss for epoch 6: 545.028289
	Epoch 7....
Epoch has taken 0:00:18.683526
Number of used sentences in train = 313
Total loss for epoch 7: 537.998139
	Epoch 8....
Epoch has taken 0:00:18.669940
Number of used sentences in train = 313
Total loss for epoch 8: 534.641070
	Epoch 9....
Epoch has taken 0:00:18.658114
Number of used sentences in train = 313
Total loss for epoch 9: 536.406150
	Epoch 10....
Epoch has taken 0:00:18.681181
Number of used sentences in train = 313
Total loss for epoch 10: 535.050569
	Epoch 11....
Epoch has taken 0:00:18.690903
Number of used sentences in train = 313
Total loss for epoch 11: 530.790105
	Epoch 12....
Epoch has taken 0:00:18.669566
Number of used sentences in train = 313
Total loss for epoch 12: 531.693547
	Epoch 13....
Epoch has taken 0:00:18.668095
Number of used sentences in train = 313
Total loss for epoch 13: 529.529953
	Epoch 14....
Epoch has taken 0:00:18.679405
Number of used sentences in train = 313
Total loss for epoch 14: 524.187273
Epoch has taken 0:00:18.434793

==================================================================================================
	Training time : 0:49:04.958399
==================================================================================================
	Identification : 0.491

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1680, 64)
  (lstm): LSTM(127, 65, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10462.129840
validation loss after epoch 0 : 738.050626
	Epoch 1....
Epoch has taken 0:02:00.565731
Number of used sentences in train = 2074
Total loss for epoch 1: 5851.569559
validation loss after epoch 1 : 629.953367
	Epoch 2....
Epoch has taken 0:02:01.519239
Number of used sentences in train = 2074
Total loss for epoch 2: 5064.996657
validation loss after epoch 2 : 673.294129
	Epoch 3....
Epoch has taken 0:02:02.064427
Number of used sentences in train = 2074
Total loss for epoch 3: 4642.326947
validation loss after epoch 3 : 685.460379
	Epoch 4....
Epoch has taken 0:02:01.922019
Number of used sentences in train = 2074
Total loss for epoch 4: 4269.793319
validation loss after epoch 4 : 685.685650
	Epoch 5....
Epoch has taken 0:02:02.051863
Number of used sentences in train = 2074
Total loss for epoch 5: 4036.782782
validation loss after epoch 5 : 754.778497
	Epoch 6....
Epoch has taken 0:02:13.155528
Number of used sentences in train = 2074
Total loss for epoch 6: 3822.413029
validation loss after epoch 6 : 735.024453
	Epoch 7....
Epoch has taken 0:02:02.089906
Number of used sentences in train = 2074
Total loss for epoch 7: 3650.471177
validation loss after epoch 7 : 830.812694
	Epoch 8....
Epoch has taken 0:02:02.075699
Number of used sentences in train = 2074
Total loss for epoch 8: 3498.624979
validation loss after epoch 8 : 835.502712
	Epoch 9....
Epoch has taken 0:02:02.130033
Number of used sentences in train = 2074
Total loss for epoch 9: 3418.978036
validation loss after epoch 9 : 883.611723
	Epoch 10....
Epoch has taken 0:02:05.086313
Number of used sentences in train = 2074
Total loss for epoch 10: 3388.777937
validation loss after epoch 10 : 876.175445
	Epoch 11....
Epoch has taken 0:02:13.730824
Number of used sentences in train = 2074
Total loss for epoch 11: 3334.251353
validation loss after epoch 11 : 879.802348
	Epoch 12....
Epoch has taken 0:02:13.749382
Number of used sentences in train = 2074
Total loss for epoch 12: 3305.839606
validation loss after epoch 12 : 878.990901
	Epoch 13....
Epoch has taken 0:02:11.759013
Number of used sentences in train = 2074
Total loss for epoch 13: 3317.624461
validation loss after epoch 13 : 909.617180
	Epoch 14....
Epoch has taken 0:02:13.674272
Number of used sentences in train = 2074
Total loss for epoch 14: 3299.487626
validation loss after epoch 14 : 923.499296
	TransitionClassifier(
  (p_embeddings): Embedding(18, 63)
  (w_embeddings): Embedding(1680, 64)
  (lstm): LSTM(127, 65, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:04.029556
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1376.176742
	Epoch 1....
Epoch has taken 0:00:12.431028
Number of used sentences in train = 231
Total loss for epoch 1: 635.367952
	Epoch 2....
Epoch has taken 0:00:12.423444
Number of used sentences in train = 231
Total loss for epoch 2: 466.479897
	Epoch 3....
Epoch has taken 0:00:12.433277
Number of used sentences in train = 231
Total loss for epoch 3: 394.263237
	Epoch 4....
Epoch has taken 0:00:12.423145
Number of used sentences in train = 231
Total loss for epoch 4: 361.084393
	Epoch 5....
Epoch has taken 0:00:12.446291
Number of used sentences in train = 231
Total loss for epoch 5: 357.451134
	Epoch 6....
Epoch has taken 0:00:12.433643
Number of used sentences in train = 231
Total loss for epoch 6: 350.139445
	Epoch 7....
Epoch has taken 0:00:12.436405
Number of used sentences in train = 231
Total loss for epoch 7: 349.100359
	Epoch 8....
Epoch has taken 0:00:12.223749
Number of used sentences in train = 231
Total loss for epoch 8: 347.834644
	Epoch 9....
Epoch has taken 0:00:12.431395
Number of used sentences in train = 231
Total loss for epoch 9: 346.899781
	Epoch 10....
Epoch has taken 0:00:12.412056
Number of used sentences in train = 231
Total loss for epoch 10: 346.955780
	Epoch 11....
Epoch has taken 0:00:12.419666
Number of used sentences in train = 231
Total loss for epoch 11: 346.382379
	Epoch 12....
Epoch has taken 0:00:12.419452
Number of used sentences in train = 231
Total loss for epoch 12: 346.203170
	Epoch 13....
Epoch has taken 0:00:12.419505
Number of used sentences in train = 231
Total loss for epoch 13: 345.629851
	Epoch 14....
Epoch has taken 0:00:12.417978
Number of used sentences in train = 231
Total loss for epoch 14: 346.595511
Epoch has taken 0:00:12.412602

==================================================================================================
	Training time : 0:34:36.124452
==================================================================================================
	Identification : 0.394

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 63)
  (w_embeddings): Embedding(3369, 64)
  (lstm): LSTM(127, 65, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 13108.721473
validation loss after epoch 0 : 1116.659890
	Epoch 1....
Epoch has taken 0:03:54.669268
Number of used sentences in train = 3226
Total loss for epoch 1: 9763.928192
validation loss after epoch 1 : 1061.984020
	Epoch 2....
Epoch has taken 0:03:54.645717
Number of used sentences in train = 3226
Total loss for epoch 2: 8855.817853
validation loss after epoch 2 : 1066.262760
	Epoch 3....
Epoch has taken 0:03:52.850002
Number of used sentences in train = 3226
Total loss for epoch 3: 8332.599585
validation loss after epoch 3 : 1117.205537
	Epoch 4....
Epoch has taken 0:03:54.706048
Number of used sentences in train = 3226
Total loss for epoch 4: 7859.937062
validation loss after epoch 4 : 1122.395979
	Epoch 5....
Epoch has taken 0:03:54.757758
Number of used sentences in train = 3226
Total loss for epoch 5: 7529.967170
validation loss after epoch 5 : 1181.475167
	Epoch 6....
Epoch has taken 0:03:54.608025
Number of used sentences in train = 3226
Total loss for epoch 6: 7285.631336
validation loss after epoch 6 : 1148.020739
	Epoch 7....
Epoch has taken 0:03:52.313987
Number of used sentences in train = 3226
Total loss for epoch 7: 7049.338842
validation loss after epoch 7 : 1286.621702
	Epoch 8....
Epoch has taken 0:03:54.003768
Number of used sentences in train = 3226
Total loss for epoch 8: 6890.172926
validation loss after epoch 8 : 1361.481675
	Epoch 9....
Epoch has taken 0:03:52.864954
Number of used sentences in train = 3226
Total loss for epoch 9: 6795.222126
validation loss after epoch 9 : 1309.526600
	Epoch 10....
Epoch has taken 0:03:54.019978
Number of used sentences in train = 3226
Total loss for epoch 10: 6665.975531
validation loss after epoch 10 : 1413.015426
	Epoch 11....
Epoch has taken 0:03:54.256812
Number of used sentences in train = 3226
Total loss for epoch 11: 6561.360537
validation loss after epoch 11 : 1437.228423
	Epoch 12....
Epoch has taken 0:03:53.408315
Number of used sentences in train = 3226
Total loss for epoch 12: 6525.504855
validation loss after epoch 12 : 1522.976785
	Epoch 13....
Epoch has taken 0:04:00.096354
Number of used sentences in train = 3226
Total loss for epoch 13: 6459.042805
validation loss after epoch 13 : 1522.627710
	Epoch 14....
Epoch has taken 0:04:16.576495
Number of used sentences in train = 3226
Total loss for epoch 14: 6386.998054
validation loss after epoch 14 : 1581.364507
	TransitionClassifier(
  (p_embeddings): Embedding(13, 63)
  (w_embeddings): Embedding(3369, 64)
  (lstm): LSTM(127, 65, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=44, bias=True)
  (linear2): Linear(in_features=44, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:09.948756
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1624.696753
	Epoch 1....
Epoch has taken 0:00:22.961292
Number of used sentences in train = 359
Total loss for epoch 1: 1039.507673
	Epoch 2....
Epoch has taken 0:00:22.960723
Number of used sentences in train = 359
Total loss for epoch 2: 913.405274
	Epoch 3....
Epoch has taken 0:00:22.940668
Number of used sentences in train = 359
Total loss for epoch 3: 864.468429
	Epoch 4....
Epoch has taken 0:00:22.961659
Number of used sentences in train = 359
Total loss for epoch 4: 785.921529
	Epoch 5....
Epoch has taken 0:00:22.953321
Number of used sentences in train = 359
Total loss for epoch 5: 738.770198
	Epoch 6....
Epoch has taken 0:00:22.960206
Number of used sentences in train = 359
Total loss for epoch 6: 711.012764
	Epoch 7....
Epoch has taken 0:00:22.932962
Number of used sentences in train = 359
Total loss for epoch 7: 689.636548
	Epoch 8....
Epoch has taken 0:00:22.955256
Number of used sentences in train = 359
Total loss for epoch 8: 689.708011
	Epoch 9....
Epoch has taken 0:00:22.936674
Number of used sentences in train = 359
Total loss for epoch 9: 678.942310
	Epoch 10....
Epoch has taken 0:00:22.942167
Number of used sentences in train = 359
Total loss for epoch 10: 672.781179
	Epoch 11....
Epoch has taken 0:00:22.938943
Number of used sentences in train = 359
Total loss for epoch 11: 671.806142
	Epoch 12....
Epoch has taken 0:00:22.939470
Number of used sentences in train = 359
Total loss for epoch 12: 671.419185
	Epoch 13....
Epoch has taken 0:00:22.949050
Number of used sentences in train = 359
Total loss for epoch 13: 671.308658
	Epoch 14....
Epoch has taken 0:00:22.952292
Number of used sentences in train = 359
Total loss for epoch 14: 670.964828
Epoch has taken 0:00:22.948467

==================================================================================================
	Training time : 1:04:58.619844
==================================================================================================
	Identification : 0.358

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 122, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 19, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 79, 'lstmDropout': 0.29, 'denseActivation': 'tanh', 'wordDim': 98, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5898, 98)
  (lstm): LSTM(117, 79, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=122, bias=True)
  (linear2): Linear(in_features=122, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14693.157483
validation loss after epoch 0 : 1144.294317
	Epoch 1....
Epoch has taken 0:02:55.510974
Number of used sentences in train = 2811
Total loss for epoch 1: 9179.695027
validation loss after epoch 1 : 1060.287725
	Epoch 2....
Epoch has taken 0:03:02.293554
Number of used sentences in train = 2811
Total loss for epoch 2: 7547.706965
validation loss after epoch 2 : 1014.842964
	Epoch 3....
Epoch has taken 0:02:51.333821
Number of used sentences in train = 2811
Total loss for epoch 3: 6484.470038
validation loss after epoch 3 : 1103.229288
	Epoch 4....
Epoch has taken 0:02:52.593864
Number of used sentences in train = 2811
Total loss for epoch 4: 5766.012367
validation loss after epoch 4 : 1192.054732
	Epoch 5....
Epoch has taken 0:02:55.077266
Number of used sentences in train = 2811
Total loss for epoch 5: 5397.234621
validation loss after epoch 5 : 1264.762772
	Epoch 6....
Epoch has taken 0:03:00.689395
Number of used sentences in train = 2811
Total loss for epoch 6: 5151.504997
validation loss after epoch 6 : 1318.178721
	Epoch 7....
Epoch has taken 0:03:08.256771
Number of used sentences in train = 2811
Total loss for epoch 7: 4959.834226
validation loss after epoch 7 : 1433.738136
	Epoch 8....
Epoch has taken 0:02:50.977329
Number of used sentences in train = 2811
Total loss for epoch 8: 4814.966628
validation loss after epoch 8 : 1410.086341
	Epoch 9....
Epoch has taken 0:02:55.387964
Number of used sentences in train = 2811
Total loss for epoch 9: 4752.717235
validation loss after epoch 9 : 1444.289062
	Epoch 10....
Epoch has taken 0:02:57.990259
Number of used sentences in train = 2811
Total loss for epoch 10: 4697.606551
validation loss after epoch 10 : 1452.468483
	Epoch 11....
Epoch has taken 0:02:59.252656
Number of used sentences in train = 2811
Total loss for epoch 11: 4621.078467
validation loss after epoch 11 : 1489.584614
	Epoch 12....
Epoch has taken 0:03:15.801966
Number of used sentences in train = 2811
Total loss for epoch 12: 4612.434547
validation loss after epoch 12 : 1521.369260
	Epoch 13....
Epoch has taken 0:02:51.318868
Number of used sentences in train = 2811
Total loss for epoch 13: 4580.406387
validation loss after epoch 13 : 1591.192851
	Epoch 14....
Epoch has taken 0:02:51.281006
Number of used sentences in train = 2811
Total loss for epoch 14: 4567.942619
validation loss after epoch 14 : 1629.538013
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5898, 98)
  (lstm): LSTM(117, 79, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=122, bias=True)
  (linear2): Linear(in_features=122, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.504275
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1981.620046
	Epoch 1....
Epoch has taken 0:00:18.704770
Number of used sentences in train = 313
Total loss for epoch 1: 981.494522
	Epoch 2....
Epoch has taken 0:00:18.420937
Number of used sentences in train = 313
Total loss for epoch 2: 753.286369
	Epoch 3....
Epoch has taken 0:00:18.724779
Number of used sentences in train = 313
Total loss for epoch 3: 649.329372
	Epoch 4....
Epoch has taken 0:00:18.721303
Number of used sentences in train = 313
Total loss for epoch 4: 576.679282
	Epoch 5....
Epoch has taken 0:00:18.727610
Number of used sentences in train = 313
Total loss for epoch 5: 560.835192
	Epoch 6....
Epoch has taken 0:00:18.092312
Number of used sentences in train = 313
Total loss for epoch 6: 537.884210
	Epoch 7....
Epoch has taken 0:00:18.076713
Number of used sentences in train = 313
Total loss for epoch 7: 531.695028
	Epoch 8....
Epoch has taken 0:00:18.085882
Number of used sentences in train = 313
Total loss for epoch 8: 519.200118
	Epoch 9....
Epoch has taken 0:00:18.093455
Number of used sentences in train = 313
Total loss for epoch 9: 517.913751
	Epoch 10....
Epoch has taken 0:00:18.094495
Number of used sentences in train = 313
Total loss for epoch 10: 519.137726
	Epoch 11....
Epoch has taken 0:00:18.085983
Number of used sentences in train = 313
Total loss for epoch 11: 521.188899
	Epoch 12....
Epoch has taken 0:00:18.087797
Number of used sentences in train = 313
Total loss for epoch 12: 517.878950
	Epoch 13....
Epoch has taken 0:00:18.092144
Number of used sentences in train = 313
Total loss for epoch 13: 516.004874
	Epoch 14....
Epoch has taken 0:00:18.429505
Number of used sentences in train = 313
Total loss for epoch 14: 516.308366
Epoch has taken 0:00:18.726873

==================================================================================================
	Training time : 0:48:59.935662
==================================================================================================
	Identification : 0.129

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5663, 98)
  (lstm): LSTM(117, 79, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=122, bias=True)
  (linear2): Linear(in_features=122, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 15325.154691
validation loss after epoch 0 : 1082.542787
	Epoch 1....
Epoch has taken 0:02:01.128517
Number of used sentences in train = 2074
Total loss for epoch 1: 7269.542321
validation loss after epoch 1 : 826.232615
	Epoch 2....
Epoch has taken 0:02:01.240149
Number of used sentences in train = 2074
Total loss for epoch 2: 5513.791661
validation loss after epoch 2 : 838.869459
	Epoch 3....
Epoch has taken 0:01:57.242912
Number of used sentences in train = 2074
Total loss for epoch 3: 4642.531469
validation loss after epoch 3 : 893.180792
	Epoch 4....
Epoch has taken 0:01:57.226583
Number of used sentences in train = 2074
Total loss for epoch 4: 4089.958170
validation loss after epoch 4 : 891.221856
	Epoch 5....
Epoch has taken 0:01:57.242735
Number of used sentences in train = 2074
Total loss for epoch 5: 3789.517038
validation loss after epoch 5 : 954.971061
	Epoch 6....
Epoch has taken 0:01:58.746556
Number of used sentences in train = 2074
Total loss for epoch 6: 3625.407255
validation loss after epoch 6 : 1071.999939
	Epoch 7....
Epoch has taken 0:01:57.262479
Number of used sentences in train = 2074
Total loss for epoch 7: 3447.991453
validation loss after epoch 7 : 1019.048079
	Epoch 8....
Epoch has taken 0:01:57.160280
Number of used sentences in train = 2074
Total loss for epoch 8: 3396.369359
validation loss after epoch 8 : 1167.351968
	Epoch 9....
Epoch has taken 0:01:57.180617
Number of used sentences in train = 2074
Total loss for epoch 9: 3316.873629
validation loss after epoch 9 : 1131.835902
	Epoch 10....
Epoch has taken 0:01:57.157610
Number of used sentences in train = 2074
Total loss for epoch 10: 3294.354221
validation loss after epoch 10 : 1197.850425
	Epoch 11....
Epoch has taken 0:01:57.371822
Number of used sentences in train = 2074
Total loss for epoch 11: 3232.273623
validation loss after epoch 11 : 1241.885561
	Epoch 12....
Epoch has taken 0:01:57.336354
Number of used sentences in train = 2074
Total loss for epoch 12: 3206.462655
validation loss after epoch 12 : 1242.753810
	Epoch 13....
Epoch has taken 0:01:58.163674
Number of used sentences in train = 2074
Total loss for epoch 13: 3210.204564
validation loss after epoch 13 : 1223.525528
	Epoch 14....
Epoch has taken 0:01:57.210029
Number of used sentences in train = 2074
Total loss for epoch 14: 3212.970344
validation loss after epoch 14 : 1241.246129
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5663, 98)
  (lstm): LSTM(117, 79, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=122, bias=True)
  (linear2): Linear(in_features=122, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.292796
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1991.680413
	Epoch 1....
Epoch has taken 0:00:11.970403
Number of used sentences in train = 231
Total loss for epoch 1: 712.931293
	Epoch 2....
Epoch has taken 0:00:11.953039
Number of used sentences in train = 231
Total loss for epoch 2: 514.319430
	Epoch 3....
Epoch has taken 0:00:11.960944
Number of used sentences in train = 231
Total loss for epoch 3: 393.512634
	Epoch 4....
Epoch has taken 0:00:11.943260
Number of used sentences in train = 231
Total loss for epoch 4: 369.249700
	Epoch 5....
Epoch has taken 0:00:11.961221
Number of used sentences in train = 231
Total loss for epoch 5: 355.468278
	Epoch 6....
Epoch has taken 0:00:11.956573
Number of used sentences in train = 231
Total loss for epoch 6: 353.275545
	Epoch 7....
Epoch has taken 0:00:11.961234
Number of used sentences in train = 231
Total loss for epoch 7: 353.941889
	Epoch 8....
Epoch has taken 0:00:11.961268
Number of used sentences in train = 231
Total loss for epoch 8: 350.744507
	Epoch 9....
Epoch has taken 0:00:11.961960
Number of used sentences in train = 231
Total loss for epoch 9: 347.707665
	Epoch 10....
Epoch has taken 0:00:11.952511
Number of used sentences in train = 231
Total loss for epoch 10: 347.319207
	Epoch 11....
Epoch has taken 0:00:11.962365
Number of used sentences in train = 231
Total loss for epoch 11: 346.113700
	Epoch 12....
Epoch has taken 0:00:11.956824
Number of used sentences in train = 231
Total loss for epoch 12: 346.027499
	Epoch 13....
Epoch has taken 0:00:11.952745
Number of used sentences in train = 231
Total loss for epoch 13: 347.894983
	Epoch 14....
Epoch has taken 0:00:11.961498
Number of used sentences in train = 231
Total loss for epoch 14: 345.599187
Epoch has taken 0:00:11.949172

==================================================================================================
	Training time : 0:32:28.686619
==================================================================================================
	Identification : 0.269

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(6881, 98)
  (lstm): LSTM(117, 79, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=122, bias=True)
  (linear2): Linear(in_features=122, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 39324.725814
validation loss after epoch 0 : 3908.615466
	Epoch 1....
Epoch has taken 0:03:47.560383
Number of used sentences in train = 3226
Total loss for epoch 1: 25215.951016
validation loss after epoch 1 : 1678.820473
	Epoch 2....
Epoch has taken 0:03:47.517637
Number of used sentences in train = 3226
Total loss for epoch 2: 13974.201857
validation loss after epoch 2 : 1531.607846
	Epoch 3....
Epoch has taken 0:03:49.332761
Number of used sentences in train = 3226
Total loss for epoch 3: 11977.162737
validation loss after epoch 3 : 1545.725610
	Epoch 4....
Epoch has taken 0:03:53.541860
Number of used sentences in train = 3226
Total loss for epoch 4: 10684.761637
validation loss after epoch 4 : 1561.657337
	Epoch 5....
Epoch has taken 0:03:47.815625
Number of used sentences in train = 3226
Total loss for epoch 5: 9796.660594
validation loss after epoch 5 : 1591.765597
	Epoch 6....
Epoch has taken 0:03:47.933947
Number of used sentences in train = 3226
Total loss for epoch 6: 9244.642520
validation loss after epoch 6 : 1762.724065
	Epoch 7....
Epoch has taken 0:03:52.918315
Number of used sentences in train = 3226
Total loss for epoch 7: 8718.644854
validation loss after epoch 7 : 1663.583899
	Epoch 8....
Epoch has taken 0:03:47.617371
Number of used sentences in train = 3226
Total loss for epoch 8: 8285.807280
validation loss after epoch 8 : 1846.114649
	Epoch 9....
Epoch has taken 0:03:47.905895
Number of used sentences in train = 3226
Total loss for epoch 9: 7943.454294
validation loss after epoch 9 : 1859.659629
	Epoch 10....
Epoch has taken 0:03:48.968879
Number of used sentences in train = 3226
Total loss for epoch 10: 7607.235929
validation loss after epoch 10 : 1985.557481
	Epoch 11....
Epoch has taken 0:03:50.511126
Number of used sentences in train = 3226
Total loss for epoch 11: 7349.181709
validation loss after epoch 11 : 1900.810297
	Epoch 12....
Epoch has taken 0:03:52.991939
Number of used sentences in train = 3226
Total loss for epoch 12: 7207.959584
validation loss after epoch 12 : 2203.649233
	Epoch 13....
Epoch has taken 0:03:49.892163
Number of used sentences in train = 3226
Total loss for epoch 13: 7033.629634
validation loss after epoch 13 : 2139.807377
	Epoch 14....
Epoch has taken 0:03:51.475542
Number of used sentences in train = 3226
Total loss for epoch 14: 6919.164868
validation loss after epoch 14 : 2310.290084
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(6881, 98)
  (lstm): LSTM(117, 79, num_layers=2, dropout=0.29, bidirectional=True)
  (linear1): Linear(in_features=1264, out_features=122, bias=True)
  (linear2): Linear(in_features=122, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:52.778534
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2686.497237
	Epoch 1....
Epoch has taken 0:00:22.244872
Number of used sentences in train = 359
Total loss for epoch 1: 1416.766125
	Epoch 2....
Epoch has taken 0:00:22.231188
Number of used sentences in train = 359
Total loss for epoch 2: 1171.677128
	Epoch 3....
Epoch has taken 0:00:22.233057
Number of used sentences in train = 359
Total loss for epoch 3: 983.833922
	Epoch 4....
Epoch has taken 0:00:22.221119
Number of used sentences in train = 359
Total loss for epoch 4: 853.504665
	Epoch 5....
Epoch has taken 0:00:22.234494
Number of used sentences in train = 359
Total loss for epoch 5: 795.349977
	Epoch 6....
Epoch has taken 0:00:22.241260
Number of used sentences in train = 359
Total loss for epoch 6: 746.577563
	Epoch 7....
Epoch has taken 0:00:22.231156
Number of used sentences in train = 359
Total loss for epoch 7: 712.609812
	Epoch 8....
Epoch has taken 0:00:22.636622
Number of used sentences in train = 359
Total loss for epoch 8: 692.826457
	Epoch 9....
Epoch has taken 0:00:22.780164
Number of used sentences in train = 359
Total loss for epoch 9: 729.404439
	Epoch 10....
Epoch has taken 0:00:22.590920
Number of used sentences in train = 359
Total loss for epoch 10: 696.092377
	Epoch 11....
Epoch has taken 0:00:22.222087
Number of used sentences in train = 359
Total loss for epoch 11: 699.388761
	Epoch 12....
Epoch has taken 0:00:22.207528
Number of used sentences in train = 359
Total loss for epoch 12: 677.840468
	Epoch 13....
Epoch has taken 0:00:22.212958
Number of used sentences in train = 359
Total loss for epoch 13: 684.365029
	Epoch 14....
Epoch has taken 0:00:22.225392
Number of used sentences in train = 359
Total loss for epoch 14: 678.179065
Epoch has taken 0:00:22.210085

==================================================================================================
	Training time : 1:03:04.134828
==================================================================================================
	Identification : 0.456

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 87, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 62, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 65, 'lstmDropout': 0.36, 'denseActivation': 'tanh', 'wordDim': 86, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 62)
  (w_embeddings): Embedding(1177, 86)
  (lstm): LSTM(148, 65, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=87, bias=True)
  (linear2): Linear(in_features=87, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11128.080720
validation loss after epoch 0 : 920.607400
	Epoch 1....
Epoch has taken 0:02:39.723733
Number of used sentences in train = 2811
Total loss for epoch 1: 7886.855515
validation loss after epoch 1 : 935.408373
	Epoch 2....
Epoch has taken 0:02:39.292705
Number of used sentences in train = 2811
Total loss for epoch 2: 7051.280383
validation loss after epoch 2 : 920.307050
	Epoch 3....
Epoch has taken 0:02:40.736474
Number of used sentences in train = 2811
Total loss for epoch 3: 6410.585826
validation loss after epoch 3 : 922.424549
	Epoch 4....
Epoch has taken 0:02:45.305620
Number of used sentences in train = 2811
Total loss for epoch 4: 6003.587268
validation loss after epoch 4 : 945.541570
	Epoch 5....
Epoch has taken 0:02:44.331210
Number of used sentences in train = 2811
Total loss for epoch 5: 5623.823721
validation loss after epoch 5 : 988.724839
	Epoch 6....
Epoch has taken 0:02:42.042803
Number of used sentences in train = 2811
Total loss for epoch 6: 5377.782256
validation loss after epoch 6 : 1027.874740
	Epoch 7....
Epoch has taken 0:02:41.915380
Number of used sentences in train = 2811
Total loss for epoch 7: 5187.683210
validation loss after epoch 7 : 1062.908519
	Epoch 8....
Epoch has taken 0:02:43.543243
Number of used sentences in train = 2811
Total loss for epoch 8: 5020.294173
validation loss after epoch 8 : 1110.200992
	Epoch 9....
Epoch has taken 0:02:43.292598
Number of used sentences in train = 2811
Total loss for epoch 9: 4857.177222
validation loss after epoch 9 : 1144.061184
	Epoch 10....
Epoch has taken 0:02:42.155271
Number of used sentences in train = 2811
Total loss for epoch 10: 4764.603034
validation loss after epoch 10 : 1226.179289
	Epoch 11....
Epoch has taken 0:02:44.072714
Number of used sentences in train = 2811
Total loss for epoch 11: 4691.336101
validation loss after epoch 11 : 1202.790501
	Epoch 12....
Epoch has taken 0:02:53.545931
Number of used sentences in train = 2811
Total loss for epoch 12: 4634.055437
validation loss after epoch 12 : 1268.364056
	Epoch 13....
Epoch has taken 0:02:42.014409
Number of used sentences in train = 2811
Total loss for epoch 13: 4591.473107
validation loss after epoch 13 : 1307.445968
	Epoch 14....
Epoch has taken 0:02:41.772531
Number of used sentences in train = 2811
Total loss for epoch 14: 4564.767004
validation loss after epoch 14 : 1319.070933
	TransitionClassifier(
  (p_embeddings): Embedding(18, 62)
  (w_embeddings): Embedding(1177, 86)
  (lstm): LSTM(148, 65, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=87, bias=True)
  (linear2): Linear(in_features=87, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:42.407166
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1733.102596
	Epoch 1....
Epoch has taken 0:00:17.429952
Number of used sentences in train = 313
Total loss for epoch 1: 758.897006
	Epoch 2....
Epoch has taken 0:00:17.432325
Number of used sentences in train = 313
Total loss for epoch 2: 617.110212
	Epoch 3....
Epoch has taken 0:00:17.443844
Number of used sentences in train = 313
Total loss for epoch 3: 566.514459
	Epoch 4....
Epoch has taken 0:00:17.432508
Number of used sentences in train = 313
Total loss for epoch 4: 543.478917
	Epoch 5....
Epoch has taken 0:00:17.414754
Number of used sentences in train = 313
Total loss for epoch 5: 530.424139
	Epoch 6....
Epoch has taken 0:00:17.426416
Number of used sentences in train = 313
Total loss for epoch 6: 525.123685
	Epoch 7....
Epoch has taken 0:00:17.425977
Number of used sentences in train = 313
Total loss for epoch 7: 521.452304
	Epoch 8....
Epoch has taken 0:00:17.425802
Number of used sentences in train = 313
Total loss for epoch 8: 518.226632
	Epoch 9....
Epoch has taken 0:00:17.424321
Number of used sentences in train = 313
Total loss for epoch 9: 515.729554
	Epoch 10....
Epoch has taken 0:00:17.407316
Number of used sentences in train = 313
Total loss for epoch 10: 515.079169
	Epoch 11....
Epoch has taken 0:00:17.422187
Number of used sentences in train = 313
Total loss for epoch 11: 513.776713
	Epoch 12....
Epoch has taken 0:00:17.432224
Number of used sentences in train = 313
Total loss for epoch 12: 512.623525
	Epoch 13....
Epoch has taken 0:00:17.428297
Number of used sentences in train = 313
Total loss for epoch 13: 511.016171
	Epoch 14....
Epoch has taken 0:00:17.441024
Number of used sentences in train = 313
Total loss for epoch 14: 509.919619
Epoch has taken 0:00:17.437370

==================================================================================================
	Training time : 0:45:08.070321
==================================================================================================
	Identification : 0.317

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 62)
  (w_embeddings): Embedding(1133, 86)
  (lstm): LSTM(148, 65, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=87, bias=True)
  (linear2): Linear(in_features=87, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8782.577844
validation loss after epoch 0 : 718.666966
	Epoch 1....
Epoch has taken 0:01:52.360949
Number of used sentences in train = 2074
Total loss for epoch 1: 5578.415186
validation loss after epoch 1 : 705.929504
	Epoch 2....
Epoch has taken 0:02:10.476456
Number of used sentences in train = 2074
Total loss for epoch 2: 4719.018616
validation loss after epoch 2 : 682.280224
	Epoch 3....
Epoch has taken 0:01:52.843527
Number of used sentences in train = 2074
Total loss for epoch 3: 4208.316790
validation loss after epoch 3 : 779.360855
	Epoch 4....
Epoch has taken 0:01:52.762965
Number of used sentences in train = 2074
Total loss for epoch 4: 3850.363450
validation loss after epoch 4 : 742.224454
	Epoch 5....
Epoch has taken 0:01:49.817428
Number of used sentences in train = 2074
Total loss for epoch 5: 3586.516109
validation loss after epoch 5 : 788.803697
	Epoch 6....
Epoch has taken 0:01:49.099562
Number of used sentences in train = 2074
Total loss for epoch 6: 3422.060244
validation loss after epoch 6 : 815.321403
	Epoch 7....
Epoch has taken 0:01:49.087676
Number of used sentences in train = 2074
Total loss for epoch 7: 3325.914455
validation loss after epoch 7 : 835.592517
	Epoch 8....
Epoch has taken 0:01:49.529254
Number of used sentences in train = 2074
Total loss for epoch 8: 3256.913929
validation loss after epoch 8 : 864.614791
	Epoch 9....
Epoch has taken 0:01:52.504535
Number of used sentences in train = 2074
Total loss for epoch 9: 3226.099643
validation loss after epoch 9 : 896.632105
	Epoch 10....
Epoch has taken 0:01:49.195675
Number of used sentences in train = 2074
Total loss for epoch 10: 3199.154946
validation loss after epoch 10 : 917.057674
	Epoch 11....
Epoch has taken 0:01:49.024337
Number of used sentences in train = 2074
Total loss for epoch 11: 3188.468853
validation loss after epoch 11 : 929.195704
	Epoch 12....
Epoch has taken 0:01:49.837162
Number of used sentences in train = 2074
Total loss for epoch 12: 3179.367269
validation loss after epoch 12 : 946.822099
	Epoch 13....
Epoch has taken 0:01:49.054283
Number of used sentences in train = 2074
Total loss for epoch 13: 3172.763752
validation loss after epoch 13 : 955.257134
	Epoch 14....
Epoch has taken 0:01:48.969859
Number of used sentences in train = 2074
Total loss for epoch 14: 3169.966871
validation loss after epoch 14 : 964.587823
	TransitionClassifier(
  (p_embeddings): Embedding(18, 62)
  (w_embeddings): Embedding(1133, 86)
  (lstm): LSTM(148, 65, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=87, bias=True)
  (linear2): Linear(in_features=87, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.639571
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1579.671564
	Epoch 1....
Epoch has taken 0:00:11.094265
Number of used sentences in train = 231
Total loss for epoch 1: 579.824636
	Epoch 2....
Epoch has taken 0:00:11.099455
Number of used sentences in train = 231
Total loss for epoch 2: 455.131597
	Epoch 3....
Epoch has taken 0:00:11.114773
Number of used sentences in train = 231
Total loss for epoch 3: 399.321663
	Epoch 4....
Epoch has taken 0:00:11.091509
Number of used sentences in train = 231
Total loss for epoch 4: 368.072550
	Epoch 5....
Epoch has taken 0:00:11.093970
Number of used sentences in train = 231
Total loss for epoch 5: 357.054039
	Epoch 6....
Epoch has taken 0:00:11.102780
Number of used sentences in train = 231
Total loss for epoch 6: 352.007965
	Epoch 7....
Epoch has taken 0:00:11.101429
Number of used sentences in train = 231
Total loss for epoch 7: 349.591792
	Epoch 8....
Epoch has taken 0:00:11.095083
Number of used sentences in train = 231
Total loss for epoch 8: 348.039085
	Epoch 9....
Epoch has taken 0:00:11.093422
Number of used sentences in train = 231
Total loss for epoch 9: 347.202914
	Epoch 10....
Epoch has taken 0:00:11.093559
Number of used sentences in train = 231
Total loss for epoch 10: 346.585375
	Epoch 11....
Epoch has taken 0:00:11.108844
Number of used sentences in train = 231
Total loss for epoch 11: 346.315884
	Epoch 12....
Epoch has taken 0:00:11.103065
Number of used sentences in train = 231
Total loss for epoch 12: 346.005687
	Epoch 13....
Epoch has taken 0:00:11.096446
Number of used sentences in train = 231
Total loss for epoch 13: 345.841482
	Epoch 14....
Epoch has taken 0:00:11.090857
Number of used sentences in train = 231
Total loss for epoch 14: 345.625019
Epoch has taken 0:00:11.101283

==================================================================================================
	Training time : 0:30:43.054476
==================================================================================================
	Identification : 0.48

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 62)
  (w_embeddings): Embedding(1202, 86)
  (lstm): LSTM(148, 65, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=87, bias=True)
  (linear2): Linear(in_features=87, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15717.233461
validation loss after epoch 0 : 1322.231734
	Epoch 1....
Epoch has taken 0:03:34.654747
Number of used sentences in train = 3226
Total loss for epoch 1: 11592.507618
validation loss after epoch 1 : 1313.815475
	Epoch 2....
Epoch has taken 0:03:37.293780
Number of used sentences in train = 3226
Total loss for epoch 2: 10788.544586
validation loss after epoch 2 : 1272.282594
	Epoch 3....
Epoch has taken 0:03:33.375202
Number of used sentences in train = 3226
Total loss for epoch 3: 10001.209798
validation loss after epoch 3 : 1281.615792
	Epoch 4....
Epoch has taken 0:03:37.408339
Number of used sentences in train = 3226
Total loss for epoch 4: 9427.167644
validation loss after epoch 4 : 1306.999448
	Epoch 5....
Epoch has taken 0:03:34.403917
Number of used sentences in train = 3226
Total loss for epoch 5: 9033.794784
validation loss after epoch 5 : 1329.783877
	Epoch 6....
Epoch has taken 0:03:34.543965
Number of used sentences in train = 3226
Total loss for epoch 6: 8617.424554
validation loss after epoch 6 : 1353.560512
	Epoch 7....
Epoch has taken 0:03:33.753355
Number of used sentences in train = 3226
Total loss for epoch 7: 8274.170465
validation loss after epoch 7 : 1420.614761
	Epoch 8....
Epoch has taken 0:03:32.607792
Number of used sentences in train = 3226
Total loss for epoch 8: 7958.686867
validation loss after epoch 8 : 1468.329726
	Epoch 9....
Epoch has taken 0:03:35.430605
Number of used sentences in train = 3226
Total loss for epoch 9: 7738.078875
validation loss after epoch 9 : 1540.091666
	Epoch 10....
Epoch has taken 0:03:37.424350
Number of used sentences in train = 3226
Total loss for epoch 10: 7514.051514
validation loss after epoch 10 : 1581.209277
	Epoch 11....
Epoch has taken 0:03:33.578050
Number of used sentences in train = 3226
Total loss for epoch 11: 7293.046058
validation loss after epoch 11 : 1622.722134
	Epoch 12....
Epoch has taken 0:03:32.112334
Number of used sentences in train = 3226
Total loss for epoch 12: 7142.126859
validation loss after epoch 12 : 1686.002023
	Epoch 13....
Epoch has taken 0:03:35.406225
Number of used sentences in train = 3226
Total loss for epoch 13: 7000.004658
validation loss after epoch 13 : 1746.648443
	Epoch 14....
Epoch has taken 0:03:37.234355
Number of used sentences in train = 3226
Total loss for epoch 14: 6894.466909
validation loss after epoch 14 : 1762.396504
	TransitionClassifier(
  (p_embeddings): Embedding(13, 62)
  (w_embeddings): Embedding(1202, 86)
  (lstm): LSTM(148, 65, bidirectional=True)
  (linear1): Linear(in_features=1040, out_features=87, bias=True)
  (linear2): Linear(in_features=87, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:32.136187
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 4127.996170
	Epoch 1....
Epoch has taken 0:00:20.635109
Number of used sentences in train = 359
Total loss for epoch 1: 1165.792620
	Epoch 2....
Epoch has taken 0:00:20.653103
Number of used sentences in train = 359
Total loss for epoch 2: 974.105832
	Epoch 3....
Epoch has taken 0:00:20.646514
Number of used sentences in train = 359
Total loss for epoch 3: 861.226996
	Epoch 4....
Epoch has taken 0:00:20.655944
Number of used sentences in train = 359
Total loss for epoch 4: 772.553122
	Epoch 5....
Epoch has taken 0:00:20.655432
Number of used sentences in train = 359
Total loss for epoch 5: 730.435916
	Epoch 6....
Epoch has taken 0:00:20.663871
Number of used sentences in train = 359
Total loss for epoch 6: 705.759617
	Epoch 7....
Epoch has taken 0:00:20.644417
Number of used sentences in train = 359
Total loss for epoch 7: 686.101570
	Epoch 8....
Epoch has taken 0:00:20.664183
Number of used sentences in train = 359
Total loss for epoch 8: 680.560288
	Epoch 9....
Epoch has taken 0:00:20.898951
Number of used sentences in train = 359
Total loss for epoch 9: 677.451793
	Epoch 10....
Epoch has taken 0:00:20.660524
Number of used sentences in train = 359
Total loss for epoch 10: 675.923015
	Epoch 11....
Epoch has taken 0:00:20.649523
Number of used sentences in train = 359
Total loss for epoch 11: 674.468664
	Epoch 12....
Epoch has taken 0:00:20.655433
Number of used sentences in train = 359
Total loss for epoch 12: 673.805191
	Epoch 13....
Epoch has taken 0:00:20.650507
Number of used sentences in train = 359
Total loss for epoch 13: 673.201633
	Epoch 14....
Epoch has taken 0:00:20.658450
Number of used sentences in train = 359
Total loss for epoch 14: 672.772766
Epoch has taken 0:00:20.635557

==================================================================================================
	Training time : 0:58:52.031223
==================================================================================================
	Identification : 0.134

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 72, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 25, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 47, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 108, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(9264, 108)
  (lstm): LSTM(133, 47, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13884.299306
validation loss after epoch 0 : 1093.417885
	Epoch 1....
Epoch has taken 0:02:53.154842
Number of used sentences in train = 2811
Total loss for epoch 1: 8257.333452
validation loss after epoch 1 : 1022.809232
	Epoch 2....
Epoch has taken 0:02:56.160962
Number of used sentences in train = 2811
Total loss for epoch 2: 6463.101099
validation loss after epoch 2 : 1147.196756
	Epoch 3....
Epoch has taken 0:02:50.554547
Number of used sentences in train = 2811
Total loss for epoch 3: 5581.625837
validation loss after epoch 3 : 1242.450313
	Epoch 4....
Epoch has taken 0:02:51.468347
Number of used sentences in train = 2811
Total loss for epoch 4: 5141.553058
validation loss after epoch 4 : 1242.519782
	Epoch 5....
Epoch has taken 0:02:50.699792
Number of used sentences in train = 2811
Total loss for epoch 5: 4903.249283
validation loss after epoch 5 : 1355.605528
	Epoch 6....
Epoch has taken 0:02:52.700720
Number of used sentences in train = 2811
Total loss for epoch 6: 4747.781214
validation loss after epoch 6 : 1417.418393
	Epoch 7....
Epoch has taken 0:02:53.603115
Number of used sentences in train = 2811
Total loss for epoch 7: 4644.697039
validation loss after epoch 7 : 1450.942778
	Epoch 8....
Epoch has taken 0:02:50.914154
Number of used sentences in train = 2811
Total loss for epoch 8: 4599.060011
validation loss after epoch 8 : 1488.915815
	Epoch 9....
Epoch has taken 0:02:50.633305
Number of used sentences in train = 2811
Total loss for epoch 9: 4555.233937
validation loss after epoch 9 : 1574.889766
	Epoch 10....
Epoch has taken 0:02:50.886861
Number of used sentences in train = 2811
Total loss for epoch 10: 4553.236175
validation loss after epoch 10 : 1570.782682
	Epoch 11....
Epoch has taken 0:02:50.858432
Number of used sentences in train = 2811
Total loss for epoch 11: 4537.700288
validation loss after epoch 11 : 1664.423480
	Epoch 12....
Epoch has taken 0:02:50.743639
Number of used sentences in train = 2811
Total loss for epoch 12: 4505.917149
validation loss after epoch 12 : 1620.856181
	Epoch 13....
Epoch has taken 0:02:50.836657
Number of used sentences in train = 2811
Total loss for epoch 13: 4492.838140
validation loss after epoch 13 : 1636.582387
	Epoch 14....
Epoch has taken 0:02:51.114378
Number of used sentences in train = 2811
Total loss for epoch 14: 4489.013731
validation loss after epoch 14 : 1715.486513
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(9264, 108)
  (lstm): LSTM(133, 47, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.868069
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1646.229429
	Epoch 1....
Epoch has taken 0:00:18.070313
Number of used sentences in train = 313
Total loss for epoch 1: 788.893103
	Epoch 2....
Epoch has taken 0:00:18.066229
Number of used sentences in train = 313
Total loss for epoch 2: 618.512941
	Epoch 3....
Epoch has taken 0:00:18.051700
Number of used sentences in train = 313
Total loss for epoch 3: 558.067355
	Epoch 4....
Epoch has taken 0:00:18.078034
Number of used sentences in train = 313
Total loss for epoch 4: 539.075460
	Epoch 5....
Epoch has taken 0:00:18.061463
Number of used sentences in train = 313
Total loss for epoch 5: 517.657573
	Epoch 6....
Epoch has taken 0:00:18.065048
Number of used sentences in train = 313
Total loss for epoch 6: 508.311482
	Epoch 7....
Epoch has taken 0:00:18.058108
Number of used sentences in train = 313
Total loss for epoch 7: 504.554894
	Epoch 8....
Epoch has taken 0:00:18.062400
Number of used sentences in train = 313
Total loss for epoch 8: 503.335963
	Epoch 9....
Epoch has taken 0:00:18.058042
Number of used sentences in train = 313
Total loss for epoch 9: 502.292816
	Epoch 10....
Epoch has taken 0:00:18.625799
Number of used sentences in train = 313
Total loss for epoch 10: 501.560477
	Epoch 11....
Epoch has taken 0:00:18.057630
Number of used sentences in train = 313
Total loss for epoch 11: 501.360655
	Epoch 12....
Epoch has taken 0:00:18.060225
Number of used sentences in train = 313
Total loss for epoch 12: 500.966405
	Epoch 13....
Epoch has taken 0:00:18.049497
Number of used sentences in train = 313
Total loss for epoch 13: 500.870420
	Epoch 14....
Epoch has taken 0:00:18.048578
Number of used sentences in train = 313
Total loss for epoch 14: 500.584502
Epoch has taken 0:00:18.051253

==================================================================================================
	Training time : 0:47:27.170793
==================================================================================================
	Identification : 0.263

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(7061, 108)
  (lstm): LSTM(133, 47, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11285.784469
validation loss after epoch 0 : 963.256328
	Epoch 1....
Epoch has taken 0:01:57.138383
Number of used sentences in train = 2074
Total loss for epoch 1: 6400.363326
validation loss after epoch 1 : 853.999121
	Epoch 2....
Epoch has taken 0:02:00.743161
Number of used sentences in train = 2074
Total loss for epoch 2: 4738.446435
validation loss after epoch 2 : 990.212744
	Epoch 3....
Epoch has taken 0:01:56.973018
Number of used sentences in train = 2074
Total loss for epoch 3: 4054.798250
validation loss after epoch 3 : 999.147010
	Epoch 4....
Epoch has taken 0:01:57.059503
Number of used sentences in train = 2074
Total loss for epoch 4: 3721.836922
validation loss after epoch 4 : 1116.626111
	Epoch 5....
Epoch has taken 0:01:57.333310
Number of used sentences in train = 2074
Total loss for epoch 5: 3487.332709
validation loss after epoch 5 : 1069.833444
	Epoch 6....
Epoch has taken 0:01:59.788824
Number of used sentences in train = 2074
Total loss for epoch 6: 3391.245032
validation loss after epoch 6 : 1100.045697
	Epoch 7....
Epoch has taken 0:01:59.266395
Number of used sentences in train = 2074
Total loss for epoch 7: 3332.192587
validation loss after epoch 7 : 1081.350471
	Epoch 8....
Epoch has taken 0:01:56.904515
Number of used sentences in train = 2074
Total loss for epoch 8: 3274.014080
validation loss after epoch 8 : 1167.803680
	Epoch 9....
Epoch has taken 0:01:56.944253
Number of used sentences in train = 2074
Total loss for epoch 9: 3248.371474
validation loss after epoch 9 : 1219.667926
	Epoch 10....
Epoch has taken 0:01:56.956075
Number of used sentences in train = 2074
Total loss for epoch 10: 3218.917304
validation loss after epoch 10 : 1282.707995
	Epoch 11....
Epoch has taken 0:01:56.991065
Number of used sentences in train = 2074
Total loss for epoch 11: 3205.284469
validation loss after epoch 11 : 1285.775343
	Epoch 12....
Epoch has taken 0:01:57.021402
Number of used sentences in train = 2074
Total loss for epoch 12: 3200.872209
validation loss after epoch 12 : 1269.536584
	Epoch 13....
Epoch has taken 0:01:56.934110
Number of used sentences in train = 2074
Total loss for epoch 13: 3184.997405
validation loss after epoch 13 : 1295.460508
	Epoch 14....
Epoch has taken 0:01:57.048446
Number of used sentences in train = 2074
Total loss for epoch 14: 3171.961549
validation loss after epoch 14 : 1215.352773
	TransitionClassifier(
  (p_embeddings): Embedding(18, 25)
  (w_embeddings): Embedding(7061, 108)
  (lstm): LSTM(133, 47, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.948042
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1773.138357
	Epoch 1....
Epoch has taken 0:00:11.898777
Number of used sentences in train = 231
Total loss for epoch 1: 606.038939
	Epoch 2....
Epoch has taken 0:00:11.897151
Number of used sentences in train = 231
Total loss for epoch 2: 445.478582
	Epoch 3....
Epoch has taken 0:00:11.892684
Number of used sentences in train = 231
Total loss for epoch 3: 390.443538
	Epoch 4....
Epoch has taken 0:00:11.892088
Number of used sentences in train = 231
Total loss for epoch 4: 370.989612
	Epoch 5....
Epoch has taken 0:00:11.892818
Number of used sentences in train = 231
Total loss for epoch 5: 361.018472
	Epoch 6....
Epoch has taken 0:00:11.899801
Number of used sentences in train = 231
Total loss for epoch 6: 351.553817
	Epoch 7....
Epoch has taken 0:00:11.891367
Number of used sentences in train = 231
Total loss for epoch 7: 350.111515
	Epoch 8....
Epoch has taken 0:00:11.891722
Number of used sentences in train = 231
Total loss for epoch 8: 348.604316
	Epoch 9....
Epoch has taken 0:00:11.893990
Number of used sentences in train = 231
Total loss for epoch 9: 347.954061
	Epoch 10....
Epoch has taken 0:00:11.885226
Number of used sentences in train = 231
Total loss for epoch 10: 346.690414
	Epoch 11....
Epoch has taken 0:00:11.894645
Number of used sentences in train = 231
Total loss for epoch 11: 346.316600
	Epoch 12....
Epoch has taken 0:00:11.890101
Number of used sentences in train = 231
Total loss for epoch 12: 346.037362
	Epoch 13....
Epoch has taken 0:00:11.890889
Number of used sentences in train = 231
Total loss for epoch 13: 345.647057
	Epoch 14....
Epoch has taken 0:00:11.894840
Number of used sentences in train = 231
Total loss for epoch 14: 345.554744
Epoch has taken 0:00:11.891832

==================================================================================================
	Training time : 0:32:22.790230
==================================================================================================
	Identification : 0.077

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 25)
  (w_embeddings): Embedding(17988, 108)
  (lstm): LSTM(133, 47, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20224.370474
validation loss after epoch 0 : 1631.758833
	Epoch 1....
Epoch has taken 0:03:51.818379
Number of used sentences in train = 3226
Total loss for epoch 1: 12132.500249
validation loss after epoch 1 : 1554.987505
	Epoch 2....
Epoch has taken 0:03:50.003043
Number of used sentences in train = 3226
Total loss for epoch 2: 9604.444911
validation loss after epoch 2 : 1688.951614
	Epoch 3....
Epoch has taken 0:04:01.628461
Number of used sentences in train = 3226
Total loss for epoch 3: 8164.029327
validation loss after epoch 3 : 1967.779872
	Epoch 4....
Epoch has taken 0:04:14.716361
Number of used sentences in train = 3226
Total loss for epoch 4: 7299.102540
validation loss after epoch 4 : 2171.649769
	Epoch 5....
Epoch has taken 0:03:46.927186
Number of used sentences in train = 3226
Total loss for epoch 5: 6845.366017
validation loss after epoch 5 : 2159.483397
	Epoch 6....
Epoch has taken 0:03:48.997997
Number of used sentences in train = 3226
Total loss for epoch 6: 6601.792863
validation loss after epoch 6 : 2531.294239
	Epoch 7....
Epoch has taken 0:03:51.574212
Number of used sentences in train = 3226
Total loss for epoch 7: 6483.668566
validation loss after epoch 7 : 2472.941009
	Epoch 8....
Epoch has taken 0:03:51.552822
Number of used sentences in train = 3226
Total loss for epoch 8: 6407.433316
validation loss after epoch 8 : 2549.366052
	Epoch 9....
Epoch has taken 0:03:47.200819
Number of used sentences in train = 3226
Total loss for epoch 9: 6318.996039
validation loss after epoch 9 : 2631.651819
	Epoch 10....
Epoch has taken 0:03:47.611147
Number of used sentences in train = 3226
Total loss for epoch 10: 6289.263771
validation loss after epoch 10 : 2619.585345
	Epoch 11....
Epoch has taken 0:03:50.040753
Number of used sentences in train = 3226
Total loss for epoch 11: 6240.196263
validation loss after epoch 11 : 2794.359477
	Epoch 12....
Epoch has taken 0:03:48.712219
Number of used sentences in train = 3226
Total loss for epoch 12: 6239.923333
validation loss after epoch 12 : 2750.926526
	Epoch 13....
Epoch has taken 0:04:08.451317
Number of used sentences in train = 3226
Total loss for epoch 13: 6225.782814
validation loss after epoch 13 : 2804.791937
	Epoch 14....
Epoch has taken 0:04:14.572807
Number of used sentences in train = 3226
Total loss for epoch 14: 6189.325032
validation loss after epoch 14 : 2973.467684
	TransitionClassifier(
  (p_embeddings): Embedding(13, 25)
  (w_embeddings): Embedding(17988, 108)
  (lstm): LSTM(133, 47, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:16.233795
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 3674.052062
	Epoch 1....
Epoch has taken 0:00:25.253061
Number of used sentences in train = 359
Total loss for epoch 1: 1179.380326
	Epoch 2....
Epoch has taken 0:00:25.278047
Number of used sentences in train = 359
Total loss for epoch 2: 840.070422
	Epoch 3....
Epoch has taken 0:00:25.258101
Number of used sentences in train = 359
Total loss for epoch 3: 738.588827
	Epoch 4....
Epoch has taken 0:00:24.993634
Number of used sentences in train = 359
Total loss for epoch 4: 705.734336
	Epoch 5....
Epoch has taken 0:00:22.885374
Number of used sentences in train = 359
Total loss for epoch 5: 694.504830
	Epoch 6....
Epoch has taken 0:00:22.863586
Number of used sentences in train = 359
Total loss for epoch 6: 691.110715
	Epoch 7....
Epoch has taken 0:00:22.845847
Number of used sentences in train = 359
Total loss for epoch 7: 676.998740
	Epoch 8....
Epoch has taken 0:00:22.839725
Number of used sentences in train = 359
Total loss for epoch 8: 672.586961
	Epoch 9....
Epoch has taken 0:00:22.841925
Number of used sentences in train = 359
Total loss for epoch 9: 673.956816
	Epoch 10....
Epoch has taken 0:00:22.847922
Number of used sentences in train = 359
Total loss for epoch 10: 671.813431
	Epoch 11....
Epoch has taken 0:00:22.838320
Number of used sentences in train = 359
Total loss for epoch 11: 671.549513
	Epoch 12....
Epoch has taken 0:00:22.835897
Number of used sentences in train = 359
Total loss for epoch 12: 671.343275
	Epoch 13....
Epoch has taken 0:00:22.856462
Number of used sentences in train = 359
Total loss for epoch 13: 671.307502
	Epoch 14....
Epoch has taken 0:00:22.847308
Number of used sentences in train = 359
Total loss for epoch 14: 671.225464
Epoch has taken 0:00:22.872579

==================================================================================================
	Training time : 1:05:02.880433
==================================================================================================
	Identification : 0.285

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 112, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 15, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 52, 'lstmDropout': 0.31, 'denseActivation': 'tanh', 'wordDim': 179, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1882, 179)
  (lstm): LSTM(194, 52, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10296.664793
validation loss after epoch 0 : 851.357193
	Epoch 1....
Epoch has taken 0:02:58.589908
Number of used sentences in train = 2811
Total loss for epoch 1: 7046.529911
validation loss after epoch 1 : 855.663867
	Epoch 2....
Epoch has taken 0:02:39.144992
Number of used sentences in train = 2811
Total loss for epoch 2: 6057.468863
validation loss after epoch 2 : 885.900581
	Epoch 3....
Epoch has taken 0:02:44.322278
Number of used sentences in train = 2811
Total loss for epoch 3: 5499.394479
validation loss after epoch 3 : 926.463546
	Epoch 4....
Epoch has taken 0:02:39.165089
Number of used sentences in train = 2811
Total loss for epoch 4: 5129.379596
validation loss after epoch 4 : 955.846076
	Epoch 5....
Epoch has taken 0:02:41.650866
Number of used sentences in train = 2811
Total loss for epoch 5: 4927.692883
validation loss after epoch 5 : 1031.286258
	Epoch 6....
Epoch has taken 0:02:39.113160
Number of used sentences in train = 2811
Total loss for epoch 6: 4814.296675
validation loss after epoch 6 : 1052.907413
	Epoch 7....
Epoch has taken 0:02:39.029869
Number of used sentences in train = 2811
Total loss for epoch 7: 4706.893936
validation loss after epoch 7 : 1090.854470
	Epoch 8....
Epoch has taken 0:02:38.939205
Number of used sentences in train = 2811
Total loss for epoch 8: 4652.101565
validation loss after epoch 8 : 1134.845788
	Epoch 9....
Epoch has taken 0:02:39.069114
Number of used sentences in train = 2811
Total loss for epoch 9: 4612.737705
validation loss after epoch 9 : 1172.044148
	Epoch 10....
Epoch has taken 0:02:38.989151
Number of used sentences in train = 2811
Total loss for epoch 10: 4585.170107
validation loss after epoch 10 : 1194.724259
	Epoch 11....
Epoch has taken 0:02:38.948303
Number of used sentences in train = 2811
Total loss for epoch 11: 4561.125603
validation loss after epoch 11 : 1217.696751
	Epoch 12....
Epoch has taken 0:02:38.823312
Number of used sentences in train = 2811
Total loss for epoch 12: 4545.986257
validation loss after epoch 12 : 1225.285051
	Epoch 13....
Epoch has taken 0:02:43.252220
Number of used sentences in train = 2811
Total loss for epoch 13: 4533.116570
validation loss after epoch 13 : 1262.823467
	Epoch 14....
Epoch has taken 0:02:38.972508
Number of used sentences in train = 2811
Total loss for epoch 14: 4527.376009
validation loss after epoch 14 : 1259.958387
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1882, 179)
  (lstm): LSTM(194, 52, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:42.492539
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1341.347455
	Epoch 1....
Epoch has taken 0:00:16.816048
Number of used sentences in train = 313
Total loss for epoch 1: 697.867373
	Epoch 2....
Epoch has taken 0:00:16.820173
Number of used sentences in train = 313
Total loss for epoch 2: 580.715960
	Epoch 3....
Epoch has taken 0:00:16.810257
Number of used sentences in train = 313
Total loss for epoch 3: 540.314637
	Epoch 4....
Epoch has taken 0:00:16.811968
Number of used sentences in train = 313
Total loss for epoch 4: 518.038792
	Epoch 5....
Epoch has taken 0:00:16.808971
Number of used sentences in train = 313
Total loss for epoch 5: 511.993680
	Epoch 6....
Epoch has taken 0:00:16.815191
Number of used sentences in train = 313
Total loss for epoch 6: 508.463397
	Epoch 7....
Epoch has taken 0:00:16.803005
Number of used sentences in train = 313
Total loss for epoch 7: 506.578368
	Epoch 8....
Epoch has taken 0:00:16.813083
Number of used sentences in train = 313
Total loss for epoch 8: 505.469491
	Epoch 9....
Epoch has taken 0:00:16.802909
Number of used sentences in train = 313
Total loss for epoch 9: 504.267200
	Epoch 10....
Epoch has taken 0:00:16.809584
Number of used sentences in train = 313
Total loss for epoch 10: 504.064944
	Epoch 11....
Epoch has taken 0:00:16.800059
Number of used sentences in train = 313
Total loss for epoch 11: 503.639483
	Epoch 12....
Epoch has taken 0:00:17.253354
Number of used sentences in train = 313
Total loss for epoch 12: 503.424481
	Epoch 13....
Epoch has taken 0:00:16.811481
Number of used sentences in train = 313
Total loss for epoch 13: 502.918330
	Epoch 14....
Epoch has taken 0:00:17.006165
Number of used sentences in train = 313
Total loss for epoch 14: 502.833259
Epoch has taken 0:00:16.815565

==================================================================================================
	Training time : 0:44:33.802626
==================================================================================================
	Identification : 0.128

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1680, 179)
  (lstm): LSTM(194, 52, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8607.738375
validation loss after epoch 0 : 729.571810
	Epoch 1....
Epoch has taken 0:01:48.727987
Number of used sentences in train = 2074
Total loss for epoch 1: 4895.181345
validation loss after epoch 1 : 690.218482
	Epoch 2....
Epoch has taken 0:01:50.651531
Number of used sentences in train = 2074
Total loss for epoch 2: 3957.778880
validation loss after epoch 2 : 746.477811
	Epoch 3....
Epoch has taken 0:01:51.044330
Number of used sentences in train = 2074
Total loss for epoch 3: 3518.445642
validation loss after epoch 3 : 869.619851
	Epoch 4....
Epoch has taken 0:01:51.334303
Number of used sentences in train = 2074
Total loss for epoch 4: 3342.100525
validation loss after epoch 4 : 860.093978
	Epoch 5....
Epoch has taken 0:01:51.000656
Number of used sentences in train = 2074
Total loss for epoch 5: 3255.023614
validation loss after epoch 5 : 920.177514
	Epoch 6....
Epoch has taken 0:01:48.906728
Number of used sentences in train = 2074
Total loss for epoch 6: 3225.618324
validation loss after epoch 6 : 955.547582
	Epoch 7....
Epoch has taken 0:01:52.410140
Number of used sentences in train = 2074
Total loss for epoch 7: 3200.395301
validation loss after epoch 7 : 968.604707
	Epoch 8....
Epoch has taken 0:01:50.038643
Number of used sentences in train = 2074
Total loss for epoch 8: 3187.406591
validation loss after epoch 8 : 987.485064
	Epoch 9....
Epoch has taken 0:01:48.929918
Number of used sentences in train = 2074
Total loss for epoch 9: 3180.053830
validation loss after epoch 9 : 1002.987165
	Epoch 10....
Epoch has taken 0:01:51.600733
Number of used sentences in train = 2074
Total loss for epoch 10: 3172.282329
validation loss after epoch 10 : 1014.193142
	Epoch 11....
Epoch has taken 0:01:49.850130
Number of used sentences in train = 2074
Total loss for epoch 11: 3168.923833
validation loss after epoch 11 : 1030.452375
	Epoch 12....
Epoch has taken 0:01:49.941037
Number of used sentences in train = 2074
Total loss for epoch 12: 3164.082720
validation loss after epoch 12 : 1043.862879
	Epoch 13....
Epoch has taken 0:01:52.008860
Number of used sentences in train = 2074
Total loss for epoch 13: 3161.497264
validation loss after epoch 13 : 1055.046295
	Epoch 14....
Epoch has taken 0:01:52.194968
Number of used sentences in train = 2074
Total loss for epoch 14: 3160.258811
validation loss after epoch 14 : 1069.420558
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1680, 179)
  (lstm): LSTM(194, 52, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.930107
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1247.615984
	Epoch 1....
Epoch has taken 0:00:11.463653
Number of used sentences in train = 231
Total loss for epoch 1: 534.706831
	Epoch 2....
Epoch has taken 0:00:11.059863
Number of used sentences in train = 231
Total loss for epoch 2: 386.274405
	Epoch 3....
Epoch has taken 0:00:11.081464
Number of used sentences in train = 231
Total loss for epoch 3: 356.687291
	Epoch 4....
Epoch has taken 0:00:11.070220
Number of used sentences in train = 231
Total loss for epoch 4: 351.175062
	Epoch 5....
Epoch has taken 0:00:11.072733
Number of used sentences in train = 231
Total loss for epoch 5: 348.616289
	Epoch 6....
Epoch has taken 0:00:11.071764
Number of used sentences in train = 231
Total loss for epoch 6: 347.440273
	Epoch 7....
Epoch has taken 0:00:11.068992
Number of used sentences in train = 231
Total loss for epoch 7: 346.858413
	Epoch 8....
Epoch has taken 0:00:11.069578
Number of used sentences in train = 231
Total loss for epoch 8: 346.338195
	Epoch 9....
Epoch has taken 0:00:11.069979
Number of used sentences in train = 231
Total loss for epoch 9: 345.999710
	Epoch 10....
Epoch has taken 0:00:11.060120
Number of used sentences in train = 231
Total loss for epoch 10: 345.746148
	Epoch 11....
Epoch has taken 0:00:11.064093
Number of used sentences in train = 231
Total loss for epoch 11: 345.544397
	Epoch 12....
Epoch has taken 0:00:11.060564
Number of used sentences in train = 231
Total loss for epoch 12: 345.379556
	Epoch 13....
Epoch has taken 0:00:11.061614
Number of used sentences in train = 231
Total loss for epoch 13: 345.258485
	Epoch 14....
Epoch has taken 0:00:11.065132
Number of used sentences in train = 231
Total loss for epoch 14: 345.150618
Epoch has taken 0:00:11.479110

==================================================================================================
	Training time : 0:30:24.722041
==================================================================================================
	Identification : 0.356

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(3369, 179)
  (lstm): LSTM(194, 52, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14703.424974
validation loss after epoch 0 : 1129.739035
	Epoch 1....
Epoch has taken 0:03:32.813258
Number of used sentences in train = 3226
Total loss for epoch 1: 9105.295130
validation loss after epoch 1 : 1158.244741
	Epoch 2....
Epoch has taken 0:03:33.947928
Number of used sentences in train = 3226
Total loss for epoch 2: 8025.625995
validation loss after epoch 2 : 1182.929447
	Epoch 3....
Epoch has taken 0:03:32.031274
Number of used sentences in train = 3226
Total loss for epoch 3: 7468.963177
validation loss after epoch 3 : 1236.516964
	Epoch 4....
Epoch has taken 0:03:31.952907
Number of used sentences in train = 3226
Total loss for epoch 4: 7111.320192
validation loss after epoch 4 : 1290.696533
	Epoch 5....
Epoch has taken 0:03:32.093189
Number of used sentences in train = 3226
Total loss for epoch 5: 6876.270869
validation loss after epoch 5 : 1441.055167
	Epoch 6....
Epoch has taken 0:03:37.216340
Number of used sentences in train = 3226
Total loss for epoch 6: 6712.012399
validation loss after epoch 6 : 1433.792418
	Epoch 7....
Epoch has taken 0:03:31.827138
Number of used sentences in train = 3226
Total loss for epoch 7: 6621.875432
validation loss after epoch 7 : 1536.620565
	Epoch 8....
Epoch has taken 0:03:31.910306
Number of used sentences in train = 3226
Total loss for epoch 8: 6548.575608
validation loss after epoch 8 : 1603.591023
	Epoch 9....
Epoch has taken 0:03:31.849178
Number of used sentences in train = 3226
Total loss for epoch 9: 6469.869493
validation loss after epoch 9 : 1642.974720
	Epoch 10....
Epoch has taken 0:03:33.891466
Number of used sentences in train = 3226
Total loss for epoch 10: 6404.249841
validation loss after epoch 10 : 1661.134381
	Epoch 11....
Epoch has taken 0:03:31.949152
Number of used sentences in train = 3226
Total loss for epoch 11: 6362.018073
validation loss after epoch 11 : 1688.422464
	Epoch 12....
Epoch has taken 0:03:31.891525
Number of used sentences in train = 3226
Total loss for epoch 12: 6316.940746
validation loss after epoch 12 : 1755.163910
	Epoch 13....
Epoch has taken 0:03:45.634070
Number of used sentences in train = 3226
Total loss for epoch 13: 6302.298863
validation loss after epoch 13 : 1823.713405
	Epoch 14....
Epoch has taken 0:03:31.889617
Number of used sentences in train = 3226
Total loss for epoch 14: 6277.497779
validation loss after epoch 14 : 1806.846295
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(3369, 179)
  (lstm): LSTM(194, 52, bidirectional=True)
  (linear1): Linear(in_features=832, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:43.186552
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1907.863653
	Epoch 1....
Epoch has taken 0:00:20.623352
Number of used sentences in train = 359
Total loss for epoch 1: 948.170462
	Epoch 2....
Epoch has taken 0:00:20.624636
Number of used sentences in train = 359
Total loss for epoch 2: 791.135074
	Epoch 3....
Epoch has taken 0:00:20.628557
Number of used sentences in train = 359
Total loss for epoch 3: 723.155481
	Epoch 4....
Epoch has taken 0:00:20.643547
Number of used sentences in train = 359
Total loss for epoch 4: 698.156871
	Epoch 5....
Epoch has taken 0:00:20.641720
Number of used sentences in train = 359
Total loss for epoch 5: 690.390446
	Epoch 6....
Epoch has taken 0:00:20.632381
Number of used sentences in train = 359
Total loss for epoch 6: 678.447016
	Epoch 7....
Epoch has taken 0:00:20.629240
Number of used sentences in train = 359
Total loss for epoch 7: 676.241218
	Epoch 8....
Epoch has taken 0:00:20.642705
Number of used sentences in train = 359
Total loss for epoch 8: 674.477070
	Epoch 9....
Epoch has taken 0:00:20.628743
Number of used sentences in train = 359
Total loss for epoch 9: 673.479611
	Epoch 10....
Epoch has taken 0:00:20.644598
Number of used sentences in train = 359
Total loss for epoch 10: 671.710206
	Epoch 11....
Epoch has taken 0:00:20.890751
Number of used sentences in train = 359
Total loss for epoch 11: 671.312545
	Epoch 12....
Epoch has taken 0:00:20.628984
Number of used sentences in train = 359
Total loss for epoch 12: 671.038153
	Epoch 13....
Epoch has taken 0:00:20.656311
Number of used sentences in train = 359
Total loss for epoch 13: 670.899119
	Epoch 14....
Epoch has taken 0:00:20.634189
Number of used sentences in train = 359
Total loss for epoch 14: 670.698286
Epoch has taken 0:00:20.657276

==================================================================================================
	Training time : 0:58:44.558451
==================================================================================================
	Identification : 0.38

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 72, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 15, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 119, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 83, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1177, 83)
  (lstm): LSTM(98, 119, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12177.538337
validation loss after epoch 0 : 966.270647
	Epoch 1....
Epoch has taken 0:02:40.216555
Number of used sentences in train = 2811
Total loss for epoch 1: 8135.986665
validation loss after epoch 1 : 894.360056
	Epoch 2....
Epoch has taken 0:02:40.470526
Number of used sentences in train = 2811
Total loss for epoch 2: 7141.283604
validation loss after epoch 2 : 900.406191
	Epoch 3....
Epoch has taken 0:02:40.251033
Number of used sentences in train = 2811
Total loss for epoch 3: 6463.634026
validation loss after epoch 3 : 901.571861
	Epoch 4....
Epoch has taken 0:02:42.575725
Number of used sentences in train = 2811
Total loss for epoch 4: 5959.308917
validation loss after epoch 4 : 910.437786
	Epoch 5....
Epoch has taken 0:02:40.265991
Number of used sentences in train = 2811
Total loss for epoch 5: 5554.406685
validation loss after epoch 5 : 922.679644
	Epoch 6....
Epoch has taken 0:02:45.078670
Number of used sentences in train = 2811
Total loss for epoch 6: 5288.789670
validation loss after epoch 6 : 974.893252
	Epoch 7....
Epoch has taken 0:02:40.247394
Number of used sentences in train = 2811
Total loss for epoch 7: 5102.036240
validation loss after epoch 7 : 973.270102
	Epoch 8....
Epoch has taken 0:02:42.555047
Number of used sentences in train = 2811
Total loss for epoch 8: 4944.157591
validation loss after epoch 8 : 1036.140354
	Epoch 9....
Epoch has taken 0:02:57.116590
Number of used sentences in train = 2811
Total loss for epoch 9: 4849.296170
validation loss after epoch 9 : 1046.549775
	Epoch 10....
Epoch has taken 0:03:01.692181
Number of used sentences in train = 2811
Total loss for epoch 10: 4788.791383
validation loss after epoch 10 : 1068.267977
	Epoch 11....
Epoch has taken 0:02:59.205639
Number of used sentences in train = 2811
Total loss for epoch 11: 4736.933816
validation loss after epoch 11 : 1053.896834
	Epoch 12....
Epoch has taken 0:03:01.823705
Number of used sentences in train = 2811
Total loss for epoch 12: 4677.163948
validation loss after epoch 12 : 1094.295514
	Epoch 13....
Epoch has taken 0:03:00.671295
Number of used sentences in train = 2811
Total loss for epoch 13: 4662.997968
validation loss after epoch 13 : 1096.480508
	Epoch 14....
Epoch has taken 0:03:01.461750
Number of used sentences in train = 2811
Total loss for epoch 14: 4617.244186
validation loss after epoch 14 : 1127.456530
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1177, 83)
  (lstm): LSTM(98, 119, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:01.820618
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1503.552702
	Epoch 1....
Epoch has taken 0:00:19.346924
Number of used sentences in train = 313
Total loss for epoch 1: 850.489730
	Epoch 2....
Epoch has taken 0:00:19.103224
Number of used sentences in train = 313
Total loss for epoch 2: 670.908745
	Epoch 3....
Epoch has taken 0:00:19.373315
Number of used sentences in train = 313
Total loss for epoch 3: 607.738694
	Epoch 4....
Epoch has taken 0:00:19.355003
Number of used sentences in train = 313
Total loss for epoch 4: 565.552346
	Epoch 5....
Epoch has taken 0:00:19.356750
Number of used sentences in train = 313
Total loss for epoch 5: 550.064878
	Epoch 6....
Epoch has taken 0:00:19.366106
Number of used sentences in train = 313
Total loss for epoch 6: 547.568816
	Epoch 7....
Epoch has taken 0:00:19.351460
Number of used sentences in train = 313
Total loss for epoch 7: 537.628689
	Epoch 8....
Epoch has taken 0:00:19.357945
Number of used sentences in train = 313
Total loss for epoch 8: 532.263697
	Epoch 9....
Epoch has taken 0:00:19.344706
Number of used sentences in train = 313
Total loss for epoch 9: 533.556583
	Epoch 10....
Epoch has taken 0:00:19.353628
Number of used sentences in train = 313
Total loss for epoch 10: 529.450522
	Epoch 11....
Epoch has taken 0:00:18.430409
Number of used sentences in train = 313
Total loss for epoch 11: 526.656101
	Epoch 12....
Epoch has taken 0:00:17.497015
Number of used sentences in train = 313
Total loss for epoch 12: 526.175373
	Epoch 13....
Epoch has taken 0:00:17.494440
Number of used sentences in train = 313
Total loss for epoch 13: 517.296911
	Epoch 14....
Epoch has taken 0:00:17.417568
Number of used sentences in train = 313
Total loss for epoch 14: 512.836239
Epoch has taken 0:00:17.181094

==================================================================================================
	Training time : 0:47:17.277314
==================================================================================================
	Identification : 0.421

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1133, 83)
  (lstm): LSTM(98, 119, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8895.501208
validation loss after epoch 0 : 752.123552
	Epoch 1....
Epoch has taken 0:01:52.512840
Number of used sentences in train = 2074
Total loss for epoch 1: 5313.200995
validation loss after epoch 1 : 643.644816
	Epoch 2....
Epoch has taken 0:01:52.971878
Number of used sentences in train = 2074
Total loss for epoch 2: 4456.758699
validation loss after epoch 2 : 674.265747
	Epoch 3....
Epoch has taken 0:01:58.366608
Number of used sentences in train = 2074
Total loss for epoch 3: 3977.591769
validation loss after epoch 3 : 654.386247
	Epoch 4....
Epoch has taken 0:01:53.044891
Number of used sentences in train = 2074
Total loss for epoch 4: 3595.181065
validation loss after epoch 4 : 697.304812
	Epoch 5....
Epoch has taken 0:02:04.170683
Number of used sentences in train = 2074
Total loss for epoch 5: 3409.175449
validation loss after epoch 5 : 719.282260
	Epoch 6....
Epoch has taken 0:01:52.960070
Number of used sentences in train = 2074
Total loss for epoch 6: 3290.902082
validation loss after epoch 6 : 753.233439
	Epoch 7....
Epoch has taken 0:02:00.073553
Number of used sentences in train = 2074
Total loss for epoch 7: 3242.921904
validation loss after epoch 7 : 792.726473
	Epoch 8....
Epoch has taken 0:01:59.638006
Number of used sentences in train = 2074
Total loss for epoch 8: 3206.268189
validation loss after epoch 8 : 792.774325
	Epoch 9....
Epoch has taken 0:01:57.926093
Number of used sentences in train = 2074
Total loss for epoch 9: 3187.290145
validation loss after epoch 9 : 814.020563
	Epoch 10....
Epoch has taken 0:01:53.056987
Number of used sentences in train = 2074
Total loss for epoch 10: 3181.841110
validation loss after epoch 10 : 815.027434
	Epoch 11....
Epoch has taken 0:01:52.981578
Number of used sentences in train = 2074
Total loss for epoch 11: 3177.665207
validation loss after epoch 11 : 824.483256
	Epoch 12....
Epoch has taken 0:01:58.872181
Number of used sentences in train = 2074
Total loss for epoch 12: 3174.715935
validation loss after epoch 12 : 829.634719
	Epoch 13....
Epoch has taken 0:01:53.027552
Number of used sentences in train = 2074
Total loss for epoch 13: 3172.990501
validation loss after epoch 13 : 843.857849
	Epoch 14....
Epoch has taken 0:01:51.742761
Number of used sentences in train = 2074
Total loss for epoch 14: 3171.792754
validation loss after epoch 14 : 845.068508
	TransitionClassifier(
  (p_embeddings): Embedding(18, 15)
  (w_embeddings): Embedding(1133, 83)
  (lstm): LSTM(98, 119, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.642823
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1167.524512
	Epoch 1....
Epoch has taken 0:00:11.556683
Number of used sentences in train = 231
Total loss for epoch 1: 570.015452
	Epoch 2....
Epoch has taken 0:00:11.557566
Number of used sentences in train = 231
Total loss for epoch 2: 434.792682
	Epoch 3....
Epoch has taken 0:00:11.559690
Number of used sentences in train = 231
Total loss for epoch 3: 384.307207
	Epoch 4....
Epoch has taken 0:00:11.563079
Number of used sentences in train = 231
Total loss for epoch 4: 358.050541
	Epoch 5....
Epoch has taken 0:00:11.558926
Number of used sentences in train = 231
Total loss for epoch 5: 352.080926
	Epoch 6....
Epoch has taken 0:00:11.548770
Number of used sentences in train = 231
Total loss for epoch 6: 348.816378
	Epoch 7....
Epoch has taken 0:00:11.558143
Number of used sentences in train = 231
Total loss for epoch 7: 347.385567
	Epoch 8....
Epoch has taken 0:00:11.566318
Number of used sentences in train = 231
Total loss for epoch 8: 346.802967
	Epoch 9....
Epoch has taken 0:00:11.553815
Number of used sentences in train = 231
Total loss for epoch 9: 346.359759
	Epoch 10....
Epoch has taken 0:00:11.558537
Number of used sentences in train = 231
Total loss for epoch 10: 346.085239
	Epoch 11....
Epoch has taken 0:00:11.565558
Number of used sentences in train = 231
Total loss for epoch 11: 345.862473
	Epoch 12....
Epoch has taken 0:00:11.559987
Number of used sentences in train = 231
Total loss for epoch 12: 345.649925
	Epoch 13....
Epoch has taken 0:00:11.560640
Number of used sentences in train = 231
Total loss for epoch 13: 345.492275
	Epoch 14....
Epoch has taken 0:00:11.557056
Number of used sentences in train = 231
Total loss for epoch 14: 345.340900
Epoch has taken 0:00:11.923069

==================================================================================================
	Training time : 0:31:47.067428
==================================================================================================
	Identification : 0.335

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(1202, 83)
  (lstm): LSTM(98, 119, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15202.428339
validation loss after epoch 0 : 1344.352413
	Epoch 1....
Epoch has taken 0:03:41.585164
Number of used sentences in train = 3226
Total loss for epoch 1: 11417.436157
validation loss after epoch 1 : 1276.653225
	Epoch 2....
Epoch has taken 0:03:39.486636
Number of used sentences in train = 3226
Total loss for epoch 2: 10283.645570
validation loss after epoch 2 : 1314.306982
	Epoch 3....
Epoch has taken 0:03:41.586060
Number of used sentences in train = 3226
Total loss for epoch 3: 9533.587963
validation loss after epoch 3 : 1313.071590
	Epoch 4....
Epoch has taken 0:03:41.105026
Number of used sentences in train = 3226
Total loss for epoch 4: 8905.892324
validation loss after epoch 4 : 1319.390975
	Epoch 5....
Epoch has taken 0:03:39.587808
Number of used sentences in train = 3226
Total loss for epoch 5: 8466.108134
validation loss after epoch 5 : 1412.447850
	Epoch 6....
Epoch has taken 0:03:38.951608
Number of used sentences in train = 3226
Total loss for epoch 6: 8047.111893
validation loss after epoch 6 : 1421.511727
	Epoch 7....
Epoch has taken 0:03:39.078455
Number of used sentences in train = 3226
Total loss for epoch 7: 7688.484148
validation loss after epoch 7 : 1470.873878
	Epoch 8....
Epoch has taken 0:03:39.368987
Number of used sentences in train = 3226
Total loss for epoch 8: 7400.899680
validation loss after epoch 8 : 1551.773797
	Epoch 9....
Epoch has taken 0:03:40.124577
Number of used sentences in train = 3226
Total loss for epoch 9: 7163.832705
validation loss after epoch 9 : 1613.322461
	Epoch 10....
Epoch has taken 0:03:41.546531
Number of used sentences in train = 3226
Total loss for epoch 10: 6942.131290
validation loss after epoch 10 : 1705.865381
	Epoch 11....
Epoch has taken 0:03:38.917588
Number of used sentences in train = 3226
Total loss for epoch 11: 6787.391535
validation loss after epoch 11 : 1728.390974
	Epoch 12....
Epoch has taken 0:03:39.150258
Number of used sentences in train = 3226
Total loss for epoch 12: 6657.634011
validation loss after epoch 12 : 1833.097297
	Epoch 13....
Epoch has taken 0:03:39.765446
Number of used sentences in train = 3226
Total loss for epoch 13: 6515.548216
validation loss after epoch 13 : 1833.366110
	Epoch 14....
Epoch has taken 0:03:41.714691
Number of used sentences in train = 3226
Total loss for epoch 14: 6438.010531
validation loss after epoch 14 : 1878.924507
	TransitionClassifier(
  (p_embeddings): Embedding(13, 15)
  (w_embeddings): Embedding(1202, 83)
  (lstm): LSTM(98, 119, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=72, bias=True)
  (linear2): Linear(in_features=72, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:40.288394
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2082.908940
	Epoch 1....
Epoch has taken 0:00:21.663642
Number of used sentences in train = 359
Total loss for epoch 1: 1142.552878
	Epoch 2....
Epoch has taken 0:00:21.698537
Number of used sentences in train = 359
Total loss for epoch 2: 965.204552
	Epoch 3....
Epoch has taken 0:00:23.634714
Number of used sentences in train = 359
Total loss for epoch 3: 856.663615
	Epoch 4....
Epoch has taken 0:00:24.011760
Number of used sentences in train = 359
Total loss for epoch 4: 791.478410
	Epoch 5....
Epoch has taken 0:00:22.066341
Number of used sentences in train = 359
Total loss for epoch 5: 760.265619
	Epoch 6....
Epoch has taken 0:00:21.667690
Number of used sentences in train = 359
Total loss for epoch 6: 724.786341
	Epoch 7....
Epoch has taken 0:00:21.696239
Number of used sentences in train = 359
Total loss for epoch 7: 705.883289
	Epoch 8....
Epoch has taken 0:00:21.665993
Number of used sentences in train = 359
Total loss for epoch 8: 684.392685
	Epoch 9....
Epoch has taken 0:00:21.686430
Number of used sentences in train = 359
Total loss for epoch 9: 679.166859
	Epoch 10....
Epoch has taken 0:00:21.661938
Number of used sentences in train = 359
Total loss for epoch 10: 676.598886
	Epoch 11....
Epoch has taken 0:00:21.462784
Number of used sentences in train = 359
Total loss for epoch 11: 673.575513
	Epoch 12....
Epoch has taken 0:00:21.690252
Number of used sentences in train = 359
Total loss for epoch 12: 672.639584
	Epoch 13....
Epoch has taken 0:00:21.668154
Number of used sentences in train = 359
Total loss for epoch 13: 672.285864
	Epoch 14....
Epoch has taken 0:00:21.654054
Number of used sentences in train = 359
Total loss for epoch 14: 671.832920
Epoch has taken 0:00:21.649874

==================================================================================================
	Training time : 1:00:32.480571
==================================================================================================
	Identification : 0.376

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 57, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 20, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 25, 'lstmDropout': 0.39, 'denseActivation': 'tanh', 'wordDim': 141, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(5958, 141)
  (lstm): LSTM(161, 25, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13797.438200
validation loss after epoch 0 : 1153.115939
	Epoch 1....
Epoch has taken 0:03:09.405455
Number of used sentences in train = 2811
Total loss for epoch 1: 9285.623053
validation loss after epoch 1 : 1099.911289
	Epoch 2....
Epoch has taken 0:03:10.424629
Number of used sentences in train = 2811
Total loss for epoch 2: 7836.379363
validation loss after epoch 2 : 1107.500895
	Epoch 3....
Epoch has taken 0:02:57.110997
Number of used sentences in train = 2811
Total loss for epoch 3: 7066.214242
validation loss after epoch 3 : 1156.288206
	Epoch 4....
Epoch has taken 0:02:56.000519
Number of used sentences in train = 2811
Total loss for epoch 4: 6614.728159
validation loss after epoch 4 : 1191.663849
	Epoch 5....
Epoch has taken 0:02:53.996347
Number of used sentences in train = 2811
Total loss for epoch 5: 6310.143640
validation loss after epoch 5 : 1166.846898
	Epoch 6....
Epoch has taken 0:02:56.113012
Number of used sentences in train = 2811
Total loss for epoch 6: 6005.337829
validation loss after epoch 6 : 1205.159430
	Epoch 7....
Epoch has taken 0:02:55.158479
Number of used sentences in train = 2811
Total loss for epoch 7: 5784.645062
validation loss after epoch 7 : 1231.142601
	Epoch 8....
Epoch has taken 0:02:55.945901
Number of used sentences in train = 2811
Total loss for epoch 8: 5631.438429
validation loss after epoch 8 : 1211.524961
	Epoch 9....
Epoch has taken 0:02:53.709518
Number of used sentences in train = 2811
Total loss for epoch 9: 5532.749554
validation loss after epoch 9 : 1370.144744
	Epoch 10....
Epoch has taken 0:02:55.922465
Number of used sentences in train = 2811
Total loss for epoch 10: 5356.732213
validation loss after epoch 10 : 1237.439100
	Epoch 11....
Epoch has taken 0:02:55.952026
Number of used sentences in train = 2811
Total loss for epoch 11: 5311.142385
validation loss after epoch 11 : 1388.281334
	Epoch 12....
Epoch has taken 0:02:56.023486
Number of used sentences in train = 2811
Total loss for epoch 12: 5246.366491
validation loss after epoch 12 : 1325.179847
	Epoch 13....
Epoch has taken 0:02:56.044611
Number of used sentences in train = 2811
Total loss for epoch 13: 5174.543503
validation loss after epoch 13 : 1341.001096
	Epoch 14....
Epoch has taken 0:02:56.031259
Number of used sentences in train = 2811
Total loss for epoch 14: 5068.064965
validation loss after epoch 14 : 1392.251169
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(5958, 141)
  (lstm): LSTM(161, 25, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:54.569822
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1856.880995
	Epoch 1....
Epoch has taken 0:00:18.321929
Number of used sentences in train = 313
Total loss for epoch 1: 897.465131
	Epoch 2....
Epoch has taken 0:00:18.624914
Number of used sentences in train = 313
Total loss for epoch 2: 703.223446
	Epoch 3....
Epoch has taken 0:00:18.625142
Number of used sentences in train = 313
Total loss for epoch 3: 643.815340
	Epoch 4....
Epoch has taken 0:00:18.620847
Number of used sentences in train = 313
Total loss for epoch 4: 614.086182
	Epoch 5....
Epoch has taken 0:00:18.619722
Number of used sentences in train = 313
Total loss for epoch 5: 592.075099
	Epoch 6....
Epoch has taken 0:00:18.625119
Number of used sentences in train = 313
Total loss for epoch 6: 575.633045
	Epoch 7....
Epoch has taken 0:00:18.623199
Number of used sentences in train = 313
Total loss for epoch 7: 565.997884
	Epoch 8....
Epoch has taken 0:00:18.626706
Number of used sentences in train = 313
Total loss for epoch 8: 550.931969
	Epoch 9....
Epoch has taken 0:00:18.623824
Number of used sentences in train = 313
Total loss for epoch 9: 554.645089
	Epoch 10....
Epoch has taken 0:00:18.618434
Number of used sentences in train = 313
Total loss for epoch 10: 547.842758
	Epoch 11....
Epoch has taken 0:00:18.608701
Number of used sentences in train = 313
Total loss for epoch 11: 534.418000
	Epoch 12....
Epoch has taken 0:00:18.420661
Number of used sentences in train = 313
Total loss for epoch 12: 540.066643
	Epoch 13....
Epoch has taken 0:00:18.627142
Number of used sentences in train = 313
Total loss for epoch 13: 527.502994
	Epoch 14....
Epoch has taken 0:00:18.596791
Number of used sentences in train = 313
Total loss for epoch 14: 517.056443
Epoch has taken 0:00:18.625543

==================================================================================================
	Training time : 0:49:01.718473
==================================================================================================
	Identification : 0.326

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(5673, 141)
  (lstm): LSTM(161, 25, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11406.835037
validation loss after epoch 0 : 938.763950
	Epoch 1....
Epoch has taken 0:01:59.557707
Number of used sentences in train = 2074
Total loss for epoch 1: 7091.579399
validation loss after epoch 1 : 869.145882
	Epoch 2....
Epoch has taken 0:01:59.628397
Number of used sentences in train = 2074
Total loss for epoch 2: 5871.281599
validation loss after epoch 2 : 828.487236
	Epoch 3....
Epoch has taken 0:01:59.411067
Number of used sentences in train = 2074
Total loss for epoch 3: 5164.167018
validation loss after epoch 3 : 898.949994
	Epoch 4....
Epoch has taken 0:01:59.710409
Number of used sentences in train = 2074
Total loss for epoch 4: 4733.935595
validation loss after epoch 4 : 826.162232
	Epoch 5....
Epoch has taken 0:01:59.188474
Number of used sentences in train = 2074
Total loss for epoch 5: 4491.680480
validation loss after epoch 5 : 957.955631
	Epoch 6....
Epoch has taken 0:01:59.680094
Number of used sentences in train = 2074
Total loss for epoch 6: 4197.817910
validation loss after epoch 6 : 839.618584
	Epoch 7....
Epoch has taken 0:01:59.665724
Number of used sentences in train = 2074
Total loss for epoch 7: 4047.340646
validation loss after epoch 7 : 931.003485
	Epoch 8....
Epoch has taken 0:01:58.873222
Number of used sentences in train = 2074
Total loss for epoch 8: 3952.821221
validation loss after epoch 8 : 951.828598
	Epoch 9....
Epoch has taken 0:01:59.821110
Number of used sentences in train = 2074
Total loss for epoch 9: 3875.674659
validation loss after epoch 9 : 983.156831
	Epoch 10....
Epoch has taken 0:01:59.767566
Number of used sentences in train = 2074
Total loss for epoch 10: 3769.066126
validation loss after epoch 10 : 981.146748
	Epoch 11....
Epoch has taken 0:01:59.361673
Number of used sentences in train = 2074
Total loss for epoch 11: 3705.014879
validation loss after epoch 11 : 1049.563494
	Epoch 12....
Epoch has taken 0:01:59.243761
Number of used sentences in train = 2074
Total loss for epoch 12: 3643.118657
validation loss after epoch 12 : 1026.760054
	Epoch 13....
Epoch has taken 0:01:59.262250
Number of used sentences in train = 2074
Total loss for epoch 13: 3614.353471
validation loss after epoch 13 : 980.730752
	Epoch 14....
Epoch has taken 0:01:59.541635
Number of used sentences in train = 2074
Total loss for epoch 14: 3517.745610
validation loss after epoch 14 : 1011.882540
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(5673, 141)
  (lstm): LSTM(161, 25, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:12.376738
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1430.801266
	Epoch 1....
Epoch has taken 0:00:12.203586
Number of used sentences in train = 231
Total loss for epoch 1: 678.208191
	Epoch 2....
Epoch has taken 0:00:12.025201
Number of used sentences in train = 231
Total loss for epoch 2: 519.813862
	Epoch 3....
Epoch has taken 0:00:11.809900
Number of used sentences in train = 231
Total loss for epoch 3: 481.498601
	Epoch 4....
Epoch has taken 0:00:11.822807
Number of used sentences in train = 231
Total loss for epoch 4: 433.956134
	Epoch 5....
Epoch has taken 0:00:11.812374
Number of used sentences in train = 231
Total loss for epoch 5: 422.579314
	Epoch 6....
Epoch has taken 0:00:11.813301
Number of used sentences in train = 231
Total loss for epoch 6: 399.167106
	Epoch 7....
Epoch has taken 0:00:11.807902
Number of used sentences in train = 231
Total loss for epoch 7: 386.452559
	Epoch 8....
Epoch has taken 0:00:11.807703
Number of used sentences in train = 231
Total loss for epoch 8: 369.700570
	Epoch 9....
Epoch has taken 0:00:11.809303
Number of used sentences in train = 231
Total loss for epoch 9: 374.757673
	Epoch 10....
Epoch has taken 0:00:11.810499
Number of used sentences in train = 231
Total loss for epoch 10: 383.007731
	Epoch 11....
Epoch has taken 0:00:11.811098
Number of used sentences in train = 231
Total loss for epoch 11: 360.639751
	Epoch 12....
Epoch has taken 0:00:11.808831
Number of used sentences in train = 231
Total loss for epoch 12: 367.137886
	Epoch 13....
Epoch has taken 0:00:11.804491
Number of used sentences in train = 231
Total loss for epoch 13: 361.289779
	Epoch 14....
Epoch has taken 0:00:11.811700
Number of used sentences in train = 231
Total loss for epoch 14: 360.944114
Epoch has taken 0:00:11.809176

==================================================================================================
	Training time : 0:33:03.193272
==================================================================================================
	Identification : 0.279

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 20)
  (w_embeddings): Embedding(6828, 141)
  (lstm): LSTM(161, 25, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18404.966161
validation loss after epoch 0 : 1543.722944
	Epoch 1....
Epoch has taken 0:03:46.318537
Number of used sentences in train = 3226
Total loss for epoch 1: 12362.745375
validation loss after epoch 1 : 1498.956177
	Epoch 2....
Epoch has taken 0:03:50.172595
Number of used sentences in train = 3226
Total loss for epoch 2: 10738.001593
validation loss after epoch 2 : 1528.939303
	Epoch 3....
Epoch has taken 0:03:46.482106
Number of used sentences in train = 3226
Total loss for epoch 3: 9825.246204
validation loss after epoch 3 : 1620.255570
	Epoch 4....
Epoch has taken 0:03:50.258714
Number of used sentences in train = 3226
Total loss for epoch 4: 9235.603501
validation loss after epoch 4 : 1562.545276
	Epoch 5....
Epoch has taken 0:03:46.200018
Number of used sentences in train = 3226
Total loss for epoch 5: 8732.166766
validation loss after epoch 5 : 1625.720987
	Epoch 6....
Epoch has taken 0:03:51.324006
Number of used sentences in train = 3226
Total loss for epoch 6: 8424.209270
validation loss after epoch 6 : 1726.822852
	Epoch 7....
Epoch has taken 0:03:46.703842
Number of used sentences in train = 3226
Total loss for epoch 7: 8058.525337
validation loss after epoch 7 : 1745.056768
	Epoch 8....
Epoch has taken 0:03:48.760856
Number of used sentences in train = 3226
Total loss for epoch 8: 7895.085245
validation loss after epoch 8 : 1737.058067
	Epoch 9....
Epoch has taken 0:03:49.872881
Number of used sentences in train = 3226
Total loss for epoch 9: 7579.818957
validation loss after epoch 9 : 1936.763124
	Epoch 10....
Epoch has taken 0:03:49.742812
Number of used sentences in train = 3226
Total loss for epoch 10: 7507.558855
validation loss after epoch 10 : 1858.936896
	Epoch 11....
Epoch has taken 0:03:50.608219
Number of used sentences in train = 3226
Total loss for epoch 11: 7318.898337
validation loss after epoch 11 : 1840.654693
	Epoch 12....
Epoch has taken 0:03:51.692331
Number of used sentences in train = 3226
Total loss for epoch 12: 7176.420723
validation loss after epoch 12 : 1930.109229
	Epoch 13....
Epoch has taken 0:04:19.775500
Number of used sentences in train = 3226
Total loss for epoch 13: 7136.391901
validation loss after epoch 13 : 1986.968760
	Epoch 14....
Epoch has taken 0:03:46.377041
Number of used sentences in train = 3226
Total loss for epoch 14: 7054.738588
validation loss after epoch 14 : 1992.399790
	TransitionClassifier(
  (p_embeddings): Embedding(13, 20)
  (w_embeddings): Embedding(6828, 141)
  (lstm): LSTM(161, 25, num_layers=2, dropout=0.39, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=57, bias=True)
  (linear2): Linear(in_features=57, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:59.061334
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2178.912881
	Epoch 1....
Epoch has taken 0:00:22.115328
Number of used sentences in train = 359
Total loss for epoch 1: 1194.866951
	Epoch 2....
Epoch has taken 0:00:22.120768
Number of used sentences in train = 359
Total loss for epoch 2: 1002.636844
	Epoch 3....
Epoch has taken 0:00:22.122039
Number of used sentences in train = 359
Total loss for epoch 3: 906.869352
	Epoch 4....
Epoch has taken 0:00:22.131705
Number of used sentences in train = 359
Total loss for epoch 4: 818.430842
	Epoch 5....
Epoch has taken 0:00:22.107959
Number of used sentences in train = 359
Total loss for epoch 5: 794.065894
	Epoch 6....
Epoch has taken 0:00:22.114447
Number of used sentences in train = 359
Total loss for epoch 6: 761.134745
	Epoch 7....
Epoch has taken 0:00:22.121200
Number of used sentences in train = 359
Total loss for epoch 7: 761.438994
	Epoch 8....
Epoch has taken 0:00:22.113388
Number of used sentences in train = 359
Total loss for epoch 8: 749.677721
	Epoch 9....
Epoch has taken 0:00:22.111490
Number of used sentences in train = 359
Total loss for epoch 9: 725.381602
	Epoch 10....
Epoch has taken 0:00:22.106263
Number of used sentences in train = 359
Total loss for epoch 10: 709.619715
	Epoch 11....
Epoch has taken 0:00:22.090495
Number of used sentences in train = 359
Total loss for epoch 11: 717.802709
	Epoch 12....
Epoch has taken 0:00:22.091094
Number of used sentences in train = 359
Total loss for epoch 12: 705.085923
	Epoch 13....
Epoch has taken 0:00:22.092133
Number of used sentences in train = 359
Total loss for epoch 13: 706.119411
	Epoch 14....
Epoch has taken 0:00:22.087146
Number of used sentences in train = 359
Total loss for epoch 14: 724.577433
Epoch has taken 0:00:22.079238

==================================================================================================
	Training time : 1:03:25.605115
==================================================================================================
	Identification : 0.16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 24, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 21, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 56, 'lstmDropout': 0.2, 'denseActivation': 'tanh', 'wordDim': 180, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(1882, 180)
  (lstm): LSTM(201, 56, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10557.240338
validation loss after epoch 0 : 846.989488
	Epoch 1....
Epoch has taken 0:03:10.347228
Number of used sentences in train = 2811
Total loss for epoch 1: 7351.526428
validation loss after epoch 1 : 816.616727
	Epoch 2....
Epoch has taken 0:03:02.079496
Number of used sentences in train = 2811
Total loss for epoch 2: 6491.645192
validation loss after epoch 2 : 785.385882
	Epoch 3....
Epoch has taken 0:03:11.724829
Number of used sentences in train = 2811
Total loss for epoch 3: 5893.723465
validation loss after epoch 3 : 780.921176
	Epoch 4....
Epoch has taken 0:03:13.776737
Number of used sentences in train = 2811
Total loss for epoch 4: 5529.538851
validation loss after epoch 4 : 845.362832
	Epoch 5....
Epoch has taken 0:02:50.399258
Number of used sentences in train = 2811
Total loss for epoch 5: 5251.282864
validation loss after epoch 5 : 874.601363
	Epoch 6....
Epoch has taken 0:02:50.486798
Number of used sentences in train = 2811
Total loss for epoch 6: 5103.220525
validation loss after epoch 6 : 865.964564
	Epoch 7....
Epoch has taken 0:03:15.090014
Number of used sentences in train = 2811
Total loss for epoch 7: 4930.134914
validation loss after epoch 7 : 866.867877
	Epoch 8....
Epoch has taken 0:02:50.480241
Number of used sentences in train = 2811
Total loss for epoch 8: 4892.719540
validation loss after epoch 8 : 896.462910
	Epoch 9....
Epoch has taken 0:02:55.641245
Number of used sentences in train = 2811
Total loss for epoch 9: 4798.518223
validation loss after epoch 9 : 908.605877
	Epoch 10....
Epoch has taken 0:02:53.192628
Number of used sentences in train = 2811
Total loss for epoch 10: 4742.469158
validation loss after epoch 10 : 970.115466
	Epoch 11....
Epoch has taken 0:03:15.686262
Number of used sentences in train = 2811
Total loss for epoch 11: 4709.457948
validation loss after epoch 11 : 945.923733
	Epoch 12....
Epoch has taken 0:02:59.366710
Number of used sentences in train = 2811
Total loss for epoch 12: 4707.876627
validation loss after epoch 12 : 948.801516
	Epoch 13....
Epoch has taken 0:02:50.653978
Number of used sentences in train = 2811
Total loss for epoch 13: 4674.674410
validation loss after epoch 13 : 911.815888
	Epoch 14....
Epoch has taken 0:02:57.235363
Number of used sentences in train = 2811
Total loss for epoch 14: 4621.576206
validation loss after epoch 14 : 944.266363
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(1882, 180)
  (lstm): LSTM(201, 56, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:06.679810
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1128.992151
	Epoch 1....
Epoch has taken 0:00:18.625183
Number of used sentences in train = 313
Total loss for epoch 1: 760.821943
	Epoch 2....
Epoch has taken 0:00:18.626881
Number of used sentences in train = 313
Total loss for epoch 2: 624.171930
	Epoch 3....
Epoch has taken 0:00:18.608365
Number of used sentences in train = 313
Total loss for epoch 3: 581.626595
	Epoch 4....
Epoch has taken 0:00:18.625619
Number of used sentences in train = 313
Total loss for epoch 4: 548.147149
	Epoch 5....
Epoch has taken 0:00:18.002043
Number of used sentences in train = 313
Total loss for epoch 5: 534.268280
	Epoch 6....
Epoch has taken 0:00:17.992481
Number of used sentences in train = 313
Total loss for epoch 6: 524.342922
	Epoch 7....
Epoch has taken 0:00:18.008516
Number of used sentences in train = 313
Total loss for epoch 7: 518.342552
	Epoch 8....
Epoch has taken 0:00:18.006823
Number of used sentences in train = 313
Total loss for epoch 8: 513.985113
	Epoch 9....
Epoch has taken 0:00:17.997982
Number of used sentences in train = 313
Total loss for epoch 9: 506.764882
	Epoch 10....
Epoch has taken 0:00:17.984123
Number of used sentences in train = 313
Total loss for epoch 10: 506.680137
	Epoch 11....
Epoch has taken 0:00:17.996085
Number of used sentences in train = 313
Total loss for epoch 11: 505.470851
	Epoch 12....
Epoch has taken 0:00:17.997825
Number of used sentences in train = 313
Total loss for epoch 12: 503.700100
	Epoch 13....
Epoch has taken 0:00:17.992860
Number of used sentences in train = 313
Total loss for epoch 13: 504.831144
	Epoch 14....
Epoch has taken 0:00:17.985012
Number of used sentences in train = 313
Total loss for epoch 14: 503.945782
Epoch has taken 0:00:18.003936

==================================================================================================
	Training time : 0:49:55.793642
==================================================================================================
	Identification : 0.056

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(1680, 180)
  (lstm): LSTM(201, 56, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9544.851274
validation loss after epoch 0 : 652.686841
	Epoch 1....
Epoch has taken 0:01:56.613652
Number of used sentences in train = 2074
Total loss for epoch 1: 5252.398357
validation loss after epoch 1 : 659.074510
	Epoch 2....
Epoch has taken 0:01:58.078269
Number of used sentences in train = 2074
Total loss for epoch 2: 4315.441271
validation loss after epoch 2 : 645.501669
	Epoch 3....
Epoch has taken 0:01:56.905602
Number of used sentences in train = 2074
Total loss for epoch 3: 3847.183767
validation loss after epoch 3 : 723.909915
	Epoch 4....
Epoch has taken 0:01:58.146485
Number of used sentences in train = 2074
Total loss for epoch 4: 3601.306834
validation loss after epoch 4 : 749.335509
	Epoch 5....
Epoch has taken 0:01:59.104155
Number of used sentences in train = 2074
Total loss for epoch 5: 3436.626623
validation loss after epoch 5 : 763.842206
	Epoch 6....
Epoch has taken 0:01:56.818319
Number of used sentences in train = 2074
Total loss for epoch 6: 3358.265393
validation loss after epoch 6 : 772.824795
	Epoch 7....
Epoch has taken 0:01:56.811299
Number of used sentences in train = 2074
Total loss for epoch 7: 3303.201408
validation loss after epoch 7 : 843.065672
	Epoch 8....
Epoch has taken 0:01:56.864514
Number of used sentences in train = 2074
Total loss for epoch 8: 3265.894568
validation loss after epoch 8 : 858.940001
	Epoch 9....
Epoch has taken 0:01:57.984745
Number of used sentences in train = 2074
Total loss for epoch 9: 3262.430991
validation loss after epoch 9 : 862.828199
	Epoch 10....
Epoch has taken 0:01:56.861144
Number of used sentences in train = 2074
Total loss for epoch 10: 3229.064624
validation loss after epoch 10 : 890.309819
	Epoch 11....
Epoch has taken 0:01:56.864819
Number of used sentences in train = 2074
Total loss for epoch 11: 3223.823473
validation loss after epoch 11 : 905.161387
	Epoch 12....
Epoch has taken 0:01:56.825826
Number of used sentences in train = 2074
Total loss for epoch 12: 3208.199986
validation loss after epoch 12 : 923.816683
	Epoch 13....
Epoch has taken 0:01:56.847394
Number of used sentences in train = 2074
Total loss for epoch 13: 3213.113683
validation loss after epoch 13 : 888.257277
	Epoch 14....
Epoch has taken 0:01:56.875137
Number of used sentences in train = 2074
Total loss for epoch 14: 3203.136443
validation loss after epoch 14 : 906.946640
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(1680, 180)
  (lstm): LSTM(201, 56, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.860164
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1325.747160
	Epoch 1....
Epoch has taken 0:00:11.901236
Number of used sentences in train = 231
Total loss for epoch 1: 546.363628
	Epoch 2....
Epoch has taken 0:00:11.902734
Number of used sentences in train = 231
Total loss for epoch 2: 423.514931
	Epoch 3....
Epoch has taken 0:00:11.906663
Number of used sentences in train = 231
Total loss for epoch 3: 379.098370
	Epoch 4....
Epoch has taken 0:00:11.905790
Number of used sentences in train = 231
Total loss for epoch 4: 366.119872
	Epoch 5....
Epoch has taken 0:00:11.908768
Number of used sentences in train = 231
Total loss for epoch 5: 355.085725
	Epoch 6....
Epoch has taken 0:00:11.916004
Number of used sentences in train = 231
Total loss for epoch 6: 350.669709
	Epoch 7....
Epoch has taken 0:00:11.917175
Number of used sentences in train = 231
Total loss for epoch 7: 350.279705
	Epoch 8....
Epoch has taken 0:00:11.910912
Number of used sentences in train = 231
Total loss for epoch 8: 350.653515
	Epoch 9....
Epoch has taken 0:00:11.909513
Number of used sentences in train = 231
Total loss for epoch 9: 356.634275
	Epoch 10....
Epoch has taken 0:00:11.905672
Number of used sentences in train = 231
Total loss for epoch 10: 347.709367
	Epoch 11....
Epoch has taken 0:00:11.902821
Number of used sentences in train = 231
Total loss for epoch 11: 347.322921
	Epoch 12....
Epoch has taken 0:00:11.894431
Number of used sentences in train = 231
Total loss for epoch 12: 346.826993
	Epoch 13....
Epoch has taken 0:00:11.895249
Number of used sentences in train = 231
Total loss for epoch 13: 346.640202
	Epoch 14....
Epoch has taken 0:00:11.899136
Number of used sentences in train = 231
Total loss for epoch 14: 347.295398
Epoch has taken 0:00:11.891528

==================================================================================================
	Training time : 0:32:17.363355
==================================================================================================
	Identification : 0.148

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 21)
  (w_embeddings): Embedding(3369, 180)
  (lstm): LSTM(201, 56, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 13720.431828
validation loss after epoch 0 : 1133.469966
	Epoch 1....
Epoch has taken 0:03:49.711429
Number of used sentences in train = 3226
Total loss for epoch 1: 9451.955804
validation loss after epoch 1 : 1084.303086
	Epoch 2....
Epoch has taken 0:03:50.456752
Number of used sentences in train = 3226
Total loss for epoch 2: 8591.272721
validation loss after epoch 2 : 1076.132755
	Epoch 3....
Epoch has taken 0:03:50.048792
Number of used sentences in train = 3226
Total loss for epoch 3: 7985.002868
validation loss after epoch 3 : 1125.218604
	Epoch 4....
Epoch has taken 0:03:46.605024
Number of used sentences in train = 3226
Total loss for epoch 4: 7559.187986
validation loss after epoch 4 : 1211.926160
	Epoch 5....
Epoch has taken 0:03:46.419825
Number of used sentences in train = 3226
Total loss for epoch 5: 7266.621856
validation loss after epoch 5 : 1178.159526
	Epoch 6....
Epoch has taken 0:03:46.093108
Number of used sentences in train = 3226
Total loss for epoch 6: 7128.081867
validation loss after epoch 6 : 1260.848866
	Epoch 7....
Epoch has taken 0:03:51.358041
Number of used sentences in train = 3226
Total loss for epoch 7: 6955.413938
validation loss after epoch 7 : 1267.121074
	Epoch 8....
Epoch has taken 0:03:46.401270
Number of used sentences in train = 3226
Total loss for epoch 8: 6817.208262
validation loss after epoch 8 : 1310.488318
	Epoch 9....
Epoch has taken 0:03:51.993378
Number of used sentences in train = 3226
Total loss for epoch 9: 6661.188044
validation loss after epoch 9 : 1394.857901
	Epoch 10....
Epoch has taken 0:03:52.052068
Number of used sentences in train = 3226
Total loss for epoch 10: 6592.526286
validation loss after epoch 10 : 1431.945186
	Epoch 11....
Epoch has taken 0:03:46.723518
Number of used sentences in train = 3226
Total loss for epoch 11: 6556.762294
validation loss after epoch 11 : 1382.543227
	Epoch 12....
Epoch has taken 0:03:46.683484
Number of used sentences in train = 3226
Total loss for epoch 12: 6513.384709
validation loss after epoch 12 : 1463.952592
	Epoch 13....
Epoch has taken 0:04:06.235231
Number of used sentences in train = 3226
Total loss for epoch 13: 6455.673326
validation loss after epoch 13 : 1579.491665
	Epoch 14....
Epoch has taken 0:04:07.480623
Number of used sentences in train = 3226
Total loss for epoch 14: 6414.093295
validation loss after epoch 14 : 1439.095885
	TransitionClassifier(
  (p_embeddings): Embedding(13, 21)
  (w_embeddings): Embedding(3369, 180)
  (lstm): LSTM(201, 56, num_layers=2, dropout=0.2, bidirectional=True)
  (linear1): Linear(in_features=896, out_features=24, bias=True)
  (linear2): Linear(in_features=24, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:58.713009
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1859.342729
	Epoch 1....
Epoch has taken 0:00:22.364669
Number of used sentences in train = 359
Total loss for epoch 1: 975.652423
	Epoch 2....
Epoch has taken 0:00:22.773983
Number of used sentences in train = 359
Total loss for epoch 2: 816.255416
	Epoch 3....
Epoch has taken 0:00:22.108060
Number of used sentences in train = 359
Total loss for epoch 3: 759.327138
	Epoch 4....
Epoch has taken 0:00:22.100203
Number of used sentences in train = 359
Total loss for epoch 4: 726.274992
	Epoch 5....
Epoch has taken 0:00:22.090141
Number of used sentences in train = 359
Total loss for epoch 5: 700.212913
	Epoch 6....
Epoch has taken 0:00:22.657082
Number of used sentences in train = 359
Total loss for epoch 6: 704.256413
	Epoch 7....
Epoch has taken 0:00:22.100652
Number of used sentences in train = 359
Total loss for epoch 7: 689.545213
	Epoch 8....
Epoch has taken 0:00:22.086840
Number of used sentences in train = 359
Total loss for epoch 8: 679.201532
	Epoch 9....
Epoch has taken 0:00:22.089489
Number of used sentences in train = 359
Total loss for epoch 9: 673.897207
	Epoch 10....
Epoch has taken 0:00:22.091997
Number of used sentences in train = 359
Total loss for epoch 10: 688.226073
	Epoch 11....
Epoch has taken 0:00:22.092997
Number of used sentences in train = 359
Total loss for epoch 11: 676.343538
	Epoch 12....
Epoch has taken 0:00:22.082067
Number of used sentences in train = 359
Total loss for epoch 12: 685.562425
	Epoch 13....
Epoch has taken 0:00:22.113152
Number of used sentences in train = 359
Total loss for epoch 13: 674.868905
	Epoch 14....
Epoch has taken 0:00:22.111789
Number of used sentences in train = 359
Total loss for epoch 14: 676.815672
Epoch has taken 0:00:22.084852

==================================================================================================
	Training time : 1:03:30.590131
==================================================================================================
	Identification : 0.199

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 115, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 70, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 58, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 65, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(1177, 65)
  (lstm): LSTM(135, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=115, bias=True)
  (linear2): Linear(in_features=115, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11988.715876
validation loss after epoch 0 : 951.224462
	Epoch 1....
Epoch has taken 0:02:39.538845
Number of used sentences in train = 2811
Total loss for epoch 1: 7924.225809
validation loss after epoch 1 : 896.626968
	Epoch 2....
Epoch has taken 0:02:39.605483
Number of used sentences in train = 2811
Total loss for epoch 2: 6970.488259
validation loss after epoch 2 : 893.301650
	Epoch 3....
Epoch has taken 0:02:39.361173
Number of used sentences in train = 2811
Total loss for epoch 3: 6366.683732
validation loss after epoch 3 : 897.079412
	Epoch 4....
Epoch has taken 0:02:39.183434
Number of used sentences in train = 2811
Total loss for epoch 4: 5961.462248
validation loss after epoch 4 : 915.855258
	Epoch 5....
Epoch has taken 0:02:39.406333
Number of used sentences in train = 2811
Total loss for epoch 5: 5586.049166
validation loss after epoch 5 : 975.490609
	Epoch 6....
Epoch has taken 0:02:44.376571
Number of used sentences in train = 2811
Total loss for epoch 6: 5290.615089
validation loss after epoch 6 : 1001.278171
	Epoch 7....
Epoch has taken 0:03:00.468258
Number of used sentences in train = 2811
Total loss for epoch 7: 5078.443275
validation loss after epoch 7 : 1051.412691
	Epoch 8....
Epoch has taken 0:03:00.172923
Number of used sentences in train = 2811
Total loss for epoch 8: 4889.696681
validation loss after epoch 8 : 1073.613112
	Epoch 9....
Epoch has taken 0:02:45.512488
Number of used sentences in train = 2811
Total loss for epoch 9: 4786.884134
validation loss after epoch 9 : 1103.849444
	Epoch 10....
Epoch has taken 0:02:44.178664
Number of used sentences in train = 2811
Total loss for epoch 10: 4690.265100
validation loss after epoch 10 : 1155.245656
	Epoch 11....
Epoch has taken 0:02:51.297666
Number of used sentences in train = 2811
Total loss for epoch 11: 4624.619116
validation loss after epoch 11 : 1189.742629
	Epoch 12....
Epoch has taken 0:03:00.917907
Number of used sentences in train = 2811
Total loss for epoch 12: 4580.183340
validation loss after epoch 12 : 1204.588299
	Epoch 13....
Epoch has taken 0:02:58.935162
Number of used sentences in train = 2811
Total loss for epoch 13: 4554.961223
validation loss after epoch 13 : 1218.522203
	Epoch 14....
Epoch has taken 0:02:44.569287
Number of used sentences in train = 2811
Total loss for epoch 14: 4531.764172
validation loss after epoch 14 : 1238.663861
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(1177, 65)
  (lstm): LSTM(135, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=115, bias=True)
  (linear2): Linear(in_features=115, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:42.582868
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1462.163414
	Epoch 1....
Epoch has taken 0:00:17.416468
Number of used sentences in train = 313
Total loss for epoch 1: 835.877280
	Epoch 2....
Epoch has taken 0:00:17.409228
Number of used sentences in train = 313
Total loss for epoch 2: 668.037061
	Epoch 3....
Epoch has taken 0:00:17.421562
Number of used sentences in train = 313
Total loss for epoch 3: 574.592660
	Epoch 4....
Epoch has taken 0:00:17.409983
Number of used sentences in train = 313
Total loss for epoch 4: 531.964642
	Epoch 5....
Epoch has taken 0:00:17.188831
Number of used sentences in train = 313
Total loss for epoch 5: 519.531307
	Epoch 6....
Epoch has taken 0:00:17.426716
Number of used sentences in train = 313
Total loss for epoch 6: 515.010263
	Epoch 7....
Epoch has taken 0:00:17.420620
Number of used sentences in train = 313
Total loss for epoch 7: 510.656423
	Epoch 8....
Epoch has taken 0:00:17.423581
Number of used sentences in train = 313
Total loss for epoch 8: 509.858556
	Epoch 9....
Epoch has taken 0:00:17.423521
Number of used sentences in train = 313
Total loss for epoch 9: 507.114301
	Epoch 10....
Epoch has taken 0:00:17.413074
Number of used sentences in train = 313
Total loss for epoch 10: 505.342426
	Epoch 11....
Epoch has taken 0:00:17.419275
Number of used sentences in train = 313
Total loss for epoch 11: 504.884550
	Epoch 12....
Epoch has taken 0:00:17.428210
Number of used sentences in train = 313
Total loss for epoch 12: 503.635184
	Epoch 13....
Epoch has taken 0:00:17.436134
Number of used sentences in train = 313
Total loss for epoch 13: 502.733262
	Epoch 14....
Epoch has taken 0:00:17.416414
Number of used sentences in train = 313
Total loss for epoch 14: 501.939999
Epoch has taken 0:00:17.419783

==================================================================================================
	Training time : 0:46:11.674490
==================================================================================================
	Identification : 0.466

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(1133, 65)
  (lstm): LSTM(135, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=115, bias=True)
  (linear2): Linear(in_features=115, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9162.402202
validation loss after epoch 0 : 907.336319
	Epoch 1....
Epoch has taken 0:01:52.423611
Number of used sentences in train = 2074
Total loss for epoch 1: 5820.207050
validation loss after epoch 1 : 718.538955
	Epoch 2....
Epoch has taken 0:01:52.529930
Number of used sentences in train = 2074
Total loss for epoch 2: 5013.296249
validation loss after epoch 2 : 766.245689
	Epoch 3....
Epoch has taken 0:01:52.512897
Number of used sentences in train = 2074
Total loss for epoch 3: 4531.325986
validation loss after epoch 3 : 685.476868
	Epoch 4....
Epoch has taken 0:01:52.541516
Number of used sentences in train = 2074
Total loss for epoch 4: 4111.749391
validation loss after epoch 4 : 711.769880
	Epoch 5....
Epoch has taken 0:01:52.620588
Number of used sentences in train = 2074
Total loss for epoch 5: 3853.787042
validation loss after epoch 5 : 728.043791
	Epoch 6....
Epoch has taken 0:01:51.129181
Number of used sentences in train = 2074
Total loss for epoch 6: 3640.889258
validation loss after epoch 6 : 741.539117
	Epoch 7....
Epoch has taken 0:01:51.688719
Number of used sentences in train = 2074
Total loss for epoch 7: 3505.852008
validation loss after epoch 7 : 771.605124
	Epoch 8....
Epoch has taken 0:01:52.640742
Number of used sentences in train = 2074
Total loss for epoch 8: 3367.858835
validation loss after epoch 8 : 751.092220
	Epoch 9....
Epoch has taken 0:01:52.662895
Number of used sentences in train = 2074
Total loss for epoch 9: 3297.945261
validation loss after epoch 9 : 791.382721
	Epoch 10....
Epoch has taken 0:01:52.116546
Number of used sentences in train = 2074
Total loss for epoch 10: 3234.572247
validation loss after epoch 10 : 819.966506
	Epoch 11....
Epoch has taken 0:01:52.486753
Number of used sentences in train = 2074
Total loss for epoch 11: 3202.588094
validation loss after epoch 11 : 826.858477
	Epoch 12....
Epoch has taken 0:01:52.535468
Number of used sentences in train = 2074
Total loss for epoch 12: 3188.585911
validation loss after epoch 12 : 847.209243
	Epoch 13....
Epoch has taken 0:01:52.569174
Number of used sentences in train = 2074
Total loss for epoch 13: 3182.132622
validation loss after epoch 13 : 856.125697
	Epoch 14....
Epoch has taken 0:01:59.178538
Number of used sentences in train = 2074
Total loss for epoch 14: 3174.309489
validation loss after epoch 14 : 865.456143
	TransitionClassifier(
  (p_embeddings): Embedding(18, 70)
  (w_embeddings): Embedding(1133, 65)
  (lstm): LSTM(135, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=115, bias=True)
  (linear2): Linear(in_features=115, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:58.402128
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2299.902839
	Epoch 1....
Epoch has taken 0:00:11.468177
Number of used sentences in train = 231
Total loss for epoch 1: 688.712936
	Epoch 2....
Epoch has taken 0:00:11.274475
Number of used sentences in train = 231
Total loss for epoch 2: 578.143751
	Epoch 3....
Epoch has taken 0:00:11.255343
Number of used sentences in train = 231
Total loss for epoch 3: 482.336327
	Epoch 4....
Epoch has taken 0:00:11.473790
Number of used sentences in train = 231
Total loss for epoch 4: 404.974919
	Epoch 5....
Epoch has taken 0:00:11.464568
Number of used sentences in train = 231
Total loss for epoch 5: 375.333473
	Epoch 6....
Epoch has taken 0:00:11.441790
Number of used sentences in train = 231
Total loss for epoch 6: 362.443280
	Epoch 7....
Epoch has taken 0:00:11.446134
Number of used sentences in train = 231
Total loss for epoch 7: 354.589245
	Epoch 8....
Epoch has taken 0:00:11.470478
Number of used sentences in train = 231
Total loss for epoch 8: 351.383900
	Epoch 9....
Epoch has taken 0:00:11.461275
Number of used sentences in train = 231
Total loss for epoch 9: 349.073691
	Epoch 10....
Epoch has taken 0:00:11.460755
Number of used sentences in train = 231
Total loss for epoch 10: 348.583654
	Epoch 11....
Epoch has taken 0:00:11.429856
Number of used sentences in train = 231
Total loss for epoch 11: 347.802896
	Epoch 12....
Epoch has taken 0:00:11.456254
Number of used sentences in train = 231
Total loss for epoch 12: 347.338922
	Epoch 13....
Epoch has taken 0:00:11.471596
Number of used sentences in train = 231
Total loss for epoch 13: 346.850449
	Epoch 14....
Epoch has taken 0:00:11.485999
Number of used sentences in train = 231
Total loss for epoch 14: 346.481936
Epoch has taken 0:00:11.468406

==================================================================================================
	Training time : 0:31:09.898782
==================================================================================================
	Identification : 0.269

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 70)
  (w_embeddings): Embedding(1202, 65)
  (lstm): LSTM(135, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=115, bias=True)
  (linear2): Linear(in_features=115, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 15670.574836
validation loss after epoch 0 : 1351.272525
	Epoch 1....
Epoch has taken 0:03:35.933980
Number of used sentences in train = 3226
Total loss for epoch 1: 11612.602521
validation loss after epoch 1 : 1295.066526
	Epoch 2....
Epoch has taken 0:03:37.562749
Number of used sentences in train = 3226
Total loss for epoch 2: 10624.065872
validation loss after epoch 2 : 1262.175905
	Epoch 3....
Epoch has taken 0:03:31.932412
Number of used sentences in train = 3226
Total loss for epoch 3: 9896.156528
validation loss after epoch 3 : 1268.666352
	Epoch 4....
Epoch has taken 0:03:32.210109
Number of used sentences in train = 3226
Total loss for epoch 4: 9396.952400
validation loss after epoch 4 : 1283.991774
	Epoch 5....
Epoch has taken 0:03:36.236069
Number of used sentences in train = 3226
Total loss for epoch 5: 9005.616677
validation loss after epoch 5 : 1287.412536
	Epoch 6....
Epoch has taken 0:03:31.940413
Number of used sentences in train = 3226
Total loss for epoch 6: 8654.970642
validation loss after epoch 6 : 1333.589557
	Epoch 7....
Epoch has taken 0:03:37.531495
Number of used sentences in train = 3226
Total loss for epoch 7: 8400.627591
validation loss after epoch 7 : 1369.146879
	Epoch 8....
Epoch has taken 0:03:32.086351
Number of used sentences in train = 3226
Total loss for epoch 8: 8134.770446
validation loss after epoch 8 : 1421.869053
	Epoch 9....
Epoch has taken 0:03:32.061820
Number of used sentences in train = 3226
Total loss for epoch 9: 7893.032708
validation loss after epoch 9 : 1447.689972
	Epoch 10....
Epoch has taken 0:03:35.999684
Number of used sentences in train = 3226
Total loss for epoch 10: 7686.445593
validation loss after epoch 10 : 1482.116899
	Epoch 11....
Epoch has taken 0:03:34.521232
Number of used sentences in train = 3226
Total loss for epoch 11: 7523.881648
validation loss after epoch 11 : 1521.955875
	Epoch 12....
Epoch has taken 0:03:32.103681
Number of used sentences in train = 3226
Total loss for epoch 12: 7323.818138
validation loss after epoch 12 : 1602.271409
	Epoch 13....
Epoch has taken 0:03:34.903516
Number of used sentences in train = 3226
Total loss for epoch 13: 7178.823850
validation loss after epoch 13 : 1679.313213
	Epoch 14....
Epoch has taken 0:03:32.494377
Number of used sentences in train = 3226
Total loss for epoch 14: 7028.659450
validation loss after epoch 14 : 1741.038464
	TransitionClassifier(
  (p_embeddings): Embedding(13, 70)
  (w_embeddings): Embedding(1202, 65)
  (lstm): LSTM(135, 58, bidirectional=True)
  (linear1): Linear(in_features=928, out_features=115, bias=True)
  (linear2): Linear(in_features=115, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:36.286705
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2281.657166
	Epoch 1....
Epoch has taken 0:00:20.681820
Number of used sentences in train = 359
Total loss for epoch 1: 1181.908963
	Epoch 2....
Epoch has taken 0:00:20.659072
Number of used sentences in train = 359
Total loss for epoch 2: 977.513245
	Epoch 3....
Epoch has taken 0:00:20.657262
Number of used sentences in train = 359
Total loss for epoch 3: 874.415927
	Epoch 4....
Epoch has taken 0:00:20.658534
Number of used sentences in train = 359
Total loss for epoch 4: 789.414615
	Epoch 5....
Epoch has taken 0:00:20.658190
Number of used sentences in train = 359
Total loss for epoch 5: 743.706580
	Epoch 6....
Epoch has taken 0:00:20.655760
Number of used sentences in train = 359
Total loss for epoch 6: 724.499573
	Epoch 7....
Epoch has taken 0:00:20.641710
Number of used sentences in train = 359
Total loss for epoch 7: 696.023823
	Epoch 8....
Epoch has taken 0:00:20.647027
Number of used sentences in train = 359
Total loss for epoch 8: 685.599090
	Epoch 9....
Epoch has taken 0:00:20.657227
Number of used sentences in train = 359
Total loss for epoch 9: 676.774644
	Epoch 10....
Epoch has taken 0:00:20.660876
Number of used sentences in train = 359
Total loss for epoch 10: 673.909083
	Epoch 11....
Epoch has taken 0:00:20.664156
Number of used sentences in train = 359
Total loss for epoch 11: 672.689918
	Epoch 12....
Epoch has taken 0:00:20.641557
Number of used sentences in train = 359
Total loss for epoch 12: 672.145728
	Epoch 13....
Epoch has taken 0:00:20.650798
Number of used sentences in train = 359
Total loss for epoch 13: 671.720847
	Epoch 14....
Epoch has taken 0:00:21.006993
Number of used sentences in train = 359
Total loss for epoch 14: 671.393387
Epoch has taken 0:00:20.653125

==================================================================================================
	Training time : 0:58:44.635426
==================================================================================================
	Identification : 0.116

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 47, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 20, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 25, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 62, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(1177, 62)
  (lstm): LSTM(82, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10498.195922
validation loss after epoch 0 : 949.589224
	Epoch 1....
Epoch has taken 0:02:40.532394
Number of used sentences in train = 2811
Total loss for epoch 1: 7737.386667
validation loss after epoch 1 : 955.016246
	Epoch 2....
Epoch has taken 0:02:39.336824
Number of used sentences in train = 2811
Total loss for epoch 2: 6951.518389
validation loss after epoch 2 : 916.651480
	Epoch 3....
Epoch has taken 0:02:39.385865
Number of used sentences in train = 2811
Total loss for epoch 3: 6344.067227
validation loss after epoch 3 : 946.902476
	Epoch 4....
Epoch has taken 0:02:43.614852
Number of used sentences in train = 2811
Total loss for epoch 4: 6029.186188
validation loss after epoch 4 : 972.762109
	Epoch 5....
Epoch has taken 0:02:41.524479
Number of used sentences in train = 2811
Total loss for epoch 5: 5703.612678
validation loss after epoch 5 : 993.988011
	Epoch 6....
Epoch has taken 0:02:39.348872
Number of used sentences in train = 2811
Total loss for epoch 6: 5459.926897
validation loss after epoch 6 : 1032.421521
	Epoch 7....
Epoch has taken 0:02:43.569179
Number of used sentences in train = 2811
Total loss for epoch 7: 5281.442836
validation loss after epoch 7 : 1055.406624
	Epoch 8....
Epoch has taken 0:02:39.403179
Number of used sentences in train = 2811
Total loss for epoch 8: 5169.419102
validation loss after epoch 8 : 1080.609097
	Epoch 9....
Epoch has taken 0:02:41.356798
Number of used sentences in train = 2811
Total loss for epoch 9: 5021.458739
validation loss after epoch 9 : 1125.371287
	Epoch 10....
Epoch has taken 0:02:40.754526
Number of used sentences in train = 2811
Total loss for epoch 10: 4923.823277
validation loss after epoch 10 : 1155.482871
	Epoch 11....
Epoch has taken 0:02:46.970314
Number of used sentences in train = 2811
Total loss for epoch 11: 4859.979679
validation loss after epoch 11 : 1189.070589
	Epoch 12....
Epoch has taken 0:02:39.536388
Number of used sentences in train = 2811
Total loss for epoch 12: 4786.258508
validation loss after epoch 12 : 1234.671658
	Epoch 13....
Epoch has taken 0:02:39.389033
Number of used sentences in train = 2811
Total loss for epoch 13: 4735.098327
validation loss after epoch 13 : 1266.736990
	Epoch 14....
Epoch has taken 0:02:39.254287
Number of used sentences in train = 2811
Total loss for epoch 14: 4693.463486
validation loss after epoch 14 : 1287.215391
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(1177, 62)
  (lstm): LSTM(82, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:44.490172
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1267.640826
	Epoch 1....
Epoch has taken 0:00:16.799296
Number of used sentences in train = 313
Total loss for epoch 1: 742.953411
	Epoch 2....
Epoch has taken 0:00:16.790480
Number of used sentences in train = 313
Total loss for epoch 2: 603.572723
	Epoch 3....
Epoch has taken 0:00:16.794880
Number of used sentences in train = 313
Total loss for epoch 3: 548.246917
	Epoch 4....
Epoch has taken 0:00:16.791234
Number of used sentences in train = 313
Total loss for epoch 4: 526.623511
	Epoch 5....
Epoch has taken 0:00:16.810562
Number of used sentences in train = 313
Total loss for epoch 5: 515.814824
	Epoch 6....
Epoch has taken 0:00:16.797884
Number of used sentences in train = 313
Total loss for epoch 6: 511.936487
	Epoch 7....
Epoch has taken 0:00:16.786996
Number of used sentences in train = 313
Total loss for epoch 7: 510.206070
	Epoch 8....
Epoch has taken 0:00:16.802979
Number of used sentences in train = 313
Total loss for epoch 8: 507.985052
	Epoch 9....
Epoch has taken 0:00:16.812444
Number of used sentences in train = 313
Total loss for epoch 9: 507.029969
	Epoch 10....
Epoch has taken 0:00:16.813174
Number of used sentences in train = 313
Total loss for epoch 10: 505.533434
	Epoch 11....
Epoch has taken 0:00:16.805260
Number of used sentences in train = 313
Total loss for epoch 11: 505.677226
	Epoch 12....
Epoch has taken 0:00:16.816148
Number of used sentences in train = 313
Total loss for epoch 12: 504.795339
	Epoch 13....
Epoch has taken 0:00:17.386383
Number of used sentences in train = 313
Total loss for epoch 13: 504.558429
	Epoch 14....
Epoch has taken 0:00:16.813777
Number of used sentences in train = 313
Total loss for epoch 14: 504.293200
Epoch has taken 0:00:16.801821

==================================================================================================
	Training time : 0:44:31.580429
==================================================================================================
	Identification : 0.145

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(1133, 62)
  (lstm): LSTM(82, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8218.308389
validation loss after epoch 0 : 716.944472
	Epoch 1....
Epoch has taken 0:01:48.760166
Number of used sentences in train = 2074
Total loss for epoch 1: 5402.815663
validation loss after epoch 1 : 678.177660
	Epoch 2....
Epoch has taken 0:01:49.900915
Number of used sentences in train = 2074
Total loss for epoch 2: 4651.517716
validation loss after epoch 2 : 671.507628
	Epoch 3....
Epoch has taken 0:01:49.271881
Number of used sentences in train = 2074
Total loss for epoch 3: 4206.346086
validation loss after epoch 3 : 710.199561
	Epoch 4....
Epoch has taken 0:01:48.800818
Number of used sentences in train = 2074
Total loss for epoch 4: 3909.380537
validation loss after epoch 4 : 728.636885
	Epoch 5....
Epoch has taken 0:01:48.864196
Number of used sentences in train = 2074
Total loss for epoch 5: 3658.752662
validation loss after epoch 5 : 746.213221
	Epoch 6....
Epoch has taken 0:01:48.733399
Number of used sentences in train = 2074
Total loss for epoch 6: 3510.184120
validation loss after epoch 6 : 769.082371
	Epoch 7....
Epoch has taken 0:01:48.701966
Number of used sentences in train = 2074
Total loss for epoch 7: 3407.909106
validation loss after epoch 7 : 807.230886
	Epoch 8....
Epoch has taken 0:01:48.729587
Number of used sentences in train = 2074
Total loss for epoch 8: 3324.485732
validation loss after epoch 8 : 811.579756
	Epoch 9....
Epoch has taken 0:01:50.465334
Number of used sentences in train = 2074
Total loss for epoch 9: 3262.998396
validation loss after epoch 9 : 853.319745
	Epoch 10....
Epoch has taken 0:01:48.806480
Number of used sentences in train = 2074
Total loss for epoch 10: 3227.625310
validation loss after epoch 10 : 870.490577
	Epoch 11....
Epoch has taken 0:01:50.331398
Number of used sentences in train = 2074
Total loss for epoch 11: 3210.646476
validation loss after epoch 11 : 901.571338
	Epoch 12....
Epoch has taken 0:01:48.943756
Number of used sentences in train = 2074
Total loss for epoch 12: 3203.041618
validation loss after epoch 12 : 922.505285
	Epoch 13....
Epoch has taken 0:01:48.777017
Number of used sentences in train = 2074
Total loss for epoch 13: 3194.104355
validation loss after epoch 13 : 897.406175
	Epoch 14....
Epoch has taken 0:01:50.980415
Number of used sentences in train = 2074
Total loss for epoch 14: 3186.591663
validation loss after epoch 14 : 926.937598
	TransitionClassifier(
  (p_embeddings): Embedding(18, 20)
  (w_embeddings): Embedding(1133, 62)
  (lstm): LSTM(82, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:48.761262
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1370.798804
	Epoch 1....
Epoch has taken 0:00:11.074551
Number of used sentences in train = 231
Total loss for epoch 1: 552.382166
	Epoch 2....
Epoch has taken 0:00:11.061480
Number of used sentences in train = 231
Total loss for epoch 2: 438.604234
	Epoch 3....
Epoch has taken 0:00:11.067594
Number of used sentences in train = 231
Total loss for epoch 3: 389.187946
	Epoch 4....
Epoch has taken 0:00:11.067841
Number of used sentences in train = 231
Total loss for epoch 4: 365.198691
	Epoch 5....
Epoch has taken 0:00:11.058283
Number of used sentences in train = 231
Total loss for epoch 5: 358.018286
	Epoch 6....
Epoch has taken 0:00:11.059798
Number of used sentences in train = 231
Total loss for epoch 6: 353.988969
	Epoch 7....
Epoch has taken 0:00:11.068077
Number of used sentences in train = 231
Total loss for epoch 7: 351.002082
	Epoch 8....
Epoch has taken 0:00:11.065110
Number of used sentences in train = 231
Total loss for epoch 8: 348.884522
	Epoch 9....
Epoch has taken 0:00:11.077358
Number of used sentences in train = 231
Total loss for epoch 9: 347.897305
	Epoch 10....
Epoch has taken 0:00:11.488657
Number of used sentences in train = 231
Total loss for epoch 10: 347.307848
	Epoch 11....
Epoch has taken 0:00:11.098586
Number of used sentences in train = 231
Total loss for epoch 11: 346.850268
	Epoch 12....
Epoch has taken 0:00:11.100592
Number of used sentences in train = 231
Total loss for epoch 12: 346.480913
	Epoch 13....
Epoch has taken 0:00:11.087173
Number of used sentences in train = 231
Total loss for epoch 13: 346.219015
	Epoch 14....
Epoch has taken 0:00:11.080843
Number of used sentences in train = 231
Total loss for epoch 14: 345.989188
Epoch has taken 0:00:11.088305

==================================================================================================
	Training time : 0:30:05.699850
==================================================================================================
	Identification : 0.08

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 20)
  (w_embeddings): Embedding(1202, 62)
  (lstm): LSTM(82, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14336.721812
validation loss after epoch 0 : 1362.664596
	Epoch 1....
Epoch has taken 0:03:34.491850
Number of used sentences in train = 3226
Total loss for epoch 1: 11384.234112
validation loss after epoch 1 : 1288.509648
	Epoch 2....
Epoch has taken 0:03:47.169110
Number of used sentences in train = 3226
Total loss for epoch 2: 10210.152012
validation loss after epoch 2 : 1278.306886
	Epoch 3....
Epoch has taken 0:03:36.374235
Number of used sentences in train = 3226
Total loss for epoch 3: 9471.614291
validation loss after epoch 3 : 1278.837319
	Epoch 4....
Epoch has taken 0:03:40.154706
Number of used sentences in train = 3226
Total loss for epoch 4: 8903.242616
validation loss after epoch 4 : 1350.220164
	Epoch 5....
Epoch has taken 0:03:40.053231
Number of used sentences in train = 3226
Total loss for epoch 5: 8422.032909
validation loss after epoch 5 : 1367.620275
	Epoch 6....
Epoch has taken 0:03:39.951661
Number of used sentences in train = 3226
Total loss for epoch 6: 8044.240284
validation loss after epoch 6 : 1429.645262
	Epoch 7....
Epoch has taken 0:03:46.373784
Number of used sentences in train = 3226
Total loss for epoch 7: 7772.201519
validation loss after epoch 7 : 1446.960059
	Epoch 8....
Epoch has taken 0:03:40.489194
Number of used sentences in train = 3226
Total loss for epoch 8: 7508.595068
validation loss after epoch 8 : 1570.884914
	Epoch 9....
Epoch has taken 0:03:38.699312
Number of used sentences in train = 3226
Total loss for epoch 9: 7283.311494
validation loss after epoch 9 : 1605.419560
	Epoch 10....
Epoch has taken 0:03:41.061347
Number of used sentences in train = 3226
Total loss for epoch 10: 7122.407275
validation loss after epoch 10 : 1720.421632
	Epoch 11....
Epoch has taken 0:03:40.463989
Number of used sentences in train = 3226
Total loss for epoch 11: 6949.242447
validation loss after epoch 11 : 1759.573034
	Epoch 12....
Epoch has taken 0:03:34.992048
Number of used sentences in train = 3226
Total loss for epoch 12: 6844.342568
validation loss after epoch 12 : 1815.885224
	Epoch 13....
Epoch has taken 0:03:34.821487
Number of used sentences in train = 3226
Total loss for epoch 13: 6721.885898
validation loss after epoch 13 : 1898.913191
	Epoch 14....
Epoch has taken 0:03:34.676950
Number of used sentences in train = 3226
Total loss for epoch 14: 6641.503771
validation loss after epoch 14 : 1968.957067
	TransitionClassifier(
  (p_embeddings): Embedding(13, 20)
  (w_embeddings): Embedding(1202, 62)
  (lstm): LSTM(82, 25, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=47, bias=True)
  (linear2): Linear(in_features=47, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:35.655463
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1701.045224
	Epoch 1....
Epoch has taken 0:00:20.965636
Number of used sentences in train = 359
Total loss for epoch 1: 1102.098810
	Epoch 2....
Epoch has taken 0:00:20.961446
Number of used sentences in train = 359
Total loss for epoch 2: 920.236821
	Epoch 3....
Epoch has taken 0:00:20.967493
Number of used sentences in train = 359
Total loss for epoch 3: 806.574455
	Epoch 4....
Epoch has taken 0:00:20.944255
Number of used sentences in train = 359
Total loss for epoch 4: 742.857727
	Epoch 5....
Epoch has taken 0:00:20.934895
Number of used sentences in train = 359
Total loss for epoch 5: 728.788918
	Epoch 6....
Epoch has taken 0:00:21.034176
Number of used sentences in train = 359
Total loss for epoch 6: 700.613274
	Epoch 7....
Epoch has taken 0:00:20.963032
Number of used sentences in train = 359
Total loss for epoch 7: 687.164024
	Epoch 8....
Epoch has taken 0:00:20.938398
Number of used sentences in train = 359
Total loss for epoch 8: 681.868566
	Epoch 9....
Epoch has taken 0:00:20.934134
Number of used sentences in train = 359
Total loss for epoch 9: 678.620998
	Epoch 10....
Epoch has taken 0:00:20.947888
Number of used sentences in train = 359
Total loss for epoch 10: 674.292757
	Epoch 11....
Epoch has taken 0:00:20.920273
Number of used sentences in train = 359
Total loss for epoch 11: 673.137363
	Epoch 12....
Epoch has taken 0:00:21.221185
Number of used sentences in train = 359
Total loss for epoch 12: 672.554620
	Epoch 13....
Epoch has taken 0:00:20.965052
Number of used sentences in train = 359
Total loss for epoch 13: 672.177752
	Epoch 14....
Epoch has taken 0:00:20.969390
Number of used sentences in train = 359
Total loss for epoch 14: 671.869377
Epoch has taken 0:00:20.971361

==================================================================================================
	Training time : 1:00:00.706955
==================================================================================================
	Identification : 0.08

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 85, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 36, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 69, 'lstmDropout': 0.34, 'denseActivation': 'tanh', 'wordDim': 91, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 36)
  (w_embeddings): Embedding(9250, 91)
  (lstm): LSTM(127, 69, num_layers=2, dropout=0.34, bidirectional=True)
  (linear1): Linear(in_features=1104, out_features=85, bias=True)
  (linear2): Linear(in_features=85, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 19552.129300
validation loss after epoch 0 : 1288.474439
	Epoch 1....
Epoch has taken 0:02:53.126716
Number of used sentences in train = 2811
Total loss for epoch 1: 10953.694088
validation loss after epoch 1 : 1131.609211
	Epoch 2....
Epoch has taken 0:02:55.717328
Number of used sentences in train = 2811
Total loss for epoch 2: 8739.989383
validation loss after epoch 2 : 1075.498296
	Epoch 3....
Epoch has taken 0:02:55.664921
Number of used sentences in train = 2811
Total loss for epoch 3: 7535.118629
validation loss after epoch 3 : 1033.983476
	Epoch 4....
Epoch has taken 0:02:56.567884
Number of used sentences in train = 2811
Total loss for epoch 4: 6614.495474
validation loss after epoch 4 : 1088.796375
	Epoch 5....
Epoch has taken 0:02:56.388221
Number of used sentences in train = 2811
Total loss for epoch 5: 6022.656217
validation loss after epoch 5 : 1178.150374
	Epoch 6....
Epoch has taken 0:02:52.754362
Number of used sentences in train = 2811
Total loss for epoch 6: 5752.102388
validation loss after epoch 6 : 1190.231356
	Epoch 7....
Epoch has taken 0:02:58.129565
Number of used sentences in train = 2811
Total loss for epoch 7: 5449.689228
validation loss after epoch 7 : 1260.222538
	Epoch 8....
Epoch has taken 0:03:10.983637
Number of used sentences in train = 2811
Total loss for epoch 8: 5274.527648
validation loss after epoch 8 : 1252.897979
	Epoch 9....
Epoch has taken 0:03:14.231420
Number of used sentences in train = 2811
Total loss for epoch 9: 5107.622338
validation loss after epoch 9 : 1405.017996
	Epoch 10....
Epoch has taken 0:03:01.149957
Number of used sentences in train = 2811
Total loss for epoch 10: 5030.765740
validation loss after epoch 10 : 1352.632717
	Epoch 11....
Epoch has taken 0:02:56.920736
Number of used sentences in train = 2811
Total loss for epoch 11: 4916.822037
validation loss after epoch 11 : 1350.619628
	Epoch 12....
Epoch has taken 0:02:54.514955
Number of used sentences in train = 2811
Total loss for epoch 12: 4864.349049
validation loss after epoch 12 : 1440.497325
	Epoch 13....
Epoch has taken 0:02:56.733857
Number of used sentences in train = 2811
Total loss for epoch 13: 4747.476874
validation loss after epoch 13 : 1505.286680
	Epoch 14....
Epoch has taken 0:02:57.062225
Number of used sentences in train = 2811
Total loss for epoch 14: 4767.956941
validation loss after epoch 14 : 1381.080782
	TransitionClassifier(
  (p_embeddings): Embedding(18, 36)
  (w_embeddings): Embedding(9250, 91)
  (lstm): LSTM(127, 69, num_layers=2, dropout=0.34, bidirectional=True)
  (linear1): Linear(in_features=1104, out_features=85, bias=True)
  (linear2): Linear(in_features=85, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:56.858554
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1827.927368
	Epoch 1....
Epoch has taken 0:00:18.742741
Number of used sentences in train = 313
Total loss for epoch 1: 863.565928
	Epoch 2....
Epoch has taken 0:00:18.742904
Number of used sentences in train = 313
Total loss for epoch 2: 731.431414
	Epoch 3....
Epoch has taken 0:00:18.722780
Number of used sentences in train = 313
Total loss for epoch 3: 613.804503
	Epoch 4....
Epoch has taken 0:00:18.732578
Number of used sentences in train = 313
Total loss for epoch 4: 584.984929
	Epoch 5....
Epoch has taken 0:00:18.735117
Number of used sentences in train = 313
Total loss for epoch 5: 579.109994
	Epoch 6....
Epoch has taken 0:00:18.719621
Number of used sentences in train = 313
Total loss for epoch 6: 541.847424
	Epoch 7....
Epoch has taken 0:00:18.711484
Number of used sentences in train = 313
Total loss for epoch 7: 536.591571
	Epoch 8....
Epoch has taken 0:00:18.740872
Number of used sentences in train = 313
Total loss for epoch 8: 527.564059
	Epoch 9....
Epoch has taken 0:00:18.714400
Number of used sentences in train = 313
Total loss for epoch 9: 532.048360
	Epoch 10....
Epoch has taken 0:00:18.716681
Number of used sentences in train = 313
Total loss for epoch 10: 524.746722
	Epoch 11....
Epoch has taken 0:00:18.724430
Number of used sentences in train = 313
Total loss for epoch 11: 514.910963
	Epoch 12....
Epoch has taken 0:00:18.715956
Number of used sentences in train = 313
Total loss for epoch 12: 512.763744
	Epoch 13....
Epoch has taken 0:00:18.724580
Number of used sentences in train = 313
Total loss for epoch 13: 513.877163
	Epoch 14....
Epoch has taken 0:00:18.721446
Number of used sentences in train = 313
Total loss for epoch 14: 511.108117
Epoch has taken 0:00:18.742822

==================================================================================================
	Training time : 0:49:18.223046
==================================================================================================
	Identification : 0.422

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 36)
  (w_embeddings): Embedding(7094, 91)
  (lstm): LSTM(127, 69, num_layers=2, dropout=0.34, bidirectional=True)
  (linear1): Linear(in_features=1104, out_features=85, bias=True)
  (linear2): Linear(in_features=85, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 16390.912863
validation loss after epoch 0 : 968.841255
	Epoch 1....
Epoch has taken 0:02:00.971236
Number of used sentences in train = 2074
Total loss for epoch 1: 7948.435103
validation loss after epoch 1 : 949.279993
	Epoch 2....
Epoch has taken 0:01:59.643044
Number of used sentences in train = 2074
Total loss for epoch 2: 6172.188382
validation loss after epoch 2 : 971.257395
	Epoch 3....
Epoch has taken 0:02:00.645908
Number of used sentences in train = 2074
Total loss for epoch 3: 5131.126019
validation loss after epoch 3 : 917.720073
	Epoch 4....
Epoch has taken 0:02:01.126136
Number of used sentences in train = 2074
Total loss for epoch 4: 4407.615296
validation loss after epoch 4 : 1094.536082
	Epoch 5....
Epoch has taken 0:02:01.121521
Number of used sentences in train = 2074
Total loss for epoch 5: 4114.775304
validation loss after epoch 5 : 1053.014618
	Epoch 6....
Epoch has taken 0:01:59.955844
Number of used sentences in train = 2074
Total loss for epoch 6: 3832.067780
validation loss after epoch 6 : 1052.688218
	Epoch 7....
Epoch has taken 0:02:00.965218
Number of used sentences in train = 2074
Total loss for epoch 7: 3744.934974
validation loss after epoch 7 : 1034.059229
	Epoch 8....
Epoch has taken 0:02:00.924875
Number of used sentences in train = 2074
Total loss for epoch 8: 3604.360504
validation loss after epoch 8 : 1051.808319
	Epoch 9....
Epoch has taken 0:02:00.894655
Number of used sentences in train = 2074
Total loss for epoch 9: 3566.259964
validation loss after epoch 9 : 1119.482851
	Epoch 10....
Epoch has taken 0:02:00.932335
Number of used sentences in train = 2074
Total loss for epoch 10: 3455.399766
validation loss after epoch 10 : 1232.429669
	Epoch 11....
Epoch has taken 0:02:00.214011
Number of used sentences in train = 2074
Total loss for epoch 11: 3395.394659
validation loss after epoch 11 : 1225.512948
	Epoch 12....
Epoch has taken 0:02:00.895146
Number of used sentences in train = 2074
Total loss for epoch 12: 3372.602827
validation loss after epoch 12 : 1284.638787
	Epoch 13....
Epoch has taken 0:02:00.236533
Number of used sentences in train = 2074
Total loss for epoch 13: 3313.691718
validation loss after epoch 13 : 1200.332915
	Epoch 14....
Epoch has taken 0:02:00.970128
Number of used sentences in train = 2074
Total loss for epoch 14: 3306.576165
validation loss after epoch 14 : 1345.578797
	TransitionClassifier(
  (p_embeddings): Embedding(18, 36)
  (w_embeddings): Embedding(7094, 91)
  (lstm): LSTM(127, 69, num_layers=2, dropout=0.34, bidirectional=True)
  (linear1): Linear(in_features=1104, out_features=85, bias=True)
  (linear2): Linear(in_features=85, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:59.849826
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1963.142902
	Epoch 1....
Epoch has taken 0:00:12.366994
Number of used sentences in train = 231
Total loss for epoch 1: 770.154583
	Epoch 2....
Epoch has taken 0:00:12.354705
Number of used sentences in train = 231
Total loss for epoch 2: 606.016936
	Epoch 3....
Epoch has taken 0:00:12.364026
Number of used sentences in train = 231
Total loss for epoch 3: 489.566522
	Epoch 4....
Epoch has taken 0:00:12.357095
Number of used sentences in train = 231
Total loss for epoch 4: 419.813986
	Epoch 5....
Epoch has taken 0:00:12.354460
Number of used sentences in train = 231
Total loss for epoch 5: 388.234538
	Epoch 6....
Epoch has taken 0:00:12.350652
Number of used sentences in train = 231
Total loss for epoch 6: 371.909327
	Epoch 7....
Epoch has taken 0:00:12.359008
Number of used sentences in train = 231
Total loss for epoch 7: 368.369014
	Epoch 8....
Epoch has taken 0:00:12.364467
Number of used sentences in train = 231
Total loss for epoch 8: 365.630027
	Epoch 9....
Epoch has taken 0:00:12.335398
Number of used sentences in train = 231
Total loss for epoch 9: 360.638068
	Epoch 10....
Epoch has taken 0:00:12.179153
Number of used sentences in train = 231
Total loss for epoch 10: 362.455975
	Epoch 11....
Epoch has taken 0:00:12.174761
Number of used sentences in train = 231
Total loss for epoch 11: 356.864616
	Epoch 12....
Epoch has taken 0:00:12.167597
Number of used sentences in train = 231
Total loss for epoch 12: 357.917780
	Epoch 13....
Epoch has taken 0:00:12.166314
Number of used sentences in train = 231
Total loss for epoch 13: 353.165412
	Epoch 14....
Epoch has taken 0:00:12.175764
Number of used sentences in train = 231
Total loss for epoch 14: 351.115304
Epoch has taken 0:00:12.172106

==================================================================================================
	Training time : 0:33:13.929681
==================================================================================================
	Identification : 0.109

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 36)
  (w_embeddings): Embedding(17993, 91)
  (lstm): LSTM(127, 69, num_layers=2, dropout=0.34, bidirectional=True)
  (linear1): Linear(in_features=1104, out_features=85, bias=True)
  (linear2): Linear(in_features=85, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 22407.017191
validation loss after epoch 0 : 1785.097303
	Epoch 1....
Epoch has taken 0:03:52.335803
Number of used sentences in train = 3226
Total loss for epoch 1: 13712.126006
validation loss after epoch 1 : 1526.694738
	Epoch 2....
Epoch has taken 0:04:07.227814
Number of used sentences in train = 3226
Total loss for epoch 2: 11262.634804
validation loss after epoch 2 : 1538.612239
	Epoch 3....
Epoch has taken 0:03:47.559846
Number of used sentences in train = 3226
Total loss for epoch 3: 9684.743493
validation loss after epoch 3 : 1608.366896
	Epoch 4....
Epoch has taken 0:04:06.383110
Number of used sentences in train = 3226
Total loss for epoch 4: 8515.210421
validation loss after epoch 4 : 1741.557823
	Epoch 5....
Epoch has taken 0:03:49.503224
Number of used sentences in train = 3226
Total loss for epoch 5: 7786.404238
validation loss after epoch 5 : 1807.678578
	Epoch 6....
Epoch has taken 0:03:51.092192
Number of used sentences in train = 3226
Total loss for epoch 6: 7310.377925
validation loss after epoch 6 : 1919.986494
	Epoch 7....
Epoch has taken 0:03:47.727534
Number of used sentences in train = 3226
Total loss for epoch 7: 6948.034772
validation loss after epoch 7 : 1987.389992
	Epoch 8....
Epoch has taken 0:03:47.576879
Number of used sentences in train = 3226
Total loss for epoch 8: 6715.567798
validation loss after epoch 8 : 2327.637366
	Epoch 9....
Epoch has taken 0:03:49.577616
Number of used sentences in train = 3226
Total loss for epoch 9: 6511.841209
validation loss after epoch 9 : 2340.239603
	Epoch 10....
Epoch has taken 0:03:50.652366
Number of used sentences in train = 3226
Total loss for epoch 10: 6482.307927
validation loss after epoch 10 : 2229.438349
	Epoch 11....
Epoch has taken 0:03:51.941013
Number of used sentences in train = 3226
Total loss for epoch 11: 6415.182006
validation loss after epoch 11 : 2396.871372
	Epoch 12....
Epoch has taken 0:03:53.256329
Number of used sentences in train = 3226
Total loss for epoch 12: 6354.835580
validation loss after epoch 12 : 2355.120528
	Epoch 13....
Epoch has taken 0:03:52.195233
Number of used sentences in train = 3226
Total loss for epoch 13: 6356.666778
validation loss after epoch 13 : 2306.598161
	Epoch 14....
Epoch has taken 0:03:53.690484
Number of used sentences in train = 3226
Total loss for epoch 14: 6319.542077
validation loss after epoch 14 : 2299.130635
	TransitionClassifier(
  (p_embeddings): Embedding(13, 36)
  (w_embeddings): Embedding(17993, 91)
  (lstm): LSTM(127, 69, num_layers=2, dropout=0.34, bidirectional=True)
  (linear1): Linear(in_features=1104, out_features=85, bias=True)
  (linear2): Linear(in_features=85, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:47.684320
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2318.699004
	Epoch 1....
Epoch has taken 0:00:22.240697
Number of used sentences in train = 359
Total loss for epoch 1: 1228.989094
	Epoch 2....
Epoch has taken 0:00:22.228894
Number of used sentences in train = 359
Total loss for epoch 2: 943.468939
	Epoch 3....
Epoch has taken 0:00:22.231013
Number of used sentences in train = 359
Total loss for epoch 3: 850.935218
	Epoch 4....
Epoch has taken 0:00:22.236072
Number of used sentences in train = 359
Total loss for epoch 4: 788.578876
	Epoch 5....
Epoch has taken 0:00:22.230869
Number of used sentences in train = 359
Total loss for epoch 5: 747.266854
	Epoch 6....
Epoch has taken 0:00:22.227709
Number of used sentences in train = 359
Total loss for epoch 6: 718.630132
	Epoch 7....
Epoch has taken 0:00:22.225130
Number of used sentences in train = 359
Total loss for epoch 7: 708.541823
	Epoch 8....
Epoch has taken 0:00:22.214651
Number of used sentences in train = 359
Total loss for epoch 8: 694.327263
	Epoch 9....
Epoch has taken 0:00:22.217062
Number of used sentences in train = 359
Total loss for epoch 9: 686.067850
	Epoch 10....
Epoch has taken 0:00:22.233649
Number of used sentences in train = 359
Total loss for epoch 10: 681.913042
	Epoch 11....
Epoch has taken 0:00:22.220370
Number of used sentences in train = 359
Total loss for epoch 11: 676.608655
	Epoch 12....
Epoch has taken 0:00:22.359883
Number of used sentences in train = 359
Total loss for epoch 12: 694.400537
	Epoch 13....
Epoch has taken 0:00:22.251110
Number of used sentences in train = 359
Total loss for epoch 13: 680.699550
	Epoch 14....
Epoch has taken 0:00:22.242869
Number of used sentences in train = 359
Total loss for epoch 14: 673.620923
Epoch has taken 0:00:22.556029

==================================================================================================
	Training time : 1:03:43.003931
==================================================================================================
	Identification : 0.415

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 12, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 17, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 60, 'lstmDropout': 0.18, 'denseActivation': 'tanh', 'wordDim': 51, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 51)
  (lstm): LSTM(68, 60, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 19196.559310
validation loss after epoch 0 : 1103.475433
	Epoch 1....
Epoch has taken 0:02:53.684823
Number of used sentences in train = 2811
Total loss for epoch 1: 9453.768018
validation loss after epoch 1 : 949.430502
	Epoch 2....
Epoch has taken 0:02:54.294488
Number of used sentences in train = 2811
Total loss for epoch 2: 7975.563210
validation loss after epoch 2 : 922.350575
	Epoch 3....
Epoch has taken 0:02:51.135301
Number of used sentences in train = 2811
Total loss for epoch 3: 7251.133226
validation loss after epoch 3 : 896.326901
	Epoch 4....
Epoch has taken 0:02:50.996751
Number of used sentences in train = 2811
Total loss for epoch 4: 6880.775246
validation loss after epoch 4 : 938.515590
	Epoch 5....
Epoch has taken 0:02:53.961554
Number of used sentences in train = 2811
Total loss for epoch 5: 6390.291386
validation loss after epoch 5 : 959.606762
	Epoch 6....
Epoch has taken 0:02:51.065259
Number of used sentences in train = 2811
Total loss for epoch 6: 6091.649037
validation loss after epoch 6 : 955.753527
	Epoch 7....
Epoch has taken 0:02:51.010746
Number of used sentences in train = 2811
Total loss for epoch 7: 5809.647300
validation loss after epoch 7 : 1033.992127
	Epoch 8....
Epoch has taken 0:02:50.944991
Number of used sentences in train = 2811
Total loss for epoch 8: 5646.176351
validation loss after epoch 8 : 1002.579113
	Epoch 9....
Epoch has taken 0:02:50.937339
Number of used sentences in train = 2811
Total loss for epoch 9: 5472.007112
validation loss after epoch 9 : 1004.046207
	Epoch 10....
Epoch has taken 0:02:50.882117
Number of used sentences in train = 2811
Total loss for epoch 10: 5317.258717
validation loss after epoch 10 : 1035.167240
	Epoch 11....
Epoch has taken 0:02:56.124129
Number of used sentences in train = 2811
Total loss for epoch 11: 5239.064277
validation loss after epoch 11 : 1109.862888
	Epoch 12....
Epoch has taken 0:02:51.091726
Number of used sentences in train = 2811
Total loss for epoch 12: 5163.065714
validation loss after epoch 12 : 1047.586605
	Epoch 13....
Epoch has taken 0:02:50.965798
Number of used sentences in train = 2811
Total loss for epoch 13: 5041.956483
validation loss after epoch 13 : 1048.247432
	Epoch 14....
Epoch has taken 0:02:55.870885
Number of used sentences in train = 2811
Total loss for epoch 14: 5032.641922
validation loss after epoch 14 : 1084.578245
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1177, 51)
  (lstm): LSTM(68, 60, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:51.171405
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1371.126044
	Epoch 1....
Epoch has taken 0:00:18.095966
Number of used sentences in train = 313
Total loss for epoch 1: 859.710772
	Epoch 2....
Epoch has taken 0:00:18.093092
Number of used sentences in train = 313
Total loss for epoch 2: 729.170067
	Epoch 3....
Epoch has taken 0:00:18.102663
Number of used sentences in train = 313
Total loss for epoch 3: 663.488889
	Epoch 4....
Epoch has taken 0:00:18.094512
Number of used sentences in train = 313
Total loss for epoch 4: 628.976212
	Epoch 5....
Epoch has taken 0:00:18.091892
Number of used sentences in train = 313
Total loss for epoch 5: 594.402007
	Epoch 6....
Epoch has taken 0:00:18.080304
Number of used sentences in train = 313
Total loss for epoch 6: 572.132312
	Epoch 7....
Epoch has taken 0:00:18.072643
Number of used sentences in train = 313
Total loss for epoch 7: 550.753792
	Epoch 8....
Epoch has taken 0:00:18.075718
Number of used sentences in train = 313
Total loss for epoch 8: 553.482829
	Epoch 9....
Epoch has taken 0:00:18.074534
Number of used sentences in train = 313
Total loss for epoch 9: 547.714725
	Epoch 10....
Epoch has taken 0:00:18.075618
Number of used sentences in train = 313
Total loss for epoch 10: 549.004524
	Epoch 11....
Epoch has taken 0:00:18.085240
Number of used sentences in train = 313
Total loss for epoch 11: 535.340995
	Epoch 12....
Epoch has taken 0:00:18.075197
Number of used sentences in train = 313
Total loss for epoch 12: 528.607331
	Epoch 13....
Epoch has taken 0:00:18.082034
Number of used sentences in train = 313
Total loss for epoch 13: 522.799542
	Epoch 14....
Epoch has taken 0:00:18.080772
Number of used sentences in train = 313
Total loss for epoch 14: 516.390174
Epoch has taken 0:00:18.084491

==================================================================================================
	Training time : 0:47:35.893672
==================================================================================================
	Identification : 0.493

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 51)
  (lstm): LSTM(68, 60, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11709.304750
validation loss after epoch 0 : 728.550173
	Epoch 1....
Epoch has taken 0:02:00.957529
Number of used sentences in train = 2074
Total loss for epoch 1: 5840.883800
validation loss after epoch 1 : 624.680068
	Epoch 2....
Epoch has taken 0:01:57.058002
Number of used sentences in train = 2074
Total loss for epoch 2: 4944.569316
validation loss after epoch 2 : 628.339118
	Epoch 3....
Epoch has taken 0:01:57.057136
Number of used sentences in train = 2074
Total loss for epoch 3: 4436.103133
validation loss after epoch 3 : 591.403126
	Epoch 4....
Epoch has taken 0:01:57.684319
Number of used sentences in train = 2074
Total loss for epoch 4: 4082.196095
validation loss after epoch 4 : 613.783011
	Epoch 5....
Epoch has taken 0:01:57.236551
Number of used sentences in train = 2074
Total loss for epoch 5: 3845.956780
validation loss after epoch 5 : 607.927230
	Epoch 6....
Epoch has taken 0:01:57.166810
Number of used sentences in train = 2074
Total loss for epoch 6: 3672.113352
validation loss after epoch 6 : 667.187555
	Epoch 7....
Epoch has taken 0:01:57.095040
Number of used sentences in train = 2074
Total loss for epoch 7: 3584.005560
validation loss after epoch 7 : 671.555642
	Epoch 8....
Epoch has taken 0:01:57.004719
Number of used sentences in train = 2074
Total loss for epoch 8: 3505.693079
validation loss after epoch 8 : 746.920177
	Epoch 9....
Epoch has taken 0:01:56.990849
Number of used sentences in train = 2074
Total loss for epoch 9: 3463.461299
validation loss after epoch 9 : 668.712156
	Epoch 10....
Epoch has taken 0:01:57.005184
Number of used sentences in train = 2074
Total loss for epoch 10: 3377.050347
validation loss after epoch 10 : 780.251530
	Epoch 11....
Epoch has taken 0:01:59.194590
Number of used sentences in train = 2074
Total loss for epoch 11: 3324.016715
validation loss after epoch 11 : 740.265180
	Epoch 12....
Epoch has taken 0:01:57.207491
Number of used sentences in train = 2074
Total loss for epoch 12: 3329.565797
validation loss after epoch 12 : 721.025257
	Epoch 13....
Epoch has taken 0:01:57.121184
Number of used sentences in train = 2074
Total loss for epoch 13: 3285.692222
validation loss after epoch 13 : 751.270346
	Epoch 14....
Epoch has taken 0:01:57.565533
Number of used sentences in train = 2074
Total loss for epoch 14: 3277.273601
validation loss after epoch 14 : 791.380921
	TransitionClassifier(
  (p_embeddings): Embedding(18, 17)
  (w_embeddings): Embedding(1133, 51)
  (lstm): LSTM(68, 60, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.081306
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1219.677897
	Epoch 1....
Epoch has taken 0:00:11.922797
Number of used sentences in train = 231
Total loss for epoch 1: 634.222721
	Epoch 2....
Epoch has taken 0:00:11.932601
Number of used sentences in train = 231
Total loss for epoch 2: 499.676020
	Epoch 3....
Epoch has taken 0:00:11.932294
Number of used sentences in train = 231
Total loss for epoch 3: 446.492968
	Epoch 4....
Epoch has taken 0:00:11.924247
Number of used sentences in train = 231
Total loss for epoch 4: 383.794643
	Epoch 5....
Epoch has taken 0:00:11.932796
Number of used sentences in train = 231
Total loss for epoch 5: 379.924893
	Epoch 6....
Epoch has taken 0:00:11.933878
Number of used sentences in train = 231
Total loss for epoch 6: 365.818481
	Epoch 7....
Epoch has taken 0:00:11.932992
Number of used sentences in train = 231
Total loss for epoch 7: 375.265856
	Epoch 8....
Epoch has taken 0:00:11.939247
Number of used sentences in train = 231
Total loss for epoch 8: 355.989276
	Epoch 9....
Epoch has taken 0:00:11.928100
Number of used sentences in train = 231
Total loss for epoch 9: 352.206671
	Epoch 10....
Epoch has taken 0:00:11.926512
Number of used sentences in train = 231
Total loss for epoch 10: 356.251125
	Epoch 11....
Epoch has taken 0:00:11.926520
Number of used sentences in train = 231
Total loss for epoch 11: 349.736276
	Epoch 12....
Epoch has taken 0:00:11.929501
Number of used sentences in train = 231
Total loss for epoch 12: 349.436511
	Epoch 13....
Epoch has taken 0:00:11.930322
Number of used sentences in train = 231
Total loss for epoch 13: 348.344449
	Epoch 14....
Epoch has taken 0:00:11.929167
Number of used sentences in train = 231
Total loss for epoch 14: 348.348562
Epoch has taken 0:00:11.934396

==================================================================================================
	Training time : 0:32:22.711142
==================================================================================================
	Identification : 0.44

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 51)
  (lstm): LSTM(68, 60, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19180.190859
validation loss after epoch 0 : 1294.267509
	Epoch 1....
Epoch has taken 0:03:48.058637
Number of used sentences in train = 3226
Total loss for epoch 1: 11536.601376
validation loss after epoch 1 : 1316.742248
	Epoch 2....
Epoch has taken 0:03:48.504792
Number of used sentences in train = 3226
Total loss for epoch 2: 10446.855367
validation loss after epoch 2 : 1273.619846
	Epoch 3....
Epoch has taken 0:03:46.750137
Number of used sentences in train = 3226
Total loss for epoch 3: 9762.072028
validation loss after epoch 3 : 1259.950823
	Epoch 4....
Epoch has taken 0:03:51.764838
Number of used sentences in train = 3226
Total loss for epoch 4: 9380.504929
validation loss after epoch 4 : 1263.834362
	Epoch 5....
Epoch has taken 0:03:59.008445
Number of used sentences in train = 3226
Total loss for epoch 5: 8998.769392
validation loss after epoch 5 : 1374.653870
	Epoch 6....
Epoch has taken 0:04:05.703148
Number of used sentences in train = 3226
Total loss for epoch 6: 8655.009262
validation loss after epoch 6 : 1393.391379
	Epoch 7....
Epoch has taken 0:03:52.609338
Number of used sentences in train = 3226
Total loss for epoch 7: 8409.688761
validation loss after epoch 7 : 1386.085860
	Epoch 8....
Epoch has taken 0:03:51.967727
Number of used sentences in train = 3226
Total loss for epoch 8: 8200.014898
validation loss after epoch 8 : 1385.549274
	Epoch 9....
Epoch has taken 0:03:52.536525
Number of used sentences in train = 3226
Total loss for epoch 9: 8005.909347
validation loss after epoch 9 : 1485.171497
	Epoch 10....
Epoch has taken 0:03:52.306382
Number of used sentences in train = 3226
Total loss for epoch 10: 7819.970773
validation loss after epoch 10 : 1439.922800
	Epoch 11....
Epoch has taken 0:03:52.400913
Number of used sentences in train = 3226
Total loss for epoch 11: 7738.616088
validation loss after epoch 11 : 1571.799506
	Epoch 12....
Epoch has taken 0:03:50.843699
Number of used sentences in train = 3226
Total loss for epoch 12: 7560.345078
validation loss after epoch 12 : 1563.132466
	Epoch 13....
Epoch has taken 0:03:51.969979
Number of used sentences in train = 3226
Total loss for epoch 13: 7411.597028
validation loss after epoch 13 : 1550.116857
	Epoch 14....
Epoch has taken 0:03:51.911457
Number of used sentences in train = 3226
Total loss for epoch 14: 7287.971901
validation loss after epoch 14 : 1561.930647
	TransitionClassifier(
  (p_embeddings): Embedding(13, 17)
  (w_embeddings): Embedding(1202, 51)
  (lstm): LSTM(68, 60, num_layers=2, dropout=0.18, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:50.789049
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2029.657602
	Epoch 1....
Epoch has taken 0:00:22.876933
Number of used sentences in train = 359
Total loss for epoch 1: 1199.584914
	Epoch 2....
Epoch has taken 0:00:22.881734
Number of used sentences in train = 359
Total loss for epoch 2: 1035.673586
	Epoch 3....
Epoch has taken 0:00:22.848724
Number of used sentences in train = 359
Total loss for epoch 3: 935.344177
	Epoch 4....
Epoch has taken 0:00:22.874012
Number of used sentences in train = 359
Total loss for epoch 4: 858.573287
	Epoch 5....
Epoch has taken 0:00:22.862365
Number of used sentences in train = 359
Total loss for epoch 5: 810.847738
	Epoch 6....
Epoch has taken 0:00:22.869535
Number of used sentences in train = 359
Total loss for epoch 6: 771.729526
	Epoch 7....
Epoch has taken 0:00:22.854080
Number of used sentences in train = 359
Total loss for epoch 7: 767.206559
	Epoch 8....
Epoch has taken 0:00:22.869276
Number of used sentences in train = 359
Total loss for epoch 8: 751.079008
	Epoch 9....
Epoch has taken 0:00:22.859443
Number of used sentences in train = 359
Total loss for epoch 9: 718.486992
	Epoch 10....
Epoch has taken 0:00:22.871889
Number of used sentences in train = 359
Total loss for epoch 10: 722.689209
	Epoch 11....
Epoch has taken 0:00:22.593732
Number of used sentences in train = 359
Total loss for epoch 11: 705.586764
	Epoch 12....
Epoch has taken 0:00:22.860698
Number of used sentences in train = 359
Total loss for epoch 12: 707.907879
	Epoch 13....
Epoch has taken 0:00:22.864341
Number of used sentences in train = 359
Total loss for epoch 13: 697.698486
	Epoch 14....
Epoch has taken 0:00:22.849237
Number of used sentences in train = 359
Total loss for epoch 14: 690.744101
Epoch has taken 0:00:22.874801

==================================================================================================
	Training time : 1:03:50.475864
==================================================================================================
	Identification : 0.346

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 21, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 57, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 85, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 132, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 57)
  (w_embeddings): Embedding(9233, 132)
  (lstm): LSTM(189, 85, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 16831.638997
validation loss after epoch 0 : 1216.116090
	Epoch 1....
Epoch has taken 0:02:56.741344
Number of used sentences in train = 2811
Total loss for epoch 1: 10033.695299
validation loss after epoch 1 : 1051.261353
	Epoch 2....
Epoch has taken 0:02:56.013810
Number of used sentences in train = 2811
Total loss for epoch 2: 7925.098922
validation loss after epoch 2 : 1040.812617
	Epoch 3....
Epoch has taken 0:02:56.903903
Number of used sentences in train = 2811
Total loss for epoch 3: 6756.184206
validation loss after epoch 3 : 1150.434907
	Epoch 4....
Epoch has taken 0:02:56.835095
Number of used sentences in train = 2811
Total loss for epoch 4: 6008.960968
validation loss after epoch 4 : 1171.078552
	Epoch 5....
Epoch has taken 0:02:56.491312
Number of used sentences in train = 2811
Total loss for epoch 5: 5522.998868
validation loss after epoch 5 : 1279.071943
	Epoch 6....
Epoch has taken 0:02:57.000578
Number of used sentences in train = 2811
Total loss for epoch 6: 5297.197983
validation loss after epoch 6 : 1316.974592
	Epoch 7....
Epoch has taken 0:02:56.949776
Number of used sentences in train = 2811
Total loss for epoch 7: 5088.757652
validation loss after epoch 7 : 1334.083507
	Epoch 8....
Epoch has taken 0:02:56.702693
Number of used sentences in train = 2811
Total loss for epoch 8: 4944.460283
validation loss after epoch 8 : 1462.644597
	Epoch 9....
Epoch has taken 0:02:57.361154
Number of used sentences in train = 2811
Total loss for epoch 9: 4858.480736
validation loss after epoch 9 : 1440.122674
	Epoch 10....
Epoch has taken 0:02:58.017400
Number of used sentences in train = 2811
Total loss for epoch 10: 4827.940204
validation loss after epoch 10 : 1425.141836
	Epoch 11....
Epoch has taken 0:02:56.930399
Number of used sentences in train = 2811
Total loss for epoch 11: 4746.080472
validation loss after epoch 11 : 1491.934119
	Epoch 12....
Epoch has taken 0:02:56.895319
Number of used sentences in train = 2811
Total loss for epoch 12: 4678.032090
validation loss after epoch 12 : 1487.766067
	Epoch 13....
Epoch has taken 0:02:56.686979
Number of used sentences in train = 2811
Total loss for epoch 13: 4639.123624
validation loss after epoch 13 : 1537.265748
	Epoch 14....
Epoch has taken 0:02:55.781313
Number of used sentences in train = 2811
Total loss for epoch 14: 4617.523914
validation loss after epoch 14 : 1539.968917
	TransitionClassifier(
  (p_embeddings): Embedding(18, 57)
  (w_embeddings): Embedding(9233, 132)
  (lstm): LSTM(189, 85, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:54.748238
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1642.510419
	Epoch 1....
Epoch has taken 0:00:18.735399
Number of used sentences in train = 313
Total loss for epoch 1: 999.985635
	Epoch 2....
Epoch has taken 0:00:18.741178
Number of used sentences in train = 313
Total loss for epoch 2: 730.453827
	Epoch 3....
Epoch has taken 0:00:18.759476
Number of used sentences in train = 313
Total loss for epoch 3: 646.311685
	Epoch 4....
Epoch has taken 0:00:18.735787
Number of used sentences in train = 313
Total loss for epoch 4: 575.336402
	Epoch 5....
Epoch has taken 0:00:18.732638
Number of used sentences in train = 313
Total loss for epoch 5: 561.674460
	Epoch 6....
Epoch has taken 0:00:18.747378
Number of used sentences in train = 313
Total loss for epoch 6: 553.666160
	Epoch 7....
Epoch has taken 0:00:18.734606
Number of used sentences in train = 313
Total loss for epoch 7: 544.621502
	Epoch 8....
Epoch has taken 0:00:18.746676
Number of used sentences in train = 313
Total loss for epoch 8: 538.606474
	Epoch 9....
Epoch has taken 0:00:18.737646
Number of used sentences in train = 313
Total loss for epoch 9: 543.197585
	Epoch 10....
Epoch has taken 0:00:18.756246
Number of used sentences in train = 313
Total loss for epoch 10: 543.969567
	Epoch 11....
Epoch has taken 0:00:18.764520
Number of used sentences in train = 313
Total loss for epoch 11: 539.111934
	Epoch 12....
Epoch has taken 0:00:18.740709
Number of used sentences in train = 313
Total loss for epoch 12: 541.197376
	Epoch 13....
Epoch has taken 0:00:18.762185
Number of used sentences in train = 313
Total loss for epoch 13: 537.397646
	Epoch 14....
Epoch has taken 0:00:18.743258
Number of used sentences in train = 313
Total loss for epoch 14: 536.994655
Epoch has taken 0:00:18.755435

==================================================================================================
	Training time : 0:48:51.765464
==================================================================================================
	Identification : 0.478

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 57)
  (w_embeddings): Embedding(7094, 132)
  (lstm): LSTM(189, 85, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 14273.171062
validation loss after epoch 0 : 1300.182945
	Epoch 1....
Epoch has taken 0:02:01.076776
Number of used sentences in train = 2074
Total loss for epoch 1: 9252.569571
validation loss after epoch 1 : 1107.755456
	Epoch 2....
Epoch has taken 0:02:01.175662
Number of used sentences in train = 2074
Total loss for epoch 2: 6958.463224
validation loss after epoch 2 : 919.440373
	Epoch 3....
Epoch has taken 0:02:01.349929
Number of used sentences in train = 2074
Total loss for epoch 3: 5370.740215
validation loss after epoch 3 : 940.452327
	Epoch 4....
Epoch has taken 0:02:00.430449
Number of used sentences in train = 2074
Total loss for epoch 4: 4517.386967
validation loss after epoch 4 : 1050.114178
	Epoch 5....
Epoch has taken 0:02:01.295398
Number of used sentences in train = 2074
Total loss for epoch 5: 4086.469199
validation loss after epoch 5 : 1203.719916
	Epoch 6....
Epoch has taken 0:02:00.471129
Number of used sentences in train = 2074
Total loss for epoch 6: 3857.750747
validation loss after epoch 6 : 1139.216109
	Epoch 7....
Epoch has taken 0:01:59.810675
Number of used sentences in train = 2074
Total loss for epoch 7: 3690.596377
validation loss after epoch 7 : 1144.033063
	Epoch 8....
Epoch has taken 0:02:01.116076
Number of used sentences in train = 2074
Total loss for epoch 8: 3554.592408
validation loss after epoch 8 : 1125.241924
	Epoch 9....
Epoch has taken 0:02:00.220722
Number of used sentences in train = 2074
Total loss for epoch 9: 3469.924670
validation loss after epoch 9 : 1278.653247
	Epoch 10....
Epoch has taken 0:02:01.283854
Number of used sentences in train = 2074
Total loss for epoch 10: 3413.874715
validation loss after epoch 10 : 1316.949643
	Epoch 11....
Epoch has taken 0:01:59.620000
Number of used sentences in train = 2074
Total loss for epoch 11: 3377.225835
validation loss after epoch 11 : 1438.359026
	Epoch 12....
Epoch has taken 0:01:59.765835
Number of used sentences in train = 2074
Total loss for epoch 12: 3355.599002
validation loss after epoch 12 : 1295.414213
	Epoch 13....
Epoch has taken 0:02:01.262501
Number of used sentences in train = 2074
Total loss for epoch 13: 3329.365566
validation loss after epoch 13 : 1291.800825
	Epoch 14....
Epoch has taken 0:02:00.354359
Number of used sentences in train = 2074
Total loss for epoch 14: 3292.209694
validation loss after epoch 14 : 1311.225981
	TransitionClassifier(
  (p_embeddings): Embedding(18, 57)
  (w_embeddings): Embedding(7094, 132)
  (lstm): LSTM(189, 85, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.237458
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1649.221277
	Epoch 1....
Epoch has taken 0:00:12.420373
Number of used sentences in train = 231
Total loss for epoch 1: 890.855012
	Epoch 2....
Epoch has taken 0:00:12.423058
Number of used sentences in train = 231
Total loss for epoch 2: 631.853338
	Epoch 3....
Epoch has taken 0:00:12.397173
Number of used sentences in train = 231
Total loss for epoch 3: 491.872352
	Epoch 4....
Epoch has taken 0:00:12.409726
Number of used sentences in train = 231
Total loss for epoch 4: 452.520796
	Epoch 5....
Epoch has taken 0:00:12.428944
Number of used sentences in train = 231
Total loss for epoch 5: 408.661454
	Epoch 6....
Epoch has taken 0:00:12.432340
Number of used sentences in train = 231
Total loss for epoch 6: 386.277117
	Epoch 7....
Epoch has taken 0:00:12.414754
Number of used sentences in train = 231
Total loss for epoch 7: 372.842730
	Epoch 8....
Epoch has taken 0:00:12.406883
Number of used sentences in train = 231
Total loss for epoch 8: 368.992337
	Epoch 9....
Epoch has taken 0:00:12.405655
Number of used sentences in train = 231
Total loss for epoch 9: 362.369983
	Epoch 10....
Epoch has taken 0:00:12.426922
Number of used sentences in train = 231
Total loss for epoch 10: 356.198378
	Epoch 11....
Epoch has taken 0:00:12.415507
Number of used sentences in train = 231
Total loss for epoch 11: 354.554267
	Epoch 12....
Epoch has taken 0:00:12.428827
Number of used sentences in train = 231
Total loss for epoch 12: 354.543316
	Epoch 13....
Epoch has taken 0:00:12.419008
Number of used sentences in train = 231
Total loss for epoch 13: 354.196492
	Epoch 14....
Epoch has taken 0:00:12.415021
Number of used sentences in train = 231
Total loss for epoch 14: 350.852740
Epoch has taken 0:00:12.662623

==================================================================================================
	Training time : 0:33:17.323037
==================================================================================================
	Identification : 0.323

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 57)
  (w_embeddings): Embedding(17886, 132)
  (lstm): LSTM(189, 85, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 28340.001715
validation loss after epoch 0 : 1757.453630
	Epoch 1....
Epoch has taken 0:04:21.096341
Number of used sentences in train = 3226
Total loss for epoch 1: 13997.502905
validation loss after epoch 1 : 1624.298806
	Epoch 2....
Epoch has taken 0:04:23.094812
Number of used sentences in train = 3226
Total loss for epoch 2: 11221.593312
validation loss after epoch 2 : 1640.336910
	Epoch 3....
Epoch has taken 0:04:23.305974
Number of used sentences in train = 3226
Total loss for epoch 3: 9503.823949
validation loss after epoch 3 : 1665.892533
	Epoch 4....
Epoch has taken 0:04:22.123462
Number of used sentences in train = 3226
Total loss for epoch 4: 8358.746604
validation loss after epoch 4 : 1838.470478
	Epoch 5....
Epoch has taken 0:04:20.335331
Number of used sentences in train = 3226
Total loss for epoch 5: 7548.145769
validation loss after epoch 5 : 2123.404904
	Epoch 6....
Epoch has taken 0:04:23.460927
Number of used sentences in train = 3226
Total loss for epoch 6: 7105.635319
validation loss after epoch 6 : 2154.119656
	Epoch 7....
Epoch has taken 0:04:21.842028
Number of used sentences in train = 3226
Total loss for epoch 7: 6911.718504
validation loss after epoch 7 : 2155.062702
	Epoch 8....
Epoch has taken 0:04:23.512400
Number of used sentences in train = 3226
Total loss for epoch 8: 6654.007403
validation loss after epoch 8 : 2352.471928
	Epoch 9....
Epoch has taken 0:04:23.409629
Number of used sentences in train = 3226
Total loss for epoch 9: 6499.777516
validation loss after epoch 9 : 2530.091100
	Epoch 10....
Epoch has taken 0:04:22.562479
Number of used sentences in train = 3226
Total loss for epoch 10: 6398.197540
validation loss after epoch 10 : 2415.852509
	Epoch 11....
Epoch has taken 0:04:22.200139
Number of used sentences in train = 3226
Total loss for epoch 11: 6364.791096
validation loss after epoch 11 : 2545.660642
	Epoch 12....
Epoch has taken 0:04:21.009189
Number of used sentences in train = 3226
Total loss for epoch 12: 6310.473821
validation loss after epoch 12 : 2568.438200
	Epoch 13....
Epoch has taken 0:04:21.698378
Number of used sentences in train = 3226
Total loss for epoch 13: 6276.294956
validation loss after epoch 13 : 2609.512560
	Epoch 14....
Epoch has taken 0:04:20.382310
Number of used sentences in train = 3226
Total loss for epoch 14: 6262.778584
validation loss after epoch 14 : 2610.449207
	TransitionClassifier(
  (p_embeddings): Embedding(13, 57)
  (w_embeddings): Embedding(17886, 132)
  (lstm): LSTM(189, 85, num_layers=2, dropout=0.16, bidirectional=True)
  (linear1): Linear(in_features=1360, out_features=21, bias=True)
  (linear2): Linear(in_features=21, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:58.029746
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2640.037784
	Epoch 1....
Epoch has taken 0:00:23.440384
Number of used sentences in train = 359
Total loss for epoch 1: 1274.413855
	Epoch 2....
Epoch has taken 0:00:23.441408
Number of used sentences in train = 359
Total loss for epoch 2: 970.736747
	Epoch 3....
Epoch has taken 0:00:23.431214
Number of used sentences in train = 359
Total loss for epoch 3: 801.175058
	Epoch 4....
Epoch has taken 0:00:23.447750
Number of used sentences in train = 359
Total loss for epoch 4: 727.622840
	Epoch 5....
Epoch has taken 0:00:23.426372
Number of used sentences in train = 359
Total loss for epoch 5: 703.048478
	Epoch 6....
Epoch has taken 0:00:23.425384
Number of used sentences in train = 359
Total loss for epoch 6: 691.592459
	Epoch 7....
Epoch has taken 0:00:23.426781
Number of used sentences in train = 359
Total loss for epoch 7: 676.289521
	Epoch 8....
Epoch has taken 0:00:23.420209
Number of used sentences in train = 359
Total loss for epoch 8: 673.989072
	Epoch 9....
Epoch has taken 0:00:23.426854
Number of used sentences in train = 359
Total loss for epoch 9: 673.375055
	Epoch 10....
Epoch has taken 0:00:23.438891
Number of used sentences in train = 359
Total loss for epoch 10: 672.774362
	Epoch 11....
Epoch has taken 0:00:23.417197
Number of used sentences in train = 359
Total loss for epoch 11: 674.460864
	Epoch 12....
Epoch has taken 0:00:23.425218
Number of used sentences in train = 359
Total loss for epoch 12: 672.435126
	Epoch 13....
Epoch has taken 0:00:23.427640
Number of used sentences in train = 359
Total loss for epoch 13: 672.265893
	Epoch 14....
Epoch has taken 0:00:23.436041
Number of used sentences in train = 359
Total loss for epoch 14: 671.800984
Epoch has taken 0:00:23.420382

==================================================================================================
	Training time : 1:11:00.205599
==================================================================================================
	Identification : 0.067

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 13, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 59, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 25, 'lstmDropout': 0.24, 'denseActivation': 'tanh', 'wordDim': 114, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 59)
  (w_embeddings): Embedding(1882, 114)
  (lstm): LSTM(173, 25, num_layers=2, dropout=0.24, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10436.044335
validation loss after epoch 0 : 858.827258
	Epoch 1....
Epoch has taken 0:02:56.265562
Number of used sentences in train = 2811
Total loss for epoch 1: 7612.842989
validation loss after epoch 1 : 846.007388
	Epoch 2....
Epoch has taken 0:03:04.280748
Number of used sentences in train = 2811
Total loss for epoch 2: 6948.429655
validation loss after epoch 2 : 845.111820
	Epoch 3....
Epoch has taken 0:03:13.246555
Number of used sentences in train = 2811
Total loss for epoch 3: 6480.544305
validation loss after epoch 3 : 855.954096
	Epoch 4....
Epoch has taken 0:02:55.972681
Number of used sentences in train = 2811
Total loss for epoch 4: 6139.631187
validation loss after epoch 4 : 847.270995
	Epoch 5....
Epoch has taken 0:02:56.693131
Number of used sentences in train = 2811
Total loss for epoch 5: 5918.314544
validation loss after epoch 5 : 895.206828
	Epoch 6....
Epoch has taken 0:03:14.184376
Number of used sentences in train = 2811
Total loss for epoch 6: 5760.701997
validation loss after epoch 6 : 873.495667
	Epoch 7....
Epoch has taken 0:02:55.789448
Number of used sentences in train = 2811
Total loss for epoch 7: 5572.747049
validation loss after epoch 7 : 875.349148
	Epoch 8....
Epoch has taken 0:02:51.478663
Number of used sentences in train = 2811
Total loss for epoch 8: 5448.353395
validation loss after epoch 8 : 919.490104
	Epoch 9....
Epoch has taken 0:02:50.947979
Number of used sentences in train = 2811
Total loss for epoch 9: 5328.853781
validation loss after epoch 9 : 942.726362
	Epoch 10....
Epoch has taken 0:02:55.518822
Number of used sentences in train = 2811
Total loss for epoch 10: 5303.466877
validation loss after epoch 10 : 918.677095
	Epoch 11....
Epoch has taken 0:02:56.323592
Number of used sentences in train = 2811
Total loss for epoch 11: 5185.509127
validation loss after epoch 11 : 950.401457
	Epoch 12....
Epoch has taken 0:02:54.522724
Number of used sentences in train = 2811
Total loss for epoch 12: 5176.039056
validation loss after epoch 12 : 913.043883
	Epoch 13....
Epoch has taken 0:02:50.643743
Number of used sentences in train = 2811
Total loss for epoch 13: 5066.088146
validation loss after epoch 13 : 961.394203
	Epoch 14....
Epoch has taken 0:02:56.023033
Number of used sentences in train = 2811
Total loss for epoch 14: 4998.786241
validation loss after epoch 14 : 958.406267
	TransitionClassifier(
  (p_embeddings): Embedding(18, 59)
  (w_embeddings): Embedding(1882, 114)
  (lstm): LSTM(173, 25, num_layers=2, dropout=0.24, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.589519
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1145.912588
	Epoch 1....
Epoch has taken 0:00:18.026990
Number of used sentences in train = 313
Total loss for epoch 1: 702.052501
	Epoch 2....
Epoch has taken 0:00:18.019674
Number of used sentences in train = 313
Total loss for epoch 2: 624.015547
	Epoch 3....
Epoch has taken 0:00:18.023356
Number of used sentences in train = 313
Total loss for epoch 3: 595.238157
	Epoch 4....
Epoch has taken 0:00:18.029131
Number of used sentences in train = 313
Total loss for epoch 4: 569.613106
	Epoch 5....
Epoch has taken 0:00:18.026124
Number of used sentences in train = 313
Total loss for epoch 5: 560.889702
	Epoch 6....
Epoch has taken 0:00:18.026657
Number of used sentences in train = 313
Total loss for epoch 6: 552.157310
	Epoch 7....
Epoch has taken 0:00:18.078218
Number of used sentences in train = 313
Total loss for epoch 7: 543.428495
	Epoch 8....
Epoch has taken 0:00:18.026594
Number of used sentences in train = 313
Total loss for epoch 8: 537.949357
	Epoch 9....
Epoch has taken 0:00:18.029240
Number of used sentences in train = 313
Total loss for epoch 9: 537.653709
	Epoch 10....
Epoch has taken 0:00:18.017566
Number of used sentences in train = 313
Total loss for epoch 10: 535.450994
	Epoch 11....
Epoch has taken 0:00:18.029499
Number of used sentences in train = 313
Total loss for epoch 11: 521.099365
	Epoch 12....
Epoch has taken 0:00:18.250908
Number of used sentences in train = 313
Total loss for epoch 12: 525.949841
	Epoch 13....
Epoch has taken 0:00:18.024177
Number of used sentences in train = 313
Total loss for epoch 13: 516.493552
	Epoch 14....
Epoch has taken 0:00:18.027431
Number of used sentences in train = 313
Total loss for epoch 14: 508.026615
Epoch has taken 0:00:18.026618

==================================================================================================
	Training time : 0:48:53.640340
==================================================================================================
	Identification : 0.473

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 59)
  (w_embeddings): Embedding(1680, 114)
  (lstm): LSTM(173, 25, num_layers=2, dropout=0.24, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7999.890746
validation loss after epoch 0 : 645.632228
	Epoch 1....
Epoch has taken 0:01:56.708744
Number of used sentences in train = 2074
Total loss for epoch 1: 5402.926889
validation loss after epoch 1 : 610.458357
	Epoch 2....
Epoch has taken 0:01:58.145456
Number of used sentences in train = 2074
Total loss for epoch 2: 4807.256864
validation loss after epoch 2 : 595.796322
	Epoch 3....
Epoch has taken 0:01:56.966467
Number of used sentences in train = 2074
Total loss for epoch 3: 4430.547482
validation loss after epoch 3 : 610.110662
	Epoch 4....
Epoch has taken 0:01:57.220908
Number of used sentences in train = 2074
Total loss for epoch 4: 4137.446997
validation loss after epoch 4 : 613.866942
	Epoch 5....
Epoch has taken 0:01:59.459599
Number of used sentences in train = 2074
Total loss for epoch 5: 3936.688062
validation loss after epoch 5 : 664.554029
	Epoch 6....
Epoch has taken 0:01:56.966283
Number of used sentences in train = 2074
Total loss for epoch 6: 3774.545807
validation loss after epoch 6 : 655.188251
	Epoch 7....
Epoch has taken 0:01:56.916136
Number of used sentences in train = 2074
Total loss for epoch 7: 3612.967108
validation loss after epoch 7 : 694.613295
	Epoch 8....
Epoch has taken 0:01:58.240333
Number of used sentences in train = 2074
Total loss for epoch 8: 3575.957029
validation loss after epoch 8 : 684.585362
	Epoch 9....
Epoch has taken 0:02:00.071587
Number of used sentences in train = 2074
Total loss for epoch 9: 3495.612574
validation loss after epoch 9 : 699.733488
	Epoch 10....
Epoch has taken 0:01:56.924504
Number of used sentences in train = 2074
Total loss for epoch 10: 3469.919834
validation loss after epoch 10 : 667.420524
	Epoch 11....
Epoch has taken 0:01:56.960477
Number of used sentences in train = 2074
Total loss for epoch 11: 3450.266842
validation loss after epoch 11 : 644.363438
	Epoch 12....
Epoch has taken 0:01:56.882349
Number of used sentences in train = 2074
Total loss for epoch 12: 3355.969155
validation loss after epoch 12 : 691.934873
	Epoch 13....
Epoch has taken 0:01:57.332822
Number of used sentences in train = 2074
Total loss for epoch 13: 3347.673892
validation loss after epoch 13 : 749.708609
	Epoch 14....
Epoch has taken 0:01:56.873356
Number of used sentences in train = 2074
Total loss for epoch 14: 3346.772035
validation loss after epoch 14 : 723.422120
	TransitionClassifier(
  (p_embeddings): Embedding(18, 59)
  (w_embeddings): Embedding(1680, 114)
  (lstm): LSTM(173, 25, num_layers=2, dropout=0.24, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.864767
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1004.908397
	Epoch 1....
Epoch has taken 0:00:11.908915
Number of used sentences in train = 231
Total loss for epoch 1: 605.590302
	Epoch 2....
Epoch has taken 0:00:11.903242
Number of used sentences in train = 231
Total loss for epoch 2: 474.421001
	Epoch 3....
Epoch has taken 0:00:11.904141
Number of used sentences in train = 231
Total loss for epoch 3: 405.294856
	Epoch 4....
Epoch has taken 0:00:11.908876
Number of used sentences in train = 231
Total loss for epoch 4: 377.977214
	Epoch 5....
Epoch has taken 0:00:11.908471
Number of used sentences in train = 231
Total loss for epoch 5: 374.034149
	Epoch 6....
Epoch has taken 0:00:12.081043
Number of used sentences in train = 231
Total loss for epoch 6: 372.553090
	Epoch 7....
Epoch has taken 0:00:11.903221
Number of used sentences in train = 231
Total loss for epoch 7: 375.594912
	Epoch 8....
Epoch has taken 0:00:11.977958
Number of used sentences in train = 231
Total loss for epoch 8: 367.905626
	Epoch 9....
Epoch has taken 0:00:12.346549
Number of used sentences in train = 231
Total loss for epoch 9: 367.501837
	Epoch 10....
Epoch has taken 0:00:11.915956
Number of used sentences in train = 231
Total loss for epoch 10: 354.212125
	Epoch 11....
Epoch has taken 0:00:11.898882
Number of used sentences in train = 231
Total loss for epoch 11: 356.781578
	Epoch 12....
Epoch has taken 0:00:12.329758
Number of used sentences in train = 231
Total loss for epoch 12: 350.427811
	Epoch 13....
Epoch has taken 0:00:12.248048
Number of used sentences in train = 231
Total loss for epoch 13: 352.304307
	Epoch 14....
Epoch has taken 0:00:11.905121
Number of used sentences in train = 231
Total loss for epoch 14: 352.241665
Epoch has taken 0:00:11.905901

==================================================================================================
	Training time : 0:32:22.911936
==================================================================================================
	Identification : 0.088

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 59)
  (w_embeddings): Embedding(3369, 114)
  (lstm): LSTM(173, 25, num_layers=2, dropout=0.24, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 13438.261551
validation loss after epoch 0 : 1125.039623
	Epoch 1....
Epoch has taken 0:04:10.144374
Number of used sentences in train = 3226
Total loss for epoch 1: 9898.848067
validation loss after epoch 1 : 1082.292465
	Epoch 2....
Epoch has taken 0:04:02.620739
Number of used sentences in train = 3226
Total loss for epoch 2: 9080.699226
validation loss after epoch 2 : 1083.537723
	Epoch 3....
Epoch has taken 0:03:51.910926
Number of used sentences in train = 3226
Total loss for epoch 3: 8535.874307
validation loss after epoch 3 : 1061.952929
	Epoch 4....
Epoch has taken 0:03:50.658451
Number of used sentences in train = 3226
Total loss for epoch 4: 8160.261501
validation loss after epoch 4 : 1099.230288
	Epoch 5....
Epoch has taken 0:04:03.034233
Number of used sentences in train = 3226
Total loss for epoch 5: 7878.344527
validation loss after epoch 5 : 1148.845540
	Epoch 6....
Epoch has taken 0:04:15.254265
Number of used sentences in train = 3226
Total loss for epoch 6: 7712.227013
validation loss after epoch 6 : 1123.150533
	Epoch 7....
Epoch has taken 0:04:06.829415
Number of used sentences in train = 3226
Total loss for epoch 7: 7536.098207
validation loss after epoch 7 : 1170.412542
	Epoch 8....
Epoch has taken 0:03:51.939306
Number of used sentences in train = 3226
Total loss for epoch 8: 7401.858590
validation loss after epoch 8 : 1213.101100
	Epoch 9....
Epoch has taken 0:04:13.145286
Number of used sentences in train = 3226
Total loss for epoch 9: 7295.104552
validation loss after epoch 9 : 1254.501038
	Epoch 10....
Epoch has taken 0:04:15.363918
Number of used sentences in train = 3226
Total loss for epoch 10: 7183.988455
validation loss after epoch 10 : 1298.595415
	Epoch 11....
Epoch has taken 0:04:00.916277
Number of used sentences in train = 3226
Total loss for epoch 11: 7103.632024
validation loss after epoch 11 : 1298.210575
	Epoch 12....
Epoch has taken 0:03:50.299853
Number of used sentences in train = 3226
Total loss for epoch 12: 7026.274188
validation loss after epoch 12 : 1308.318633
	Epoch 13....
Epoch has taken 0:03:49.759227
Number of used sentences in train = 3226
Total loss for epoch 13: 6887.390940
validation loss after epoch 13 : 1373.151291
	Epoch 14....
Epoch has taken 0:04:06.163437
Number of used sentences in train = 3226
Total loss for epoch 14: 6897.101177
validation loss after epoch 14 : 1330.810474
	TransitionClassifier(
  (p_embeddings): Embedding(13, 59)
  (w_embeddings): Embedding(3369, 114)
  (lstm): LSTM(173, 25, num_layers=2, dropout=0.24, bidirectional=True)
  (linear1): Linear(in_features=400, out_features=13, bias=True)
  (linear2): Linear(in_features=13, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:14.937292
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1495.401447
	Epoch 1....
Epoch has taken 0:00:25.156077
Number of used sentences in train = 359
Total loss for epoch 1: 974.440493
	Epoch 2....
Epoch has taken 0:00:25.148924
Number of used sentences in train = 359
Total loss for epoch 2: 888.501065
	Epoch 3....
Epoch has taken 0:00:25.136354
Number of used sentences in train = 359
Total loss for epoch 3: 832.650988
	Epoch 4....
Epoch has taken 0:00:25.144300
Number of used sentences in train = 359
Total loss for epoch 4: 780.951197
	Epoch 5....
Epoch has taken 0:00:25.130685
Number of used sentences in train = 359
Total loss for epoch 5: 753.432201
	Epoch 6....
Epoch has taken 0:00:25.153152
Number of used sentences in train = 359
Total loss for epoch 6: 741.437171
	Epoch 7....
Epoch has taken 0:00:24.884867
Number of used sentences in train = 359
Total loss for epoch 7: 757.226902
	Epoch 8....
Epoch has taken 0:00:25.134616
Number of used sentences in train = 359
Total loss for epoch 8: 712.982457
	Epoch 9....
Epoch has taken 0:00:25.121231
Number of used sentences in train = 359
Total loss for epoch 9: 705.493636
	Epoch 10....
Epoch has taken 0:00:25.129087
Number of used sentences in train = 359
Total loss for epoch 10: 694.869847
	Epoch 11....
Epoch has taken 0:00:25.130214
Number of used sentences in train = 359
Total loss for epoch 11: 704.260840
	Epoch 12....
Epoch has taken 0:00:25.124940
Number of used sentences in train = 359
Total loss for epoch 12: 697.770194
	Epoch 13....
Epoch has taken 0:00:24.940217
Number of used sentences in train = 359
Total loss for epoch 13: 695.443274
	Epoch 14....
Epoch has taken 0:00:25.143239
Number of used sentences in train = 359
Total loss for epoch 14: 687.101192
Epoch has taken 0:00:25.143393

==================================================================================================
	Training time : 1:07:00.260522
==================================================================================================
	Identification : 0.305

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 30, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 73, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 43, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 73)
  (w_embeddings): Embedding(1177, 63)
  (lstm): LSTM(136, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10877.219873
validation loss after epoch 0 : 950.556576
	Epoch 1....
Epoch has taken 0:03:01.325809
Number of used sentences in train = 2811
Total loss for epoch 1: 8133.341677
validation loss after epoch 1 : 916.142628
	Epoch 2....
Epoch has taken 0:03:01.162166
Number of used sentences in train = 2811
Total loss for epoch 2: 7359.938118
validation loss after epoch 2 : 897.452324
	Epoch 3....
Epoch has taken 0:03:01.191127
Number of used sentences in train = 2811
Total loss for epoch 3: 6767.781861
validation loss after epoch 3 : 903.497144
	Epoch 4....
Epoch has taken 0:02:45.296478
Number of used sentences in train = 2811
Total loss for epoch 4: 6376.021975
validation loss after epoch 4 : 890.761222
	Epoch 5....
Epoch has taken 0:02:54.035406
Number of used sentences in train = 2811
Total loss for epoch 5: 6070.852617
validation loss after epoch 5 : 941.591023
	Epoch 6....
Epoch has taken 0:02:44.360240
Number of used sentences in train = 2811
Total loss for epoch 6: 5797.476425
validation loss after epoch 6 : 957.822026
	Epoch 7....
Epoch has taken 0:02:45.848467
Number of used sentences in train = 2811
Total loss for epoch 7: 5587.458445
validation loss after epoch 7 : 959.315460
	Epoch 8....
Epoch has taken 0:02:42.913894
Number of used sentences in train = 2811
Total loss for epoch 8: 5397.831580
validation loss after epoch 8 : 1016.827705
	Epoch 9....
Epoch has taken 0:02:44.019173
Number of used sentences in train = 2811
Total loss for epoch 9: 5246.578589
validation loss after epoch 9 : 1085.308156
	Epoch 10....
Epoch has taken 0:02:44.315373
Number of used sentences in train = 2811
Total loss for epoch 10: 5115.327113
validation loss after epoch 10 : 1086.260853
	Epoch 11....
Epoch has taken 0:02:44.068028
Number of used sentences in train = 2811
Total loss for epoch 11: 4984.992062
validation loss after epoch 11 : 1089.978101
	Epoch 12....
Epoch has taken 0:02:42.131348
Number of used sentences in train = 2811
Total loss for epoch 12: 4891.054239
validation loss after epoch 12 : 1143.952842
	Epoch 13....
Epoch has taken 0:02:44.364734
Number of used sentences in train = 2811
Total loss for epoch 13: 4834.834977
validation loss after epoch 13 : 1169.430686
	Epoch 14....
Epoch has taken 0:02:43.600565
Number of used sentences in train = 2811
Total loss for epoch 14: 4776.692027
validation loss after epoch 14 : 1196.071445
	TransitionClassifier(
  (p_embeddings): Embedding(18, 73)
  (w_embeddings): Embedding(1177, 63)
  (lstm): LSTM(136, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:44.259596
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1336.173765
	Epoch 1....
Epoch has taken 0:00:17.412269
Number of used sentences in train = 313
Total loss for epoch 1: 770.815479
	Epoch 2....
Epoch has taken 0:00:17.408820
Number of used sentences in train = 313
Total loss for epoch 2: 637.040360
	Epoch 3....
Epoch has taken 0:00:17.392278
Number of used sentences in train = 313
Total loss for epoch 3: 576.395938
	Epoch 4....
Epoch has taken 0:00:17.410012
Number of used sentences in train = 313
Total loss for epoch 4: 545.319738
	Epoch 5....
Epoch has taken 0:00:17.406731
Number of used sentences in train = 313
Total loss for epoch 5: 534.810565
	Epoch 6....
Epoch has taken 0:00:17.396602
Number of used sentences in train = 313
Total loss for epoch 6: 525.343387
	Epoch 7....
Epoch has taken 0:00:17.395643
Number of used sentences in train = 313
Total loss for epoch 7: 519.217096
	Epoch 8....
Epoch has taken 0:00:17.391388
Number of used sentences in train = 313
Total loss for epoch 8: 515.792413
	Epoch 9....
Epoch has taken 0:00:17.413363
Number of used sentences in train = 313
Total loss for epoch 9: 513.704582
	Epoch 10....
Epoch has taken 0:00:17.396297
Number of used sentences in train = 313
Total loss for epoch 10: 512.797595
	Epoch 11....
Epoch has taken 0:00:17.397725
Number of used sentences in train = 313
Total loss for epoch 11: 510.930159
	Epoch 12....
Epoch has taken 0:00:18.239411
Number of used sentences in train = 313
Total loss for epoch 12: 509.257403
	Epoch 13....
Epoch has taken 0:00:19.281948
Number of used sentences in train = 313
Total loss for epoch 13: 508.211797
	Epoch 14....
Epoch has taken 0:00:19.277360
Number of used sentences in train = 313
Total loss for epoch 14: 508.590583
Epoch has taken 0:00:19.289134

==================================================================================================
	Training time : 0:46:30.892000
==================================================================================================
	Identification : 0.484

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 73)
  (w_embeddings): Embedding(1133, 63)
  (lstm): LSTM(136, 43, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8596.692262
validation loss after epoch 0 : 752.339615
	Epoch 1....
Epoch has taken 0:01:54.116151
Number of used sentences in train = 2074
Total loss for epoch 1: 5703.933575
validation loss after epoch 1 : 737.489975
	Epoch 2....
Epoch has taken 0:01:52.395141
