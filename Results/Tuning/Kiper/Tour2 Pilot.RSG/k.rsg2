INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 16, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 67, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 51, 'lstmDropout': 0.1, 'denseActivation': 'tanh', 'wordDim': 63, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11354.017598
validation loss after epoch 0 : 866.454756
	Epoch 1....
Epoch has taken 0:02:55.623366
Number of used sentences in train = 2811
Total loss for epoch 1: 7612.758761
validation loss after epoch 1 : 843.044730
	Epoch 2....
Epoch has taken 0:03:01.681095
Number of used sentences in train = 2811
Total loss for epoch 2: 6858.608082
validation loss after epoch 2 : 803.027593
	Epoch 3....
Epoch has taken 0:03:01.995966
Number of used sentences in train = 2811
Total loss for epoch 3: 6385.488079
validation loss after epoch 3 : 793.367005
	Epoch 4....
Epoch has taken 0:03:21.648668
Number of used sentences in train = 2811
Total loss for epoch 4: 6085.406482
validation loss after epoch 4 : 774.751086
	Epoch 5....
Epoch has taken 0:03:20.611014
Number of used sentences in train = 2811
Total loss for epoch 5: 5739.886095
validation loss after epoch 5 : 838.206632
	Epoch 6....
Epoch has taken 0:03:18.155966
Number of used sentences in train = 2811
Total loss for epoch 6: 5491.828758
validation loss after epoch 6 : 842.585631
	Epoch 7....
Epoch has taken 0:03:21.509073
Number of used sentences in train = 2811
Total loss for epoch 7: 5348.627263
validation loss after epoch 7 : 819.915338
	Epoch 8....
Epoch has taken 0:03:22.794617
Number of used sentences in train = 2811
Total loss for epoch 8: 5185.333820
validation loss after epoch 8 : 845.496372
	Epoch 9....
Epoch has taken 0:03:19.909951
Number of used sentences in train = 2811
Total loss for epoch 9: 5106.997238
validation loss after epoch 9 : 860.032624
	Epoch 10....
Epoch has taken 0:03:10.440790
Number of used sentences in train = 2811
Total loss for epoch 10: 5015.988879
validation loss after epoch 10 : 868.138370
	Epoch 11....
Epoch has taken 0:03:04.855286
Number of used sentences in train = 2811
Total loss for epoch 11: 4903.079279
validation loss after epoch 11 : 933.219282
	Epoch 12....
Epoch has taken 0:02:58.429049
Number of used sentences in train = 2811
Total loss for epoch 12: 4866.431682
validation loss after epoch 12 : 937.839239
	Epoch 13....
Epoch has taken 0:02:52.214844
Number of used sentences in train = 2811
Total loss for epoch 13: 4836.422326
validation loss after epoch 13 : 963.357719
	Epoch 14....
Epoch has taken 0:02:55.283799
Number of used sentences in train = 2811
Total loss for epoch 14: 4807.044559
validation loss after epoch 14 : 982.480989
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1882, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:01.131468
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1269.924790
	Epoch 1....
Epoch has taken 0:00:19.003252
Number of used sentences in train = 313
Total loss for epoch 1: 793.844377
	Epoch 2....
Epoch has taken 0:00:17.831305
Number of used sentences in train = 313
Total loss for epoch 2: 716.926477
	Epoch 3....
Epoch has taken 0:00:17.845156
Number of used sentences in train = 313
Total loss for epoch 3: 621.397684
	Epoch 4....
Epoch has taken 0:00:17.871917
Number of used sentences in train = 313
Total loss for epoch 4: 597.966245
	Epoch 5....
Epoch has taken 0:00:17.843241
Number of used sentences in train = 313
Total loss for epoch 5: 571.770360
	Epoch 6....
Epoch has taken 0:00:17.837699
Number of used sentences in train = 313
Total loss for epoch 6: 558.664540
	Epoch 7....
Epoch has taken 0:00:17.868115
Number of used sentences in train = 313
Total loss for epoch 7: 553.820126
	Epoch 8....
Epoch has taken 0:00:17.860470
Number of used sentences in train = 313
Total loss for epoch 8: 544.278940
	Epoch 9....
Epoch has taken 0:00:17.865514
Number of used sentences in train = 313
Total loss for epoch 9: 539.025276
	Epoch 10....
Epoch has taken 0:00:17.842403
Number of used sentences in train = 313
Total loss for epoch 10: 536.994028
	Epoch 11....
Epoch has taken 0:00:17.858421
Number of used sentences in train = 313
Total loss for epoch 11: 537.272140
	Epoch 12....
Epoch has taken 0:00:17.866143
Number of used sentences in train = 313
Total loss for epoch 12: 536.162639
	Epoch 13....
Epoch has taken 0:00:17.865078
Number of used sentences in train = 313
Total loss for epoch 13: 535.663985
	Epoch 14....
Epoch has taken 0:00:17.878765
Number of used sentences in train = 313
Total loss for epoch 14: 535.530124
Epoch has taken 0:00:17.856394

==================================================================================================
	Training time : 0:51:41.831185
==================================================================================================
	Identification : 0.467

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9088.608989
validation loss after epoch 0 : 654.254293
	Epoch 1....
Epoch has taken 0:01:56.610841
Number of used sentences in train = 2074
Total loss for epoch 1: 5451.230651
validation loss after epoch 1 : 649.104341
	Epoch 2....
Epoch has taken 0:01:56.564900
Number of used sentences in train = 2074
Total loss for epoch 2: 4799.612832
validation loss after epoch 2 : 650.005339
	Epoch 3....
Epoch has taken 0:01:55.600032
Number of used sentences in train = 2074
Total loss for epoch 3: 4382.826671
validation loss after epoch 3 : 655.479226
	Epoch 4....
Epoch has taken 0:01:56.793942
Number of used sentences in train = 2074
Total loss for epoch 4: 4005.620282
validation loss after epoch 4 : 688.440311
	Epoch 5....
Epoch has taken 0:01:56.968779
Number of used sentences in train = 2074
Total loss for epoch 5: 3761.346738
validation loss after epoch 5 : 675.778915
	Epoch 6....
Epoch has taken 0:02:14.800494
Number of used sentences in train = 2074
Total loss for epoch 6: 3553.358722
validation loss after epoch 6 : 748.266921
	Epoch 7....
Epoch has taken 0:01:56.825524
Number of used sentences in train = 2074
Total loss for epoch 7: 3448.443132
validation loss after epoch 7 : 767.692205
	Epoch 8....
Epoch has taken 0:01:55.682286
Number of used sentences in train = 2074
Total loss for epoch 8: 3391.126084
validation loss after epoch 8 : 849.193012
	Epoch 9....
Epoch has taken 0:01:55.698693
Number of used sentences in train = 2074
Total loss for epoch 9: 3345.618516
validation loss after epoch 9 : 779.055673
	Epoch 10....
Epoch has taken 0:01:56.730174
Number of used sentences in train = 2074
Total loss for epoch 10: 3307.953354
validation loss after epoch 10 : 811.269098
	Epoch 11....
Epoch has taken 0:01:56.961793
Number of used sentences in train = 2074
Total loss for epoch 11: 3277.707860
validation loss after epoch 11 : 793.321617
	Epoch 12....
Epoch has taken 0:02:14.800951
Number of used sentences in train = 2074
Total loss for epoch 12: 3273.514832
validation loss after epoch 12 : 820.319923
	Epoch 13....
Epoch has taken 0:01:56.844969
Number of used sentences in train = 2074
Total loss for epoch 13: 3228.031591
validation loss after epoch 13 : 864.134277
	Epoch 14....
Epoch has taken 0:01:55.595631
Number of used sentences in train = 2074
Total loss for epoch 14: 3220.951910
validation loss after epoch 14 : 977.097102
	TransitionClassifier(
  (p_embeddings): Embedding(18, 67)
  (w_embeddings): Embedding(1680, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:55.583488
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1405.465862
	Epoch 1....
Epoch has taken 0:00:11.897775
Number of used sentences in train = 231
Total loss for epoch 1: 601.413521
	Epoch 2....
Epoch has taken 0:00:11.893300
Number of used sentences in train = 231
Total loss for epoch 2: 437.031852
	Epoch 3....
Epoch has taken 0:00:11.880507
Number of used sentences in train = 231
Total loss for epoch 3: 407.062737
	Epoch 4....
Epoch has taken 0:00:11.881145
Number of used sentences in train = 231
Total loss for epoch 4: 382.370567
	Epoch 5....
Epoch has taken 0:00:11.875211
Number of used sentences in train = 231
Total loss for epoch 5: 366.484039
	Epoch 6....
Epoch has taken 0:00:11.900420
Number of used sentences in train = 231
Total loss for epoch 6: 356.915113
	Epoch 7....
Epoch has taken 0:00:11.875967
Number of used sentences in train = 231
Total loss for epoch 7: 355.793580
	Epoch 8....
Epoch has taken 0:00:12.966240
Number of used sentences in train = 231
Total loss for epoch 8: 351.996756
	Epoch 9....
Epoch has taken 0:00:13.663333
Number of used sentences in train = 231
Total loss for epoch 9: 352.513711
	Epoch 10....
Epoch has taken 0:00:13.665415
Number of used sentences in train = 231
Total loss for epoch 10: 349.887915
	Epoch 11....
Epoch has taken 0:00:13.673955
Number of used sentences in train = 231
Total loss for epoch 11: 350.468814
	Epoch 12....
Epoch has taken 0:00:13.681029
Number of used sentences in train = 231
Total loss for epoch 12: 348.969832
	Epoch 13....
Epoch has taken 0:00:13.683261
Number of used sentences in train = 231
Total loss for epoch 13: 348.274477
	Epoch 14....
Epoch has taken 0:00:13.670497
Number of used sentences in train = 231
Total loss for epoch 14: 348.907451
Epoch has taken 0:00:13.676081

==================================================================================================
	Training time : 0:32:54.282558
==================================================================================================
	Identification : 0.318

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 26022.889513
validation loss after epoch 0 : 1200.788921
	Epoch 1....
Epoch has taken 0:03:46.697885
Number of used sentences in train = 3226
Total loss for epoch 1: 10242.427515
validation loss after epoch 1 : 1095.412873
	Epoch 2....
Epoch has taken 0:03:45.798140
Number of used sentences in train = 3226
Total loss for epoch 2: 9129.637850
validation loss after epoch 2 : 1078.736054
	Epoch 3....
Epoch has taken 0:03:47.452646
Number of used sentences in train = 3226
Total loss for epoch 3: 8541.357185
validation loss after epoch 3 : 1094.847199
	Epoch 4....
Epoch has taken 0:03:47.282175
Number of used sentences in train = 3226
Total loss for epoch 4: 8120.349236
validation loss after epoch 4 : 1108.893417
	Epoch 5....
Epoch has taken 0:03:47.538732
Number of used sentences in train = 3226
Total loss for epoch 5: 7791.531427
validation loss after epoch 5 : 1114.918948
	Epoch 6....
Epoch has taken 0:03:59.625165
Number of used sentences in train = 3226
Total loss for epoch 6: 7506.829431
validation loss after epoch 6 : 1186.920447
	Epoch 7....
Epoch has taken 0:03:49.024200
Number of used sentences in train = 3226
Total loss for epoch 7: 7278.070315
validation loss after epoch 7 : 1286.173839
	Epoch 8....
Epoch has taken 0:03:47.476715
Number of used sentences in train = 3226
Total loss for epoch 8: 7126.362793
validation loss after epoch 8 : 1192.159668
	Epoch 9....
Epoch has taken 0:03:47.389764
Number of used sentences in train = 3226
Total loss for epoch 9: 6967.074001
validation loss after epoch 9 : 1284.107947
	Epoch 10....
Epoch has taken 0:03:44.884381
Number of used sentences in train = 3226
Total loss for epoch 10: 6898.397374
validation loss after epoch 10 : 1313.128130
	Epoch 11....
Epoch has taken 0:03:44.930297
Number of used sentences in train = 3226
Total loss for epoch 11: 6796.812208
validation loss after epoch 11 : 1301.751980
	Epoch 12....
Epoch has taken 0:03:46.929450
Number of used sentences in train = 3226
Total loss for epoch 12: 6689.995410
validation loss after epoch 12 : 1379.871936
	Epoch 13....
Epoch has taken 0:03:47.548953
Number of used sentences in train = 3226
Total loss for epoch 13: 6632.245554
validation loss after epoch 13 : 1442.085934
	Epoch 14....
Epoch has taken 0:03:47.470200
Number of used sentences in train = 3226
Total loss for epoch 14: 6560.946379
validation loss after epoch 14 : 1464.611978
	TransitionClassifier(
  (p_embeddings): Embedding(13, 67)
  (w_embeddings): Embedding(3369, 63)
  (lstm): LSTM(130, 51, num_layers=2, dropout=0.1, bidirectional=True)
  (linear1): Linear(in_features=816, out_features=16, bias=True)
  (linear2): Linear(in_features=16, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:47.211514
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1549.515919
	Epoch 1....
Epoch has taken 0:00:22.027083
Number of used sentences in train = 359
Total loss for epoch 1: 983.021814
	Epoch 2....
Epoch has taken 0:00:22.013246
Number of used sentences in train = 359
Total loss for epoch 2: 880.812747
	Epoch 3....
Epoch has taken 0:00:22.013714
Number of used sentences in train = 359
Total loss for epoch 3: 814.890700
	Epoch 4....
Epoch has taken 0:00:22.022840
Number of used sentences in train = 359
Total loss for epoch 4: 772.573687
	Epoch 5....
Epoch has taken 0:00:22.022840
Number of used sentences in train = 359
Total loss for epoch 5: 739.772774
	Epoch 6....
Epoch has taken 0:00:22.012955
Number of used sentences in train = 359
Total loss for epoch 6: 716.198056
	Epoch 7....
Epoch has taken 0:00:21.997236
Number of used sentences in train = 359
Total loss for epoch 7: 700.252350
	Epoch 8....
Epoch has taken 0:00:22.010401
Number of used sentences in train = 359
Total loss for epoch 8: 705.989960
	Epoch 9....
Epoch has taken 0:00:22.011709
Number of used sentences in train = 359
Total loss for epoch 9: 684.114018
	Epoch 10....
Epoch has taken 0:00:22.046033
Number of used sentences in train = 359
Total loss for epoch 10: 684.423565
	Epoch 11....
Epoch has taken 0:00:22.055335
Number of used sentences in train = 359
Total loss for epoch 11: 678.957939
	Epoch 12....
Epoch has taken 0:00:22.021893
Number of used sentences in train = 359
Total loss for epoch 12: 683.734787
	Epoch 13....
Epoch has taken 0:00:22.037606
Number of used sentences in train = 359
Total loss for epoch 13: 674.456145
	Epoch 14....
Epoch has taken 0:00:22.013104
Number of used sentences in train = 359
Total loss for epoch 14: 674.284725
Epoch has taken 0:00:22.029987

==================================================================================================
	Training time : 1:02:28.259104
==================================================================================================
	Identification : 0.023

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 31, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 64, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 47, 'lstmDropout': 0.21, 'denseActivation': 'tanh', 'wordDim': 75, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(1177, 75)
  (lstm): LSTM(139, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11142.021942
validation loss after epoch 0 : 1006.895586
	Epoch 1....
Epoch has taken 0:02:38.374666
Number of used sentences in train = 2811
Total loss for epoch 1: 8052.755998
validation loss after epoch 1 : 917.688789
	Epoch 2....
Epoch has taken 0:02:38.562452
Number of used sentences in train = 2811
Total loss for epoch 2: 7146.420624
validation loss after epoch 2 : 922.998004
	Epoch 3....
Epoch has taken 0:02:38.536604
Number of used sentences in train = 2811
Total loss for epoch 3: 6660.367051
validation loss after epoch 3 : 924.421558
	Epoch 4....
Epoch has taken 0:02:38.312208
Number of used sentences in train = 2811
Total loss for epoch 4: 6248.584931
validation loss after epoch 4 : 991.975465
	Epoch 5....
Epoch has taken 0:02:37.046729
Number of used sentences in train = 2811
Total loss for epoch 5: 5956.787504
validation loss after epoch 5 : 971.692341
	Epoch 6....
Epoch has taken 0:02:38.626905
Number of used sentences in train = 2811
Total loss for epoch 6: 5703.314976
validation loss after epoch 6 : 996.790957
	Epoch 7....
Epoch has taken 0:02:38.340411
Number of used sentences in train = 2811
Total loss for epoch 7: 5510.984393
validation loss after epoch 7 : 1027.438100
	Epoch 8....
Epoch has taken 0:03:03.866941
Number of used sentences in train = 2811
Total loss for epoch 8: 5337.227575
validation loss after epoch 8 : 1070.497535
	Epoch 9....
Epoch has taken 0:02:38.326353
Number of used sentences in train = 2811
Total loss for epoch 9: 5202.601978
validation loss after epoch 9 : 1095.293826
	Epoch 10....
Epoch has taken 0:02:36.765085
Number of used sentences in train = 2811
Total loss for epoch 10: 5075.584731
validation loss after epoch 10 : 1089.612742
	Epoch 11....
Epoch has taken 0:02:37.605888
Number of used sentences in train = 2811
Total loss for epoch 11: 4977.084787
validation loss after epoch 11 : 1190.435355
	Epoch 12....
Epoch has taken 0:02:54.723092
Number of used sentences in train = 2811
Total loss for epoch 12: 4930.873491
validation loss after epoch 12 : 1181.166977
	Epoch 13....
Epoch has taken 0:02:53.654503
Number of used sentences in train = 2811
Total loss for epoch 13: 4851.028578
validation loss after epoch 13 : 1171.362209
	Epoch 14....
Epoch has taken 0:02:40.578798
Number of used sentences in train = 2811
Total loss for epoch 14: 4789.324841
validation loss after epoch 14 : 1225.497402
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(1177, 75)
  (lstm): LSTM(139, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:43.432121
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1683.264461
	Epoch 1....
Epoch has taken 0:00:18.507387
Number of used sentences in train = 313
Total loss for epoch 1: 786.789370
	Epoch 2....
Epoch has taken 0:00:18.438238
Number of used sentences in train = 313
Total loss for epoch 2: 650.742375
	Epoch 3....
Epoch has taken 0:00:18.355516
Number of used sentences in train = 313
Total loss for epoch 3: 576.359708
	Epoch 4....
Epoch has taken 0:00:20.117452
Number of used sentences in train = 313
Total loss for epoch 4: 550.226429
	Epoch 5....
Epoch has taken 0:00:20.014189
Number of used sentences in train = 313
Total loss for epoch 5: 536.388049
	Epoch 6....
Epoch has taken 0:00:20.107774
Number of used sentences in train = 313
Total loss for epoch 6: 525.698998
	Epoch 7....
Epoch has taken 0:00:20.250388
Number of used sentences in train = 313
Total loss for epoch 7: 520.516490
	Epoch 8....
Epoch has taken 0:00:19.210303
Number of used sentences in train = 313
Total loss for epoch 8: 517.546696
	Epoch 9....
Epoch has taken 0:00:19.991702
Number of used sentences in train = 313
Total loss for epoch 9: 515.057774
	Epoch 10....
Epoch has taken 0:00:20.089018
Number of used sentences in train = 313
Total loss for epoch 10: 513.315611
	Epoch 11....
Epoch has taken 0:00:19.932532
Number of used sentences in train = 313
Total loss for epoch 11: 513.577127
	Epoch 12....
Epoch has taken 0:00:20.052539
Number of used sentences in train = 313
Total loss for epoch 12: 510.838812
	Epoch 13....
Epoch has taken 0:00:20.275899
Number of used sentences in train = 313
Total loss for epoch 13: 509.751998
	Epoch 14....
Epoch has taken 0:00:20.344930
Number of used sentences in train = 313
Total loss for epoch 14: 508.879414
Epoch has taken 0:00:20.484810

==================================================================================================
	Training time : 0:45:33.415538
==================================================================================================
	Identification : 0.489

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(1133, 75)
  (lstm): LSTM(139, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8668.342105
validation loss after epoch 0 : 818.679325
	Epoch 1....
Epoch has taken 0:02:11.346020
Number of used sentences in train = 2074
Total loss for epoch 1: 5954.922167
validation loss after epoch 1 : 729.783317
	Epoch 2....
Epoch has taken 0:02:11.202657
Number of used sentences in train = 2074
Total loss for epoch 2: 5150.985735
validation loss after epoch 2 : 718.465812
	Epoch 3....
Epoch has taken 0:02:10.637716
Number of used sentences in train = 2074
Total loss for epoch 3: 4594.873018
validation loss after epoch 3 : 772.895562
	Epoch 4....
Epoch has taken 0:02:01.880948
Number of used sentences in train = 2074
Total loss for epoch 4: 4192.726677
validation loss after epoch 4 : 714.037744
	Epoch 5....
Epoch has taken 0:01:58.680663
Number of used sentences in train = 2074
Total loss for epoch 5: 3896.663323
validation loss after epoch 5 : 732.688493
	Epoch 6....
Epoch has taken 0:02:00.580236
Number of used sentences in train = 2074
Total loss for epoch 6: 3696.110905
validation loss after epoch 6 : 766.825808
	Epoch 7....
Epoch has taken 0:02:00.721941
Number of used sentences in train = 2074
Total loss for epoch 7: 3554.228550
validation loss after epoch 7 : 810.973122
	Epoch 8....
Epoch has taken 0:02:00.785440
Number of used sentences in train = 2074
Total loss for epoch 8: 3462.522103
validation loss after epoch 8 : 784.614089
	Epoch 9....
Epoch has taken 0:01:52.353281
Number of used sentences in train = 2074
Total loss for epoch 9: 3382.502060
validation loss after epoch 9 : 829.563624
	Epoch 10....
Epoch has taken 0:01:49.959129
Number of used sentences in train = 2074
Total loss for epoch 10: 3327.965361
validation loss after epoch 10 : 838.402325
	Epoch 11....
Epoch has taken 0:01:50.092987
Number of used sentences in train = 2074
Total loss for epoch 11: 3288.296224
validation loss after epoch 11 : 852.347931
	Epoch 12....
Epoch has taken 0:01:51.095483
Number of used sentences in train = 2074
Total loss for epoch 12: 3265.772150
validation loss after epoch 12 : 859.806399
	Epoch 13....
Epoch has taken 0:01:50.794197
Number of used sentences in train = 2074
Total loss for epoch 13: 3243.457647
validation loss after epoch 13 : 872.725893
	Epoch 14....
Epoch has taken 0:01:51.092657
Number of used sentences in train = 2074
Total loss for epoch 14: 3230.283590
validation loss after epoch 14 : 876.989457
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(1133, 75)
  (lstm): LSTM(139, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.916519
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1144.549300
	Epoch 1....
Epoch has taken 0:00:11.211657
Number of used sentences in train = 231
Total loss for epoch 1: 642.709591
	Epoch 2....
Epoch has taken 0:00:11.199981
Number of used sentences in train = 231
Total loss for epoch 2: 469.161691
	Epoch 3....
Epoch has taken 0:00:11.161378
Number of used sentences in train = 231
Total loss for epoch 3: 403.306271
	Epoch 4....
Epoch has taken 0:00:11.181927
Number of used sentences in train = 231
Total loss for epoch 4: 379.452717
	Epoch 5....
Epoch has taken 0:00:11.217319
Number of used sentences in train = 231
Total loss for epoch 5: 360.268245
	Epoch 6....
Epoch has taken 0:00:11.238624
Number of used sentences in train = 231
Total loss for epoch 6: 353.491242
	Epoch 7....
Epoch has taken 0:00:11.152028
Number of used sentences in train = 231
Total loss for epoch 7: 350.891847
	Epoch 8....
Epoch has taken 0:00:11.188225
Number of used sentences in train = 231
Total loss for epoch 8: 349.424271
	Epoch 9....
Epoch has taken 0:00:11.233214
Number of used sentences in train = 231
Total loss for epoch 9: 348.490529
	Epoch 10....
Epoch has taken 0:00:11.184193
Number of used sentences in train = 231
Total loss for epoch 10: 347.984733
	Epoch 11....
Epoch has taken 0:00:11.187650
Number of used sentences in train = 231
Total loss for epoch 11: 347.580831
	Epoch 12....
Epoch has taken 0:00:11.219567
Number of used sentences in train = 231
Total loss for epoch 12: 347.148687
	Epoch 13....
Epoch has taken 0:00:11.228589
Number of used sentences in train = 231
Total loss for epoch 13: 346.917177
	Epoch 14....
Epoch has taken 0:00:11.227417
Number of used sentences in train = 231
Total loss for epoch 14: 346.737655
Epoch has taken 0:00:11.213632

==================================================================================================
	Training time : 0:32:20.524618
==================================================================================================
	Identification : 0.205

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 64)
  (w_embeddings): Embedding(1202, 75)
  (lstm): LSTM(139, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14772.883386
validation loss after epoch 0 : 1386.115379
	Epoch 1....
Epoch has taken 0:03:39.212779
Number of used sentences in train = 3226
Total loss for epoch 1: 11669.636737
validation loss after epoch 1 : 1346.846972
	Epoch 2....
Epoch has taken 0:03:39.970154
Number of used sentences in train = 3226
Total loss for epoch 2: 10622.721882
validation loss after epoch 2 : 1298.722087
	Epoch 3....
Epoch has taken 0:03:39.207837
Number of used sentences in train = 3226
Total loss for epoch 3: 9939.334199
validation loss after epoch 3 : 1339.884467
	Epoch 4....
Epoch has taken 0:04:27.072868
Number of used sentences in train = 3226
Total loss for epoch 4: 9415.290201
validation loss after epoch 4 : 1386.077856
	Epoch 5....
Epoch has taken 0:03:44.654674
Number of used sentences in train = 3226
Total loss for epoch 5: 9026.828980
validation loss after epoch 5 : 1355.002375
	Epoch 6....
Epoch has taken 0:03:46.228528
Number of used sentences in train = 3226
Total loss for epoch 6: 8676.274516
validation loss after epoch 6 : 1385.058783
	Epoch 7....
Epoch has taken 0:03:40.358985
Number of used sentences in train = 3226
Total loss for epoch 7: 8395.758958
validation loss after epoch 7 : 1427.847253
	Epoch 8....
Epoch has taken 0:03:41.077636
Number of used sentences in train = 3226
Total loss for epoch 8: 8158.462806
validation loss after epoch 8 : 1481.206566
	Epoch 9....
Epoch has taken 0:03:43.432554
Number of used sentences in train = 3226
Total loss for epoch 9: 7936.237833
validation loss after epoch 9 : 1484.908655
	Epoch 10....
Epoch has taken 0:03:37.359002
Number of used sentences in train = 3226
Total loss for epoch 10: 7733.700393
validation loss after epoch 10 : 1563.598288
	Epoch 11....
Epoch has taken 0:03:39.680390
Number of used sentences in train = 3226
Total loss for epoch 11: 7559.253056
validation loss after epoch 11 : 1554.226032
	Epoch 12....
Epoch has taken 0:04:26.731926
Number of used sentences in train = 3226
Total loss for epoch 12: 7384.987767
validation loss after epoch 12 : 1631.944476
	Epoch 13....
Epoch has taken 0:03:52.560214
Number of used sentences in train = 3226
Total loss for epoch 13: 7237.715949
validation loss after epoch 13 : 1631.159224
	Epoch 14....
Epoch has taken 0:03:44.137546
Number of used sentences in train = 3226
Total loss for epoch 14: 7114.042861
validation loss after epoch 14 : 1718.382517
	TransitionClassifier(
  (p_embeddings): Embedding(13, 64)
  (w_embeddings): Embedding(1202, 75)
  (lstm): LSTM(139, 47, bidirectional=True)
  (linear1): Linear(in_features=752, out_features=31, bias=True)
  (linear2): Linear(in_features=31, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:41.432654
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1788.121599
	Epoch 1....
Epoch has taken 0:00:21.637949
Number of used sentences in train = 359
Total loss for epoch 1: 1141.968592
	Epoch 2....
Epoch has taken 0:00:21.496065
Number of used sentences in train = 359
Total loss for epoch 2: 963.023751
	Epoch 3....
Epoch has taken 0:00:22.610555
Number of used sentences in train = 359
Total loss for epoch 3: 876.045393
	Epoch 4....
Epoch has taken 0:00:22.656045
Number of used sentences in train = 359
Total loss for epoch 4: 799.591456
	Epoch 5....
Epoch has taken 0:00:22.744364
Number of used sentences in train = 359
Total loss for epoch 5: 751.350226
	Epoch 6....
Epoch has taken 0:00:22.749244
Number of used sentences in train = 359
Total loss for epoch 6: 729.722473
	Epoch 7....
Epoch has taken 0:00:22.652004
Number of used sentences in train = 359
Total loss for epoch 7: 711.717168
	Epoch 8....
Epoch has taken 0:00:22.714991
Number of used sentences in train = 359
Total loss for epoch 8: 699.992571
	Epoch 9....
Epoch has taken 0:00:22.630200
Number of used sentences in train = 359
Total loss for epoch 9: 689.713056
	Epoch 10....
Epoch has taken 0:00:22.642452
Number of used sentences in train = 359
Total loss for epoch 10: 682.512882
	Epoch 11....
Epoch has taken 0:00:22.642440
Number of used sentences in train = 359
Total loss for epoch 11: 679.033518
	Epoch 12....
Epoch has taken 0:00:22.462391
Number of used sentences in train = 359
Total loss for epoch 12: 681.016378
	Epoch 13....
Epoch has taken 0:00:22.603606
Number of used sentences in train = 359
Total loss for epoch 13: 675.247146
	Epoch 14....
Epoch has taken 0:00:22.579665
Number of used sentences in train = 359
Total loss for epoch 14: 674.750162
Epoch has taken 0:00:22.806459

==================================================================================================
	Training time : 1:02:41.409127
==================================================================================================
	Identification : 0.079

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 62, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 33, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 109, 'lstmDropout': 0.29, 'denseActivation': 'tanh', 'wordDim': 153, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1882, 153)
  (lstm): LSTM(186, 109, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11794.131860
validation loss after epoch 0 : 944.779778
	Epoch 1....
Epoch has taken 0:03:19.341476
Number of used sentences in train = 2811
Total loss for epoch 1: 8308.993670
validation loss after epoch 1 : 885.083933
	Epoch 2....
Epoch has taken 0:03:18.957762
Number of used sentences in train = 2811
Total loss for epoch 2: 7376.927814
validation loss after epoch 2 : 864.240545
	Epoch 3....
Epoch has taken 0:02:52.397894
Number of used sentences in train = 2811
Total loss for epoch 3: 6825.890624
validation loss after epoch 3 : 869.473319
	Epoch 4....
Epoch has taken 0:02:50.791482
Number of used sentences in train = 2811
Total loss for epoch 4: 6349.918790
validation loss after epoch 4 : 1028.074400
	Epoch 5....
Epoch has taken 0:02:52.634611
Number of used sentences in train = 2811
Total loss for epoch 5: 5962.503098
validation loss after epoch 5 : 900.947297
	Epoch 6....
Epoch has taken 0:02:55.378088
Number of used sentences in train = 2811
Total loss for epoch 6: 5666.622286
validation loss after epoch 6 : 915.251664
	Epoch 7....
Epoch has taken 0:02:52.333992
Number of used sentences in train = 2811
Total loss for epoch 7: 5416.378963
validation loss after epoch 7 : 916.928079
	Epoch 8....
Epoch has taken 0:02:51.120494
Number of used sentences in train = 2811
Total loss for epoch 8: 5228.049940
validation loss after epoch 8 : 961.696887
	Epoch 9....
Epoch has taken 0:02:51.040746
Number of used sentences in train = 2811
Total loss for epoch 9: 5081.757550
validation loss after epoch 9 : 998.134391
	Epoch 10....
Epoch has taken 0:02:53.164682
Number of used sentences in train = 2811
Total loss for epoch 10: 4973.951049
validation loss after epoch 10 : 998.784069
	Epoch 11....
Epoch has taken 0:03:19.251584
Number of used sentences in train = 2811
Total loss for epoch 11: 4888.996240
validation loss after epoch 11 : 1040.050411
	Epoch 12....
Epoch has taken 0:02:52.585858
Number of used sentences in train = 2811
Total loss for epoch 12: 4825.870416
validation loss after epoch 12 : 1049.747381
	Epoch 13....
Epoch has taken 0:02:50.914305
Number of used sentences in train = 2811
Total loss for epoch 13: 4772.678815
validation loss after epoch 13 : 1040.025057
	Epoch 14....
Epoch has taken 0:02:50.979732
Number of used sentences in train = 2811
Total loss for epoch 14: 4745.605143
validation loss after epoch 14 : 1078.011459
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1882, 153)
  (lstm): LSTM(186, 109, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.531875
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1365.884794
	Epoch 1....
Epoch has taken 0:00:18.749997
Number of used sentences in train = 313
Total loss for epoch 1: 877.959821
	Epoch 2....
Epoch has taken 0:00:18.365887
Number of used sentences in train = 313
Total loss for epoch 2: 718.229201
	Epoch 3....
Epoch has taken 0:00:18.391753
Number of used sentences in train = 313
Total loss for epoch 3: 667.986760
	Epoch 4....
Epoch has taken 0:00:18.407111
Number of used sentences in train = 313
Total loss for epoch 4: 628.483178
	Epoch 5....
Epoch has taken 0:00:18.489474
Number of used sentences in train = 313
Total loss for epoch 5: 592.386612
	Epoch 6....
Epoch has taken 0:00:18.516914
Number of used sentences in train = 313
Total loss for epoch 6: 570.661561
	Epoch 7....
Epoch has taken 0:00:18.452089
Number of used sentences in train = 313
Total loss for epoch 7: 569.287718
	Epoch 8....
Epoch has taken 0:00:18.457892
Number of used sentences in train = 313
Total loss for epoch 8: 554.332694
	Epoch 9....
Epoch has taken 0:00:18.416142
Number of used sentences in train = 313
Total loss for epoch 9: 540.989076
	Epoch 10....
Epoch has taken 0:00:18.593977
Number of used sentences in train = 313
Total loss for epoch 10: 536.359632
	Epoch 11....
Epoch has taken 0:00:18.408630
Number of used sentences in train = 313
Total loss for epoch 11: 530.948795
	Epoch 12....
Epoch has taken 0:00:18.467885
Number of used sentences in train = 313
Total loss for epoch 12: 528.667090
	Epoch 13....
Epoch has taken 0:00:18.317231
Number of used sentences in train = 313
Total loss for epoch 13: 526.617468
	Epoch 14....
Epoch has taken 0:00:18.378993
Number of used sentences in train = 313
Total loss for epoch 14: 525.190734
Epoch has taken 0:00:18.585963

==================================================================================================
	Training time : 0:49:00.948664
==================================================================================================
	Identification : 0.465

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1680, 153)
  (lstm): LSTM(186, 109, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7556.964170
validation loss after epoch 0 : 676.732678
	Epoch 1....
Epoch has taken 0:01:56.450433
Number of used sentences in train = 2074
Total loss for epoch 1: 4873.201945
validation loss after epoch 1 : 645.943393
	Epoch 2....
Epoch has taken 0:01:56.857373
Number of used sentences in train = 2074
Total loss for epoch 2: 4083.067527
validation loss after epoch 2 : 661.801991
	Epoch 3....
Epoch has taken 0:01:58.202606
Number of used sentences in train = 2074
Total loss for epoch 3: 3618.343017
validation loss after epoch 3 : 744.464866
	Epoch 4....
Epoch has taken 0:01:58.175486
Number of used sentences in train = 2074
Total loss for epoch 4: 3400.500344
validation loss after epoch 4 : 787.346324
	Epoch 5....
Epoch has taken 0:01:58.141195
Number of used sentences in train = 2074
Total loss for epoch 5: 3287.067521
validation loss after epoch 5 : 855.757776
	Epoch 6....
Epoch has taken 0:01:57.862374
Number of used sentences in train = 2074
Total loss for epoch 6: 3242.134283
validation loss after epoch 6 : 858.200249
	Epoch 7....
Epoch has taken 0:01:56.688818
Number of used sentences in train = 2074
Total loss for epoch 7: 3208.075581
validation loss after epoch 7 : 878.939626
	Epoch 8....
Epoch has taken 0:01:56.870059
Number of used sentences in train = 2074
Total loss for epoch 8: 3184.544456
validation loss after epoch 8 : 887.004914
	Epoch 9....
Epoch has taken 0:01:58.208114
Number of used sentences in train = 2074
Total loss for epoch 9: 3175.834502
validation loss after epoch 9 : 905.901865
	Epoch 10....
Epoch has taken 0:01:58.259433
Number of used sentences in train = 2074
Total loss for epoch 10: 3166.698833
validation loss after epoch 10 : 917.884689
	Epoch 11....
Epoch has taken 0:01:58.354131
Number of used sentences in train = 2074
Total loss for epoch 11: 3163.380076
validation loss after epoch 11 : 931.027239
	Epoch 12....
Epoch has taken 0:01:58.123343
Number of used sentences in train = 2074
Total loss for epoch 12: 3160.841585
validation loss after epoch 12 : 942.527854
	Epoch 13....
Epoch has taken 0:01:57.185099
Number of used sentences in train = 2074
Total loss for epoch 13: 3159.046008
validation loss after epoch 13 : 945.546511
	Epoch 14....
Epoch has taken 0:01:57.326657
Number of used sentences in train = 2074
Total loss for epoch 14: 3159.046407
validation loss after epoch 14 : 950.360506
	TransitionClassifier(
  (p_embeddings): Embedding(18, 33)
  (w_embeddings): Embedding(1680, 153)
  (lstm): LSTM(186, 109, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:58.308455
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1243.062718
	Epoch 1....
Epoch has taken 0:00:11.873005
Number of used sentences in train = 231
Total loss for epoch 1: 551.914955
	Epoch 2....
Epoch has taken 0:00:12.269823
Number of used sentences in train = 231
Total loss for epoch 2: 419.324266
	Epoch 3....
Epoch has taken 0:00:12.079570
Number of used sentences in train = 231
Total loss for epoch 3: 367.665958
	Epoch 4....
Epoch has taken 0:00:12.152473
Number of used sentences in train = 231
Total loss for epoch 4: 354.919922
	Epoch 5....
Epoch has taken 0:00:12.050424
Number of used sentences in train = 231
Total loss for epoch 5: 352.254129
	Epoch 6....
Epoch has taken 0:00:12.225553
Number of used sentences in train = 231
Total loss for epoch 6: 349.727362
	Epoch 7....
Epoch has taken 0:00:12.106561
Number of used sentences in train = 231
Total loss for epoch 7: 348.541322
	Epoch 8....
Epoch has taken 0:00:12.175702
Number of used sentences in train = 231
Total loss for epoch 8: 347.694495
	Epoch 9....
Epoch has taken 0:00:12.095324
Number of used sentences in train = 231
Total loss for epoch 9: 347.140155
	Epoch 10....
Epoch has taken 0:00:12.254044
Number of used sentences in train = 231
Total loss for epoch 10: 346.770746
	Epoch 11....
Epoch has taken 0:00:13.829027
Number of used sentences in train = 231
Total loss for epoch 11: 346.582613
	Epoch 12....
Epoch has taken 0:00:13.969982
Number of used sentences in train = 231
Total loss for epoch 12: 346.373917
	Epoch 13....
Epoch has taken 0:00:13.939317
Number of used sentences in train = 231
Total loss for epoch 13: 346.155734
	Epoch 14....
Epoch has taken 0:00:13.792475
Number of used sentences in train = 231
Total loss for epoch 14: 346.104206
Epoch has taken 0:00:14.044539

==================================================================================================
	Training time : 0:32:36.208041
==================================================================================================
	Identification : 0.325

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 33)
  (w_embeddings): Embedding(3369, 153)
  (lstm): LSTM(186, 109, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 24303.063683
validation loss after epoch 0 : 1227.954068
	Epoch 1....
Epoch has taken 0:03:49.610576
Number of used sentences in train = 3226
Total loss for epoch 1: 10267.189703
validation loss after epoch 1 : 1119.200625
	Epoch 2....
Epoch has taken 0:03:44.329091
Number of used sentences in train = 3226
Total loss for epoch 2: 9142.227921
validation loss after epoch 2 : 1119.201287
	Epoch 3....
Epoch has taken 0:03:46.063384
Number of used sentences in train = 3226
Total loss for epoch 3: 8372.204684
validation loss after epoch 3 : 1146.593737
	Epoch 4....
Epoch has taken 0:03:48.829981
Number of used sentences in train = 3226
Total loss for epoch 4: 7839.834750
validation loss after epoch 4 : 1169.761374
	Epoch 5....
Epoch has taken 0:03:44.332163
Number of used sentences in train = 3226
Total loss for epoch 5: 7518.641588
validation loss after epoch 5 : 1194.811266
	Epoch 6....
Epoch has taken 0:03:41.423007
Number of used sentences in train = 3226
Total loss for epoch 6: 7273.270604
validation loss after epoch 6 : 1268.727964
	Epoch 7....
Epoch has taken 0:04:26.643855
Number of used sentences in train = 3226
Total loss for epoch 7: 7055.266756
validation loss after epoch 7 : 1276.271035
	Epoch 8....
Epoch has taken 0:03:51.566996
Number of used sentences in train = 3226
Total loss for epoch 8: 6880.370930
validation loss after epoch 8 : 1259.294483
	Epoch 9....
Epoch has taken 0:03:48.639063
Number of used sentences in train = 3226
Total loss for epoch 9: 6763.087262
validation loss after epoch 9 : 1357.098542
	Epoch 10....
Epoch has taken 0:03:51.363217
Number of used sentences in train = 3226
Total loss for epoch 10: 6644.343505
validation loss after epoch 10 : 1385.475682
	Epoch 11....
Epoch has taken 0:04:25.211690
Number of used sentences in train = 3226
Total loss for epoch 11: 6545.401901
validation loss after epoch 11 : 1433.375021
	Epoch 12....
Epoch has taken 0:03:51.388786
Number of used sentences in train = 3226
Total loss for epoch 12: 6500.065974
validation loss after epoch 12 : 1441.533914
	Epoch 13....
Epoch has taken 0:03:47.962015
Number of used sentences in train = 3226
Total loss for epoch 13: 6449.182814
validation loss after epoch 13 : 1435.646849
	Epoch 14....
Epoch has taken 0:03:47.709179
Number of used sentences in train = 3226
Total loss for epoch 14: 6393.175374
validation loss after epoch 14 : 1529.606663
	TransitionClassifier(
  (p_embeddings): Embedding(13, 33)
  (w_embeddings): Embedding(3369, 153)
  (lstm): LSTM(186, 109, bidirectional=True)
  (linear1): Linear(in_features=1744, out_features=62, bias=True)
  (linear2): Linear(in_features=62, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:26.172641
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1770.595981
	Epoch 1....
Epoch has taken 0:00:22.449071
Number of used sentences in train = 359
Total loss for epoch 1: 1090.880197
	Epoch 2....
Epoch has taken 0:00:22.401938
Number of used sentences in train = 359
Total loss for epoch 2: 924.099111
	Epoch 3....
Epoch has taken 0:00:22.434769
Number of used sentences in train = 359
Total loss for epoch 3: 831.114539
	Epoch 4....
Epoch has taken 0:00:22.354744
Number of used sentences in train = 359
Total loss for epoch 4: 762.278148
	Epoch 5....
Epoch has taken 0:00:22.222046
Number of used sentences in train = 359
Total loss for epoch 5: 731.852073
	Epoch 6....
Epoch has taken 0:00:22.449348
Number of used sentences in train = 359
Total loss for epoch 6: 710.097650
	Epoch 7....
Epoch has taken 0:00:22.437496
Number of used sentences in train = 359
Total loss for epoch 7: 701.622034
	Epoch 8....
Epoch has taken 0:00:22.476762
Number of used sentences in train = 359
Total loss for epoch 8: 692.005662
	Epoch 9....
Epoch has taken 0:00:22.510744
Number of used sentences in train = 359
Total loss for epoch 9: 683.965657
	Epoch 10....
Epoch has taken 0:00:22.510088
Number of used sentences in train = 359
Total loss for epoch 10: 681.222676
	Epoch 11....
Epoch has taken 0:00:22.427213
Number of used sentences in train = 359
Total loss for epoch 11: 678.754866
	Epoch 12....
Epoch has taken 0:00:22.500115
Number of used sentences in train = 359
Total loss for epoch 12: 676.870408
	Epoch 13....
Epoch has taken 0:00:22.463178
Number of used sentences in train = 359
Total loss for epoch 13: 675.033169
	Epoch 14....
Epoch has taken 0:00:22.382476
Number of used sentences in train = 359
Total loss for epoch 14: 674.094392
Epoch has taken 0:00:22.366427

==================================================================================================
	Training time : 1:04:28.299855
==================================================================================================
	Identification : 0.015

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 12, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 21, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 119, 'lstmDropout': 0.31, 'denseActivation': 'tanh', 'wordDim': 125, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(5909, 125)
  (lstm): LSTM(146, 119, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 26130.947075
validation loss after epoch 0 : 2717.473112
	Epoch 1....
Epoch has taken 0:03:31.893927
Number of used sentences in train = 2811
Total loss for epoch 1: 23744.380385
validation loss after epoch 1 : 1743.929496
	Epoch 2....
Epoch has taken 0:03:32.483737
Number of used sentences in train = 2811
Total loss for epoch 2: 13615.608164
validation loss after epoch 2 : 1312.327379
	Epoch 3....
Epoch has taken 0:03:04.806415
Number of used sentences in train = 2811
Total loss for epoch 3: 10078.716109
validation loss after epoch 3 : 1075.661090
	Epoch 4....
Epoch has taken 0:03:03.216484
Number of used sentences in train = 2811
Total loss for epoch 4: 8489.903803
validation loss after epoch 4 : 1072.604792
	Epoch 5....
Epoch has taken 0:03:03.158832
Number of used sentences in train = 2811
Total loss for epoch 5: 7517.907881
validation loss after epoch 5 : 1158.392924
	Epoch 6....
Epoch has taken 0:03:03.502453
Number of used sentences in train = 2811
Total loss for epoch 6: 6862.258921
validation loss after epoch 6 : 1106.215600
	Epoch 7....
Epoch has taken 0:03:05.238269
Number of used sentences in train = 2811
Total loss for epoch 7: 6463.211775
validation loss after epoch 7 : 1203.683583
	Epoch 8....
Epoch has taken 0:03:32.878603
Number of used sentences in train = 2811
Total loss for epoch 8: 6150.240802
validation loss after epoch 8 : 1196.153249
	Epoch 9....
Epoch has taken 0:03:05.451153
Number of used sentences in train = 2811
Total loss for epoch 9: 5919.843248
validation loss after epoch 9 : 1228.841046
	Epoch 10....
Epoch has taken 0:03:03.245537
Number of used sentences in train = 2811
Total loss for epoch 10: 5682.669439
validation loss after epoch 10 : 1379.227989
	Epoch 11....
Epoch has taken 0:03:02.937609
Number of used sentences in train = 2811
Total loss for epoch 11: 5510.020379
validation loss after epoch 11 : 1321.156012
	Epoch 12....
Epoch has taken 0:03:05.556845
Number of used sentences in train = 2811
Total loss for epoch 12: 5424.842936
validation loss after epoch 12 : 1392.495120
	Epoch 13....
Epoch has taken 0:03:05.712010
Number of used sentences in train = 2811
Total loss for epoch 13: 5271.692423
validation loss after epoch 13 : 1355.997294
	Epoch 14....
Epoch has taken 0:03:03.478319
Number of used sentences in train = 2811
Total loss for epoch 14: 5177.217184
validation loss after epoch 14 : 1412.029297
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(5909, 125)
  (lstm): LSTM(146, 119, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:03.618621
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1755.687608
	Epoch 1....
Epoch has taken 0:00:19.661581
Number of used sentences in train = 313
Total loss for epoch 1: 1159.410165
	Epoch 2....
Epoch has taken 0:00:19.673863
Number of used sentences in train = 313
Total loss for epoch 2: 955.813509
	Epoch 3....
Epoch has taken 0:00:19.727353
Number of used sentences in train = 313
Total loss for epoch 3: 826.392773
	Epoch 4....
Epoch has taken 0:00:19.589087
Number of used sentences in train = 313
Total loss for epoch 4: 781.243799
	Epoch 5....
Epoch has taken 0:00:19.828171
Number of used sentences in train = 313
Total loss for epoch 5: 707.577748
	Epoch 6....
Epoch has taken 0:00:19.735474
Number of used sentences in train = 313
Total loss for epoch 6: 656.716744
	Epoch 7....
Epoch has taken 0:00:19.653560
Number of used sentences in train = 313
Total loss for epoch 7: 658.458772
	Epoch 8....
Epoch has taken 0:00:19.697313
Number of used sentences in train = 313
Total loss for epoch 8: 642.590999
	Epoch 9....
Epoch has taken 0:00:19.704282
Number of used sentences in train = 313
Total loss for epoch 9: 640.581127
	Epoch 10....
Epoch has taken 0:00:19.506904
Number of used sentences in train = 313
Total loss for epoch 10: 632.388012
	Epoch 11....
Epoch has taken 0:00:19.863061
Number of used sentences in train = 313
Total loss for epoch 11: 610.771899
	Epoch 12....
Epoch has taken 0:00:19.775645
Number of used sentences in train = 313
Total loss for epoch 12: 601.418703
	Epoch 13....
Epoch has taken 0:00:19.663083
Number of used sentences in train = 313
Total loss for epoch 13: 597.120835
	Epoch 14....
Epoch has taken 0:00:19.694373
Number of used sentences in train = 313
Total loss for epoch 14: 587.806827
Epoch has taken 0:00:19.709858

==================================================================================================
	Training time : 0:52:23.175983
==================================================================================================
	Identification : 0.415

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(5650, 125)
  (lstm): LSTM(146, 119, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 16694.164045
validation loss after epoch 0 : 1370.265224
	Epoch 1....
Epoch has taken 0:02:25.079682
Number of used sentences in train = 2074
Total loss for epoch 1: 9956.203238
validation loss after epoch 1 : 1165.807281
	Epoch 2....
Epoch has taken 0:02:25.649499
Number of used sentences in train = 2074
Total loss for epoch 2: 8089.396759
validation loss after epoch 2 : 1133.263816
	Epoch 3....
Epoch has taken 0:02:05.879894
Number of used sentences in train = 2074
Total loss for epoch 3: 6953.158621
validation loss after epoch 3 : 1058.363438
	Epoch 4....
Epoch has taken 0:02:06.903935
Number of used sentences in train = 2074
Total loss for epoch 4: 6157.272016
validation loss after epoch 4 : 1154.773932
	Epoch 5....
Epoch has taken 0:02:24.411442
Number of used sentences in train = 2074
Total loss for epoch 5: 5556.558737
validation loss after epoch 5 : 1101.795343
	Epoch 6....
Epoch has taken 0:02:03.235583
Number of used sentences in train = 2074
Total loss for epoch 6: 5113.509638
validation loss after epoch 6 : 1158.123346
	Epoch 7....
Epoch has taken 0:02:06.029241
Number of used sentences in train = 2074
Total loss for epoch 7: 4829.390394
validation loss after epoch 7 : 1204.468163
	Epoch 8....
Epoch has taken 0:02:02.462935
Number of used sentences in train = 2074
Total loss for epoch 8: 4522.142787
validation loss after epoch 8 : 1265.365721
	Epoch 9....
Epoch has taken 0:02:03.087996
Number of used sentences in train = 2074
Total loss for epoch 9: 4541.744712
validation loss after epoch 9 : 1319.337336
	Epoch 10....
Epoch has taken 0:02:03.289157
Number of used sentences in train = 2074
Total loss for epoch 10: 4336.904460
validation loss after epoch 10 : 1275.437814
	Epoch 11....
Epoch has taken 0:01:59.579482
Number of used sentences in train = 2074
Total loss for epoch 11: 4177.580487
validation loss after epoch 11 : 1250.400602
	Epoch 12....
Epoch has taken 0:02:02.727063
Number of used sentences in train = 2074
Total loss for epoch 12: 4049.319780
validation loss after epoch 12 : 1328.503950
	Epoch 13....
Epoch has taken 0:02:00.953997
Number of used sentences in train = 2074
Total loss for epoch 13: 3939.745683
validation loss after epoch 13 : 1269.865293
	Epoch 14....
Epoch has taken 0:02:25.730596
Number of used sentences in train = 2074
Total loss for epoch 14: 3917.661142
validation loss after epoch 14 : 1190.856208
	TransitionClassifier(
  (p_embeddings): Embedding(18, 21)
  (w_embeddings): Embedding(5650, 125)
  (lstm): LSTM(146, 119, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:07.037813
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2286.292232
	Epoch 1....
Epoch has taken 0:00:12.898983
Number of used sentences in train = 231
Total loss for epoch 1: 948.846990
	Epoch 2....
Epoch has taken 0:00:12.888836
Number of used sentences in train = 231
Total loss for epoch 2: 804.100386
	Epoch 3....
Epoch has taken 0:00:12.830352
Number of used sentences in train = 231
Total loss for epoch 3: 591.246989
	Epoch 4....
Epoch has taken 0:00:12.916143
Number of used sentences in train = 231
Total loss for epoch 4: 511.458049
	Epoch 5....
Epoch has taken 0:00:12.876756
Number of used sentences in train = 231
Total loss for epoch 5: 493.045229
	Epoch 6....
Epoch has taken 0:00:12.917321
Number of used sentences in train = 231
Total loss for epoch 6: 460.989023
	Epoch 7....
Epoch has taken 0:00:12.866601
Number of used sentences in train = 231
Total loss for epoch 7: 438.218783
	Epoch 8....
Epoch has taken 0:00:12.832412
Number of used sentences in train = 231
Total loss for epoch 8: 413.660844
	Epoch 9....
Epoch has taken 0:00:12.958665
Number of used sentences in train = 231
Total loss for epoch 9: 416.437192
	Epoch 10....
Epoch has taken 0:00:12.848502
Number of used sentences in train = 231
Total loss for epoch 10: 397.306545
	Epoch 11....
Epoch has taken 0:00:12.941391
Number of used sentences in train = 231
Total loss for epoch 11: 394.532459
	Epoch 12....
Epoch has taken 0:00:12.755557
Number of used sentences in train = 231
Total loss for epoch 12: 390.988808
	Epoch 13....
Epoch has taken 0:00:13.036862
Number of used sentences in train = 231
Total loss for epoch 13: 390.714380
	Epoch 14....
Epoch has taken 0:00:12.826135
Number of used sentences in train = 231
Total loss for epoch 14: 390.120594
Epoch has taken 0:00:13.012078

==================================================================================================
	Training time : 0:35:35.810665
==================================================================================================
	Identification : 0.4

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 21)
  (w_embeddings): Embedding(6828, 125)
  (lstm): LSTM(146, 119, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 34901.175705
validation loss after epoch 0 : 2301.853331
	Epoch 1....
Epoch has taken 0:04:39.862156
Number of used sentences in train = 3226
Total loss for epoch 1: 19596.385926
validation loss after epoch 1 : 1938.443135
	Epoch 2....
Epoch has taken 0:04:38.640413
Number of used sentences in train = 3226
Total loss for epoch 2: 16938.870912
validation loss after epoch 2 : 1828.920536
	Epoch 3....
Epoch has taken 0:04:04.659498
Number of used sentences in train = 3226
Total loss for epoch 3: 14459.134916
validation loss after epoch 3 : 1648.853108
	Epoch 4....
Epoch has taken 0:04:02.348161
Number of used sentences in train = 3226
Total loss for epoch 4: 12719.148716
validation loss after epoch 4 : 1691.106750
	Epoch 5....
Epoch has taken 0:03:56.052275
Number of used sentences in train = 3226
Total loss for epoch 5: 11672.190187
validation loss after epoch 5 : 1605.847033
	Epoch 6....
Epoch has taken 0:04:02.626900
Number of used sentences in train = 3226
Total loss for epoch 6: 10830.900762
validation loss after epoch 6 : 1637.325815
	Epoch 7....
Epoch has taken 0:04:04.477242
Number of used sentences in train = 3226
Total loss for epoch 7: 10188.463906
validation loss after epoch 7 : 1658.126342
	Epoch 8....
Epoch has taken 0:04:38.284563
Number of used sentences in train = 3226
Total loss for epoch 8: 9576.161289
validation loss after epoch 8 : 1661.421231
	Epoch 9....
Epoch has taken 0:04:04.353172
Number of used sentences in train = 3226
Total loss for epoch 9: 9059.148556
validation loss after epoch 9 : 1724.424241
	Epoch 10....
Epoch has taken 0:04:02.771600
Number of used sentences in train = 3226
Total loss for epoch 10: 8629.022482
validation loss after epoch 10 : 1815.900503
	Epoch 11....
Epoch has taken 0:04:04.683353
Number of used sentences in train = 3226
Total loss for epoch 11: 8267.738752
validation loss after epoch 11 : 1758.967250
	Epoch 12....
Epoch has taken 0:04:07.821432
Number of used sentences in train = 3226
Total loss for epoch 12: 8075.016482
validation loss after epoch 12 : 1771.194890
	Epoch 13....
Epoch has taken 0:04:05.041511
Number of used sentences in train = 3226
Total loss for epoch 13: 7775.830582
validation loss after epoch 13 : 1941.502987
	Epoch 14....
Epoch has taken 0:04:02.888738
Number of used sentences in train = 3226
Total loss for epoch 14: 7533.429119
validation loss after epoch 14 : 1973.140219
	TransitionClassifier(
  (p_embeddings): Embedding(13, 21)
  (w_embeddings): Embedding(6828, 125)
  (lstm): LSTM(146, 119, num_layers=2, dropout=0.31, bidirectional=True)
  (linear1): Linear(in_features=1904, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:04.620140
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2448.114908
	Epoch 1....
Epoch has taken 0:00:24.196300
Number of used sentences in train = 359
Total loss for epoch 1: 1640.290730
	Epoch 2....
Epoch has taken 0:00:24.156450
Number of used sentences in train = 359
Total loss for epoch 2: 1335.319925
	Epoch 3....
Epoch has taken 0:00:24.275714
Number of used sentences in train = 359
Total loss for epoch 3: 1232.593750
	Epoch 4....
Epoch has taken 0:00:23.998584
Number of used sentences in train = 359
Total loss for epoch 4: 1105.530684
	Epoch 5....
Epoch has taken 0:00:24.147563
Number of used sentences in train = 359
Total loss for epoch 5: 972.927853
	Epoch 6....
Epoch has taken 0:00:24.190627
Number of used sentences in train = 359
Total loss for epoch 6: 929.973661
	Epoch 7....
Epoch has taken 0:00:24.033192
Number of used sentences in train = 359
Total loss for epoch 7: 878.380203
	Epoch 8....
Epoch has taken 0:00:24.196477
Number of used sentences in train = 359
Total loss for epoch 8: 829.794973
	Epoch 9....
Epoch has taken 0:00:24.108358
Number of used sentences in train = 359
Total loss for epoch 9: 826.593656
	Epoch 10....
Epoch has taken 0:00:24.130731
Number of used sentences in train = 359
Total loss for epoch 10: 790.000208
	Epoch 11....
Epoch has taken 0:00:24.103946
Number of used sentences in train = 359
Total loss for epoch 11: 769.958325
	Epoch 12....
Epoch has taken 0:00:24.076329
Number of used sentences in train = 359
Total loss for epoch 12: 769.797482
	Epoch 13....
Epoch has taken 0:00:27.606516
Number of used sentences in train = 359
Total loss for epoch 13: 730.983302
	Epoch 14....
Epoch has taken 0:00:25.831924
Number of used sentences in train = 359
Total loss for epoch 14: 722.349957
Epoch has taken 0:00:24.228945

==================================================================================================
	Training time : 1:08:47.067023
==================================================================================================
	Identification : 0.505

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 136, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 19, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 102, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 67, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5849, 67)
  (lstm): LSTM(86, 102, bidirectional=True)
  (linear1): Linear(in_features=1632, out_features=136, bias=True)
  (linear2): Linear(in_features=136, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14373.266852
validation loss after epoch 0 : 1050.802437
	Epoch 1....
Epoch has taken 0:02:51.253453
Number of used sentences in train = 2811
Total loss for epoch 1: 8583.188362
validation loss after epoch 1 : 1048.223343
	Epoch 2....
Epoch has taken 0:02:50.705207
Number of used sentences in train = 2811
Total loss for epoch 2: 6979.907944
validation loss after epoch 2 : 1032.086224
	Epoch 3....
Epoch has taken 0:02:53.284794
Number of used sentences in train = 2811
Total loss for epoch 3: 5875.842484
validation loss after epoch 3 : 1098.606374
	Epoch 4....
Epoch has taken 0:02:52.991842
Number of used sentences in train = 2811
Total loss for epoch 4: 5296.828422
validation loss after epoch 4 : 1147.259662
	Epoch 5....
Epoch has taken 0:02:53.202476
Number of used sentences in train = 2811
Total loss for epoch 5: 4915.987878
validation loss after epoch 5 : 1206.900988
	Epoch 6....
Epoch has taken 0:02:51.322181
Number of used sentences in train = 2811
Total loss for epoch 6: 4723.064920
validation loss after epoch 6 : 1277.948980
	Epoch 7....
Epoch has taken 0:02:51.156820
Number of used sentences in train = 2811
Total loss for epoch 7: 4656.442990
validation loss after epoch 7 : 1299.945634
	Epoch 8....
Epoch has taken 0:02:52.462192
Number of used sentences in train = 2811
Total loss for epoch 8: 4611.379037
validation loss after epoch 8 : 1363.159771
	Epoch 9....
Epoch has taken 0:02:54.732093
Number of used sentences in train = 2811
Total loss for epoch 9: 4572.347698
validation loss after epoch 9 : 1365.439708
	Epoch 10....
Epoch has taken 0:02:53.030395
Number of used sentences in train = 2811
Total loss for epoch 10: 4554.340984
validation loss after epoch 10 : 1373.423389
	Epoch 11....
Epoch has taken 0:02:51.108142
Number of used sentences in train = 2811
Total loss for epoch 11: 4540.047922
validation loss after epoch 11 : 1403.899900
	Epoch 12....
Epoch has taken 0:02:50.741587
Number of used sentences in train = 2811
Total loss for epoch 12: 4527.031261
validation loss after epoch 12 : 1414.731723
	Epoch 13....
Epoch has taken 0:03:17.854104
Number of used sentences in train = 2811
Total loss for epoch 13: 4519.455446
validation loss after epoch 13 : 1439.775117
	Epoch 14....
Epoch has taken 0:02:51.931746
Number of used sentences in train = 2811
Total loss for epoch 14: 4511.598961
validation loss after epoch 14 : 1458.330406
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5849, 67)
  (lstm): LSTM(86, 102, bidirectional=True)
  (linear1): Linear(in_features=1632, out_features=136, bias=True)
  (linear2): Linear(in_features=136, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.574378
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1892.546140
	Epoch 1....
Epoch has taken 0:00:18.103564
Number of used sentences in train = 313
Total loss for epoch 1: 886.127825
	Epoch 2....
Epoch has taken 0:00:18.069305
Number of used sentences in train = 313
Total loss for epoch 2: 669.466782
	Epoch 3....
Epoch has taken 0:00:18.288771
Number of used sentences in train = 313
Total loss for epoch 3: 566.717234
	Epoch 4....
Epoch has taken 0:00:18.251500
Number of used sentences in train = 313
Total loss for epoch 4: 545.068695
	Epoch 5....
Epoch has taken 0:00:18.263726
Number of used sentences in train = 313
Total loss for epoch 5: 525.512541
	Epoch 6....
Epoch has taken 0:00:18.310973
Number of used sentences in train = 313
Total loss for epoch 6: 519.238103
	Epoch 7....
Epoch has taken 0:00:18.393670
Number of used sentences in train = 313
Total loss for epoch 7: 515.177600
	Epoch 8....
Epoch has taken 0:00:18.434775
Number of used sentences in train = 313
Total loss for epoch 8: 513.107342
	Epoch 9....
Epoch has taken 0:00:18.393940
Number of used sentences in train = 313
Total loss for epoch 9: 512.446067
	Epoch 10....
Epoch has taken 0:00:18.258278
Number of used sentences in train = 313
Total loss for epoch 10: 510.556090
	Epoch 11....
Epoch has taken 0:00:18.166238
Number of used sentences in train = 313
Total loss for epoch 11: 509.177760
	Epoch 12....
Epoch has taken 0:00:18.255163
Number of used sentences in train = 313
Total loss for epoch 12: 508.615672
	Epoch 13....
Epoch has taken 0:00:18.441394
Number of used sentences in train = 313
Total loss for epoch 13: 507.868358
	Epoch 14....
Epoch has taken 0:00:18.114954
Number of used sentences in train = 313
Total loss for epoch 14: 508.143211
Epoch has taken 0:00:18.227893

==================================================================================================
	Training time : 0:48:00.824803
==================================================================================================
	Identification : 0.361

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5622, 67)
  (lstm): LSTM(86, 102, bidirectional=True)
  (linear1): Linear(in_features=1632, out_features=136, bias=True)
  (linear2): Linear(in_features=136, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11610.227511
validation loss after epoch 0 : 918.346258
	Epoch 1....
Epoch has taken 0:01:51.262466
Number of used sentences in train = 2074
Total loss for epoch 1: 6739.456349
validation loss after epoch 1 : 865.165542
	Epoch 2....
Epoch has taken 0:01:50.997581
Number of used sentences in train = 2074
Total loss for epoch 2: 5254.286387
validation loss after epoch 2 : 836.694558
	Epoch 3....
Epoch has taken 0:01:50.820881
Number of used sentences in train = 2074
Total loss for epoch 3: 4310.342670
validation loss after epoch 3 : 1012.659457
	Epoch 4....
Epoch has taken 0:01:52.127429
Number of used sentences in train = 2074
Total loss for epoch 4: 3760.092922
validation loss after epoch 4 : 1063.687371
	Epoch 5....
Epoch has taken 0:01:51.920861
Number of used sentences in train = 2074
Total loss for epoch 5: 3509.307637
validation loss after epoch 5 : 994.003214
	Epoch 6....
Epoch has taken 0:01:52.795779
Number of used sentences in train = 2074
Total loss for epoch 6: 3384.588128
validation loss after epoch 6 : 1106.315055
	Epoch 7....
Epoch has taken 0:01:50.843136
Number of used sentences in train = 2074
Total loss for epoch 7: 3293.956621
validation loss after epoch 7 : 1112.921301
	Epoch 8....
Epoch has taken 0:01:51.071387
Number of used sentences in train = 2074
Total loss for epoch 8: 3253.720449
validation loss after epoch 8 : 1158.279642
	Epoch 9....
Epoch has taken 0:01:53.470780
Number of used sentences in train = 2074
Total loss for epoch 9: 3223.740866
validation loss after epoch 9 : 1189.253558
	Epoch 10....
Epoch has taken 0:01:54.124333
Number of used sentences in train = 2074
Total loss for epoch 10: 3200.161979
validation loss after epoch 10 : 1212.118784
	Epoch 11....
Epoch has taken 0:01:52.145396
Number of used sentences in train = 2074
Total loss for epoch 11: 3184.979000
validation loss after epoch 11 : 1242.500144
	Epoch 12....
Epoch has taken 0:01:50.508207
Number of used sentences in train = 2074
Total loss for epoch 12: 3172.396524
validation loss after epoch 12 : 1268.175467
	Epoch 13....
Epoch has taken 0:01:51.497578
Number of used sentences in train = 2074
Total loss for epoch 13: 3166.420710
validation loss after epoch 13 : 1284.512122
	Epoch 14....
Epoch has taken 0:01:51.691920
Number of used sentences in train = 2074
Total loss for epoch 14: 3164.155041
validation loss after epoch 14 : 1286.691397
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5622, 67)
  (lstm): LSTM(86, 102, bidirectional=True)
  (linear1): Linear(in_features=1632, out_features=136, bias=True)
  (linear2): Linear(in_features=136, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.814281
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1990.799786
	Epoch 1....
Epoch has taken 0:00:11.376314
Number of used sentences in train = 231
Total loss for epoch 1: 737.463755
	Epoch 2....
Epoch has taken 0:00:11.336141
Number of used sentences in train = 231
Total loss for epoch 2: 536.477728
	Epoch 3....
Epoch has taken 0:00:11.456104
Number of used sentences in train = 231
Total loss for epoch 3: 430.413996
	Epoch 4....
Epoch has taken 0:00:11.260456
Number of used sentences in train = 231
Total loss for epoch 4: 379.519146
	Epoch 5....
Epoch has taken 0:00:11.674986
Number of used sentences in train = 231
Total loss for epoch 5: 366.749505
	Epoch 6....
Epoch has taken 0:00:11.880434
Number of used sentences in train = 231
Total loss for epoch 6: 359.669039
	Epoch 7....
Epoch has taken 0:00:12.027089
Number of used sentences in train = 231
Total loss for epoch 7: 352.939108
	Epoch 8....
Epoch has taken 0:00:11.341303
Number of used sentences in train = 231
Total loss for epoch 8: 350.614979
	Epoch 9....
Epoch has taken 0:00:11.322060
Number of used sentences in train = 231
Total loss for epoch 9: 349.705390
	Epoch 10....
Epoch has taken 0:00:11.345228
Number of used sentences in train = 231
Total loss for epoch 10: 348.055487
	Epoch 11....
Epoch has taken 0:00:11.374705
Number of used sentences in train = 231
Total loss for epoch 11: 347.059099
	Epoch 12....
Epoch has taken 0:00:11.343475
Number of used sentences in train = 231
Total loss for epoch 12: 346.446864
	Epoch 13....
Epoch has taken 0:00:11.361936
Number of used sentences in train = 231
Total loss for epoch 13: 346.344018
	Epoch 14....
Epoch has taken 0:00:12.001990
Number of used sentences in train = 231
Total loss for epoch 14: 346.194321
Epoch has taken 0:00:11.346472

==================================================================================================
	Training time : 0:30:49.880205
==================================================================================================
	Identification : 0.247

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(6867, 67)
  (lstm): LSTM(86, 102, bidirectional=True)
  (linear1): Linear(in_features=1632, out_features=136, bias=True)
  (linear2): Linear(in_features=136, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 19636.531650
validation loss after epoch 0 : 1546.565091
	Epoch 1....
Epoch has taken 0:03:38.046557
Number of used sentences in train = 3226
Total loss for epoch 1: 12617.750491
validation loss after epoch 1 : 1518.232921
	Epoch 2....
Epoch has taken 0:03:37.567222
Number of used sentences in train = 3226
Total loss for epoch 2: 10757.934658
validation loss after epoch 2 : 1502.817120
	Epoch 3....
Epoch has taken 0:03:37.606275
Number of used sentences in train = 3226
Total loss for epoch 3: 9410.422641
validation loss after epoch 3 : 1578.421410
	Epoch 4....
Epoch has taken 0:03:35.789733
Number of used sentences in train = 3226
Total loss for epoch 4: 8330.956522
validation loss after epoch 4 : 1719.288009
	Epoch 5....
Epoch has taken 0:03:37.594079
Number of used sentences in train = 3226
Total loss for epoch 5: 7522.964453
validation loss after epoch 5 : 1803.658318
	Epoch 6....
Epoch has taken 0:03:37.986745
Number of used sentences in train = 3226
Total loss for epoch 6: 6946.036183
validation loss after epoch 6 : 1962.842880
	Epoch 7....
Epoch has taken 0:03:37.763260
Number of used sentences in train = 3226
Total loss for epoch 7: 6588.654019
validation loss after epoch 7 : 2104.055955
	Epoch 8....
Epoch has taken 0:03:37.638766
Number of used sentences in train = 3226
Total loss for epoch 8: 6466.683392
validation loss after epoch 8 : 2099.132164
	Epoch 9....
Epoch has taken 0:03:37.300302
Number of used sentences in train = 3226
Total loss for epoch 9: 6345.347288
validation loss after epoch 9 : 2190.803412
	Epoch 10....
Epoch has taken 0:03:39.821800
Number of used sentences in train = 3226
Total loss for epoch 10: 6253.502801
validation loss after epoch 10 : 2304.957654
	Epoch 11....
Epoch has taken 0:03:39.951366
Number of used sentences in train = 3226
Total loss for epoch 11: 6221.798307
validation loss after epoch 11 : 2291.657315
	Epoch 12....
Epoch has taken 0:03:39.038079
Number of used sentences in train = 3226
Total loss for epoch 12: 6201.167636
validation loss after epoch 12 : 2383.951882
	Epoch 13....
Epoch has taken 0:03:36.331195
Number of used sentences in train = 3226
Total loss for epoch 13: 6173.101807
validation loss after epoch 13 : 2356.046058
	Epoch 14....
Epoch has taken 0:03:36.032068
Number of used sentences in train = 3226
Total loss for epoch 14: 6157.604062
validation loss after epoch 14 : 2358.713956
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(6867, 67)
  (lstm): LSTM(86, 102, bidirectional=True)
  (linear1): Linear(in_features=1632, out_features=136, bias=True)
  (linear2): Linear(in_features=136, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:37.998304
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2486.711067
	Epoch 1....
Epoch has taken 0:00:21.215588
Number of used sentences in train = 359
Total loss for epoch 1: 1252.821149
	Epoch 2....
Epoch has taken 0:00:21.221818
Number of used sentences in train = 359
Total loss for epoch 2: 942.620866
	Epoch 3....
Epoch has taken 0:00:21.213843
Number of used sentences in train = 359
Total loss for epoch 3: 789.058416
	Epoch 4....
Epoch has taken 0:00:21.220298
Number of used sentences in train = 359
Total loss for epoch 4: 708.557154
	Epoch 5....
Epoch has taken 0:00:21.280548
Number of used sentences in train = 359
Total loss for epoch 5: 680.982231
	Epoch 6....
Epoch has taken 0:00:21.276001
Number of used sentences in train = 359
Total loss for epoch 6: 678.059402
	Epoch 7....
Epoch has taken 0:00:21.173648
Number of used sentences in train = 359
Total loss for epoch 7: 674.018889
	Epoch 8....
Epoch has taken 0:00:21.269506
Number of used sentences in train = 359
Total loss for epoch 8: 672.743310
	Epoch 9....
Epoch has taken 0:00:21.239881
Number of used sentences in train = 359
Total loss for epoch 9: 672.287830
	Epoch 10....
Epoch has taken 0:00:21.276795
Number of used sentences in train = 359
Total loss for epoch 10: 671.965142
	Epoch 11....
Epoch has taken 0:00:21.215636
Number of used sentences in train = 359
Total loss for epoch 11: 671.574204
	Epoch 12....
Epoch has taken 0:00:21.190065
Number of used sentences in train = 359
Total loss for epoch 12: 671.340795
	Epoch 13....
Epoch has taken 0:00:21.281763
Number of used sentences in train = 359
Total loss for epoch 13: 671.159102
	Epoch 14....
Epoch has taken 0:00:21.210042
Number of used sentences in train = 359
Total loss for epoch 14: 671.060809
Epoch has taken 0:00:21.179848

==================================================================================================
	Training time : 0:59:45.595326
==================================================================================================
	Identification : 0.218

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 12, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 22, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 70, 'lstmDropout': 0.12, 'denseActivation': 'tanh', 'wordDim': 188, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1882, 188)
  (lstm): LSTM(210, 70, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12636.935370
validation loss after epoch 0 : 899.573411
	Epoch 1....
Epoch has taken 0:02:53.457082
Number of used sentences in train = 2811
Total loss for epoch 1: 7696.920740
validation loss after epoch 1 : 849.845687
	Epoch 2....
Epoch has taken 0:02:53.575148
Number of used sentences in train = 2811
Total loss for epoch 2: 6695.291575
validation loss after epoch 2 : 829.032650
	Epoch 3....
Epoch has taken 0:02:54.652857
Number of used sentences in train = 2811
Total loss for epoch 3: 6036.414941
validation loss after epoch 3 : 843.927089
	Epoch 4....
Epoch has taken 0:02:55.426108
Number of used sentences in train = 2811
Total loss for epoch 4: 5557.192880
validation loss after epoch 4 : 912.758602
	Epoch 5....
Epoch has taken 0:02:53.699056
Number of used sentences in train = 2811
Total loss for epoch 5: 5339.282140
validation loss after epoch 5 : 921.972940
	Epoch 6....
Epoch has taken 0:02:54.181001
Number of used sentences in train = 2811
Total loss for epoch 6: 5150.457481
validation loss after epoch 6 : 939.017041
	Epoch 7....
Epoch has taken 0:02:53.548752
Number of used sentences in train = 2811
Total loss for epoch 7: 5006.818298
validation loss after epoch 7 : 967.209617
	Epoch 8....
Epoch has taken 0:02:55.299906
Number of used sentences in train = 2811
Total loss for epoch 8: 4905.260394
validation loss after epoch 8 : 1008.038884
	Epoch 9....
Epoch has taken 0:02:55.253060
Number of used sentences in train = 2811
Total loss for epoch 9: 4867.110952
validation loss after epoch 9 : 1042.222030
	Epoch 10....
Epoch has taken 0:02:53.525930
Number of used sentences in train = 2811
Total loss for epoch 10: 4830.506999
validation loss after epoch 10 : 1022.980445
	Epoch 11....
Epoch has taken 0:03:24.955870
Number of used sentences in train = 2811
Total loss for epoch 11: 4757.915120
validation loss after epoch 11 : 1099.881420
	Epoch 12....
Epoch has taken 0:02:55.219727
Number of used sentences in train = 2811
Total loss for epoch 12: 4739.713796
validation loss after epoch 12 : 1048.622254
	Epoch 13....
Epoch has taken 0:02:54.885624
Number of used sentences in train = 2811
Total loss for epoch 13: 4705.925556
validation loss after epoch 13 : 1055.663708
	Epoch 14....
Epoch has taken 0:02:53.570775
Number of used sentences in train = 2811
Total loss for epoch 14: 4705.043052
validation loss after epoch 14 : 1043.265136
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1882, 188)
  (lstm): LSTM(210, 70, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:53.503475
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1233.548054
	Epoch 1....
Epoch has taken 0:00:18.540475
Number of used sentences in train = 313
Total loss for epoch 1: 835.272354
	Epoch 2....
Epoch has taken 0:00:18.459899
Number of used sentences in train = 313
Total loss for epoch 2: 719.625881
	Epoch 3....
Epoch has taken 0:00:18.468294
Number of used sentences in train = 313
Total loss for epoch 3: 655.421858
	Epoch 4....
Epoch has taken 0:00:18.477174
Number of used sentences in train = 313
Total loss for epoch 4: 596.887977
	Epoch 5....
Epoch has taken 0:00:18.816100
Number of used sentences in train = 313
Total loss for epoch 5: 580.419595
	Epoch 6....
Epoch has taken 0:00:18.443318
Number of used sentences in train = 313
Total loss for epoch 6: 577.181915
	Epoch 7....
Epoch has taken 0:00:18.413050
Number of used sentences in train = 313
Total loss for epoch 7: 565.342714
	Epoch 8....
Epoch has taken 0:00:18.547921
Number of used sentences in train = 313
Total loss for epoch 8: 555.893698
	Epoch 9....
Epoch has taken 0:00:18.866869
Number of used sentences in train = 313
Total loss for epoch 9: 557.743110
	Epoch 10....
Epoch has taken 0:00:18.498835
Number of used sentences in train = 313
Total loss for epoch 10: 552.402801
	Epoch 11....
Epoch has taken 0:00:18.534128
Number of used sentences in train = 313
Total loss for epoch 11: 539.905097
	Epoch 12....
Epoch has taken 0:00:18.507286
Number of used sentences in train = 313
Total loss for epoch 12: 535.321512
	Epoch 13....
Epoch has taken 0:00:18.525010
Number of used sentences in train = 313
Total loss for epoch 13: 532.412313
	Epoch 14....
Epoch has taken 0:00:18.486001
Number of used sentences in train = 313
Total loss for epoch 14: 530.191689
Epoch has taken 0:00:18.494959

==================================================================================================
	Training time : 0:48:43.348773
==================================================================================================
	Identification : 0.484

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1680, 188)
  (lstm): LSTM(210, 70, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8464.854966
validation loss after epoch 0 : 600.341965
	Epoch 1....
Epoch has taken 0:02:01.411048
Number of used sentences in train = 2074
Total loss for epoch 1: 5129.705027
validation loss after epoch 1 : 585.269993
	Epoch 2....
Epoch has taken 0:02:00.046544
Number of used sentences in train = 2074
Total loss for epoch 2: 4282.501208
validation loss after epoch 2 : 596.341349
	Epoch 3....
Epoch has taken 0:02:09.037025
Number of used sentences in train = 2074
Total loss for epoch 3: 3803.714201
validation loss after epoch 3 : 656.113426
	Epoch 4....
Epoch has taken 0:02:07.820734
Number of used sentences in train = 2074
Total loss for epoch 4: 3498.430890
validation loss after epoch 4 : 656.538416
	Epoch 5....
Epoch has taken 0:02:09.356572
Number of used sentences in train = 2074
Total loss for epoch 5: 3378.248403
validation loss after epoch 5 : 737.804951
	Epoch 6....
Epoch has taken 0:02:09.403260
Number of used sentences in train = 2074
Total loss for epoch 6: 3302.066833
validation loss after epoch 6 : 676.039213
	Epoch 7....
Epoch has taken 0:02:09.787927
Number of used sentences in train = 2074
Total loss for epoch 7: 3247.718093
validation loss after epoch 7 : 732.711673
	Epoch 8....
Epoch has taken 0:02:08.000690
Number of used sentences in train = 2074
Total loss for epoch 8: 3226.064389
validation loss after epoch 8 : 693.816821
	Epoch 9....
Epoch has taken 0:02:09.195473
Number of used sentences in train = 2074
Total loss for epoch 9: 3210.202036
validation loss after epoch 9 : 720.337315
	Epoch 10....
Epoch has taken 0:02:10.336700
Number of used sentences in train = 2074
Total loss for epoch 10: 3209.917153
validation loss after epoch 10 : 718.953993
	Epoch 11....
Epoch has taken 0:02:09.541545
Number of used sentences in train = 2074
Total loss for epoch 11: 3189.933071
validation loss after epoch 11 : 773.535776
	Epoch 12....
Epoch has taken 0:02:09.697767
Number of used sentences in train = 2074
Total loss for epoch 12: 3185.194315
validation loss after epoch 12 : 726.878494
	Epoch 13....
Epoch has taken 0:02:10.825064
Number of used sentences in train = 2074
Total loss for epoch 13: 3188.468960
validation loss after epoch 13 : 750.184042
	Epoch 14....
Epoch has taken 0:02:10.965640
Number of used sentences in train = 2074
Total loss for epoch 14: 3173.870305
validation loss after epoch 14 : 780.666978
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1680, 188)
  (lstm): LSTM(210, 70, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:00.470851
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1084.850944
	Epoch 1....
Epoch has taken 0:00:12.046113
Number of used sentences in train = 231
Total loss for epoch 1: 580.928205
	Epoch 2....
Epoch has taken 0:00:12.050202
Number of used sentences in train = 231
Total loss for epoch 2: 465.838426
	Epoch 3....
Epoch has taken 0:00:12.044541
Number of used sentences in train = 231
Total loss for epoch 3: 407.057022
	Epoch 4....
Epoch has taken 0:00:12.031680
Number of used sentences in train = 231
Total loss for epoch 4: 378.836575
	Epoch 5....
Epoch has taken 0:00:12.060128
Number of used sentences in train = 231
Total loss for epoch 5: 367.580171
	Epoch 6....
Epoch has taken 0:00:12.025955
Number of used sentences in train = 231
Total loss for epoch 6: 362.708058
	Epoch 7....
Epoch has taken 0:00:12.068366
Number of used sentences in train = 231
Total loss for epoch 7: 355.639513
	Epoch 8....
Epoch has taken 0:00:12.055366
Number of used sentences in train = 231
Total loss for epoch 8: 358.754542
	Epoch 9....
Epoch has taken 0:00:12.066537
Number of used sentences in train = 231
Total loss for epoch 9: 357.377055
	Epoch 10....
Epoch has taken 0:00:12.036458
Number of used sentences in train = 231
Total loss for epoch 10: 351.771937
	Epoch 11....
Epoch has taken 0:00:12.052940
Number of used sentences in train = 231
Total loss for epoch 11: 348.702368
	Epoch 12....
Epoch has taken 0:00:12.044170
Number of used sentences in train = 231
Total loss for epoch 12: 348.148165
	Epoch 13....
Epoch has taken 0:00:12.073988
Number of used sentences in train = 231
Total loss for epoch 13: 346.838441
	Epoch 14....
Epoch has taken 0:00:12.064369
Number of used sentences in train = 231
Total loss for epoch 14: 347.207811
Epoch has taken 0:00:12.044595

==================================================================================================
	Training time : 0:34:57.008633
==================================================================================================
	Identification : 0.371

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 22)
  (w_embeddings): Embedding(3369, 188)
  (lstm): LSTM(210, 70, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 20719.621611
validation loss after epoch 0 : 1124.095199
	Epoch 1....
Epoch has taken 0:04:07.158198
Number of used sentences in train = 3226
Total loss for epoch 1: 9822.876684
validation loss after epoch 1 : 1128.194072
	Epoch 2....
Epoch has taken 0:03:46.134688
Number of used sentences in train = 3226
Total loss for epoch 2: 8679.378910
validation loss after epoch 2 : 1092.587341
	Epoch 3....
Epoch has taken 0:03:48.775846
Number of used sentences in train = 3226
Total loss for epoch 3: 8023.142767
validation loss after epoch 3 : 1126.227715
	Epoch 4....
Epoch has taken 0:03:48.996876
Number of used sentences in train = 3226
Total loss for epoch 4: 7514.896868
validation loss after epoch 4 : 1144.960836
	Epoch 5....
Epoch has taken 0:03:49.234903
Number of used sentences in train = 3226
Total loss for epoch 5: 7228.768684
validation loss after epoch 5 : 1198.697591
	Epoch 6....
Epoch has taken 0:03:46.798046
Number of used sentences in train = 3226
Total loss for epoch 6: 6943.105861
validation loss after epoch 6 : 1285.672674
	Epoch 7....
Epoch has taken 0:03:47.664361
Number of used sentences in train = 3226
Total loss for epoch 7: 6776.689038
validation loss after epoch 7 : 1331.051363
	Epoch 8....
Epoch has taken 0:03:51.145125
Number of used sentences in train = 3226
Total loss for epoch 8: 6679.364242
validation loss after epoch 8 : 1361.234245
	Epoch 9....
Epoch has taken 0:03:48.200687
Number of used sentences in train = 3226
Total loss for epoch 9: 6589.963345
validation loss after epoch 9 : 1353.261975
	Epoch 10....
Epoch has taken 0:03:47.755509
Number of used sentences in train = 3226
Total loss for epoch 10: 6470.586984
validation loss after epoch 10 : 1398.343765
	Epoch 11....
Epoch has taken 0:03:48.679726
Number of used sentences in train = 3226
Total loss for epoch 11: 6434.950237
validation loss after epoch 11 : 1424.219379
	Epoch 12....
Epoch has taken 0:04:23.605099
Number of used sentences in train = 3226
Total loss for epoch 12: 6385.008248
validation loss after epoch 12 : 1656.698712
	Epoch 13....
Epoch has taken 0:03:48.417938
Number of used sentences in train = 3226
Total loss for epoch 13: 6318.734333
validation loss after epoch 13 : 1502.177004
	Epoch 14....
Epoch has taken 0:03:46.050487
Number of used sentences in train = 3226
Total loss for epoch 14: 6296.336565
validation loss after epoch 14 : 1526.669214
	TransitionClassifier(
  (p_embeddings): Embedding(13, 22)
  (w_embeddings): Embedding(3369, 188)
  (lstm): LSTM(210, 70, num_layers=2, dropout=0.12, bidirectional=True)
  (linear1): Linear(in_features=1120, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.131288
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2039.534602
	Epoch 1....
Epoch has taken 0:00:22.361572
Number of used sentences in train = 359
Total loss for epoch 1: 1024.554753
	Epoch 2....
Epoch has taken 0:00:22.356662
Number of used sentences in train = 359
Total loss for epoch 2: 898.366023
	Epoch 3....
Epoch has taken 0:00:22.384087
Number of used sentences in train = 359
Total loss for epoch 3: 812.430023
	Epoch 4....
Epoch has taken 0:00:22.349148
Number of used sentences in train = 359
Total loss for epoch 4: 756.917110
	Epoch 5....
Epoch has taken 0:00:22.328575
Number of used sentences in train = 359
Total loss for epoch 5: 727.621837
	Epoch 6....
Epoch has taken 0:00:22.349684
Number of used sentences in train = 359
Total loss for epoch 6: 716.096623
	Epoch 7....
Epoch has taken 0:00:25.689630
Number of used sentences in train = 359
Total loss for epoch 7: 698.473190
	Epoch 8....
Epoch has taken 0:00:22.376057
Number of used sentences in train = 359
Total loss for epoch 8: 682.682438
	Epoch 9....
Epoch has taken 0:00:22.364675
Number of used sentences in train = 359
Total loss for epoch 9: 678.095678
	Epoch 10....
Epoch has taken 0:00:22.346211
Number of used sentences in train = 359
Total loss for epoch 10: 676.655808
	Epoch 11....
Epoch has taken 0:00:22.366081
Number of used sentences in train = 359
Total loss for epoch 11: 689.640311
	Epoch 12....
Epoch has taken 0:00:22.359570
Number of used sentences in train = 359
Total loss for epoch 12: 679.250069
	Epoch 13....
Epoch has taken 0:00:22.360638
Number of used sentences in train = 359
Total loss for epoch 13: 675.101543
	Epoch 14....
Epoch has taken 0:00:22.354641
Number of used sentences in train = 359
Total loss for epoch 14: 676.137867
Epoch has taken 0:00:22.340349

==================================================================================================
	Training time : 1:03:36.102480
==================================================================================================
	Identification : 0.484

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 88, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 22, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 21, 'lstmDropout': 0.28, 'denseActivation': 'tanh', 'wordDim': 131, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1177, 131)
  (lstm): LSTM(153, 21, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 11882.202325
validation loss after epoch 0 : 985.078842
	Epoch 1....
Epoch has taken 0:02:49.760081
Number of used sentences in train = 2811
Total loss for epoch 1: 8045.544219
validation loss after epoch 1 : 893.446847
	Epoch 2....
Epoch has taken 0:02:52.111598
Number of used sentences in train = 2811
Total loss for epoch 2: 7159.686144
validation loss after epoch 2 : 874.399928
	Epoch 3....
Epoch has taken 0:04:31.185356
Number of used sentences in train = 2811
Total loss for epoch 3: 6672.565036
validation loss after epoch 3 : 868.826120
	Epoch 4....
Epoch has taken 0:02:52.906035
Number of used sentences in train = 2811
Total loss for epoch 4: 6342.808726
validation loss after epoch 4 : 863.104066
	Epoch 5....
Epoch has taken 0:02:50.426639
Number of used sentences in train = 2811
Total loss for epoch 5: 6079.163012
validation loss after epoch 5 : 929.451332
	Epoch 6....
Epoch has taken 0:02:51.967550
Number of used sentences in train = 2811
Total loss for epoch 6: 5897.721201
validation loss after epoch 6 : 876.365851
	Epoch 7....
Epoch has taken 0:02:51.415349
Number of used sentences in train = 2811
Total loss for epoch 7: 5726.276276
validation loss after epoch 7 : 901.183275
	Epoch 8....
Epoch has taken 0:03:15.545735
Number of used sentences in train = 2811
Total loss for epoch 8: 5644.234404
validation loss after epoch 8 : 920.761408
	Epoch 9....
Epoch has taken 0:02:51.629368
Number of used sentences in train = 2811
Total loss for epoch 9: 5500.809504
validation loss after epoch 9 : 910.943990
	Epoch 10....
Epoch has taken 0:02:49.794203
Number of used sentences in train = 2811
Total loss for epoch 10: 5413.874183
validation loss after epoch 10 : 911.794621
	Epoch 11....
Epoch has taken 0:02:51.288161
Number of used sentences in train = 2811
Total loss for epoch 11: 5333.040884
validation loss after epoch 11 : 917.989529
	Epoch 12....
Epoch has taken 0:02:51.828880
Number of used sentences in train = 2811
Total loss for epoch 12: 5295.044911
validation loss after epoch 12 : 945.185172
	Epoch 13....
Epoch has taken 0:03:15.635251
Number of used sentences in train = 2811
Total loss for epoch 13: 5163.690199
validation loss after epoch 13 : 933.647531
	Epoch 14....
Epoch has taken 0:02:51.531947
Number of used sentences in train = 2811
Total loss for epoch 14: 5194.288558
validation loss after epoch 14 : 983.865855
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1177, 131)
  (lstm): LSTM(153, 21, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.091832
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1335.638944
	Epoch 1....
Epoch has taken 0:00:17.963481
Number of used sentences in train = 313
Total loss for epoch 1: 748.459626
	Epoch 2....
Epoch has taken 0:00:17.947136
Number of used sentences in train = 313
Total loss for epoch 2: 646.875645
	Epoch 3....
Epoch has taken 0:00:17.974226
Number of used sentences in train = 313
Total loss for epoch 3: 601.784194
	Epoch 4....
Epoch has taken 0:00:18.119912
Number of used sentences in train = 313
Total loss for epoch 4: 586.190810
	Epoch 5....
Epoch has taken 0:00:18.098219
Number of used sentences in train = 313
Total loss for epoch 5: 560.663102
	Epoch 6....
Epoch has taken 0:00:18.110003
Number of used sentences in train = 313
Total loss for epoch 6: 554.743856
	Epoch 7....
Epoch has taken 0:00:18.131701
Number of used sentences in train = 313
Total loss for epoch 7: 528.038002
	Epoch 8....
Epoch has taken 0:00:18.131782
Number of used sentences in train = 313
Total loss for epoch 8: 528.931658
	Epoch 9....
Epoch has taken 0:00:18.105089
Number of used sentences in train = 313
Total loss for epoch 9: 532.367884
	Epoch 10....
Epoch has taken 0:00:18.109314
Number of used sentences in train = 313
Total loss for epoch 10: 522.306840
	Epoch 11....
Epoch has taken 0:00:18.111498
Number of used sentences in train = 313
Total loss for epoch 11: 520.010055
	Epoch 12....
Epoch has taken 0:00:18.131596
Number of used sentences in train = 313
Total loss for epoch 12: 515.249306
	Epoch 13....
Epoch has taken 0:00:18.088447
Number of used sentences in train = 313
Total loss for epoch 13: 512.828916
	Epoch 14....
Epoch has taken 0:00:18.095169
Number of used sentences in train = 313
Total loss for epoch 14: 510.996589
Epoch has taken 0:00:18.116466

==================================================================================================
	Training time : 0:49:48.850089
==================================================================================================
	Identification : 0.295

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1133, 131)
  (lstm): LSTM(153, 21, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9000.760459
validation loss after epoch 0 : 684.797029
	Epoch 1....
Epoch has taken 0:01:55.470026
Number of used sentences in train = 2074
Total loss for epoch 1: 5663.672134
validation loss after epoch 1 : 701.117869
	Epoch 2....
Epoch has taken 0:01:55.397829
Number of used sentences in train = 2074
Total loss for epoch 2: 4983.497359
validation loss after epoch 2 : 661.087741
	Epoch 3....
Epoch has taken 0:01:55.901150
Number of used sentences in train = 2074
Total loss for epoch 3: 4536.703617
validation loss after epoch 3 : 662.635690
	Epoch 4....
Epoch has taken 0:01:57.135140
Number of used sentences in train = 2074
Total loss for epoch 4: 4317.692263
validation loss after epoch 4 : 718.627769
	Epoch 5....
Epoch has taken 0:02:14.832942
Number of used sentences in train = 2074
Total loss for epoch 5: 4125.138623
validation loss after epoch 5 : 736.401370
	Epoch 6....
Epoch has taken 0:01:56.784443
Number of used sentences in train = 2074
Total loss for epoch 6: 3970.210817
validation loss after epoch 6 : 770.496052
	Epoch 7....
Epoch has taken 0:01:56.598364
Number of used sentences in train = 2074
Total loss for epoch 7: 3832.241261
validation loss after epoch 7 : 760.779477
	Epoch 8....
Epoch has taken 0:01:55.773346
Number of used sentences in train = 2074
Total loss for epoch 8: 3710.816735
validation loss after epoch 8 : 713.591159
	Epoch 9....
Epoch has taken 0:01:55.655497
Number of used sentences in train = 2074
Total loss for epoch 9: 3637.090841
validation loss after epoch 9 : 813.027297
	Epoch 10....
Epoch has taken 0:01:56.719141
Number of used sentences in train = 2074
Total loss for epoch 10: 3536.037043
validation loss after epoch 10 : 837.520125
	Epoch 11....
Epoch has taken 0:01:56.500830
Number of used sentences in train = 2074
Total loss for epoch 11: 3571.681779
validation loss after epoch 11 : 760.280127
	Epoch 12....
Epoch has taken 0:01:56.841711
Number of used sentences in train = 2074
Total loss for epoch 12: 3489.198026
validation loss after epoch 12 : 906.380974
	Epoch 13....
Epoch has taken 0:01:55.562349
Number of used sentences in train = 2074
Total loss for epoch 13: 3465.892819
validation loss after epoch 13 : 878.532084
	Epoch 14....
Epoch has taken 0:01:56.003136
Number of used sentences in train = 2074
Total loss for epoch 14: 3410.676624
validation loss after epoch 14 : 909.774004
	TransitionClassifier(
  (p_embeddings): Embedding(18, 22)
  (w_embeddings): Embedding(1133, 131)
  (lstm): LSTM(153, 21, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.934176
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1421.446305
	Epoch 1....
Epoch has taken 0:00:11.890719
Number of used sentences in train = 231
Total loss for epoch 1: 503.394481
	Epoch 2....
Epoch has taken 0:00:11.927473
Number of used sentences in train = 231
Total loss for epoch 2: 414.994096
	Epoch 3....
Epoch has taken 0:00:11.905579
Number of used sentences in train = 231
Total loss for epoch 3: 393.082831
	Epoch 4....
Epoch has taken 0:00:11.909343
Number of used sentences in train = 231
Total loss for epoch 4: 364.195096
	Epoch 5....
Epoch has taken 0:00:11.906963
Number of used sentences in train = 231
Total loss for epoch 5: 363.826315
	Epoch 6....
Epoch has taken 0:00:11.923403
Number of used sentences in train = 231
Total loss for epoch 6: 354.767148
	Epoch 7....
Epoch has taken 0:00:11.896780
Number of used sentences in train = 231
Total loss for epoch 7: 355.784222
	Epoch 8....
Epoch has taken 0:00:11.914055
Number of used sentences in train = 231
Total loss for epoch 8: 355.787548
	Epoch 9....
Epoch has taken 0:00:11.900577
Number of used sentences in train = 231
Total loss for epoch 9: 355.056145
	Epoch 10....
Epoch has taken 0:00:11.924970
Number of used sentences in train = 231
Total loss for epoch 10: 348.682788
	Epoch 11....
Epoch has taken 0:00:11.895937
Number of used sentences in train = 231
Total loss for epoch 11: 348.830383
	Epoch 12....
Epoch has taken 0:00:11.920253
Number of used sentences in train = 231
Total loss for epoch 12: 348.023401
	Epoch 13....
Epoch has taken 0:00:11.884964
Number of used sentences in train = 231
Total loss for epoch 13: 347.427533
	Epoch 14....
Epoch has taken 0:00:11.910560
Number of used sentences in train = 231
Total loss for epoch 14: 353.247626
Epoch has taken 0:00:11.898100

==================================================================================================
	Training time : 0:32:21.049472
==================================================================================================
	Identification : 0.413

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 22)
  (w_embeddings): Embedding(1202, 131)
  (lstm): LSTM(153, 21, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17724.965391
validation loss after epoch 0 : 1403.052692
	Epoch 1....
Epoch has taken 0:03:47.293335
Number of used sentences in train = 3226
Total loss for epoch 1: 11828.297202
validation loss after epoch 1 : 1361.551024
	Epoch 2....
Epoch has taken 0:03:47.169790
Number of used sentences in train = 3226
Total loss for epoch 2: 10789.248693
validation loss after epoch 2 : 1313.564943
	Epoch 3....
Epoch has taken 0:03:47.490943
Number of used sentences in train = 3226
Total loss for epoch 3: 10061.381773
validation loss after epoch 3 : 1296.256079
	Epoch 4....
Epoch has taken 0:04:22.643921
Number of used sentences in train = 3226
Total loss for epoch 4: 9663.851703
validation loss after epoch 4 : 1353.840875
	Epoch 5....
Epoch has taken 0:03:45.540528
Number of used sentences in train = 3226
Total loss for epoch 5: 9335.808986
validation loss after epoch 5 : 1307.472610
	Epoch 6....
Epoch has taken 0:03:45.971872
Number of used sentences in train = 3226
Total loss for epoch 6: 9063.878146
validation loss after epoch 6 : 1336.985509
	Epoch 7....
Epoch has taken 0:03:47.232388
Number of used sentences in train = 3226
Total loss for epoch 7: 8811.456593
validation loss after epoch 7 : 1311.506052
	Epoch 8....
Epoch has taken 0:03:48.235767
Number of used sentences in train = 3226
Total loss for epoch 8: 8653.397814
validation loss after epoch 8 : 1407.164575
	Epoch 9....
Epoch has taken 0:03:48.155691
Number of used sentences in train = 3226
Total loss for epoch 9: 8526.584012
validation loss after epoch 9 : 1359.215980
	Epoch 10....
Epoch has taken 0:03:48.265792
Number of used sentences in train = 3226
Total loss for epoch 10: 8395.044768
validation loss after epoch 10 : 1413.835588
	Epoch 11....
Epoch has taken 0:03:46.422726
Number of used sentences in train = 3226
Total loss for epoch 11: 8275.798606
validation loss after epoch 11 : 1409.243312
	Epoch 12....
Epoch has taken 0:03:46.560176
Number of used sentences in train = 3226
Total loss for epoch 12: 8168.759981
validation loss after epoch 12 : 1464.116744
	Epoch 13....
Epoch has taken 0:03:51.343518
Number of used sentences in train = 3226
Total loss for epoch 13: 7982.502247
validation loss after epoch 13 : 1532.292502
	Epoch 14....
Epoch has taken 0:03:48.736195
Number of used sentences in train = 3226
Total loss for epoch 14: 7912.906208
validation loss after epoch 14 : 1509.015986
	TransitionClassifier(
  (p_embeddings): Embedding(13, 22)
  (w_embeddings): Embedding(1202, 131)
  (lstm): LSTM(153, 21, num_layers=2, dropout=0.28, bidirectional=True)
  (linear1): Linear(in_features=336, out_features=88, bias=True)
  (linear2): Linear(in_features=88, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.020530
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2018.689346
	Epoch 1....
Epoch has taken 0:00:22.179303
Number of used sentences in train = 359
Total loss for epoch 1: 1133.574332
	Epoch 2....
Epoch has taken 0:00:22.178187
Number of used sentences in train = 359
Total loss for epoch 2: 1006.359838
	Epoch 3....
Epoch has taken 0:00:22.168640
Number of used sentences in train = 359
Total loss for epoch 3: 916.612761
	Epoch 4....
Epoch has taken 0:00:22.171134
Number of used sentences in train = 359
Total loss for epoch 4: 902.739523
	Epoch 5....
Epoch has taken 0:00:22.174525
Number of used sentences in train = 359
Total loss for epoch 5: 830.818847
	Epoch 6....
Epoch has taken 0:00:22.173356
Number of used sentences in train = 359
Total loss for epoch 6: 804.553075
	Epoch 7....
Epoch has taken 0:00:22.162248
Number of used sentences in train = 359
Total loss for epoch 7: 796.826356
	Epoch 8....
Epoch has taken 0:00:22.166463
Number of used sentences in train = 359
Total loss for epoch 8: 767.634993
	Epoch 9....
Epoch has taken 0:00:22.151501
Number of used sentences in train = 359
Total loss for epoch 9: 770.782949
	Epoch 10....
Epoch has taken 0:00:22.169265
Number of used sentences in train = 359
Total loss for epoch 10: 753.351964
	Epoch 11....
Epoch has taken 0:00:22.177599
Number of used sentences in train = 359
Total loss for epoch 11: 736.071698
	Epoch 12....
Epoch has taken 0:00:22.173354
Number of used sentences in train = 359
Total loss for epoch 12: 724.348686
	Epoch 13....
Epoch has taken 0:00:22.154784
Number of used sentences in train = 359
Total loss for epoch 13: 741.078442
	Epoch 14....
Epoch has taken 0:00:22.201906
Number of used sentences in train = 359
Total loss for epoch 14: 722.636141
Epoch has taken 0:00:22.367002

==================================================================================================
	Training time : 1:03:02.490876
==================================================================================================
	Identification : 0.362

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 112, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 48, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 32, 'lstmDropout': 0.12, 'denseActivation': 'tanh', 'wordDim': 94, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(1882, 94)
  (lstm): LSTM(142, 32, bidirectional=True)
  (linear1): Linear(in_features=512, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
	Epoch 0....
Number of used sentences in train = 2811
Total loss for epoch 0: 10527.540336
validation loss after epoch 0 : 863.229952
	Epoch 1....
Epoch has taken 0:03:06.251789
Number of used sentences in train = 2811
Total loss for epoch 1: 7491.315194
validation loss after epoch 1 : 875.349501
	Epoch 2....
Epoch has taken 0:03:06.370373
Number of used sentences in train = 2811
Total loss for epoch 2: 6673.058338
validation loss after epoch 2 : 854.806230
	Epoch 3....
Epoch has taken 0:02:40.465200
Number of used sentences in train = 2811
Total loss for epoch 3: 6117.543023
validation loss after epoch 3 : 890.680721
	Epoch 4....
Epoch has taken 0:02:39.300993
Number of used sentences in train = 2811
Total loss for epoch 4: 5752.861763
validation loss after epoch 4 : 923.665107
	Epoch 5....
Epoch has taken 0:02:39.177879
Number of used sentences in train = 2811
Total loss for epoch 5: 5411.154973
validation loss after epoch 5 : 947.678808
	Epoch 6....
Epoch has taken 0:02:40.824218
Number of used sentences in train = 2811
Total loss for epoch 6: 5186.360581
validation loss after epoch 6 : 1006.813845
	Epoch 7....
Epoch has taken 0:02:40.762568
Number of used sentences in train = 2811
Total loss for epoch 7: 5002.563229
validation loss after epoch 7 : 1037.872531
	Epoch 8....
Epoch has taken 0:02:38.998383
Number of used sentences in train = 2811
Total loss for epoch 8: 4863.427956
validation loss after epoch 8 : 1108.963652
	Epoch 9....
Epoch has taken 0:02:39.253708
Number of used sentences in train = 2811
Total loss for epoch 9: 4782.741394
validation loss after epoch 9 : 1159.696258
	Epoch 10....
Epoch has taken 0:02:53.834322
Number of used sentences in train = 2811
Total loss for epoch 10: 4694.415303
validation loss after epoch 10 : 1192.280647
	Epoch 11....
Epoch has taken 0:02:42.372298
Number of used sentences in train = 2811
Total loss for epoch 11: 4645.619199
validation loss after epoch 11 : 1202.322185
	Epoch 12....
Epoch has taken 0:02:56.328742
Number of used sentences in train = 2811
Total loss for epoch 12: 4607.832260
validation loss after epoch 12 : 1240.272650
	Epoch 13....
Epoch has taken 0:02:49.417041
Number of used sentences in train = 2811
Total loss for epoch 13: 4580.263555
validation loss after epoch 13 : 1297.119875
	Epoch 14....
Epoch has taken 0:02:59.257705
Number of used sentences in train = 2811
Total loss for epoch 14: 4559.028439
validation loss after epoch 14 : 1314.663202
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(1882, 94)
  (lstm): LSTM(142, 32, bidirectional=True)
  (linear1): Linear(in_features=512, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:40.793444
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2152.066336
	Epoch 1....
Epoch has taken 0:00:16.934452
Number of used sentences in train = 313
Total loss for epoch 1: 667.815555
	Epoch 2....
Epoch has taken 0:00:16.956581
Number of used sentences in train = 313
Total loss for epoch 2: 570.766574
	Epoch 3....
Epoch has taken 0:00:16.941711
Number of used sentences in train = 313
Total loss for epoch 3: 533.341342
	Epoch 4....
Epoch has taken 0:00:16.902831
Number of used sentences in train = 313
Total loss for epoch 4: 515.951989
	Epoch 5....
Epoch has taken 0:00:16.924711
Number of used sentences in train = 313
Total loss for epoch 5: 509.384878
	Epoch 6....
Epoch has taken 0:00:16.950270
Number of used sentences in train = 313
Total loss for epoch 6: 507.603034
	Epoch 7....
Epoch has taken 0:00:16.788153
Number of used sentences in train = 313
Total loss for epoch 7: 507.313767
	Epoch 8....
Epoch has taken 0:00:16.770256
Number of used sentences in train = 313
Total loss for epoch 8: 508.707425
	Epoch 9....
Epoch has taken 0:00:16.784114
Number of used sentences in train = 313
Total loss for epoch 9: 506.051830
	Epoch 10....
Epoch has taken 0:00:16.804313
Number of used sentences in train = 313
Total loss for epoch 10: 504.037739
	Epoch 11....
Epoch has taken 0:00:16.793616
Number of used sentences in train = 313
Total loss for epoch 11: 503.318390
	Epoch 12....
Epoch has taken 0:00:16.769486
Number of used sentences in train = 313
Total loss for epoch 12: 502.498469
	Epoch 13....
Epoch has taken 0:00:16.805611
Number of used sentences in train = 313
Total loss for epoch 13: 502.514091
	Epoch 14....
Epoch has taken 0:00:16.770367
Number of used sentences in train = 313
Total loss for epoch 14: 501.932936
Epoch has taken 0:00:16.776729

==================================================================================================
	Training time : 0:46:06.587013
==================================================================================================
	Identification : 0.133

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(1680, 94)
  (lstm): LSTM(142, 32, bidirectional=True)
  (linear1): Linear(in_features=512, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8146.367887
validation loss after epoch 0 : 756.204630
	Epoch 1....
Epoch has taken 0:01:51.629397
Number of used sentences in train = 2074
Total loss for epoch 1: 5074.100858
validation loss after epoch 1 : 770.747119
	Epoch 2....
Epoch has taken 0:02:01.563149
Number of used sentences in train = 2074
Total loss for epoch 2: 4161.758514
validation loss after epoch 2 : 822.149859
	Epoch 3....
Epoch has taken 0:02:05.579698
Number of used sentences in train = 2074
Total loss for epoch 3: 3661.965492
validation loss after epoch 3 : 880.724801
	Epoch 4....
Epoch has taken 0:02:01.360086
Number of used sentences in train = 2074
Total loss for epoch 4: 3402.414324
validation loss after epoch 4 : 972.799118
	Epoch 5....
Epoch has taken 0:02:00.403602
Number of used sentences in train = 2074
Total loss for epoch 5: 3275.844706
validation loss after epoch 5 : 992.947095
	Epoch 6....
Epoch has taken 0:02:00.374816
Number of used sentences in train = 2074
Total loss for epoch 6: 3218.690680
validation loss after epoch 6 : 1024.991873
	Epoch 7....
Epoch has taken 0:02:01.622706
Number of used sentences in train = 2074
Total loss for epoch 7: 3199.242975
validation loss after epoch 7 : 1058.733682
	Epoch 8....
Epoch has taken 0:02:01.460882
Number of used sentences in train = 2074
Total loss for epoch 8: 3189.030528
validation loss after epoch 8 : 1068.345415
	Epoch 9....
Epoch has taken 0:02:00.349997
Number of used sentences in train = 2074
Total loss for epoch 9: 3175.867208
validation loss after epoch 9 : 1096.267915
	Epoch 10....
Epoch has taken 0:02:01.490958
Number of used sentences in train = 2074
Total loss for epoch 10: 3171.562378
validation loss after epoch 10 : 1101.210611
	Epoch 11....
Epoch has taken 0:02:01.555761
Number of used sentences in train = 2074
Total loss for epoch 11: 3165.802047
validation loss after epoch 11 : 1117.674172
	Epoch 12....
Epoch has taken 0:01:50.401308
Number of used sentences in train = 2074
Total loss for epoch 12: 3162.750599
validation loss after epoch 12 : 1131.499060
	Epoch 13....
Epoch has taken 0:01:48.696511
Number of used sentences in train = 2074
Total loss for epoch 13: 3162.080795
validation loss after epoch 13 : 1138.656391
	Epoch 14....
Epoch has taken 0:01:49.615429
Number of used sentences in train = 2074
Total loss for epoch 14: 3159.130535
validation loss after epoch 14 : 1158.101196
	TransitionClassifier(
  (p_embeddings): Embedding(18, 48)
  (w_embeddings): Embedding(1680, 94)
  (lstm): LSTM(142, 32, bidirectional=True)
  (linear1): Linear(in_features=512, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.867699
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1552.616935
	Epoch 1....
Epoch has taken 0:00:11.174380
Number of used sentences in train = 231
Total loss for epoch 1: 542.342242
	Epoch 2....
Epoch has taken 0:00:11.165591
Number of used sentences in train = 231
Total loss for epoch 2: 406.100069
	Epoch 3....
Epoch has taken 0:00:11.163104
Number of used sentences in train = 231
Total loss for epoch 3: 372.147597
	Epoch 4....
Epoch has taken 0:00:11.181788
Number of used sentences in train = 231
Total loss for epoch 4: 362.551368
	Epoch 5....
Epoch has taken 0:00:11.169720
Number of used sentences in train = 231
Total loss for epoch 5: 357.084174
	Epoch 6....
Epoch has taken 0:00:11.182570
Number of used sentences in train = 231
Total loss for epoch 6: 354.076367
	Epoch 7....
Epoch has taken 0:00:11.156593
Number of used sentences in train = 231
Total loss for epoch 7: 351.650515
	Epoch 8....
Epoch has taken 0:00:11.185977
Number of used sentences in train = 231
Total loss for epoch 8: 348.248862
	Epoch 9....
Epoch has taken 0:00:11.159493
Number of used sentences in train = 231
Total loss for epoch 9: 347.333847
	Epoch 10....
Epoch has taken 0:00:11.168721
Number of used sentences in train = 231
Total loss for epoch 10: 346.789390
	Epoch 11....
Epoch has taken 0:00:11.176196
Number of used sentences in train = 231
Total loss for epoch 11: 346.319276
	Epoch 12....
Epoch has taken 0:00:11.174185
Number of used sentences in train = 231
Total loss for epoch 12: 346.054211
	Epoch 13....
Epoch has taken 0:00:11.156597
Number of used sentences in train = 231
Total loss for epoch 13: 345.909540
	Epoch 14....
Epoch has taken 0:00:11.169251
Number of used sentences in train = 231
Total loss for epoch 14: 345.674557
Epoch has taken 0:00:11.176862

==================================================================================================
	Training time : 0:32:13.865636
==================================================================================================
	Identification : 0.244

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 48)
  (w_embeddings): Embedding(3369, 94)
  (lstm): LSTM(142, 32, bidirectional=True)
  (linear1): Linear(in_features=512, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12218.878288
validation loss after epoch 0 : 1118.263547
	Epoch 1....
Epoch has taken 0:03:33.205256
Number of used sentences in train = 3226
Total loss for epoch 1: 9369.128708
validation loss after epoch 1 : 1094.256403
	Epoch 2....
Epoch has taken 0:03:33.451115
Number of used sentences in train = 3226
Total loss for epoch 2: 8479.353274
validation loss after epoch 2 : 1110.914857
	Epoch 3....
Epoch has taken 0:03:31.561341
Number of used sentences in train = 3226
Total loss for epoch 3: 7891.542187
validation loss after epoch 3 : 1163.935978
	Epoch 4....
Epoch has taken 0:03:31.761926
Number of used sentences in train = 3226
Total loss for epoch 4: 7483.855690
validation loss after epoch 4 : 1215.894989
	Epoch 5....
Epoch has taken 0:03:33.421376
Number of used sentences in train = 3226
Total loss for epoch 5: 7171.279695
validation loss after epoch 5 : 1263.337954
	Epoch 6....
Epoch has taken 0:03:33.584671
Number of used sentences in train = 3226
Total loss for epoch 6: 6951.597024
validation loss after epoch 6 : 1320.799180
	Epoch 7....
Epoch has taken 0:03:31.466590
Number of used sentences in train = 3226
Total loss for epoch 7: 6783.922063
validation loss after epoch 7 : 1383.187238
	Epoch 8....
Epoch has taken 0:03:31.478064
Number of used sentences in train = 3226
Total loss for epoch 8: 6676.190656
validation loss after epoch 8 : 1408.807973
	Epoch 9....
Epoch has taken 0:03:33.500290
Number of used sentences in train = 3226
Total loss for epoch 9: 6581.068705
validation loss after epoch 9 : 1452.327997
	Epoch 10....
Epoch has taken 0:03:33.782952
Number of used sentences in train = 3226
Total loss for epoch 10: 6493.212560
validation loss after epoch 10 : 1508.959682
	Epoch 11....
Epoch has taken 0:03:31.300892
Number of used sentences in train = 3226
Total loss for epoch 11: 6445.360399
validation loss after epoch 11 : 1517.620487
	Epoch 12....
Epoch has taken 0:03:31.421311
Number of used sentences in train = 3226
Total loss for epoch 12: 6397.082831
validation loss after epoch 12 : 1630.833446
	Epoch 13....
Epoch has taken 0:03:31.437805
Number of used sentences in train = 3226
Total loss for epoch 13: 6355.670478
validation loss after epoch 13 : 1647.117554
	Epoch 14....
Epoch has taken 0:03:33.671481
Number of used sentences in train = 3226
Total loss for epoch 14: 6324.110566
validation loss after epoch 14 : 1674.653445
	TransitionClassifier(
  (p_embeddings): Embedding(13, 48)
  (w_embeddings): Embedding(3369, 94)
  (lstm): LSTM(142, 32, bidirectional=True)
  (linear1): Linear(in_features=512, out_features=112, bias=True)
  (linear2): Linear(in_features=112, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:07.156744
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2249.793051
	Epoch 1....
Epoch has taken 0:00:20.811599
Number of used sentences in train = 359
Total loss for epoch 1: 928.341432
	Epoch 2....
Epoch has taken 0:00:20.617431
Number of used sentences in train = 359
Total loss for epoch 2: 804.370601
	Epoch 3....
Epoch has taken 0:00:20.606296
Number of used sentences in train = 359
Total loss for epoch 3: 732.517363
	Epoch 4....
Epoch has taken 0:00:20.582325
Number of used sentences in train = 359
Total loss for epoch 4: 705.338326
	Epoch 5....
Epoch has taken 0:00:20.604580
Number of used sentences in train = 359
Total loss for epoch 5: 686.218217
	Epoch 6....
Epoch has taken 0:00:20.644412
Number of used sentences in train = 359
Total loss for epoch 6: 677.681104
	Epoch 7....
Epoch has taken 0:00:20.637401
Number of used sentences in train = 359
Total loss for epoch 7: 675.999240
	Epoch 8....
Epoch has taken 0:00:20.624537
Number of used sentences in train = 359
Total loss for epoch 8: 674.776888
	Epoch 9....
Epoch has taken 0:00:20.635325
Number of used sentences in train = 359
Total loss for epoch 9: 674.414053
	Epoch 10....
Epoch has taken 0:00:20.599140
Number of used sentences in train = 359
Total loss for epoch 10: 673.396384
	Epoch 11....
Epoch has taken 0:00:20.619118
Number of used sentences in train = 359
Total loss for epoch 11: 672.466836
	Epoch 12....
Epoch has taken 0:00:20.621190
Number of used sentences in train = 359
Total loss for epoch 12: 671.593154
	Epoch 13....
Epoch has taken 0:00:20.609262
Number of used sentences in train = 359
Total loss for epoch 13: 671.525099
	Epoch 14....
Epoch has taken 0:00:20.624365
Number of used sentences in train = 359
Total loss for epoch 14: 671.150026
Epoch has taken 0:00:20.619966

==================================================================================================
	Training time : 0:58:52.321441
==================================================================================================
	Identification : 0.414

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 125, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 16, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 50, 'lstmDropout': 0.11, 'denseActivation': 'tanh', 'wordDim': 157, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1882, 157)
  (lstm): LSTM(173, 50, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=800, out_features=125, bias=True)
  (linear2): Linear(in_features=125, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10486.419416
validation loss after epoch 0 : 834.897750
	Epoch 1....
Epoch has taken 0:02:51.714803
Number of used sentences in train = 2811
Total loss for epoch 1: 6997.906082
validation loss after epoch 1 : 820.505764
	Epoch 2....
Epoch has taken 0:02:51.955209
Number of used sentences in train = 2811
Total loss for epoch 2: 6087.916721
validation loss after epoch 2 : 829.765107
	Epoch 3....
Epoch has taken 0:02:52.365092
Number of used sentences in train = 2811
Total loss for epoch 3: 5582.448718
validation loss after epoch 3 : 855.528170
	Epoch 4....
Epoch has taken 0:02:50.490659
Number of used sentences in train = 2811
Total loss for epoch 4: 5311.143658
validation loss after epoch 4 : 899.373677
	Epoch 5....
Epoch has taken 0:02:50.625089
Number of used sentences in train = 2811
Total loss for epoch 5: 5068.341220
validation loss after epoch 5 : 925.767772
	Epoch 6....
Epoch has taken 0:02:52.336342
Number of used sentences in train = 2811
Total loss for epoch 6: 4960.528700
validation loss after epoch 6 : 922.084554
	Epoch 7....
Epoch has taken 0:03:06.513993
Number of used sentences in train = 2811
Total loss for epoch 7: 4808.822429
validation loss after epoch 7 : 982.016146
	Epoch 8....
Epoch has taken 0:03:06.971436
Number of used sentences in train = 2811
Total loss for epoch 8: 4727.392940
validation loss after epoch 8 : 991.891847
	Epoch 9....
Epoch has taken 0:02:59.945606
Number of used sentences in train = 2811
Total loss for epoch 9: 4705.498969
validation loss after epoch 9 : 1021.967600
	Epoch 10....
Epoch has taken 0:02:52.601320
Number of used sentences in train = 2811
Total loss for epoch 10: 4641.309729
validation loss after epoch 10 : 1074.360710
	Epoch 11....
Epoch has taken 0:03:18.829463
Number of used sentences in train = 2811
Total loss for epoch 11: 4605.891178
validation loss after epoch 11 : 1035.820324
	Epoch 12....
Epoch has taken 0:02:51.201596
Number of used sentences in train = 2811
Total loss for epoch 12: 4591.522839
validation loss after epoch 12 : 1051.198578
	Epoch 13....
Epoch has taken 0:02:51.875622
Number of used sentences in train = 2811
Total loss for epoch 13: 4559.084782
validation loss after epoch 13 : 1072.452248
	Epoch 14....
Epoch has taken 0:02:52.377400
Number of used sentences in train = 2811
Total loss for epoch 14: 4554.190538
validation loss after epoch 14 : 1057.693614
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1882, 157)
  (lstm): LSTM(173, 50, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=800, out_features=125, bias=True)
  (linear2): Linear(in_features=125, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.521035
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1434.443255
	Epoch 1....
Epoch has taken 0:00:17.998191
Number of used sentences in train = 313
Total loss for epoch 1: 726.993843
	Epoch 2....
Epoch has taken 0:00:17.966432
Number of used sentences in train = 313
Total loss for epoch 2: 599.527685
	Epoch 3....
Epoch has taken 0:00:17.985736
Number of used sentences in train = 313
Total loss for epoch 3: 549.825752
	Epoch 4....
Epoch has taken 0:00:17.994501
Number of used sentences in train = 313
Total loss for epoch 4: 532.925240
	Epoch 5....
Epoch has taken 0:00:17.968621
Number of used sentences in train = 313
Total loss for epoch 5: 521.580221
	Epoch 6....
Epoch has taken 0:00:17.970377
Number of used sentences in train = 313
Total loss for epoch 6: 512.818341
	Epoch 7....
Epoch has taken 0:00:17.993137
Number of used sentences in train = 313
Total loss for epoch 7: 512.308921
	Epoch 8....
Epoch has taken 0:00:17.973991
Number of used sentences in train = 313
Total loss for epoch 8: 508.198532
	Epoch 9....
Epoch has taken 0:00:18.016376
Number of used sentences in train = 313
Total loss for epoch 9: 506.942777
	Epoch 10....
Epoch has taken 0:00:17.996682
Number of used sentences in train = 313
Total loss for epoch 10: 505.986408
	Epoch 11....
Epoch has taken 0:00:17.998505
Number of used sentences in train = 313
Total loss for epoch 11: 505.655948
	Epoch 12....
Epoch has taken 0:00:18.000799
Number of used sentences in train = 313
Total loss for epoch 12: 505.121259
	Epoch 13....
Epoch has taken 0:00:18.019731
Number of used sentences in train = 313
Total loss for epoch 13: 504.942097
	Epoch 14....
Epoch has taken 0:00:17.999761
Number of used sentences in train = 313
Total loss for epoch 14: 504.532674
Epoch has taken 0:00:18.007441

==================================================================================================
	Training time : 0:48:32.716475
==================================================================================================
	Identification : 0.435

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1680, 157)
  (lstm): LSTM(173, 50, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=800, out_features=125, bias=True)
  (linear2): Linear(in_features=125, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9810.655704
validation loss after epoch 0 : 749.523162
	Epoch 1....
Epoch has taken 0:01:58.071798
Number of used sentences in train = 2074
Total loss for epoch 1: 5133.140081
validation loss after epoch 1 : 690.434551
	Epoch 2....
Epoch has taken 0:01:58.126571
Number of used sentences in train = 2074
Total loss for epoch 2: 4223.341109
validation loss after epoch 2 : 714.480376
	Epoch 3....
Epoch has taken 0:02:14.463554
Number of used sentences in train = 2074
Total loss for epoch 3: 3776.826113
validation loss after epoch 3 : 812.704395
	Epoch 4....
Epoch has taken 0:01:56.889458
Number of used sentences in train = 2074
Total loss for epoch 4: 3507.590053
validation loss after epoch 4 : 843.024152
	Epoch 5....
Epoch has taken 0:01:56.889827
Number of used sentences in train = 2074
Total loss for epoch 5: 3368.544090
validation loss after epoch 5 : 844.392776
	Epoch 6....
Epoch has taken 0:01:57.835563
Number of used sentences in train = 2074
Total loss for epoch 6: 3290.147017
validation loss after epoch 6 : 899.843972
	Epoch 7....
Epoch has taken 0:01:58.197441
Number of used sentences in train = 2074
Total loss for epoch 7: 3261.648802
validation loss after epoch 7 : 907.045787
	Epoch 8....
Epoch has taken 0:01:59.510587
Number of used sentences in train = 2074
Total loss for epoch 8: 3246.080720
validation loss after epoch 8 : 909.968121
	Epoch 9....
Epoch has taken 0:01:57.350807
Number of used sentences in train = 2074
Total loss for epoch 9: 3209.137060
validation loss after epoch 9 : 963.448373
	Epoch 10....
Epoch has taken 0:01:56.400367
Number of used sentences in train = 2074
Total loss for epoch 10: 3193.864058
validation loss after epoch 10 : 957.109081
	Epoch 11....
Epoch has taken 0:01:57.651892
Number of used sentences in train = 2074
Total loss for epoch 11: 3196.289847
validation loss after epoch 11 : 977.570333
	Epoch 12....
Epoch has taken 0:02:15.706953
Number of used sentences in train = 2074
Total loss for epoch 12: 3179.231316
validation loss after epoch 12 : 961.406509
	Epoch 13....
Epoch has taken 0:02:15.657564
Number of used sentences in train = 2074
Total loss for epoch 13: 3170.600843
validation loss after epoch 13 : 1047.241929
	Epoch 14....
Epoch has taken 0:01:57.728354
Number of used sentences in train = 2074
Total loss for epoch 14: 3166.235353
validation loss after epoch 14 : 1000.139627
	TransitionClassifier(
  (p_embeddings): Embedding(18, 16)
  (w_embeddings): Embedding(1680, 157)
  (lstm): LSTM(173, 50, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=800, out_features=125, bias=True)
  (linear2): Linear(in_features=125, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.645329
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1851.363010
	Epoch 1....
Epoch has taken 0:00:13.788206
Number of used sentences in train = 231
Total loss for epoch 1: 536.210273
	Epoch 2....
Epoch has taken 0:00:13.787699
Number of used sentences in train = 231
Total loss for epoch 2: 397.058243
	Epoch 3....
Epoch has taken 0:00:13.781614
Number of used sentences in train = 231
Total loss for epoch 3: 368.496332
	Epoch 4....
Epoch has taken 0:00:13.795850
Number of used sentences in train = 231
Total loss for epoch 4: 354.461583
	Epoch 5....
Epoch has taken 0:00:13.780715
Number of used sentences in train = 231
Total loss for epoch 5: 349.547138
	Epoch 6....
Epoch has taken 0:00:13.798007
Number of used sentences in train = 231
Total loss for epoch 6: 346.944199
	Epoch 7....
Epoch has taken 0:00:13.783499
Number of used sentences in train = 231
Total loss for epoch 7: 346.606487
	Epoch 8....
Epoch has taken 0:00:13.805994
Number of used sentences in train = 231
Total loss for epoch 8: 346.175255
	Epoch 9....
Epoch has taken 0:00:13.786668
Number of used sentences in train = 231
Total loss for epoch 9: 345.739448
	Epoch 10....
Epoch has taken 0:00:13.765597
Number of used sentences in train = 231
Total loss for epoch 10: 346.202658
	Epoch 11....
Epoch has taken 0:00:13.792987
Number of used sentences in train = 231
Total loss for epoch 11: 345.756084
	Epoch 12....
Epoch has taken 0:00:13.789906
Number of used sentences in train = 231
Total loss for epoch 12: 345.250116
	Epoch 13....
Epoch has taken 0:00:13.786127
Number of used sentences in train = 231
Total loss for epoch 13: 345.033982
	Epoch 14....
Epoch has taken 0:00:13.791247
Number of used sentences in train = 231
Total loss for epoch 14: 344.931951
Epoch has taken 0:00:13.771408

==================================================================================================
	Training time : 0:33:45.296784
==================================================================================================
	Identification : 0.407

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(3369, 157)
  (lstm): LSTM(173, 50, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=800, out_features=125, bias=True)
  (linear2): Linear(in_features=125, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17599.192518
validation loss after epoch 0 : 1178.359621
	Epoch 1....
Epoch has taken 0:03:46.043927
Number of used sentences in train = 3226
Total loss for epoch 1: 9695.201978
validation loss after epoch 1 : 1085.586962
	Epoch 2....
Epoch has taken 0:03:46.002554
Number of used sentences in train = 3226
Total loss for epoch 2: 8580.995848
validation loss after epoch 2 : 1126.273654
	Epoch 3....
Epoch has taken 0:03:46.042686
Number of used sentences in train = 3226
Total loss for epoch 3: 7937.966528
validation loss after epoch 3 : 1134.789307
	Epoch 4....
Epoch has taken 0:03:48.273916
Number of used sentences in train = 3226
Total loss for epoch 4: 7465.744343
validation loss after epoch 4 : 1220.321399
	Epoch 5....
Epoch has taken 0:03:50.988561
Number of used sentences in train = 3226
Total loss for epoch 5: 7104.848211
validation loss after epoch 5 : 1249.015587
	Epoch 6....
Epoch has taken 0:03:48.526006
Number of used sentences in train = 3226
Total loss for epoch 6: 6894.648647
validation loss after epoch 6 : 1351.632099
	Epoch 7....
Epoch has taken 0:03:47.968911
Number of used sentences in train = 3226
Total loss for epoch 7: 6745.880272
validation loss after epoch 7 : 1407.571993
	Epoch 8....
Epoch has taken 0:03:46.372142
Number of used sentences in train = 3226
Total loss for epoch 8: 6588.977412
validation loss after epoch 8 : 1415.149953
	Epoch 9....
Epoch has taken 0:03:45.990277
Number of used sentences in train = 3226
Total loss for epoch 9: 6537.709507
validation loss after epoch 9 : 1504.725453
	Epoch 10....
Epoch has taken 0:03:48.012289
Number of used sentences in train = 3226
Total loss for epoch 10: 6471.854884
validation loss after epoch 10 : 1590.190334
	Epoch 11....
Epoch has taken 0:03:48.091704
Number of used sentences in train = 3226
Total loss for epoch 11: 6419.803860
validation loss after epoch 11 : 1613.421315
	Epoch 12....
Epoch has taken 0:03:48.573178
Number of used sentences in train = 3226
Total loss for epoch 12: 6373.931003
validation loss after epoch 12 : 1622.477226
	Epoch 13....
Epoch has taken 0:03:46.454756
Number of used sentences in train = 3226
Total loss for epoch 13: 6317.646443
validation loss after epoch 13 : 1664.446975
	Epoch 14....
Epoch has taken 0:03:47.879147
Number of used sentences in train = 3226
Total loss for epoch 14: 6291.225332
validation loss after epoch 14 : 1637.028036
	TransitionClassifier(
  (p_embeddings): Embedding(13, 16)
  (w_embeddings): Embedding(3369, 157)
  (lstm): LSTM(173, 50, num_layers=2, dropout=0.11, bidirectional=True)
  (linear1): Linear(in_features=800, out_features=125, bias=True)
  (linear2): Linear(in_features=125, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:48.789484
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2495.190564
	Epoch 1....
Epoch has taken 0:00:25.684333
Number of used sentences in train = 359
Total loss for epoch 1: 956.066411
	Epoch 2....
Epoch has taken 0:00:25.679813
Number of used sentences in train = 359
Total loss for epoch 2: 822.034684
	Epoch 3....
Epoch has taken 0:00:25.709000
Number of used sentences in train = 359
Total loss for epoch 3: 758.903738
	Epoch 4....
Epoch has taken 0:00:25.702773
Number of used sentences in train = 359
Total loss for epoch 4: 719.819276
	Epoch 5....
Epoch has taken 0:00:22.392703
Number of used sentences in train = 359
Total loss for epoch 5: 719.881652
	Epoch 6....
Epoch has taken 0:00:22.388714
Number of used sentences in train = 359
Total loss for epoch 6: 689.442204
	Epoch 7....
Epoch has taken 0:00:22.398431
Number of used sentences in train = 359
Total loss for epoch 7: 681.121287
	Epoch 8....
Epoch has taken 0:00:22.409850
Number of used sentences in train = 359
Total loss for epoch 8: 684.214834
	Epoch 9....
Epoch has taken 0:00:22.374805
Number of used sentences in train = 359
Total loss for epoch 9: 679.235566
	Epoch 10....
Epoch has taken 0:00:22.381579
Number of used sentences in train = 359
Total loss for epoch 10: 675.406285
	Epoch 11....
Epoch has taken 0:00:22.376835
Number of used sentences in train = 359
Total loss for epoch 11: 674.518376
	Epoch 12....
Epoch has taken 0:00:22.389370
Number of used sentences in train = 359
Total loss for epoch 12: 672.476681
	Epoch 13....
Epoch has taken 0:00:22.381277
Number of used sentences in train = 359
Total loss for epoch 13: 674.739537
	Epoch 14....
Epoch has taken 0:00:22.378196
Number of used sentences in train = 359
Total loss for epoch 14: 683.312163
Epoch has taken 0:00:22.367848

==================================================================================================
	Training time : 1:02:43.690709
==================================================================================================
	Identification : 0.358

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 29, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 28, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 28, 'lstmDropout': 0.15, 'denseActivation': 'tanh', 'wordDim': 99, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(1882, 99)
  (lstm): LSTM(127, 28, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 10986.459321
validation loss after epoch 0 : 869.778255
	Epoch 1....
Epoch has taken 0:02:51.153207
Number of used sentences in train = 2811
Total loss for epoch 1: 7431.019177
validation loss after epoch 1 : 840.272970
	Epoch 2....
Epoch has taken 0:02:50.943378
Number of used sentences in train = 2811
Total loss for epoch 2: 6688.456543
validation loss after epoch 2 : 850.095903
	Epoch 3....
Epoch has taken 0:02:51.369811
Number of used sentences in train = 2811
Total loss for epoch 3: 6218.633050
validation loss after epoch 3 : 909.197660
	Epoch 4....
Epoch has taken 0:02:51.354431
Number of used sentences in train = 2811
Total loss for epoch 4: 5905.459889
validation loss after epoch 4 : 916.224104
	Epoch 5....
Epoch has taken 0:02:49.565242
Number of used sentences in train = 2811
Total loss for epoch 5: 5672.171275
validation loss after epoch 5 : 932.654438
	Epoch 6....
Epoch has taken 0:02:50.862702
Number of used sentences in train = 2811
Total loss for epoch 6: 5462.691139
validation loss after epoch 6 : 945.068369
	Epoch 7....
Epoch has taken 0:02:51.575902
Number of used sentences in train = 2811
Total loss for epoch 7: 5324.816357
validation loss after epoch 7 : 1014.046963
	Epoch 8....
Epoch has taken 0:02:52.492521
Number of used sentences in train = 2811
Total loss for epoch 8: 5152.132466
validation loss after epoch 8 : 1076.264523
	Epoch 9....
Epoch has taken 0:02:53.506975
Number of used sentences in train = 2811
Total loss for epoch 9: 5062.837360
validation loss after epoch 9 : 1060.434349
	Epoch 10....
Epoch has taken 0:02:55.959895
Number of used sentences in train = 2811
Total loss for epoch 10: 4987.073951
validation loss after epoch 10 : 1045.512325
	Epoch 11....
Epoch has taken 0:02:49.488298
Number of used sentences in train = 2811
Total loss for epoch 11: 4912.451756
validation loss after epoch 11 : 1080.297689
	Epoch 12....
Epoch has taken 0:02:50.091594
Number of used sentences in train = 2811
Total loss for epoch 12: 4869.049098
validation loss after epoch 12 : 1080.455622
	Epoch 13....
Epoch has taken 0:02:51.234106
Number of used sentences in train = 2811
Total loss for epoch 13: 4828.684636
validation loss after epoch 13 : 1064.767048
	Epoch 14....
Epoch has taken 0:02:50.937292
Number of used sentences in train = 2811
Total loss for epoch 14: 4818.189484
validation loss after epoch 14 : 1123.510328
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(1882, 99)
  (lstm): LSTM(127, 28, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:17.724678
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1378.241899
	Epoch 1....
Epoch has taken 0:00:18.126824
Number of used sentences in train = 313
Total loss for epoch 1: 726.058675
	Epoch 2....
Epoch has taken 0:00:18.137072
Number of used sentences in train = 313
Total loss for epoch 2: 623.519064
	Epoch 3....
Epoch has taken 0:00:18.144473
Number of used sentences in train = 313
Total loss for epoch 3: 562.869820
	Epoch 4....
Epoch has taken 0:00:18.093125
Number of used sentences in train = 313
Total loss for epoch 4: 543.219399
	Epoch 5....
Epoch has taken 0:00:18.132915
Number of used sentences in train = 313
Total loss for epoch 5: 536.795795
	Epoch 6....
Epoch has taken 0:00:18.126223
Number of used sentences in train = 313
Total loss for epoch 6: 519.661660
	Epoch 7....
Epoch has taken 0:00:18.123324
Number of used sentences in train = 313
Total loss for epoch 7: 512.526534
	Epoch 8....
Epoch has taken 0:00:18.141942
Number of used sentences in train = 313
Total loss for epoch 8: 511.854766
	Epoch 9....
Epoch has taken 0:00:18.143118
Number of used sentences in train = 313
Total loss for epoch 9: 508.398055
	Epoch 10....
Epoch has taken 0:00:18.131123
Number of used sentences in train = 313
Total loss for epoch 10: 507.515881
	Epoch 11....
Epoch has taken 0:00:18.138525
Number of used sentences in train = 313
Total loss for epoch 11: 505.656868
	Epoch 12....
Epoch has taken 0:00:18.107735
Number of used sentences in train = 313
Total loss for epoch 12: 512.851567
	Epoch 13....
Epoch has taken 0:00:18.145146
Number of used sentences in train = 313
Total loss for epoch 13: 508.506458
	Epoch 14....
Epoch has taken 0:00:18.148282
Number of used sentences in train = 313
Total loss for epoch 14: 508.806728
Epoch has taken 0:00:18.118664

==================================================================================================
	Training time : 0:47:50.719673
==================================================================================================
	Identification : 0.195

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(1680, 99)
  (lstm): LSTM(127, 28, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 7990.719623
validation loss after epoch 0 : 640.698518
	Epoch 1....
Epoch has taken 0:01:56.303864
Number of used sentences in train = 2074
Total loss for epoch 1: 4997.869869
validation loss after epoch 1 : 623.750633
	Epoch 2....
Epoch has taken 0:01:56.281934
Number of used sentences in train = 2074
Total loss for epoch 2: 4339.840017
validation loss after epoch 2 : 648.638023
	Epoch 3....
Epoch has taken 0:01:57.666229
Number of used sentences in train = 2074
Total loss for epoch 3: 3934.242014
validation loss after epoch 3 : 675.693813
	Epoch 4....
Epoch has taken 0:01:56.456008
Number of used sentences in train = 2074
Total loss for epoch 4: 3651.339437
validation loss after epoch 4 : 709.149944
	Epoch 5....
Epoch has taken 0:01:57.350453
Number of used sentences in train = 2074
Total loss for epoch 5: 3515.245847
validation loss after epoch 5 : 779.879332
	Epoch 6....
Epoch has taken 0:01:57.354230
Number of used sentences in train = 2074
Total loss for epoch 6: 3412.088562
validation loss after epoch 6 : 759.406366
	Epoch 7....
Epoch has taken 0:01:58.018454
Number of used sentences in train = 2074
Total loss for epoch 7: 3349.477303
validation loss after epoch 7 : 904.619433
	Epoch 8....
Epoch has taken 0:01:56.574578
Number of used sentences in train = 2074
Total loss for epoch 8: 3316.447454
validation loss after epoch 8 : 788.306979
	Epoch 9....
Epoch has taken 0:01:57.842109
Number of used sentences in train = 2074
Total loss for epoch 9: 3287.491071
validation loss after epoch 9 : 879.378132
	Epoch 10....
Epoch has taken 0:02:15.476689
Number of used sentences in train = 2074
Total loss for epoch 10: 3269.975749
validation loss after epoch 10 : 867.871464
	Epoch 11....
Epoch has taken 0:01:57.494697
Number of used sentences in train = 2074
Total loss for epoch 11: 3246.374677
validation loss after epoch 11 : 891.799308
	Epoch 12....
Epoch has taken 0:01:56.316689
Number of used sentences in train = 2074
Total loss for epoch 12: 3236.719092
validation loss after epoch 12 : 830.165768
	Epoch 13....
Epoch has taken 0:01:57.579908
Number of used sentences in train = 2074
Total loss for epoch 13: 3240.906387
validation loss after epoch 13 : 874.660625
	Epoch 14....
Epoch has taken 0:01:57.629688
Number of used sentences in train = 2074
Total loss for epoch 14: 3207.715161
validation loss after epoch 14 : 890.296231
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(1680, 99)
  (lstm): LSTM(127, 28, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.591019
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1107.678596
	Epoch 1....
Epoch has taken 0:00:11.991693
Number of used sentences in train = 231
Total loss for epoch 1: 525.442012
	Epoch 2....
Epoch has taken 0:00:11.951955
Number of used sentences in train = 231
Total loss for epoch 2: 440.876885
	Epoch 3....
Epoch has taken 0:00:11.991738
Number of used sentences in train = 231
Total loss for epoch 3: 398.659021
	Epoch 4....
Epoch has taken 0:00:11.951846
Number of used sentences in train = 231
Total loss for epoch 4: 383.430929
	Epoch 5....
Epoch has taken 0:00:11.981261
Number of used sentences in train = 231
Total loss for epoch 5: 358.526268
	Epoch 6....
Epoch has taken 0:00:11.959173
Number of used sentences in train = 231
Total loss for epoch 6: 353.211016
	Epoch 7....
Epoch has taken 0:00:11.973921
Number of used sentences in train = 231
Total loss for epoch 7: 350.942896
	Epoch 8....
Epoch has taken 0:00:11.959495
Number of used sentences in train = 231
Total loss for epoch 8: 349.671208
	Epoch 9....
Epoch has taken 0:00:11.984478
Number of used sentences in train = 231
Total loss for epoch 9: 349.209677
	Epoch 10....
Epoch has taken 0:00:11.952306
Number of used sentences in train = 231
Total loss for epoch 10: 347.993812
	Epoch 11....
Epoch has taken 0:00:11.979413
Number of used sentences in train = 231
Total loss for epoch 11: 347.199850
	Epoch 12....
Epoch has taken 0:00:11.965495
Number of used sentences in train = 231
Total loss for epoch 12: 347.010213
	Epoch 13....
Epoch has taken 0:00:11.971290
Number of used sentences in train = 231
Total loss for epoch 13: 346.176780
	Epoch 14....
Epoch has taken 0:00:11.862135
Number of used sentences in train = 231
Total loss for epoch 14: 346.471584
Epoch has taken 0:00:11.871072

==================================================================================================
	Training time : 0:32:35.615995
==================================================================================================
	Identification : 0.245

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(3369, 99)
  (lstm): LSTM(127, 28, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 12796.742790
validation loss after epoch 0 : 1125.724131
	Epoch 1....
Epoch has taken 0:03:46.528781
Number of used sentences in train = 3226
Total loss for epoch 1: 9366.062830
validation loss after epoch 1 : 1066.832753
	Epoch 2....
Epoch has taken 0:03:46.874671
Number of used sentences in train = 3226
Total loss for epoch 2: 8550.797724
validation loss after epoch 2 : 1087.183634
	Epoch 3....
Epoch has taken 0:03:47.405835
Number of used sentences in train = 3226
Total loss for epoch 3: 7983.927875
validation loss after epoch 3 : 1104.371080
	Epoch 4....
Epoch has taken 0:03:47.278811
Number of used sentences in train = 3226
Total loss for epoch 4: 7587.012423
validation loss after epoch 4 : 1143.936093
	Epoch 5....
Epoch has taken 0:03:45.085062
Number of used sentences in train = 3226
Total loss for epoch 5: 7270.707151
validation loss after epoch 5 : 1181.673126
	Epoch 6....
Epoch has taken 0:03:46.761275
Number of used sentences in train = 3226
Total loss for epoch 6: 7023.230390
validation loss after epoch 6 : 1301.068710
	Epoch 7....
Epoch has taken 0:03:47.481615
Number of used sentences in train = 3226
Total loss for epoch 7: 6885.776470
validation loss after epoch 7 : 1257.369104
	Epoch 8....
Epoch has taken 0:04:22.346360
Number of used sentences in train = 3226
Total loss for epoch 8: 6756.802546
validation loss after epoch 8 : 1330.916683
	Epoch 9....
Epoch has taken 0:03:47.109919
Number of used sentences in train = 3226
Total loss for epoch 9: 6603.099532
validation loss after epoch 9 : 1373.246816
	Epoch 10....
Epoch has taken 0:03:45.219136
Number of used sentences in train = 3226
Total loss for epoch 10: 6579.263070
validation loss after epoch 10 : 1454.613206
	Epoch 11....
Epoch has taken 0:03:47.017158
Number of used sentences in train = 3226
Total loss for epoch 11: 6500.642660
validation loss after epoch 11 : 1437.873668
	Epoch 12....
Epoch has taken 0:03:47.481268
Number of used sentences in train = 3226
Total loss for epoch 12: 6439.986865
validation loss after epoch 12 : 1534.246699
	Epoch 13....
Epoch has taken 0:04:22.336885
Number of used sentences in train = 3226
Total loss for epoch 13: 6415.586664
validation loss after epoch 13 : 1540.125030
	Epoch 14....
Epoch has taken 0:03:47.906329
Number of used sentences in train = 3226
Total loss for epoch 14: 6396.389911
validation loss after epoch 14 : 1581.418788
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(3369, 99)
  (lstm): LSTM(127, 28, num_layers=2, dropout=0.15, bidirectional=True)
  (linear1): Linear(in_features=448, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:45.971520
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1484.244216
	Epoch 1....
Epoch has taken 0:00:22.255137
Number of used sentences in train = 359
Total loss for epoch 1: 968.311457
	Epoch 2....
Epoch has taken 0:00:22.250491
Number of used sentences in train = 359
Total loss for epoch 2: 830.310011
	Epoch 3....
Epoch has taken 0:00:22.335588
Number of used sentences in train = 359
Total loss for epoch 3: 765.870789
	Epoch 4....
Epoch has taken 0:00:22.355995
Number of used sentences in train = 359
Total loss for epoch 4: 734.025467
	Epoch 5....
Epoch has taken 0:00:22.395648
Number of used sentences in train = 359
Total loss for epoch 5: 718.441516
	Epoch 6....
Epoch has taken 0:00:22.364891
Number of used sentences in train = 359
Total loss for epoch 6: 699.703021
	Epoch 7....
Epoch has taken 0:00:22.362289
Number of used sentences in train = 359
Total loss for epoch 7: 697.169199
	Epoch 8....
Epoch has taken 0:00:22.360000
Number of used sentences in train = 359
Total loss for epoch 8: 685.573211
	Epoch 9....
Epoch has taken 0:00:22.367248
Number of used sentences in train = 359
Total loss for epoch 9: 685.466171
	Epoch 10....
Epoch has taken 0:00:22.382682
Number of used sentences in train = 359
Total loss for epoch 10: 681.236936
	Epoch 11....
Epoch has taken 0:00:22.358304
Number of used sentences in train = 359
Total loss for epoch 11: 675.774468
	Epoch 12....
Epoch has taken 0:00:22.361445
Number of used sentences in train = 359
Total loss for epoch 12: 673.198470
	Epoch 13....
Epoch has taken 0:00:22.363424
Number of used sentences in train = 359
Total loss for epoch 13: 673.539862
	Epoch 14....
Epoch has taken 0:00:22.325692
Number of used sentences in train = 359
Total loss for epoch 14: 678.033186
Epoch has taken 0:00:22.341937

==================================================================================================
	Training time : 1:03:28.644063
==================================================================================================
	Identification : 0.187

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 34, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 53, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 131, 'lstmDropout': 0.12, 'denseActivation': 'tanh', 'wordDim': 58, 'batch': 1}False,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07
	TransitionClassifier(
  (p_embeddings): Embedding(18, 53)
  (w_embeddings): Embedding(1882, 58)
  (lstm): LSTM(111, 131, bidirectional=True)
  (linear1): Linear(in_features=2096, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Number of used sentences in train = 2811
Total loss for epoch 0: 11174.684600
validation loss after epoch 0 : 876.370240
	Epoch 1....
Epoch has taken 0:02:40.075512
Number of used sentences in train = 2811
Total loss for epoch 1: 7944.790795
validation loss after epoch 1 : 883.034132
	Epoch 2....
Epoch has taken 0:02:40.101111
Number of used sentences in train = 2811
Total loss for epoch 2: 7207.538394
validation loss after epoch 2 : 812.955874
	Epoch 3....
Epoch has taken 0:02:40.208080
Number of used sentences in train = 2811
Total loss for epoch 3: 6634.320323
validation loss after epoch 3 : 793.937282
	Epoch 4....
Epoch has taken 0:02:41.855169
Number of used sentences in train = 2811
Total loss for epoch 4: 6196.248538
validation loss after epoch 4 : 832.600206
	Epoch 5....
Epoch has taken 0:02:41.459566
Number of used sentences in train = 2811
Total loss for epoch 5: 5984.130213
validation loss after epoch 5 : 832.076512
	Epoch 6....
Epoch has taken 0:02:41.650643
Number of used sentences in train = 2811
Total loss for epoch 6: 5621.722097
validation loss after epoch 6 : 880.080075
	Epoch 7....
Epoch has taken 0:02:39.952559
Number of used sentences in train = 2811
Total loss for epoch 7: 5392.394971
validation loss after epoch 7 : 862.057982
	Epoch 8....
Epoch has taken 0:02:41.553295
Number of used sentences in train = 2811
Total loss for epoch 8: 5224.295708
validation loss after epoch 8 : 908.621498
	Epoch 9....
Epoch has taken 0:03:07.200550
Number of used sentences in train = 2811
Total loss for epoch 9: 5110.179142
validation loss after epoch 9 : 931.634334
	Epoch 10....
Epoch has taken 0:02:41.749425
Number of used sentences in train = 2811
Total loss for epoch 10: 4998.865245
validation loss after epoch 10 : 950.826409
	Epoch 11....
Epoch has taken 0:02:40.120228
Number of used sentences in train = 2811
Total loss for epoch 11: 4918.073810
validation loss after epoch 11 : 962.943485
	Epoch 12....
Epoch has taken 0:02:40.330325
Number of used sentences in train = 2811
Total loss for epoch 12: 4811.875476
validation loss after epoch 12 : 996.498753
	Epoch 13....
Epoch has taken 0:02:41.874786
Number of used sentences in train = 2811
Total loss for epoch 13: 4747.479017
validation loss after epoch 13 : 996.564396
	Epoch 14....
Epoch has taken 0:02:42.157851
Number of used sentences in train = 2811
Total loss for epoch 14: 4718.617317
validation loss after epoch 14 : 1015.377725
	TransitionClassifier(
  (p_embeddings): Embedding(18, 53)
  (w_embeddings): Embedding(1882, 58)
  (lstm): LSTM(111, 131, bidirectional=True)
  (linear1): Linear(in_features=2096, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:41.886600
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1254.583255
	Epoch 1....
Epoch has taken 0:00:16.982726
Number of used sentences in train = 313
Total loss for epoch 1: 842.634423
	Epoch 2....
Epoch has taken 0:00:16.952511
Number of used sentences in train = 313
Total loss for epoch 2: 756.893904
	Epoch 3....
Epoch has taken 0:00:16.967256
Number of used sentences in train = 313
Total loss for epoch 3: 669.778442
	Epoch 4....
Epoch has taken 0:00:16.967743
Number of used sentences in train = 313
Total loss for epoch 4: 611.694154
	Epoch 5....
Epoch has taken 0:00:16.974388
Number of used sentences in train = 313
Total loss for epoch 5: 583.627686
	Epoch 6....
Epoch has taken 0:00:16.951596
Number of used sentences in train = 313
Total loss for epoch 6: 561.618738
	Epoch 7....
Epoch has taken 0:00:16.938619
Number of used sentences in train = 313
Total loss for epoch 7: 552.129550
	Epoch 8....
Epoch has taken 0:00:17.133723
Number of used sentences in train = 313
Total loss for epoch 8: 547.545300
	Epoch 9....
Epoch has taken 0:00:17.092555
Number of used sentences in train = 313
Total loss for epoch 9: 544.425363
	Epoch 10....
Epoch has taken 0:00:17.113688
Number of used sentences in train = 313
Total loss for epoch 10: 542.525773
	Epoch 11....
Epoch has taken 0:00:17.106006
Number of used sentences in train = 313
Total loss for epoch 11: 535.996699
	Epoch 12....
Epoch has taken 0:00:17.133351
Number of used sentences in train = 313
Total loss for epoch 12: 526.363886
	Epoch 13....
Epoch has taken 0:00:17.102179
Number of used sentences in train = 313
Total loss for epoch 13: 521.579424
	Epoch 14....
Epoch has taken 0:00:17.104936
Number of used sentences in train = 313
Total loss for epoch 14: 520.111833
Epoch has taken 0:00:17.132073

==================================================================================================
	Training time : 0:44:58.327835
==================================================================================================
	Identification : 0.255

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 53)
  (w_embeddings): Embedding(1680, 58)
  (lstm): LSTM(111, 131, bidirectional=True)
  (linear1): Linear(in_features=2096, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8635.378033
validation loss after epoch 0 : 722.996371
	Epoch 1....
Epoch has taken 0:01:50.812597
Number of used sentences in train = 2074
Total loss for epoch 1: 5715.015418
validation loss after epoch 1 : 670.975542
	Epoch 2....
Epoch has taken 0:01:50.904805
Number of used sentences in train = 2074
Total loss for epoch 2: 4954.198083
validation loss after epoch 2 : 634.517246
	Epoch 3....
Epoch has taken 0:01:50.804841
Number of used sentences in train = 2074
Total loss for epoch 3: 4457.589926
validation loss after epoch 3 : 669.916911
	Epoch 4....
Epoch has taken 0:02:01.284431
Number of used sentences in train = 2074
Total loss for epoch 4: 4070.254157
validation loss after epoch 4 : 709.732090
	Epoch 5....
Epoch has taken 0:01:51.797284
Number of used sentences in train = 2074
Total loss for epoch 5: 3835.278500
validation loss after epoch 5 : 693.572162
	Epoch 6....
Epoch has taken 0:01:52.381158
Number of used sentences in train = 2074
Total loss for epoch 6: 3604.996073
validation loss after epoch 6 : 741.979960
	Epoch 7....
Epoch has taken 0:01:55.677867
Number of used sentences in train = 2074
Total loss for epoch 7: 3498.215080
validation loss after epoch 7 : 737.918746
	Epoch 8....
Epoch has taken 0:01:51.818865
Number of used sentences in train = 2074
Total loss for epoch 8: 3399.350003
validation loss after epoch 8 : 732.625231
	Epoch 9....
Epoch has taken 0:02:05.184066
Number of used sentences in train = 2074
Total loss for epoch 9: 3352.163454
validation loss after epoch 9 : 766.195309
	Epoch 10....
Epoch has taken 0:02:10.634260
Number of used sentences in train = 2074
Total loss for epoch 10: 3301.279818
validation loss after epoch 10 : 768.413789
	Epoch 11....
Epoch has taken 0:02:10.285817
Number of used sentences in train = 2074
Total loss for epoch 11: 3268.950212
validation loss after epoch 11 : 779.630526
	Epoch 12....
Epoch has taken 0:02:04.416131
Number of used sentences in train = 2074
Total loss for epoch 12: 3250.965355
validation loss after epoch 12 : 785.932507
	Epoch 13....
Epoch has taken 0:02:04.129725
Number of used sentences in train = 2074
Total loss for epoch 13: 3238.884993
validation loss after epoch 13 : 802.172571
	Epoch 14....
Epoch has taken 0:02:02.497819
Number of used sentences in train = 2074
Total loss for epoch 14: 3228.485070
validation loss after epoch 14 : 812.130626
	TransitionClassifier(
  (p_embeddings): Embedding(18, 53)
  (w_embeddings): Embedding(1680, 58)
  (lstm): LSTM(111, 131, bidirectional=True)
  (linear1): Linear(in_features=2096, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:01.487220
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1137.971101
	Epoch 1....
Epoch has taken 0:00:12.446272
Number of used sentences in train = 231
Total loss for epoch 1: 619.051716
	Epoch 2....
Epoch has taken 0:00:12.459130
Number of used sentences in train = 231
Total loss for epoch 2: 524.927381
	Epoch 3....
Epoch has taken 0:00:12.438551
Number of used sentences in train = 231
Total loss for epoch 3: 457.934240
	Epoch 4....
Epoch has taken 0:00:12.377422
Number of used sentences in train = 231
Total loss for epoch 4: 420.899438
	Epoch 5....
Epoch has taken 0:00:12.434749
Number of used sentences in train = 231
Total loss for epoch 5: 389.834754
	Epoch 6....
Epoch has taken 0:00:12.448128
Number of used sentences in train = 231
Total loss for epoch 6: 383.566298
	Epoch 7....
Epoch has taken 0:00:12.436299
Number of used sentences in train = 231
Total loss for epoch 7: 373.060199
	Epoch 8....
Epoch has taken 0:00:12.445576
Number of used sentences in train = 231
Total loss for epoch 8: 370.487534
	Epoch 9....
Epoch has taken 0:00:12.433001
Number of used sentences in train = 231
Total loss for epoch 9: 366.804065
	Epoch 10....
Epoch has taken 0:00:12.533435
Number of used sentences in train = 231
Total loss for epoch 10: 365.371828
	Epoch 11....
Epoch has taken 0:00:12.530538
Number of used sentences in train = 231
Total loss for epoch 11: 364.531283
	Epoch 12....
Epoch has taken 0:00:12.555726
Number of used sentences in train = 231
Total loss for epoch 12: 363.758826
	Epoch 13....
Epoch has taken 0:00:12.546609
Number of used sentences in train = 231
Total loss for epoch 13: 361.001556
	Epoch 14....
Epoch has taken 0:00:12.561607
Number of used sentences in train = 231
Total loss for epoch 14: 360.545517
Epoch has taken 0:00:12.531099

==================================================================================================
	Training time : 0:32:51.628647
==================================================================================================
	Identification : 0.221

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 53)
  (w_embeddings): Embedding(3369, 58)
  (lstm): LSTM(111, 131, bidirectional=True)
  (linear1): Linear(in_features=2096, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 14948.798075
validation loss after epoch 0 : 1114.383440
	Epoch 1....
Epoch has taken 0:04:02.626343
Number of used sentences in train = 3226
Total loss for epoch 1: 9836.734970
validation loss after epoch 1 : 1061.720853
	Epoch 2....
Epoch has taken 0:04:09.180982
Number of used sentences in train = 3226
Total loss for epoch 2: 8915.540387
validation loss after epoch 2 : 1052.542611
	Epoch 3....
Epoch has taken 0:03:34.924897
Number of used sentences in train = 3226
Total loss for epoch 3: 8362.385380
validation loss after epoch 3 : 1076.081168
	Epoch 4....
Epoch has taken 0:03:33.214705
Number of used sentences in train = 3226
Total loss for epoch 4: 7823.068155
validation loss after epoch 4 : 1093.674005
	Epoch 5....
Epoch has taken 0:03:35.097749
Number of used sentences in train = 3226
Total loss for epoch 5: 7527.638328
validation loss after epoch 5 : 1105.094790
	Epoch 6....
Epoch has taken 0:03:35.616326
Number of used sentences in train = 3226
Total loss for epoch 6: 7258.220681
validation loss after epoch 6 : 1219.694678
	Epoch 7....
Epoch has taken 0:03:35.239617
Number of used sentences in train = 3226
Total loss for epoch 7: 7016.639194
validation loss after epoch 7 : 1240.883202
	Epoch 8....
Epoch has taken 0:03:33.418570
Number of used sentences in train = 3226
Total loss for epoch 8: 6830.616942
validation loss after epoch 8 : 1288.853647
	Epoch 9....
Epoch has taken 0:03:55.350717
Number of used sentences in train = 3226
Total loss for epoch 9: 6685.627712
validation loss after epoch 9 : 1384.067549
	Epoch 10....
Epoch has taken 0:03:36.081411
Number of used sentences in train = 3226
Total loss for epoch 10: 6579.412896
validation loss after epoch 10 : 1344.361009
	Epoch 11....
Epoch has taken 0:03:35.015577
Number of used sentences in train = 3226
Total loss for epoch 11: 6519.738437
validation loss after epoch 11 : 1388.266141
	Epoch 12....
Epoch has taken 0:03:35.232257
Number of used sentences in train = 3226
Total loss for epoch 12: 6408.996463
validation loss after epoch 12 : 1458.699623
	Epoch 13....
Epoch has taken 0:03:35.883880
Number of used sentences in train = 3226
Total loss for epoch 13: 6339.175179
validation loss after epoch 13 : 1498.488372
	Epoch 14....
Epoch has taken 0:03:45.811471
Number of used sentences in train = 3226
Total loss for epoch 14: 6308.069565
validation loss after epoch 14 : 1485.171419
	TransitionClassifier(
  (p_embeddings): Embedding(13, 53)
  (w_embeddings): Embedding(3369, 58)
  (lstm): LSTM(111, 131, bidirectional=True)
  (linear1): Linear(in_features=2096, out_features=34, bias=True)
  (linear2): Linear(in_features=34, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:35.392297
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1570.092769
	Epoch 1....
Epoch has taken 0:00:20.822361
Number of used sentences in train = 359
Total loss for epoch 1: 1052.564600
	Epoch 2....
Epoch has taken 0:00:20.845594
Number of used sentences in train = 359
Total loss for epoch 2: 920.096365
	Epoch 3....
Epoch has taken 0:00:20.840034
Number of used sentences in train = 359
Total loss for epoch 3: 859.935779
	Epoch 4....
Epoch has taken 0:00:20.843046
Number of used sentences in train = 359
Total loss for epoch 4: 813.945607
	Epoch 5....
Epoch has taken 0:00:20.834253
Number of used sentences in train = 359
Total loss for epoch 5: 775.949319
	Epoch 6....
Epoch has taken 0:00:20.851730
Number of used sentences in train = 359
Total loss for epoch 6: 748.331052
	Epoch 7....
Epoch has taken 0:00:20.854000
Number of used sentences in train = 359
Total loss for epoch 7: 725.896303
	Epoch 8....
Epoch has taken 0:00:20.859828
Number of used sentences in train = 359
Total loss for epoch 8: 714.219111
	Epoch 9....
Epoch has taken 0:00:20.842881
Number of used sentences in train = 359
Total loss for epoch 9: 698.222917
	Epoch 10....
Epoch has taken 0:00:20.822224
Number of used sentences in train = 359
Total loss for epoch 10: 689.000966
	Epoch 11....
Epoch has taken 0:00:20.820219
Number of used sentences in train = 359
Total loss for epoch 11: 681.485041
	Epoch 12....
Epoch has taken 0:00:20.850674
Number of used sentences in train = 359
Total loss for epoch 12: 679.946263
	Epoch 13....
Epoch has taken 0:00:20.833420
Number of used sentences in train = 359
Total loss for epoch 13: 676.028382
	Epoch 14....
Epoch has taken 0:00:20.837241
Number of used sentences in train = 359
Total loss for epoch 14: 674.584407
Epoch has taken 0:00:20.844725

==================================================================================================
	Training time : 1:00:31.359087
==================================================================================================
	Identification : 0.085

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 14, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 45, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 97, 'lstmDropout': 0.23, 'denseActivation': 'tanh', 'wordDim': 196, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 45)
  (w_embeddings): Embedding(5918, 196)
  (lstm): LSTM(241, 97, bidirectional=True)
  (linear1): Linear(in_features=1552, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13435.872754
validation loss after epoch 0 : 1226.263371
	Epoch 1....
Epoch has taken 0:02:39.281779
Number of used sentences in train = 2811
Total loss for epoch 1: 8952.068833
validation loss after epoch 1 : 1103.910859
	Epoch 2....
Epoch has taken 0:02:39.305893
Number of used sentences in train = 2811
Total loss for epoch 2: 7286.470247
validation loss after epoch 2 : 1143.329667
	Epoch 3....
Epoch has taken 0:02:40.829776
Number of used sentences in train = 2811
Total loss for epoch 3: 6325.828245
validation loss after epoch 3 : 1192.667036
	Epoch 4....
Epoch has taken 0:03:06.527473
Number of used sentences in train = 2811
Total loss for epoch 4: 5760.614059
validation loss after epoch 4 : 1230.410116
	Epoch 5....
Epoch has taken 0:02:41.077677
Number of used sentences in train = 2811
Total loss for epoch 5: 5463.522433
validation loss after epoch 5 : 1272.736696
	Epoch 6....
Epoch has taken 0:02:38.807862
Number of used sentences in train = 2811
Total loss for epoch 6: 5240.967661
validation loss after epoch 6 : 1314.369189
	Epoch 7....
Epoch has taken 0:02:38.766227
Number of used sentences in train = 2811
Total loss for epoch 7: 5112.417309
validation loss after epoch 7 : 1384.741232
	Epoch 8....
Epoch has taken 0:02:39.892049
Number of used sentences in train = 2811
Total loss for epoch 8: 5036.902266
validation loss after epoch 8 : 1414.361955
	Epoch 9....
Epoch has taken 0:02:39.315661
Number of used sentences in train = 2811
Total loss for epoch 9: 4979.237929
validation loss after epoch 9 : 1453.877663
	Epoch 10....
Epoch has taken 0:02:40.661111
Number of used sentences in train = 2811
Total loss for epoch 10: 4934.066108
validation loss after epoch 10 : 1501.959017
	Epoch 11....
Epoch has taken 0:02:41.138459
Number of used sentences in train = 2811
Total loss for epoch 11: 4894.730850
validation loss after epoch 11 : 1518.503181
	Epoch 12....
Epoch has taken 0:03:06.460503
Number of used sentences in train = 2811
Total loss for epoch 12: 4850.546497
validation loss after epoch 12 : 1546.817667
	Epoch 13....
Epoch has taken 0:02:39.295058
Number of used sentences in train = 2811
Total loss for epoch 13: 4829.576342
validation loss after epoch 13 : 1556.187947
	Epoch 14....
Epoch has taken 0:02:39.325806
Number of used sentences in train = 2811
Total loss for epoch 14: 4780.810042
validation loss after epoch 14 : 1566.466247
Epoch has taken 0:02:41.126381
# Network optimizer = Adagrad, learning rate = 0.07
	TransitionClassifier(
  (p_embeddings): Embedding(18, 45)
  (w_embeddings): Embedding(5918, 196)
  (lstm): LSTM(241, 97, bidirectional=True)
  (linear1): Linear(in_features=1552, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Number of used sentences in train = 313
Total loss for epoch 0: 1633.330982
	Epoch 1....
Epoch has taken 0:00:19.596100
Number of used sentences in train = 313
Total loss for epoch 1: 954.756563
	Epoch 2....
Epoch has taken 0:00:19.594415
Number of used sentences in train = 313
Total loss for epoch 2: 699.153947
	Epoch 3....
Epoch has taken 0:00:19.598906
Number of used sentences in train = 313
Total loss for epoch 3: 612.850038
	Epoch 4....
Epoch has taken 0:00:19.597322
Number of used sentences in train = 313
Total loss for epoch 4: 572.291102
	Epoch 5....
Epoch has taken 0:00:19.590422
Number of used sentences in train = 313
Total loss for epoch 5: 561.328061
	Epoch 6....
Epoch has taken 0:00:19.565212
Number of used sentences in train = 313
Total loss for epoch 6: 552.597813
	Epoch 7....
Epoch has taken 0:00:19.590290
Number of used sentences in train = 313
Total loss for epoch 7: 548.511026
	Epoch 8....
Epoch has taken 0:00:19.605277
Number of used sentences in train = 313
Total loss for epoch 8: 543.946398
	Epoch 9....
Epoch has taken 0:00:19.591577
Number of used sentences in train = 313
Total loss for epoch 9: 540.781397
	Epoch 10....
Epoch has taken 0:00:19.579158
Number of used sentences in train = 313
Total loss for epoch 10: 536.770992
	Epoch 11....
Epoch has taken 0:00:19.582712
Number of used sentences in train = 313
Total loss for epoch 11: 539.723423
	Epoch 12....
Epoch has taken 0:00:19.590710
Number of used sentences in train = 313
Total loss for epoch 12: 539.009156
	Epoch 13....
Epoch has taken 0:00:19.574505
Number of used sentences in train = 313
Total loss for epoch 13: 538.453427
	Epoch 14....
Epoch has taken 0:00:19.612806
Number of used sentences in train = 313
Total loss for epoch 14: 537.905380
Epoch has taken 0:00:19.581443

==================================================================================================
	Training time : 0:45:46.169780
==================================================================================================
	Identification : 0.485

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 45)
  (w_embeddings): Embedding(5602, 196)
  (lstm): LSTM(241, 97, bidirectional=True)
  (linear1): Linear(in_features=1552, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11946.569298
validation loss after epoch 0 : 1080.944433
	Epoch 1....
Epoch has taken 0:01:48.884578
Number of used sentences in train = 2074
Total loss for epoch 1: 7755.255544
validation loss after epoch 1 : 951.417561
	Epoch 2....
Epoch has taken 0:01:48.915408
Number of used sentences in train = 2074
Total loss for epoch 2: 5941.629077
validation loss after epoch 2 : 944.901952
	Epoch 3....
Epoch has taken 0:01:49.457716
Number of used sentences in train = 2074
Total loss for epoch 3: 4986.122308
validation loss after epoch 3 : 976.811276
	Epoch 4....
Epoch has taken 0:02:07.013811
Number of used sentences in train = 2074
Total loss for epoch 4: 4393.411803
validation loss after epoch 4 : 1066.103181
	Epoch 5....
Epoch has taken 0:01:49.514574
Number of used sentences in train = 2074
Total loss for epoch 5: 4027.706599
validation loss after epoch 5 : 1077.668445
	Epoch 6....
Epoch has taken 0:01:48.954825
Number of used sentences in train = 2074
Total loss for epoch 6: 3826.600928
validation loss after epoch 6 : 1120.369905
	Epoch 7....
Epoch has taken 0:01:49.701831
Number of used sentences in train = 2074
Total loss for epoch 7: 3705.836958
validation loss after epoch 7 : 1168.636914
	Epoch 8....
Epoch has taken 0:01:49.380248
Number of used sentences in train = 2074
Total loss for epoch 8: 3600.836933
validation loss after epoch 8 : 1185.803407
	Epoch 9....
Epoch has taken 0:02:07.027158
Number of used sentences in train = 2074
Total loss for epoch 9: 3535.197585
validation loss after epoch 9 : 1229.557076
	Epoch 10....
Epoch has taken 0:01:49.554841
Number of used sentences in train = 2074
Total loss for epoch 10: 3481.236792
validation loss after epoch 10 : 1278.512688
	Epoch 11....
Epoch has taken 0:01:48.479787
Number of used sentences in train = 2074
Total loss for epoch 11: 3423.612582
validation loss after epoch 11 : 1286.195714
	Epoch 12....
Epoch has taken 0:01:48.682973
Number of used sentences in train = 2074
Total loss for epoch 12: 3383.255235
validation loss after epoch 12 : 1331.326335
	Epoch 13....
Epoch has taken 0:01:49.522963
Number of used sentences in train = 2074
Total loss for epoch 13: 3362.700805
validation loss after epoch 13 : 1337.429825
	Epoch 14....
Epoch has taken 0:01:49.370813
Number of used sentences in train = 2074
Total loss for epoch 14: 3345.792765
validation loss after epoch 14 : 1368.889174
	TransitionClassifier(
  (p_embeddings): Embedding(18, 45)
  (w_embeddings): Embedding(5602, 196)
  (lstm): LSTM(241, 97, bidirectional=True)
  (linear1): Linear(in_features=1552, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:49.715012
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1515.945501
	Epoch 1....
Epoch has taken 0:00:11.134467
Number of used sentences in train = 231
Total loss for epoch 1: 717.989152
	Epoch 2....
Epoch has taken 0:00:11.147831
Number of used sentences in train = 231
Total loss for epoch 2: 520.297598
	Epoch 3....
Epoch has taken 0:00:11.140075
Number of used sentences in train = 231
Total loss for epoch 3: 455.954286
	Epoch 4....
Epoch has taken 0:00:11.151924
Number of used sentences in train = 231
Total loss for epoch 4: 410.732728
	Epoch 5....
Epoch has taken 0:00:11.126834
Number of used sentences in train = 231
Total loss for epoch 5: 391.927301
	Epoch 6....
Epoch has taken 0:00:11.173210
Number of used sentences in train = 231
Total loss for epoch 6: 381.959552
	Epoch 7....
Epoch has taken 0:00:11.113362
Number of used sentences in train = 231
Total loss for epoch 7: 377.214954
	Epoch 8....
Epoch has taken 0:00:11.150610
Number of used sentences in train = 231
Total loss for epoch 8: 375.063791
	Epoch 9....
Epoch has taken 0:00:11.033937
Number of used sentences in train = 231
Total loss for epoch 9: 373.668997
	Epoch 10....
Epoch has taken 0:00:11.047822
Number of used sentences in train = 231
Total loss for epoch 10: 372.435836
	Epoch 11....
Epoch has taken 0:00:11.011167
Number of used sentences in train = 231
Total loss for epoch 11: 368.834451
	Epoch 12....
Epoch has taken 0:00:11.050805
Number of used sentences in train = 231
Total loss for epoch 12: 367.435750
	Epoch 13....
Epoch has taken 0:00:11.024665
Number of used sentences in train = 231
Total loss for epoch 13: 364.786869
	Epoch 14....
Epoch has taken 0:00:11.021803
Number of used sentences in train = 231
Total loss for epoch 14: 364.965945
Epoch has taken 0:00:11.035159

==================================================================================================
	Training time : 0:30:40.885894
==================================================================================================
	Identification : 0.414

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 45)
  (w_embeddings): Embedding(6922, 196)
  (lstm): LSTM(241, 97, bidirectional=True)
  (linear1): Linear(in_features=1552, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16899.092559
validation loss after epoch 0 : 1599.884112
	Epoch 1....
Epoch has taken 0:03:32.929367
Number of used sentences in train = 3226
Total loss for epoch 1: 12568.553734
validation loss after epoch 1 : 1537.598878
	Epoch 2....
Epoch has taken 0:03:34.766392
Number of used sentences in train = 3226
Total loss for epoch 2: 10689.700238
validation loss after epoch 2 : 1620.192481
	Epoch 3....
Epoch has taken 0:04:07.061495
Number of used sentences in train = 3226
Total loss for epoch 3: 9412.041731
validation loss after epoch 3 : 1611.935030
	Epoch 4....
Epoch has taken 0:03:31.644557
Number of used sentences in train = 3226
Total loss for epoch 4: 8501.289434
validation loss after epoch 4 : 1673.086846
	Epoch 5....
Epoch has taken 0:03:32.749612
Number of used sentences in train = 3226
Total loss for epoch 5: 7776.727095
validation loss after epoch 5 : 1823.108480
	Epoch 6....
Epoch has taken 0:03:33.509261
Number of used sentences in train = 3226
Total loss for epoch 6: 7287.847000
validation loss after epoch 6 : 1864.886566
	Epoch 7....
Epoch has taken 0:04:06.921950
Number of used sentences in train = 3226
Total loss for epoch 7: 6991.069974
validation loss after epoch 7 : 1925.274785
	Epoch 8....
Epoch has taken 0:03:33.199837
Number of used sentences in train = 3226
Total loss for epoch 8: 6760.756270
validation loss after epoch 8 : 2010.769262
	Epoch 9....
Epoch has taken 0:03:31.267603
Number of used sentences in train = 3226
Total loss for epoch 9: 6577.350617
validation loss after epoch 9 : 2136.462849
	Epoch 10....
Epoch has taken 0:03:32.834830
Number of used sentences in train = 3226
Total loss for epoch 10: 6454.851853
validation loss after epoch 10 : 2231.128448
	Epoch 11....
Epoch has taken 0:03:32.936125
Number of used sentences in train = 3226
Total loss for epoch 11: 6403.122335
validation loss after epoch 11 : 2262.685708
	Epoch 12....
Epoch has taken 0:03:33.427687
Number of used sentences in train = 3226
Total loss for epoch 12: 6348.379038
validation loss after epoch 12 : 2360.365407
	Epoch 13....
Epoch has taken 0:03:33.427215
Number of used sentences in train = 3226
Total loss for epoch 13: 6316.076878
validation loss after epoch 13 : 2376.465083
	Epoch 14....
Epoch has taken 0:03:33.122465
Number of used sentences in train = 3226
Total loss for epoch 14: 6297.826151
validation loss after epoch 14 : 2395.813601
	TransitionClassifier(
  (p_embeddings): Embedding(13, 45)
  (w_embeddings): Embedding(6922, 196)
  (lstm): LSTM(241, 97, bidirectional=True)
  (linear1): Linear(in_features=1552, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:31.443455
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2049.291472
	Epoch 1....
Epoch has taken 0:00:20.807719
Number of used sentences in train = 359
Total loss for epoch 1: 1326.005943
	Epoch 2....
Epoch has taken 0:00:20.806361
Number of used sentences in train = 359
Total loss for epoch 2: 1082.409230
	Epoch 3....
Epoch has taken 0:00:20.818791
Number of used sentences in train = 359
Total loss for epoch 3: 928.675617
	Epoch 4....
Epoch has taken 0:00:20.803515
Number of used sentences in train = 359
Total loss for epoch 4: 835.288566
	Epoch 5....
Epoch has taken 0:00:20.759659
Number of used sentences in train = 359
Total loss for epoch 5: 786.814062
	Epoch 6....
Epoch has taken 0:00:20.783019
Number of used sentences in train = 359
Total loss for epoch 6: 733.500620
	Epoch 7....
Epoch has taken 0:00:20.816884
Number of used sentences in train = 359
Total loss for epoch 7: 713.791353
	Epoch 8....
Epoch has taken 0:00:20.784919
Number of used sentences in train = 359
Total loss for epoch 8: 702.645510
	Epoch 9....
Epoch has taken 0:00:20.826835
Number of used sentences in train = 359
Total loss for epoch 9: 697.314811
	Epoch 10....
Epoch has taken 0:00:20.801066
Number of used sentences in train = 359
Total loss for epoch 10: 692.409863
	Epoch 11....
Epoch has taken 0:00:20.800415
Number of used sentences in train = 359
Total loss for epoch 11: 690.300894
	Epoch 12....
Epoch has taken 0:00:20.794218
Number of used sentences in train = 359
Total loss for epoch 12: 689.150001
	Epoch 13....
Epoch has taken 0:00:20.814863
Number of used sentences in train = 359
Total loss for epoch 13: 688.461588
	Epoch 14....
Epoch has taken 0:00:20.817444
Number of used sentences in train = 359
Total loss for epoch 14: 687.834232
Epoch has taken 0:00:20.806432

==================================================================================================
	Training time : 0:59:33.938348
==================================================================================================
	Identification : 0.498

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 10, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 64, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 90, 'lstmDropout': 0.35, 'denseActivation': 'tanh', 'wordDim': 224, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(1177, 224)
  (lstm): LSTM(288, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13771.721542
validation loss after epoch 0 : 1129.731643
	Epoch 1....
Epoch has taken 0:02:37.955745
Number of used sentences in train = 2811
Total loss for epoch 1: 9732.890340
validation loss after epoch 1 : 1118.354173
	Epoch 2....
Epoch has taken 0:02:38.127770
Number of used sentences in train = 2811
Total loss for epoch 2: 8637.168782
validation loss after epoch 2 : 1019.425334
	Epoch 3....
Epoch has taken 0:02:39.607606
Number of used sentences in train = 2811
Total loss for epoch 3: 7797.013957
validation loss after epoch 3 : 1016.999820
	Epoch 4....
Epoch has taken 0:02:40.001335
Number of used sentences in train = 2811
Total loss for epoch 4: 7264.778550
validation loss after epoch 4 : 1072.744936
	Epoch 5....
Epoch has taken 0:03:05.303952
Number of used sentences in train = 2811
Total loss for epoch 5: 6862.595955
validation loss after epoch 5 : 1068.138087
	Epoch 6....
Epoch has taken 0:02:40.005738
Number of used sentences in train = 2811
Total loss for epoch 6: 6556.208293
validation loss after epoch 6 : 1019.258643
	Epoch 7....
Epoch has taken 0:02:38.625465
Number of used sentences in train = 2811
Total loss for epoch 7: 6276.803417
validation loss after epoch 7 : 1113.304315
	Epoch 8....
Epoch has taken 0:02:39.853695
Number of used sentences in train = 2811
Total loss for epoch 8: 6080.111035
validation loss after epoch 8 : 1097.021527
	Epoch 9....
Epoch has taken 0:02:40.265138
Number of used sentences in train = 2811
Total loss for epoch 9: 5883.839726
validation loss after epoch 9 : 1138.112054
	Epoch 10....
Epoch has taken 0:03:05.522408
Number of used sentences in train = 2811
Total loss for epoch 10: 5765.770250
validation loss after epoch 10 : 1083.665439
	Epoch 11....
Epoch has taken 0:02:40.064142
Number of used sentences in train = 2811
Total loss for epoch 11: 5655.371815
validation loss after epoch 11 : 1137.838825
	Epoch 12....
Epoch has taken 0:02:38.752187
Number of used sentences in train = 2811
Total loss for epoch 12: 5546.860242
validation loss after epoch 12 : 1150.243961
	Epoch 13....
Epoch has taken 0:02:39.921101
Number of used sentences in train = 2811
Total loss for epoch 13: 5460.102658
validation loss after epoch 13 : 1116.413592
	Epoch 14....
Epoch has taken 0:02:39.754842
Number of used sentences in train = 2811
Total loss for epoch 14: 5385.152756
validation loss after epoch 14 : 1134.569348
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(1177, 224)
  (lstm): LSTM(288, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:40.408762
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1559.070126
	Epoch 1....
Epoch has taken 0:00:16.737345
Number of used sentences in train = 313
Total loss for epoch 1: 1001.793883
	Epoch 2....
Epoch has taken 0:00:16.752514
Number of used sentences in train = 313
Total loss for epoch 2: 895.690414
	Epoch 3....
Epoch has taken 0:00:16.750479
Number of used sentences in train = 313
Total loss for epoch 3: 785.130814
	Epoch 4....
Epoch has taken 0:00:16.761380
Number of used sentences in train = 313
Total loss for epoch 4: 736.655492
	Epoch 5....
Epoch has taken 0:00:16.736081
Number of used sentences in train = 313
Total loss for epoch 5: 683.376371
	Epoch 6....
Epoch has taken 0:00:16.728604
Number of used sentences in train = 313
Total loss for epoch 6: 645.993338
	Epoch 7....
Epoch has taken 0:00:16.765884
Number of used sentences in train = 313
Total loss for epoch 7: 629.725246
	Epoch 8....
Epoch has taken 0:00:16.754676
Number of used sentences in train = 313
Total loss for epoch 8: 612.343409
	Epoch 9....
Epoch has taken 0:00:16.759083
Number of used sentences in train = 313
Total loss for epoch 9: 595.167078
	Epoch 10....
Epoch has taken 0:00:16.765311
Number of used sentences in train = 313
Total loss for epoch 10: 589.032561
	Epoch 11....
Epoch has taken 0:00:16.787462
Number of used sentences in train = 313
Total loss for epoch 11: 580.110634
	Epoch 12....
Epoch has taken 0:00:16.780722
Number of used sentences in train = 313
Total loss for epoch 12: 565.633880
	Epoch 13....
Epoch has taken 0:00:16.753150
Number of used sentences in train = 313
Total loss for epoch 13: 560.016512
	Epoch 14....
Epoch has taken 0:00:16.770852
Number of used sentences in train = 313
Total loss for epoch 14: 556.561438
Epoch has taken 0:00:16.780753

==================================================================================================
	Training time : 0:44:56.044749
==================================================================================================
	Identification : 0.527

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(1133, 224)
  (lstm): LSTM(288, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 9514.944224
validation loss after epoch 0 : 797.904541
	Epoch 1....
Epoch has taken 0:01:50.006906
Number of used sentences in train = 2074
Total loss for epoch 1: 6246.972482
validation loss after epoch 1 : 776.834169
	Epoch 2....
Epoch has taken 0:01:50.010120
Number of used sentences in train = 2074
Total loss for epoch 2: 5342.986541
validation loss after epoch 2 : 743.974448
	Epoch 3....
Epoch has taken 0:01:51.422004
Number of used sentences in train = 2074
Total loss for epoch 3: 4772.852185
validation loss after epoch 3 : 759.184491
	Epoch 4....
Epoch has taken 0:01:49.836972
Number of used sentences in train = 2074
Total loss for epoch 4: 4376.865798
validation loss after epoch 4 : 754.412484
	Epoch 5....
Epoch has taken 0:01:48.627821
Number of used sentences in train = 2074
Total loss for epoch 5: 4112.382246
validation loss after epoch 5 : 790.793023
	Epoch 6....
Epoch has taken 0:02:01.026901
Number of used sentences in train = 2074
Total loss for epoch 6: 3947.668685
validation loss after epoch 6 : 816.594496
	Epoch 7....
Epoch has taken 0:01:49.421999
Number of used sentences in train = 2074
Total loss for epoch 7: 3791.637334
validation loss after epoch 7 : 801.103624
	Epoch 8....
Epoch has taken 0:02:06.973228
Number of used sentences in train = 2074
Total loss for epoch 8: 3683.165002
validation loss after epoch 8 : 863.872322
	Epoch 9....
Epoch has taken 0:01:49.968921
Number of used sentences in train = 2074
Total loss for epoch 9: 3618.752867
validation loss after epoch 9 : 843.217738
	Epoch 10....
Epoch has taken 0:01:48.871881
Number of used sentences in train = 2074
Total loss for epoch 10: 3563.039775
validation loss after epoch 10 : 872.162282
	Epoch 11....
Epoch has taken 0:01:49.371107
Number of used sentences in train = 2074
Total loss for epoch 11: 3528.385545
validation loss after epoch 11 : 886.967493
	Epoch 12....
Epoch has taken 0:01:57.617624
Number of used sentences in train = 2074
Total loss for epoch 12: 3493.373708
validation loss after epoch 12 : 894.035162
	Epoch 13....
Epoch has taken 0:02:04.338928
Number of used sentences in train = 2074
Total loss for epoch 13: 3468.534101
validation loss after epoch 13 : 914.837017
	Epoch 14....
Epoch has taken 0:01:55.293203
Number of used sentences in train = 2074
Total loss for epoch 14: 3444.299611
validation loss after epoch 14 : 912.046165
	TransitionClassifier(
  (p_embeddings): Embedding(18, 64)
  (w_embeddings): Embedding(1133, 224)
  (lstm): LSTM(288, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:51.722148
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1353.571797
	Epoch 1....
Epoch has taken 0:00:12.142145
Number of used sentences in train = 231
Total loss for epoch 1: 714.705006
	Epoch 2....
Epoch has taken 0:00:12.342711
Number of used sentences in train = 231
Total loss for epoch 2: 563.885158
	Epoch 3....
Epoch has taken 0:00:12.500550
Number of used sentences in train = 231
Total loss for epoch 3: 478.898516
	Epoch 4....
Epoch has taken 0:00:12.018682
Number of used sentences in train = 231
Total loss for epoch 4: 449.088353
	Epoch 5....
Epoch has taken 0:00:13.520475
Number of used sentences in train = 231
Total loss for epoch 5: 419.916459
	Epoch 6....
Epoch has taken 0:00:13.469031
Number of used sentences in train = 231
Total loss for epoch 6: 404.745889
	Epoch 7....
Epoch has taken 0:00:13.668930
Number of used sentences in train = 231
Total loss for epoch 7: 395.884266
	Epoch 8....
Epoch has taken 0:00:12.793820
Number of used sentences in train = 231
Total loss for epoch 8: 391.294508
	Epoch 9....
Epoch has taken 0:00:13.547224
Number of used sentences in train = 231
Total loss for epoch 9: 384.170063
	Epoch 10....
Epoch has taken 0:00:13.520957
Number of used sentences in train = 231
Total loss for epoch 10: 376.207569
	Epoch 11....
Epoch has taken 0:00:13.476548
Number of used sentences in train = 231
Total loss for epoch 11: 371.437146
	Epoch 12....
Epoch has taken 0:00:13.540979
Number of used sentences in train = 231
Total loss for epoch 12: 368.626088
	Epoch 13....
Epoch has taken 0:00:13.350745
Number of used sentences in train = 231
Total loss for epoch 13: 367.433299
	Epoch 14....
Epoch has taken 0:00:12.834234
Number of used sentences in train = 231
Total loss for epoch 14: 365.883664
Epoch has taken 0:00:13.574656

==================================================================================================
	Training time : 0:31:41.147398
==================================================================================================
	Identification : 0.122

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 64)
  (w_embeddings): Embedding(1202, 224)
  (lstm): LSTM(288, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16341.771623
validation loss after epoch 0 : 1506.784789
	Epoch 1....
Epoch has taken 0:04:11.163834
Number of used sentences in train = 3226
Total loss for epoch 1: 12688.961342
validation loss after epoch 1 : 1399.432562
	Epoch 2....
Epoch has taken 0:04:00.109352
Number of used sentences in train = 3226
Total loss for epoch 2: 11340.108492
validation loss after epoch 2 : 1347.631056
	Epoch 3....
Epoch has taken 0:04:07.428457
Number of used sentences in train = 3226
Total loss for epoch 3: 10364.174302
validation loss after epoch 3 : 1369.085416
	Epoch 4....
Epoch has taken 0:03:34.202285
Number of used sentences in train = 3226
Total loss for epoch 4: 9595.303230
validation loss after epoch 4 : 1443.021781
	Epoch 5....
Epoch has taken 0:03:31.698092
Number of used sentences in train = 3226
Total loss for epoch 5: 9062.933774
validation loss after epoch 5 : 1444.345964
	Epoch 6....
Epoch has taken 0:03:45.033887
Number of used sentences in train = 3226
Total loss for epoch 6: 8585.323662
validation loss after epoch 6 : 1478.070832
	Epoch 7....
Epoch has taken 0:03:38.189433
Number of used sentences in train = 3226
Total loss for epoch 7: 8275.605347
validation loss after epoch 7 : 1526.734425
	Epoch 8....
Epoch has taken 0:03:46.404404
Number of used sentences in train = 3226
Total loss for epoch 8: 7972.763119
validation loss after epoch 8 : 1581.124586
	Epoch 9....
Epoch has taken 0:03:40.874423
Number of used sentences in train = 3226
Total loss for epoch 9: 7746.015794
validation loss after epoch 9 : 1592.530496
	Epoch 10....
Epoch has taken 0:03:52.112550
Number of used sentences in train = 3226
Total loss for epoch 10: 7536.852348
validation loss after epoch 10 : 1664.308828
	Epoch 11....
Epoch has taken 0:03:44.639512
Number of used sentences in train = 3226
Total loss for epoch 11: 7390.503042
validation loss after epoch 11 : 1727.418581
	Epoch 12....
Epoch has taken 0:03:33.245160
Number of used sentences in train = 3226
Total loss for epoch 12: 7214.861570
validation loss after epoch 12 : 1744.573109
	Epoch 13....
Epoch has taken 0:03:42.221807
Number of used sentences in train = 3226
Total loss for epoch 13: 7098.246047
validation loss after epoch 13 : 1780.828977
	Epoch 14....
Epoch has taken 0:03:49.961996
Number of used sentences in train = 3226
Total loss for epoch 14: 6973.846943
validation loss after epoch 14 : 1781.724993
	TransitionClassifier(
  (p_embeddings): Embedding(13, 64)
  (w_embeddings): Embedding(1202, 224)
  (lstm): LSTM(288, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=10, bias=True)
  (linear2): Linear(in_features=10, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:52.461623
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2093.375949
	Epoch 1....
Epoch has taken 0:00:22.891506
Number of used sentences in train = 359
Total loss for epoch 1: 1305.649260
	Epoch 2....
Epoch has taken 0:00:22.969997
Number of used sentences in train = 359
Total loss for epoch 2: 1149.044670
	Epoch 3....
Epoch has taken 0:00:22.913058
Number of used sentences in train = 359
Total loss for epoch 3: 1004.180942
	Epoch 4....
Epoch has taken 0:00:22.974648
Number of used sentences in train = 359
Total loss for epoch 4: 916.350332
	Epoch 5....
Epoch has taken 0:00:22.944156
Number of used sentences in train = 359
Total loss for epoch 5: 848.779427
	Epoch 6....
Epoch has taken 0:00:22.939030
Number of used sentences in train = 359
Total loss for epoch 6: 818.311828
	Epoch 7....
Epoch has taken 0:00:22.912666
Number of used sentences in train = 359
Total loss for epoch 7: 775.875072
	Epoch 8....
Epoch has taken 0:00:22.853888
Number of used sentences in train = 359
Total loss for epoch 8: 751.105251
	Epoch 9....
Epoch has taken 0:00:22.740471
Number of used sentences in train = 359
Total loss for epoch 9: 745.767077
	Epoch 10....
Epoch has taken 0:00:22.858811
Number of used sentences in train = 359
Total loss for epoch 10: 730.211556
	Epoch 11....
Epoch has taken 0:00:22.749428
Number of used sentences in train = 359
Total loss for epoch 11: 712.778424
	Epoch 12....
Epoch has taken 0:00:22.835093
Number of used sentences in train = 359
Total loss for epoch 12: 704.927872
	Epoch 13....
Epoch has taken 0:00:22.950680
Number of used sentences in train = 359
Total loss for epoch 13: 702.760755
	Epoch 14....
Epoch has taken 0:00:23.025431
Number of used sentences in train = 359
Total loss for epoch 14: 700.756289
Epoch has taken 0:00:22.849015

==================================================================================================
	Training time : 1:02:33.813221
==================================================================================================
	Identification : 0.487

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 36, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 18, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 93, 'lstmDropout': 0.24, 'denseActivation': 'tanh', 'wordDim': 61, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5935, 61)
  (lstm): LSTM(79, 93, bidirectional=True)
  (linear1): Linear(in_features=1488, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13342.966577
validation loss after epoch 0 : 1142.531438
	Epoch 1....
Epoch has taken 0:02:56.255928
Number of used sentences in train = 2811
Total loss for epoch 1: 8881.860861
validation loss after epoch 1 : 1089.361718
	Epoch 2....
Epoch has taken 0:02:56.008167
Number of used sentences in train = 2811
Total loss for epoch 2: 7226.927086
validation loss after epoch 2 : 1082.779939
	Epoch 3....
Epoch has taken 0:02:57.092633
Number of used sentences in train = 2811
Total loss for epoch 3: 6155.745870
validation loss after epoch 3 : 1121.441019
	Epoch 4....
Epoch has taken 0:02:56.089269
Number of used sentences in train = 2811
Total loss for epoch 4: 5449.618538
validation loss after epoch 4 : 1233.971558
	Epoch 5....
Epoch has taken 0:02:56.223815
Number of used sentences in train = 2811
Total loss for epoch 5: 5094.408986
validation loss after epoch 5 : 1246.413283
	Epoch 6....
Epoch has taken 0:02:54.839525
Number of used sentences in train = 2811
Total loss for epoch 6: 4884.698312
validation loss after epoch 6 : 1384.618231
	Epoch 7....
Epoch has taken 0:03:02.352127
Number of used sentences in train = 2811
Total loss for epoch 7: 4775.264472
validation loss after epoch 7 : 1353.134599
	Epoch 8....
Epoch has taken 0:02:52.435739
Number of used sentences in train = 2811
Total loss for epoch 8: 4693.123722
validation loss after epoch 8 : 1382.131167
	Epoch 9....
Epoch has taken 0:02:53.606764
Number of used sentences in train = 2811
Total loss for epoch 9: 4642.861544
validation loss after epoch 9 : 1427.328686
	Epoch 10....
Epoch has taken 0:02:41.292270
Number of used sentences in train = 2811
Total loss for epoch 10: 4613.414357
validation loss after epoch 10 : 1451.463215
	Epoch 11....
Epoch has taken 0:02:40.744546
Number of used sentences in train = 2811
Total loss for epoch 11: 4591.687716
validation loss after epoch 11 : 1462.236476
	Epoch 12....
Epoch has taken 0:02:53.867612
Number of used sentences in train = 2811
Total loss for epoch 12: 4573.603532
validation loss after epoch 12 : 1490.383043
	Epoch 13....
Epoch has taken 0:03:07.285659
Number of used sentences in train = 2811
Total loss for epoch 13: 4557.946795
validation loss after epoch 13 : 1513.081325
	Epoch 14....
Epoch has taken 0:02:50.682468
Number of used sentences in train = 2811
Total loss for epoch 14: 4548.540995
validation loss after epoch 14 : 1517.428470
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5935, 61)
  (lstm): LSTM(79, 93, bidirectional=True)
  (linear1): Linear(in_features=1488, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:48.135551
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1482.999923
	Epoch 1....
Epoch has taken 0:00:16.906541
Number of used sentences in train = 313
Total loss for epoch 1: 908.923429
	Epoch 2....
Epoch has taken 0:00:16.920863
Number of used sentences in train = 313
Total loss for epoch 2: 728.950583
	Epoch 3....
Epoch has taken 0:00:16.885938
Number of used sentences in train = 313
Total loss for epoch 3: 605.338802
	Epoch 4....
Epoch has taken 0:00:16.899455
Number of used sentences in train = 313
Total loss for epoch 4: 568.136127
	Epoch 5....
Epoch has taken 0:00:16.920018
Number of used sentences in train = 313
Total loss for epoch 5: 545.615948
	Epoch 6....
Epoch has taken 0:00:16.911138
Number of used sentences in train = 313
Total loss for epoch 6: 538.527927
	Epoch 7....
Epoch has taken 0:00:16.907136
Number of used sentences in train = 313
Total loss for epoch 7: 535.049033
	Epoch 8....
Epoch has taken 0:00:16.912531
Number of used sentences in train = 313
Total loss for epoch 8: 532.996984
	Epoch 9....
Epoch has taken 0:00:16.920082
Number of used sentences in train = 313
Total loss for epoch 9: 530.132671
	Epoch 10....
Epoch has taken 0:00:16.907922
Number of used sentences in train = 313
Total loss for epoch 10: 528.908948
	Epoch 11....
Epoch has taken 0:00:16.919747
Number of used sentences in train = 313
Total loss for epoch 11: 527.760732
	Epoch 12....
Epoch has taken 0:00:16.916849
Number of used sentences in train = 313
Total loss for epoch 12: 526.718106
	Epoch 13....
Epoch has taken 0:00:16.941448
Number of used sentences in train = 313
Total loss for epoch 13: 526.655581
	Epoch 14....
Epoch has taken 0:00:16.903891
Number of used sentences in train = 313
Total loss for epoch 14: 525.751429
Epoch has taken 0:00:16.909213

==================================================================================================
	Training time : 0:47:41.111990
==================================================================================================
	Identification : 0.52

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5605, 61)
  (lstm): LSTM(79, 93, bidirectional=True)
  (linear1): Linear(in_features=1488, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10310.259135
validation loss after epoch 0 : 887.479226
	Epoch 1....
Epoch has taken 0:02:07.528776
Number of used sentences in train = 2074
Total loss for epoch 1: 6424.968777
validation loss after epoch 1 : 822.679328
	Epoch 2....
Epoch has taken 0:02:07.456229
Number of used sentences in train = 2074
Total loss for epoch 2: 5057.614870
validation loss after epoch 2 : 844.545223
	Epoch 3....
Epoch has taken 0:01:49.987457
Number of used sentences in train = 2074
Total loss for epoch 3: 4197.383438
validation loss after epoch 3 : 939.401520
	Epoch 4....
Epoch has taken 0:01:51.805227
Number of used sentences in train = 2074
Total loss for epoch 4: 3762.371314
validation loss after epoch 4 : 969.534743
	Epoch 5....
Epoch has taken 0:02:01.230827
Number of used sentences in train = 2074
Total loss for epoch 5: 3505.462271
validation loss after epoch 5 : 1017.639920
	Epoch 6....
Epoch has taken 0:02:02.113146
Number of used sentences in train = 2074
Total loss for epoch 6: 3388.185318
validation loss after epoch 6 : 1072.094200
	Epoch 7....
Epoch has taken 0:02:02.283531
Number of used sentences in train = 2074
Total loss for epoch 7: 3342.133007
validation loss after epoch 7 : 1066.508575
	Epoch 8....
Epoch has taken 0:01:50.453496
Number of used sentences in train = 2074
Total loss for epoch 8: 3294.073975
validation loss after epoch 8 : 1090.226648
	Epoch 9....
Epoch has taken 0:01:49.064118
Number of used sentences in train = 2074
Total loss for epoch 9: 3271.333782
validation loss after epoch 9 : 1089.536562
	Epoch 10....
Epoch has taken 0:01:50.042761
Number of used sentences in train = 2074
Total loss for epoch 10: 3256.748363
validation loss after epoch 10 : 1102.208792
	Epoch 11....
Epoch has taken 0:01:49.989708
Number of used sentences in train = 2074
Total loss for epoch 11: 3246.197406
validation loss after epoch 11 : 1119.581595
	Epoch 12....
Epoch has taken 0:01:50.305616
Number of used sentences in train = 2074
Total loss for epoch 12: 3240.295128
validation loss after epoch 12 : 1125.903049
	Epoch 13....
Epoch has taken 0:01:49.179395
Number of used sentences in train = 2074
Total loss for epoch 13: 3236.797554
validation loss after epoch 13 : 1133.622606
	Epoch 14....
Epoch has taken 0:01:50.034991
Number of used sentences in train = 2074
Total loss for epoch 14: 3232.615473
validation loss after epoch 14 : 1143.601177
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(5605, 61)
  (lstm): LSTM(79, 93, bidirectional=True)
  (linear1): Linear(in_features=1488, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:50.279210
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1590.786130
	Epoch 1....
Epoch has taken 0:00:11.196985
Number of used sentences in train = 231
Total loss for epoch 1: 710.562694
	Epoch 2....
Epoch has taken 0:00:11.188315
Number of used sentences in train = 231
Total loss for epoch 2: 518.324579
	Epoch 3....
Epoch has taken 0:00:11.197117
Number of used sentences in train = 231
Total loss for epoch 3: 450.839012
	Epoch 4....
Epoch has taken 0:00:11.212119
Number of used sentences in train = 231
Total loss for epoch 4: 410.355091
	Epoch 5....
Epoch has taken 0:00:11.187499
Number of used sentences in train = 231
Total loss for epoch 5: 385.733346
	Epoch 6....
Epoch has taken 0:00:11.213785
Number of used sentences in train = 231
Total loss for epoch 6: 368.397580
	Epoch 7....
Epoch has taken 0:00:11.189560
Number of used sentences in train = 231
Total loss for epoch 7: 358.209854
	Epoch 8....
Epoch has taken 0:00:11.194154
Number of used sentences in train = 231
Total loss for epoch 8: 351.455137
	Epoch 9....
Epoch has taken 0:00:11.205692
Number of used sentences in train = 231
Total loss for epoch 9: 348.885346
	Epoch 10....
Epoch has taken 0:00:11.179869
Number of used sentences in train = 231
Total loss for epoch 10: 348.353401
	Epoch 11....
Epoch has taken 0:00:12.926876
Number of used sentences in train = 231
Total loss for epoch 11: 347.700261
	Epoch 12....
Epoch has taken 0:00:12.938338
Number of used sentences in train = 231
Total loss for epoch 12: 347.220092
	Epoch 13....
Epoch has taken 0:00:12.935172
Number of used sentences in train = 231
Total loss for epoch 13: 346.850094
	Epoch 14....
Epoch has taken 0:00:12.928312
Number of used sentences in train = 231
Total loss for epoch 14: 346.715980
Epoch has taken 0:00:12.918740

==================================================================================================
	Training time : 0:31:38.703396
==================================================================================================
	Identification : 0.191

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(6893, 61)
  (lstm): LSTM(79, 93, bidirectional=True)
  (linear1): Linear(in_features=1488, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 17719.755703
validation loss after epoch 0 : 1548.931561
	Epoch 1....
Epoch has taken 0:03:35.200835
Number of used sentences in train = 3226
Total loss for epoch 1: 12504.189293
validation loss after epoch 1 : 1454.306001
	Epoch 2....
Epoch has taken 0:03:35.880852
Number of used sentences in train = 3226
Total loss for epoch 2: 10682.458875
validation loss after epoch 2 : 1441.697342
	Epoch 3....
Epoch has taken 0:03:35.940446
Number of used sentences in train = 3226
Total loss for epoch 3: 9411.503782
validation loss after epoch 3 : 1496.041585
	Epoch 4....
Epoch has taken 0:03:36.423417
Number of used sentences in train = 3226
Total loss for epoch 4: 8382.832913
validation loss after epoch 4 : 1590.996154
	Epoch 5....
Epoch has taken 0:03:35.886319
Number of used sentences in train = 3226
Total loss for epoch 5: 7650.406762
validation loss after epoch 5 : 1721.984183
	Epoch 6....
Epoch has taken 0:03:33.903837
Number of used sentences in train = 3226
Total loss for epoch 6: 7103.692769
validation loss after epoch 6 : 1890.269039
	Epoch 7....
Epoch has taken 0:03:34.953841
Number of used sentences in train = 3226
Total loss for epoch 7: 6703.894988
validation loss after epoch 7 : 1919.012193
	Epoch 8....
Epoch has taken 0:03:59.720678
Number of used sentences in train = 3226
Total loss for epoch 8: 6507.310903
validation loss after epoch 8 : 2072.150197
	Epoch 9....
Epoch has taken 0:04:01.271418
Number of used sentences in train = 3226
Total loss for epoch 9: 6338.498266
validation loss after epoch 9 : 2226.927239
	Epoch 10....
Epoch has taken 0:03:58.703373
Number of used sentences in train = 3226
Total loss for epoch 10: 6282.629591
validation loss after epoch 10 : 2324.873518
	Epoch 11....
Epoch has taken 0:03:57.112662
Number of used sentences in train = 3226
Total loss for epoch 11: 6238.828456
validation loss after epoch 11 : 2293.750597
	Epoch 12....
Epoch has taken 0:03:53.483858
Number of used sentences in train = 3226
Total loss for epoch 12: 6194.418551
validation loss after epoch 12 : 2389.198868
	Epoch 13....
Epoch has taken 0:04:03.878863
Number of used sentences in train = 3226
Total loss for epoch 13: 6175.391752
validation loss after epoch 13 : 2383.263361
	Epoch 14....
Epoch has taken 0:03:47.348280
Number of used sentences in train = 3226
Total loss for epoch 14: 6158.500360
validation loss after epoch 14 : 2442.852466
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(6893, 61)
  (lstm): LSTM(79, 93, bidirectional=True)
  (linear1): Linear(in_features=1488, out_features=36, bias=True)
  (linear2): Linear(in_features=36, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:58.854283
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2101.952314
	Epoch 1....
Epoch has taken 0:00:23.498751
Number of used sentences in train = 359
Total loss for epoch 1: 1215.591814
	Epoch 2....
Epoch has taken 0:00:23.478992
Number of used sentences in train = 359
Total loss for epoch 2: 912.300076
	Epoch 3....
Epoch has taken 0:00:23.485467
Number of used sentences in train = 359
Total loss for epoch 3: 764.900903
	Epoch 4....
Epoch has taken 0:00:23.491322
Number of used sentences in train = 359
Total loss for epoch 4: 733.773288
	Epoch 5....
Epoch has taken 0:00:23.498142
Number of used sentences in train = 359
Total loss for epoch 5: 681.867288
	Epoch 6....
Epoch has taken 0:00:23.490892
Number of used sentences in train = 359
Total loss for epoch 6: 674.804510
	Epoch 7....
Epoch has taken 0:00:23.526968
Number of used sentences in train = 359
Total loss for epoch 7: 673.313098
	Epoch 8....
Epoch has taken 0:00:23.514220
Number of used sentences in train = 359
Total loss for epoch 8: 672.621427
	Epoch 9....
Epoch has taken 0:00:23.515111
Number of used sentences in train = 359
Total loss for epoch 9: 672.156618
	Epoch 10....
Epoch has taken 0:00:23.532905
Number of used sentences in train = 359
Total loss for epoch 10: 671.834557
	Epoch 11....
Epoch has taken 0:00:23.525167
Number of used sentences in train = 359
Total loss for epoch 11: 671.598083
	Epoch 12....
Epoch has taken 0:00:23.535965
Number of used sentences in train = 359
Total loss for epoch 12: 671.413431
	Epoch 13....
Epoch has taken 0:00:23.537043
Number of used sentences in train = 359
Total loss for epoch 13: 671.257234
	Epoch 14....
Epoch has taken 0:00:23.528060
Number of used sentences in train = 359
Total loss for epoch 14: 671.129387
Epoch has taken 0:00:23.526324

==================================================================================================
	Training time : 1:02:41.905423
==================================================================================================
	Identification : 0.437

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 18, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 19, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 43, 'lstmDropout': 0.36, 'denseActivation': 'tanh', 'wordDim': 55, 'batch': 1}True,True
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(1177, 55)
  (lstm): LSTM(74, 43, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 12211.747761
validation loss after epoch 0 : 940.385899
	Epoch 1....
Epoch has taken 0:03:12.755737
Number of used sentences in train = 2811
Total loss for epoch 1: 8270.755617
validation loss after epoch 1 : 930.826680
	Epoch 2....
Epoch has taken 0:03:10.952536
Number of used sentences in train = 2811
Total loss for epoch 2: 7512.162410
validation loss after epoch 2 : 886.622326
	Epoch 3....
Epoch has taken 0:03:09.114713
Number of used sentences in train = 2811
Total loss for epoch 3: 7078.435591
validation loss after epoch 3 : 934.349434
	Epoch 4....
Epoch has taken 0:03:10.329610
Number of used sentences in train = 2811
Total loss for epoch 4: 6757.613392
validation loss after epoch 4 : 960.904207
	Epoch 5....
Epoch has taken 0:03:10.918928
Number of used sentences in train = 2811
Total loss for epoch 5: 6444.849754
validation loss after epoch 5 : 921.038000
	Epoch 6....
Epoch has taken 0:03:16.079138
Number of used sentences in train = 2811
Total loss for epoch 6: 6254.317021
validation loss after epoch 6 : 933.297111
	Epoch 7....
Epoch has taken 0:02:56.781174
Number of used sentences in train = 2811
Total loss for epoch 7: 6052.471935
validation loss after epoch 7 : 968.925583
	Epoch 8....
Epoch has taken 0:03:10.651480
Number of used sentences in train = 2811
Total loss for epoch 8: 5856.509032
validation loss after epoch 8 : 984.642941
	Epoch 9....
Epoch has taken 0:03:10.291526
Number of used sentences in train = 2811
Total loss for epoch 9: 5710.745255
validation loss after epoch 9 : 995.798419
	Epoch 10....
Epoch has taken 0:03:08.966763
Number of used sentences in train = 2811
Total loss for epoch 10: 5525.036701
validation loss after epoch 10 : 1052.613636
	Epoch 11....
Epoch has taken 0:03:04.267641
Number of used sentences in train = 2811
Total loss for epoch 11: 5464.729648
validation loss after epoch 11 : 994.294574
	Epoch 12....
Epoch has taken 0:02:52.171974
Number of used sentences in train = 2811
Total loss for epoch 12: 5349.006544
validation loss after epoch 12 : 1027.338465
	Epoch 13....
Epoch has taken 0:02:52.514232
Number of used sentences in train = 2811
Total loss for epoch 13: 5206.365935
validation loss after epoch 13 : 1011.488895
	Epoch 14....
Epoch has taken 0:02:50.528243
Number of used sentences in train = 2811
Total loss for epoch 14: 5214.570839
validation loss after epoch 14 : 999.977211
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(1177, 55)
  (lstm): LSTM(74, 43, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.626490
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1343.913501
	Epoch 1....
Epoch has taken 0:00:18.231536
Number of used sentences in train = 313
Total loss for epoch 1: 798.405808
	Epoch 2....
Epoch has taken 0:00:19.719969
Number of used sentences in train = 313
Total loss for epoch 2: 680.712180
	Epoch 3....
Epoch has taken 0:00:20.283893
Number of used sentences in train = 313
Total loss for epoch 3: 621.275165
	Epoch 4....
Epoch has taken 0:00:20.312462
Number of used sentences in train = 313
Total loss for epoch 4: 581.953743
	Epoch 5....
Epoch has taken 0:00:20.309829
Number of used sentences in train = 313
Total loss for epoch 5: 589.516869
	Epoch 6....
Epoch has taken 0:00:20.326289
Number of used sentences in train = 313
Total loss for epoch 6: 555.397604
	Epoch 7....
Epoch has taken 0:00:20.333022
Number of used sentences in train = 313
Total loss for epoch 7: 555.633807
	Epoch 8....
Epoch has taken 0:00:20.316395
Number of used sentences in train = 313
Total loss for epoch 8: 542.067223
	Epoch 9....
Epoch has taken 0:00:20.315827
Number of used sentences in train = 313
Total loss for epoch 9: 545.899096
	Epoch 10....
Epoch has taken 0:00:20.310403
Number of used sentences in train = 313
Total loss for epoch 10: 540.947290
	Epoch 11....
Epoch has taken 0:00:20.322654
Number of used sentences in train = 313
Total loss for epoch 11: 534.609214
	Epoch 12....
Epoch has taken 0:00:20.311440
Number of used sentences in train = 313
Total loss for epoch 12: 535.662436
	Epoch 13....
Epoch has taken 0:00:20.338278
Number of used sentences in train = 313
Total loss for epoch 13: 531.810177
	Epoch 14....
Epoch has taken 0:00:20.312813
Number of used sentences in train = 313
Total loss for epoch 14: 532.255141
Epoch has taken 0:00:20.316686

==================================================================================================
	Training time : 0:51:09.506931
==================================================================================================
	Identification : 0.466

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(1133, 55)
  (lstm): LSTM(74, 43, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 8988.666792
validation loss after epoch 0 : 709.687087
	Epoch 1....
Epoch has taken 0:02:09.356671
Number of used sentences in train = 2074
Total loss for epoch 1: 5930.083911
validation loss after epoch 1 : 654.085369
	Epoch 2....
Epoch has taken 0:02:09.542189
Number of used sentences in train = 2074
Total loss for epoch 2: 5171.873387
validation loss after epoch 2 : 601.533089
	Epoch 3....
Epoch has taken 0:02:11.172147
Number of used sentences in train = 2074
Total loss for epoch 3: 4734.492343
validation loss after epoch 3 : 591.674644
	Epoch 4....
Epoch has taken 0:02:08.632621
Number of used sentences in train = 2074
Total loss for epoch 4: 4400.501933
validation loss after epoch 4 : 640.423221
	Epoch 5....
Epoch has taken 0:01:59.037868
Number of used sentences in train = 2074
Total loss for epoch 5: 4185.331088
validation loss after epoch 5 : 643.021802
	Epoch 6....
Epoch has taken 0:01:59.763303
Number of used sentences in train = 2074
Total loss for epoch 6: 4004.261861
validation loss after epoch 6 : 657.452663
	Epoch 7....
Epoch has taken 0:01:59.245168
Number of used sentences in train = 2074
Total loss for epoch 7: 3837.241123
validation loss after epoch 7 : 647.814761
	Epoch 8....
Epoch has taken 0:01:58.373341
Number of used sentences in train = 2074
Total loss for epoch 8: 3784.433432
validation loss after epoch 8 : 728.212199
	Epoch 9....
Epoch has taken 0:01:56.776549
Number of used sentences in train = 2074
Total loss for epoch 9: 3691.176925
validation loss after epoch 9 : 701.130403
	Epoch 10....
Epoch has taken 0:01:57.884494
Number of used sentences in train = 2074
Total loss for epoch 10: 3609.908712
validation loss after epoch 10 : 661.480881
	Epoch 11....
Epoch has taken 0:01:58.101062
Number of used sentences in train = 2074
Total loss for epoch 11: 3559.706272
validation loss after epoch 11 : 686.694800
	Epoch 12....
Epoch has taken 0:01:57.946681
Number of used sentences in train = 2074
Total loss for epoch 12: 3525.481482
validation loss after epoch 12 : 686.377880
	Epoch 13....
Epoch has taken 0:01:58.050303
Number of used sentences in train = 2074
Total loss for epoch 13: 3468.603422
validation loss after epoch 13 : 803.770222
	Epoch 14....
Epoch has taken 0:01:56.829898
Number of used sentences in train = 2074
Total loss for epoch 14: 3398.202025
validation loss after epoch 14 : 719.792744
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(1133, 55)
  (lstm): LSTM(74, 43, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.018483
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1104.729713
	Epoch 1....
Epoch has taken 0:00:11.915553
Number of used sentences in train = 231
Total loss for epoch 1: 598.676885
	Epoch 2....
Epoch has taken 0:00:11.940762
Number of used sentences in train = 231
Total loss for epoch 2: 485.964493
	Epoch 3....
Epoch has taken 0:00:11.931724
Number of used sentences in train = 231
Total loss for epoch 3: 411.047105
	Epoch 4....
Epoch has taken 0:00:11.928756
Number of used sentences in train = 231
Total loss for epoch 4: 401.258846
	Epoch 5....
Epoch has taken 0:00:11.899525
Number of used sentences in train = 231
Total loss for epoch 5: 368.694062
	Epoch 6....
Epoch has taken 0:00:11.947300
Number of used sentences in train = 231
Total loss for epoch 6: 366.086186
	Epoch 7....
Epoch has taken 0:00:11.912408
Number of used sentences in train = 231
Total loss for epoch 7: 368.759499
	Epoch 8....
Epoch has taken 0:00:11.942106
Number of used sentences in train = 231
Total loss for epoch 8: 359.114899
	Epoch 9....
Epoch has taken 0:00:12.025229
Number of used sentences in train = 231
Total loss for epoch 9: 361.398536
	Epoch 10....
Epoch has taken 0:00:12.045416
Number of used sentences in train = 231
Total loss for epoch 10: 361.363570
	Epoch 11....
Epoch has taken 0:00:12.023476
Number of used sentences in train = 231
Total loss for epoch 11: 353.232913
	Epoch 12....
Epoch has taken 0:00:12.051753
Number of used sentences in train = 231
Total loss for epoch 12: 348.283566
	Epoch 13....
Epoch has taken 0:00:12.025371
Number of used sentences in train = 231
Total loss for epoch 13: 348.122212
	Epoch 14....
Epoch has taken 0:00:12.048614
Number of used sentences in train = 231
Total loss for epoch 14: 350.253995
Epoch has taken 0:00:12.042510

==================================================================================================
	Training time : 0:33:17.741317
==================================================================================================
	Identification : 0.189

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(1202, 55)
  (lstm): LSTM(74, 43, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16605.507568
validation loss after epoch 0 : 1373.482364
	Epoch 1....
Epoch has taken 0:03:48.552074
Number of used sentences in train = 3226
Total loss for epoch 1: 11791.367956
validation loss after epoch 1 : 1253.595794
	Epoch 2....
Epoch has taken 0:03:48.555488
Number of used sentences in train = 3226
Total loss for epoch 2: 10689.374568
validation loss after epoch 2 : 1254.310293
	Epoch 3....
Epoch has taken 0:03:49.337739
Number of used sentences in train = 3226
Total loss for epoch 3: 10086.869745
validation loss after epoch 3 : 1290.568696
	Epoch 4....
Epoch has taken 0:03:47.109337
Number of used sentences in train = 3226
Total loss for epoch 4: 9681.544846
validation loss after epoch 4 : 1265.341319
	Epoch 5....
Epoch has taken 0:03:46.839424
Number of used sentences in train = 3226
Total loss for epoch 5: 9282.481927
validation loss after epoch 5 : 1277.764616
	Epoch 6....
Epoch has taken 0:04:00.339333
Number of used sentences in train = 3226
Total loss for epoch 6: 9061.453305
validation loss after epoch 6 : 1321.675806
	Epoch 7....
Epoch has taken 0:03:47.034663
Number of used sentences in train = 3226
Total loss for epoch 7: 8860.192796
validation loss after epoch 7 : 1377.060326
	Epoch 8....
Epoch has taken 0:03:48.516337
Number of used sentences in train = 3226
Total loss for epoch 8: 8673.016501
validation loss after epoch 8 : 1313.135787
	Epoch 9....
Epoch has taken 0:03:49.429912
Number of used sentences in train = 3226
Total loss for epoch 9: 8530.482172
validation loss after epoch 9 : 1388.214631
	Epoch 10....
Epoch has taken 0:03:48.911299
Number of used sentences in train = 3226
Total loss for epoch 10: 8289.459696
validation loss after epoch 10 : 1429.432540
	Epoch 11....
Epoch has taken 0:03:49.529873
Number of used sentences in train = 3226
Total loss for epoch 11: 8165.239527
validation loss after epoch 11 : 1458.015207
	Epoch 12....
Epoch has taken 0:03:46.875441
Number of used sentences in train = 3226
Total loss for epoch 12: 8075.742870
validation loss after epoch 12 : 1414.244031
	Epoch 13....
Epoch has taken 0:03:47.230259
Number of used sentences in train = 3226
Total loss for epoch 13: 7984.834786
validation loss after epoch 13 : 1513.432035
	Epoch 14....
Epoch has taken 0:03:47.973854
Number of used sentences in train = 3226
Total loss for epoch 14: 7876.362442
validation loss after epoch 14 : 1531.066445
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(1202, 55)
  (lstm): LSTM(74, 43, num_layers=2, dropout=0.36, bidirectional=True)
  (linear1): Linear(in_features=688, out_features=18, bias=True)
  (linear2): Linear(in_features=18, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:49.228096
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 1726.264238
	Epoch 1....
Epoch has taken 0:00:22.445134
Number of used sentences in train = 359
Total loss for epoch 1: 1185.217504
	Epoch 2....
Epoch has taken 0:00:22.463594
Number of used sentences in train = 359
Total loss for epoch 2: 1062.861076
	Epoch 3....
Epoch has taken 0:00:22.463575
Number of used sentences in train = 359
Total loss for epoch 3: 960.439837
	Epoch 4....
Epoch has taken 0:00:22.440065
Number of used sentences in train = 359
Total loss for epoch 4: 884.188540
	Epoch 5....
Epoch has taken 0:00:22.462857
Number of used sentences in train = 359
Total loss for epoch 5: 852.343004
	Epoch 6....
Epoch has taken 0:00:22.448832
Number of used sentences in train = 359
Total loss for epoch 6: 811.853754
	Epoch 7....
Epoch has taken 0:00:22.402704
Number of used sentences in train = 359
Total loss for epoch 7: 781.910753
	Epoch 8....
Epoch has taken 0:00:22.359084
Number of used sentences in train = 359
Total loss for epoch 8: 777.694888
	Epoch 9....
Epoch has taken 0:00:22.374533
Number of used sentences in train = 359
Total loss for epoch 9: 747.325717
	Epoch 10....
Epoch has taken 0:00:22.392690
Number of used sentences in train = 359
Total loss for epoch 10: 759.157323
	Epoch 11....
Epoch has taken 0:00:22.377847
Number of used sentences in train = 359
Total loss for epoch 11: 734.080042
	Epoch 12....
Epoch has taken 0:00:22.380505
Number of used sentences in train = 359
Total loss for epoch 12: 704.778441
	Epoch 13....
Epoch has taken 0:00:22.362695
Number of used sentences in train = 359
Total loss for epoch 13: 699.695131
	Epoch 14....
Epoch has taken 0:00:22.382024
Number of used sentences in train = 359
Total loss for epoch 14: 704.056884
Epoch has taken 0:00:22.370268

==================================================================================================
	Training time : 1:02:52.233213
==================================================================================================
	Identification : 0.283

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 14, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 19, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 34, 'lstmDropout': 0.22, 'denseActivation': 'tanh', 'wordDim': 109, 'batch': 1}True,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5936, 109)
  (lstm): LSTM(128, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 13001.914298
validation loss after epoch 0 : 1102.742306
	Epoch 1....
Epoch has taken 0:02:38.879091
Number of used sentences in train = 2811
Total loss for epoch 1: 8664.351486
validation loss after epoch 1 : 1084.794921
	Epoch 2....
Epoch has taken 0:02:38.875609
Number of used sentences in train = 2811
Total loss for epoch 2: 7058.295589
validation loss after epoch 2 : 1096.230214
	Epoch 3....
Epoch has taken 0:02:40.319979
Number of used sentences in train = 2811
Total loss for epoch 3: 6094.496629
validation loss after epoch 3 : 1156.314821
	Epoch 4....
Epoch has taken 0:02:40.797569
Number of used sentences in train = 2811
Total loss for epoch 4: 5571.236721
validation loss after epoch 4 : 1279.236687
	Epoch 5....
Epoch has taken 0:02:40.564924
Number of used sentences in train = 2811
Total loss for epoch 5: 5219.863348
validation loss after epoch 5 : 1348.611336
	Epoch 6....
Epoch has taken 0:02:39.583222
Number of used sentences in train = 2811
Total loss for epoch 6: 5018.601008
validation loss after epoch 6 : 1417.415712
	Epoch 7....
Epoch has taken 0:02:38.126933
Number of used sentences in train = 2811
Total loss for epoch 7: 4879.644176
validation loss after epoch 7 : 1443.295286
	Epoch 8....
Epoch has taken 0:02:39.995291
Number of used sentences in train = 2811
Total loss for epoch 8: 4797.608913
validation loss after epoch 8 : 1480.907350
	Epoch 9....
Epoch has taken 0:02:38.315048
Number of used sentences in train = 2811
Total loss for epoch 9: 4710.878003
validation loss after epoch 9 : 1500.997463
	Epoch 10....
Epoch has taken 0:02:39.618599
Number of used sentences in train = 2811
Total loss for epoch 10: 4654.897736
validation loss after epoch 10 : 1517.235566
	Epoch 11....
Epoch has taken 0:02:39.575165
Number of used sentences in train = 2811
Total loss for epoch 11: 4607.642277
validation loss after epoch 11 : 1555.424415
	Epoch 12....
Epoch has taken 0:02:39.971671
Number of used sentences in train = 2811
Total loss for epoch 12: 4583.484142
validation loss after epoch 12 : 1560.179855
	Epoch 13....
Epoch has taken 0:02:38.307865
Number of used sentences in train = 2811
Total loss for epoch 13: 4563.878614
validation loss after epoch 13 : 1597.396222
	Epoch 14....
Epoch has taken 0:02:39.695395
Number of used sentences in train = 2811
Total loss for epoch 14: 4550.737786
validation loss after epoch 14 : 1612.531808
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5936, 109)
  (lstm): LSTM(128, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:40.119453
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 1707.824507
	Epoch 1....
Epoch has taken 0:00:16.907708
Number of used sentences in train = 313
Total loss for epoch 1: 826.096692
	Epoch 2....
Epoch has taken 0:00:16.908784
Number of used sentences in train = 313
Total loss for epoch 2: 658.115428
	Epoch 3....
Epoch has taken 0:00:16.885622
Number of used sentences in train = 313
Total loss for epoch 3: 572.816898
	Epoch 4....
Epoch has taken 0:00:16.896849
Number of used sentences in train = 313
Total loss for epoch 4: 547.123977
	Epoch 5....
Epoch has taken 0:00:16.902083
Number of used sentences in train = 313
Total loss for epoch 5: 534.897695
	Epoch 6....
Epoch has taken 0:00:16.891160
Number of used sentences in train = 313
Total loss for epoch 6: 521.390934
	Epoch 7....
Epoch has taken 0:00:16.905186
Number of used sentences in train = 313
Total loss for epoch 7: 520.618985
	Epoch 8....
Epoch has taken 0:00:16.907015
Number of used sentences in train = 313
Total loss for epoch 8: 517.045624
	Epoch 9....
Epoch has taken 0:00:16.905156
Number of used sentences in train = 313
Total loss for epoch 9: 515.143198
	Epoch 10....
Epoch has taken 0:00:16.894739
Number of used sentences in train = 313
Total loss for epoch 10: 513.903513
	Epoch 11....
Epoch has taken 0:00:16.903772
Number of used sentences in train = 313
Total loss for epoch 11: 512.395153
	Epoch 12....
Epoch has taken 0:00:16.899596
Number of used sentences in train = 313
Total loss for epoch 12: 510.659758
	Epoch 13....
Epoch has taken 0:00:16.909862
Number of used sentences in train = 313
Total loss for epoch 13: 510.986628
	Epoch 14....
Epoch has taken 0:00:16.910794
Number of used sentences in train = 313
Total loss for epoch 14: 515.322295
Epoch has taken 0:00:16.913399

==================================================================================================
	Training time : 0:44:06.788724
==================================================================================================
	Identification : 0.238

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1396, occurrences : 2518
	Impotant words in tain : 1131
	MWE length mean : 2.22
	Seen MWEs : 340 (61 %)
	New MWEs : 213 (38 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5605, 109)
  (lstm): LSTM(128, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 10451.869816
validation loss after epoch 0 : 963.902162
	Epoch 1....
Epoch has taken 0:01:48.121923
Number of used sentences in train = 2074
Total loss for epoch 1: 6374.166200
validation loss after epoch 1 : 929.325245
	Epoch 2....
Epoch has taken 0:01:48.175139
Number of used sentences in train = 2074
Total loss for epoch 2: 5053.585645
validation loss after epoch 2 : 968.271188
	Epoch 3....
Epoch has taken 0:01:48.127645
Number of used sentences in train = 2074
Total loss for epoch 3: 4259.926878
validation loss after epoch 3 : 1035.250014
	Epoch 4....
Epoch has taken 0:01:49.522980
Number of used sentences in train = 2074
Total loss for epoch 4: 3830.544630
validation loss after epoch 4 : 1112.054469
	Epoch 5....
Epoch has taken 0:02:06.650094
Number of used sentences in train = 2074
Total loss for epoch 5: 3577.384750
validation loss after epoch 5 : 1169.763685
	Epoch 6....
Epoch has taken 0:01:49.224343
Number of used sentences in train = 2074
Total loss for epoch 6: 3431.941844
validation loss after epoch 6 : 1170.960983
	Epoch 7....
Epoch has taken 0:01:48.084421
Number of used sentences in train = 2074
Total loss for epoch 7: 3368.901624
validation loss after epoch 7 : 1189.322938
	Epoch 8....
Epoch has taken 0:01:49.099612
Number of used sentences in train = 2074
Total loss for epoch 8: 3310.317579
validation loss after epoch 8 : 1244.512009
	Epoch 9....
Epoch has taken 0:01:49.285327
Number of used sentences in train = 2074
Total loss for epoch 9: 3283.278888
validation loss after epoch 9 : 1251.343029
	Epoch 10....
Epoch has taken 0:02:06.566399
Number of used sentences in train = 2074
Total loss for epoch 10: 3252.958992
validation loss after epoch 10 : 1255.680953
	Epoch 11....
Epoch has taken 0:01:49.217372
Number of used sentences in train = 2074
Total loss for epoch 11: 3239.731939
validation loss after epoch 11 : 1277.586780
	Epoch 12....
Epoch has taken 0:01:48.989494
Number of used sentences in train = 2074
Total loss for epoch 12: 3226.874144
validation loss after epoch 12 : 1297.624206
	Epoch 13....
Epoch has taken 0:01:49.142349
Number of used sentences in train = 2074
Total loss for epoch 13: 3223.035189
validation loss after epoch 13 : 1304.435247
	Epoch 14....
Epoch has taken 0:01:49.157579
Number of used sentences in train = 2074
Total loss for epoch 14: 3211.148907
validation loss after epoch 14 : 1305.774575
	TransitionClassifier(
  (p_embeddings): Embedding(18, 19)
  (w_embeddings): Embedding(5605, 109)
  (lstm): LSTM(128, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:56.032422
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1326.836188
	Epoch 1....
Epoch has taken 0:00:11.130673
Number of used sentences in train = 231
Total loss for epoch 1: 647.108244
	Epoch 2....
Epoch has taken 0:00:11.109667
Number of used sentences in train = 231
Total loss for epoch 2: 453.536009
	Epoch 3....
Epoch has taken 0:00:11.099096
Number of used sentences in train = 231
Total loss for epoch 3: 403.811577
	Epoch 4....
Epoch has taken 0:00:11.097702
Number of used sentences in train = 231
Total loss for epoch 4: 378.628534
	Epoch 5....
Epoch has taken 0:00:11.095430
Number of used sentences in train = 231
Total loss for epoch 5: 367.387666
	Epoch 6....
Epoch has taken 0:00:11.096753
Number of used sentences in train = 231
Total loss for epoch 6: 360.508594
	Epoch 7....
Epoch has taken 0:00:11.096358
Number of used sentences in train = 231
Total loss for epoch 7: 357.607584
	Epoch 8....
Epoch has taken 0:00:11.127176
Number of used sentences in train = 231
Total loss for epoch 8: 354.859066
	Epoch 9....
Epoch has taken 0:00:11.121499
Number of used sentences in train = 231
Total loss for epoch 9: 353.371066
	Epoch 10....
Epoch has taken 0:00:11.106455
Number of used sentences in train = 231
Total loss for epoch 10: 352.438308
	Epoch 11....
Epoch has taken 0:00:10.990443
Number of used sentences in train = 231
Total loss for epoch 11: 351.661734
	Epoch 12....
Epoch has taken 0:00:11.028901
Number of used sentences in train = 231
Total loss for epoch 12: 351.040816
	Epoch 13....
Epoch has taken 0:00:10.991041
Number of used sentences in train = 231
Total loss for epoch 13: 350.292710
	Epoch 14....
Epoch has taken 0:00:11.016787
Number of used sentences in train = 231
Total loss for epoch 14: 349.745797
Epoch has taken 0:00:11.009264

==================================================================================================
	Training time : 0:30:41.851807
==================================================================================================
	Identification : 0.401

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 1790, occurrences : 4856
	Impotant words in tain : 1200
	MWE length mean : 2.06
	Seen MWEs : 345 (67 %)
	New MWEs : 165 (32 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(6868, 109)
  (lstm): LSTM(128, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 16945.371566
validation loss after epoch 0 : 1583.934014
	Epoch 1....
Epoch has taken 0:03:32.745052
Number of used sentences in train = 3226
Total loss for epoch 1: 12074.676129
validation loss after epoch 1 : 1526.044273
	Epoch 2....
Epoch has taken 0:03:33.091751
Number of used sentences in train = 3226
Total loss for epoch 2: 10262.096451
validation loss after epoch 2 : 1575.390896
	Epoch 3....
Epoch has taken 0:03:32.194051
Number of used sentences in train = 3226
Total loss for epoch 3: 9049.848158
validation loss after epoch 3 : 1599.498395
	Epoch 4....
Epoch has taken 0:03:30.364251
Number of used sentences in train = 3226
Total loss for epoch 4: 8168.812299
validation loss after epoch 4 : 1761.832032
	Epoch 5....
Epoch has taken 0:03:31.083934
Number of used sentences in train = 3226
Total loss for epoch 5: 7463.183547
validation loss after epoch 5 : 1965.569283
	Epoch 6....
Epoch has taken 0:03:32.706269
Number of used sentences in train = 3226
Total loss for epoch 6: 7046.587208
validation loss after epoch 6 : 2074.940299
	Epoch 7....
Epoch has taken 0:04:06.553878
Number of used sentences in train = 3226
Total loss for epoch 7: 6712.646378
validation loss after epoch 7 : 2183.198302
	Epoch 8....
Epoch has taken 0:03:30.721026
Number of used sentences in train = 3226
Total loss for epoch 8: 6525.085386
validation loss after epoch 8 : 2241.247554
	Epoch 9....
Epoch has taken 0:03:33.151479
Number of used sentences in train = 3226
Total loss for epoch 9: 6428.298015
validation loss after epoch 9 : 2339.739215
	Epoch 10....
Epoch has taken 0:03:32.555082
Number of used sentences in train = 3226
Total loss for epoch 10: 6361.411134
validation loss after epoch 10 : 2334.816025
	Epoch 11....
Epoch has taken 0:03:32.904515
Number of used sentences in train = 3226
Total loss for epoch 11: 6318.943902
validation loss after epoch 11 : 2404.775203
	Epoch 12....
Epoch has taken 0:03:32.485497
Number of used sentences in train = 3226
Total loss for epoch 12: 6302.530892
validation loss after epoch 12 : 2416.402690
	Epoch 13....
Epoch has taken 0:03:30.861400
Number of used sentences in train = 3226
Total loss for epoch 13: 6275.898268
validation loss after epoch 13 : 2473.304450
	Epoch 14....
Epoch has taken 0:03:32.425963
Number of used sentences in train = 3226
Total loss for epoch 14: 6250.122476
validation loss after epoch 14 : 2492.468372
	TransitionClassifier(
  (p_embeddings): Embedding(13, 19)
  (w_embeddings): Embedding(6868, 109)
  (lstm): LSTM(128, 34, bidirectional=True)
  (linear1): Linear(in_features=544, out_features=14, bias=True)
  (linear2): Linear(in_features=14, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:35.575653
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2458.526310
	Epoch 1....
Epoch has taken 0:00:20.760353
Number of used sentences in train = 359
Total loss for epoch 1: 1106.554134
	Epoch 2....
Epoch has taken 0:00:20.800104
Number of used sentences in train = 359
Total loss for epoch 2: 858.939255
	Epoch 3....
Epoch has taken 0:00:20.768115
Number of used sentences in train = 359
Total loss for epoch 3: 765.851667
	Epoch 4....
Epoch has taken 0:00:20.784346
Number of used sentences in train = 359
Total loss for epoch 4: 732.170086
	Epoch 5....
Epoch has taken 0:00:20.779938
Number of used sentences in train = 359
Total loss for epoch 5: 714.455114
	Epoch 6....
Epoch has taken 0:00:20.773900
Number of used sentences in train = 359
Total loss for epoch 6: 706.656582
	Epoch 7....
Epoch has taken 0:00:20.759499
Number of used sentences in train = 359
Total loss for epoch 7: 702.368996
	Epoch 8....
Epoch has taken 0:00:20.770183
Number of used sentences in train = 359
Total loss for epoch 8: 700.045802
	Epoch 9....
Epoch has taken 0:00:20.801449
Number of used sentences in train = 359
Total loss for epoch 9: 698.377254
	Epoch 10....
Epoch has taken 0:00:20.780824
Number of used sentences in train = 359
Total loss for epoch 10: 697.111823
	Epoch 11....
Epoch has taken 0:00:20.769466
Number of used sentences in train = 359
Total loss for epoch 11: 695.164236
	Epoch 12....
Epoch has taken 0:00:20.747438
Number of used sentences in train = 359
Total loss for epoch 12: 693.192768
	Epoch 13....
Epoch has taken 0:00:20.718292
Number of used sentences in train = 359
Total loss for epoch 13: 687.401398
	Epoch 14....
Epoch has taken 0:00:20.722766
Number of used sentences in train = 359
Total loss for epoch 14: 685.698957
Epoch has taken 0:00:20.723841

==================================================================================================
	Training time : 0:58:51.521033
==================================================================================================
	Identification : 0.466

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 2, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 12, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 28, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 78, 'lstmDropout': 0.27, 'denseActivation': 'tanh', 'wordDim': 99, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(9325, 99)
  (lstm): LSTM(127, 78, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1248, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 26674.107277
validation loss after epoch 0 : 2713.660040
	Epoch 1....
Epoch has taken 0:02:57.664774
Number of used sentences in train = 2811
Total loss for epoch 1: 20946.567576
validation loss after epoch 1 : 1924.765904
	Epoch 2....
Epoch has taken 0:03:11.040310
Number of used sentences in train = 2811
Total loss for epoch 2: 15351.832250
validation loss after epoch 2 : 1511.452559
	Epoch 3....
Epoch has taken 0:03:11.404226
Number of used sentences in train = 2811
Total loss for epoch 3: 13089.891364
validation loss after epoch 3 : 1445.566371
	Epoch 4....
Epoch has taken 0:03:09.762992
Number of used sentences in train = 2811
Total loss for epoch 4: 11706.929065
validation loss after epoch 4 : 1340.641586
	Epoch 5....
Epoch has taken 0:03:09.894930
Number of used sentences in train = 2811
Total loss for epoch 5: 10186.706898
validation loss after epoch 5 : 1332.142103
	Epoch 6....
Epoch has taken 0:03:09.730693
Number of used sentences in train = 2811
Total loss for epoch 6: 9285.514896
validation loss after epoch 6 : 1361.443420
	Epoch 7....
Epoch has taken 0:03:13.417569
Number of used sentences in train = 2811
Total loss for epoch 7: 8762.880609
validation loss after epoch 7 : 1333.556755
	Epoch 8....
Epoch has taken 0:03:11.175144
Number of used sentences in train = 2811
Total loss for epoch 8: 8384.769802
validation loss after epoch 8 : 1362.991249
	Epoch 9....
Epoch has taken 0:03:11.382106
Number of used sentences in train = 2811
Total loss for epoch 9: 8082.342742
validation loss after epoch 9 : 1451.374780
	Epoch 10....
Epoch has taken 0:03:11.147973
Number of used sentences in train = 2811
Total loss for epoch 10: 7860.164306
validation loss after epoch 10 : 1431.216434
	Epoch 11....
Epoch has taken 0:03:09.503346
Number of used sentences in train = 2811
Total loss for epoch 11: 7242.849952
validation loss after epoch 11 : 1379.514203
	Epoch 12....
Epoch has taken 0:03:09.678795
Number of used sentences in train = 2811
Total loss for epoch 12: 6721.897947
validation loss after epoch 12 : 1493.217382
	Epoch 13....
Epoch has taken 0:03:11.115770
Number of used sentences in train = 2811
Total loss for epoch 13: 6342.939338
validation loss after epoch 13 : 1448.636030
	Epoch 14....
Epoch has taken 0:02:53.490453
Number of used sentences in train = 2811
Total loss for epoch 14: 6043.693506
validation loss after epoch 14 : 1495.409105
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(9325, 99)
  (lstm): LSTM(127, 78, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1248, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:52.830643
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2085.039788
	Epoch 1....
Epoch has taken 0:00:18.356261
Number of used sentences in train = 313
Total loss for epoch 1: 1093.996593
	Epoch 2....
Epoch has taken 0:00:18.356383
Number of used sentences in train = 313
Total loss for epoch 2: 901.851241
	Epoch 3....
Epoch has taken 0:00:18.331197
Number of used sentences in train = 313
Total loss for epoch 3: 761.965138
	Epoch 4....
Epoch has taken 0:00:18.327437
Number of used sentences in train = 313
Total loss for epoch 4: 716.903241
	Epoch 5....
Epoch has taken 0:00:18.356310
Number of used sentences in train = 313
Total loss for epoch 5: 686.482203
	Epoch 6....
Epoch has taken 0:00:18.329801
Number of used sentences in train = 313
Total loss for epoch 6: 661.277398
	Epoch 7....
Epoch has taken 0:00:18.328108
Number of used sentences in train = 313
Total loss for epoch 7: 638.116228
	Epoch 8....
Epoch has taken 0:00:18.299213
Number of used sentences in train = 313
Total loss for epoch 8: 620.615818
	Epoch 9....
Epoch has taken 0:00:18.335605
Number of used sentences in train = 313
Total loss for epoch 9: 623.449205
	Epoch 10....
Epoch has taken 0:00:18.318635
Number of used sentences in train = 313
Total loss for epoch 10: 616.585348
	Epoch 11....
Epoch has taken 0:00:18.321480
Number of used sentences in train = 313
Total loss for epoch 11: 602.734354
	Epoch 12....
Epoch has taken 0:00:18.326729
Number of used sentences in train = 313
Total loss for epoch 12: 599.916871
	Epoch 13....
Epoch has taken 0:00:18.331563
Number of used sentences in train = 313
Total loss for epoch 13: 602.535797
	Epoch 14....
Epoch has taken 0:00:18.345803
Number of used sentences in train = 313
Total loss for epoch 14: 591.626062
Epoch has taken 0:00:18.348596

==================================================================================================
	Training time : 0:51:28.766622
==================================================================================================
	Identification : 0.505

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(7083, 99)
  (lstm): LSTM(127, 78, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1248, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 13848.606625
validation loss after epoch 0 : 1086.341045
	Epoch 1....
Epoch has taken 0:02:12.031313
Number of used sentences in train = 2074
Total loss for epoch 1: 8351.012300
validation loss after epoch 1 : 975.638361
	Epoch 2....
Epoch has taken 0:01:58.448128
Number of used sentences in train = 2074
Total loss for epoch 2: 6692.378833
validation loss after epoch 2 : 956.171225
	Epoch 3....
Epoch has taken 0:01:57.266730
Number of used sentences in train = 2074
Total loss for epoch 3: 5736.023741
validation loss after epoch 3 : 953.954797
	Epoch 4....
Epoch has taken 0:01:57.551887
Number of used sentences in train = 2074
Total loss for epoch 4: 4963.784505
validation loss after epoch 4 : 957.018760
	Epoch 5....
Epoch has taken 0:01:58.517777
Number of used sentences in train = 2074
Total loss for epoch 5: 4485.742072
validation loss after epoch 5 : 982.950342
	Epoch 6....
Epoch has taken 0:02:09.562707
Number of used sentences in train = 2074
Total loss for epoch 6: 4226.297348
validation loss after epoch 6 : 980.330438
	Epoch 7....
Epoch has taken 0:02:09.877109
Number of used sentences in train = 2074
Total loss for epoch 7: 3919.744253
validation loss after epoch 7 : 1135.158575
	Epoch 8....
Epoch has taken 0:01:57.512947
Number of used sentences in train = 2074
Total loss for epoch 8: 3808.575653
validation loss after epoch 8 : 1128.322177
	Epoch 9....
Epoch has taken 0:01:58.482453
Number of used sentences in train = 2074
Total loss for epoch 9: 3691.211963
validation loss after epoch 9 : 1135.056811
	Epoch 10....
Epoch has taken 0:02:02.116444
Number of used sentences in train = 2074
Total loss for epoch 10: 3570.001193
validation loss after epoch 10 : 1103.419525
	Epoch 11....
Epoch has taken 0:02:16.930410
Number of used sentences in train = 2074
Total loss for epoch 11: 3539.586295
validation loss after epoch 11 : 1199.989022
	Epoch 12....
Epoch has taken 0:01:58.839773
Number of used sentences in train = 2074
Total loss for epoch 12: 3497.189865
validation loss after epoch 12 : 1136.150492
	Epoch 13....
Epoch has taken 0:01:57.591220
Number of used sentences in train = 2074
Total loss for epoch 13: 3423.378925
validation loss after epoch 13 : 1214.035473
	Epoch 14....
Epoch has taken 0:01:57.587266
Number of used sentences in train = 2074
Total loss for epoch 14: 3354.130906
validation loss after epoch 14 : 1256.332932
	TransitionClassifier(
  (p_embeddings): Embedding(18, 28)
  (w_embeddings): Embedding(7083, 99)
  (lstm): LSTM(127, 78, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1248, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:57.598536
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 2044.969828
	Epoch 1....
Epoch has taken 0:00:12.094055
Number of used sentences in train = 231
Total loss for epoch 1: 912.579298
	Epoch 2....
Epoch has taken 0:00:12.074355
Number of used sentences in train = 231
Total loss for epoch 2: 695.744363
	Epoch 3....
Epoch has taken 0:00:12.085449
Number of used sentences in train = 231
Total loss for epoch 3: 551.012428
	Epoch 4....
Epoch has taken 0:00:12.064843
Number of used sentences in train = 231
Total loss for epoch 4: 472.512653
	Epoch 5....
Epoch has taken 0:00:12.692137
Number of used sentences in train = 231
Total loss for epoch 5: 418.430377
	Epoch 6....
Epoch has taken 0:00:13.391295
Number of used sentences in train = 231
Total loss for epoch 6: 400.659672
	Epoch 7....
Epoch has taken 0:00:13.400689
Number of used sentences in train = 231
Total loss for epoch 7: 373.558787
	Epoch 8....
Epoch has taken 0:00:13.371407
Number of used sentences in train = 231
Total loss for epoch 8: 367.373103
	Epoch 9....
Epoch has taken 0:00:13.371777
Number of used sentences in train = 231
Total loss for epoch 9: 369.846956
	Epoch 10....
Epoch has taken 0:00:13.418628
Number of used sentences in train = 231
Total loss for epoch 10: 363.286002
	Epoch 11....
Epoch has taken 0:00:13.338909
Number of used sentences in train = 231
Total loss for epoch 11: 361.893154
	Epoch 12....
Epoch has taken 0:00:13.400409
Number of used sentences in train = 231
Total loss for epoch 12: 358.863780
	Epoch 13....
Epoch has taken 0:00:13.369143
Number of used sentences in train = 231
Total loss for epoch 13: 351.826068
	Epoch 14....
Epoch has taken 0:00:13.375102
Number of used sentences in train = 231
Total loss for epoch 14: 351.153239
Epoch has taken 0:00:13.096496

==================================================================================================
	Training time : 0:33:44.823441
==================================================================================================
	Identification : 0.353

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(18007, 99)
  (lstm): LSTM(127, 78, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1248, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 21999.974481
validation loss after epoch 0 : 1741.745308
	Epoch 1....
Epoch has taken 0:04:12.378523
Number of used sentences in train = 3226
Total loss for epoch 1: 14137.258213
validation loss after epoch 1 : 1632.397617
	Epoch 2....
Epoch has taken 0:04:19.990906
Number of used sentences in train = 3226
Total loss for epoch 2: 11200.777589
validation loss after epoch 2 : 1597.958620
	Epoch 3....
Epoch has taken 0:04:06.512078
Number of used sentences in train = 3226
Total loss for epoch 3: 9510.050087
validation loss after epoch 3 : 1629.711267
	Epoch 4....
Epoch has taken 0:04:17.360035
Number of used sentences in train = 3226
Total loss for epoch 4: 8435.071463
validation loss after epoch 4 : 1714.857332
	Epoch 5....
Epoch has taken 0:04:06.057009
Number of used sentences in train = 3226
Total loss for epoch 5: 7677.589731
validation loss after epoch 5 : 1961.253712
	Epoch 6....
Epoch has taken 0:04:14.459785
Number of used sentences in train = 3226
Total loss for epoch 6: 7370.210885
validation loss after epoch 6 : 1819.778398
	Epoch 7....
Epoch has taken 0:04:05.998913
Number of used sentences in train = 3226
Total loss for epoch 7: 7039.113747
validation loss after epoch 7 : 1904.045172
	Epoch 8....
Epoch has taken 0:03:50.227638
Number of used sentences in train = 3226
Total loss for epoch 8: 6828.663757
validation loss after epoch 8 : 2017.569443
	Epoch 9....
Epoch has taken 0:04:26.816151
Number of used sentences in train = 3226
Total loss for epoch 9: 6724.950323
validation loss after epoch 9 : 2008.784956
	Epoch 10....
Epoch has taken 0:04:01.248501
Number of used sentences in train = 3226
Total loss for epoch 10: 6628.562444
validation loss after epoch 10 : 2191.102102
	Epoch 11....
Epoch has taken 0:03:51.250548
Number of used sentences in train = 3226
Total loss for epoch 11: 6583.199047
validation loss after epoch 11 : 2197.469153
	Epoch 12....
Epoch has taken 0:03:49.276593
Number of used sentences in train = 3226
Total loss for epoch 12: 6484.429911
validation loss after epoch 12 : 2301.884205
	Epoch 13....
Epoch has taken 0:04:10.494506
Number of used sentences in train = 3226
Total loss for epoch 13: 6425.159436
validation loss after epoch 13 : 2335.480645
	Epoch 14....
Epoch has taken 0:04:26.794783
Number of used sentences in train = 3226
Total loss for epoch 14: 6439.178325
validation loss after epoch 14 : 2201.914818
	TransitionClassifier(
  (p_embeddings): Embedding(13, 28)
  (w_embeddings): Embedding(18007, 99)
  (lstm): LSTM(127, 78, num_layers=2, dropout=0.27, bidirectional=True)
  (linear1): Linear(in_features=1248, out_features=12, bias=True)
  (linear2): Linear(in_features=12, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:04:21.025992
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 2308.953751
	Epoch 1....
Epoch has taken 0:00:23.034001
Number of used sentences in train = 359
Total loss for epoch 1: 1404.972495
	Epoch 2....
Epoch has taken 0:00:23.029563
Number of used sentences in train = 359
Total loss for epoch 2: 1083.296159
	Epoch 3....
Epoch has taken 0:00:23.037547
Number of used sentences in train = 359
Total loss for epoch 3: 896.827119
	Epoch 4....
Epoch has taken 0:00:23.022305
Number of used sentences in train = 359
Total loss for epoch 4: 814.440743
	Epoch 5....
Epoch has taken 0:00:23.012081
Number of used sentences in train = 359
Total loss for epoch 5: 776.947153
	Epoch 6....
Epoch has taken 0:00:23.053706
Number of used sentences in train = 359
Total loss for epoch 6: 730.239025
	Epoch 7....
Epoch has taken 0:00:23.194442
Number of used sentences in train = 359
Total loss for epoch 7: 721.876190
	Epoch 8....
Epoch has taken 0:00:23.188876
Number of used sentences in train = 359
Total loss for epoch 8: 692.281052
	Epoch 9....
Epoch has taken 0:00:23.194950
Number of used sentences in train = 359
Total loss for epoch 9: 684.528687
	Epoch 10....
Epoch has taken 0:00:23.179938
Number of used sentences in train = 359
Total loss for epoch 10: 684.474295
	Epoch 11....
Epoch has taken 0:00:23.196465
Number of used sentences in train = 359
Total loss for epoch 11: 691.819289
	Epoch 12....
Epoch has taken 0:00:23.186070
Number of used sentences in train = 359
Total loss for epoch 12: 682.506521
	Epoch 13....
Epoch has taken 0:00:23.190713
Number of used sentences in train = 359
Total loss for epoch 13: 677.237827
	Epoch 14....
Epoch has taken 0:00:23.187714
Number of used sentences in train = 359
Total loss for epoch 14: 677.765611
Epoch has taken 0:00:23.191182

==================================================================================================
	Training time : 1:08:07.503150
==================================================================================================
	Identification : 0.441

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 139, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 18, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 62, 'lstmDropout': 0.16, 'denseActivation': 'tanh', 'wordDim': 229, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(9302, 229)
  (lstm): LSTM(247, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=139, bias=True)
  (linear2): Linear(in_features=139, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 14713.617630
validation loss after epoch 0 : 1148.673618
	Epoch 1....
Epoch has taken 0:02:43.807943
Number of used sentences in train = 2811
Total loss for epoch 1: 7519.794945
validation loss after epoch 1 : 1114.945955
	Epoch 2....
Epoch has taken 0:02:43.421978
Number of used sentences in train = 2811
Total loss for epoch 2: 5633.089930
validation loss after epoch 2 : 1388.852555
	Epoch 3....
Epoch has taken 0:02:42.177676
Number of used sentences in train = 2811
Total loss for epoch 3: 4925.431284
validation loss after epoch 3 : 1420.508337
	Epoch 4....
Epoch has taken 0:02:43.506368
Number of used sentences in train = 2811
Total loss for epoch 4: 4664.526854
validation loss after epoch 4 : 1581.008653
	Epoch 5....
Epoch has taken 0:02:43.633110
Number of used sentences in train = 2811
Total loss for epoch 5: 4564.151521
validation loss after epoch 5 : 1670.340001
	Epoch 6....
Epoch has taken 0:02:43.355455
Number of used sentences in train = 2811
Total loss for epoch 6: 4520.182690
validation loss after epoch 6 : 1703.027821
	Epoch 7....
Epoch has taken 0:02:41.892393
Number of used sentences in train = 2811
Total loss for epoch 7: 4505.817119
validation loss after epoch 7 : 1765.145339
	Epoch 8....
Epoch has taken 0:02:42.833926
Number of used sentences in train = 2811
Total loss for epoch 8: 4496.880092
validation loss after epoch 8 : 1794.532893
	Epoch 9....
Epoch has taken 0:02:43.461355
Number of used sentences in train = 2811
Total loss for epoch 9: 4490.694203
validation loss after epoch 9 : 1822.866869
	Epoch 10....
Epoch has taken 0:02:43.950337
Number of used sentences in train = 2811
Total loss for epoch 10: 4487.323297
validation loss after epoch 10 : 1844.893839
	Epoch 11....
Epoch has taken 0:02:43.907154
Number of used sentences in train = 2811
Total loss for epoch 11: 4484.541606
validation loss after epoch 11 : 1869.427409
	Epoch 12....
Epoch has taken 0:02:41.795156
Number of used sentences in train = 2811
Total loss for epoch 12: 4482.349995
validation loss after epoch 12 : 1890.471332
	Epoch 13....
Epoch has taken 0:02:43.328889
Number of used sentences in train = 2811
Total loss for epoch 13: 4480.281181
validation loss after epoch 13 : 1906.319427
	Epoch 14....
Epoch has taken 0:02:52.650161
Number of used sentences in train = 2811
Total loss for epoch 14: 4478.545558
validation loss after epoch 14 : 1928.046928
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(9302, 229)
  (lstm): LSTM(247, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=139, bias=True)
  (linear2): Linear(in_features=139, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:02:50.968558
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 313
Total loss for epoch 0: 2419.002913
	Epoch 1....
Epoch has taken 0:00:18.370791
Number of used sentences in train = 313
Total loss for epoch 1: 768.847193
	Epoch 2....
Epoch has taken 0:00:18.523896
Number of used sentences in train = 313
Total loss for epoch 2: 589.865901
	Epoch 3....
Epoch has taken 0:00:18.323677
Number of used sentences in train = 313
Total loss for epoch 3: 531.386681
	Epoch 4....
Epoch has taken 0:00:18.325199
Number of used sentences in train = 313
Total loss for epoch 4: 514.856921
	Epoch 5....
Epoch has taken 0:00:18.301494
Number of used sentences in train = 313
Total loss for epoch 5: 510.545897
	Epoch 6....
Epoch has taken 0:00:18.529089
Number of used sentences in train = 313
Total loss for epoch 6: 507.802547
	Epoch 7....
Epoch has taken 0:00:18.075779
Number of used sentences in train = 313
Total loss for epoch 7: 507.339605
	Epoch 8....
Epoch has taken 0:00:18.181678
Number of used sentences in train = 313
Total loss for epoch 8: 506.671491
	Epoch 9....
Epoch has taken 0:00:18.118086
Number of used sentences in train = 313
Total loss for epoch 9: 505.442890
	Epoch 10....
Epoch has taken 0:00:18.261806
Number of used sentences in train = 313
Total loss for epoch 10: 504.876477
	Epoch 11....
Epoch has taken 0:00:18.283954
Number of used sentences in train = 313
Total loss for epoch 11: 504.237558
	Epoch 12....
Epoch has taken 0:00:17.393036
Number of used sentences in train = 313
Total loss for epoch 12: 503.390157
	Epoch 13....
Epoch has taken 0:00:18.067826
Number of used sentences in train = 313
Total loss for epoch 13: 503.425756
	Epoch 14....
Epoch has taken 0:00:18.214929
Number of used sentences in train = 313
Total loss for epoch 14: 503.103329
Epoch has taken 0:00:18.272117

==================================================================================================
	Training time : 0:45:38.504178
==================================================================================================
	Identification : 0.289

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 2305, Test : 3117
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 192 (34 %)
	New MWEs : 361 (65 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(7063, 229)
  (lstm): LSTM(247, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=139, bias=True)
  (linear2): Linear(in_features=139, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2074
Total loss for epoch 0: 11640.823521
validation loss after epoch 0 : 1033.096610
	Epoch 1....
Epoch has taken 0:01:57.789800
Number of used sentences in train = 2074
Total loss for epoch 1: 5610.955277
validation loss after epoch 1 : 1145.590089
	Epoch 2....
Epoch has taken 0:01:57.686880
Number of used sentences in train = 2074
Total loss for epoch 2: 4026.330475
validation loss after epoch 2 : 1337.114777
	Epoch 3....
Epoch has taken 0:01:58.152429
Number of used sentences in train = 2074
Total loss for epoch 3: 3470.513114
validation loss after epoch 3 : 1436.747691
	Epoch 4....
Epoch has taken 0:01:57.994112
Number of used sentences in train = 2074
Total loss for epoch 4: 3297.952268
validation loss after epoch 4 : 1536.005166
	Epoch 5....
Epoch has taken 0:01:57.011463
Number of used sentences in train = 2074
Total loss for epoch 5: 3225.141206
validation loss after epoch 5 : 1599.547567
	Epoch 6....
Epoch has taken 0:01:58.198383
Number of used sentences in train = 2074
Total loss for epoch 6: 3191.226190
validation loss after epoch 6 : 1654.442246
	Epoch 7....
Epoch has taken 0:02:15.771435
Number of used sentences in train = 2074
Total loss for epoch 7: 3177.206024
validation loss after epoch 7 : 1702.817161
	Epoch 8....
Epoch has taken 0:01:57.914029
Number of used sentences in train = 2074
Total loss for epoch 8: 3170.401719
validation loss after epoch 8 : 1740.288737
	Epoch 9....
Epoch has taken 0:01:57.954150
Number of used sentences in train = 2074
Total loss for epoch 9: 3163.872436
validation loss after epoch 9 : 1764.169136
	Epoch 10....
Epoch has taken 0:01:56.508954
Number of used sentences in train = 2074
Total loss for epoch 10: 3161.619437
validation loss after epoch 10 : 1792.382862
	Epoch 11....
Epoch has taken 0:01:57.865420
Number of used sentences in train = 2074
Total loss for epoch 11: 3159.151115
validation loss after epoch 11 : 1806.950570
	Epoch 12....
Epoch has taken 0:02:14.328593
Number of used sentences in train = 2074
Total loss for epoch 12: 3157.674659
validation loss after epoch 12 : 1823.287987
	Epoch 13....
Epoch has taken 0:01:54.810921
Number of used sentences in train = 2074
Total loss for epoch 13: 3156.482316
validation loss after epoch 13 : 1838.143044
	Epoch 14....
Epoch has taken 0:01:56.943934
Number of used sentences in train = 2074
Total loss for epoch 14: 3155.316055
validation loss after epoch 14 : 1851.849837
	TransitionClassifier(
  (p_embeddings): Embedding(18, 18)
  (w_embeddings): Embedding(7063, 229)
  (lstm): LSTM(247, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=139, bias=True)
  (linear2): Linear(in_features=139, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:01:58.204627
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 231
Total loss for epoch 0: 1997.646944
	Epoch 1....
Epoch has taken 0:00:13.854121
Number of used sentences in train = 231
Total loss for epoch 1: 564.737845
	Epoch 2....
Epoch has taken 0:00:13.830588
Number of used sentences in train = 231
Total loss for epoch 2: 410.248189
	Epoch 3....
Epoch has taken 0:00:13.920659
Number of used sentences in train = 231
Total loss for epoch 3: 368.960831
	Epoch 4....
Epoch has taken 0:00:13.640236
Number of used sentences in train = 231
Total loss for epoch 4: 354.577046
	Epoch 5....
Epoch has taken 0:00:13.829842
Number of used sentences in train = 231
Total loss for epoch 5: 352.220399
	Epoch 6....
Epoch has taken 0:00:13.913595
Number of used sentences in train = 231
Total loss for epoch 6: 349.100355
	Epoch 7....
Epoch has taken 0:00:13.692255
Number of used sentences in train = 231
Total loss for epoch 7: 347.998617
	Epoch 8....
Epoch has taken 0:00:12.245728
Number of used sentences in train = 231
Total loss for epoch 8: 346.994538
	Epoch 9....
Epoch has taken 0:00:11.885993
Number of used sentences in train = 231
Total loss for epoch 9: 346.590932
	Epoch 10....
Epoch has taken 0:00:12.097909
Number of used sentences in train = 231
Total loss for epoch 10: 346.240515
	Epoch 11....
Epoch has taken 0:00:11.807107
Number of used sentences in train = 231
Total loss for epoch 11: 346.003107
	Epoch 12....
Epoch has taken 0:00:12.116588
Number of used sentences in train = 231
Total loss for epoch 12: 345.848507
	Epoch 13....
Epoch has taken 0:00:12.062679
Number of used sentences in train = 231
Total loss for epoch 13: 345.665226
	Epoch 14....
Epoch has taken 0:00:12.265189
Number of used sentences in train = 231
Total loss for epoch 14: 345.466089
Epoch has taken 0:00:11.899243

==================================================================================================
	Training time : 0:33:10.549534
==================================================================================================
	Identification : 0.333

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3585, Test : 1320
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 122 (23 %)
	New MWEs : 388 (76 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(18050, 229)
  (lstm): LSTM(247, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=139, bias=True)
  (linear2): Linear(in_features=139, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 3226
Total loss for epoch 0: 18834.339350
validation loss after epoch 0 : 1595.352243
	Epoch 1....
Epoch has taken 0:03:37.882724
Number of used sentences in train = 3226
Total loss for epoch 1: 10679.511531
validation loss after epoch 1 : 1653.871577
	Epoch 2....
Epoch has taken 0:03:45.233122
Number of used sentences in train = 3226
Total loss for epoch 2: 8111.304592
validation loss after epoch 2 : 1814.187333
	Epoch 3....
Epoch has taken 0:03:45.252447
Number of used sentences in train = 3226
Total loss for epoch 3: 6929.624837
validation loss after epoch 3 : 1978.273699
	Epoch 4....
Epoch has taken 0:03:47.749729
Number of used sentences in train = 3226
Total loss for epoch 4: 6517.392550
validation loss after epoch 4 : 2131.544503
	Epoch 5....
Epoch has taken 0:03:53.999428
Number of used sentences in train = 3226
Total loss for epoch 5: 6326.806905
validation loss after epoch 5 : 2293.647562
	Epoch 6....
Epoch has taken 0:03:38.627124
Number of used sentences in train = 3226
Total loss for epoch 6: 6245.260538
validation loss after epoch 6 : 2348.939531
	Epoch 7....
Epoch has taken 0:03:47.314916
Number of used sentences in train = 3226
Total loss for epoch 7: 6207.218095
validation loss after epoch 7 : 2386.762483
	Epoch 8....
Epoch has taken 0:03:41.764045
Number of used sentences in train = 3226
Total loss for epoch 8: 6187.086693
validation loss after epoch 8 : 2466.618280
	Epoch 9....
Epoch has taken 0:03:53.989668
Number of used sentences in train = 3226
Total loss for epoch 9: 6174.050654
validation loss after epoch 9 : 2448.951656
	Epoch 10....
Epoch has taken 0:03:47.858473
Number of used sentences in train = 3226
Total loss for epoch 10: 6164.493065
validation loss after epoch 10 : 2513.469470
	Epoch 11....
Epoch has taken 0:03:48.467401
Number of used sentences in train = 3226
Total loss for epoch 11: 6158.090897
validation loss after epoch 11 : 2509.041219
	Epoch 12....
Epoch has taken 0:03:48.672555
Number of used sentences in train = 3226
Total loss for epoch 12: 6152.908720
validation loss after epoch 12 : 2509.540610
	Epoch 13....
Epoch has taken 0:03:45.756674
Number of used sentences in train = 3226
Total loss for epoch 13: 6146.446722
validation loss after epoch 13 : 2596.394933
	Epoch 14....
Epoch has taken 0:03:48.857198
Number of used sentences in train = 3226
Total loss for epoch 14: 6143.006097
validation loss after epoch 14 : 2605.806562
	TransitionClassifier(
  (p_embeddings): Embedding(13, 18)
  (w_embeddings): Embedding(18050, 229)
  (lstm): LSTM(247, 62, bidirectional=True)
  (linear1): Linear(in_features=992, out_features=139, bias=True)
  (linear2): Linear(in_features=139, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
Epoch has taken 0:03:47.863318
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 359
Total loss for epoch 0: 3275.618144
	Epoch 1....
Epoch has taken 0:00:22.401349
Number of used sentences in train = 359
Total loss for epoch 1: 962.528793
	Epoch 2....
Epoch has taken 0:00:22.298167
Number of used sentences in train = 359
Total loss for epoch 2: 733.777279
	Epoch 3....
Epoch has taken 0:00:22.230757
Number of used sentences in train = 359
Total loss for epoch 3: 684.879488
	Epoch 4....
Epoch has taken 0:00:22.331558
Number of used sentences in train = 359
Total loss for epoch 4: 675.482188
	Epoch 5....
Epoch has taken 0:00:22.231329
Number of used sentences in train = 359
Total loss for epoch 5: 673.532484
	Epoch 6....
Epoch has taken 0:00:22.516455
Number of used sentences in train = 359
Total loss for epoch 6: 672.715003
	Epoch 7....
Epoch has taken 0:00:22.406478
Number of used sentences in train = 359
Total loss for epoch 7: 672.133847
	Epoch 8....
Epoch has taken 0:00:22.592034
Number of used sentences in train = 359
Total loss for epoch 8: 671.721690
	Epoch 9....
Epoch has taken 0:00:22.534259
Number of used sentences in train = 359
Total loss for epoch 9: 671.362429
	Epoch 10....
Epoch has taken 0:00:22.233218
Number of used sentences in train = 359
Total loss for epoch 10: 671.144553
	Epoch 11....
Epoch has taken 0:00:22.294816
Number of used sentences in train = 359
Total loss for epoch 11: 670.987365
	Epoch 12....
Epoch has taken 0:00:22.252661
Number of used sentences in train = 359
Total loss for epoch 12: 670.846196
	Epoch 13....
Epoch has taken 0:00:22.247932
Number of used sentences in train = 359
Total loss for epoch 13: 670.761326
	Epoch 14....
Epoch has taken 0:00:22.220072
Number of used sentences in train = 359
Total loss for epoch 14: 670.654898
Epoch has taken 0:00:22.389466

==================================================================================================
	Training time : 1:02:15.194580
==================================================================================================
	Identification : 0.279

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
# Kiper conf: {'verbose': True, 'lstmLayerNum': 1, 'focusedElemNum': 8, 'epochs': 15, 'dense1': 29, 'file': 'kiperwasser.p', 'dense2': 0, 'earlyStop': False, 'moreTrans': False, 'lr': 0.07, 'posDim': 34, 'optimizer': 'adagrad', 'denseDropout': False, 'lstmUnitNum': 121, 'lstmDropout': 0.21, 'denseActivation': 'tanh', 'wordDim': 89, 'batch': 1}False,False
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.13
	Seen MWEs : 279 (41 %)
	New MWEs : 391 (58 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 34)
  (w_embeddings): Embedding(9277, 89)
  (lstm): LSTM(123, 121, bidirectional=True)
  (linear1): Linear(in_features=1936, out_features=29, bias=True)
  (linear2): Linear(in_features=29, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07
Number of used sentences in train = 2811
Total loss for epoch 0: 15766.666156
validation loss after epoch 0 : 1250.984874
	Epoch 1....
Epoch has taken 0:02:51.906265
Number of used sentences in train = 2811
Total loss for epoch 1: 10401.687071
validation loss after epoch 1 : 1091.313424
	Epoch 2....
Epoch has taken 0:02:51.803914
Number of used sentences in train = 2811
Total loss for epoch 2: 8066.045417
validation loss after epoch 2 : 1079.122884
	Epoch 3....
Epoch has taken 0:02:50.386717
Number of used sentences in train = 2811
Total loss for epoch 3: 6824.140609
validation loss after epoch 3 : 1109.117676
	Epoch 4....
Epoch has taken 0:02:50.210616
Number of used sentences in train = 2811
Total loss for epoch 4: 5969.363624
validation loss after epoch 4 : 1178.064267
	Epoch 5....
Epoch has taken 0:02:52.059473
