INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
	Language : BG
==================================================================================================
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/423.kiperwasser.p
Total loss for epoch 0: 12156.476794
validation loss after epoch 0 : 1058.017071
	Epoch 1....
validAcc: 0.804
Epoch has taken 0:02:54.622135
Total loss for epoch 1: 8501.295781
validation loss after epoch 1 : 813.090292
validAcc: 0.816
	Epoch 2....
Epoch has taken 0:02:50.825630
Total loss for epoch 2: 7446.065280
validation loss after epoch 2 : 767.665083
validAcc: 0.824
	Epoch 3....
Epoch has taken 0:02:48.878257
Total loss for epoch 3: 6874.296168
validation loss after epoch 3 : 688.247755
validAcc: 0.827
	Epoch 4....
Epoch has taken 0:02:47.145763
Total loss for epoch 4: 6416.813108
validation loss after epoch 4 : 697.860404
	Epoch 5....
validAcc: 0.82
Epoch has taken 0:02:47.056549
Total loss for epoch 5: 6088.450225
validation loss after epoch 5 : 768.559701
validAcc: 0.835
	Epoch 6....
Epoch has taken 0:02:49.936085
Total loss for epoch 6: 5810.400195
validation loss after epoch 6 : 592.760902
	Epoch 7....
validAcc: 0.063
Epoch has taken 0:02:48.653078
Total loss for epoch 7: 5598.776387
validation loss after epoch 7 : 350.254142
	Epoch 8....
validAcc: 0.828
Epoch has taken 0:02:46.771202
Total loss for epoch 8: 5432.760751
validation loss after epoch 8 : 577.996729
	Epoch 9....
validAcc: 0.827
Epoch has taken 0:02:50.583376
Total loss for epoch 9: 5322.124394
validation loss after epoch 9 : 568.478647
	Epoch 10....
validAcc: 0.819
Epoch has taken 0:02:49.011135
Total loss for epoch 10: 5242.594113
validation loss after epoch 10 : 561.778573
	Epoch 11....
validAcc: 0.053
Epoch has taken 0:02:49.281962
Total loss for epoch 11: 5166.348877
validation loss after epoch 11 : 323.224474
	Epoch 12....
validAcc: 0.799
Epoch has taken 0:02:47.780674
Total loss for epoch 12: 5143.682701
validation loss after epoch 12 : 550.175625
	Epoch 13....
validAcc: 0.027
Epoch has taken 0:02:51.608969
Total loss for epoch 13: 5105.170521
validation loss after epoch 13 : 308.069210
	Epoch 14....
validAcc: 0.111
Epoch has taken 0:03:05.843979
Total loss for epoch 14: 5073.965101
validation loss after epoch 14 : 327.612670
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
validAcc: 0.819
Epoch has taken 0:02:48.058123
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/0.kiperwasser.p
Number of used sentences in train = 313
Total loss for epoch 0: 1182.382578
	Epoch 1....
Epoch has taken 0:00:16.916009
Number of used sentences in train = 313
Total loss for epoch 1: 729.521353
	Epoch 2....
Epoch has taken 0:00:16.911470
Number of used sentences in train = 313
Total loss for epoch 2: 597.179125
	Epoch 3....
Epoch has taken 0:00:17.002546
Number of used sentences in train = 313
Total loss for epoch 3: 555.661164
	Epoch 4....
Epoch has taken 0:00:16.919184
Number of used sentences in train = 313
Total loss for epoch 4: 538.227837
	Epoch 5....
Epoch has taken 0:00:16.905087
Number of used sentences in train = 313
Total loss for epoch 5: 521.329337
	Epoch 6....
Epoch has taken 0:00:16.918741
Number of used sentences in train = 313
Total loss for epoch 6: 516.468541
	Epoch 7....
Epoch has taken 0:00:16.955781
Number of used sentences in train = 313
Total loss for epoch 7: 513.560089
	Epoch 8....
Epoch has taken 0:00:16.899331
Number of used sentences in train = 313
Total loss for epoch 8: 510.646516
	Epoch 9....
Epoch has taken 0:00:16.888712
Number of used sentences in train = 313
Total loss for epoch 9: 509.509326
	Epoch 10....
Epoch has taken 0:00:16.963896
Number of used sentences in train = 313
Total loss for epoch 10: 508.648613
	Epoch 11....
Epoch has taken 0:00:17.908275
Number of used sentences in train = 313
Total loss for epoch 11: 508.379928
	Epoch 12....
Epoch has taken 0:00:16.936183
Number of used sentences in train = 313
Total loss for epoch 12: 507.421843
	Epoch 13....
Epoch has taken 0:00:17.944349
Number of used sentences in train = 313
Total loss for epoch 13: 507.022967
	Epoch 14....
Epoch has taken 0:00:17.915378
Number of used sentences in train = 313
Total loss for epoch 14: 506.599729
Epoch has taken 0:00:17.096242

==================================================================================================
	Training time : 0:47:00.728984
==================================================================================================
	Identification : 0.148

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/137.kiperwasser.p
Total loss for epoch 0: 12428.844674
validation loss after epoch 0 : 1019.143800
	Epoch 1....
validAcc: 0.838
Epoch has taken 0:03:03.341062
Total loss for epoch 1: 8739.715469
validation loss after epoch 1 : 755.562495
	Epoch 2....
validAcc: 0.811
Epoch has taken 0:02:56.991625
Total loss for epoch 2: 7752.177290
validation loss after epoch 2 : 682.002602
	Epoch 3....
validAcc: 0.816
Epoch has taken 0:03:03.285197
Total loss for epoch 3: 7079.043945
validation loss after epoch 3 : 801.534482
	Epoch 4....
validAcc: 0.011
Epoch has taken 0:03:02.050098
Total loss for epoch 4: 6588.439509
validation loss after epoch 4 : 430.237248
	Epoch 5....
validAcc: 0.027
Epoch has taken 0:03:05.777332
Total loss for epoch 5: 6196.257862
validation loss after epoch 5 : 406.083659
	Epoch 6....
validAcc: 0.763
Epoch has taken 0:03:05.783368
Total loss for epoch 6: 6001.903694
validation loss after epoch 6 : 711.278818
	Epoch 7....
validAcc: 0.058
Epoch has taken 0:02:50.021588
Total loss for epoch 7: 5746.098288
validation loss after epoch 7 : 364.635234
	Epoch 8....
validAcc: 0.022
Epoch has taken 0:02:50.380496
Total loss for epoch 8: 5626.819241
validation loss after epoch 8 : 338.159078
	Epoch 9....
validAcc: 0.131
Epoch has taken 0:02:49.536257
Total loss for epoch 9: 5509.383226
validation loss after epoch 9 : 367.721111
	Epoch 10....
validAcc: 0.814
Epoch has taken 0:02:47.497886
Total loss for epoch 10: 5417.533812
validation loss after epoch 10 : 593.988118
	Epoch 11....
validAcc: 0.817
Epoch has taken 0:02:50.785391
Total loss for epoch 11: 5339.816057
validation loss after epoch 11 : 601.745571
	Epoch 12....
validAcc: 0.043
Epoch has taken 0:03:01.855515
Total loss for epoch 12: 5280.121313
validation loss after epoch 12 : 331.521473
	Epoch 13....
validAcc: 0.184
Epoch has taken 0:02:49.536178
Total loss for epoch 13: 5215.682704
validation loss after epoch 13 : 358.949275
	Epoch 14....
validAcc: 0.068
Epoch has taken 0:02:50.449350
Total loss for epoch 14: 5157.544557
validation loss after epoch 14 : 334.663776
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
validAcc: 0.108
Epoch has taken 0:02:48.154846
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/0.kiperwasser.p
Number of used sentences in train = 313
Total loss for epoch 0: 774.904829
	Epoch 1....
Epoch has taken 0:00:16.713171
Number of used sentences in train = 313
Total loss for epoch 1: 348.292906
	Epoch 2....
Epoch has taken 0:00:16.725141
Number of used sentences in train = 313
Total loss for epoch 2: 225.551850
	Epoch 3....
Epoch has taken 0:00:16.719775
Number of used sentences in train = 313
Total loss for epoch 3: 211.914210
	Epoch 4....
Epoch has taken 0:00:16.695446
Number of used sentences in train = 313
Total loss for epoch 4: 145.168601
	Epoch 5....
Epoch has taken 0:00:16.690547
Number of used sentences in train = 313
Total loss for epoch 5: 127.328433
	Epoch 6....
Epoch has taken 0:00:16.675915
Number of used sentences in train = 313
Total loss for epoch 6: 119.399466
	Epoch 7....
Epoch has taken 0:00:16.712859
Number of used sentences in train = 313
Total loss for epoch 7: 115.190159
	Epoch 8....
Epoch has taken 0:00:16.600548
Number of used sentences in train = 313
Total loss for epoch 8: 103.980152
	Epoch 9....
Epoch has taken 0:00:16.735199
Number of used sentences in train = 313
Total loss for epoch 9: 95.460414
	Epoch 10....
Epoch has taken 0:00:16.718519
Number of used sentences in train = 313
Total loss for epoch 10: 84.781415
	Epoch 11....
Epoch has taken 0:00:16.696706
Number of used sentences in train = 313
Total loss for epoch 11: 79.383669
	Epoch 12....
Epoch has taken 0:00:16.701260
Number of used sentences in train = 313
Total loss for epoch 12: 78.860584
	Epoch 13....
Epoch has taken 0:00:16.705519
Number of used sentences in train = 313
Total loss for epoch 13: 74.567556
	Epoch 14....
Epoch has taken 0:00:16.712356
Number of used sentences in train = 313
Total loss for epoch 14: 70.233012
Epoch has taken 0:00:16.706480

==================================================================================================
	Training time : 0:48:06.449438
==================================================================================================
	Identification : 0.012

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/146.kiperwasser.p
Total loss for epoch 0: 14061.250160
validation loss after epoch 0 : 1118.589245
	Epoch 1....
validAcc: 0.798
Epoch has taken 0:02:49.023888
Total loss for epoch 1: 9401.967258
validation loss after epoch 1 : 782.827266
validAcc: 0.823
	Epoch 2....
Epoch has taken 0:02:50.081530
Total loss for epoch 2: 8421.439370
validation loss after epoch 2 : 705.745718
	Epoch 3....
validAcc: 0.011
Epoch has taken 0:02:51.490802
Total loss for epoch 3: 7628.467465
validation loss after epoch 3 : 468.709624
	Epoch 4....
validAcc: 0.817
Epoch has taken 0:02:50.645789
Total loss for epoch 4: 7110.607381
validation loss after epoch 4 : 653.736284
	Epoch 5....
validAcc: 0.82
Epoch has taken 0:02:48.357073
Total loss for epoch 5: 6668.002078
validation loss after epoch 5 : 653.477880
	Epoch 6....
validAcc: 0.817
Epoch has taken 0:02:49.959732
Total loss for epoch 6: 6396.834944
validation loss after epoch 6 : 624.957460
	Epoch 7....
validAcc: 0.821
Epoch has taken 0:02:50.835563
Total loss for epoch 7: 6226.548719
validation loss after epoch 7 : 630.389145
	Epoch 8....
validAcc: 0.794
Epoch has taken 0:02:50.560363
Total loss for epoch 8: 6077.745286
validation loss after epoch 8 : 601.677142
	Epoch 9....
validAcc: 0.821
Epoch has taken 0:02:50.412663
Total loss for epoch 9: 5989.920227
validation loss after epoch 9 : 581.303830
	Epoch 10....
validAcc: 0.818
Epoch has taken 0:02:49.487862
Total loss for epoch 10: 5793.060527
validation loss after epoch 10 : 615.163054
	Epoch 11....
validAcc: 0.192
Epoch has taken 0:02:46.443396
Total loss for epoch 11: 5607.986906
validation loss after epoch 11 : 379.419855
	Epoch 12....
validAcc: 0.793
Epoch has taken 0:02:47.290369
Total loss for epoch 12: 5522.662061
validation loss after epoch 12 : 546.387905
	Epoch 13....
validAcc: 0.799
Epoch has taken 0:02:50.628313
Total loss for epoch 13: 5384.321551
validation loss after epoch 13 : 549.005430
	Epoch 14....
validAcc: 0.805
Epoch has taken 0:02:51.308782
Total loss for epoch 14: 5275.699070
validation loss after epoch 14 : 558.707845
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
validAcc: 0.8
Epoch has taken 0:02:45.610157
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/0.kiperwasser.p
Number of used sentences in train = 313
Total loss for epoch 0: 1562.895522
	Epoch 1....
Epoch has taken 0:00:16.523973
Number of used sentences in train = 313
Total loss for epoch 1: 749.557173
	Epoch 2....
Epoch has taken 0:00:16.503224
Number of used sentences in train = 313
Total loss for epoch 2: 648.959491
	Epoch 3....
Epoch has taken 0:00:16.502262
Number of used sentences in train = 313
Total loss for epoch 3: 570.699150
	Epoch 4....
Epoch has taken 0:00:16.497744
Number of used sentences in train = 313
Total loss for epoch 4: 541.096291
	Epoch 5....
Epoch has taken 0:00:16.490847
Number of used sentences in train = 313
Total loss for epoch 5: 526.918746
	Epoch 6....
Epoch has taken 0:00:16.482870
Number of used sentences in train = 313
Total loss for epoch 6: 518.440580
	Epoch 7....
Epoch has taken 0:00:16.486457
Number of used sentences in train = 313
Total loss for epoch 7: 510.312157
	Epoch 8....
Epoch has taken 0:00:16.475340
Number of used sentences in train = 313
Total loss for epoch 8: 504.704838
	Epoch 9....
Epoch has taken 0:00:16.486246
Number of used sentences in train = 313
Total loss for epoch 9: 501.231878
	Epoch 10....
Epoch has taken 0:00:16.490886
Number of used sentences in train = 313
Total loss for epoch 10: 499.110983
	Epoch 11....
Epoch has taken 0:00:16.481555
Number of used sentences in train = 313
Total loss for epoch 11: 498.599837
	Epoch 12....
Epoch has taken 0:00:16.505879
Number of used sentences in train = 313
Total loss for epoch 12: 497.767660
	Epoch 13....
Epoch has taken 0:00:16.516053
Number of used sentences in train = 313
Total loss for epoch 13: 497.381160
	Epoch 14....
Epoch has taken 0:00:16.529361
Number of used sentences in train = 313
Total loss for epoch 14: 496.873583
Epoch has taken 0:00:16.524066

==================================================================================================
	Training time : 0:46:30.125253
==================================================================================================
	Identification : 0.139

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/164.kiperwasser.p
Total loss for epoch 0: 13097.677463
validation loss after epoch 0 : 1050.065685
	Epoch 1....
validAcc: 0.81
Epoch has taken 0:02:47.190806
Total loss for epoch 1: 8936.660353
validation loss after epoch 1 : 890.719517
	Epoch 2....
validAcc: 0.022
Epoch has taken 0:02:46.605013
Total loss for epoch 2: 7919.387192
validation loss after epoch 2 : 509.107941
validAcc: 0.824
	Epoch 3....
Epoch has taken 0:02:44.233684
Total loss for epoch 3: 7162.920632
validation loss after epoch 3 : 743.683214
validAcc: 0.827
	Epoch 4....
Epoch has taken 0:02:46.046378
Total loss for epoch 4: 6589.482633
validation loss after epoch 4 : 714.868170
	Epoch 5....
validAcc: 0.032
Epoch has taken 0:02:44.376426
Total loss for epoch 5: 6226.140576
validation loss after epoch 5 : 383.497508
	Epoch 6....
validAcc: 0.011
Epoch has taken 0:02:44.072299
Total loss for epoch 6: 5961.705330
validation loss after epoch 6 : 334.301089
	Epoch 7....
validAcc: 0.083
Epoch has taken 0:02:45.740104
Total loss for epoch 7: 5785.082624
validation loss after epoch 7 : 350.708086
	Epoch 8....
validAcc: 0.812
Epoch has taken 0:02:45.653095
Total loss for epoch 8: 5647.999532
validation loss after epoch 8 : 552.575730
	Epoch 9....
validAcc: 0.811
Epoch has taken 0:02:49.158691
Total loss for epoch 9: 5516.303375
validation loss after epoch 9 : 550.217927
	Epoch 10....
validAcc: 0.824
Epoch has taken 0:02:50.785882
Total loss for epoch 10: 5433.251018
validation loss after epoch 10 : 533.140455
	Epoch 11....
validAcc: 0.818
Epoch has taken 0:02:50.776547
Total loss for epoch 11: 5341.754342
validation loss after epoch 11 : 531.067098
	Epoch 12....
validAcc: 0.814
Epoch has taken 0:02:50.609339
Total loss for epoch 12: 5265.682416
validation loss after epoch 12 : 536.238158
	Epoch 13....
validAcc: 0.103
Epoch has taken 0:03:07.264016
Total loss for epoch 13: 5210.174886
validation loss after epoch 13 : 321.246448
	Epoch 14....
validAcc: 0.821
Epoch has taken 0:03:06.384425
Total loss for epoch 14: 5179.900141
validation loss after epoch 14 : 530.480551
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
validAcc: 0.822
Epoch has taken 0:02:49.194157
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/0.kiperwasser.p
Number of used sentences in train = 313
Total loss for epoch 0: 1482.816379
	Epoch 1....
Epoch has taken 0:00:16.966790
Number of used sentences in train = 313
Total loss for epoch 1: 729.668204
	Epoch 2....
Epoch has taken 0:00:16.971310
Number of used sentences in train = 313
Total loss for epoch 2: 590.907724
	Epoch 3....
Epoch has taken 0:00:16.949375
Number of used sentences in train = 313
Total loss for epoch 3: 527.945838
	Epoch 4....
Epoch has taken 0:00:16.963517
Number of used sentences in train = 313
Total loss for epoch 4: 511.494124
	Epoch 5....
Epoch has taken 0:00:16.960746
Number of used sentences in train = 313
Total loss for epoch 5: 507.014049
	Epoch 6....
Epoch has taken 0:00:17.180061
Number of used sentences in train = 313
Total loss for epoch 6: 500.264432
	Epoch 7....
Epoch has taken 0:00:16.971822
Number of used sentences in train = 313
Total loss for epoch 7: 498.822912
	Epoch 8....
Epoch has taken 0:00:16.972381
Number of used sentences in train = 313
Total loss for epoch 8: 497.838926
	Epoch 9....
Epoch has taken 0:00:16.996136
Number of used sentences in train = 313
Total loss for epoch 9: 497.306857
	Epoch 10....
Epoch has taken 0:00:16.999122
Number of used sentences in train = 313
Total loss for epoch 10: 496.621081
	Epoch 11....
Epoch has taken 0:00:18.568636
Number of used sentences in train = 313
Total loss for epoch 11: 496.226706
	Epoch 12....
Epoch has taken 0:00:18.775877
Number of used sentences in train = 313
Total loss for epoch 12: 496.042897
	Epoch 13....
Epoch has taken 0:00:18.860654
Number of used sentences in train = 313
Total loss for epoch 13: 495.873801
	Epoch 14....
Epoch has taken 0:00:18.748666
Number of used sentences in train = 313
Total loss for epoch 14: 495.702767
Epoch has taken 0:00:18.762741

==================================================================================================
	Training time : 0:46:52.230125
==================================================================================================
	Identification : 0.148

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/323.kiperwasser.p
Total loss for epoch 0: 11985.089100
validation loss after epoch 0 : 1046.199434
	Epoch 1....
validAcc: 0.832
Epoch has taken 0:03:07.914114
Total loss for epoch 1: 8491.434672
validation loss after epoch 1 : 806.651872
	Epoch 2....
validAcc: 0.798
Epoch has taken 0:03:05.736235
Total loss for epoch 2: 7460.031270
validation loss after epoch 2 : 696.351364
	Epoch 3....
validAcc: 0.108
Epoch has taken 0:03:04.852590
Total loss for epoch 3: 6812.677104
validation loss after epoch 3 : 443.051984
	Epoch 4....
validAcc: 0.032
Epoch has taken 0:02:50.331503
Total loss for epoch 4: 6386.664028
validation loss after epoch 4 : 389.543163
	Epoch 5....
validAcc: 0.057
Epoch has taken 0:02:49.909454
Total loss for epoch 5: 6038.890170
validation loss after epoch 5 : 387.386510
	Epoch 6....
validAcc: 0.108
Epoch has taken 0:02:57.774955
Total loss for epoch 6: 5778.663135
validation loss after epoch 6 : 444.884193
	Epoch 7....
validAcc: 0.81
Epoch has taken 0:03:08.279230
Total loss for epoch 7: 5612.251072
validation loss after epoch 7 : 657.806788
	Epoch 8....
validAcc: 0.097
Epoch has taken 0:02:57.327471
Total loss for epoch 8: 5463.705534
validation loss after epoch 8 : 414.612236
	Epoch 9....
validAcc: 0.114
Epoch has taken 0:02:47.274194
Total loss for epoch 9: 5315.972004
validation loss after epoch 9 : 390.541322
	Epoch 10....
validAcc: 0.091
Epoch has taken 0:02:52.642453
Total loss for epoch 10: 5232.797709
validation loss after epoch 10 : 364.405036
	Epoch 11....
validAcc: 0.073
Epoch has taken 0:02:50.428598
Total loss for epoch 11: 5176.768299
validation loss after epoch 11 : 355.987584
	Epoch 12....
validAcc: 0.082
Epoch has taken 0:02:49.827415
Total loss for epoch 12: 5132.219640
validation loss after epoch 12 : 353.837604
	Epoch 13....
validAcc: 0.086
Epoch has taken 0:02:50.188764
Total loss for epoch 13: 5054.548680
validation loss after epoch 13 : 350.413958
	Epoch 14....
validAcc: 0.81
Epoch has taken 0:02:50.145650
Total loss for epoch 14: 5021.457856
validation loss after epoch 14 : 556.121416
	TransitionClassifier(
  (p_embeddings): Embedding(18, 41)
  (w_embeddings): Embedding(1177, 239)
  (lstm): LSTM(280, 90, bidirectional=True)
  (linear1): Linear(in_features=1440, out_features=11, bias=True)
  (linear2): Linear(in_features=11, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	NLLLoss()
==================================================================================================
	Epoch 0....
validAcc: 0.112
Epoch has taken 0:02:50.765453
# Network optimizer = Adagrad, learning rate = 0.07

/home/halsaied/NNIdenSys/tmp/0.kiperwasser.p
Number of used sentences in train = 313
Total loss for epoch 0: 755.299209
	Epoch 1....
Epoch has taken 0:00:16.771701
Number of used sentences in train = 313
Total loss for epoch 1: 388.258498
	Epoch 2....
Epoch has taken 0:00:16.778230
Number of used sentences in train = 313
Total loss for epoch 2: 228.029173
	Epoch 3....
Epoch has taken 0:00:16.765444
Number of used sentences in train = 313
Total loss for epoch 3: 141.933576
	Epoch 4....
Epoch has taken 0:00:16.751601
Number of used sentences in train = 313
Total loss for epoch 4: 122.872211
	Epoch 5....
Epoch has taken 0:00:16.774638
Number of used sentences in train = 313
Total loss for epoch 5: 104.374024
	Epoch 6....
Epoch has taken 0:00:16.784117
Number of used sentences in train = 313
Total loss for epoch 6: 91.555247
	Epoch 7....
Epoch has taken 0:00:16.784548
Number of used sentences in train = 313
Total loss for epoch 7: 83.726658
	Epoch 8....
Epoch has taken 0:00:16.787449
Number of used sentences in train = 313
Total loss for epoch 8: 80.159829
	Epoch 9....
Epoch has taken 0:00:16.792867
Number of used sentences in train = 313
Total loss for epoch 9: 73.858216
	Epoch 10....
Epoch has taken 0:00:16.781326
Number of used sentences in train = 313
Total loss for epoch 10: 71.927165
	Epoch 11....
Epoch has taken 0:00:16.765918
Number of used sentences in train = 313
Total loss for epoch 11: 69.765906
	Epoch 12....
Epoch has taken 0:00:16.777195
Number of used sentences in train = 313
Total loss for epoch 12: 68.316496
	Epoch 13....
Epoch has taken 0:00:16.764764
Number of used sentences in train = 313
Total loss for epoch 13: 67.349596
	Epoch 14....
Epoch has taken 0:00:16.777640
Number of used sentences in train = 313
Total loss for epoch 14: 67.862012
Epoch has taken 0:00:16.766031

==================================================================================================
	Training time : 0:48:05.513169
==================================================================================================
	Identification : 0.012

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
