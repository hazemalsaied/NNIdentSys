INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_e48O2L.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 3841/4043 Mb (0.950000) on cuda
Mapped name None to device cuda: GeForce GTX 980 (0000:03:00.0)
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp3uhz2w and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp4JTogh). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp62bd9a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp7WzhOI). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp1wbq89 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBCJQaR). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBRhjyL). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBRhjyL). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBRhjyL). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBRhjyL). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp2XNh7G and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBYRaL0). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp2ImG5Y and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpEEav3_). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp2ImG5Y and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpEEav3_). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp7WzhOI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpGYVcbr). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp7WzhOI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpGYVcbr). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp7WzhOI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpGYVcbr). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp65oLm2 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpGeVd_N). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp65oLm2 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpGeVd_N). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp65oLm2 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpGeVd_N). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp65oLm2 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpGeVd_N). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp3doMOA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpHF9ncl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp3doMOA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpHF9ncl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpCivAN6 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLzhazU). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpCivAN6 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpMf_v4n). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpB6CyU9 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpOdq3XZ). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpB6CyU9 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpOdq3XZ). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpB6CyU9 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpOdq3XZ). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpG_RBBX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpPwfSz_). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpRH7GqB and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpUArlaF). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpRH7GqB and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpUArlaF). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpUfRfWe). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpUfRfWe). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBRhjyL and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpUfRfWe). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpUfRfWe). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpUfRfWe). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpRH7GqB and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpW64UTx). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpRH7GqB and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpW64UTx). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLiGkip and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpWBA4jy). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLiGkip and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpWBA4jy). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8UaPFg and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpXgR2eV). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8UaPFg and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpXgR2eV). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpTkscOT and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpYveYsN). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp2ImG5Y and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpZR9370). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8UaPFg and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpZZ4HQb). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8UaPFg and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpZZ4HQb). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLiGkip and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpZZ4HQb). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLiGkip and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpZZ4HQb). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLreGyj and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpZhnR01). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp60z96m and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpZlrkwd). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp62bd9a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp_setR0). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp62bd9a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp_setR0). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp62bd9a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp_setR0). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp62bd9a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp_setR0). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp1RdjrX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp_setR0). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp62bd9a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp_setR0). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp1RdjrX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp_setR0). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpIunj8Q and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpdxezSl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpIunj8Q and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpdxezSl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBRhjyL and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgVkWZv). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgVkWZv). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBRhjyL and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgVkWZv). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgVkWZv). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgVkWZv). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpUfRfWe and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgVkWZv). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp8SKjCn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgVkWZv). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp62bd9a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgYUvOn). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp7WzhOI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgYUvOn). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp0h18hU and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgYUvOn). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp62bd9a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgYUvOn). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp7WzhOI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgYUvOn). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpIgQH0o and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpgkQy66). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpBYRaL0 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmphRSywy). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp4Zxnwn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpibqG0b). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpD7vlza and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpjX0TUL). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp9oNsf3 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpmiCAko). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpZhnR01 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpov4mEH). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmphRSywy and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmppkIoJO). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp45OJ60 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmppkIoJO). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpPJ2q6Q and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpq5aypJ). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp86K7Ma and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqtM4F9). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp86K7Ma and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqtM4F9). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLiGkip and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqtM4F9). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLiGkip and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpqtM4F9). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmphRSywy and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpsQFlBt). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpSKDn4a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmptIym6X). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpSKDn4a and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmptIym6X). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpLVepcV and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmptua1QG). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp4Zxnwn and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpuv6CJZ). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpfZdhYO and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpvUZa76). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpfZdhYO and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpvUZa76). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpMeIyYu and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpw9ilIg). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmp2XNh7G and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpxi2CuW). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpCivAN6 and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpxnrP3C). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpQ_vj_O and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpySP0Y4). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 20, True, True, False, True, True, 2, 64, False, True, True, relu, 0.25, 45, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 44, False, 5, 5, 358, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3601931
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 358)       3495154     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 44)        34188       input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 1432)         0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 176)          0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 1608)         0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 45)           72405       concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 45)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 4)            184         dropout_1[0][0]                  
==================================================================================================
Total params: 3,601,931
Trainable params: 3,601,931
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 20

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 54s - loss: 0.1783 - acc: 0.9633 - val_loss: 0.1330 - val_acc: 0.9738
Epoch 2/40
 - 54s - loss: 0.1339 - acc: 0.9737 - val_loss: 0.1233 - val_acc: 0.9765
Epoch 3/40
 - 54s - loss: 0.1263 - acc: 0.9756 - val_loss: 0.1212 - val_acc: 0.9773

==================================================================================================
	Training time : 0:03:44.181812
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.287946
==================================================================================================
	Identification : 0.734
	P, R  : 0.74, 0.729

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 20, True, True, False, True, True, 2, 64, False, True, True, relu, 0.12, 23, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 37, False, 5, 5, 340, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3382972
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 340)       3319420     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 37)        28749       input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 1360)         0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 148)          0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 1508)         0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 23)           34707       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 4)            96          dropout_2[0][0]                  
==================================================================================================
Total params: 3,382,972
Trainable params: 3,382,972
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 20

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 53s - loss: 0.1806 - acc: 0.9626 - val_loss: 0.1332 - val_acc: 0.9731
Epoch 2/40
 - 53s - loss: 0.1371 - acc: 0.9725 - val_loss: 0.1267 - val_acc: 0.9755
Epoch 3/40
 - 53s - loss: 0.1297 - acc: 0.9744 - val_loss: 0.1240 - val_acc: 0.9763

==================================================================================================
	Training time : 0:03:30.952051
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.448869
==================================================================================================
	Identification : 0.726
	P, R  : 0.742, 0.711

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 21, True, True, False, True, True, 2, 64, False, True, True, relu, 0.15, 58, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 18, False, 5, 5, 443, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4446241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 443)       4325009     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 18)        13986       input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 1772)         0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 72)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 1844)         0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 58)           107010      concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 58)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 4)            236         dropout_3[0][0]                  
==================================================================================================
Total params: 4,446,241
Trainable params: 4,446,241
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 60s - loss: 0.1751 - acc: 0.9649 - val_loss: 0.1293 - val_acc: 0.9748
Epoch 2/40
 - 60s - loss: 0.1292 - acc: 0.9752 - val_loss: 0.1214 - val_acc: 0.9770
Epoch 3/40
 - 60s - loss: 0.1227 - acc: 0.9767 - val_loss: 0.1200 - val_acc: 0.9777

==================================================================================================
	Training time : 0:03:53.424140
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.275944
==================================================================================================
	Identification : 0.744
	P, R  : 0.737, 0.751

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 15, True, True, False, True, True, 2, 64, False, True, True, relu, 0.18, 115, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 47, False, 5, 5, 392, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4066134
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 392)       3827096     input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 47)        36519       input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 1568)         0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 188)          0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 1756)         0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 115)          202055      concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 115)          0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 4)            464         dropout_4[0][0]                  
==================================================================================================
Total params: 4,066,134
Trainable params: 4,066,134
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 57s - loss: 0.1595 - acc: 0.9669 - val_loss: 0.1193 - val_acc: 0.9760
Epoch 2/40
 - 57s - loss: 0.1194 - acc: 0.9762 - val_loss: 0.1136 - val_acc: 0.9776
Epoch 3/40
 - 57s - loss: 0.1131 - acc: 0.9773 - val_loss: 0.1113 - val_acc: 0.9783

==================================================================================================
	Training time : 0:03:42.653377
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.264602
==================================================================================================
	Identification : 0.737
	P, R  : 0.751, 0.723

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 24, True, True, False, True, True, 2, 64, False, False, True, relu, 0.35, 95, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 132, False, 5, 5, 449, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 7378062
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 449)       7054239     input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 132)       102564      input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 1796)         0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 528)          0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 2324)         0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 95)           220875      concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 95)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 4)            384         dropout_5[0][0]                  
==================================================================================================
Total params: 7,378,062
Trainable params: 7,378,062
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 24

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 75s - loss: 0.1937 - acc: 0.9612 - val_loss: 0.1376 - val_acc: 0.9729
Epoch 2/40
 - 75s - loss: 0.1394 - acc: 0.9731 - val_loss: 0.1308 - val_acc: 0.9752
Epoch 3/40
 - 75s - loss: 0.1306 - acc: 0.9751 - val_loss: 0.1277 - val_acc: 0.9767

==================================================================================================
	Training time : 0:04:45.565606
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.708595
==================================================================================================
	Identification : 0.714
	P, R  : 0.742, 0.687

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 23, True, True, False, True, True, 2, 64, False, True, True, relu, 0.22, 195, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 89, False, 5, 5, 418, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4546526
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 418)       4080934     input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 89)        69153       input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 1672)         0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 356)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 2028)         0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 195)          395655      concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 195)          0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 4)            784         dropout_6[0][0]                  
==================================================================================================
Total params: 4,546,526
Trainable params: 4,546,526
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 59s - loss: 0.1761 - acc: 0.9657 - val_loss: 0.1290 - val_acc: 0.9763
Epoch 2/40
 - 59s - loss: 0.1282 - acc: 0.9760 - val_loss: 0.1237 - val_acc: 0.9775
Epoch 3/40
 - 59s - loss: 0.1216 - acc: 0.9773 - val_loss: 0.1200 - val_acc: 0.9780

==================================================================================================
	Training time : 0:03:55.095405
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.290265
==================================================================================================
	Identification : 0.732
	P, R  : 0.759, 0.707

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 16, True, True, False, True, True, 2, 64, False, True, True, relu, 0.23, 186, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 119, False, 5, 5, 445, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4857548
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 445)       4344535     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 119)       92463       input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 1780)         0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 476)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 2256)         0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 186)          419802      concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 186)          0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 4)            748         dropout_7[0][0]                  
==================================================================================================
Total params: 4,857,548
Trainable params: 4,857,548
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 60s - loss: 0.1635 - acc: 0.9668 - val_loss: 0.1204 - val_acc: 0.9765
Epoch 2/40
 - 60s - loss: 0.1202 - acc: 0.9763 - val_loss: 0.1140 - val_acc: 0.9780
Epoch 3/40
 - 60s - loss: 0.1138 - acc: 0.9776 - val_loss: 0.1121 - val_acc: 0.9785

==================================================================================================
	Training time : 0:03:55.569038
==================================================================================================

==================================================================================================
	Parsing time : 0:00:24.992314
==================================================================================================
	Identification : 0.741
	P, R  : 0.744, 0.738

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 13, True, True, False, True, True, 2, 64, False, False, True, relu, 0.34, 193, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 43, False, 5, 5, 203, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 3413625
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 203)       3189333     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 43)        33411       input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 812)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 172)          0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 984)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 193)          190105      concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 193)          0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 4)            776         dropout_8[0][0]                  
==================================================================================================
Total params: 3,413,625
Trainable params: 3,413,625
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 13

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 52s - loss: 0.1557 - acc: 0.9661 - val_loss: 0.1169 - val_acc: 0.9757
Epoch 2/40
 - 52s - loss: 0.1148 - acc: 0.9762 - val_loss: 0.1107 - val_acc: 0.9775
Epoch 3/40
 - 52s - loss: 0.1084 - acc: 0.9775 - val_loss: 0.1096 - val_acc: 0.9778

==================================================================================================
	Training time : 0:03:29.517498
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.131940
==================================================================================================
	Identification : 0.726
	P, R  : 0.759, 0.696

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 22, True, True, False, True, True, 2, 64, False, False, True, relu, 0.3, 24, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 52, False, 5, 5, 165, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 2653675
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 165)       2592315     input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 52)        40404       input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 660)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 208)          0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 868)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 24)           20856       concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 24)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 4)            100         dropout_9[0][0]                  
==================================================================================================
Total params: 2,653,675
Trainable params: 2,653,675
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 22

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 49s - loss: 0.2123 - acc: 0.9549 - val_loss: 0.1466 - val_acc: 0.9689
Epoch 2/40
 - 49s - loss: 0.1576 - acc: 0.9669 - val_loss: 0.1390 - val_acc: 0.9713
Epoch 3/40
 - 49s - loss: 0.1481 - acc: 0.9691 - val_loss: 0.1364 - val_acc: 0.9730

==================================================================================================
	Training time : 0:03:16.192227
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.401156
==================================================================================================
	Identification : 0.678
	P, R  : 0.737, 0.628

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 2, True, True, False, True, True, 2, 64, False, True, True, relu, 0.22, 59, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 62, False, 5, 5, 468, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4742637
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 468)       4569084     input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 62)        48174       input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 1872)         0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 248)          0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 2120)         0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 59)           125139      concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 59)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 4)            240         dropout_10[0][0]                 
==================================================================================================
Total params: 4,742,637
Trainable params: 4,742,637
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 2

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 60s - loss: 0.1102 - acc: 0.9678 - val_loss: 0.0843 - val_acc: 0.9758
Epoch 2/40
 - 60s - loss: 0.0835 - acc: 0.9757 - val_loss: 0.0783 - val_acc: 0.9776
Epoch 3/40
 - 60s - loss: 0.0786 - acc: 0.9769 - val_loss: 0.0769 - val_acc: 0.9781

==================================================================================================
	Training time : 0:03:53.538285
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.470032
==================================================================================================
	Identification : 0.761
	P, R  : 0.739, 0.785

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '11889')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/lock_dir
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 24, True, True, False, True, True, 2, 64, False, False, True, relu, 0.12, 96, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 36, False, 5, 5, 250, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 4066030
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 250)       3927750     input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 36)        27972       input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 1000)         0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 144)          0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 1144)         0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 96)           109920      concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 96)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 4)            388         dropout_11[0][0]                 
==================================================================================================
Total params: 4,066,030
Trainable params: 4,066,030
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 24

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 57s - loss: 0.1699 - acc: 0.9661 - val_loss: 0.1279 - val_acc: 0.9760
Epoch 2/40
 - 57s - loss: 0.1251 - acc: 0.9765 - val_loss: 0.1239 - val_acc: 0.9771
Epoch 3/40
 - 57s - loss: 0.1193 - acc: 0.9777 - val_loss: 0.1213 - val_acc: 0.9779

==================================================================================================
	Training time : 0:04:00.644295
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.682702
==================================================================================================
	Identification : 0.727
	P, R  : 0.761, 0.696

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 21, True, True, False, True, True, 2, 64, False, True, True, relu, 0.14, 78, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 37, False, 5, 5, 171, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 1763512
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 171)       1669473     input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 37)        28749       input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 684)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 148)          0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 832)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 78)           64974       concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 78)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 4)            316         dropout_12[0][0]                 
==================================================================================================
Total params: 1,763,512
Trainable params: 1,763,512
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 44s - loss: 0.1672 - acc: 0.9659 - val_loss: 0.1280 - val_acc: 0.9752
Epoch 2/40
 - 44s - loss: 0.1273 - acc: 0.9756 - val_loss: 0.1201 - val_acc: 0.9772
Epoch 3/40
 - 44s - loss: 0.1210 - acc: 0.9771 - val_loss: 0.1187 - val_acc: 0.9779

==================================================================================================
	Training time : 0:02:59.921084
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.577400
==================================================================================================
	Identification : 0.732
	P, R  : 0.764, 0.702

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 18, True, True, False, True, True, 2, 64, False, True, True, relu, 0.32, 91, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 108, False, 5, 5, 389, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4063090
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 389)       3797807     input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 108)       83916       input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 1556)         0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 432)          0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 1988)         0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 91)           180999      concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 91)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 4)            368         dropout_13[0][0]                 
==================================================================================================
Total params: 4,063,090
Trainable params: 4,063,090
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 18

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 58s - loss: 0.1775 - acc: 0.9640 - val_loss: 0.1287 - val_acc: 0.9749
Epoch 2/40
 - 56s - loss: 0.1310 - acc: 0.9744 - val_loss: 0.1216 - val_acc: 0.9767
Epoch 3/40
 - 56s - loss: 0.1234 - acc: 0.9761 - val_loss: 0.1191 - val_acc: 0.9775

==================================================================================================
	Training time : 0:03:43.010430
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.730333
==================================================================================================
	Identification : 0.729
	P, R  : 0.758, 0.703

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 21, True, True, False, True, True, 2, 64, False, True, True, relu, 0.12, 18, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 23, False, 5, 5, 389, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3845436
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 389)       3797807     input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 23)        17871       input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 1556)         0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 92)           0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 1648)         0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 18)           29682       concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 18)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 4)            76          dropout_14[0][0]                 
==================================================================================================
Total params: 3,845,436
Trainable params: 3,845,436
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 60s - loss: 0.1894 - acc: 0.9609 - val_loss: 0.1392 - val_acc: 0.9714
Epoch 2/40
 - 60s - loss: 0.1420 - acc: 0.9719 - val_loss: 0.1306 - val_acc: 0.9751
Epoch 3/40
 - 60s - loss: 0.1350 - acc: 0.9738 - val_loss: 0.1279 - val_acc: 0.9758

==================================================================================================
	Training time : 0:03:55.467976
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.418596
==================================================================================================
	Identification : 0.738
	P, R  : 0.738, 0.738

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 19, True, True, False, True, True, 2, 64, False, False, True, relu, 0.27, 122, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 125, False, 5, 5, 412, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 6832727
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 412)       6472932     input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 125)       97125       input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 1648)         0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 500)          0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 2148)         0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 122)          262178      concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 122)          0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 4)            492         dropout_15[0][0]                 
==================================================================================================
Total params: 6,832,727
Trainable params: 6,832,727
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 19

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 72s - loss: 0.1719 - acc: 0.9648 - val_loss: 0.1259 - val_acc: 0.9754
Epoch 2/40
 - 72s - loss: 0.1247 - acc: 0.9758 - val_loss: 0.1191 - val_acc: 0.9773
Epoch 3/40
 - 72s - loss: 0.1180 - acc: 0.9772 - val_loss: 0.1173 - val_acc: 0.9777

==================================================================================================
	Training time : 0:04:33.122208
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.387573
==================================================================================================
	Identification : 0.731
	P, R  : 0.748, 0.716

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 23, True, True, False, True, True, 2, 64, False, True, True, relu, 0.27, 149, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 67, False, 5, 5, 310, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3304030
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 310)       3026530     input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 67)        52059       input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 1240)         0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 268)          0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 1508)         0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 149)          224841      concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 149)          0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 4)            600         dropout_16[0][0]                 
==================================================================================================
Total params: 3,304,030
Trainable params: 3,304,030
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 53s - loss: 0.1798 - acc: 0.9647 - val_loss: 0.1309 - val_acc: 0.9750
Epoch 2/40
 - 53s - loss: 0.1319 - acc: 0.9752 - val_loss: 0.1237 - val_acc: 0.9770
Epoch 3/40
 - 53s - loss: 0.1248 - acc: 0.9767 - val_loss: 0.1239 - val_acc: 0.9771

==================================================================================================
	Training time : 0:03:32.300466
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.376867
==================================================================================================
	Identification : 0.734
	P, R  : 0.739, 0.73

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 18, True, True, False, True, True, 2, 64, False, True, True, relu, 0.38, 179, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 91, False, 5, 5, 260, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 2861302
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 260)       2538380     input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 91)        70707       input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 1040)         0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 364)          0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 1404)         0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 179)          251495      concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 179)          0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 4)            720         dropout_17[0][0]                 
==================================================================================================
Total params: 2,861,302
Trainable params: 2,861,302
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 18

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 49s - loss: 0.1730 - acc: 0.9646 - val_loss: 0.1265 - val_acc: 0.9750
Epoch 2/40
 - 49s - loss: 0.1275 - acc: 0.9750 - val_loss: 0.1180 - val_acc: 0.9774
Epoch 3/40
 - 49s - loss: 0.1202 - acc: 0.9766 - val_loss: 0.1163 - val_acc: 0.9780

==================================================================================================
	Training time : 0:03:23.804426
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.126722
==================================================================================================
	Identification : 0.741
	P, R  : 0.765, 0.719

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '11889')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '14036' (I am process '11889')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '11889')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '11889')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/lock_dir
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 22, True, True, False, True, True, 2, 64, False, False, True, relu, 0.22, 118, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 71, False, 5, 5, 480, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 7857113
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 480)       7541280     input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 71)        55167       input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 1920)         0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 284)          0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 2204)         0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 118)          260190      concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 118)          0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 4)            476         dropout_18[0][0]                 
==================================================================================================
Total params: 7,857,113
Trainable params: 7,857,113
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 22

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 78s - loss: 0.1772 - acc: 0.9647 - val_loss: 0.1290 - val_acc: 0.9751
Epoch 2/40
 - 78s - loss: 0.1282 - acc: 0.9756 - val_loss: 0.1228 - val_acc: 0.9771
Epoch 3/40
 - 78s - loss: 0.1207 - acc: 0.9771 - val_loss: 0.1208 - val_acc: 0.9776

==================================================================================================
	Training time : 0:05:48.133695
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.112758
==================================================================================================
	Identification : 0.725
	P, R  : 0.756, 0.696

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 21, True, True, False, True, True, 2, 64, False, True, True, relu, 0.21, 120, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 65, False, 5, 5, 359, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3759546
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 359)       3504917     input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 65)        50505       input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 1436)         0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 260)          0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 1696)         0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 120)          203640      concatenate_19[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 120)          0           dense_37[0][0]                   
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 4)            484         dropout_19[0][0]                 
==================================================================================================
Total params: 3,759,546
Trainable params: 3,759,546
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 54s - loss: 0.1699 - acc: 0.9666 - val_loss: 0.1257 - val_acc: 0.9758
Epoch 2/40
 - 54s - loss: 0.1263 - acc: 0.9761 - val_loss: 0.1196 - val_acc: 0.9775
Epoch 3/40
 - 54s - loss: 0.1200 - acc: 0.9773 - val_loss: 0.1197 - val_acc: 0.9777

==================================================================================================
	Training time : 0:03:37.577195
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.400505
==================================================================================================
	Identification : 0.738
	P, R  : 0.775, 0.705

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 7, True, True, False, True, True, 2, 64, False, False, True, relu, 0.39, 50, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 97, False, 5, 5, 468, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 7541371
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_39 (Embedding)        (None, 4, 468)       7352748     input_39[0][0]                   
__________________________________________________________________________________________________
embedding_40 (Embedding)        (None, 4, 97)        75369       input_40[0][0]                   
__________________________________________________________________________________________________
flatten_39 (Flatten)            (None, 1872)         0           embedding_39[0][0]               
__________________________________________________________________________________________________
flatten_40 (Flatten)            (None, 388)          0           embedding_40[0][0]               
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 2260)         0           flatten_39[0][0]                 
                                                                 flatten_40[0][0]                 
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 50)           113050      concatenate_20[0][0]             
__________________________________________________________________________________________________
dropout_20 (Dropout)            (None, 50)           0           dense_39[0][0]                   
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 4)            204         dropout_20[0][0]                 
==================================================================================================
Total params: 7,541,371
Trainable params: 7,541,371
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 76s - loss: 0.1579 - acc: 0.9620 - val_loss: 0.1152 - val_acc: 0.9725
Epoch 2/40
 - 76s - loss: 0.1168 - acc: 0.9726 - val_loss: 0.1068 - val_acc: 0.9753
Epoch 3/40
 - 76s - loss: 0.1096 - acc: 0.9747 - val_loss: 0.1041 - val_acc: 0.9766

==================================================================================================
	Training time : 0:04:50.049663
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.513689
==================================================================================================
	Identification : 0.73
	P, R  : 0.752, 0.708

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 9, True, True, False, True, True, 2, 64, False, True, True, relu, 0.21, 68, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 139, False, 5, 5, 361, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3768790
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_41 (Embedding)        (None, 4, 361)       3524443     input_41[0][0]                   
__________________________________________________________________________________________________
embedding_42 (Embedding)        (None, 4, 139)       108003      input_42[0][0]                   
__________________________________________________________________________________________________
flatten_41 (Flatten)            (None, 1444)         0           embedding_41[0][0]               
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 556)          0           embedding_42[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 2000)         0           flatten_41[0][0]                 
                                                                 flatten_42[0][0]                 
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 68)           136068      concatenate_21[0][0]             
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 68)           0           dense_41[0][0]                   
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 4)            276         dropout_21[0][0]                 
==================================================================================================
Total params: 3,768,790
Trainable params: 3,768,790
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 9

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 56s - loss: 0.1502 - acc: 0.9664 - val_loss: 0.1136 - val_acc: 0.9753
Epoch 2/40
 - 56s - loss: 0.1139 - acc: 0.9750 - val_loss: 0.1066 - val_acc: 0.9768
Epoch 3/40
 - 56s - loss: 0.1082 - acc: 0.9765 - val_loss: 0.1051 - val_acc: 0.9778

==================================================================================================
	Training time : 0:03:42.119908
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.461876
==================================================================================================
	Identification : 0.728
	P, R  : 0.756, 0.702

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 16, True, True, False, True, True, 2, 64, False, True, True, relu, 0.38, 166, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 96, False, 5, 5, 195, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 2172435
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 4, 195)       1903785     input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 4, 96)        74592       input_44[0][0]                   
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 780)          0           embedding_43[0][0]               
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 384)          0           embedding_44[0][0]               
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 1164)         0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 166)          193390      concatenate_22[0][0]             
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 166)          0           dense_43[0][0]                   
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 4)            668         dropout_22[0][0]                 
==================================================================================================
Total params: 2,172,435
Trainable params: 2,172,435
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 46s - loss: 0.1665 - acc: 0.9655 - val_loss: 0.1239 - val_acc: 0.9753
Epoch 2/40
 - 46s - loss: 0.1246 - acc: 0.9754 - val_loss: 0.1167 - val_acc: 0.9773
Epoch 3/40
 - 46s - loss: 0.1176 - acc: 0.9769 - val_loss: 0.1145 - val_acc: 0.9780

==================================================================================================
	Training time : 0:03:05.884277
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.542241
==================================================================================================
	Identification : 0.73
	P, R  : 0.761, 0.702

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 9, True, True, False, True, True, 2, 64, False, True, True, relu, 0.35, 78, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 115, False, 5, 5, 222, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 2362279
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_45 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 4, 222)       2167386     input_45[0][0]                   
__________________________________________________________________________________________________
embedding_46 (Embedding)        (None, 4, 115)       89355       input_46[0][0]                   
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 888)          0           embedding_45[0][0]               
__________________________________________________________________________________________________
flatten_46 (Flatten)            (None, 460)          0           embedding_46[0][0]               
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 1348)         0           flatten_45[0][0]                 
                                                                 flatten_46[0][0]                 
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 78)           105222      concatenate_23[0][0]             
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 78)           0           dense_45[0][0]                   
__________________________________________________________________________________________________
dense_46 (Dense)                (None, 4)            316         dropout_23[0][0]                 
==================================================================================================
Total params: 2,362,279
Trainable params: 2,362,279
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 9

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 47s - loss: 0.1542 - acc: 0.9651 - val_loss: 0.1143 - val_acc: 0.9748
Epoch 2/40
 - 46s - loss: 0.1165 - acc: 0.9742 - val_loss: 0.1080 - val_acc: 0.9765
Epoch 3/40
 - 46s - loss: 0.1103 - acc: 0.9758 - val_loss: 0.1058 - val_acc: 0.9776

==================================================================================================
	Training time : 0:03:08.431092
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.589021
==================================================================================================
	Identification : 0.732
	P, R  : 0.761, 0.706

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 9, True, True, False, True, True, 2, 64, False, False, True, relu, 0.36, 15, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 38, False, 5, 5, 264, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 4195429
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_47 (Embedding)        (None, 4, 264)       4147704     input_47[0][0]                   
__________________________________________________________________________________________________
embedding_48 (Embedding)        (None, 4, 38)        29526       input_48[0][0]                   
__________________________________________________________________________________________________
flatten_47 (Flatten)            (None, 1056)         0           embedding_47[0][0]               
__________________________________________________________________________________________________
flatten_48 (Flatten)            (None, 152)          0           embedding_48[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 1208)         0           flatten_47[0][0]                 
                                                                 flatten_48[0][0]                 
__________________________________________________________________________________________________
dense_47 (Dense)                (None, 15)           18135       concatenate_24[0][0]             
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 15)           0           dense_47[0][0]                   
__________________________________________________________________________________________________
dense_48 (Dense)                (None, 4)            64          dropout_24[0][0]                 
==================================================================================================
Total params: 4,195,429
Trainable params: 4,195,429
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 9

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 59s - loss: 0.3707 - acc: 0.9036 - val_loss: 0.1379 - val_acc: 0.9664
Epoch 2/40
 - 59s - loss: 0.3217 - acc: 0.9018 - val_loss: 0.1325 - val_acc: 0.9674
Epoch 3/40
 - 59s - loss: 0.3112 - acc: 0.9058 - val_loss: 0.1313 - val_acc: 0.9681

==================================================================================================
	Training time : 0:03:50.049047
==================================================================================================

==================================================================================================
	Parsing time : 0:00:26.855621
==================================================================================================
	Identification : 0.592
	P, R  : 0.748, 0.49

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 21, True, True, False, True, True, 2, 64, False, False, True, relu, 0.39, 199, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 61, False, 5, 5, 499, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 8333945
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_49 (Embedding)        (None, 4, 499)       7839789     input_49[0][0]                   
__________________________________________________________________________________________________
embedding_50 (Embedding)        (None, 4, 61)        47397       input_50[0][0]                   
__________________________________________________________________________________________________
flatten_49 (Flatten)            (None, 1996)         0           embedding_49[0][0]               
__________________________________________________________________________________________________
flatten_50 (Flatten)            (None, 244)          0           embedding_50[0][0]               
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 2240)         0           flatten_49[0][0]                 
                                                                 flatten_50[0][0]                 
__________________________________________________________________________________________________
dense_49 (Dense)                (None, 199)          445959      concatenate_25[0][0]             
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 199)          0           dense_49[0][0]                   
__________________________________________________________________________________________________
dense_50 (Dense)                (None, 4)            800         dropout_25[0][0]                 
==================================================================================================
Total params: 8,333,945
Trainable params: 8,333,945
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 21

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 80s - loss: 0.1835 - acc: 0.9626 - val_loss: 0.1300 - val_acc: 0.9748
Epoch 2/40
 - 80s - loss: 0.1290 - acc: 0.9750 - val_loss: 0.1223 - val_acc: 0.9767
Epoch 3/40
 - 80s - loss: 0.1212 - acc: 0.9766 - val_loss: 0.1201 - val_acc: 0.9777

==================================================================================================
	Training time : 0:05:00.199418
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.783591
==================================================================================================
	Identification : 0.726
	P, R  : 0.752, 0.702

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 7, True, True, False, True, True, 2, 64, False, True, True, relu, 0.22, 85, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 40, False, 5, 5, 473, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4823828
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_51 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_51 (Embedding)        (None, 4, 473)       4617899     input_51[0][0]                   
__________________________________________________________________________________________________
embedding_52 (Embedding)        (None, 4, 40)        31080       input_52[0][0]                   
__________________________________________________________________________________________________
flatten_51 (Flatten)            (None, 1892)         0           embedding_51[0][0]               
__________________________________________________________________________________________________
flatten_52 (Flatten)            (None, 160)          0           embedding_52[0][0]               
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 2052)         0           flatten_51[0][0]                 
                                                                 flatten_52[0][0]                 
__________________________________________________________________________________________________
dense_51 (Dense)                (None, 85)           174505      concatenate_26[0][0]             
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 85)           0           dense_51[0][0]                   
__________________________________________________________________________________________________
dense_52 (Dense)                (None, 4)            344         dropout_26[0][0]                 
==================================================================================================
Total params: 4,823,828
Trainable params: 4,823,828
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 61s - loss: 0.1397 - acc: 0.9676 - val_loss: 0.1051 - val_acc: 0.9763
Epoch 2/40
 - 61s - loss: 0.1048 - acc: 0.9761 - val_loss: 0.0996 - val_acc: 0.9778
Epoch 3/40
 - 61s - loss: 0.0994 - acc: 0.9772 - val_loss: 0.0983 - val_acc: 0.9782

==================================================================================================
	Training time : 0:03:57.833977
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.519008
==================================================================================================
	Identification : 0.748
	P, R  : 0.735, 0.761

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 6, True, True, False, True, True, 2, 64, False, True, True, relu, 0.31, 72, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 37, False, 5, 5, 322, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3276191
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_53 (Embedding)        (None, 4, 322)       3143686     input_53[0][0]                   
__________________________________________________________________________________________________
embedding_54 (Embedding)        (None, 4, 37)        28749       input_54[0][0]                   
__________________________________________________________________________________________________
flatten_53 (Flatten)            (None, 1288)         0           embedding_53[0][0]               
__________________________________________________________________________________________________
flatten_54 (Flatten)            (None, 148)          0           embedding_54[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 1436)         0           flatten_53[0][0]                 
                                                                 flatten_54[0][0]                 
__________________________________________________________________________________________________
dense_53 (Dense)                (None, 72)           103464      concatenate_27[0][0]             
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 72)           0           dense_53[0][0]                   
__________________________________________________________________________________________________
dense_54 (Dense)                (None, 4)            292         dropout_27[0][0]                 
==================================================================================================
Total params: 3,276,191
Trainable params: 3,276,191
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 53s - loss: 0.1445 - acc: 0.9650 - val_loss: 0.1083 - val_acc: 0.9743
Epoch 2/40
 - 53s - loss: 0.1084 - acc: 0.9743 - val_loss: 0.1009 - val_acc: 0.9767
Epoch 3/40
 - 53s - loss: 0.1022 - acc: 0.9760 - val_loss: 0.0988 - val_acc: 0.9775

==================================================================================================
	Training time : 0:03:32.514441
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.672840
==================================================================================================
	Identification : 0.734
	P, R  : 0.75, 0.72

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 5, True, True, False, True, True, 2, 64, False, True, True, relu, 0.34, 30, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 80, False, 5, 5, 469, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4707041
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_55 (Embedding)        (None, 4, 469)       4578847     input_55[0][0]                   
__________________________________________________________________________________________________
embedding_56 (Embedding)        (None, 4, 80)        62160       input_56[0][0]                   
__________________________________________________________________________________________________
flatten_55 (Flatten)            (None, 1876)         0           embedding_55[0][0]               
__________________________________________________________________________________________________
flatten_56 (Flatten)            (None, 320)          0           embedding_56[0][0]               
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 2196)         0           flatten_55[0][0]                 
                                                                 flatten_56[0][0]                 
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 30)           65910       concatenate_28[0][0]             
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 30)           0           dense_55[0][0]                   
__________________________________________________________________________________________________
dense_56 (Dense)                (None, 4)            124         dropout_28[0][0]                 
==================================================================================================
Total params: 4,707,041
Trainable params: 4,707,041
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 60s - loss: 0.1513 - acc: 0.9618 - val_loss: 0.1099 - val_acc: 0.9724
Epoch 2/40
 - 60s - loss: 0.1155 - acc: 0.9712 - val_loss: 0.1028 - val_acc: 0.9754
Epoch 3/40
 - 60s - loss: 0.1087 - acc: 0.9734 - val_loss: 0.0994 - val_acc: 0.9762

==================================================================================================
	Training time : 0:03:57.785710
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.647235
==================================================================================================
	Identification : 0.741
	P, R  : 0.761, 0.723

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 8, True, True, False, True, True, 2, 64, False, False, True, relu, 0.38, 116, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 64, False, 5, 5, 192, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 3185608
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_57 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_58 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_57 (Embedding)        (None, 4, 192)       3016512     input_57[0][0]                   
__________________________________________________________________________________________________
embedding_58 (Embedding)        (None, 4, 64)        49728       input_58[0][0]                   
__________________________________________________________________________________________________
flatten_57 (Flatten)            (None, 768)          0           embedding_57[0][0]               
__________________________________________________________________________________________________
flatten_58 (Flatten)            (None, 256)          0           embedding_58[0][0]               
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 1024)         0           flatten_57[0][0]                 
                                                                 flatten_58[0][0]                 
__________________________________________________________________________________________________
dense_57 (Dense)                (None, 116)          118900      concatenate_29[0][0]             
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 116)          0           dense_57[0][0]                   
__________________________________________________________________________________________________
dense_58 (Dense)                (None, 4)            468         dropout_29[0][0]                 
==================================================================================================
Total params: 3,185,608
Trainable params: 3,185,608
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 8

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 52s - loss: 0.1457 - acc: 0.9657 - val_loss: 0.1089 - val_acc: 0.9753
Epoch 2/40
 - 52s - loss: 0.1085 - acc: 0.9754 - val_loss: 0.1039 - val_acc: 0.9769
Epoch 3/40
 - 52s - loss: 0.1024 - acc: 0.9769 - val_loss: 0.1018 - val_acc: 0.9778

==================================================================================================
	Training time : 0:03:28.789122
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.177000
==================================================================================================
	Identification : 0.735
	P, R  : 0.769, 0.704

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 15, True, True, False, True, True, 2, 64, False, False, True, relu, 0.36, 165, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 80, False, 5, 5, 256, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 4306765
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_59 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_60 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_59 (Embedding)        (None, 4, 256)       4022016     input_59[0][0]                   
__________________________________________________________________________________________________
embedding_60 (Embedding)        (None, 4, 80)        62160       input_60[0][0]                   
__________________________________________________________________________________________________
flatten_59 (Flatten)            (None, 1024)         0           embedding_59[0][0]               
__________________________________________________________________________________________________
flatten_60 (Flatten)            (None, 320)          0           embedding_60[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 1344)         0           flatten_59[0][0]                 
                                                                 flatten_60[0][0]                 
__________________________________________________________________________________________________
dense_59 (Dense)                (None, 165)          221925      concatenate_30[0][0]             
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 165)          0           dense_59[0][0]                   
__________________________________________________________________________________________________
dense_60 (Dense)                (None, 4)            664         dropout_30[0][0]                 
==================================================================================================
Total params: 4,306,765
Trainable params: 4,306,765
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 15

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 58s - loss: 0.1672 - acc: 0.9642 - val_loss: 0.1223 - val_acc: 0.9748
Epoch 2/40
 - 61s - loss: 0.1221 - acc: 0.9751 - val_loss: 0.1154 - val_acc: 0.9768
Epoch 3/40
 - 58s - loss: 0.1146 - acc: 0.9767 - val_loss: 0.1135 - val_acc: 0.9776

==================================================================================================
	Training time : 0:03:50.409638
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.566031
==================================================================================================
	Identification : 0.729
	P, R  : 0.753, 0.707

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 16, True, True, False, True, True, 2, 64, False, True, True, relu, 0.39, 58, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 112, False, 5, 5, 366, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3771472
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_61 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_62 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_61 (Embedding)        (None, 4, 366)       3573258     input_61[0][0]                   
__________________________________________________________________________________________________
embedding_62 (Embedding)        (None, 4, 112)       87024       input_62[0][0]                   
__________________________________________________________________________________________________
flatten_61 (Flatten)            (None, 1464)         0           embedding_61[0][0]               
__________________________________________________________________________________________________
flatten_62 (Flatten)            (None, 448)          0           embedding_62[0][0]               
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 1912)         0           flatten_61[0][0]                 
                                                                 flatten_62[0][0]                 
__________________________________________________________________________________________________
dense_61 (Dense)                (None, 58)           110954      concatenate_31[0][0]             
__________________________________________________________________________________________________
dropout_31 (Dropout)            (None, 58)           0           dense_61[0][0]                   
__________________________________________________________________________________________________
dense_62 (Dense)                (None, 4)            236         dropout_31[0][0]                 
==================================================================================================
Total params: 3,771,472
Trainable params: 3,771,472
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 56s - loss: 0.1861 - acc: 0.9605 - val_loss: 0.1327 - val_acc: 0.9719
Epoch 2/40
 - 56s - loss: 0.1378 - acc: 0.9714 - val_loss: 0.1241 - val_acc: 0.9750
Epoch 3/40
 - 56s - loss: 0.1297 - acc: 0.9736 - val_loss: 0.1212 - val_acc: 0.9763

==================================================================================================
	Training time : 0:03:44.552228
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.163488
==================================================================================================
	Identification : 0.724
	P, R  : 0.756, 0.694

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 3, True, True, False, True, True, 2, 64, False, False, True, relu, 0.25, 32, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 142, False, 5, 5, 405, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 6543469
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_63 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_64 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_63 (Embedding)        (None, 4, 405)       6362955     input_63[0][0]                   
__________________________________________________________________________________________________
embedding_64 (Embedding)        (None, 4, 142)       110334      input_64[0][0]                   
__________________________________________________________________________________________________
flatten_63 (Flatten)            (None, 1620)         0           embedding_63[0][0]               
__________________________________________________________________________________________________
flatten_64 (Flatten)            (None, 568)          0           embedding_64[0][0]               
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 2188)         0           flatten_63[0][0]                 
                                                                 flatten_64[0][0]                 
__________________________________________________________________________________________________
dense_63 (Dense)                (None, 32)           70048       concatenate_32[0][0]             
__________________________________________________________________________________________________
dropout_32 (Dropout)            (None, 32)           0           dense_63[0][0]                   
__________________________________________________________________________________________________
dense_64 (Dense)                (None, 4)            132         dropout_32[0][0]                 
==================================================================================================
Total params: 6,543,469
Trainable params: 6,543,469
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 3

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 72s - loss: 0.1231 - acc: 0.9663 - val_loss: 0.0917 - val_acc: 0.9751
Epoch 2/40
 - 72s - loss: 0.0923 - acc: 0.9749 - val_loss: 0.0868 - val_acc: 0.9769
Epoch 3/40
 - 72s - loss: 0.0872 - acc: 0.9763 - val_loss: 0.0856 - val_acc: 0.9772

==================================================================================================
	Training time : 0:04:33.913862
==================================================================================================

==================================================================================================
	Parsing time : 0:00:26.771894
==================================================================================================
	Identification : 0.735
	P, R  : 0.748, 0.723

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 18, True, True, False, True, True, 2, 64, False, False, True, relu, 0.35, 30, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 145, False, 5, 5, 270, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 4404589
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_65 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_66 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_65 (Embedding)        (None, 4, 270)       4241970     input_65[0][0]                   
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 4, 145)       112665      input_66[0][0]                   
__________________________________________________________________________________________________
flatten_65 (Flatten)            (None, 1080)         0           embedding_65[0][0]               
__________________________________________________________________________________________________
flatten_66 (Flatten)            (None, 580)          0           embedding_66[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 1660)         0           flatten_65[0][0]                 
                                                                 flatten_66[0][0]                 
__________________________________________________________________________________________________
dense_65 (Dense)                (None, 30)           49830       concatenate_33[0][0]             
__________________________________________________________________________________________________
dropout_33 (Dropout)            (None, 30)           0           dense_65[0][0]                   
__________________________________________________________________________________________________
dense_66 (Dense)                (None, 4)            124         dropout_33[0][0]                 
==================================================================================================
Total params: 4,404,589
Trainable params: 4,404,589
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 18

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 59s - loss: 0.2008 - acc: 0.9566 - val_loss: 0.1408 - val_acc: 0.9700
Epoch 2/40
 - 59s - loss: 0.1476 - acc: 0.9681 - val_loss: 0.1331 - val_acc: 0.9715
Epoch 3/40
 - 59s - loss: 0.1396 - acc: 0.9702 - val_loss: 0.1339 - val_acc: 0.9727

==================================================================================================
	Training time : 0:03:50.433388
==================================================================================================

==================================================================================================
	Parsing time : 0:00:26.146089
==================================================================================================
	Identification : 0.671
	P, R  : 0.74, 0.614

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 7, True, True, False, True, True, 2, 64, False, False, True, relu, 0.22, 192, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 37, False, 5, 5, 474, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 7869175
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_67 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_68 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 4, 474)       7447014     input_67[0][0]                   
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 4, 37)        28749       input_68[0][0]                   
__________________________________________________________________________________________________
flatten_67 (Flatten)            (None, 1896)         0           embedding_67[0][0]               
__________________________________________________________________________________________________
flatten_68 (Flatten)            (None, 148)          0           embedding_68[0][0]               
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 2044)         0           flatten_67[0][0]                 
                                                                 flatten_68[0][0]                 
__________________________________________________________________________________________________
dense_67 (Dense)                (None, 192)          392640      concatenate_34[0][0]             
__________________________________________________________________________________________________
dropout_34 (Dropout)            (None, 192)          0           dense_67[0][0]                   
__________________________________________________________________________________________________
dense_68 (Dense)                (None, 4)            772         dropout_34[0][0]                 
==================================================================================================
Total params: 7,869,175
Trainable params: 7,869,175
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 7

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 79s - loss: 0.1351 - acc: 0.9684 - val_loss: 0.1008 - val_acc: 0.9769
Epoch 2/40
 - 79s - loss: 0.0986 - acc: 0.9773 - val_loss: 0.0971 - val_acc: 0.9782
Epoch 3/40
 - 79s - loss: 0.0937 - acc: 0.9782 - val_loss: 0.0966 - val_acc: 0.9784

==================================================================================================
	Training time : 0:04:57.620985
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.800085
==================================================================================================
	Identification : 0.748
	P, R  : 0.757, 0.739

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 5, True, True, False, True, True, 2, 64, False, True, True, relu, 0.21, 102, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 72, False, 5, 5, 330, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3442264
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_69 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_70 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_69 (Embedding)        (None, 4, 330)       3221790     input_69[0][0]                   
__________________________________________________________________________________________________
embedding_70 (Embedding)        (None, 4, 72)        55944       input_70[0][0]                   
__________________________________________________________________________________________________
flatten_69 (Flatten)            (None, 1320)         0           embedding_69[0][0]               
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 288)          0           embedding_70[0][0]               
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 1608)         0           flatten_69[0][0]                 
                                                                 flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_69 (Dense)                (None, 102)          164118      concatenate_35[0][0]             
__________________________________________________________________________________________________
dropout_35 (Dropout)            (None, 102)          0           dense_69[0][0]                   
__________________________________________________________________________________________________
dense_70 (Dense)                (None, 4)            412         dropout_35[0][0]                 
==================================================================================================
Total params: 3,442,264
Trainable params: 3,442,264
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 53s - loss: 0.1240 - acc: 0.9695 - val_loss: 0.0960 - val_acc: 0.9769
Epoch 2/40
 - 53s - loss: 0.0945 - acc: 0.9770 - val_loss: 0.0910 - val_acc: 0.9784
Epoch 3/40
 - 53s - loss: 0.0900 - acc: 0.9779 - val_loss: 0.0896 - val_acc: 0.9786

==================================================================================================
	Training time : 0:03:36.953132
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.009508
==================================================================================================
	Identification : 0.756
	P, R  : 0.745, 0.767

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 12, True, True, False, True, True, 2, 64, False, True, True, relu, 0.32, 124, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 26, False, 5, 5, 217, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 2259925
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_71 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_72 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_71 (Embedding)        (None, 4, 217)       2118571     input_71[0][0]                   
__________________________________________________________________________________________________
embedding_72 (Embedding)        (None, 4, 26)        20202       input_72[0][0]                   
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 868)          0           embedding_71[0][0]               
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 104)          0           embedding_72[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 972)          0           flatten_71[0][0]                 
                                                                 flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_71 (Dense)                (None, 124)          120652      concatenate_36[0][0]             
__________________________________________________________________________________________________
dropout_36 (Dropout)            (None, 124)          0           dense_71[0][0]                   
__________________________________________________________________________________________________
dense_72 (Dense)                (None, 4)            500         dropout_36[0][0]                 
==================================================================================================
Total params: 2,259,925
Trainable params: 2,259,925
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 12

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 47s - loss: 0.1553 - acc: 0.9664 - val_loss: 0.1175 - val_acc: 0.9755
Epoch 2/40
 - 47s - loss: 0.1179 - acc: 0.9754 - val_loss: 0.1106 - val_acc: 0.9772
Epoch 3/40
 - 47s - loss: 0.1116 - acc: 0.9768 - val_loss: 0.1088 - val_acc: 0.9781

==================================================================================================
	Training time : 0:03:09.394657
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.263312
==================================================================================================
	Identification : 0.748
	P, R  : 0.749, 0.747

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 17, True, True, False, True, True, 2, 64, False, False, True, relu, 0.17, 19, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 62, False, 5, 5, 244, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 3905013
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_73 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_74 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_73 (Embedding)        (None, 4, 244)       3833484     input_73[0][0]                   
__________________________________________________________________________________________________
embedding_74 (Embedding)        (None, 4, 62)        48174       input_74[0][0]                   
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 976)          0           embedding_73[0][0]               
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 248)          0           embedding_74[0][0]               
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 1224)         0           flatten_73[0][0]                 
                                                                 flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_73 (Dense)                (None, 19)           23275       concatenate_37[0][0]             
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 19)           0           dense_73[0][0]                   
__________________________________________________________________________________________________
dense_74 (Dense)                (None, 4)            80          dropout_37[0][0]                 
==================================================================================================
Total params: 3,905,013
Trainable params: 3,905,013
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 17

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 55s - loss: 0.1832 - acc: 0.9610 - val_loss: 0.1339 - val_acc: 0.9715
Epoch 2/40
 - 55s - loss: 0.1368 - acc: 0.9713 - val_loss: 0.1264 - val_acc: 0.9742
Epoch 3/40
 - 55s - loss: 0.1295 - acc: 0.9734 - val_loss: 0.1264 - val_acc: 0.9751

==================================================================================================
	Training time : 0:03:38.518916
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.836309
==================================================================================================
	Identification : 0.709
	P, R  : 0.739, 0.681

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 2, True, True, False, True, True, 2, 64, False, True, True, relu, 0.28, 44, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 31, False, 5, 5, 284, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 2852443
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_75 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_76 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_75 (Embedding)        (None, 4, 284)       2772692     input_75[0][0]                   
__________________________________________________________________________________________________
embedding_76 (Embedding)        (None, 4, 31)        24087       input_76[0][0]                   
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 1136)         0           embedding_75[0][0]               
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 124)          0           embedding_76[0][0]               
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 1260)         0           flatten_75[0][0]                 
                                                                 flatten_76[0][0]                 
__________________________________________________________________________________________________
dense_75 (Dense)                (None, 44)           55484       concatenate_38[0][0]             
__________________________________________________________________________________________________
dropout_38 (Dropout)            (None, 44)           0           dense_75[0][0]                   
__________________________________________________________________________________________________
dense_76 (Dense)                (None, 4)            180         dropout_38[0][0]                 
==================================================================================================
Total params: 2,852,443
Trainable params: 2,852,443
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 2

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 51s - loss: 0.1114 - acc: 0.9672 - val_loss: 0.0853 - val_acc: 0.9756
Epoch 2/40
 - 51s - loss: 0.0851 - acc: 0.9751 - val_loss: 0.0798 - val_acc: 0.9770
Epoch 3/40
 - 51s - loss: 0.0805 - acc: 0.9765 - val_loss: 0.0779 - val_acc: 0.9779

==================================================================================================
	Training time : 0:03:24.101398
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.714043
==================================================================================================
	Identification : 0.75
	P, R  : 0.738, 0.762

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 22, True, True, False, True, True, 2, 64, False, False, True, relu, 0.29, 168, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 138, False, 5, 5, 351, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 5951239
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_77 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_78 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_77 (Embedding)        (None, 4, 351)       5514561     input_77[0][0]                   
__________________________________________________________________________________________________
embedding_78 (Embedding)        (None, 4, 138)       107226      input_78[0][0]                   
__________________________________________________________________________________________________
flatten_77 (Flatten)            (None, 1404)         0           embedding_77[0][0]               
__________________________________________________________________________________________________
flatten_78 (Flatten)            (None, 552)          0           embedding_78[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 1956)         0           flatten_77[0][0]                 
                                                                 flatten_78[0][0]                 
__________________________________________________________________________________________________
dense_77 (Dense)                (None, 168)          328776      concatenate_39[0][0]             
__________________________________________________________________________________________________
dropout_39 (Dropout)            (None, 168)          0           dense_77[0][0]                   
__________________________________________________________________________________________________
dense_78 (Dense)                (None, 4)            676         dropout_39[0][0]                 
==================================================================================================
Total params: 5,951,239
Trainable params: 5,951,239
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 22

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 66s - loss: 0.1759 - acc: 0.9649 - val_loss: 0.1271 - val_acc: 0.9756
Epoch 2/40
 - 66s - loss: 0.1268 - acc: 0.9759 - val_loss: 0.1214 - val_acc: 0.9773
Epoch 3/40
 - 66s - loss: 0.1194 - acc: 0.9774 - val_loss: 0.1196 - val_acc: 0.9780

==================================================================================================
	Training time : 0:04:16.234323
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.176873
==================================================================================================
	Identification : 0.73
	P, R  : 0.749, 0.713

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 1, True, True, False, True, True, 2, 64, False, False, True, relu, 0.23, 155, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 81, False, 5, 5, 194, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 3282150
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_79 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_80 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_79 (Embedding)        (None, 4, 194)       3047934     input_79[0][0]                   
__________________________________________________________________________________________________
embedding_80 (Embedding)        (None, 4, 81)        62937       input_80[0][0]                   
__________________________________________________________________________________________________
flatten_79 (Flatten)            (None, 776)          0           embedding_79[0][0]               
__________________________________________________________________________________________________
flatten_80 (Flatten)            (None, 324)          0           embedding_80[0][0]               
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 1100)         0           flatten_79[0][0]                 
                                                                 flatten_80[0][0]                 
__________________________________________________________________________________________________
dense_79 (Dense)                (None, 155)          170655      concatenate_40[0][0]             
__________________________________________________________________________________________________
dropout_40 (Dropout)            (None, 155)          0           dense_79[0][0]                   
__________________________________________________________________________________________________
dense_80 (Dense)                (None, 4)            624         dropout_40[0][0]                 
==================================================================================================
Total params: 3,282,150
Trainable params: 3,282,150
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 1

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 51s - loss: 0.0873 - acc: 0.9698 - val_loss: 0.0656 - val_acc: 0.9765
Epoch 2/40
 - 51s - loss: 0.0628 - acc: 0.9772 - val_loss: 0.0626 - val_acc: 0.9777
Epoch 3/40
 - 51s - loss: 0.0590 - acc: 0.9782 - val_loss: 0.0618 - val_acc: 0.9777

==================================================================================================
	Training time : 0:03:25.890962
==================================================================================================

==================================================================================================
	Parsing time : 0:00:22.798702
==================================================================================================
	Identification : 0.728
	P, R  : 0.689, 0.771

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 22, True, True, False, True, True, 2, 64, False, True, True, relu, 0.16, 112, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 38, False, 5, 5, 404, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4172358
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_81 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_82 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_81 (Embedding)        (None, 4, 404)       3944252     input_81[0][0]                   
__________________________________________________________________________________________________
embedding_82 (Embedding)        (None, 4, 38)        29526       input_82[0][0]                   
__________________________________________________________________________________________________
flatten_81 (Flatten)            (None, 1616)         0           embedding_81[0][0]               
__________________________________________________________________________________________________
flatten_82 (Flatten)            (None, 152)          0           embedding_82[0][0]               
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 1768)         0           flatten_81[0][0]                 
                                                                 flatten_82[0][0]                 
__________________________________________________________________________________________________
dense_81 (Dense)                (None, 112)          198128      concatenate_41[0][0]             
__________________________________________________________________________________________________
dropout_41 (Dropout)            (None, 112)          0           dense_81[0][0]                   
__________________________________________________________________________________________________
dense_82 (Dense)                (None, 4)            452         dropout_41[0][0]                 
==================================================================================================
Total params: 4,172,358
Trainable params: 4,172,358
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 22

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 58s - loss: 0.1713 - acc: 0.9662 - val_loss: 0.1264 - val_acc: 0.9761
Epoch 2/40
 - 58s - loss: 0.1272 - acc: 0.9760 - val_loss: 0.1212 - val_acc: 0.9775
Epoch 3/40
 - 58s - loss: 0.1207 - acc: 0.9773 - val_loss: 0.1188 - val_acc: 0.9782

==================================================================================================
	Training time : 0:03:47.490732
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.413191
==================================================================================================
	Identification : 0.737
	P, R  : 0.75, 0.725

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 6, True, True, False, True, True, 2, 64, False, False, True, relu, 0.21, 23, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 105, False, 5, 5, 333, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 5353763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_83 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_84 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_83 (Embedding)        (None, 4, 333)       5231763     input_83[0][0]                   
__________________________________________________________________________________________________
embedding_84 (Embedding)        (None, 4, 105)       81585       input_84[0][0]                   
__________________________________________________________________________________________________
flatten_83 (Flatten)            (None, 1332)         0           embedding_83[0][0]               
__________________________________________________________________________________________________
flatten_84 (Flatten)            (None, 420)          0           embedding_84[0][0]               
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 1752)         0           flatten_83[0][0]                 
                                                                 flatten_84[0][0]                 
__________________________________________________________________________________________________
dense_83 (Dense)                (None, 23)           40319       concatenate_42[0][0]             
__________________________________________________________________________________________________
dropout_42 (Dropout)            (None, 23)           0           dense_83[0][0]                   
__________________________________________________________________________________________________
dense_84 (Dense)                (None, 4)            96          dropout_42[0][0]                 
==================================================================================================
Total params: 5,353,763
Trainable params: 5,353,763
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 6

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 63s - loss: 0.1464 - acc: 0.9639 - val_loss: 0.1117 - val_acc: 0.9732
Epoch 2/40
 - 63s - loss: 0.1112 - acc: 0.9731 - val_loss: 0.1038 - val_acc: 0.9756
Epoch 3/40
 - 63s - loss: 0.1051 - acc: 0.9749 - val_loss: 0.1011 - val_acc: 0.9766

==================================================================================================
	Training time : 0:04:06.179415
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.118271
==================================================================================================
	Identification : 0.719
	P, R  : 0.745, 0.695

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 4, True, True, False, True, True, 2, 64, False, False, True, relu, 0.24, 174, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 31, False, 5, 5, 301, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 4985044
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_85 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_86 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_85 (Embedding)        (None, 4, 301)       4729011     input_85[0][0]                   
__________________________________________________________________________________________________
embedding_86 (Embedding)        (None, 4, 31)        24087       input_86[0][0]                   
__________________________________________________________________________________________________
flatten_85 (Flatten)            (None, 1204)         0           embedding_85[0][0]               
__________________________________________________________________________________________________
flatten_86 (Flatten)            (None, 124)          0           embedding_86[0][0]               
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 1328)         0           flatten_85[0][0]                 
                                                                 flatten_86[0][0]                 
__________________________________________________________________________________________________
dense_85 (Dense)                (None, 174)          231246      concatenate_43[0][0]             
__________________________________________________________________________________________________
dropout_43 (Dropout)            (None, 174)          0           dense_85[0][0]                   
__________________________________________________________________________________________________
dense_86 (Dense)                (None, 4)            700         dropout_43[0][0]                 
==================================================================================================
Total params: 4,985,044
Trainable params: 4,985,044
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 4

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 62s - loss: 0.1193 - acc: 0.9693 - val_loss: 0.0901 - val_acc: 0.9770
Epoch 2/40
 - 62s - loss: 0.0880 - acc: 0.9774 - val_loss: 0.0867 - val_acc: 0.9780
Epoch 3/40
 - 62s - loss: 0.0836 - acc: 0.9783 - val_loss: 0.0864 - val_acc: 0.9783

==================================================================================================
	Training time : 0:04:05.128693
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.666016
==================================================================================================
	Identification : 0.743
	P, R  : 0.752, 0.733

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 2, True, True, False, True, True, 2, 64, False, True, True, relu, 0.29, 132, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 66, False, 5, 5, 490, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 5129384
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_87 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_88 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_87 (Embedding)        (None, 4, 490)       4783870     input_87[0][0]                   
__________________________________________________________________________________________________
embedding_88 (Embedding)        (None, 4, 66)        51282       input_88[0][0]                   
__________________________________________________________________________________________________
flatten_87 (Flatten)            (None, 1960)         0           embedding_87[0][0]               
__________________________________________________________________________________________________
flatten_88 (Flatten)            (None, 264)          0           embedding_88[0][0]               
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 2224)         0           flatten_87[0][0]                 
                                                                 flatten_88[0][0]                 
__________________________________________________________________________________________________
dense_87 (Dense)                (None, 132)          293700      concatenate_44[0][0]             
__________________________________________________________________________________________________
dropout_44 (Dropout)            (None, 132)          0           dense_87[0][0]                   
__________________________________________________________________________________________________
dense_88 (Dense)                (None, 4)            532         dropout_44[0][0]                 
==================================================================================================
Total params: 5,129,384
Trainable params: 5,129,384
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 2

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 64s - loss: 0.1055 - acc: 0.9695 - val_loss: 0.0798 - val_acc: 0.9771
Epoch 2/40
 - 63s - loss: 0.0783 - acc: 0.9769 - val_loss: 0.0748 - val_acc: 0.9782
Epoch 3/40
 - 63s - loss: 0.0739 - acc: 0.9779 - val_loss: 0.0730 - val_acc: 0.9787

==================================================================================================
	Training time : 0:04:06.425293
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.265465
==================================================================================================
	Identification : 0.758
	P, R  : 0.733, 0.785

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '11889')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/lock_dir
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 20, True, True, False, True, True, 2, 64, False, False, True, relu, 0.24, 29, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 18, False, 5, 5, 461, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 7312470
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_89 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_90 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_89 (Embedding)        (None, 4, 461)       7242771     input_89[0][0]                   
__________________________________________________________________________________________________
embedding_90 (Embedding)        (None, 4, 18)        13986       input_90[0][0]                   
__________________________________________________________________________________________________
flatten_89 (Flatten)            (None, 1844)         0           embedding_89[0][0]               
__________________________________________________________________________________________________
flatten_90 (Flatten)            (None, 72)           0           embedding_90[0][0]               
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 1916)         0           flatten_89[0][0]                 
                                                                 flatten_90[0][0]                 
__________________________________________________________________________________________________
dense_89 (Dense)                (None, 29)           55593       concatenate_45[0][0]             
__________________________________________________________________________________________________
dropout_45 (Dropout)            (None, 29)           0           dense_89[0][0]                   
__________________________________________________________________________________________________
dense_90 (Dense)                (None, 4)            120         dropout_45[0][0]                 
==================================================================================================
Total params: 7,312,470
Trainable params: 7,312,470
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 20

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 77s - loss: 0.1965 - acc: 0.9582 - val_loss: 0.1367 - val_acc: 0.9714
Epoch 2/40
 - 77s - loss: 0.1411 - acc: 0.9709 - val_loss: 0.1281 - val_acc: 0.9743
Epoch 3/40
 - 77s - loss: 0.1325 - acc: 0.9732 - val_loss: 0.1251 - val_acc: 0.9756

==================================================================================================
	Training time : 0:05:05.302117
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.850089
==================================================================================================
	Identification : 0.712
	P, R  : 0.74, 0.687

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 23, True, True, False, True, True, 2, 64, False, True, True, relu, 0.14, 129, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 65, False, 5, 5, 220, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 2346074
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_91 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_92 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_91 (Embedding)        (None, 4, 220)       2147860     input_91[0][0]                   
__________________________________________________________________________________________________
embedding_92 (Embedding)        (None, 4, 65)        50505       input_92[0][0]                   
__________________________________________________________________________________________________
flatten_91 (Flatten)            (None, 880)          0           embedding_91[0][0]               
__________________________________________________________________________________________________
flatten_92 (Flatten)            (None, 260)          0           embedding_92[0][0]               
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 1140)         0           flatten_91[0][0]                 
                                                                 flatten_92[0][0]                 
__________________________________________________________________________________________________
dense_91 (Dense)                (None, 129)          147189      concatenate_46[0][0]             
__________________________________________________________________________________________________
dropout_46 (Dropout)            (None, 129)          0           dense_91[0][0]                   
__________________________________________________________________________________________________
dense_92 (Dense)                (None, 4)            520         dropout_46[0][0]                 
==================================================================================================
Total params: 2,346,074
Trainable params: 2,346,074
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 47s - loss: 0.1695 - acc: 0.9665 - val_loss: 0.1293 - val_acc: 0.9755
Epoch 2/40
 - 47s - loss: 0.1269 - acc: 0.9762 - val_loss: 0.1217 - val_acc: 0.9775
Epoch 3/40
 - 47s - loss: 0.1206 - acc: 0.9773 - val_loss: 0.1204 - val_acc: 0.9781

==================================================================================================
	Training time : 0:03:13.649672
==================================================================================================

==================================================================================================
	Parsing time : 0:00:23.048031
==================================================================================================
	Identification : 0.746
	P, R  : 0.753, 0.738

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 23, True, True, False, True, True, 2, 64, False, False, True, relu, 0.25, 138, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 75, False, 5, 5, 463, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 7630138
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_93 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_94 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_93 (Embedding)        (None, 4, 463)       7274193     input_93[0][0]                   
__________________________________________________________________________________________________
embedding_94 (Embedding)        (None, 4, 75)        58275       input_94[0][0]                   
__________________________________________________________________________________________________
flatten_93 (Flatten)            (None, 1852)         0           embedding_93[0][0]               
__________________________________________________________________________________________________
flatten_94 (Flatten)            (None, 300)          0           embedding_94[0][0]               
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 2152)         0           flatten_93[0][0]                 
                                                                 flatten_94[0][0]                 
__________________________________________________________________________________________________
dense_93 (Dense)                (None, 138)          297114      concatenate_47[0][0]             
__________________________________________________________________________________________________
dropout_47 (Dropout)            (None, 138)          0           dense_93[0][0]                   
__________________________________________________________________________________________________
dense_94 (Dense)                (None, 4)            556         dropout_47[0][0]                 
==================================================================================================
Total params: 7,630,138
Trainable params: 7,630,138
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 23

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 75s - loss: 0.1793 - acc: 0.9643 - val_loss: 0.1304 - val_acc: 0.9750
Epoch 2/40
 - 75s - loss: 0.1288 - acc: 0.9756 - val_loss: 0.1236 - val_acc: 0.9768
Epoch 3/40
 - 76s - loss: 0.1216 - acc: 0.9771 - val_loss: 0.1218 - val_acc: 0.9775

==================================================================================================
	Training time : 0:04:46.225554
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.796690
==================================================================================================
	Identification : 0.727
	P, R  : 0.759, 0.697

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 5, True, True, False, True, True, 2, 64, False, True, True, relu, 0.27, 141, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 92, False, 5, 5, 452, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 4791885
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_95 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_96 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_95 (Embedding)        (None, 4, 452)       4412876     input_95[0][0]                   
__________________________________________________________________________________________________
embedding_96 (Embedding)        (None, 4, 92)        71484       input_96[0][0]                   
__________________________________________________________________________________________________
flatten_95 (Flatten)            (None, 1808)         0           embedding_95[0][0]               
__________________________________________________________________________________________________
flatten_96 (Flatten)            (None, 368)          0           embedding_96[0][0]               
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 2176)         0           flatten_95[0][0]                 
                                                                 flatten_96[0][0]                 
__________________________________________________________________________________________________
dense_95 (Dense)                (None, 141)          306957      concatenate_48[0][0]             
__________________________________________________________________________________________________
dropout_48 (Dropout)            (None, 141)          0           dense_95[0][0]                   
__________________________________________________________________________________________________
dense_96 (Dense)                (None, 4)            568         dropout_48[0][0]                 
==================================================================================================
Total params: 4,791,885
Trainable params: 4,791,885
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 5

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 60s - loss: 0.1281 - acc: 0.9688 - val_loss: 0.0968 - val_acc: 0.9769
Epoch 2/40
 - 60s - loss: 0.0964 - acc: 0.9767 - val_loss: 0.0919 - val_acc: 0.9780
Epoch 3/40
 - 60s - loss: 0.0915 - acc: 0.9777 - val_loss: 0.0903 - val_acc: 0.9786

==================================================================================================
	Training time : 0:03:54.634895
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.415948
==================================================================================================
	Identification : 0.751
	P, R  : 0.764, 0.738

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, False, 3, True, True, False, True, True, 2, 64, False, False, True, relu, 0.13, 33, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 107, False, 5, 5, 253, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17631
	After : 15711
# Parameters = 4105711
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_97 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_98 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_97 (Embedding)        (None, 4, 253)       3974883     input_97[0][0]                   
__________________________________________________________________________________________________
embedding_98 (Embedding)        (None, 4, 107)       83139       input_98[0][0]                   
__________________________________________________________________________________________________
flatten_97 (Flatten)            (None, 1012)         0           embedding_97[0][0]               
__________________________________________________________________________________________________
flatten_98 (Flatten)            (None, 428)          0           embedding_98[0][0]               
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 1440)         0           flatten_97[0][0]                 
                                                                 flatten_98[0][0]                 
__________________________________________________________________________________________________
dense_97 (Dense)                (None, 33)           47553       concatenate_49[0][0]             
__________________________________________________________________________________________________
dropout_49 (Dropout)            (None, 33)           0           dense_97[0][0]                   
__________________________________________________________________________________________________
dense_98 (Dense)                (None, 4)            136         dropout_49[0][0]                 
==================================================================================================
Total params: 4,105,711
Trainable params: 4,105,711
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108661
	data size after focused sampling = 1108661
	data size before sampling = 1108661
	data size after sampling = 1913308
	4 Labels in train : Counter({0: 478327, 1: 478327, 2: 478327, 3: 478327})
	4 Labels in valid : Counter({3: 48072, 2: 47904, 0: 47680, 1: 47675})
	Favorisation Coeff : 3

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721977 samples, validate on 191331 samples
Epoch 1/40
 - 56s - loss: 0.1153 - acc: 0.9686 - val_loss: 0.0878 - val_acc: 0.9767
Epoch 2/40
 - 57s - loss: 0.0863 - acc: 0.9766 - val_loss: 0.0837 - val_acc: 0.9776
Epoch 3/40
 - 56s - loss: 0.0816 - acc: 0.9776 - val_loss: 0.0829 - val_acc: 0.9782

==================================================================================================
	Training time : 0:03:44.580701
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.660224
==================================================================================================
	Identification : 0.742
	P, R  : 0.747, 0.737

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '11889')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/lock_dir
	Mode: NON.COMPO
==================================================================================================
	Dataset: FTB
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, favorisationCoeff, focused, importantSentences, importantTransitions, overSampling, sampleWeight, bPadding, batchSize, chickPoint, compactVocab, dense1, dense1Activation, dense1Dropout, dense1UnitNumber, dense2, dense2Activation, dense2Dropout, dense2UnitNumber, earlyStop, epochs, features, inputItems, lemma, loss, lr, minDelta, optimizer, posEmb, predictVerbose, s0Padding, s1Padding, tokenEmb, validationSplit, verbose
==================================================================================================
# Configs: NonCompo, ftb, fixedSize, True, True, 10, True, True, False, True, True, 2, 64, False, True, True, relu, 0.36, 51, False, relu, 0, 0, True, 40, False, 4, True, categorical_crossentropy, 0.059, 0.2, adagrad, 145, False, 5, 5, 340, 0.1, 0
==================================================================================================
	Language : FR
==================================================================================================
	Training (Important) : 6572, Test : 1235
	MWEs in tain : 4262, occurrences : 14976
	Impotant words in tain : 3200
	MWE length mean : 2.66
	Seen MWEs : 1701 (80 %)
	New MWEs : 415 (19 %)
==================================================================================================
	Compact Vocabulary cleaning:
==================================================================================================
	Before : 17631
	After : 9763
# Parameters = 3531284
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_99 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_100 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_99 (Embedding)        (None, 4, 340)       3319420     input_99[0][0]                   
__________________________________________________________________________________________________
embedding_100 (Embedding)       (None, 4, 145)       112665      input_100[0][0]                  
__________________________________________________________________________________________________
flatten_99 (Flatten)            (None, 1360)         0           embedding_99[0][0]               
__________________________________________________________________________________________________
flatten_100 (Flatten)           (None, 580)          0           embedding_100[0][0]              
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 1940)         0           flatten_99[0][0]                 
                                                                 flatten_100[0][0]                
__________________________________________________________________________________________________
dense_99 (Dense)                (None, 51)           98991       concatenate_50[0][0]             
__________________________________________________________________________________________________
dropout_50 (Dropout)            (None, 51)           0           dense_99[0][0]                   
__________________________________________________________________________________________________
dense_100 (Dense)               (None, 4)            208         dropout_50[0][0]                 
==================================================================================================
Total params: 3,531,284
Trainable params: 3,531,284
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 1108491
	data size after focused sampling = 1108491
	data size before sampling = 1108491
	data size after sampling = 1913036
	4 Labels in train : Counter({0: 478259, 1: 478259, 2: 478259, 3: 478259})
	4 Labels in valid : Counter({3: 48032, 0: 47808, 2: 47797, 1: 47667})
	Favorisation Coeff : 10

__________________________________________________________________________________________________
	Optimizer : Adagrad,  learning rate = 0.059
__________________________________________________________________________________________________
Train on 1721732 samples, validate on 191304 samples
Epoch 1/40
 - 54s - loss: 0.1644 - acc: 0.9632 - val_loss: 0.1211 - val_acc: 0.9733
Epoch 2/40
 - 54s - loss: 0.1248 - acc: 0.9725 - val_loss: 0.1129 - val_acc: 0.9761
Epoch 3/40
 - 54s - loss: 0.1186 - acc: 0.9743 - val_loss: 0.1114 - val_acc: 0.9767

==================================================================================================
	Training time : 0:03:50.798443
==================================================================================================

==================================================================================================
	Parsing time : 0:00:25.548717
==================================================================================================
	Identification : 0.724
	P, R  : 0.757, 0.695

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
Traceback (most recent call last):
  File "src/xpNonCompo.py", line 30, in <module>
    runRSG(['FR'], Dataset.ftb, None, Evaluation.fixedSize, fileName='ftbMlp.p', xpNumByThread=100, xpNum=1)
  File "/home/halsaied/NNIdenSys/src/rsg.py", line 9, in runRSG
    exps = getGrid(fileName)
  File "/home/halsaied/NNIdenSys/src/rsg.py", line 33, in getGrid
    return pickle.load(open(randomSearchGridPath, 'rb'))
EOFError
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpasxeXI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpaS3l9T). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpasxeXI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpaS3l9T). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpasxeXI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpaS3l9T). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpasxeXI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpaS3l9T). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpasxeXI and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.5--2.7.14-64/tmpaS3l9T). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
