==================================================================================================
        Corpus Mode
==================================================================================================

==================================================================================================
        Numerical expressions:
        Train: 0.11 Test: 0.15
==================================================================================================
        Language : FR
==================================================================================================
        Dataset : FTB
        Training (Important) : 11514, Test : 2541
        MWEs in tain : 5868, occurrences : 22772
        Impotant words in tain : 4155
        MWE length mean : 2.65
        Seen MWEs : 2836 (83 %)
        New MWEs : 580 (16 %)
==================================================================================================

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 11670 * POS : 82
__________________________________________________________________________________________________
        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 4)            0
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 200)       2334000     input_1[0][0]
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 15)        1230        input_2[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 800)          0           embedding_1[0][0]
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 60)           0           embedding_2[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 860)          0           flatten_1[0][0]
                                                                 flatten_2[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 25)           21525       concatenate_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 25)           0           dense_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            208         dropout_1[0][0]
==================================================================================================
Total params: 2,356,963
Trainable params: 2,356,963
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        11098 importatnt sents of 11514
        data size before sampling = 782199
        data size after sampling = 1559676
{0.0: 389919, 1.0: 389919, 2.0: 389919, 7.0: 389919}
        Class weights : {0: 1, 1: 1, 2: 10, 7: 10}
__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 1403708 samples, validate on 155968 samples
Epoch 1/40
 - 39s - loss: 0.0336 - acc: 0.9951 - val_loss: 0.0092 - val_acc: 1.0000
Epoch 2/40
 - 39s - loss: 0.0200 - acc: 0.9972 - val_loss: 0.0106 - val_acc: 1.0000
Epoch 3/40
 - 39s - loss: 0.0187 - acc: 0.9974 - val_loss: 0.0117 - val_acc: 1.0000
Epoch 4/40
 - 39s - loss: 0.0180 - acc: 0.9975 - val_loss: 0.0111 - val_acc: 1.0000
Epoch 5/40
 - 39s - loss: 0.0177 - acc: 0.9975 - val_loss: 0.0115 - val_acc: 1.0000
Epoch 6/40
 - 39s - loss: 0.0173 - acc: 0.9976 - val_loss: 0.0101 - val_acc: 1.0000
Epoch 7/40
 - 39s - loss: 0.0172 - acc: 0.9976 - val_loss: 0.0092 - val_acc: 1.0000
Epoch 8/40
 - 39s - loss: 0.0171 - acc: 0.9977 - val_loss: 0.0071 - val_acc: 1.0000
Epoch 9/40
 - 39s - loss: 0.0169 - acc: 0.9977 - val_loss: 0.0110 - val_acc: 1.0000
Epoch 10/40
 - 39s - loss: 0.0168 - acc: 0.9977 - val_loss: 0.0102 - val_acc: 1.0000
Epoch 11/40
 - 39s - loss: 0.0167 - acc: 0.9977 - val_loss: 0.0083 - val_acc: 1.0000
Epoch 12/40
 - 39s - loss: 0.0165 - acc: 0.9977 - val_loss: 0.0104 - val_acc: 1.0000
Epoch 13/40
 - 39s - loss: 0.0166 - acc: 0.9978 - val_loss: 0.0088 - val_acc: 1.0000
Epoch 14/40
 - 39s - loss: 0.0164 - acc: 0.9978 - val_loss: 0.0094 - val_acc: 1.0000
Epoch 15/40
 - 39s - loss: 0.0163 - acc: 0.9978 - val_loss: 0.0089 - val_acc: 1.0000
Epoch 16/40
 - 39s - loss: 0.0163 - acc: 0.9978 - val_loss: 0.0103 - val_acc: 1.0000
Epoch 17/40
 - 39s - loss: 0.0162 - acc: 0.9978 - val_loss: 0.0085 - val_acc: 1.0000
Epoch 18/40
 - 39s - loss: 0.0161 - acc: 0.9978 - val_loss: 0.0087 - val_acc: 1.0000
Epoch 19/40
 - 39s - loss: 0.0161 - acc: 0.9978 - val_loss: 0.0097 - val_acc: 1.0000
Epoch 20/40
 - 39s - loss: 0.0161 - acc: 0.9978 - val_loss: 0.0110 - val_acc: 1.0000
Epoch 21/40
 - 39s - loss: 0.0159 - acc: 0.9978 - val_loss: 0.0095 - val_acc: 1.0000
Epoch 22/40
 - 39s - loss: 0.0160 - acc: 0.9978 - val_loss: 0.0078 - val_acc: 1.0000
Epoch 23/40
 - 39s - loss: 0.0160 - acc: 0.9978 - val_loss: 0.0106 - val_acc: 1.0000
Epoch 24/40
 - 39s - loss: 0.0159 - acc: 0.9978 - val_loss: 0.0111 - val_acc: 1.0000
Epoch 25/40
 - 39s - loss: 0.0159 - acc: 0.9978 - val_loss: 0.0089 - val_acc: 1.0000
Epoch 26/40
 - 39s - loss: 0.0159 - acc: 0.9978 - val_loss: 0.0104 - val_acc: 1.0000
Epoch 27/40
 - 39s - loss: 0.0159 - acc: 0.9979 - val_loss: 0.0093 - val_acc: 1.0000
Epoch 28/40
 - 39s - loss: 0.0158 - acc: 0.9979 - val_loss: 0.0103 - val_acc: 1.0000
Epoch 29/40
 - 39s - loss: 0.0159 - acc: 0.9979 - val_loss: 0.0115 - val_acc: 1.0000
Epoch 30/40
 - 39s - loss: 0.0158 - acc: 0.9979 - val_loss: 0.0087 - val_acc: 1.0000
Epoch 31/40
 - 39s - loss: 0.0158 - acc: 0.9979 - val_loss: 0.0102 - val_acc: 1.0000
Epoch 32/40
 - 39s - loss: 0.0157 - acc: 0.9979 - val_loss: 0.0100 - val_acc: 1.0000
Epoch 33/40
 - 39s - loss: 0.0157 - acc: 0.9979 - val_loss: 0.0102 - val_acc: 1.0000
Epoch 34/40
 - 39s - loss: 0.0158 - acc: 0.9979 - val_loss: 0.0082 - val_acc: 1.0000
Epoch 35/40
 - 39s - loss: 0.0157 - acc: 0.9979 - val_loss: 0.0093 - val_acc: 1.0000
Epoch 36/40
 - 39s - loss: 0.0157 - acc: 0.9979 - val_loss: 0.0100 - val_acc: 1.0000
Epoch 37/40
 - 41s - loss: 0.0156 - acc: 0.9979 - val_loss: 0.0097 - val_acc: 1.0000
Epoch 38/40
 - 41s - loss: 0.0156 - acc: 0.9979 - val_loss: 0.0088 - val_acc: 1.0000
Epoch 39/40
 - 41s - loss: 0.0156 - acc: 0.9979 - val_loss: 0.0095 - val_acc: 1.0000
Epoch 40/40
 - 41s - loss: 0.0156 - acc: 0.9979 - val_loss: 0.0111 - val_acc: 1.0000

==================================================================================================
        Training time : 0:30:02.790139
==================================================================================================
        Identification : 0.001
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 1
        25 : 1

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 482
        100 : 32
        5 : 501
        10 : 169
        50 : 74
        500 : 19
        25 : 204

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
(base) halsaied@graphique-6:~$ env MKL_THREADING_LAYER=GNU  python NNIdenSys/src/xpNonCompo.py
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_KPHbdz.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 3841/4043 Mb (0.950000) on cuda
Mapped name None to device cuda: GeForce GTX 980 (0000:03:00.0)

==================================================================================================
        Corpus Mode
==================================================================================================

==================================================================================================
        Numerical expressions:
        Train: 0.11 Test: 0.15
==================================================================================================
        Language : FR
==================================================================================================
        Dataset : FTB
        Training (Important) : 11514, Test : 2541
        MWEs in tain : 5868, occurrences : 22772
        Impotant words in tain : 4155
        MWE length mean : 2.65
        Seen MWEs : 2836 (83 %)
        New MWEs : 580 (16 %)
==================================================================================================

__________________________________________________________________________________________________
        Vocabulary
==================================================================================================
        Tokens := 19174 * POS : 969
__________________________________________________________________________________________________
        Embedding
==================================================================================================
        Initialisation = None
        Concatenation = False
        Lemma : 200
        POS = 15
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 4)            0
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 200)       3834800     input_1[0][0]
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 15)        14535       input_2[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 800)          0           embedding_1[0][0]
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 60)           0           embedding_2[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 860)          0           flatten_1[0][0]
                                                                 flatten_2[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 25)           21525       concatenate_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 25)           0           dense_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            208         dropout_1[0][0]
==================================================================================================
Total params: 3,871,068
Trainable params: 3,871,068
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
        Sampling
==================================================================================================
        11098 importatnt sents of 11514
        data size before sampling = 802565
        data size after sampling = 1559676
{0.0: 389919, 1.0: 389919, 2.0: 389919, 7.0: 389919}
        Class weights : {0: 1, 1: 1, 2: 10, 7: 10}
__________________________________________________________________________________________________
        Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 1403708 samples, validate on 155968 samples
Epoch 1/40
 - 46s - loss: 0.1667 - acc: 0.9649 - val_loss: 0.1134 - val_acc: 1.0000
Epoch 2/40
 - 46s - loss: 0.1381 - acc: 0.9703 - val_loss: 0.1070 - val_acc: 1.0000
Epoch 3/40
 - 46s - loss: 0.1308 - acc: 0.9726 - val_loss: 0.1145 - val_acc: 1.0000
Epoch 4/40
 - 46s - loss: 0.1269 - acc: 0.9738 - val_loss: 0.1223 - val_acc: 1.0000
Epoch 5/40
 - 46s - loss: 0.1252 - acc: 0.9741 - val_loss: 0.0995 - val_acc: 1.0000
Epoch 6/40
 - 46s - loss: 0.1238 - acc: 0.9744 - val_loss: 0.1041 - val_acc: 1.0000
Epoch 7/40
 - 46s - loss: 0.1229 - acc: 0.9745 - val_loss: 0.1041 - val_acc: 1.0000
Epoch 8/40
 - 46s - loss: 0.1221 - acc: 0.9747 - val_loss: 0.1217 - val_acc: 1.0000
Epoch 9/40
 - 46s - loss: 0.1213 - acc: 0.9748 - val_loss: 0.1118 - val_acc: 1.0000
Epoch 10/40
 - 46s - loss: 0.1206 - acc: 0.9750 - val_loss: 0.1028 - val_acc: 1.0000
Epoch 11/40
 - 46s - loss: 0.1201 - acc: 0.9751 - val_loss: 0.1045 - val_acc: 1.0000
Epoch 12/40
 - 46s - loss: 0.1197 - acc: 0.9752 - val_loss: 0.1180 - val_acc: 1.0000
Epoch 13/40
 - 46s - loss: 0.1193 - acc: 0.9752 - val_loss: 0.1098 - val_acc: 1.0000
Epoch 14/40
 - 46s - loss: 0.1188 - acc: 0.9754 - val_loss: 0.1084 - val_acc: 1.0000
Epoch 15/40
 - 46s - loss: 0.1184 - acc: 0.9755 - val_loss: 0.1093 - val_acc: 1.0000
Epoch 16/40
 - 46s - loss: 0.1182 - acc: 0.9756 - val_loss: 0.1195 - val_acc: 1.0000
Epoch 17/40
 - 46s - loss: 0.1179 - acc: 0.9756 - val_loss: 0.1180 - val_acc: 1.0000
Epoch 18/40
 - 46s - loss: 0.1176 - acc: 0.9757 - val_loss: 0.1101 - val_acc: 1.0000
Epoch 19/40
 - 46s - loss: 0.1175 - acc: 0.9758 - val_loss: 0.1134 - val_acc: 1.0000
Epoch 20/40
 - 46s - loss: 0.1172 - acc: 0.9758 - val_loss: 0.1148 - val_acc: 1.0000
Epoch 21/40
 - 46s - loss: 0.1171 - acc: 0.9758 - val_loss: 0.1149 - val_acc: 1.0000
Epoch 22/40
 - 46s - loss: 0.1169 - acc: 0.9759 - val_loss: 0.1082 - val_acc: 1.0000
Epoch 23/40
 - 46s - loss: 0.1165 - acc: 0.9760 - val_loss: 0.1128 - val_acc: 1.0000
Epoch 24/40
 - 46s - loss: 0.1165 - acc: 0.9760 - val_loss: 0.1136 - val_acc: 1.0000
Epoch 25/40
 - 45s - loss: 0.1162 - acc: 0.9761 - val_loss: 0.1151 - val_acc: 1.0000
Epoch 26/40
 - 46s - loss: 0.1163 - acc: 0.9760 - val_loss: 0.1117 - val_acc: 1.0000
Epoch 27/40
 - 46s - loss: 0.1159 - acc: 0.9761 - val_loss: 0.1143 - val_acc: 1.0000
Epoch 28/40
 - 46s - loss: 0.1160 - acc: 0.9761 - val_loss: 0.1164 - val_acc: 1.0000
Epoch 29/40
 - 46s - loss: 0.1159 - acc: 0.9762 - val_loss: 0.1101 - val_acc: 1.0000
Epoch 30/40
 - 45s - loss: 0.1155 - acc: 0.9762 - val_loss: 0.1110 - val_acc: 1.0000
Epoch 31/40
 - 45s - loss: 0.1157 - acc: 0.9762 - val_loss: 0.1157 - val_acc: 1.0000
Epoch 32/40
 - 46s - loss: 0.1155 - acc: 0.9762 - val_loss: 0.1175 - val_acc: 1.0000
Epoch 33/40
 - 46s - loss: 0.1154 - acc: 0.9763 - val_loss: 0.1119 - val_acc: 1.0000
Epoch 34/40
 - 45s - loss: 0.1153 - acc: 0.9763 - val_loss: 0.1137 - val_acc: 1.0000
Epoch 35/40
 - 46s - loss: 0.1152 - acc: 0.9764 - val_loss: 0.1088 - val_acc: 1.0000
Epoch 36/40
 - 46s - loss: 0.1151 - acc: 0.9763 - val_loss: 0.1087 - val_acc: 1.0000
Epoch 37/40
 - 46s - loss: 0.1151 - acc: 0.9764 - val_loss: 0.1156 - val_acc: 1.0000
Epoch 38/40
 - 45s - loss: 0.1149 - acc: 0.9764 - val_loss: 0.1123 - val_acc: 1.0000
Epoch 39/40
 - 45s - loss: 0.1149 - acc: 0.9764 - val_loss: 0.1195 - val_acc: 1.0000
Epoch 40/40
 - 45s - loss: 0.1148 - acc: 0.9764 - val_loss: 0.1118 - val_acc: 1.0000

==================================================================================================
        Training time : 0:34:55.923631
==================================================================================================
        Identification : 0.495
        Test analysis
==================================================================================================
        Correctly identified MWEs
==================================================================================================
        0 : 4
        100 : 24
        5 : 199
        10 : 67
        50 : 42
        500 : 14
        25 : 79

__________________________________________________________________________________________________
        Non Identified MWEs
==================================================================================================
        0 : 479
        100 : 18
        5 : 312
        10 : 109
        50 : 48
        500 : 15
        25 : 139

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
