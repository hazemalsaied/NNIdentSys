INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
s6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_RQiVxg.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 3841/4043 Mb (0.950000) on cuda
Mapped name None to device cuda: GeForce GTX 980 (0000:03:00.0)

==================================================================================================
	Corpus Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 1752, Test : 1947
	MWEs in tain : 1196, occurrences : 1933
	Impotant words in tain : 1253
	MWE length mean : 2.47
	Seen MWEs : 216 (45 %)
	New MWEs : 257 (54 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5957 * POS : 12
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 200)       1191400     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 15)        180         input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 800)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 60)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 860)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 25)           21525       concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 25)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            208         dropout_1[0][0]                  
==================================================================================================
Total params: 1,213,313
Trainable params: 1,213,313
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1752 importatnt sents of 1752
	data size before sampling = 102521
	data size after sampling = 352058
{0.0: 50294, 1.0: 50294, 2.0: 50294, 4.0: 50294, 5.0: 50294, 6.0: 50294, 7.0: 50294}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 316852 samples, validate on 35206 samples
Epoch 1/40
 - 8s - loss: 0.2204 - acc: 0.9774 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 8s - loss: 0.0882 - acc: 0.9886 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 8s - loss: 0.0779 - acc: 0.9901 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 8s - loss: 0.0739 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 8s - loss: 0.0708 - acc: 0.9911 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 8s - loss: 0.0690 - acc: 0.9913 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 8s - loss: 0.0675 - acc: 0.9915 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 8s - loss: 0.0665 - acc: 0.9915 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 8s - loss: 0.0657 - acc: 0.9916 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 8s - loss: 0.0649 - acc: 0.9918 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 8s - loss: 0.0646 - acc: 0.9918 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 8s - loss: 0.0639 - acc: 0.9918 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 8s - loss: 0.0634 - acc: 0.9919 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 8s - loss: 0.0629 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 8s - loss: 0.0624 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 8s - loss: 0.0623 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 8s - loss: 0.0622 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 8s - loss: 0.0621 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 8s - loss: 0.0619 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 8s - loss: 0.0615 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 8s - loss: 0.0614 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 8s - loss: 0.0613 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 8s - loss: 0.0609 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 8s - loss: 0.0611 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 8s - loss: 0.0609 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 8s - loss: 0.0608 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 8s - loss: 0.0601 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 8s - loss: 0.0604 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 8s - loss: 0.0603 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 8s - loss: 0.0603 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 8s - loss: 0.0599 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 8s - loss: 0.0601 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 8s - loss: 0.0598 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 8s - loss: 0.0597 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 8s - loss: 0.0597 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 8s - loss: 0.0599 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 8s - loss: 0.0599 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 8s - loss: 0.0597 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 8s - loss: 0.0596 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 8s - loss: 0.0595 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000

==================================================================================================
	Training time : 0:06:04.704683
==================================================================================================
	Identification : 0.535
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 2
	25 : 3
	50 : 1
	None : 82
	5 : 26

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 243
	None : 11
	5 : 1

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : CS
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 10954, Test : 5476
	MWEs in tain : 4398, occurrences : 12852
	Impotant words in tain : 2470
	MWE length mean : 2.3
	Seen MWEs : 1298 (77 %)
	New MWEs : 386 (22 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 17135 * POS : 337
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 200)       3427000     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 15)        5055        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 800)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 60)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 860)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 25)           21525       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 25)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            208         dropout_2[0][0]                  
==================================================================================================
Total params: 3,453,788
Trainable params: 3,453,788
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	10954 importatnt sents of 10954
	data size before sampling = 498585
	data size after sampling = 1700797
{0.0: 242971, 1.0: 242971, 2.0: 242971, 4.0: 242971, 5.0: 242971, 6.0: 242971, 7.0: 242971}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 1530717 samples, validate on 170080 samples
Epoch 1/40
 - 48s - loss: 0.1512 - acc: 0.9789 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 48s - loss: 0.0959 - acc: 0.9854 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 48s - loss: 0.0907 - acc: 0.9864 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 48s - loss: 0.0881 - acc: 0.9869 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 48s - loss: 0.0856 - acc: 0.9873 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 48s - loss: 0.0845 - acc: 0.9875 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 48s - loss: 0.0833 - acc: 0.9877 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 48s - loss: 0.0826 - acc: 0.9878 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 48s - loss: 0.0820 - acc: 0.9879 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 48s - loss: 0.0813 - acc: 0.9881 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 48s - loss: 0.0807 - acc: 0.9881 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 48s - loss: 0.0804 - acc: 0.9882 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 49s - loss: 0.0801 - acc: 0.9883 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 48s - loss: 0.0797 - acc: 0.9883 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 48s - loss: 0.0793 - acc: 0.9884 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 48s - loss: 0.0792 - acc: 0.9884 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 48s - loss: 0.0788 - acc: 0.9885 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 48s - loss: 0.0786 - acc: 0.9885 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 48s - loss: 0.0784 - acc: 0.9886 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 48s - loss: 0.0781 - acc: 0.9886 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 48s - loss: 0.0781 - acc: 0.9886 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 48s - loss: 0.0778 - acc: 0.9887 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 48s - loss: 0.0776 - acc: 0.9887 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 48s - loss: 0.0775 - acc: 0.9888 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 48s - loss: 0.0772 - acc: 0.9888 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 48s - loss: 0.0771 - acc: 0.9888 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 48s - loss: 0.0771 - acc: 0.9888 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 48s - loss: 0.0769 - acc: 0.9889 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 48s - loss: 0.0769 - acc: 0.9889 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 48s - loss: 0.0766 - acc: 0.9889 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 48s - loss: 0.0766 - acc: 0.9889 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 48s - loss: 0.0765 - acc: 0.9889 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 48s - loss: 0.0765 - acc: 0.9889 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 48s - loss: 0.0763 - acc: 0.9890 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 48s - loss: 0.0761 - acc: 0.9890 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 48s - loss: 0.0761 - acc: 0.9890 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 48s - loss: 0.0760 - acc: 0.9890 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 48s - loss: 0.0760 - acc: 0.9890 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 48s - loss: 0.0758 - acc: 0.9891 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 48s - loss: 0.0757 - acc: 0.9891 - val_loss: 1.1921e-06 - val_acc: 1.0000

==================================================================================================
	Training time : 0:36:27.267399
==================================================================================================
	Identification : 0.711
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	100 : 5
	5 : 206
	300 : 1
	50 : 20
	25 : 42
	None : 237

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 369
	100 : 5
	5 : 59
	300 : 1
	50 : 11
	25 : 15
	None : 99

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : DE
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 2091, Test : 1239
	MWEs in tain : 1676, occurrences : 2447
	Impotant words in tain : 1505
	MWE length mean : 2.02
	Seen MWEs : 201 (40 %)
	New MWEs : 299 (59 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5451 * POS : 277
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 200)       1090200     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 15)        4155        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 800)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 60)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 860)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 25)           21525       concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 25)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            208         dropout_3[0][0]                  
==================================================================================================
Total params: 1,116,088
Trainable params: 1,116,088
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	2091 importatnt sents of 2091
	data size before sampling = 96069
	data size after sampling = 374640
{0.0: 46830, 1.0: 46830, 2.0: 46830, 3.0: 46830, 4.0: 46830, 5.0: 46830, 6.0: 46830, 7.0: 46830}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 337176 samples, validate on 37464 samples
Epoch 1/40
 - 8s - loss: 0.3418 - acc: 0.9612 - val_loss: 0.0033 - val_acc: 1.0000
Epoch 2/40
 - 8s - loss: 0.1600 - acc: 0.9806 - val_loss: 0.0023 - val_acc: 1.0000
Epoch 3/40
 - 8s - loss: 0.1416 - acc: 0.9833 - val_loss: 0.0012 - val_acc: 1.0000
Epoch 4/40
 - 8s - loss: 0.1341 - acc: 0.9842 - val_loss: 0.0013 - val_acc: 1.0000
Epoch 5/40
 - 8s - loss: 0.1290 - acc: 0.9851 - val_loss: 9.9044e-04 - val_acc: 1.0000
Epoch 6/40
 - 8s - loss: 0.1239 - acc: 0.9856 - val_loss: 0.0011 - val_acc: 1.0000
Epoch 7/40
 - 8s - loss: 0.1209 - acc: 0.9860 - val_loss: 8.2140e-04 - val_acc: 1.0000
Epoch 8/40
 - 8s - loss: 0.1189 - acc: 0.9862 - val_loss: 9.2671e-04 - val_acc: 1.0000
Epoch 9/40
 - 8s - loss: 0.1174 - acc: 0.9864 - val_loss: 7.4511e-04 - val_acc: 1.0000
Epoch 10/40
 - 8s - loss: 0.1158 - acc: 0.9866 - val_loss: 6.9558e-04 - val_acc: 1.0000
Epoch 11/40
 - 8s - loss: 0.1155 - acc: 0.9867 - val_loss: 7.2024e-04 - val_acc: 1.0000
Epoch 12/40
 - 8s - loss: 0.1138 - acc: 0.9870 - val_loss: 5.6405e-04 - val_acc: 1.0000
Epoch 13/40
 - 8s - loss: 0.1133 - acc: 0.9870 - val_loss: 6.6226e-04 - val_acc: 1.0000
Epoch 14/40
 - 8s - loss: 0.1130 - acc: 0.9871 - val_loss: 5.4757e-04 - val_acc: 1.0000
Epoch 15/40
 - 8s - loss: 0.1113 - acc: 0.9873 - val_loss: 7.2013e-04 - val_acc: 1.0000
Epoch 16/40
 - 8s - loss: 0.1107 - acc: 0.9873 - val_loss: 5.9245e-04 - val_acc: 1.0000
Epoch 17/40
 - 8s - loss: 0.1099 - acc: 0.9874 - val_loss: 6.9146e-04 - val_acc: 1.0000
Epoch 18/40
 - 8s - loss: 0.1106 - acc: 0.9875 - val_loss: 5.8768e-04 - val_acc: 1.0000
Epoch 19/40
 - 8s - loss: 0.1094 - acc: 0.9876 - val_loss: 9.8972e-04 - val_acc: 1.0000
Epoch 20/40
 - 8s - loss: 0.1090 - acc: 0.9876 - val_loss: 6.5722e-04 - val_acc: 1.0000
Epoch 21/40
 - 8s - loss: 0.1081 - acc: 0.9875 - val_loss: 5.3819e-04 - val_acc: 1.0000
Epoch 22/40
 - 8s - loss: 0.1085 - acc: 0.9876 - val_loss: 5.2854e-04 - val_acc: 1.0000
Epoch 23/40
 - 8s - loss: 0.1082 - acc: 0.9876 - val_loss: 4.8802e-04 - val_acc: 1.0000
Epoch 24/40
 - 8s - loss: 0.1075 - acc: 0.9878 - val_loss: 5.3451e-04 - val_acc: 1.0000
Epoch 25/40
 - 8s - loss: 0.1078 - acc: 0.9877 - val_loss: 6.7272e-04 - val_acc: 1.0000
Epoch 26/40
 - 8s - loss: 0.1073 - acc: 0.9878 - val_loss: 6.4931e-04 - val_acc: 1.0000
Epoch 27/40
 - 8s - loss: 0.1072 - acc: 0.9879 - val_loss: 6.2573e-04 - val_acc: 1.0000
Epoch 28/40
 - 8s - loss: 0.1061 - acc: 0.9879 - val_loss: 7.1458e-04 - val_acc: 1.0000
Epoch 29/40
 - 8s - loss: 0.1062 - acc: 0.9880 - val_loss: 6.1758e-04 - val_acc: 1.0000
Epoch 30/40
 - 8s - loss: 0.1056 - acc: 0.9881 - val_loss: 6.2999e-04 - val_acc: 1.0000
Epoch 31/40
 - 8s - loss: 0.1061 - acc: 0.9879 - val_loss: 6.0864e-04 - val_acc: 1.0000
Epoch 32/40
 - 8s - loss: 0.1059 - acc: 0.9881 - val_loss: 7.4025e-04 - val_acc: 1.0000
Epoch 33/40
 - 8s - loss: 0.1058 - acc: 0.9880 - val_loss: 5.9444e-04 - val_acc: 1.0000
Epoch 34/40
 - 8s - loss: 0.1055 - acc: 0.9881 - val_loss: 6.6201e-04 - val_acc: 1.0000
Epoch 35/40
 - 8s - loss: 0.1051 - acc: 0.9880 - val_loss: 5.9177e-04 - val_acc: 1.0000
Epoch 36/40
 - 8s - loss: 0.1049 - acc: 0.9881 - val_loss: 6.6579e-04 - val_acc: 1.0000
Epoch 37/40
 - 8s - loss: 0.1050 - acc: 0.9881 - val_loss: 5.7050e-04 - val_acc: 1.0000
Epoch 38/40
 - 8s - loss: 0.1049 - acc: 0.9882 - val_loss: 6.5376e-04 - val_acc: 1.0000
Epoch 39/40
 - 8s - loss: 0.1049 - acc: 0.9882 - val_loss: 7.2333e-04 - val_acc: 1.0000
Epoch 40/40
 - 8s - loss: 0.1043 - acc: 0.9882 - val_loss: 6.1993e-04 - val_acc: 1.0000

==================================================================================================
	Training time : 0:06:09.565334
==================================================================================================
	Identification : 0.422
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 2
	50 : 1
	None : 83
	5 : 21

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 292
	25 : 1
	50 : 1
	None : 39
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : EL
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 1376, Test : 3567
	MWEs in tain : 849, occurrences : 1518
	Impotant words in tain : 752
	MWE length mean : 2.45
	Seen MWEs : 213 (42 %)
	New MWEs : 287 (57 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4395 * POS : 214
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 200)       879000      input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 15)        3210        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 800)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 60)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 860)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 25)           21525       concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 25)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            208         dropout_4[0][0]                  
==================================================================================================
Total params: 903,943
Trainable params: 903,943
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1376 importatnt sents of 1376
	data size before sampling = 94269
	data size after sampling = 324667
{0.0: 46381, 1.0: 46381, 2.0: 46381, 3.0: 46381, 5.0: 46381, 6.0: 46381, 7.0: 46381}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 292200 samples, validate on 32467 samples
Epoch 1/40
 - 7s - loss: 0.2806 - acc: 0.9725 - val_loss: 0.0613 - val_acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.1363 - acc: 0.9865 - val_loss: 0.0819 - val_acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.1213 - acc: 0.9883 - val_loss: 0.0851 - val_acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.1150 - acc: 0.9892 - val_loss: 0.1570 - val_acc: 1.0000
Epoch 5/40
 - 7s - loss: 0.1098 - acc: 0.9897 - val_loss: 0.0921 - val_acc: 1.0000
Epoch 6/40
 - 7s - loss: 0.1084 - acc: 0.9898 - val_loss: 0.0674 - val_acc: 1.0000
Epoch 7/40
 - 7s - loss: 0.1069 - acc: 0.9901 - val_loss: 0.1081 - val_acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.1053 - acc: 0.9903 - val_loss: 0.0976 - val_acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.1040 - acc: 0.9905 - val_loss: 0.0682 - val_acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.1030 - acc: 0.9907 - val_loss: 0.0928 - val_acc: 1.0000
Epoch 11/40
 - 7s - loss: 0.1030 - acc: 0.9908 - val_loss: 0.1209 - val_acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.1015 - acc: 0.9908 - val_loss: 0.0784 - val_acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.1006 - acc: 0.9908 - val_loss: 0.1097 - val_acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.1016 - acc: 0.9908 - val_loss: 0.1032 - val_acc: 1.0000
Epoch 15/40
 - 7s - loss: 0.1007 - acc: 0.9909 - val_loss: 0.0934 - val_acc: 1.0000
Epoch 16/40
 - 7s - loss: 0.0995 - acc: 0.9910 - val_loss: 0.1247 - val_acc: 1.0000
Epoch 17/40
 - 7s - loss: 0.0980 - acc: 0.9911 - val_loss: 0.1252 - val_acc: 1.0000
Epoch 18/40
 - 7s - loss: 0.0993 - acc: 0.9911 - val_loss: 0.0967 - val_acc: 1.0000
Epoch 19/40
 - 7s - loss: 0.0985 - acc: 0.9912 - val_loss: 0.1027 - val_acc: 1.0000
Epoch 20/40
 - 7s - loss: 0.0989 - acc: 0.9911 - val_loss: 0.1218 - val_acc: 1.0000
Epoch 21/40
 - 7s - loss: 0.0979 - acc: 0.9912 - val_loss: 0.1056 - val_acc: 1.0000
Epoch 22/40
 - 7s - loss: 0.0979 - acc: 0.9912 - val_loss: 0.1059 - val_acc: 1.0000
Epoch 23/40
 - 7s - loss: 0.0971 - acc: 0.9913 - val_loss: 0.0839 - val_acc: 1.0000
Epoch 24/40
 - 7s - loss: 0.0973 - acc: 0.9913 - val_loss: 0.1033 - val_acc: 1.0000
Epoch 25/40
 - 7s - loss: 0.0973 - acc: 0.9913 - val_loss: 0.0915 - val_acc: 1.0000
Epoch 26/40
 - 7s - loss: 0.0964 - acc: 0.9913 - val_loss: 0.0798 - val_acc: 1.0000
Epoch 27/40
 - 7s - loss: 0.0959 - acc: 0.9913 - val_loss: 0.1023 - val_acc: 1.0000
Epoch 28/40
 - 7s - loss: 0.0957 - acc: 0.9913 - val_loss: 0.1326 - val_acc: 1.0000
Epoch 29/40
 - 7s - loss: 0.0963 - acc: 0.9913 - val_loss: 0.0828 - val_acc: 1.0000
Epoch 30/40
 - 7s - loss: 0.0961 - acc: 0.9915 - val_loss: 0.0786 - val_acc: 1.0000
Epoch 31/40
 - 7s - loss: 0.0950 - acc: 0.9914 - val_loss: 0.0938 - val_acc: 1.0000
Epoch 32/40
 - 7s - loss: 0.0958 - acc: 0.9914 - val_loss: 0.1374 - val_acc: 1.0000
Epoch 33/40
 - 7s - loss: 0.0954 - acc: 0.9914 - val_loss: 0.0984 - val_acc: 1.0000
Epoch 34/40
 - 7s - loss: 0.0955 - acc: 0.9915 - val_loss: 0.1052 - val_acc: 1.0000
Epoch 35/40
 - 7s - loss: 0.0948 - acc: 0.9915 - val_loss: 0.0973 - val_acc: 1.0000
Epoch 36/40
 - 7s - loss: 0.0945 - acc: 0.9916 - val_loss: 0.0722 - val_acc: 1.0000
Epoch 37/40
 - 7s - loss: 0.0951 - acc: 0.9916 - val_loss: 0.0979 - val_acc: 1.0000
Epoch 38/40
 - 7s - loss: 0.0956 - acc: 0.9915 - val_loss: 0.1459 - val_acc: 1.0000
Epoch 39/40
 - 7s - loss: 0.0945 - acc: 0.9916 - val_loss: 0.1191 - val_acc: 1.0000
Epoch 40/40
 - 7s - loss: 0.0941 - acc: 0.9917 - val_loss: 0.0913 - val_acc: 1.0000

==================================================================================================
	Training time : 0:05:09.467443
==================================================================================================
	Identification : 0.35
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 1
	None : 51
	5 : 28

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 255
	None : 47
	5 : 14

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : ES
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 638, Test : 2132
	MWEs in tain : 394, occurrences : 748
	Impotant words in tain : 366
	MWE length mean : 2.28
	Seen MWEs : 266 (53 %)
	New MWEs : 234 (46 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2584 * POS : 82
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 200)       516800      input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 15)        1230        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 800)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 60)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 860)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 25)           21525       concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 25)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            208         dropout_5[0][0]                  
==================================================================================================
Total params: 539,763
Trainable params: 539,763
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	638 importatnt sents of 638
	data size before sampling = 58364
	data size after sampling = 201663
{0.0: 28809, 1.0: 28809, 2.0: 28809, 4.0: 28809, 5.0: 28809, 6.0: 28809, 7.0: 28809}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 181496 samples, validate on 20167 samples
Epoch 1/40
 - 4s - loss: 0.1651 - acc: 0.9821 - val_loss: 3.2084e-05 - val_acc: 1.0000
Epoch 2/40
 - 4s - loss: 0.0610 - acc: 0.9915 - val_loss: 2.3476e-05 - val_acc: 1.0000
Epoch 3/40
 - 4s - loss: 0.0532 - acc: 0.9928 - val_loss: 9.8026e-06 - val_acc: 1.0000
Epoch 4/40
 - 4s - loss: 0.0501 - acc: 0.9935 - val_loss: 6.5385e-06 - val_acc: 1.0000
Epoch 5/40
 - 4s - loss: 0.0476 - acc: 0.9938 - val_loss: 3.8627e-06 - val_acc: 1.0000
Epoch 6/40
 - 4s - loss: 0.0459 - acc: 0.9940 - val_loss: 3.2693e-06 - val_acc: 1.0000
Epoch 7/40
 - 4s - loss: 0.0456 - acc: 0.9941 - val_loss: 2.3790e-06 - val_acc: 1.0000
Epoch 8/40
 - 4s - loss: 0.0442 - acc: 0.9944 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 9/40
 - 4s - loss: 0.0438 - acc: 0.9945 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 10/40
 - 4s - loss: 0.0429 - acc: 0.9945 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 11/40
 - 4s - loss: 0.0428 - acc: 0.9946 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 12/40
 - 4s - loss: 0.0425 - acc: 0.9946 - val_loss: 1.4888e-06 - val_acc: 1.0000
Epoch 13/40
 - 4s - loss: 0.0422 - acc: 0.9945 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 14/40
 - 4s - loss: 0.0415 - acc: 0.9947 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 15/40
 - 4s - loss: 0.0418 - acc: 0.9947 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 16/40
 - 4s - loss: 0.0413 - acc: 0.9947 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 17/40
 - 4s - loss: 0.0414 - acc: 0.9947 - val_loss: 1.4888e-06 - val_acc: 1.0000
Epoch 18/40
 - 4s - loss: 0.0410 - acc: 0.9948 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 4s - loss: 0.0412 - acc: 0.9948 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 20/40
 - 4s - loss: 0.0407 - acc: 0.9948 - val_loss: 1.7856e-06 - val_acc: 1.0000
Epoch 21/40
 - 4s - loss: 0.0406 - acc: 0.9948 - val_loss: 1.4888e-06 - val_acc: 1.0000
Epoch 22/40
 - 4s - loss: 0.0402 - acc: 0.9949 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 4s - loss: 0.0405 - acc: 0.9949 - val_loss: 1.4888e-06 - val_acc: 1.0000
Epoch 24/40
 - 4s - loss: 0.0402 - acc: 0.9948 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 4s - loss: 0.0404 - acc: 0.9949 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 4s - loss: 0.0400 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 4s - loss: 0.0396 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 4s - loss: 0.0398 - acc: 0.9949 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 4s - loss: 0.0394 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 4s - loss: 0.0398 - acc: 0.9949 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 4s - loss: 0.0397 - acc: 0.9949 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 4s - loss: 0.0397 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 4s - loss: 0.0400 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 4s - loss: 0.0396 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 4s - loss: 0.0395 - acc: 0.9951 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 4s - loss: 0.0393 - acc: 0.9951 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 4s - loss: 0.0394 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 4s - loss: 0.0392 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 4s - loss: 0.0390 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 4s - loss: 0.0390 - acc: 0.9950 - val_loss: 1.1921e-06 - val_acc: 1.0000

==================================================================================================
	Training time : 0:03:03.731792
==================================================================================================
	Identification : 0.543
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 1
	None : 82
	5 : 22

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 205
	None : 33
	5 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FA
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 1722, Test : 490
	MWEs in tain : 1264, occurrences : 2707
	Impotant words in tain : 828
	MWE length mean : 2.15
	Seen MWEs : 347 (69 %)
	New MWEs : 153 (30 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3734 * POS : 87
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 200)       746800      input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 15)        1305        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 800)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 60)           0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 860)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 25)           21525       concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 25)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            208         dropout_6[0][0]                  
==================================================================================================
Total params: 769,838
Trainable params: 769,838
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1722 importatnt sents of 1722
	data size before sampling = 73061
	data size after sampling = 140712
{0.0: 35178, 1.0: 35178, 2.0: 35178, 7.0: 35178}
	Class weights : {0: 1, 1: 1, 2: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 126640 samples, validate on 14072 samples
Epoch 1/40
 - 3s - loss: 0.2286 - acc: 0.9552 - val_loss: 0.1252 - val_acc: 1.0000
Epoch 2/40
 - 3s - loss: 0.1549 - acc: 0.9698 - val_loss: 0.1055 - val_acc: 1.0000
Epoch 3/40
 - 3s - loss: 0.1480 - acc: 0.9712 - val_loss: 0.1270 - val_acc: 1.0000
Epoch 4/40
 - 3s - loss: 0.1438 - acc: 0.9722 - val_loss: 0.1215 - val_acc: 1.0000
Epoch 5/40
 - 3s - loss: 0.1417 - acc: 0.9726 - val_loss: 0.1160 - val_acc: 1.0000
Epoch 6/40
 - 3s - loss: 0.1395 - acc: 0.9731 - val_loss: 0.1171 - val_acc: 1.0000
Epoch 7/40
 - 3s - loss: 0.1386 - acc: 0.9734 - val_loss: 0.1101 - val_acc: 1.0000
Epoch 8/40
 - 3s - loss: 0.1380 - acc: 0.9734 - val_loss: 0.1389 - val_acc: 1.0000
Epoch 9/40
 - 3s - loss: 0.1372 - acc: 0.9737 - val_loss: 0.1217 - val_acc: 1.0000
Epoch 10/40
 - 3s - loss: 0.1365 - acc: 0.9737 - val_loss: 0.1428 - val_acc: 1.0000
Epoch 11/40
 - 3s - loss: 0.1356 - acc: 0.9736 - val_loss: 0.1393 - val_acc: 1.0000
Epoch 12/40
 - 3s - loss: 0.1354 - acc: 0.9738 - val_loss: 0.1399 - val_acc: 1.0000
Epoch 13/40
 - 3s - loss: 0.1351 - acc: 0.9739 - val_loss: 0.1534 - val_acc: 1.0000
Epoch 14/40
 - 3s - loss: 0.1342 - acc: 0.9738 - val_loss: 0.1243 - val_acc: 1.0000
Epoch 15/40
 - 3s - loss: 0.1346 - acc: 0.9740 - val_loss: 0.1477 - val_acc: 1.0000
Epoch 16/40
 - 3s - loss: 0.1333 - acc: 0.9741 - val_loss: 0.1305 - val_acc: 1.0000
Epoch 17/40
 - 3s - loss: 0.1336 - acc: 0.9741 - val_loss: 0.1273 - val_acc: 1.0000
Epoch 18/40
 - 3s - loss: 0.1329 - acc: 0.9743 - val_loss: 0.1506 - val_acc: 1.0000
Epoch 19/40
 - 3s - loss: 0.1325 - acc: 0.9742 - val_loss: 0.1216 - val_acc: 1.0000
Epoch 20/40
 - 3s - loss: 0.1323 - acc: 0.9744 - val_loss: 0.1366 - val_acc: 1.0000
Epoch 21/40
 - 3s - loss: 0.1319 - acc: 0.9744 - val_loss: 0.1214 - val_acc: 1.0000
Epoch 22/40
 - 3s - loss: 0.1319 - acc: 0.9743 - val_loss: 0.1386 - val_acc: 1.0000
Epoch 23/40
 - 3s - loss: 0.1312 - acc: 0.9745 - val_loss: 0.1540 - val_acc: 1.0000
Epoch 24/40
 - 3s - loss: 0.1310 - acc: 0.9746 - val_loss: 0.1436 - val_acc: 1.0000
Epoch 25/40
 - 3s - loss: 0.1312 - acc: 0.9745 - val_loss: 0.1447 - val_acc: 1.0000
Epoch 26/40
 - 3s - loss: 0.1308 - acc: 0.9745 - val_loss: 0.1433 - val_acc: 1.0000
Epoch 27/40
 - 3s - loss: 0.1310 - acc: 0.9746 - val_loss: 0.1458 - val_acc: 1.0000
Epoch 28/40
 - 3s - loss: 0.1305 - acc: 0.9747 - val_loss: 0.1429 - val_acc: 1.0000
Epoch 29/40
 - 3s - loss: 0.1301 - acc: 0.9746 - val_loss: 0.1251 - val_acc: 1.0000
Epoch 30/40
 - 3s - loss: 0.1301 - acc: 0.9747 - val_loss: 0.1458 - val_acc: 1.0000
Epoch 31/40
 - 3s - loss: 0.1304 - acc: 0.9746 - val_loss: 0.1448 - val_acc: 1.0000
Epoch 32/40
 - 3s - loss: 0.1304 - acc: 0.9748 - val_loss: 0.1321 - val_acc: 1.0000
Epoch 33/40
 - 3s - loss: 0.1299 - acc: 0.9748 - val_loss: 0.1487 - val_acc: 1.0000
Epoch 34/40
 - 3s - loss: 0.1299 - acc: 0.9749 - val_loss: 0.1525 - val_acc: 1.0000
Epoch 35/40
 - 3s - loss: 0.1298 - acc: 0.9747 - val_loss: 0.1471 - val_acc: 1.0000
Epoch 36/40
 - 3s - loss: 0.1297 - acc: 0.9749 - val_loss: 0.1551 - val_acc: 1.0000
Epoch 37/40
 - 3s - loss: 0.1289 - acc: 0.9748 - val_loss: 0.1351 - val_acc: 1.0000
Epoch 38/40
 - 3s - loss: 0.1292 - acc: 0.9748 - val_loss: 0.1488 - val_acc: 1.0000
Epoch 39/40
 - 3s - loss: 0.1294 - acc: 0.9749 - val_loss: 0.1511 - val_acc: 1.0000
Epoch 40/40
 - 3s - loss: 0.1292 - acc: 0.9749 - val_loss: 0.1544 - val_acc: 1.0000

==================================================================================================
	Training time : 0:02:14.701941
==================================================================================================
	Identification : 0.755
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 43
	25 : 4
	50 : 1
	None : 113
	5 : 54

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 103
	25 : 2
	50 : 1
	None : 29
	5 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : FR
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 3878, Test : 1667
	MWEs in tain : 1557, occurrences : 4462
	Impotant words in tain : 1138
	MWE length mean : 2.28
	Seen MWEs : 230 (46 %)
	New MWEs : 270 (54 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7417 * POS : 244
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 200)       1483400     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 15)        3660        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 800)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 60)           0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 860)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 25)           21525       concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 25)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            208         dropout_7[0][0]                  
==================================================================================================
Total params: 1,508,793
Trainable params: 1,508,793
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	3878 importatnt sents of 3878
	data size before sampling = 235153
	data size after sampling = 807555
{0.0: 115365, 1.0: 115365, 2.0: 115365, 4.0: 115365, 5.0: 115365, 6.0: 115365, 7.0: 115365}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 726799 samples, validate on 80756 samples
Epoch 1/40
 - 18s - loss: 0.1367 - acc: 0.9831 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 18s - loss: 0.0750 - acc: 0.9897 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 18s - loss: 0.0683 - acc: 0.9906 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 18s - loss: 0.0654 - acc: 0.9910 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 18s - loss: 0.0631 - acc: 0.9913 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 18s - loss: 0.0617 - acc: 0.9915 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 18s - loss: 0.0610 - acc: 0.9916 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 18s - loss: 0.0605 - acc: 0.9916 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 18s - loss: 0.0595 - acc: 0.9918 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 18s - loss: 0.0589 - acc: 0.9918 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 18s - loss: 0.0584 - acc: 0.9919 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 18s - loss: 0.0579 - acc: 0.9919 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 18s - loss: 0.0578 - acc: 0.9919 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 18s - loss: 0.0574 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 18s - loss: 0.0571 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 18s - loss: 0.0570 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 18s - loss: 0.0565 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 18s - loss: 0.0564 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 18s - loss: 0.0563 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 18s - loss: 0.0563 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 18s - loss: 0.0560 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 18s - loss: 0.0558 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 18s - loss: 0.0556 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 18s - loss: 0.0555 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 18s - loss: 0.0553 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 18s - loss: 0.0554 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 18s - loss: 0.0551 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 18s - loss: 0.0551 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 18s - loss: 0.0549 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 18s - loss: 0.0547 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 18s - loss: 0.0548 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 18s - loss: 0.0548 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 18s - loss: 0.0544 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 18s - loss: 0.0544 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 18s - loss: 0.0543 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 18s - loss: 0.0542 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 18s - loss: 0.0544 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 18s - loss: 0.0542 - acc: 0.9925 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 18s - loss: 0.0540 - acc: 0.9925 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 18s - loss: 0.0541 - acc: 0.9925 - val_loss: 1.1921e-06 - val_acc: 1.0000

==================================================================================================
	Training time : 0:13:58.477428
==================================================================================================
	Identification : 0.541
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	100 : 2
	50 : 2
	None : 70
	5 : 32
	25 : 3

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 168
	100 : 1
	50 : 1
	None : 19
	5 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HE
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 1030, Test : 2327
	MWEs in tain : 1008, occurrences : 1282
	Impotant words in tain : 1912
	MWE length mean : 2.67
	Seen MWEs : 125 (25 %)
	New MWEs : 375 (75 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 6055 * POS : 25
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 200)       1211000     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 15)        375         input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 800)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 60)           0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 860)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 25)           21525       concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 25)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            208         dropout_8[0][0]                  
==================================================================================================
Total params: 1,233,108
Trainable params: 1,233,108
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1030 importatnt sents of 1030
	data size before sampling = 51038
	data size after sampling = 174475
{0.0: 24925, 1.0: 24925, 2.0: 24925, 3.0: 24925, 5.0: 24925, 6.0: 24925, 7.0: 24925}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 157027 samples, validate on 17448 samples
Epoch 1/40
 - 4s - loss: 0.4232 - acc: 0.9594 - val_loss: 0.1536 - val_acc: 0.9954
Epoch 2/40
 - 4s - loss: 0.1533 - acc: 0.9819 - val_loss: 0.2598 - val_acc: 0.9891
Epoch 3/40
 - 4s - loss: 0.1283 - acc: 0.9859 - val_loss: 0.2097 - val_acc: 0.9891
Epoch 4/40
 - 4s - loss: 0.1184 - acc: 0.9874 - val_loss: 0.1584 - val_acc: 0.9954
Epoch 5/40
 - 4s - loss: 0.1138 - acc: 0.9880 - val_loss: 0.1494 - val_acc: 0.9954
Epoch 6/40
 - 4s - loss: 0.1089 - acc: 0.9886 - val_loss: 0.1772 - val_acc: 0.9954
Epoch 7/40
 - 4s - loss: 0.1075 - acc: 0.9888 - val_loss: 0.1756 - val_acc: 0.9954
Epoch 8/40
 - 4s - loss: 0.1047 - acc: 0.9891 - val_loss: 0.1785 - val_acc: 0.9855
Epoch 9/40
 - 4s - loss: 0.1023 - acc: 0.9893 - val_loss: 0.2437 - val_acc: 0.9855
Epoch 10/40
 - 4s - loss: 0.1013 - acc: 0.9894 - val_loss: 0.1953 - val_acc: 0.9855
Epoch 11/40
 - 4s - loss: 0.0982 - acc: 0.9895 - val_loss: 0.2281 - val_acc: 0.9855
Epoch 12/40
 - 4s - loss: 0.0952 - acc: 0.9894 - val_loss: 0.1810 - val_acc: 0.9954
Epoch 13/40
 - 4s - loss: 0.0935 - acc: 0.9897 - val_loss: 0.1855 - val_acc: 0.9919
Epoch 14/40
 - 4s - loss: 0.0930 - acc: 0.9896 - val_loss: 0.1893 - val_acc: 0.9919
Epoch 15/40
 - 4s - loss: 0.0918 - acc: 0.9898 - val_loss: 0.2132 - val_acc: 0.9919
Epoch 16/40
 - 4s - loss: 0.0910 - acc: 0.9898 - val_loss: 0.1865 - val_acc: 0.9919
Epoch 17/40
 - 4s - loss: 0.0899 - acc: 0.9900 - val_loss: 0.1934 - val_acc: 0.9919
Epoch 18/40
 - 4s - loss: 0.0900 - acc: 0.9899 - val_loss: 0.2006 - val_acc: 0.9855
Epoch 19/40
 - 4s - loss: 0.0890 - acc: 0.9900 - val_loss: 0.1881 - val_acc: 0.9954
Epoch 20/40
 - 4s - loss: 0.0879 - acc: 0.9901 - val_loss: 0.1912 - val_acc: 0.9919
Epoch 21/40
 - 4s - loss: 0.0861 - acc: 0.9900 - val_loss: 0.2063 - val_acc: 0.9855
Epoch 22/40
 - 4s - loss: 0.0872 - acc: 0.9900 - val_loss: 0.1878 - val_acc: 0.9919
Epoch 23/40
 - 4s - loss: 0.0871 - acc: 0.9901 - val_loss: 0.2193 - val_acc: 0.9891
Epoch 24/40
 - 4s - loss: 0.0860 - acc: 0.9902 - val_loss: 0.2055 - val_acc: 0.9919
Epoch 25/40
 - 4s - loss: 0.0854 - acc: 0.9902 - val_loss: 0.1892 - val_acc: 0.9954
Epoch 26/40
 - 4s - loss: 0.0859 - acc: 0.9900 - val_loss: 0.1829 - val_acc: 0.9919
Epoch 27/40
 - 4s - loss: 0.0863 - acc: 0.9903 - val_loss: 0.2082 - val_acc: 0.9855
Epoch 28/40
 - 4s - loss: 0.0855 - acc: 0.9902 - val_loss: 0.1955 - val_acc: 0.9954
Epoch 29/40
 - 4s - loss: 0.0850 - acc: 0.9902 - val_loss: 0.1975 - val_acc: 0.9919
Epoch 30/40
 - 4s - loss: 0.0834 - acc: 0.9904 - val_loss: 0.1938 - val_acc: 0.9919
Epoch 31/40
 - 4s - loss: 0.0841 - acc: 0.9902 - val_loss: 0.1965 - val_acc: 0.9919
Epoch 32/40
 - 4s - loss: 0.0832 - acc: 0.9905 - val_loss: 0.1879 - val_acc: 0.9919
Epoch 33/40
 - 4s - loss: 0.0833 - acc: 0.9904 - val_loss: 0.1906 - val_acc: 0.9919
Epoch 34/40
 - 4s - loss: 0.0832 - acc: 0.9905 - val_loss: 0.1998 - val_acc: 0.9919
Epoch 35/40
 - 4s - loss: 0.0829 - acc: 0.9904 - val_loss: 0.1868 - val_acc: 0.9919
Epoch 36/40
 - 4s - loss: 0.0817 - acc: 0.9905 - val_loss: 0.1797 - val_acc: 0.9954
Epoch 37/40
 - 4s - loss: 0.0834 - acc: 0.9904 - val_loss: 0.1896 - val_acc: 0.9919
Epoch 38/40
 - 4s - loss: 0.0822 - acc: 0.9905 - val_loss: 0.1827 - val_acc: 0.9919
Epoch 39/40
 - 4s - loss: 0.0821 - acc: 0.9905 - val_loss: 0.1995 - val_acc: 0.9919
Epoch 40/40
 - 4s - loss: 0.0818 - acc: 0.9904 - val_loss: 0.1889 - val_acc: 0.9954

==================================================================================================
	Training time : 0:02:53.858626
==================================================================================================
	Identification : 0.161
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 3
	None : 31
	5 : 6

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 327
	None : 29
	5 : 3

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : HU
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 1945, Test : 742
	MWEs in tain : 501, occurrences : 2999
	Impotant words in tain : 455
	MWE length mean : 1.28
	Seen MWEs : 395 (79 %)
	New MWEs : 105 (21 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 2816 * POS : 44
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 200)       563200      input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 15)        660         input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 800)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 60)           0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 860)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 25)           21525       concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 25)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            208         dropout_9[0][0]                  
==================================================================================================
Total params: 585,593
Trainable params: 585,593
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1945 importatnt sents of 1945
	data size before sampling = 113031
	data size after sampling = 275120
{0.0: 55024, 1.0: 55024, 2.0: 55024, 3.0: 55024, 6.0: 55024}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 247608 samples, validate on 27512 samples
Epoch 1/40
 - 6s - loss: 0.2129 - acc: 0.9712 - val_loss: 0.0099 - val_acc: 1.0000
Epoch 2/40
 - 6s - loss: 0.1313 - acc: 0.9819 - val_loss: 0.0140 - val_acc: 1.0000
Epoch 3/40
 - 6s - loss: 0.1244 - acc: 0.9828 - val_loss: 0.0131 - val_acc: 1.0000
Epoch 4/40
 - 6s - loss: 0.1200 - acc: 0.9834 - val_loss: 0.0112 - val_acc: 1.0000
Epoch 5/40
 - 6s - loss: 0.1180 - acc: 0.9838 - val_loss: 0.0124 - val_acc: 1.0000
Epoch 6/40
 - 6s - loss: 0.1164 - acc: 0.9840 - val_loss: 0.0144 - val_acc: 1.0000
Epoch 7/40
 - 6s - loss: 0.1148 - acc: 0.9841 - val_loss: 0.0143 - val_acc: 1.0000
Epoch 8/40
 - 6s - loss: 0.1139 - acc: 0.9843 - val_loss: 0.0171 - val_acc: 1.0000
Epoch 9/40
 - 6s - loss: 0.1127 - acc: 0.9845 - val_loss: 0.0134 - val_acc: 1.0000
Epoch 10/40
 - 6s - loss: 0.1122 - acc: 0.9846 - val_loss: 0.0120 - val_acc: 1.0000
Epoch 11/40
 - 6s - loss: 0.1113 - acc: 0.9848 - val_loss: 0.0140 - val_acc: 1.0000
Epoch 12/40
 - 6s - loss: 0.1107 - acc: 0.9849 - val_loss: 0.0193 - val_acc: 1.0000
Epoch 13/40
 - 6s - loss: 0.1099 - acc: 0.9848 - val_loss: 0.0172 - val_acc: 1.0000
Epoch 14/40
 - 6s - loss: 0.1097 - acc: 0.9850 - val_loss: 0.0150 - val_acc: 1.0000
Epoch 15/40
 - 6s - loss: 0.1088 - acc: 0.9850 - val_loss: 0.0140 - val_acc: 1.0000
Epoch 16/40
 - 6s - loss: 0.1092 - acc: 0.9850 - val_loss: 0.0163 - val_acc: 1.0000
Epoch 17/40
 - 6s - loss: 0.1088 - acc: 0.9851 - val_loss: 0.0163 - val_acc: 1.0000
Epoch 18/40
 - 6s - loss: 0.1083 - acc: 0.9853 - val_loss: 0.0161 - val_acc: 1.0000
Epoch 19/40
 - 6s - loss: 0.1083 - acc: 0.9853 - val_loss: 0.0176 - val_acc: 1.0000
Epoch 20/40
 - 6s - loss: 0.1075 - acc: 0.9854 - val_loss: 0.0188 - val_acc: 1.0000
Epoch 21/40
 - 6s - loss: 0.1073 - acc: 0.9853 - val_loss: 0.0152 - val_acc: 1.0000
Epoch 22/40
 - 6s - loss: 0.1075 - acc: 0.9854 - val_loss: 0.0143 - val_acc: 1.0000
Epoch 23/40
 - 6s - loss: 0.1071 - acc: 0.9854 - val_loss: 0.0164 - val_acc: 1.0000
Epoch 24/40
 - 6s - loss: 0.1069 - acc: 0.9853 - val_loss: 0.0165 - val_acc: 1.0000
Epoch 25/40
 - 6s - loss: 0.1069 - acc: 0.9855 - val_loss: 0.0184 - val_acc: 1.0000
Epoch 26/40
 - 6s - loss: 0.1068 - acc: 0.9855 - val_loss: 0.0158 - val_acc: 1.0000
Epoch 27/40
 - 6s - loss: 0.1066 - acc: 0.9855 - val_loss: 0.0175 - val_acc: 1.0000
Epoch 28/40
 - 6s - loss: 0.1067 - acc: 0.9855 - val_loss: 0.0207 - val_acc: 1.0000
Epoch 29/40
 - 6s - loss: 0.1064 - acc: 0.9855 - val_loss: 0.0187 - val_acc: 1.0000
Epoch 30/40
 - 6s - loss: 0.1064 - acc: 0.9855 - val_loss: 0.0160 - val_acc: 1.0000
Epoch 31/40
 - 6s - loss: 0.1060 - acc: 0.9855 - val_loss: 0.0160 - val_acc: 1.0000
Epoch 32/40
 - 6s - loss: 0.1060 - acc: 0.9857 - val_loss: 0.0178 - val_acc: 1.0000
Epoch 33/40
 - 6s - loss: 0.1055 - acc: 0.9855 - val_loss: 0.0201 - val_acc: 1.0000
Epoch 34/40
 - 6s - loss: 0.1059 - acc: 0.9857 - val_loss: 0.0175 - val_acc: 1.0000
Epoch 35/40
 - 6s - loss: 0.1054 - acc: 0.9857 - val_loss: 0.0171 - val_acc: 1.0000
Epoch 36/40
 - 6s - loss: 0.1056 - acc: 0.9856 - val_loss: 0.0182 - val_acc: 1.0000
Epoch 37/40
 - 6s - loss: 0.1053 - acc: 0.9857 - val_loss: 0.0190 - val_acc: 1.0000
Epoch 38/40
 - 6s - loss: 0.1050 - acc: 0.9857 - val_loss: 0.0160 - val_acc: 1.0000
Epoch 39/40
 - 6s - loss: 0.1043 - acc: 0.9858 - val_loss: 0.0167 - val_acc: 1.0000
Epoch 40/40
 - 6s - loss: 0.1050 - acc: 0.9857 - val_loss: 0.0186 - val_acc: 1.0000

==================================================================================================
	Training time : 0:04:16.844877
==================================================================================================
	Identification : 0.668
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	100 : 3
	5 : 29
	50 : 2
	500 : 1
	25 : 7
	None : 36

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 84
	100 : 2
	5 : 16
	50 : 2
	500 : 1
	25 : 4
	None : 23

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : IT
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 1703, Test : 1272
	MWEs in tain : 1294, occurrences : 1954
	Impotant words in tain : 1071
	MWE length mean : 2.58
	Seen MWEs : 181 (36 %)
	New MWEs : 319 (63 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 6039 * POS : 241
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 200)       1207800     input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 15)        3615        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 800)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 60)           0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 860)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 25)           21525       concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 25)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            208         dropout_10[0][0]                 
==================================================================================================
Total params: 1,233,148
Trainable params: 1,233,148
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1703 importatnt sents of 1703
	data size before sampling = 136464
	data size after sampling = 538080
{0.0: 67260, 1.0: 67260, 2.0: 67260, 3.0: 67260, 4.0: 67260, 5.0: 67260, 6.0: 67260, 7.0: 67260}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 484272 samples, validate on 53808 samples
Epoch 1/40
 - 12s - loss: 0.1521 - acc: 0.9841 - val_loss: 4.2367e-05 - val_acc: 1.0000
Epoch 2/40
 - 12s - loss: 0.0722 - acc: 0.9905 - val_loss: 1.6667e-05 - val_acc: 1.0000
Epoch 3/40
 - 12s - loss: 0.0650 - acc: 0.9915 - val_loss: 9.8297e-06 - val_acc: 1.0000
Epoch 4/40
 - 12s - loss: 0.0615 - acc: 0.9922 - val_loss: 5.8047e-06 - val_acc: 1.0000
Epoch 5/40
 - 12s - loss: 0.0587 - acc: 0.9925 - val_loss: 3.5742e-06 - val_acc: 1.0000
Epoch 6/40
 - 12s - loss: 0.0576 - acc: 0.9928 - val_loss: 2.2320e-06 - val_acc: 1.0000
Epoch 7/40
 - 12s - loss: 0.0564 - acc: 0.9930 - val_loss: 2.3806e-06 - val_acc: 1.0000
Epoch 8/40
 - 12s - loss: 0.0551 - acc: 0.9932 - val_loss: 1.7863e-06 - val_acc: 1.0000
Epoch 9/40
 - 12s - loss: 0.0549 - acc: 0.9932 - val_loss: 1.6378e-06 - val_acc: 1.0000
Epoch 10/40
 - 12s - loss: 0.0537 - acc: 0.9934 - val_loss: 1.4892e-06 - val_acc: 1.0000
Epoch 11/40
 - 12s - loss: 0.0535 - acc: 0.9934 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 12s - loss: 0.0531 - acc: 0.9935 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 12s - loss: 0.0525 - acc: 0.9936 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 12s - loss: 0.0524 - acc: 0.9936 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 12s - loss: 0.0524 - acc: 0.9936 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 12s - loss: 0.0518 - acc: 0.9937 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 12s - loss: 0.0518 - acc: 0.9937 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 12s - loss: 0.0513 - acc: 0.9937 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 12s - loss: 0.0509 - acc: 0.9938 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 12s - loss: 0.0508 - acc: 0.9938 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 12s - loss: 0.0508 - acc: 0.9939 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 12s - loss: 0.0506 - acc: 0.9939 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 12s - loss: 0.0506 - acc: 0.9939 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 12s - loss: 0.0503 - acc: 0.9939 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 12s - loss: 0.0500 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 12s - loss: 0.0502 - acc: 0.9939 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 12s - loss: 0.0499 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 12s - loss: 0.0499 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 12s - loss: 0.0498 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 12s - loss: 0.0496 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 12s - loss: 0.0495 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 12s - loss: 0.0497 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 12s - loss: 0.0493 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 12s - loss: 0.0491 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 12s - loss: 0.0493 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 12s - loss: 0.0490 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 12s - loss: 0.0491 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 12s - loss: 0.0488 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 12s - loss: 0.0490 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 12s - loss: 0.0488 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000

==================================================================================================
	Training time : 0:08:47.983068
==================================================================================================
	Identification : 0.381
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 1
	50 : 1
	None : 71
	5 : 13

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 291
	None : 32
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : LT
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 394, Test : 2710
	MWEs in tain : 351, occurrences : 402
	Impotant words in tain : 614
	MWE length mean : 2.37
	Seen MWEs : 20 (20 %)
	New MWEs : 80 (80 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 1899 * POS : 11
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 200)       379800      input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 15)        165         input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 800)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 60)           0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 860)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 25)           21525       concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 25)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            208         dropout_11[0][0]                 
==================================================================================================
Total params: 401,698
Trainable params: 401,698
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	394 importatnt sents of 394
	data size before sampling = 18576
	data size after sampling = 45435
{0.0: 9087, 1.0: 9087, 2.0: 9087, 5.0: 9087, 6.0: 9087}
	Class weights : {0: 1, 1: 1, 2: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 40891 samples, validate on 4544 samples
Epoch 1/40
 - 1s - loss: 0.4495 - acc: 0.9408 - val_loss: 0.0208 - val_acc: 1.0000
Epoch 2/40
 - 1s - loss: 0.1276 - acc: 0.9819 - val_loss: 0.0144 - val_acc: 1.0000
Epoch 3/40
 - 1s - loss: 0.1091 - acc: 0.9857 - val_loss: 0.0241 - val_acc: 1.0000
Epoch 4/40
 - 1s - loss: 0.0977 - acc: 0.9871 - val_loss: 0.0225 - val_acc: 1.0000
Epoch 5/40
 - 1s - loss: 0.0941 - acc: 0.9877 - val_loss: 0.0261 - val_acc: 1.0000
Epoch 6/40
 - 1s - loss: 0.0920 - acc: 0.9880 - val_loss: 0.0336 - val_acc: 1.0000
Epoch 7/40
 - 1s - loss: 0.0906 - acc: 0.9881 - val_loss: 0.0270 - val_acc: 1.0000
Epoch 8/40
 - 1s - loss: 0.0865 - acc: 0.9883 - val_loss: 0.0317 - val_acc: 1.0000
Epoch 9/40
 - 1s - loss: 0.0859 - acc: 0.9885 - val_loss: 0.0294 - val_acc: 1.0000
Epoch 10/40
 - 1s - loss: 0.0843 - acc: 0.9887 - val_loss: 0.0245 - val_acc: 1.0000
Epoch 11/40
 - 1s - loss: 0.0860 - acc: 0.9889 - val_loss: 0.0319 - val_acc: 1.0000
Epoch 12/40
 - 1s - loss: 0.0829 - acc: 0.9888 - val_loss: 0.0257 - val_acc: 1.0000
Epoch 13/40
 - 1s - loss: 0.0835 - acc: 0.9887 - val_loss: 0.0245 - val_acc: 1.0000
Epoch 14/40
 - 1s - loss: 0.0825 - acc: 0.9890 - val_loss: 0.0370 - val_acc: 1.0000
Epoch 15/40
 - 1s - loss: 0.0817 - acc: 0.9890 - val_loss: 0.0284 - val_acc: 1.0000
Epoch 16/40
 - 1s - loss: 0.0819 - acc: 0.9888 - val_loss: 0.0344 - val_acc: 1.0000
Epoch 17/40
 - 1s - loss: 0.0816 - acc: 0.9890 - val_loss: 0.0297 - val_acc: 1.0000
Epoch 18/40
 - 1s - loss: 0.0831 - acc: 0.9888 - val_loss: 0.0274 - val_acc: 1.0000
Epoch 19/40
 - 1s - loss: 0.0793 - acc: 0.9890 - val_loss: 0.0341 - val_acc: 1.0000
Epoch 20/40
 - 1s - loss: 0.0811 - acc: 0.9890 - val_loss: 0.0273 - val_acc: 1.0000
Epoch 21/40
 - 1s - loss: 0.0813 - acc: 0.9890 - val_loss: 0.0277 - val_acc: 1.0000
Epoch 22/40
 - 1s - loss: 0.0809 - acc: 0.9889 - val_loss: 0.0353 - val_acc: 1.0000
Epoch 23/40
 - 1s - loss: 0.0800 - acc: 0.9891 - val_loss: 0.0381 - val_acc: 1.0000
Epoch 24/40
 - 1s - loss: 0.0795 - acc: 0.9892 - val_loss: 0.0306 - val_acc: 1.0000
Epoch 25/40
 - 1s - loss: 0.0801 - acc: 0.9891 - val_loss: 0.0320 - val_acc: 1.0000
Epoch 26/40
 - 1s - loss: 0.0798 - acc: 0.9891 - val_loss: 0.0327 - val_acc: 1.0000
Epoch 27/40
 - 1s - loss: 0.0802 - acc: 0.9891 - val_loss: 0.0291 - val_acc: 1.0000
Epoch 28/40
 - 1s - loss: 0.0800 - acc: 0.9891 - val_loss: 0.0284 - val_acc: 1.0000
Epoch 29/40
 - 1s - loss: 0.0805 - acc: 0.9890 - val_loss: 0.0260 - val_acc: 1.0000
Epoch 30/40
 - 1s - loss: 0.0795 - acc: 0.9892 - val_loss: 0.0283 - val_acc: 1.0000
Epoch 31/40
 - 1s - loss: 0.0788 - acc: 0.9892 - val_loss: 0.0240 - val_acc: 1.0000
Epoch 32/40
 - 1s - loss: 0.0789 - acc: 0.9891 - val_loss: 0.0353 - val_acc: 1.0000
Epoch 33/40
 - 1s - loss: 0.0781 - acc: 0.9893 - val_loss: 0.0395 - val_acc: 1.0000
Epoch 34/40
 - 1s - loss: 0.0784 - acc: 0.9893 - val_loss: 0.0302 - val_acc: 1.0000
Epoch 35/40
 - 1s - loss: 0.0782 - acc: 0.9893 - val_loss: 0.0355 - val_acc: 1.0000
Epoch 36/40
 - 1s - loss: 0.0775 - acc: 0.9893 - val_loss: 0.0383 - val_acc: 1.0000
Epoch 37/40
 - 1s - loss: 0.0782 - acc: 0.9892 - val_loss: 0.0361 - val_acc: 1.0000
Epoch 38/40
 - 1s - loss: 0.0773 - acc: 0.9892 - val_loss: 0.0364 - val_acc: 1.0000
Epoch 39/40
 - 1s - loss: 0.0785 - acc: 0.9893 - val_loss: 0.0301 - val_acc: 1.0000
Epoch 40/40
 - 1s - loss: 0.0782 - acc: 0.9892 - val_loss: 0.0268 - val_acc: 1.0000

==================================================================================================
	Training time : 0:00:41.302911
==================================================================================================
	Identification : 0.286
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	None : 17
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 79

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : MT
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 689, Test : 4635
	MWEs in tain : 711, occurrences : 772
	Impotant words in tain : 860
	MWE length mean : 2.6
	Seen MWEs : 77 (15 %)
	New MWEs : 423 (84 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 3039 * POS : 14
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 200)       607800      input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 15)        210         input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 800)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 60)           0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 860)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 25)           21525       concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 25)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            208         dropout_12[0][0]                 
==================================================================================================
Total params: 629,743
Trainable params: 629,743
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	689 importatnt sents of 689
	data size before sampling = 43707
	data size after sampling = 128814
{0.0: 21469, 1.0: 21469, 2.0: 21469, 5.0: 21469, 6.0: 21469, 7.0: 21469}
	Class weights : {0: 1, 1: 1, 2: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 115932 samples, validate on 12882 samples
Epoch 1/40
 - 3s - loss: 0.3861 - acc: 0.9620 - val_loss: 0.0648 - val_acc: 1.0000
Epoch 2/40
 - 3s - loss: 0.1314 - acc: 0.9813 - val_loss: 0.0524 - val_acc: 1.0000
Epoch 3/40
 - 3s - loss: 0.1108 - acc: 0.9848 - val_loss: 0.0347 - val_acc: 1.0000
Epoch 4/40
 - 3s - loss: 0.1014 - acc: 0.9863 - val_loss: 0.0668 - val_acc: 1.0000
Epoch 5/40
 - 3s - loss: 0.0924 - acc: 0.9874 - val_loss: 0.0782 - val_acc: 1.0000
Epoch 6/40
 - 3s - loss: 0.0891 - acc: 0.9880 - val_loss: 0.0766 - val_acc: 1.0000
Epoch 7/40
 - 3s - loss: 0.0864 - acc: 0.9885 - val_loss: 0.0502 - val_acc: 1.0000
Epoch 8/40
 - 3s - loss: 0.0827 - acc: 0.9890 - val_loss: 0.1821 - val_acc: 0.9864
Epoch 9/40
 - 3s - loss: 0.0822 - acc: 0.9893 - val_loss: 0.0742 - val_acc: 1.0000
Epoch 10/40
 - 3s - loss: 0.0807 - acc: 0.9895 - val_loss: 0.0753 - val_acc: 1.0000
Epoch 11/40
 - 3s - loss: 0.0795 - acc: 0.9895 - val_loss: 0.0510 - val_acc: 1.0000
Epoch 12/40
 - 3s - loss: 0.0783 - acc: 0.9896 - val_loss: 0.0559 - val_acc: 1.0000
Epoch 13/40
 - 3s - loss: 0.0778 - acc: 0.9899 - val_loss: 0.0748 - val_acc: 1.0000
Epoch 14/40
 - 3s - loss: 0.0769 - acc: 0.9899 - val_loss: 0.0459 - val_acc: 1.0000
Epoch 15/40
 - 3s - loss: 0.0769 - acc: 0.9898 - val_loss: 0.0682 - val_acc: 1.0000
Epoch 16/40
 - 3s - loss: 0.0760 - acc: 0.9900 - val_loss: 0.0661 - val_acc: 1.0000
Epoch 17/40
 - 3s - loss: 0.0750 - acc: 0.9903 - val_loss: 0.0729 - val_acc: 1.0000
Epoch 18/40
 - 3s - loss: 0.0749 - acc: 0.9904 - val_loss: 0.0657 - val_acc: 1.0000
Epoch 19/40
 - 3s - loss: 0.0745 - acc: 0.9902 - val_loss: 0.0672 - val_acc: 1.0000
Epoch 20/40
 - 3s - loss: 0.0749 - acc: 0.9904 - val_loss: 0.0654 - val_acc: 1.0000
Epoch 21/40
 - 3s - loss: 0.0741 - acc: 0.9903 - val_loss: 0.0693 - val_acc: 1.0000
Epoch 22/40
 - 3s - loss: 0.0732 - acc: 0.9905 - val_loss: 0.0747 - val_acc: 1.0000
Epoch 23/40
 - 3s - loss: 0.0733 - acc: 0.9905 - val_loss: 0.0667 - val_acc: 1.0000
Epoch 24/40
 - 3s - loss: 0.0721 - acc: 0.9905 - val_loss: 0.0807 - val_acc: 1.0000
Epoch 25/40
 - 3s - loss: 0.0723 - acc: 0.9905 - val_loss: 0.0724 - val_acc: 1.0000
Epoch 26/40
 - 3s - loss: 0.0719 - acc: 0.9908 - val_loss: 0.0698 - val_acc: 1.0000
Epoch 27/40
 - 3s - loss: 0.0724 - acc: 0.9905 - val_loss: 0.0698 - val_acc: 1.0000
Epoch 28/40
 - 3s - loss: 0.0720 - acc: 0.9907 - val_loss: 0.0751 - val_acc: 1.0000
Epoch 29/40
 - 3s - loss: 0.0728 - acc: 0.9906 - val_loss: 0.0679 - val_acc: 1.0000
Epoch 30/40
 - 3s - loss: 0.0723 - acc: 0.9908 - val_loss: 0.0632 - val_acc: 1.0000
Epoch 31/40
 - 3s - loss: 0.0716 - acc: 0.9908 - val_loss: 0.0746 - val_acc: 1.0000
Epoch 32/40
 - 3s - loss: 0.0721 - acc: 0.9907 - val_loss: 0.0661 - val_acc: 1.0000
Epoch 33/40
 - 3s - loss: 0.0712 - acc: 0.9908 - val_loss: 0.0704 - val_acc: 1.0000
Epoch 34/40
 - 3s - loss: 0.0707 - acc: 0.9908 - val_loss: 0.0683 - val_acc: 1.0000
Epoch 35/40
 - 3s - loss: 0.0711 - acc: 0.9909 - val_loss: 0.0605 - val_acc: 1.0000
Epoch 36/40
 - 3s - loss: 0.0706 - acc: 0.9908 - val_loss: 0.0659 - val_acc: 1.0000
Epoch 37/40
 - 3s - loss: 0.0707 - acc: 0.9908 - val_loss: 0.0692 - val_acc: 1.0000
Epoch 38/40
 - 3s - loss: 0.0703 - acc: 0.9910 - val_loss: 0.0655 - val_acc: 1.0000
Epoch 39/40
 - 3s - loss: 0.0699 - acc: 0.9910 - val_loss: 0.0627 - val_acc: 1.0000
Epoch 40/40
 - 3s - loss: 0.0709 - acc: 0.9909 - val_loss: 0.0652 - val_acc: 1.0000
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '9446')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--2.7.14-64/lock_dir

==================================================================================================
	Training time : 0:01:57.617936
==================================================================================================
	Identification : 0.121
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	None : 22
	5 : 1

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 393
	None : 24

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PL
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 2774, Test : 2028
	MWEs in tain : 1313, occurrences : 3149
	Impotant words in tain : 1085
	MWE length mean : 2.11
	Seen MWEs : 347 (69 %)
	New MWEs : 153 (30 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 5870 * POS : 107
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 200)       1174000     input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 15)        1605        input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 800)          0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 60)           0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 860)          0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 25)           21525       concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 25)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            208         dropout_13[0][0]                 
==================================================================================================
Total params: 1,197,338
Trainable params: 1,197,338
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	2774 importatnt sents of 2774
	data size before sampling = 115720
	data size after sampling = 337884
{0.0: 56314, 1.0: 56314, 2.0: 56314, 4.0: 56314, 5.0: 56314, 6.0: 56314}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 304095 samples, validate on 33789 samples
Epoch 1/40
 - 7s - loss: 0.2022 - acc: 0.9719 - val_loss: 0.0559 - val_acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.1076 - acc: 0.9834 - val_loss: 0.0588 - val_acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0996 - acc: 0.9845 - val_loss: 0.0527 - val_acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.0948 - acc: 0.9855 - val_loss: 0.0623 - val_acc: 1.0000
Epoch 5/40
 - 7s - loss: 0.0916 - acc: 0.9860 - val_loss: 0.0640 - val_acc: 0.9993
Epoch 6/40
 - 7s - loss: 0.0898 - acc: 0.9866 - val_loss: 0.0493 - val_acc: 1.0000
Epoch 7/40
 - 7s - loss: 0.0886 - acc: 0.9867 - val_loss: 0.0512 - val_acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0874 - acc: 0.9869 - val_loss: 0.0533 - val_acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.0861 - acc: 0.9871 - val_loss: 0.0463 - val_acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.0856 - acc: 0.9871 - val_loss: 0.0650 - val_acc: 0.9993
Epoch 11/40
 - 7s - loss: 0.0848 - acc: 0.9874 - val_loss: 0.0561 - val_acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.0844 - acc: 0.9874 - val_loss: 0.0596 - val_acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0839 - acc: 0.9874 - val_loss: 0.0624 - val_acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0836 - acc: 0.9875 - val_loss: 0.0540 - val_acc: 1.0000
Epoch 15/40
 - 7s - loss: 0.0834 - acc: 0.9876 - val_loss: 0.0600 - val_acc: 1.0000
Epoch 16/40
 - 7s - loss: 0.0832 - acc: 0.9876 - val_loss: 0.0636 - val_acc: 1.0000
Epoch 17/40
 - 7s - loss: 0.0825 - acc: 0.9877 - val_loss: 0.0573 - val_acc: 1.0000
Epoch 18/40
 - 7s - loss: 0.0826 - acc: 0.9878 - val_loss: 0.0573 - val_acc: 1.0000
Epoch 19/40
 - 7s - loss: 0.0822 - acc: 0.9878 - val_loss: 0.0526 - val_acc: 1.0000
Epoch 20/40
 - 7s - loss: 0.0822 - acc: 0.9878 - val_loss: 0.0644 - val_acc: 1.0000
Epoch 21/40
 - 7s - loss: 0.0815 - acc: 0.9879 - val_loss: 0.0621 - val_acc: 1.0000
Epoch 22/40
 - 7s - loss: 0.0816 - acc: 0.9879 - val_loss: 0.0572 - val_acc: 1.0000
Epoch 23/40
 - 7s - loss: 0.0816 - acc: 0.9879 - val_loss: 0.0690 - val_acc: 1.0000
Epoch 24/40
 - 7s - loss: 0.0812 - acc: 0.9879 - val_loss: 0.0568 - val_acc: 1.0000
Epoch 25/40
 - 7s - loss: 0.0812 - acc: 0.9879 - val_loss: 0.0570 - val_acc: 1.0000
Epoch 26/40
 - 7s - loss: 0.0808 - acc: 0.9880 - val_loss: 0.0645 - val_acc: 1.0000
Epoch 27/40
 - 7s - loss: 0.0806 - acc: 0.9881 - val_loss: 0.0562 - val_acc: 1.0000
Epoch 28/40
 - 7s - loss: 0.0805 - acc: 0.9880 - val_loss: 0.0600 - val_acc: 1.0000
Epoch 29/40
 - 7s - loss: 0.0805 - acc: 0.9880 - val_loss: 0.0615 - val_acc: 1.0000
Epoch 30/40
 - 7s - loss: 0.0805 - acc: 0.9881 - val_loss: 0.0601 - val_acc: 1.0000
Epoch 31/40
 - 7s - loss: 0.0801 - acc: 0.9881 - val_loss: 0.0618 - val_acc: 1.0000
Epoch 32/40
 - 7s - loss: 0.0803 - acc: 0.9881 - val_loss: 0.0667 - val_acc: 1.0000
Epoch 33/40
 - 7s - loss: 0.0800 - acc: 0.9881 - val_loss: 0.0644 - val_acc: 1.0000
Epoch 34/40
 - 7s - loss: 0.0795 - acc: 0.9882 - val_loss: 0.0635 - val_acc: 1.0000
Epoch 35/40
 - 7s - loss: 0.0798 - acc: 0.9881 - val_loss: 0.0634 - val_acc: 1.0000
Epoch 36/40
 - 7s - loss: 0.0796 - acc: 0.9882 - val_loss: 0.0571 - val_acc: 1.0000
Epoch 37/40
 - 7s - loss: 0.0796 - acc: 0.9882 - val_loss: 0.0639 - val_acc: 0.9993
Epoch 38/40
 - 7s - loss: 0.0793 - acc: 0.9883 - val_loss: 0.0632 - val_acc: 1.0000
Epoch 39/40
 - 7s - loss: 0.0796 - acc: 0.9883 - val_loss: 0.0642 - val_acc: 1.0000
Epoch 40/40
 - 7s - loss: 0.0792 - acc: 0.9883 - val_loss: 0.0661 - val_acc: 1.0000

==================================================================================================
	Training time : 0:05:33.732628
==================================================================================================
	Identification : 0.612
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 2
	25 : 5
	50 : 5
	None : 49
	5 : 52

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 146
	25 : 1
	50 : 2
	None : 64
	5 : 31

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 3124, Test : 2600
	MWEs in tain : 1700, occurrences : 3447
	Impotant words in tain : 1369
	MWE length mean : 2.19
	Seen MWEs : 319 (63 %)
	New MWEs : 181 (36 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 6181 * POS : 224
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 200)       1236200     input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 15)        3360        input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 800)          0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 60)           0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 860)          0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 25)           21525       concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 25)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            208         dropout_14[0][0]                 
==================================================================================================
Total params: 1,261,293
Trainable params: 1,261,293
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	3124 importatnt sents of 3124
	data size before sampling = 143994
	data size after sampling = 492205
{0.0: 70315, 1.0: 70315, 2.0: 70315, 4.0: 70315, 5.0: 70315, 6.0: 70315, 7.0: 70315}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 442984 samples, validate on 49221 samples
Epoch 1/40
 - 11s - loss: 0.1904 - acc: 0.9765 - val_loss: 4.1240e-05 - val_acc: 1.0000
Epoch 2/40
 - 11s - loss: 0.0905 - acc: 0.9870 - val_loss: 2.2710e-05 - val_acc: 1.0000
Epoch 3/40
 - 11s - loss: 0.0821 - acc: 0.9883 - val_loss: 1.3745e-05 - val_acc: 1.0000
Epoch 4/40
 - 11s - loss: 0.0787 - acc: 0.9888 - val_loss: 9.2648e-06 - val_acc: 1.0000
Epoch 5/40
 - 11s - loss: 0.0759 - acc: 0.9893 - val_loss: 7.7706e-06 - val_acc: 1.0000
Epoch 6/40
 - 11s - loss: 0.0746 - acc: 0.9895 - val_loss: 7.4712e-06 - val_acc: 1.0000
Epoch 7/40
 - 11s - loss: 0.0734 - acc: 0.9897 - val_loss: 5.3782e-06 - val_acc: 1.0000
Epoch 8/40
 - 11s - loss: 0.0726 - acc: 0.9899 - val_loss: 3.5873e-06 - val_acc: 1.0000
Epoch 9/40
 - 11s - loss: 0.0718 - acc: 0.9900 - val_loss: 2.6891e-06 - val_acc: 1.0000
Epoch 10/40
 - 11s - loss: 0.0707 - acc: 0.9901 - val_loss: 2.3897e-06 - val_acc: 1.0000
Epoch 11/40
 - 11s - loss: 0.0699 - acc: 0.9901 - val_loss: 2.3897e-06 - val_acc: 1.0000
Epoch 12/40
 - 11s - loss: 0.0701 - acc: 0.9902 - val_loss: 1.7909e-06 - val_acc: 1.0000
Epoch 13/40
 - 11s - loss: 0.0694 - acc: 0.9903 - val_loss: 1.7909e-06 - val_acc: 1.0000
Epoch 14/40
 - 11s - loss: 0.0695 - acc: 0.9903 - val_loss: 2.0903e-06 - val_acc: 1.0000
Epoch 15/40
 - 11s - loss: 0.0690 - acc: 0.9904 - val_loss: 1.7909e-06 - val_acc: 1.0000
Epoch 16/40
 - 11s - loss: 0.0685 - acc: 0.9905 - val_loss: 1.4915e-06 - val_acc: 1.0000
Epoch 17/40
 - 11s - loss: 0.0682 - acc: 0.9905 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 11s - loss: 0.0675 - acc: 0.9906 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 11s - loss: 0.0680 - acc: 0.9905 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 11s - loss: 0.0676 - acc: 0.9906 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 11s - loss: 0.0672 - acc: 0.9906 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 11s - loss: 0.0674 - acc: 0.9906 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 11s - loss: 0.0669 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 11s - loss: 0.0668 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 11s - loss: 0.0668 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 11s - loss: 0.0668 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 11s - loss: 0.0664 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 11s - loss: 0.0659 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 11s - loss: 0.0663 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 11s - loss: 0.0660 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 11s - loss: 0.0659 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 11s - loss: 0.0659 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 11s - loss: 0.0658 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 11s - loss: 0.0658 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 11s - loss: 0.0654 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 11s - loss: 0.0656 - acc: 0.9909 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 11s - loss: 0.0653 - acc: 0.9909 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 11s - loss: 0.0652 - acc: 0.9909 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 11s - loss: 0.0654 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 11s - loss: 0.0650 - acc: 0.9909 - val_loss: 1.1921e-06 - val_acc: 1.0000

==================================================================================================
	Training time : 0:08:14.193085
==================================================================================================
	Identification : 0.682
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 5
	None : 132
	5 : 63

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 173
	25 : 1
	None : 38
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : RO
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 3614, Test : 6031
	MWEs in tain : 676, occurrences : 4040
	Impotant words in tain : 518
	MWE length mean : 2.15
	Seen MWEs : 450 (90 %)
	New MWEs : 50 (10 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 6324 * POS : 627
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 200)       1264800     input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 15)        9405        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 800)          0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 60)           0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 860)          0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 25)           21525       concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 25)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            208         dropout_15[0][0]                 
==================================================================================================
Total params: 1,295,938
Trainable params: 1,295,938
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	3614 importatnt sents of 3614
	data size before sampling = 277414
	data size after sampling = 956907
{0.0: 136701, 1.0: 136701, 2.0: 136701, 4.0: 136701, 5.0: 136701, 6.0: 136701, 7.0: 136701}
	Class weights : {0: 1, 1: 1, 2: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 861216 samples, validate on 95691 samples
Epoch 1/40
 - 21s - loss: 0.1063 - acc: 0.9887 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 2/40
 - 21s - loss: 0.0596 - acc: 0.9929 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 3/40
 - 21s - loss: 0.0555 - acc: 0.9934 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 4/40
 - 21s - loss: 0.0531 - acc: 0.9936 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 5/40
 - 21s - loss: 0.0517 - acc: 0.9938 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 21s - loss: 0.0508 - acc: 0.9938 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 21s - loss: 0.0499 - acc: 0.9939 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 21s - loss: 0.0495 - acc: 0.9939 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 21s - loss: 0.0491 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 21s - loss: 0.0486 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 21s - loss: 0.0485 - acc: 0.9940 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 21s - loss: 0.0480 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 21s - loss: 0.0478 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 21s - loss: 0.0474 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 21s - loss: 0.0473 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 21s - loss: 0.0473 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 21s - loss: 0.0469 - acc: 0.9941 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 21s - loss: 0.0468 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 21s - loss: 0.0468 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 21s - loss: 0.0464 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 21s - loss: 0.0462 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 21s - loss: 0.0464 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 21s - loss: 0.0462 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 21s - loss: 0.0460 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 21s - loss: 0.0459 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 21s - loss: 0.0461 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 21s - loss: 0.0459 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 21s - loss: 0.0458 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 21s - loss: 0.0456 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 21s - loss: 0.0456 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 21s - loss: 0.0454 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 21s - loss: 0.0454 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 21s - loss: 0.0455 - acc: 0.9942 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 21s - loss: 0.0454 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 21s - loss: 0.0451 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 21s - loss: 0.0453 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 21s - loss: 0.0452 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 21s - loss: 0.0451 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 21s - loss: 0.0450 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 21s - loss: 0.0451 - acc: 0.9943 - val_loss: 1.1921e-06 - val_acc: 1.0000

==================================================================================================
	Training time : 0:16:06.227266
==================================================================================================
	Identification : 0.796
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	100 : 1
	5 : 81
	200 : 1
	300 : 1
	50 : 7
	25 : 9
	None : 52

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 48
	25 : 3
	50 : 2
	None : 14
	5 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : SV
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 46, Test : 1600
	MWEs in tain : 55, occurrences : 56
	Impotant words in tain : 76
	MWE length mean : 2.15
	Seen MWEs : 19 (8 %)
	New MWEs : 217 (91 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 192 * POS : 31
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 200)       38400       input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 15)        465         input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 800)          0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 60)           0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 860)          0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 25)           21525       concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 25)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            208         dropout_16[0][0]                 
==================================================================================================
Total params: 60,598
Trainable params: 60,598
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	46 importatnt sents of 46
	data size before sampling = 1944
	data size after sampling = 6615
{0.0: 945, 1.0: 945, 2.0: 945, 3.0: 945, 4.0: 945, 5.0: 945, 6.0: 945}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 5953 samples, validate on 662 samples
Epoch 1/40
 - 0s - loss: 1.4963 - acc: 0.8507 - val_loss: 0.1259 - val_acc: 1.0000
Epoch 2/40
 - 0s - loss: 0.2325 - acc: 0.9704 - val_loss: 0.0400 - val_acc: 1.0000
Epoch 3/40
 - 0s - loss: 0.1654 - acc: 0.9805 - val_loss: 0.0142 - val_acc: 1.0000
Epoch 4/40
 - 0s - loss: 0.1489 - acc: 0.9830 - val_loss: 0.0033 - val_acc: 1.0000
Epoch 5/40
 - 0s - loss: 0.1279 - acc: 0.9854 - val_loss: 0.0103 - val_acc: 1.0000
Epoch 6/40
 - 0s - loss: 0.1151 - acc: 0.9864 - val_loss: 0.0089 - val_acc: 1.0000
Epoch 7/40
 - 0s - loss: 0.1177 - acc: 0.9884 - val_loss: 0.0062 - val_acc: 1.0000
Epoch 8/40
 - 0s - loss: 0.1090 - acc: 0.9869 - val_loss: 0.0084 - val_acc: 1.0000
Epoch 9/40
 - 0s - loss: 0.1124 - acc: 0.9874 - val_loss: 0.0068 - val_acc: 1.0000
Epoch 10/40
 - 0s - loss: 0.1143 - acc: 0.9886 - val_loss: 0.0096 - val_acc: 1.0000
Epoch 11/40
 - 0s - loss: 0.1055 - acc: 0.9889 - val_loss: 0.0106 - val_acc: 1.0000
Epoch 12/40
 - 0s - loss: 0.1064 - acc: 0.9886 - val_loss: 0.0063 - val_acc: 1.0000
Epoch 13/40
 - 0s - loss: 0.1010 - acc: 0.9887 - val_loss: 0.0075 - val_acc: 1.0000
Epoch 14/40
 - 0s - loss: 0.0992 - acc: 0.9891 - val_loss: 0.0129 - val_acc: 1.0000
Epoch 15/40
 - 0s - loss: 0.1035 - acc: 0.9882 - val_loss: 0.0108 - val_acc: 1.0000
Epoch 16/40
 - 0s - loss: 0.1017 - acc: 0.9886 - val_loss: 0.0104 - val_acc: 1.0000
Epoch 17/40
 - 0s - loss: 0.1032 - acc: 0.9882 - val_loss: 0.0155 - val_acc: 1.0000
Epoch 18/40
 - 0s - loss: 0.0959 - acc: 0.9886 - val_loss: 0.0156 - val_acc: 1.0000
Epoch 19/40
 - 0s - loss: 0.0960 - acc: 0.9889 - val_loss: 0.0136 - val_acc: 1.0000
Epoch 20/40
 - 0s - loss: 0.0956 - acc: 0.9892 - val_loss: 0.0146 - val_acc: 1.0000
Epoch 21/40
 - 0s - loss: 0.0972 - acc: 0.9887 - val_loss: 0.0150 - val_acc: 1.0000
Epoch 22/40
 - 0s - loss: 0.0942 - acc: 0.9887 - val_loss: 0.0129 - val_acc: 1.0000
Epoch 23/40
 - 0s - loss: 0.0940 - acc: 0.9896 - val_loss: 0.0159 - val_acc: 1.0000
Epoch 24/40
 - 0s - loss: 0.0940 - acc: 0.9892 - val_loss: 0.0132 - val_acc: 1.0000
Epoch 25/40
 - 0s - loss: 0.0861 - acc: 0.9896 - val_loss: 0.0142 - val_acc: 1.0000
Epoch 26/40
 - 0s - loss: 0.0884 - acc: 0.9899 - val_loss: 0.0188 - val_acc: 1.0000
Epoch 27/40
 - 0s - loss: 0.0925 - acc: 0.9894 - val_loss: 0.0158 - val_acc: 1.0000
Epoch 28/40
 - 0s - loss: 0.0894 - acc: 0.9896 - val_loss: 0.0096 - val_acc: 1.0000
Epoch 29/40
 - 0s - loss: 0.0923 - acc: 0.9896 - val_loss: 0.0175 - val_acc: 1.0000
Epoch 30/40
 - 0s - loss: 0.0877 - acc: 0.9892 - val_loss: 0.0136 - val_acc: 1.0000
Epoch 31/40
 - 0s - loss: 0.0938 - acc: 0.9892 - val_loss: 0.0118 - val_acc: 1.0000
Epoch 32/40
 - 0s - loss: 0.0890 - acc: 0.9891 - val_loss: 0.0158 - val_acc: 1.0000
Epoch 33/40
 - 0s - loss: 0.0892 - acc: 0.9896 - val_loss: 0.0177 - val_acc: 1.0000
Epoch 34/40
 - 0s - loss: 0.0883 - acc: 0.9896 - val_loss: 0.0143 - val_acc: 1.0000
Epoch 35/40
 - 0s - loss: 0.0886 - acc: 0.9898 - val_loss: 0.0176 - val_acc: 1.0000
Epoch 36/40
 - 0s - loss: 0.0853 - acc: 0.9894 - val_loss: 0.0166 - val_acc: 1.0000
Epoch 37/40
 - 0s - loss: 0.0832 - acc: 0.9898 - val_loss: 0.0218 - val_acc: 1.0000
Epoch 38/40
 - 0s - loss: 0.0870 - acc: 0.9892 - val_loss: 0.0226 - val_acc: 1.0000
Epoch 39/40
 - 0s - loss: 0.0839 - acc: 0.9898 - val_loss: 0.0202 - val_acc: 1.0000
Epoch 40/40
 - 0s - loss: 0.0865 - acc: 0.9896 - val_loss: 0.0173 - val_acc: 1.0000

==================================================================================================
	Training time : 0:00:07.264688
==================================================================================================
	Identification : 0.105
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1
	None : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 189
	None : 2

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : SL
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 1583, Test : 2530
	MWEs in tain : 819, occurrences : 1787
	Impotant words in tain : 749
	MWE length mean : 2.26
	Seen MWEs : 322 (64 %)
	New MWEs : 178 (35 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 4377 * POS : 1484
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 200)       875400      input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 15)        22260       input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 800)          0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 60)           0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 860)          0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 25)           21525       concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 25)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            208         dropout_17[0][0]                 
==================================================================================================
Total params: 919,393
Trainable params: 919,393
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	1583 importatnt sents of 1583
	data size before sampling = 85308
	data size after sampling = 334128
{0.0: 41766, 1.0: 41766, 2.0: 41766, 3.0: 41766, 4.0: 41766, 5.0: 41766, 6.0: 41766, 7.0: 41766}
	Class weights : {0: 1, 1: 1, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 300715 samples, validate on 33413 samples
Epoch 1/40
 - 7s - loss: 0.2424 - acc: 0.9726 - val_loss: 2.3239e-05 - val_acc: 1.0000
Epoch 2/40
 - 7s - loss: 0.0914 - acc: 0.9871 - val_loss: 6.2570e-06 - val_acc: 1.0000
Epoch 3/40
 - 7s - loss: 0.0814 - acc: 0.9884 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 4/40
 - 7s - loss: 0.0762 - acc: 0.9891 - val_loss: 2.0857e-06 - val_acc: 1.0000
Epoch 5/40
 - 7s - loss: 0.0733 - acc: 0.9896 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 7s - loss: 0.0706 - acc: 0.9900 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 7s - loss: 0.0681 - acc: 0.9903 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 7s - loss: 0.0670 - acc: 0.9906 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 7s - loss: 0.0659 - acc: 0.9907 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 10/40
 - 7s - loss: 0.0648 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 7s - loss: 0.0639 - acc: 0.9910 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 12/40
 - 7s - loss: 0.0633 - acc: 0.9910 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 7s - loss: 0.0627 - acc: 0.9912 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 14/40
 - 7s - loss: 0.0621 - acc: 0.9914 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 15/40
 - 7s - loss: 0.0612 - acc: 0.9915 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 7s - loss: 0.0610 - acc: 0.9914 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 17/40
 - 7s - loss: 0.0609 - acc: 0.9915 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 18/40
 - 7s - loss: 0.0607 - acc: 0.9916 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 7s - loss: 0.0601 - acc: 0.9917 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 20/40
 - 7s - loss: 0.0600 - acc: 0.9917 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 21/40
 - 7s - loss: 0.0592 - acc: 0.9917 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 7s - loss: 0.0590 - acc: 0.9918 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 23/40
 - 7s - loss: 0.0590 - acc: 0.9918 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 24/40
 - 7s - loss: 0.0582 - acc: 0.9918 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 25/40
 - 7s - loss: 0.0586 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 26/40
 - 7s - loss: 0.0583 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 27/40
 - 7s - loss: 0.0582 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 7s - loss: 0.0578 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 29/40
 - 7s - loss: 0.0574 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 30/40
 - 7s - loss: 0.0574 - acc: 0.9920 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 31/40
 - 7s - loss: 0.0571 - acc: 0.9921 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 32/40
 - 7s - loss: 0.0570 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 33/40
 - 7s - loss: 0.0571 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 34/40
 - 7s - loss: 0.0566 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 35/40
 - 7s - loss: 0.0570 - acc: 0.9922 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 36/40
 - 7s - loss: 0.0562 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 37/40
 - 7s - loss: 0.0560 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 38/40
 - 7s - loss: 0.0558 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 39/40
 - 7s - loss: 0.0561 - acc: 0.9923 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 40/40
 - 7s - loss: 0.0557 - acc: 0.9924 - val_loss: 1.1921e-06 - val_acc: 1.0000

==================================================================================================
	Training time : 0:05:19.974240
==================================================================================================
	Identification : 0.533
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	25 : 2
	50 : 3
	None : 59
	5 : 45

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 164
	25 : 2
	50 : 2
	None : 41
	5 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.0
	Training (Important) : 4476, Test : 1321
	MWEs in tain : 2127, occurrences : 6169
	Impotant words in tain : 1374
	MWE length mean : 2.06
	Seen MWEs : 348 (69 %)
	New MWEs : 153 (30 %)
==================================================================================================

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7981 * POS : 119
__________________________________________________________________________________________________
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Lemma : 200
	POS = 15 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 200)       1596200     input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 15)        1785        input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 800)          0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 60)           0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 860)          0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 25)           21525       concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 25)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            208         dropout_18[0][0]                 
==================================================================================================
Total params: 1,619,718
Trainable params: 1,619,718
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	4476 importatnt sents of 4476
	data size before sampling = 304145
	data size after sampling = 894036
{0.0: 149006, 1.0: 149006, 2.0: 149006, 5.0: 149006, 6.0: 149006, 7.0: 149006}
	Class weights : {0: 1, 1: 1, 2: 10, 5: 10, 6: 10, 7: 10}
__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.02
__________________________________________________________________________________________________
Train on 804632 samples, validate on 89404 samples
Epoch 1/40
 - 21s - loss: 0.3710 - acc: 0.9711 - val_loss: 0.3006 - val_acc: 0.9934
Epoch 2/40
 - 21s - loss: 0.2014 - acc: 0.9802 - val_loss: 0.1982 - val_acc: 0.9917
Epoch 3/40
 - 21s - loss: 0.1789 - acc: 0.9816 - val_loss: 0.2047 - val_acc: 0.9933
Epoch 4/40
 - 21s - loss: 0.1691 - acc: 0.9823 - val_loss: 0.2420 - val_acc: 0.9934
Epoch 5/40
 - 21s - loss: 0.1615 - acc: 0.9827 - val_loss: 0.2403 - val_acc: 0.9934
Epoch 6/40
 - 21s - loss: 0.1565 - acc: 0.9832 - val_loss: 0.2165 - val_acc: 0.9950
Epoch 7/40
 - 21s - loss: 0.1535 - acc: 0.9834 - val_loss: 0.2331 - val_acc: 0.9934
Epoch 8/40
 - 20s - loss: 0.1507 - acc: 0.9836 - val_loss: 0.1730 - val_acc: 0.9950
Epoch 9/40
 - 20s - loss: 0.1487 - acc: 0.9836 - val_loss: 0.2033 - val_acc: 0.9934
Epoch 10/40
 - 21s - loss: 0.1471 - acc: 0.9839 - val_loss: 0.2203 - val_acc: 0.9935
Epoch 11/40
 - 21s - loss: 0.1453 - acc: 0.9839 - val_loss: 0.2034 - val_acc: 0.9950
Epoch 12/40
 - 21s - loss: 0.1440 - acc: 0.9841 - val_loss: 0.1904 - val_acc: 0.9950
Epoch 13/40
 - 21s - loss: 0.1423 - acc: 0.9842 - val_loss: 0.1665 - val_acc: 0.9968
Epoch 14/40
 - 20s - loss: 0.1425 - acc: 0.9842 - val_loss: 0.2107 - val_acc: 0.9950
Epoch 15/40
 - 20s - loss: 0.1410 - acc: 0.9843 - val_loss: 0.1799 - val_acc: 0.9950
Epoch 16/40
 - 21s - loss: 0.1407 - acc: 0.9844 - val_loss: 0.2018 - val_acc: 0.9950
Epoch 17/40
 - 21s - loss: 0.1401 - acc: 0.9844 - val_loss: 0.1968 - val_acc: 0.9950
Epoch 18/40
 - 21s - loss: 0.1393 - acc: 0.9845 - val_loss: 0.1926 - val_acc: 0.9950
Epoch 19/40
 - 21s - loss: 0.1383 - acc: 0.9844 - val_loss: 0.2121 - val_acc: 0.9934
Epoch 20/40
 - 21s - loss: 0.1385 - acc: 0.9846 - val_loss: 0.1769 - val_acc: 0.9950
Epoch 21/40
 - 20s - loss: 0.1375 - acc: 0.9845 - val_loss: 0.1875 - val_acc: 0.9950
Epoch 22/40
 - 20s - loss: 0.1378 - acc: 0.9846 - val_loss: 0.1895 - val_acc: 0.9950
Epoch 23/40
 - 20s - loss: 0.1370 - acc: 0.9846 - val_loss: 0.1860 - val_acc: 0.9950
Epoch 24/40
 - 20s - loss: 0.1361 - acc: 0.9847 - val_loss: 0.1816 - val_acc: 0.9950
Epoch 25/40
 - 21s - loss: 0.1357 - acc: 0.9848 - val_loss: 0.2042 - val_acc: 0.9933
Epoch 26/40
 - 20s - loss: 0.1355 - acc: 0.9848 - val_loss: 0.1808 - val_acc: 0.9950
Epoch 27/40
 - 20s - loss: 0.1351 - acc: 0.9848 - val_loss: 0.1867 - val_acc: 0.9950
Epoch 28/40
 - 21s - loss: 0.1352 - acc: 0.9849 - val_loss: 0.2052 - val_acc: 0.9950
Epoch 29/40
 - 20s - loss: 0.1340 - acc: 0.9849 - val_loss: 0.2103 - val_acc: 0.9950
Epoch 30/40
 - 20s - loss: 0.1338 - acc: 0.9850 - val_loss: 0.1808 - val_acc: 0.9950
Epoch 31/40
 - 20s - loss: 0.1339 - acc: 0.9850 - val_loss: 0.1869 - val_acc: 0.9950
Epoch 32/40
 - 20s - loss: 0.1339 - acc: 0.9848 - val_loss: 0.1942 - val_acc: 0.9950
Epoch 33/40
 - 20s - loss: 0.1338 - acc: 0.9849 - val_loss: 0.1885 - val_acc: 0.9950
Epoch 34/40
 - 21s - loss: 0.1335 - acc: 0.9850 - val_loss: 0.1897 - val_acc: 0.9950
Epoch 35/40
 - 20s - loss: 0.1326 - acc: 0.9851 - val_loss: 0.2167 - val_acc: 0.9950
Epoch 36/40
 - 20s - loss: 0.1333 - acc: 0.9850 - val_loss: 0.2102 - val_acc: 0.9950
Epoch 37/40
 - 20s - loss: 0.1323 - acc: 0.9851 - val_loss: 0.1896 - val_acc: 0.9950
Epoch 38/40
 - 20s - loss: 0.1324 - acc: 0.9851 - val_loss: 0.1944 - val_acc: 0.9950
Epoch 39/40
 - 20s - loss: 0.1322 - acc: 0.9850 - val_loss: 0.1809 - val_acc: 0.9950
Epoch 40/40
 - 20s - loss: 0.1314 - acc: 0.9851 - val_loss: 0.1908 - val_acc: 0.9950
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--2.7.14-64/tmplz92BL and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--2.7.14-64/tmpb7eScr). This is not supposed to happen! You may need to manually delete your cache directory to fix this.

==================================================================================================
	Training time : 0:15:40.755996
==================================================================================================
	Identification : 0.423
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 1
	25 : 13
	50 : 9
	None : 43
	5 : 45

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 138
	25 : 11
	50 : 4
	None : 64
	5 : 39

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
