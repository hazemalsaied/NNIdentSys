INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_e6pj3W.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 10869/11441 Mb (0.950000) on cuda
Mapped name None to device cuda: Tesla K40m (0000:03:00.0)
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
1000 726
1000 254
1000 247
1000 746
1000 580
1000 342
1000 849
1000 815
1000 40
1000 144
1000 51
1000 620
1000 168
1000 463
1000 863
1000 305
1000 549
1000 781
1000 77
1000 838
1000 235
1000 464
1000 823
1000 865
1000 169
1000 417
1000 975
1000 76
1000 95
1000 844
1000 947
1000 748
1000 42
1000 301
1000 921
1000 915
1000 956
1000 755
1000 565
1000 297
1000 796
1000 116
1000 959
1000 813
1000 245
1000 570
1000 239
1000 494
1000 505
1000 703
16_True_False_352_68_frequent_True_10_relu_0.363_False_512_relu_0.2_adagrad_0.01_128_40
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19950

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19950 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19950
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 352)       7022400     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 68)        10336       input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 1408)         0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 272)          0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 1680)         0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           16810       concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 10)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            88          dropout_1[0][0]                  
==================================================================================================
Total params: 7,049,634
Trainable params: 7,049,634
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34236, 1.0: 26917, 2.0: 7788, 6.0: 2559, 4.0: 2141, 5.0: 1769, 3.0: 120})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 23s - loss: 0.1982 - acc: 0.9182 - val_loss: 0.0519 - val_acc: 0.9898
Epoch 2/40
 - 22s - loss: 0.1429 - acc: 0.9337 - val_loss: 0.0442 - val_acc: 0.9902
Epoch 3/40
 - 22s - loss: 0.1323 - acc: 0.9388 - val_loss: 0.0423 - val_acc: 0.9901

==================================================================================================
	Training time : 0:04:26.878448
==================================================================================================
	Identification : 0.66
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 139
	25 : 7
	50 : 3
	100 : 1
	5 : 55

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 254
	25 : 3
	50 : 1
	100 : 1
	5 : 10

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19870

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19870 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19870
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 352)       6994240     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 68)        10336       input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 1408)         0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 272)          0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 1680)         0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 10)           16810       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 10)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            88          dropout_2[0][0]                  
==================================================================================================
Total params: 7,021,474
Trainable params: 7,021,474
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34574, 1.0: 26767, 2.0: 7679, 6.0: 2579, 4.0: 2101, 5.0: 1713, 3.0: 117})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 22s - loss: 0.2077 - acc: 0.9154 - val_loss: 0.0574 - val_acc: 0.9892
Epoch 2/40
 - 22s - loss: 0.1454 - acc: 0.9357 - val_loss: 0.0439 - val_acc: 0.9901
Epoch 3/40
 - 22s - loss: 0.1334 - acc: 0.9417 - val_loss: 0.0423 - val_acc: 0.9900

==================================================================================================
	Training time : 0:02:35.813490
==================================================================================================
	Identification : 0.663
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 133
	25 : 6
	50 : 3
	100 : 1
	5 : 56

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 257
	25 : 4
	5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 11990, Test : 1931
	MWEs in tain : 1881, occurrences : 3583
	Impotant words in tain : 1880
	MWE length mean : 2.12
	Seen MWEs : 400 (63 %)
	New MWEs : 231 (36 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 25528
	After : 19918

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 19918 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 19918
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 352)       7011136     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 68)        10336       input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 1408)         0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 272)          0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 1680)         0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 10)           16810       concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 10)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            88          dropout_3[0][0]                  
==================================================================================================
Total params: 7,038,370
Trainable params: 7,038,370
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 543605
	data size after focused sampling = 755292
	7 Labels in train : Counter({0.0: 344648, 1.0: 267337, 2.0: 77311, 6.0: 25935, 4.0: 21336, 5.0: 17536, 3.0: 1189})
	7 Labels in valid : Counter({0.0: 34326, 1.0: 26788, 2.0: 7816, 6.0: 2626, 4.0: 2111, 5.0: 1743, 3.0: 120})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 679762 samples, validate on 75530 samples
Epoch 1/40
 - 22s - loss: 0.1881 - acc: 0.9296 - val_loss: 0.0414 - val_acc: 0.9900
Epoch 2/40
 - 22s - loss: 0.1278 - acc: 0.9471 - val_loss: 0.0359 - val_acc: 0.9903
Epoch 3/40
 - 22s - loss: 0.1180 - acc: 0.9521 - val_loss: 0.0346 - val_acc: 0.9902

==================================================================================================
	Training time : 0:02:35.594513
==================================================================================================
	Identification : 0.643
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 130
	25 : 6
	50 : 3
	100 : 1
	5 : 56

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 262
	25 : 6
	50 : 1
	100 : 1
	5 : 12

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20259

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20259 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20259
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 352)       7131168     input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 68)        11492       input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 1408)         0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 272)          0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 1680)         0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 10)           16810       concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 10)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            88          dropout_4[0][0]                  
==================================================================================================
Total params: 7,159,558
Trainable params: 7,159,558
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 34868, 1.0: 26793, 2.0: 8392, 6.0: 4738, 5.0: 1403, 4.0: 924})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 21s - loss: 0.1814 - acc: 0.9359 - val_loss: 0.0372 - val_acc: 0.9919
Epoch 2/40
 - 21s - loss: 0.1218 - acc: 0.9524 - val_loss: 0.0334 - val_acc: 0.9925
Epoch 3/40
 - 21s - loss: 0.1136 - acc: 0.9540 - val_loss: 0.0331 - val_acc: 0.9926

==================================================================================================
	Training time : 0:02:32.096156
==================================================================================================
	Identification : 0.394
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 70
	5 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 251
	5 : 4

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20424

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20424 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20424
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 352)       7189248     input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 68)        11492       input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 1408)         0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 272)          0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 1680)         0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 10)           16810       concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 10)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            88          dropout_5[0][0]                  
==================================================================================================
Total params: 7,217,638
Trainable params: 7,217,638
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 35252, 1.0: 26683, 2.0: 8341, 6.0: 4538, 5.0: 1388, 4.0: 916})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 22s - loss: 0.2537 - acc: 0.9073 - val_loss: 0.0690 - val_acc: 0.9805
Epoch 2/40
 - 22s - loss: 0.1888 - acc: 0.9246 - val_loss: 0.0612 - val_acc: 0.9812
Epoch 3/40
 - 22s - loss: 0.1782 - acc: 0.9265 - val_loss: 0.0576 - val_acc: 0.9816

==================================================================================================
	Training time : 0:02:33.856808
==================================================================================================
	Identification : 0.478
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 122
	5 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 202
	5 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 12631, Test : 1937
	MWEs in tain : 1967, occurrences : 2518
	Impotant words in tain : 1678
	MWE length mean : 2.22
	Seen MWEs : 119 (34 %)
	New MWEs : 230 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 26651
	After : 20435

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 20435 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 2361
	One occurrence keys in vocabulary 2361 / 20435
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 352)       7193120     input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 68)        11492       input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 1408)         0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 272)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 1680)         0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 10)           16810       concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 10)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            88          dropout_6[0][0]                  
==================================================================================================
Total params: 7,221,510
Trainable params: 7,221,510
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 542608
	data size after focused sampling = 771175
	6 Labels in train : Counter({0.0: 351080, 1.0: 268058, 2.0: 83124, 6.0: 46162, 5.0: 13721, 4.0: 9030})
	6 Labels in valid : Counter({0.0: 35023, 1.0: 26847, 2.0: 8321, 6.0: 4636, 5.0: 1389, 4.0: 902})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 694057 samples, validate on 77118 samples
Epoch 1/40
 - 21s - loss: 0.2046 - acc: 0.9249 - val_loss: 0.0415 - val_acc: 0.9913
Epoch 2/40
 - 21s - loss: 0.1464 - acc: 0.9458 - val_loss: 0.0347 - val_acc: 0.9926
Epoch 3/40
 - 21s - loss: 0.1358 - acc: 0.9498 - val_loss: 0.0330 - val_acc: 0.9928

==================================================================================================
	Training time : 0:02:30.930812
==================================================================================================
	Identification : 0.378
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 68
	5 : 8

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 255
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35965

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35965 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35965
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 352)       12659680    input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 68)        7480        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 1408)         0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 272)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 1680)         0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 10)           16810       concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 10)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            88          dropout_7[0][0]                  
==================================================================================================
Total params: 12,684,058
Trainable params: 12,684,058
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40372, 1.0: 26774, 2.0: 13875, 5.0: 7046, 6.0: 6299})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 34s - loss: 0.2428 - acc: 0.9049 - val_loss: 0.0511 - val_acc: 0.9913
Epoch 2/40
 - 34s - loss: 0.1781 - acc: 0.9331 - val_loss: 0.0473 - val_acc: 0.9914
Epoch 3/40
 - 34s - loss: 0.1658 - acc: 0.9348 - val_loss: 0.0446 - val_acc: 0.9917

==================================================================================================
	Training time : 0:03:20.998521
==================================================================================================
	Identification : 0.422
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 155
	25 : 2
	5 : 30

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 547
	25 : 1
	5 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35818

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35818 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35818
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 352)       12607936    input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 68)        7480        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 1408)         0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 272)          0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 1680)         0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 10)           16810       concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 10)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            88          dropout_8[0][0]                  
==================================================================================================
Total params: 12,632,314
Trainable params: 12,632,314
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40512, 1.0: 26847, 2.0: 13880, 5.0: 6775, 6.0: 6352})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 34s - loss: 0.1723 - acc: 0.9338 - val_loss: 0.0383 - val_acc: 0.9910
Epoch 2/40
 - 34s - loss: 0.1180 - acc: 0.9529 - val_loss: 0.0380 - val_acc: 0.9910
Epoch 3/40
 - 34s - loss: 0.1085 - acc: 0.9596 - val_loss: 0.0390 - val_acc: 0.9906

==================================================================================================
	Training time : 0:03:17.691025
==================================================================================================
	Identification : 0.522
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 283
	25 : 2
	5 : 31

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 423
	25 : 1
	5 : 8

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 13501, Test : 2136
	MWEs in tain : 3767, occurrences : 4856
	Impotant words in tain : 3367
	MWE length mean : 2.06
	Seen MWEs : 280 (34 %)
	New MWEs : 540 (65 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 48374
	After : 35723

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 35723 * POS : 110
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3974
	One occurrence keys in vocabulary 3974 / 35723
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 352)       12574496    input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 68)        7480        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 1408)         0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 272)          0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 1680)         0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 10)           16810       concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 10)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            88          dropout_9[0][0]                  
==================================================================================================
Total params: 12,598,874
Trainable params: 12,598,874
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 544858
	data size after focused sampling = 943659
	5 Labels in train : Counter({0.0: 405907, 1.0: 268683, 2.0: 137224, 5.0: 68593, 6.0: 63252})
	5 Labels in valid : Counter({0.0: 40890, 1.0: 26610, 2.0: 13804, 5.0: 6808, 6.0: 6254})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 849293 samples, validate on 94366 samples
Epoch 1/40
 - 34s - loss: 0.1899 - acc: 0.9253 - val_loss: 0.0389 - val_acc: 0.9912
Epoch 2/40
 - 34s - loss: 0.1367 - acc: 0.9457 - val_loss: 0.0393 - val_acc: 0.9910
Epoch 3/40
 - 34s - loss: 0.1282 - acc: 0.9545 - val_loss: 0.0388 - val_acc: 0.9912

==================================================================================================
	Training time : 0:03:17.619614
==================================================================================================
	Identification : 0.509
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 238
	25 : 2
	5 : 33

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 466
	25 : 2
	5 : 6

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	GPU Enabled
==================================================================================================
	Dev Mode
==================================================================================================
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24724

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24724 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24724
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 352)       8702848     input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 68)        11424       input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 1408)         0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 272)          0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 1680)         0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 10)           16810       concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 10)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            88          dropout_10[0][0]                 
==================================================================================================
Total params: 8,731,170
Trainable params: 8,731,170
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 50000, 1.0: 39471, 2.0: 10440, 6.0: 3502, 4.0: 2736, 5.0: 2576, 3.0: 149})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 32s - loss: 0.1848 - acc: 0.9233 - val_loss: 0.0508 - val_acc: 0.9900
Epoch 2/40
 - 32s - loss: 0.1364 - acc: 0.9367 - val_loss: 0.0436 - val_acc: 0.9901
Epoch 3/40
 - 32s - loss: 0.1256 - acc: 0.9405 - val_loss: 0.0409 - val_acc: 0.9901

==================================================================================================
	Training time : 0:03:42.722556
==================================================================================================
	Identification : 0.513
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 130
	200 : 1
	50 : 5
	5 : 51
	25 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 325
	25 : 1
	50 : 5
	5 : 21

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24666

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24666 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24666
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 352)       8682432     input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 68)        11424       input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 1408)         0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 272)          0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 1680)         0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 10)           16810       concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 10)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            88          dropout_11[0][0]                 
==================================================================================================
Total params: 8,710,754
Trainable params: 8,710,754
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 50148, 1.0: 39495, 2.0: 10498, 6.0: 3419, 4.0: 2677, 5.0: 2515, 3.0: 122})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 32s - loss: 0.2199 - acc: 0.9102 - val_loss: 0.0697 - val_acc: 0.9893
Epoch 2/40
 - 32s - loss: 0.1643 - acc: 0.9312 - val_loss: 0.0577 - val_acc: 0.9902
Epoch 3/40
 - 32s - loss: 0.1522 - acc: 0.9333 - val_loss: 0.0516 - val_acc: 0.9906

==================================================================================================
	Training time : 0:03:41.168113
==================================================================================================
	Identification : 0.52
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 106
	200 : 1
	50 : 5
	5 : 55
	25 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 349
	25 : 1
	5 : 16

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : BG
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 17813, Test : 1954
	MWEs in tain : 2523, occurrences : 5364
	Impotant words in tain : 2369
	MWE length mean : 2.13
	Seen MWEs : 318 (47 %)
	New MWEs : 352 (52 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 31203
	After : 24727

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 24727 * POS : 168
__________________________________________________________________________________________________
	Dashed keys in vocabulary 2949
	One occurrence keys in vocabulary 2949 / 24727
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 352)       8703904     input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 68)        11424       input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 1408)         0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 272)          0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 1680)         0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 10)           16810       concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 10)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            88          dropout_12[0][0]                 
==================================================================================================
Total params: 8,732,226
Trainable params: 8,732,226
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 803710
	data size after focused sampling = 1088735
	7 Labels in train : Counter({0.0: 500026, 1.0: 395172, 2.0: 104854, 6.0: 35005, 4.0: 27008, 5.0: 25271, 3.0: 1399})
	7 Labels in valid : Counter({0.0: 50049, 1.0: 39464, 2.0: 10551, 6.0: 3493, 4.0: 2684, 5.0: 2498, 3.0: 135})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 979861 samples, validate on 108874 samples
Epoch 1/40
 - 33s - loss: 0.2359 - acc: 0.9189 - val_loss: 0.0566 - val_acc: 0.9891
Epoch 2/40
 - 33s - loss: 0.1715 - acc: 0.9425 - val_loss: 0.0473 - val_acc: 0.9902
Epoch 3/40
 - 33s - loss: 0.1608 - acc: 0.9467 - val_loss: 0.0447 - val_acc: 0.9901

==================================================================================================
	Training time : 0:03:43.795537
==================================================================================================
	Identification : 0.501
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 103
	200 : 1
	50 : 5
	5 : 51
	25 : 11

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 353
	25 : 3
	5 : 18

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28435

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28435 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28435
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 352)       10009120    input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 68)        15232       input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 1408)         0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 272)          0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 1680)         0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 10)           16810       concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 10)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            88          dropout_13[0][0]                 
==================================================================================================
Total params: 10,041,250
Trainable params: 10,041,250
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 60269, 1.0: 46811, 2.0: 13080, 6.0: 7402, 5.0: 2097, 4.0: 1436})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 42s - loss: 0.1833 - acc: 0.9232 - val_loss: 0.0439 - val_acc: 0.9914
Epoch 2/40
 - 42s - loss: 0.1334 - acc: 0.9473 - val_loss: 0.0392 - val_acc: 0.9923
Epoch 3/40
 - 42s - loss: 0.1241 - acc: 0.9506 - val_loss: 0.0374 - val_acc: 0.9926

==================================================================================================
	Training time : 0:04:36.865071
==================================================================================================
	Identification : 0.506
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 124
	25 : 1
	5 : 27

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 341
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28458

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28458 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28458
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 352)       10017216    input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 68)        15232       input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 1408)         0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 272)          0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 1680)         0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 10)           16810       concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 10)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            88          dropout_14[0][0]                 
==================================================================================================
Total params: 10,049,346
Trainable params: 10,049,346
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 59898, 1.0: 46925, 2.0: 13115, 6.0: 7547, 5.0: 2166, 4.0: 1444})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 42s - loss: 0.1808 - acc: 0.9323 - val_loss: 0.0511 - val_acc: 0.9912
Epoch 2/40
 - 42s - loss: 0.1337 - acc: 0.9520 - val_loss: 0.0457 - val_acc: 0.9918
Epoch 3/40
 - 42s - loss: 0.1247 - acc: 0.9558 - val_loss: 0.0443 - val_acc: 0.9923

==================================================================================================
	Training time : 0:04:37.770870
==================================================================================================
	Identification : 0.469
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 116
	5 : 25

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 350
	25 : 1
	5 : 9

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : PT
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 22017, Test : 3117
	MWEs in tain : 3115, occurrences : 4337
	Impotant words in tain : 2284
	MWE length mean : 2.22
	Seen MWEs : 234 (42 %)
	New MWEs : 319 (57 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 36575
	After : 28422

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 28422 * POS : 224
__________________________________________________________________________________________________
	Important words not in vocabulary 1
se_vira_em_os_30
	MWE not in vocabulary 1
	Dashed keys in vocabulary 3691
	One occurrence keys in vocabulary 3691 / 28422
	Attention: important lemmas are not in vocbulary	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 352)       10004544    input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 68)        15232       input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 1408)         0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 272)          0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 1680)         0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 10)           16810       concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 10)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            88          dropout_15[0][0]                 
==================================================================================================
Total params: 10,036,674
Trainable params: 10,036,674
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 951229
	data size after focused sampling = 1310945
	6 Labels in train : Counter({0.0: 600773, 1.0: 469851, 2.0: 131126, 6.0: 74125, 5.0: 21071, 4.0: 13999})
	6 Labels in valid : Counter({0.0: 60006, 1.0: 47094, 2.0: 12997, 6.0: 7464, 5.0: 2060, 4.0: 1474})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 1179850 samples, validate on 131095 samples
Epoch 1/40
 - 42s - loss: 0.1726 - acc: 0.9399 - val_loss: 0.0486 - val_acc: 0.9917
Epoch 2/40
 - 42s - loss: 0.1253 - acc: 0.9579 - val_loss: 0.0438 - val_acc: 0.9922
Epoch 3/40
 - 42s - loss: 0.1147 - acc: 0.9592 - val_loss: 0.0408 - val_acc: 0.9924

==================================================================================================
	Training time : 0:04:37.801797
==================================================================================================
	Identification : 0.492
	Test analysis
==================================================================================================
	Correctly identified MWEs
==================================================================================================
	0 : 123
	25 : 1
	5 : 26

__________________________________________________________________________________________________
	Non Identified MWEs
==================================================================================================
	0 : 343
	25 : 1
	5 : 7

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
	Language : TR
==================================================================================================
	Dataset : Sharedtask 1.1
	Training  : 16715, Test : 1320
	MWEs in tain : 4569, occurrences : 6091
	Impotant words in tain : 3897
	MWE length mean : 2.06
	Seen MWEs : 135 (26 %)
	New MWEs : 375 (73 %)
==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 55011
	After : 41352

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 41352 * POS : 119
__________________________________________________________________________________________________
	Dashed keys in vocabulary 4818
	One occurrence keys in vocabulary 4818 / 41352
	Embedding
==================================================================================================
	Initialisation = None
	Concatenation = False
	Token : 352
	POS = 68 
Deep model(Non compositional)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 352)       14555904    input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 68)        8092        input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 1408)         0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 272)          0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 1680)         0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 10)           16810       concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 10)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            88          dropout_16[0][0]                 
==================================================================================================
Total params: 14,580,894
Trainable params: 14,580,894
Non-trainable params: 0
__________________________________________________________________________________________________
None

__________________________________________________________________________________________________
	Sampling
==================================================================================================
	data size before focused sampling = 675851
	data size after focused sampling = 1159213
	5 Labels in train : Counter({0.0: 499647, 1.0: 333227, 2.0: 166420, 5.0: 83436, 6.0: 76483})
	5 Labels in valid : Counter({0.0: 49736, 1.0: 33469, 2.0: 16624, 5.0: 8409, 6.0: 7684})
	Favorisation Coeff : 16

__________________________________________________________________________________________________
	Optimizer : adagrad,  learning rate = 0.01
__________________________________________________________________________________________________
Train on 1043291 samples, validate on 115922 samples
Epoch 1/40
 - 46s - loss: 0.2066 - acc: 0.9200 - val_loss: 0.0429 - val_acc: 0.9906
Epoch 2/40
 - 45s - loss: 0.1508 - acc: 0.9344 - val_loss: 0.0409 - val_acc: 0.9907
