INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
********************
# XP = FR: Lemma(125) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 140
# Features = False
Deep model(Non compositional)
# Parameters = 1844638
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 125)       1836625     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 15)        3525        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 500)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 60)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 560)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 8)            4488        concatenate_1[0][0]              
==================================================================================================
Total params: 1,844,638
Trainable params: 1,844,638
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0805 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0752 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0765 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0772 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0781 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0788 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0789 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0791 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0796 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0804 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0806 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0825 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0826 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0825 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0833 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0828 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0836 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0841 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0851 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0856 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0855 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0867 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0864 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0872 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0875 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0881 - acc: 0.9880 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0888 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0890 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0912 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0898 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0899 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0908 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0921 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0924 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0923 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0924 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0938 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0935 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0950 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0941 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:39.713177
# F-Score(Ordinary) = 0.575, Recall: 0.514, Precision: 0.653
# F-Score(lvc) = 0.426, Recall: 0.384, Precision: 0.477
# F-Score(ireflv) = 0.569, Recall: 0.412, Precision: 0.918
# F-Score(id) = 0.71, Recall: 0.878, Precision: 0.596
********************
********************
# XP = FR: Lemma(125) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 140
# Features = False
Deep model(Non compositional)
# Parameters = 1844638
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 125)       1836625     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 15)        3525        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 500)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 60)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 560)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            4488        concatenate_2[0][0]              
==================================================================================================
Total params: 1,844,638
Trainable params: 1,844,638
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 14s - loss: 0.0802 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 14s - loss: 0.0735 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 14s - loss: 0.0747 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 14s - loss: 0.0755 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 14s - loss: 0.0769 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0783 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0780 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0797 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0799 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0803 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0816 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0821 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0830 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0827 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0839 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0843 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0847 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0856 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0860 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0860 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0877 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0872 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0889 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0867 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0880 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0891 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0888 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0902 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0907 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0896 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0911 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0918 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 14s - loss: 0.0922 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 14s - loss: 0.0927 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 14s - loss: 0.0930 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 14s - loss: 0.0934 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 14s - loss: 0.0935 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 14s - loss: 0.0948 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 14s - loss: 0.0957 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 14s - loss: 0.0961 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:23.119540
# F-Score(Ordinary) = 0.559, Recall: 0.504, Precision: 0.629
# F-Score(lvc) = 0.4, Recall: 0.378, Precision: 0.424
# F-Score(ireflv) = 0.541, Recall: 0.393, Precision: 0.869
# F-Score(id) = 0.703, Recall: 0.836, Precision: 0.606
********************
********************
# XP = FR: Lemma(125) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 140
# Features = False
Deep model(Non compositional)
# Parameters = 1844638
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 125)       1836625     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 15)        3525        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 500)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 60)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 560)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 8)            4488        concatenate_3[0][0]              
==================================================================================================
Total params: 1,844,638
Trainable params: 1,844,638
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0804 - acc: 0.9842 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0741 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0745 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0765 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0769 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0779 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0782 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0789 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0806 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0807 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0818 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0818 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0819 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0823 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0836 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0848 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0850 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0848 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0848 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0858 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0868 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0872 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0874 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0878 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0890 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0892 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0889 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0906 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0898 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0914 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0912 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0908 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0923 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0937 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0951 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0935 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0939 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0926 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0936 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0951 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:53.606054
# F-Score(Ordinary) = 0.607, Recall: 0.617, Precision: 0.597
# F-Score(lvc) = 0.498, Recall: 0.602, Precision: 0.424
# F-Score(ireflv) = 0.619, Recall: 0.476, Precision: 0.885
# F-Score(id) = 0.673, Recall: 0.912, Precision: 0.534
********************
********************
# XP = FR: Lemma(125) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 140
# Features = False
Deep model(Non compositional)
# Parameters = 1844638
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 125)       1836625     input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 15)        3525        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 500)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 60)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 560)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            4488        concatenate_4[0][0]              
==================================================================================================
Total params: 1,844,638
Trainable params: 1,844,638
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0798 - acc: 0.9842 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0752 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0756 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0767 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0770 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0783 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0792 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0799 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0797 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0812 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0814 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0807 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0828 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0824 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0836 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0838 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0844 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0847 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0853 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0854 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0862 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0857 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0874 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0865 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0874 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0882 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0887 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0898 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0890 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0891 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0907 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0906 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0906 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0929 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0920 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0906 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0919 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0929 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0931 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0931 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:12.678627
# F-Score(Ordinary) = 0.598, Recall: 0.57, Precision: 0.629
# F-Score(lvc) = 0.467, Recall: 0.519, Precision: 0.424
# F-Score(ireflv) = 0.636, Recall: 0.505, Precision: 0.861
# F-Score(id) = 0.649, Recall: 0.678, Precision: 0.622
********************
********************
# XP = FR: Lemma(125) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 140
# Features = False
Deep model(Non compositional)
# Parameters = 1844638
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 125)       1836625     input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 15)        3525        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 500)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 60)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 560)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 8)            4488        concatenate_5[0][0]              
==================================================================================================
Total params: 1,844,638
Trainable params: 1,844,638
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0791 - acc: 0.9844 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0737 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0754 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0762 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0772 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0783 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0793 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0803 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0812 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0822 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0813 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0822 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0832 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0842 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0837 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0847 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0851 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0857 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0843 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0873 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0864 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0866 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0885 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0887 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0885 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0907 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0897 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0896 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0901 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0899 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0915 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0913 - acc: 0.9880 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0921 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0939 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0918 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0933 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0917 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0936 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0957 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0935 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:53.295761
# F-Score(Ordinary) = 0.618, Recall: 0.628, Precision: 0.609
# F-Score(lvc) = 0.602, Recall: 0.774, Precision: 0.492
# F-Score(ireflv) = 0.617, Recall: 0.466, Precision: 0.91
# F-Score(id) = 0.625, Recall: 0.856, Precision: 0.492
********************
********************
# XP = FR: Lemma(125) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1847308
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 125)       1836625     input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 25)        5875        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 500)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 100)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 600)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            4808        concatenate_6[0][0]              
==================================================================================================
Total params: 1,847,308
Trainable params: 1,847,308
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0814 - acc: 0.9842 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0760 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0769 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0783 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0796 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0798 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0801 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0823 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0821 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0829 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0833 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0838 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0850 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0849 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0861 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0858 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0859 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0868 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0868 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0892 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0887 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0882 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0881 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0904 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0903 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0928 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0935 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0923 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0967 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0936 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0937 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0959 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0963 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.1066 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1099 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.1072 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.1097 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.1111 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1091 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.1078 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:34.819329
# F-Score(Ordinary) = 0.585, Recall: 0.51, Precision: 0.685
# F-Score(lvc) = 0.587, Recall: 0.803, Precision: 0.462
# F-Score(ireflv) = 0.507, Recall: 0.351, Precision: 0.91
# F-Score(id) = 0.628, Recall: 0.606, Precision: 0.653
********************
********************
# XP = FR: Lemma(125) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1847308
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 125)       1836625     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 25)        5875        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 500)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 100)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 600)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 8)            4808        concatenate_7[0][0]              
==================================================================================================
Total params: 1,847,308
Trainable params: 1,847,308
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0816 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0748 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0767 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0774 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0783 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0780 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0785 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0800 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0817 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0815 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0814 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0833 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0843 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0846 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0844 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0852 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0852 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0872 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0862 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0867 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0864 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0876 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0887 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0897 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0895 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0906 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0900 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0910 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0908 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0915 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0925 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0923 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0920 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0947 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0943 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0959 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0948 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0959 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0971 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0964 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:34.460236
# F-Score(Ordinary) = 0.569, Recall: 0.503, Precision: 0.655
# F-Score(lvc) = 0.467, Recall: 0.451, Precision: 0.485
# F-Score(ireflv) = 0.513, Recall: 0.357, Precision: 0.91
# F-Score(id) = 0.712, Recall: 0.885, Precision: 0.596
********************
********************
# XP = FR: Lemma(125) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1847308
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 125)       1836625     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 25)        5875        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 500)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 100)          0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 600)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            4808        concatenate_8[0][0]              
==================================================================================================
Total params: 1,847,308
Trainable params: 1,847,308
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0824 - acc: 0.9839 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0756 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0770 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0780 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0800 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0807 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0806 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0826 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0838 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0839 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0852 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0849 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0839 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0848 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0861 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0865 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0859 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0866 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0869 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0878 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0895 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0888 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0893 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0887 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0897 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0896 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0902 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0910 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0900 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0921 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0914 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0919 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0912 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0922 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0925 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0928 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0940 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0944 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0952 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0945 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:58.163735
# F-Score(Ordinary) = 0.602, Recall: 0.597, Precision: 0.606
# F-Score(lvc) = 0.505, Recall: 0.828, Precision: 0.364
# F-Score(ireflv) = 0.546, Recall: 0.394, Precision: 0.893
# F-Score(id) = 0.724, Recall: 0.95, Precision: 0.585
********************
********************
# XP = FR: Lemma(125) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1847308
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 125)       1836625     input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 25)        5875        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 500)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 100)          0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 600)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 8)            4808        concatenate_9[0][0]              
==================================================================================================
Total params: 1,847,308
Trainable params: 1,847,308
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 14s - loss: 0.0811 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 14s - loss: 0.0755 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 14s - loss: 0.0755 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 14s - loss: 0.0773 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 14s - loss: 0.0790 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 14s - loss: 0.0794 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 14s - loss: 0.0797 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 14s - loss: 0.0814 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 14s - loss: 0.0806 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 14s - loss: 0.0819 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 14s - loss: 0.0820 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 14s - loss: 0.0825 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 14s - loss: 0.0830 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 14s - loss: 0.0837 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 14s - loss: 0.0846 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 14s - loss: 0.0846 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 14s - loss: 0.0857 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 14s - loss: 0.0858 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 14s - loss: 0.0861 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 14s - loss: 0.0860 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 14s - loss: 0.0868 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 14s - loss: 0.0857 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 14s - loss: 0.0884 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 14s - loss: 0.0884 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 14s - loss: 0.0877 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 14s - loss: 0.0887 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 14s - loss: 0.0897 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 14s - loss: 0.0899 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0903 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0899 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0909 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0902 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0916 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0931 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0920 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0921 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0938 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0939 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0942 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0947 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:10.825070
# F-Score(Ordinary) = 0.612, Recall: 0.617, Precision: 0.606
# F-Score(lvc) = 0.459, Recall: 0.5, Precision: 0.424
# F-Score(ireflv) = 0.584, Recall: 0.484, Precision: 0.738
# F-Score(id) = 0.749, Recall: 0.887, Precision: 0.648
********************
********************
# XP = FR: Lemma(125) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1847308
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 125)       1836625     input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 25)        5875        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 500)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 100)          0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 600)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            4808        concatenate_10[0][0]             
==================================================================================================
Total params: 1,847,308
Trainable params: 1,847,308
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0811 - acc: 0.9842 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0753 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0766 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0776 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0783 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0792 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0791 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0801 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0815 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0814 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0810 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0813 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0827 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0835 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0832 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0847 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0855 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0868 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0851 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0874 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0872 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0883 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0877 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0892 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0888 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0887 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0906 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0901 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0908 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0911 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0915 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0918 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0920 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0939 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0928 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0940 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0945 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0939 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0944 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0949 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:57.462050
# F-Score(Ordinary) = 0.613, Recall: 0.624, Precision: 0.602
# F-Score(lvc) = 0.504, Recall: 0.555, Precision: 0.462
# F-Score(ireflv) = 0.661, Recall: 0.521, Precision: 0.902
# F-Score(id) = 0.636, Recall: 0.881, Precision: 0.497
********************
********************
# XP = FR: Lemma(125) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 160
# Features = False
Deep model(Non compositional)
# Parameters = 1849978
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 125)       1836625     input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 35)        8225        input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 500)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 140)          0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 640)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 8)            5128        concatenate_11[0][0]             
==================================================================================================
Total params: 1,849,978
Trainable params: 1,849,978
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0821 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0757 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0766 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0774 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0792 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0804 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0806 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0813 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0839 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0829 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0836 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0842 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0857 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0854 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0853 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0865 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0870 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0871 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0869 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0892 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0889 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0904 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0889 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0899 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0902 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0903 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0916 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0912 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0924 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0932 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0929 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0924 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0930 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0955 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0942 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0949 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0975 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0955 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0956 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0963 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:45.045839
# F-Score(Ordinary) = 0.647, Recall: 0.627, Precision: 0.669
# F-Score(lvc) = 0.428, Recall: 0.383, Precision: 0.485
# F-Score(ireflv) = 0.737, Recall: 0.615, Precision: 0.918
# F-Score(id) = 0.755, Recall: 0.96, Precision: 0.622
********************
********************
# XP = FR: Lemma(125) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 160
# Features = False
Deep model(Non compositional)
# Parameters = 1849978
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 125)       1836625     input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 35)        8225        input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 500)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 140)          0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 640)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            5128        concatenate_12[0][0]             
==================================================================================================
Total params: 1,849,978
Trainable params: 1,849,978
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 14s - loss: 0.0819 - acc: 0.9839 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 14s - loss: 0.0755 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 14s - loss: 0.0772 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 14s - loss: 0.0776 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 14s - loss: 0.0802 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 14s - loss: 0.0801 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 14s - loss: 0.0807 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 14s - loss: 0.0820 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 14s - loss: 0.0831 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 14s - loss: 0.0837 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 14s - loss: 0.0837 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 14s - loss: 0.0841 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 14s - loss: 0.0851 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 14s - loss: 0.0851 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 14s - loss: 0.0861 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 14s - loss: 0.0866 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 14s - loss: 0.0876 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 14s - loss: 0.0873 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 14s - loss: 0.0876 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 14s - loss: 0.0877 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 14s - loss: 0.0887 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 14s - loss: 0.0887 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 14s - loss: 0.0887 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 14s - loss: 0.0895 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 14s - loss: 0.0898 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 14s - loss: 0.0909 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 14s - loss: 0.0901 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 14s - loss: 0.0906 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 14s - loss: 0.0914 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 14s - loss: 0.0913 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 14s - loss: 0.0924 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 14s - loss: 0.0916 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 14s - loss: 0.0926 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 14s - loss: 0.0947 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 14s - loss: 0.0934 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 14s - loss: 0.0932 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 14s - loss: 0.0936 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 14s - loss: 0.0953 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 14s - loss: 0.0958 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 14s - loss: 0.0964 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:10:59.866495
# F-Score(Ordinary) = 0.516, Recall: 0.431, Precision: 0.642
# F-Score(lvc) = 0.446, Recall: 0.402, Precision: 0.5
# F-Score(ireflv) = 0.49, Recall: 0.34, Precision: 0.877
# F-Score(id) = 0.595, Recall: 0.604, Precision: 0.585
********************
********************
# XP = FR: Lemma(125) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 160
# Features = False
Deep model(Non compositional)
# Parameters = 1849978
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 125)       1836625     input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 35)        8225        input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 500)          0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 140)          0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 640)          0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 8)            5128        concatenate_13[0][0]             
==================================================================================================
Total params: 1,849,978
Trainable params: 1,849,978
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0825 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0748 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0754 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0767 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0783 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0792 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0795 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0799 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0813 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0813 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0821 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0836 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0832 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0830 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0838 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0852 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0859 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0858 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0859 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0871 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0882 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0868 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0891 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0879 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0893 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0895 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0899 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0915 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0905 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0920 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0927 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0926 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0933 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0928 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0935 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0934 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0942 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0943 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0962 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0967 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:35.419743
# F-Score(Ordinary) = 0.596, Recall: 0.605, Precision: 0.588
# F-Score(lvc) = 0.48, Recall: 0.581, Precision: 0.409
# F-Score(ireflv) = 0.599, Recall: 0.455, Precision: 0.877
# F-Score(id) = 0.673, Recall: 0.944, Precision: 0.523
********************
********************
# XP = FR: Lemma(125) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 160
# Features = False
Deep model(Non compositional)
# Parameters = 1849978
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 125)       1836625     input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 35)        8225        input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 500)          0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 140)          0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 640)          0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            5128        concatenate_14[0][0]             
==================================================================================================
Total params: 1,849,978
Trainable params: 1,849,978
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 14s - loss: 0.0819 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 14s - loss: 0.0769 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 14s - loss: 0.0775 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 14s - loss: 0.0784 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 14s - loss: 0.0796 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 14s - loss: 0.0797 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 14s - loss: 0.0808 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 14s - loss: 0.0825 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 14s - loss: 0.0822 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 14s - loss: 0.0823 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 14s - loss: 0.0831 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 14s - loss: 0.0823 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 14s - loss: 0.0836 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 14s - loss: 0.0844 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 14s - loss: 0.0867 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 14s - loss: 0.0869 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 14s - loss: 0.0878 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 14s - loss: 0.0876 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 14s - loss: 0.0877 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 14s - loss: 0.0883 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 14s - loss: 0.0887 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 14s - loss: 0.0883 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 14s - loss: 0.0893 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 14s - loss: 0.0897 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 14s - loss: 0.0918 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 14s - loss: 0.0910 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 14s - loss: 0.0921 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 14s - loss: 0.0933 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 14s - loss: 0.0923 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 14s - loss: 0.0940 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 14s - loss: 0.0946 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 14s - loss: 0.0949 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 14s - loss: 0.0955 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 14s - loss: 0.0954 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 14s - loss: 0.0971 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 14s - loss: 0.0931 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 14s - loss: 0.0967 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 14s - loss: 0.0965 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 14s - loss: 0.0974 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 14s - loss: 0.0977 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:01.351584
# F-Score(Ordinary) = 0.548, Recall: 0.526, Precision: 0.573
# F-Score(lvc) = 0.446, Recall: 0.643, Precision: 0.341
# F-Score(ireflv) = 0.647, Recall: 0.532, Precision: 0.828
# F-Score(id) = 0.496, Recall: 0.46, Precision: 0.539
********************
********************
# XP = FR: Lemma(125) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 160
# Features = False
Deep model(Non compositional)
# Parameters = 1849978
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 125)       1836625     input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 35)        8225        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 500)          0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 140)          0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 640)          0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 8)            5128        concatenate_15[0][0]             
==================================================================================================
Total params: 1,849,978
Trainable params: 1,849,978
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0820 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0754 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0766 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0773 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0800 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0798 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0802 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0826 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0815 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0828 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0844 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0842 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0856 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0864 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0863 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0865 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0874 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0874 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0861 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0884 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0884 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0892 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0885 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0895 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0904 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0908 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0914 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0922 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0910 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0938 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0924 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0929 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0933 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0944 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0946 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0968 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0935 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0963 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0967 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0954 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:19.514044
# F-Score(Ordinary) = 0.6, Recall: 0.584, Precision: 0.617
# F-Score(lvc) = 0.476, Recall: 0.5, Precision: 0.455
# F-Score(ireflv) = 0.627, Recall: 0.477, Precision: 0.918
# F-Score(id) = 0.66, Recall: 0.879, Precision: 0.528
********************
********************
# XP = FR: Lemma(125) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 1853983
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 125)       1836625     input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 50)        11750       input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 500)          0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 200)          0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 700)          0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            5608        concatenate_16[0][0]             
==================================================================================================
Total params: 1,853,983
Trainable params: 1,853,983
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0837 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0780 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0793 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0800 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0816 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0827 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0827 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0844 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0843 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0852 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0866 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0860 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0881 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0865 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0876 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0879 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0890 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0890 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0901 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0899 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0916 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0914 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0918 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0918 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0909 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0936 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0929 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0943 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0936 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0940 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0932 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0941 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0949 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0951 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0962 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0952 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0964 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0957 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0977 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0988 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:38.907238
# F-Score(Ordinary) = 0.629, Recall: 0.611, Precision: 0.649
# F-Score(lvc) = 0.512, Recall: 0.542, Precision: 0.485
# F-Score(ireflv) = 0.611, Recall: 0.464, Precision: 0.893
# F-Score(id) = 0.737, Recall: 0.951, Precision: 0.601
********************
********************
# XP = FR: Lemma(125) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 1853983
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 125)       1836625     input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 50)        11750       input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 500)          0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 200)          0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 700)          0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 8)            5608        concatenate_17[0][0]             
==================================================================================================
Total params: 1,853,983
Trainable params: 1,853,983
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0832 - acc: 0.9839 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0768 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0781 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0788 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0805 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0814 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0814 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0831 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0846 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0863 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0857 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0865 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0868 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0874 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0878 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0881 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0897 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0902 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0892 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0911 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0907 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0908 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0918 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0913 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0921 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0936 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0939 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0943 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0929 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0934 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0944 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0942 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0961 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0974 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0973 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0965 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0962 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0979 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0985 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0985 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:42.073328
# F-Score(Ordinary) = 0.617, Recall: 0.578, Precision: 0.66
# F-Score(lvc) = 0.45, Recall: 0.426, Precision: 0.477
# F-Score(ireflv) = 0.685, Recall: 0.55, Precision: 0.91
# F-Score(id) = 0.667, Recall: 0.741, Precision: 0.606
********************
********************
# XP = FR: Lemma(125) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 1853983
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 125)       1836625     input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 50)        11750       input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 500)          0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 200)          0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 700)          0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            5608        concatenate_18[0][0]             
==================================================================================================
Total params: 1,853,983
Trainable params: 1,853,983
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0837 - acc: 0.9840 - val_loss: 5.6029e-06 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0761 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0769 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0787 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0806 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0807 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0819 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0828 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0838 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0839 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0844 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0856 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0856 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0864 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0867 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0872 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0871 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0889 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0890 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0908 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0896 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0894 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0918 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0912 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0917 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0917 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0918 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0933 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0937 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0938 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0931 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0928 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0948 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0952 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0954 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0963 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0978 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0966 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.0973 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0971 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:41.141787
# F-Score(Ordinary) = 0.623, Recall: 0.685, Precision: 0.57
# F-Score(lvc) = 0.508, Recall: 0.842, Precision: 0.364
# F-Score(ireflv) = 0.604, Recall: 0.487, Precision: 0.795
# F-Score(id) = 0.706, Recall: 0.94, Precision: 0.565
********************
********************
# XP = FR: Lemma(125) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 1853983
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 125)       1836625     input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 50)        11750       input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 500)          0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 200)          0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 700)          0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 8)            5608        concatenate_19[0][0]             
==================================================================================================
Total params: 1,853,983
Trainable params: 1,853,983
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0831 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0767 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0776 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0782 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0803 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0808 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0812 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0825 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0832 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0841 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0842 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0840 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0862 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0868 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0868 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0876 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0885 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0883 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0907 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0894 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0908 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.0906 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0916 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.0917 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.0910 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.0923 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.0927 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.0937 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.0945 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.0940 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.0957 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.0955 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.0957 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.0966 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.0965 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.0965 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.0966 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.0978 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.0983 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.0985 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:56.035247
# F-Score(Ordinary) = 0.616, Recall: 0.59, Precision: 0.644
# F-Score(lvc) = 0.509, Recall: 0.675, Precision: 0.409
# F-Score(ireflv) = 0.601, Recall: 0.451, Precision: 0.902
# F-Score(id) = 0.689, Recall: 0.75, Precision: 0.637
********************
********************
# XP = FR: Lemma(125) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 1853983
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_39 (Embedding)        (None, 4, 125)       1836625     input_39[0][0]                   
__________________________________________________________________________________________________
embedding_40 (Embedding)        (None, 4, 50)        11750       input_40[0][0]                   
__________________________________________________________________________________________________
flatten_39 (Flatten)            (None, 500)          0           embedding_39[0][0]               
__________________________________________________________________________________________________
flatten_40 (Flatten)            (None, 200)          0           embedding_40[0][0]               
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 700)          0           flatten_39[0][0]                 
                                                                 flatten_40[0][0]                 
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            5608        concatenate_20[0][0]             
==================================================================================================
Total params: 1,853,983
Trainable params: 1,853,983
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0826 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0761 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0774 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0778 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0793 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0803 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0798 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0815 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0826 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0840 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0845 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0844 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0860 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0859 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0865 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0890 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0891 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.0900 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0899 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.0907 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.0909 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.0916 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.0917 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.0922 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.0932 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.0933 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.0930 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.0936 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.0938 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.0969 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.0971 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.0964 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.0963 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.0974 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.0976 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.0972 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.0965 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.0985 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1000 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.0977 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:42.679722
# F-Score(Ordinary) = 0.584, Recall: 0.579, Precision: 0.588
# F-Score(lvc) = 0.512, Recall: 0.533, Precision: 0.492
# F-Score(ireflv) = 0.614, Recall: 0.47, Precision: 0.885
# F-Score(id) = 0.599, Recall: 0.871, Precision: 0.456
********************
********************
# XP = FR: Lemma(150) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 165
# Features = False
Deep model(Non compositional)
# Parameters = 2212763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_41 (Embedding)        (None, 4, 150)       2203950     input_41[0][0]                   
__________________________________________________________________________________________________
embedding_42 (Embedding)        (None, 4, 15)        3525        input_42[0][0]                   
__________________________________________________________________________________________________
flatten_41 (Flatten)            (None, 600)          0           embedding_41[0][0]               
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 60)           0           embedding_42[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 660)          0           flatten_41[0][0]                 
                                                                 flatten_42[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 8)            5288        concatenate_21[0][0]             
==================================================================================================
Total params: 2,212,763
Trainable params: 2,212,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0831 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0797 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0816 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0830 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0856 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0872 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0876 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0902 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0914 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0920 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0932 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0925 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0959 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0958 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0969 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0966 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0983 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0991 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0980 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.0989 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1002 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1013 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.0998 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1025 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1022 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1006 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1033 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1056 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1052 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1080 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1054 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1052 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1063 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1074 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1065 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1066 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1153 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1180 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1161 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1190 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:02.362071
# F-Score(Ordinary) = 0.548, Recall: 0.48, Precision: 0.64
# F-Score(lvc) = 0.393, Recall: 0.351, Precision: 0.447
# F-Score(ireflv) = 0.56, Recall: 0.426, Precision: 0.82
# F-Score(id) = 0.616, Recall: 0.621, Precision: 0.611
********************
********************
# XP = FR: Lemma(150) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 165
# Features = False
Deep model(Non compositional)
# Parameters = 2212763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 4, 150)       2203950     input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 4, 15)        3525        input_44[0][0]                   
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 600)          0           embedding_43[0][0]               
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 60)           0           embedding_44[0][0]               
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 660)          0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            5288        concatenate_22[0][0]             
==================================================================================================
Total params: 2,212,763
Trainable params: 2,212,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0842 - acc: 0.9834 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0797 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0833 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0843 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0857 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0871 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0893 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0920 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0919 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0933 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0937 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0956 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0957 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0971 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0976 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0991 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.0999 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.1003 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.0999 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.1033 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.1029 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.1037 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.1058 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.1050 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.1039 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.1046 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.1071 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.1059 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.1084 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.1072 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.1092 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.1082 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.1093 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.1105 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1107 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.1119 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.1120 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.1165 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1171 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.1142 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:42.889595
# F-Score(Ordinary) = 0.479, Recall: 0.364, Precision: 0.698
# F-Score(lvc) = 0.261, Recall: 0.172, Precision: 0.538
# F-Score(ireflv) = 0.671, Recall: 0.54, Precision: 0.885
# F-Score(id) = 0.54, Recall: 0.484, Precision: 0.611
********************
********************
# XP = FR: Lemma(150) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 165
# Features = False
Deep model(Non compositional)
# Parameters = 2212763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_45 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 4, 150)       2203950     input_45[0][0]                   
__________________________________________________________________________________________________
embedding_46 (Embedding)        (None, 4, 15)        3525        input_46[0][0]                   
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 600)          0           embedding_45[0][0]               
__________________________________________________________________________________________________
flatten_46 (Flatten)            (None, 60)           0           embedding_46[0][0]               
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 660)          0           flatten_45[0][0]                 
                                                                 flatten_46[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 8)            5288        concatenate_23[0][0]             
==================================================================================================
Total params: 2,212,763
Trainable params: 2,212,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0844 - acc: 0.9837 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0804 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0826 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0843 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0873 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0880 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0884 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0896 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0919 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0944 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0961 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0978 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0979 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0981 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0989 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1006 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1009 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1002 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1006 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1031 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1050 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1027 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1034 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1051 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1061 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1062 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1053 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1063 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1066 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1079 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1079 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1097 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1087 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1058 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1091 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1089 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1099 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1075 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1112 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1109 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:07.375302
# F-Score(Ordinary) = 0.562, Recall: 0.52, Precision: 0.611
# F-Score(lvc) = 0.529, Recall: 0.632, Precision: 0.455
# F-Score(ireflv) = 0.461, Recall: 0.32, Precision: 0.828
# F-Score(id) = 0.695, Recall: 0.946, Precision: 0.549
********************
********************
# XP = FR: Lemma(150) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 165
# Features = False
Deep model(Non compositional)
# Parameters = 2212763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_47 (Embedding)        (None, 4, 150)       2203950     input_47[0][0]                   
__________________________________________________________________________________________________
embedding_48 (Embedding)        (None, 4, 15)        3525        input_48[0][0]                   
__________________________________________________________________________________________________
flatten_47 (Flatten)            (None, 600)          0           embedding_47[0][0]               
__________________________________________________________________________________________________
flatten_48 (Flatten)            (None, 60)           0           embedding_48[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 660)          0           flatten_47[0][0]                 
                                                                 flatten_48[0][0]                 
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            5288        concatenate_24[0][0]             
==================================================================================================
Total params: 2,212,763
Trainable params: 2,212,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0846 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0821 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0834 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0850 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0870 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0876 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0891 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0908 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0920 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0950 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0942 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0947 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0988 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0980 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0998 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0996 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1018 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1006 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1018 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1027 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1029 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1039 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1037 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1050 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1059 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1062 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1063 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1065 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1075 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1069 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1094 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1089 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1114 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1126 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1130 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1119 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1139 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1193 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1187 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1240 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:12.216402
# F-Score(Ordinary) = 0.51, Recall: 0.421, Precision: 0.647
# F-Score(lvc) = 0.383, Recall: 0.314, Precision: 0.492
# F-Score(ireflv) = 0.536, Recall: 0.398, Precision: 0.82
# F-Score(id) = 0.567, Recall: 0.532, Precision: 0.606
********************
********************
# XP = FR: Lemma(150) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 165
# Features = False
Deep model(Non compositional)
# Parameters = 2212763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_49 (Embedding)        (None, 4, 150)       2203950     input_49[0][0]                   
__________________________________________________________________________________________________
embedding_50 (Embedding)        (None, 4, 15)        3525        input_50[0][0]                   
__________________________________________________________________________________________________
flatten_49 (Flatten)            (None, 600)          0           embedding_49[0][0]               
__________________________________________________________________________________________________
flatten_50 (Flatten)            (None, 60)           0           embedding_50[0][0]               
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 660)          0           flatten_49[0][0]                 
                                                                 flatten_50[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 8)            5288        concatenate_25[0][0]             
==================================================================================================
Total params: 2,212,763
Trainable params: 2,212,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0837 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0792 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0798 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0826 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0853 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0858 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0876 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0900 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0892 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0906 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0926 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0931 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0947 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0945 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0957 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0976 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0982 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.0993 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.0991 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1006 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.0987 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1001 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1013 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1028 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1036 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1029 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1034 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1046 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1046 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1049 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1057 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1066 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1058 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1100 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1077 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1099 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1097 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1091 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1174 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1175 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:59.728900
# F-Score(Ordinary) = 0.558, Recall: 0.501, Precision: 0.631
# F-Score(lvc) = 0.466, Recall: 0.437, Precision: 0.5
# F-Score(ireflv) = 0.545, Recall: 0.394, Precision: 0.885
# F-Score(id) = 0.632, Recall: 0.785, Precision: 0.528
********************
********************
# XP = FR: Lemma(150) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 2215433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_51 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_51 (Embedding)        (None, 4, 150)       2203950     input_51[0][0]                   
__________________________________________________________________________________________________
embedding_52 (Embedding)        (None, 4, 25)        5875        input_52[0][0]                   
__________________________________________________________________________________________________
flatten_51 (Flatten)            (None, 600)          0           embedding_51[0][0]               
__________________________________________________________________________________________________
flatten_52 (Flatten)            (None, 100)          0           embedding_52[0][0]               
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 700)          0           flatten_51[0][0]                 
                                                                 flatten_52[0][0]                 
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            5608        concatenate_26[0][0]             
==================================================================================================
Total params: 2,215,433
Trainable params: 2,215,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0856 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0823 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0833 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0850 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0877 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0889 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0893 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0914 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0926 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0936 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0948 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0955 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0972 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0973 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0985 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0990 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1003 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1010 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1017 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1030 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1031 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1039 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1034 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1049 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1052 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1050 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1082 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1075 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1110 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1180 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1177 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1180 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1181 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1210 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1207 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1205 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1235 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1337 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1413 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1423 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:00.369778
# F-Score(Ordinary) = 0.481, Recall: 0.372, Precision: 0.68
# F-Score(lvc) = 0.347, Recall: 0.266, Precision: 0.5
# F-Score(ireflv) = 0.615, Recall: 0.484, Precision: 0.844
# F-Score(id) = 0.44, Recall: 0.339, Precision: 0.627
********************
********************
# XP = FR: Lemma(150) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 2215433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_53 (Embedding)        (None, 4, 150)       2203950     input_53[0][0]                   
__________________________________________________________________________________________________
embedding_54 (Embedding)        (None, 4, 25)        5875        input_54[0][0]                   
__________________________________________________________________________________________________
flatten_53 (Flatten)            (None, 600)          0           embedding_53[0][0]               
__________________________________________________________________________________________________
flatten_54 (Flatten)            (None, 100)          0           embedding_54[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 700)          0           flatten_53[0][0]                 
                                                                 flatten_54[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 8)            5608        concatenate_27[0][0]             
==================================================================================================
Total params: 2,215,433
Trainable params: 2,215,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0860 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0815 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0846 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0853 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0878 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0896 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0911 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0928 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0947 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0962 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0962 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0981 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0986 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0997 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.1003 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1017 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1012 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1025 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1034 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1036 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1031 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1048 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1059 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1061 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1088 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1082 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1087 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1077 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1092 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1158 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1172 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1195 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1196 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1217 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1219 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1205 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1199 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1232 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1359 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1425 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:02.302338
# F-Score(Ordinary) = 0.49, Recall: 0.375, Precision: 0.709
# F-Score(lvc) = 0.432, Recall: 0.411, Precision: 0.455
# F-Score(ireflv) = 0.603, Recall: 0.453, Precision: 0.902
# F-Score(id) = 0.388, Recall: 0.276, Precision: 0.653
********************
********************
# XP = FR: Lemma(150) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 2215433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_55 (Embedding)        (None, 4, 150)       2203950     input_55[0][0]                   
__________________________________________________________________________________________________
embedding_56 (Embedding)        (None, 4, 25)        5875        input_56[0][0]                   
__________________________________________________________________________________________________
flatten_55 (Flatten)            (None, 600)          0           embedding_55[0][0]               
__________________________________________________________________________________________________
flatten_56 (Flatten)            (None, 100)          0           embedding_56[0][0]               
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 700)          0           flatten_55[0][0]                 
                                                                 flatten_56[0][0]                 
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            5608        concatenate_28[0][0]             
==================================================================================================
Total params: 2,215,433
Trainable params: 2,215,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0864 - acc: 0.9834 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0814 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0836 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0859 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0883 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0887 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0903 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0914 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0934 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0938 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0946 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0964 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0965 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0968 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0983 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.0991 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.0995 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1005 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1009 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1011 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1032 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1032 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1032 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1025 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1055 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1046 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1043 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1066 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1068 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1081 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1073 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1078 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1102 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1084 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1143 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1171 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1200 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1229 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1375 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1397 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:05.719903
# F-Score(Ordinary) = 0.54, Recall: 0.448, Precision: 0.68
# F-Score(lvc) = 0.407, Recall: 0.34, Precision: 0.508
# F-Score(ireflv) = 0.563, Recall: 0.426, Precision: 0.828
# F-Score(id) = 0.548, Recall: 0.49, Precision: 0.622
********************
********************
# XP = FR: Lemma(150) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 2215433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_57 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_58 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_57 (Embedding)        (None, 4, 150)       2203950     input_57[0][0]                   
__________________________________________________________________________________________________
embedding_58 (Embedding)        (None, 4, 25)        5875        input_58[0][0]                   
__________________________________________________________________________________________________
flatten_57 (Flatten)            (None, 600)          0           embedding_57[0][0]               
__________________________________________________________________________________________________
flatten_58 (Flatten)            (None, 100)          0           embedding_58[0][0]               
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 700)          0           flatten_57[0][0]                 
                                                                 flatten_58[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 8)            5608        concatenate_29[0][0]             
==================================================================================================
Total params: 2,215,433
Trainable params: 2,215,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0851 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0814 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0821 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0849 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0878 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0891 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0897 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0920 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0918 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0939 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0942 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0954 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0970 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0981 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0998 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1006 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1015 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1002 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1009 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1014 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1026 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1031 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1033 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1037 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1042 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1057 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1063 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1058 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1071 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1074 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1114 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1090 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1117 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1171 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1186 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1223 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1342 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1385 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1380 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1375 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:11.417167
# F-Score(Ordinary) = 0.54, Recall: 0.435, Precision: 0.711
# F-Score(lvc) = 0.393, Recall: 0.315, Precision: 0.523
# F-Score(ireflv) = 0.54, Recall: 0.407, Precision: 0.803
# F-Score(id) = 0.553, Recall: 0.474, Precision: 0.663
********************
********************
# XP = FR: Lemma(150) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 175
# Features = False
Deep model(Non compositional)
# Parameters = 2215433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_59 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_60 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_59 (Embedding)        (None, 4, 150)       2203950     input_59[0][0]                   
__________________________________________________________________________________________________
embedding_60 (Embedding)        (None, 4, 25)        5875        input_60[0][0]                   
__________________________________________________________________________________________________
flatten_59 (Flatten)            (None, 600)          0           embedding_59[0][0]               
__________________________________________________________________________________________________
flatten_60 (Flatten)            (None, 100)          0           embedding_60[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 700)          0           flatten_59[0][0]                 
                                                                 flatten_60[0][0]                 
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            5608        concatenate_30[0][0]             
==================================================================================================
Total params: 2,215,433
Trainable params: 2,215,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0859 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0819 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0837 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0853 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0878 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0893 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0915 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0924 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0939 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0946 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0964 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0973 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0984 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0993 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0998 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.1004 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.1019 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.1023 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.1017 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.1047 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.1028 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.1053 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.1061 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.1077 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.1083 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.1167 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.1181 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.1191 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.1218 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.1183 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.1176 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.1205 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.1212 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.1221 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1212 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.1217 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.1236 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.1217 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1307 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.1436 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:45.072118
# F-Score(Ordinary) = 0.552, Recall: 0.454, Precision: 0.702
# F-Score(lvc) = 0.554, Recall: 0.728, Precision: 0.447
# F-Score(ireflv) = 0.747, Recall: 0.641, Precision: 0.893
# F-Score(id) = 0.411, Recall: 0.295, Precision: 0.674
********************
********************
# XP = FR: Lemma(150) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 185
# Features = False
Deep model(Non compositional)
# Parameters = 2218103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_61 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_62 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_61 (Embedding)        (None, 4, 150)       2203950     input_61[0][0]                   
__________________________________________________________________________________________________
embedding_62 (Embedding)        (None, 4, 35)        8225        input_62[0][0]                   
__________________________________________________________________________________________________
flatten_61 (Flatten)            (None, 600)          0           embedding_61[0][0]               
__________________________________________________________________________________________________
flatten_62 (Flatten)            (None, 140)          0           embedding_62[0][0]               
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 740)          0           flatten_61[0][0]                 
                                                                 flatten_62[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 8)            5928        concatenate_31[0][0]             
==================================================================================================
Total params: 2,218,103
Trainable params: 2,218,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0867 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0829 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0845 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0868 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0878 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0899 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0900 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0909 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0922 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0931 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0936 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0950 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0980 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.0972 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.0978 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.0990 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.1009 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.1007 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.1008 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.1025 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1025 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.1040 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.1038 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1050 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.1036 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.1058 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.1073 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.1066 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.1079 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1098 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1084 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1126 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1120 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.1116 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1181 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.1200 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.1204 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.1212 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1207 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.1246 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:48.580121
# F-Score(Ordinary) = 0.534, Recall: 0.46, Precision: 0.635
# F-Score(lvc) = 0.406, Recall: 0.429, Precision: 0.386
# F-Score(ireflv) = 0.512, Recall: 0.374, Precision: 0.811
# F-Score(id) = 0.59, Recall: 0.549, Precision: 0.637
********************
********************
# XP = FR: Lemma(150) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 185
# Features = False
Deep model(Non compositional)
# Parameters = 2218103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_63 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_64 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_63 (Embedding)        (None, 4, 150)       2203950     input_63[0][0]                   
__________________________________________________________________________________________________
embedding_64 (Embedding)        (None, 4, 35)        8225        input_64[0][0]                   
__________________________________________________________________________________________________
flatten_63 (Flatten)            (None, 600)          0           embedding_63[0][0]               
__________________________________________________________________________________________________
flatten_64 (Flatten)            (None, 140)          0           embedding_64[0][0]               
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 740)          0           flatten_63[0][0]                 
                                                                 flatten_64[0][0]                 
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            5928        concatenate_32[0][0]             
==================================================================================================
Total params: 2,218,103
Trainable params: 2,218,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0867 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0819 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0832 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0848 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0871 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0891 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0906 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0922 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0940 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0963 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0969 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0969 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0983 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.1000 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.1017 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.1010 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.1028 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.1026 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.1024 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.1045 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.1053 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.1051 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.1081 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.1068 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.1065 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.1083 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.1072 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.1093 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.1085 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.1096 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.1114 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.1124 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.1135 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.1177 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1192 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.1194 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.1196 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.1207 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1314 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.1464 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:31.093174
# F-Score(Ordinary) = 0.476, Recall: 0.359, Precision: 0.705
# F-Score(lvc) = 0.448, Recall: 0.416, Precision: 0.485
# F-Score(ireflv) = 0.385, Recall: 0.246, Precision: 0.885
# F-Score(id) = 0.516, Recall: 0.433, Precision: 0.637
********************
********************
# XP = FR: Lemma(150) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 185
# Features = False
Deep model(Non compositional)
# Parameters = 2218103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_65 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_66 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_65 (Embedding)        (None, 4, 150)       2203950     input_65[0][0]                   
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 4, 35)        8225        input_66[0][0]                   
__________________________________________________________________________________________________
flatten_65 (Flatten)            (None, 600)          0           embedding_65[0][0]               
__________________________________________________________________________________________________
flatten_66 (Flatten)            (None, 140)          0           embedding_66[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 740)          0           flatten_65[0][0]                 
                                                                 flatten_66[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 8)            5928        concatenate_33[0][0]             
==================================================================================================
Total params: 2,218,103
Trainable params: 2,218,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0870 - acc: 0.9835 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0821 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0845 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0874 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0884 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0908 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0921 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0936 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0952 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0957 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0971 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0994 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0981 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.0990 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.0997 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1021 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1028 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1011 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1014 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1030 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1058 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1045 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1054 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1059 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1072 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1077 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1059 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1075 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1090 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1087 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1101 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1089 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1081 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1083 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1127 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1173 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1183 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1212 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1212 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1223 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:05.537936
# F-Score(Ordinary) = 0.509, Recall: 0.428, Precision: 0.629
# F-Score(lvc) = 0.401, Recall: 0.342, Precision: 0.485
# F-Score(ireflv) = 0.458, Recall: 0.314, Precision: 0.844
# F-Score(id) = 0.696, Recall: 0.868, Precision: 0.58
********************
********************
# XP = FR: Lemma(150) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 185
# Features = False
Deep model(Non compositional)
# Parameters = 2218103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_67 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_68 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 4, 150)       2203950     input_67[0][0]                   
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 4, 35)        8225        input_68[0][0]                   
__________________________________________________________________________________________________
flatten_67 (Flatten)            (None, 600)          0           embedding_67[0][0]               
__________________________________________________________________________________________________
flatten_68 (Flatten)            (None, 140)          0           embedding_68[0][0]               
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 740)          0           flatten_67[0][0]                 
                                                                 flatten_68[0][0]                 
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            5928        concatenate_34[0][0]             
==================================================================================================
Total params: 2,218,103
Trainable params: 2,218,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0860 - acc: 0.9837 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0821 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0833 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0849 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0870 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0891 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0900 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0907 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0937 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0947 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0954 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0967 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0973 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.1009 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.1008 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.1011 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.1031 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.1020 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.1044 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.1040 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.1040 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.1054 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.1059 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.1073 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.1077 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.1084 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1096 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.1119 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.1107 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.1098 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.1120 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.1170 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.1183 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.1210 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1246 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.1255 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.1228 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.1245 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1292 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1451 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:35.023790
# F-Score(Ordinary) = 0.523, Recall: 0.414, Precision: 0.709
# F-Score(lvc) = 0.488, Recall: 0.5, Precision: 0.477
# F-Score(ireflv) = 0.59, Recall: 0.438, Precision: 0.902
# F-Score(id) = 0.423, Recall: 0.317, Precision: 0.637
********************
********************
# XP = FR: Lemma(150) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 185
# Features = False
Deep model(Non compositional)
# Parameters = 2218103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_69 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_70 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_69 (Embedding)        (None, 4, 150)       2203950     input_69[0][0]                   
__________________________________________________________________________________________________
embedding_70 (Embedding)        (None, 4, 35)        8225        input_70[0][0]                   
__________________________________________________________________________________________________
flatten_69 (Flatten)            (None, 600)          0           embedding_69[0][0]               
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 140)          0           embedding_70[0][0]               
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 740)          0           flatten_69[0][0]                 
                                                                 flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 8)            5928        concatenate_35[0][0]             
==================================================================================================
Total params: 2,218,103
Trainable params: 2,218,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0858 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0816 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0835 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0861 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0891 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0902 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0916 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0934 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0944 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0950 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0969 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0978 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.0997 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.1005 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.1021 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1025 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1030 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1034 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1025 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1043 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1045 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1043 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1073 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1059 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1079 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1078 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1071 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1092 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1080 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1108 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1115 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1130 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1151 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1227 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1249 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1212 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1214 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1238 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1260 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1356 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:58.361021
# F-Score(Ordinary) = 0.425, Recall: 0.302, Precision: 0.716
# F-Score(lvc) = 0.563, Recall: 0.741, Precision: 0.455
# F-Score(ireflv) = 0.262, Recall: 0.153, Precision: 0.918
# F-Score(id) = 0.532, Recall: 0.477, Precision: 0.601
********************
********************
# XP = FR: Lemma(150) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 200
# Features = False
Deep model(Non compositional)
# Parameters = 2222108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_71 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_72 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_71 (Embedding)        (None, 4, 150)       2203950     input_71[0][0]                   
__________________________________________________________________________________________________
embedding_72 (Embedding)        (None, 4, 50)        11750       input_72[0][0]                   
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 600)          0           embedding_71[0][0]               
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 200)          0           embedding_72[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 800)          0           flatten_71[0][0]                 
                                                                 flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            6408        concatenate_36[0][0]             
==================================================================================================
Total params: 2,222,108
Trainable params: 2,222,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0881 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0839 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0856 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0866 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0892 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0904 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0911 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0931 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0949 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0958 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0971 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0977 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.1015 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.1016 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.1025 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.1016 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.1061 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.1044 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.1041 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.1059 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.1077 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.1072 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.1090 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.1084 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.1067 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.1085 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.1116 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.1123 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.1104 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.1137 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.1129 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.1126 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.1120 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.1195 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1186 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.1209 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.1232 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.1240 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1330 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.1404 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:25.447787
# F-Score(Ordinary) = 0.569, Recall: 0.489, Precision: 0.68
# F-Score(lvc) = 0.474, Recall: 0.464, Precision: 0.485
# F-Score(ireflv) = 0.554, Recall: 0.4, Precision: 0.902
# F-Score(id) = 0.597, Recall: 0.574, Precision: 0.622
********************
********************
# XP = FR: Lemma(150) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 200
# Features = False
Deep model(Non compositional)
# Parameters = 2222108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_73 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_74 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_73 (Embedding)        (None, 4, 150)       2203950     input_73[0][0]                   
__________________________________________________________________________________________________
embedding_74 (Embedding)        (None, 4, 50)        11750       input_74[0][0]                   
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 600)          0           embedding_73[0][0]               
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 200)          0           embedding_74[0][0]               
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 800)          0           flatten_73[0][0]                 
                                                                 flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 8)            6408        concatenate_37[0][0]             
==================================================================================================
Total params: 2,222,108
Trainable params: 2,222,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0881 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0830 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0860 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0865 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0888 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0906 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0918 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0936 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0948 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0976 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0981 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0996 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.0997 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.1008 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.1037 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.1022 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.1043 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.1024 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.1042 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.1052 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.1082 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1090 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.1090 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1066 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.1099 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1100 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1098 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1105 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1106 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1113 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1127 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1144 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1178 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1239 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1255 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1274 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1406 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1416 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1399 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.1406 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:48.782782
# F-Score(Ordinary) = 0.52, Recall: 0.407, Precision: 0.72
# F-Score(lvc) = 0.421, Recall: 0.356, Precision: 0.515
# F-Score(ireflv) = 0.75, Recall: 0.651, Precision: 0.885
# F-Score(id) = 0.408, Recall: 0.295, Precision: 0.663
********************
********************
# XP = FR: Lemma(150) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 200
# Features = False
Deep model(Non compositional)
# Parameters = 2222108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_75 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_76 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_75 (Embedding)        (None, 4, 150)       2203950     input_75[0][0]                   
__________________________________________________________________________________________________
embedding_76 (Embedding)        (None, 4, 50)        11750       input_76[0][0]                   
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 600)          0           embedding_75[0][0]               
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 200)          0           embedding_76[0][0]               
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 800)          0           flatten_75[0][0]                 
                                                                 flatten_76[0][0]                 
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            6408        concatenate_38[0][0]             
==================================================================================================
Total params: 2,222,108
Trainable params: 2,222,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0881 - acc: 0.9834 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0819 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0835 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0858 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0896 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0905 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0920 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0932 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0960 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0970 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0982 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0992 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.1007 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.1000 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.1024 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1028 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1025 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1031 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1031 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1048 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1083 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1074 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1094 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1084 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1111 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1140 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1150 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1206 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1215 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1198 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1203 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1239 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1235 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1236 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1243 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1291 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1399 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1477 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1407 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1414 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:08.172658
# F-Score(Ordinary) = 0.486, Recall: 0.367, Precision: 0.72
# F-Score(lvc) = 0.512, Recall: 0.483, Precision: 0.545
# F-Score(ireflv) = 0.326, Recall: 0.199, Precision: 0.893
# F-Score(id) = 0.647, Recall: 0.669, Precision: 0.627
********************
********************
# XP = FR: Lemma(150) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 200
# Features = False
Deep model(Non compositional)
# Parameters = 2222108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_77 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_78 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_77 (Embedding)        (None, 4, 150)       2203950     input_77[0][0]                   
__________________________________________________________________________________________________
embedding_78 (Embedding)        (None, 4, 50)        11750       input_78[0][0]                   
__________________________________________________________________________________________________
flatten_77 (Flatten)            (None, 600)          0           embedding_77[0][0]               
__________________________________________________________________________________________________
flatten_78 (Flatten)            (None, 200)          0           embedding_78[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 800)          0           flatten_77[0][0]                 
                                                                 flatten_78[0][0]                 
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 8)            6408        concatenate_39[0][0]             
==================================================================================================
Total params: 2,222,108
Trainable params: 2,222,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 15s - loss: 0.0878 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 15s - loss: 0.0853 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 15s - loss: 0.0855 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 15s - loss: 0.0878 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 15s - loss: 0.0910 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 15s - loss: 0.0934 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 15s - loss: 0.0939 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 15s - loss: 0.0955 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 15s - loss: 0.0967 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 15s - loss: 0.0983 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 15s - loss: 0.0996 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 15s - loss: 0.0991 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 15s - loss: 0.1023 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 15s - loss: 0.1008 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 15s - loss: 0.1040 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 15s - loss: 0.1038 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 15s - loss: 0.1061 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 15s - loss: 0.1058 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 15s - loss: 0.1054 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 15s - loss: 0.1054 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 15s - loss: 0.1085 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 15s - loss: 0.1066 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 15s - loss: 0.1079 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 15s - loss: 0.1075 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 15s - loss: 0.1089 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 15s - loss: 0.1099 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 15s - loss: 0.1108 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 15s - loss: 0.1155 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 15s - loss: 0.1293 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 15s - loss: 0.1378 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 15s - loss: 0.1394 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 15s - loss: 0.1400 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 15s - loss: 0.1383 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 15s - loss: 0.1359 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 15s - loss: 0.1359 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 15s - loss: 0.1359 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 15s - loss: 0.1343 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 15s - loss: 0.1386 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 15s - loss: 0.1374 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 15s - loss: 0.1375 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:11:43.763854
# F-Score(Ordinary) = 0.43, Recall: 0.306, Precision: 0.72
# F-Score(lvc) = 0.302, Recall: 0.206, Precision: 0.568
# F-Score(ireflv) = 0.526, Recall: 0.37, Precision: 0.91
# F-Score(id) = 0.417, Recall: 0.312, Precision: 0.627
********************
********************
# XP = FR: Lemma(150) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 200
# Features = False
Deep model(Non compositional)
# Parameters = 2222108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_79 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_80 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_79 (Embedding)        (None, 4, 150)       2203950     input_79[0][0]                   
__________________________________________________________________________________________________
embedding_80 (Embedding)        (None, 4, 50)        11750       input_80[0][0]                   
__________________________________________________________________________________________________
flatten_79 (Flatten)            (None, 600)          0           embedding_79[0][0]               
__________________________________________________________________________________________________
flatten_80 (Flatten)            (None, 200)          0           embedding_80[0][0]               
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 800)          0           flatten_79[0][0]                 
                                                                 flatten_80[0][0]                 
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 8)            6408        concatenate_40[0][0]             
==================================================================================================
Total params: 2,222,108
Trainable params: 2,222,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 16s - loss: 0.0876 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 16s - loss: 0.0833 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 16s - loss: 0.0852 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 16s - loss: 0.0868 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 16s - loss: 0.0897 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 16s - loss: 0.0912 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 16s - loss: 0.0909 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 16s - loss: 0.0935 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 16s - loss: 0.0956 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 16s - loss: 0.0953 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 16s - loss: 0.0962 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 16s - loss: 0.0975 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 16s - loss: 0.1000 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 16s - loss: 0.1014 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 16s - loss: 0.1010 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 16s - loss: 0.1026 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 16s - loss: 0.1036 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 16s - loss: 0.1039 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 16s - loss: 0.1041 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 16s - loss: 0.1072 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 16s - loss: 0.1051 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 16s - loss: 0.1071 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 16s - loss: 0.1074 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 16s - loss: 0.1090 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 16s - loss: 0.1095 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 16s - loss: 0.1133 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 16s - loss: 0.1094 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 16s - loss: 0.1114 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 16s - loss: 0.1122 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 16s - loss: 0.1125 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 16s - loss: 0.1131 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 16s - loss: 0.1154 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 16s - loss: 0.1188 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 16s - loss: 0.1243 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 16s - loss: 0.1222 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 16s - loss: 0.1232 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 16s - loss: 0.1234 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 16s - loss: 0.1287 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 16s - loss: 0.1415 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 16s - loss: 0.1434 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:12:06.662923
# F-Score(Ordinary) = 0.525, Recall: 0.417, Precision: 0.709
# F-Score(lvc) = 0.487, Recall: 0.481, Precision: 0.492
# F-Score(ireflv) = 0.532, Recall: 0.373, Precision: 0.926
# F-Score(id) = 0.453, Recall: 0.362, Precision: 0.606
********************
