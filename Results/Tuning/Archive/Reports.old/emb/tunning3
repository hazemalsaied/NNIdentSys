INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
********************
# XP = FR: Lemma(70) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 85
# Features = False
Deep model(Non compositional)
# Parameters = 1034763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 70)        1028510     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 15)        3525        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 280)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 60)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 340)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 8)            2728        concatenate_1[0][0]              
==================================================================================================
Total params: 1,034,763
Trainable params: 1,034,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0722 - acc: 0.9855 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0634 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0626 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0623 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0629 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0628 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0628 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0633 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0634 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0632 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0631 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0637 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0641 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0641 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0644 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0646 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0649 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0646 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0648 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0654 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0657 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0662 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0653 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0660 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0656 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0658 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0659 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0664 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0669 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0672 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0670 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0668 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0674 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0676 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0676 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0676 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0675 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0677 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0685 - acc: 0.9889 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0686 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:42.665949
# F-Score(Ordinary) = 0.597, Recall: 0.548, Precision: 0.655
# F-Score(lvc) = 0.446, Recall: 0.425, Precision: 0.47
# F-Score(ireflv) = 0.581, Recall: 0.432, Precision: 0.885
# F-Score(id) = 0.737, Recall: 0.884, Precision: 0.632
********************
********************
# XP = FR: Lemma(70) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 85
# Features = False
Deep model(Non compositional)
# Parameters = 1034763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 70)        1028510     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 15)        3525        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 280)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 60)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 340)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            2728        concatenate_2[0][0]              
==================================================================================================
Total params: 1,034,763
Trainable params: 1,034,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0721 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 22s - loss: 0.0631 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 22s - loss: 0.0635 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 22s - loss: 0.0623 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 22s - loss: 0.0622 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 22s - loss: 0.0620 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 22s - loss: 0.0616 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 22s - loss: 0.0625 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 22s - loss: 0.0624 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 22s - loss: 0.0624 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 22s - loss: 0.0626 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 22s - loss: 0.0629 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0630 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0634 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0632 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0637 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0639 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0639 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0645 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0650 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0646 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0644 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0656 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0661 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0657 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0654 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0658 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0656 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0668 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0662 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0665 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0670 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0677 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0671 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0675 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0677 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0674 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0687 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0684 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0689 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:26.965359
# F-Score(Ordinary) = 0.566, Recall: 0.507, Precision: 0.64
# F-Score(lvc) = 0.407, Recall: 0.385, Precision: 0.432
# F-Score(ireflv) = 0.593, Recall: 0.459, Precision: 0.836
# F-Score(id) = 0.646, Recall: 0.644, Precision: 0.648
********************
********************
# XP = FR: Lemma(70) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 85
# Features = False
Deep model(Non compositional)
# Parameters = 1034763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 70)        1028510     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 15)        3525        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 280)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 60)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 340)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 8)            2728        concatenate_3[0][0]              
==================================================================================================
Total params: 1,034,763
Trainable params: 1,034,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0722 - acc: 0.9856 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 2/40
 - 22s - loss: 0.0634 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0626 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 22s - loss: 0.0621 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 22s - loss: 0.0620 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 22s - loss: 0.0626 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 22s - loss: 0.0623 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 22s - loss: 0.0618 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 22s - loss: 0.0624 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 22s - loss: 0.0625 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 22s - loss: 0.0626 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 22s - loss: 0.0624 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 22s - loss: 0.0638 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 22s - loss: 0.0639 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 22s - loss: 0.0633 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 22s - loss: 0.0638 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 22s - loss: 0.0638 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 22s - loss: 0.0642 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 22s - loss: 0.0647 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 22s - loss: 0.0656 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 22s - loss: 0.0651 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 22s - loss: 0.0655 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 22s - loss: 0.0655 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 22s - loss: 0.0661 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 22s - loss: 0.0662 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 22s - loss: 0.0665 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 22s - loss: 0.0653 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 22s - loss: 0.0674 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 22s - loss: 0.0660 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 22s - loss: 0.0672 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 22s - loss: 0.0671 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 22s - loss: 0.0675 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 22s - loss: 0.0672 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 22s - loss: 0.0678 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 22s - loss: 0.0688 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 22s - loss: 0.0683 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 22s - loss: 0.0691 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 22s - loss: 0.0687 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 22s - loss: 0.0689 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 22s - loss: 0.0698 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:10.301734
# F-Score(Ordinary) = 0.556, Recall: 0.538, Precision: 0.575
# F-Score(lvc) = 0.415, Recall: 0.573, Precision: 0.326
# F-Score(ireflv) = 0.499, Recall: 0.358, Precision: 0.82
# F-Score(id) = 0.716, Recall: 0.933, Precision: 0.58
********************
********************
# XP = FR: Lemma(70) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 85
# Features = False
Deep model(Non compositional)
# Parameters = 1034763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 70)        1028510     input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 15)        3525        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 280)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 60)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 340)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            2728        concatenate_4[0][0]              
==================================================================================================
Total params: 1,034,763
Trainable params: 1,034,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 22s - loss: 0.0721 - acc: 0.9855 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 2/40
 - 22s - loss: 0.0639 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 22s - loss: 0.0632 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 22s - loss: 0.0630 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 22s - loss: 0.0628 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 22s - loss: 0.0624 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 22s - loss: 0.0624 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 22s - loss: 0.0628 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 22s - loss: 0.0623 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 22s - loss: 0.0625 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 22s - loss: 0.0633 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 22s - loss: 0.0629 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 22s - loss: 0.0637 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 22s - loss: 0.0637 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 22s - loss: 0.0638 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 22s - loss: 0.0645 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0641 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0645 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 22s - loss: 0.0651 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 22s - loss: 0.0647 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0654 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 22s - loss: 0.0658 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0656 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0658 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0666 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0661 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0663 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0667 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 22s - loss: 0.0664 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 22s - loss: 0.0662 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 22s - loss: 0.0668 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 22s - loss: 0.0674 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 22s - loss: 0.0675 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 22s - loss: 0.0676 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 22s - loss: 0.0679 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 22s - loss: 0.0680 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 22s - loss: 0.0678 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 22s - loss: 0.0678 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 22s - loss: 0.0682 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 22s - loss: 0.0680 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:11.974747
# F-Score(Ordinary) = 0.554, Recall: 0.562, Precision: 0.546
# F-Score(lvc) = 0.413, Recall: 0.505, Precision: 0.348
# F-Score(ireflv) = 0.635, Recall: 0.568, Precision: 0.721
# F-Score(id) = 0.579, Recall: 0.588, Precision: 0.57
********************
********************
# XP = FR: Lemma(70) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 85
# Features = False
Deep model(Non compositional)
# Parameters = 1034763
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 70)        1028510     input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 15)        3525        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 280)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 60)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 340)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 8)            2728        concatenate_5[0][0]              
==================================================================================================
Total params: 1,034,763
Trainable params: 1,034,763
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0724 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 22s - loss: 0.0638 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 22s - loss: 0.0643 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0639 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0636 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0630 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0629 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0631 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0629 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0636 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0633 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0630 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0633 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0639 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0640 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0640 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0638 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0643 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0639 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0645 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0651 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0650 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0662 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0655 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0663 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0669 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0665 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0665 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0672 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0670 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0677 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0674 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0674 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0682 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0679 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0682 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0684 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0692 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0695 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0693 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:28.977555
# F-Score(Ordinary) = 0.6, Recall: 0.654, Precision: 0.555
# F-Score(lvc) = 0.554, Recall: 0.8, Precision: 0.424
# F-Score(ireflv) = 0.617, Recall: 0.484, Precision: 0.852
# F-Score(id) = 0.608, Recall: 0.935, Precision: 0.451
********************
********************
# XP = FR: Lemma(70) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 95
# Features = False
Deep model(Non compositional)
# Parameters = 1037433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 70)        1028510     input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 25)        5875        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 280)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 100)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 380)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            3048        concatenate_6[0][0]              
==================================================================================================
Total params: 1,037,433
Trainable params: 1,037,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0729 - acc: 0.9855 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0642 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0638 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0636 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0642 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0643 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0646 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0643 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0644 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0640 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0646 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0643 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0650 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0647 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0653 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0660 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0655 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0650 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0657 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0662 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0663 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0665 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0669 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0671 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0668 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0678 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0674 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0673 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0677 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0677 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0679 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0680 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0693 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0699 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0691 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0694 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0699 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0700 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0690 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:39.940524
# F-Score(Ordinary) = 0.623, Recall: 0.613, Precision: 0.633
# F-Score(lvc) = 0.458, Recall: 0.519, Precision: 0.409
# F-Score(ireflv) = 0.622, Recall: 0.482, Precision: 0.877
# F-Score(id) = 0.736, Recall: 0.89, Precision: 0.627
********************
********************
# XP = FR: Lemma(70) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 95
# Features = False
Deep model(Non compositional)
# Parameters = 1037433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 70)        1028510     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 25)        5875        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 280)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 100)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 380)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 8)            3048        concatenate_7[0][0]              
==================================================================================================
Total params: 1,037,433
Trainable params: 1,037,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0729 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0644 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0642 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0632 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0630 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0630 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0630 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0632 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0636 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0631 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0637 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0633 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0637 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0640 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0643 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0654 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0657 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0661 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0662 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0661 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0662 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0665 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0666 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0664 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0667 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0670 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0674 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0678 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0678 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0683 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0682 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0686 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0694 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0683 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0695 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0689 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0687 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0705 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0697 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0698 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:05.861251
# F-Score(Ordinary) = 0.587, Recall: 0.563, Precision: 0.613
# F-Score(lvc) = 0.472, Recall: 0.545, Precision: 0.417
# F-Score(ireflv) = 0.622, Recall: 0.508, Precision: 0.803
# F-Score(id) = 0.622, Recall: 0.622, Precision: 0.622
********************
********************
# XP = FR: Lemma(70) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 95
# Features = False
Deep model(Non compositional)
# Parameters = 1037433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 70)        1028510     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 25)        5875        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 280)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 100)          0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 380)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            3048        concatenate_8[0][0]              
==================================================================================================
Total params: 1,037,433
Trainable params: 1,037,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0732 - acc: 0.9855 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0635 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0630 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0627 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0629 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0629 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0634 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0633 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0634 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0640 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0638 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0641 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0644 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0641 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0650 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0645 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0654 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0653 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0658 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0664 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0666 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0660 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0667 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0669 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0668 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0671 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0668 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0675 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0677 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0679 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0675 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0679 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0678 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0696 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0693 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0695 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0694 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0696 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0704 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:31.171003
# F-Score(Ordinary) = 0.619, Recall: 0.668, Precision: 0.577
# F-Score(lvc) = 0.482, Recall: 0.746, Precision: 0.356
# F-Score(ireflv) = 0.59, Recall: 0.469, Precision: 0.795
# F-Score(id) = 0.718, Recall: 0.957, Precision: 0.575
********************
********************
# XP = FR: Lemma(70) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 95
# Features = False
Deep model(Non compositional)
# Parameters = 1037433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 70)        1028510     input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 25)        5875        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 280)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 100)          0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 380)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 8)            3048        concatenate_9[0][0]              
==================================================================================================
Total params: 1,037,433
Trainable params: 1,037,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0726 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0638 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0629 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0623 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0627 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0630 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0632 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0631 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0633 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0637 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0631 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0636 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0647 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0648 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0644 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0650 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0650 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0659 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0655 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0656 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0664 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0662 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0665 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0673 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0673 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0667 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0671 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0675 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0668 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0672 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0676 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0674 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0675 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0674 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0684 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0669 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0684 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0690 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0690 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0688 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:28.224863
# F-Score(Ordinary) = 0.607, Recall: 0.651, Precision: 0.568
# F-Score(lvc) = 0.364, Recall: 0.494, Precision: 0.288
# F-Score(ireflv) = 0.647, Recall: 0.545, Precision: 0.795
# F-Score(id) = 0.72, Recall: 0.874, Precision: 0.611
********************
********************
# XP = FR: Lemma(70) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 95
# Features = False
Deep model(Non compositional)
# Parameters = 1037433
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 70)        1028510     input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 25)        5875        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 280)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 100)          0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 380)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            3048        concatenate_10[0][0]             
==================================================================================================
Total params: 1,037,433
Trainable params: 1,037,433
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0726 - acc: 0.9855 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0630 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0624 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0620 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0623 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0621 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0626 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0625 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0629 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0628 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0632 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0632 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0637 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0637 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0639 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0642 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0645 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0654 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0647 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0657 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0657 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0656 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0664 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0666 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0666 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0670 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0668 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0683 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0679 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0688 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0692 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0681 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0693 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0691 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0687 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0689 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0694 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0699 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0701 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:34.780213
# F-Score(Ordinary) = 0.613, Recall: 0.675, Precision: 0.562
# F-Score(lvc) = 0.525, Recall: 0.757, Precision: 0.402
# F-Score(ireflv) = 0.611, Recall: 0.5, Precision: 0.787
# F-Score(id) = 0.673, Recall: 0.927, Precision: 0.528
********************
********************
# XP = FR: Lemma(70) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 105
# Features = False
Deep model(Non compositional)
# Parameters = 1040103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 70)        1028510     input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 35)        8225        input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 280)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 140)          0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 420)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 8)            3368        concatenate_11[0][0]             
==================================================================================================
Total params: 1,040,103
Trainable params: 1,040,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0736 - acc: 0.9854 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0642 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0636 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0632 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0638 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0639 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0635 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0645 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0652 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0643 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0647 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0648 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0651 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0652 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0648 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0652 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0653 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0656 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0654 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0654 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0659 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0663 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0665 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0668 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0672 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0668 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0677 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0685 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0683 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0686 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0684 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0685 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0696 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0699 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0692 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0698 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0705 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0709 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0703 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:35.639195
# F-Score(Ordinary) = 0.638, Recall: 0.636, Precision: 0.64
# F-Score(lvc) = 0.515, Recall: 0.594, Precision: 0.455
# F-Score(ireflv) = 0.618, Recall: 0.478, Precision: 0.877
# F-Score(id) = 0.744, Recall: 0.952, Precision: 0.611
********************
********************
# XP = FR: Lemma(70) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 105
# Features = False
Deep model(Non compositional)
# Parameters = 1040103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 70)        1028510     input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 35)        8225        input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 280)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 140)          0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 420)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            3368        concatenate_12[0][0]             
==================================================================================================
Total params: 1,040,103
Trainable params: 1,040,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0741 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0654 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0651 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0637 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0641 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0636 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0629 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0641 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0635 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0641 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0641 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0642 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0651 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0645 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0646 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0651 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0656 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0654 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0661 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0663 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0663 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0668 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0672 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0669 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0667 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0671 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0676 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0672 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0682 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0683 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0681 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0684 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0690 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0692 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0694 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0698 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0697 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0704 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0699 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0701 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:55.270336
# F-Score(Ordinary) = 0.585, Recall: 0.576, Precision: 0.595
# F-Score(lvc) = 0.456, Recall: 0.505, Precision: 0.417
# F-Score(ireflv) = 0.634, Recall: 0.587, Precision: 0.689
# F-Score(id) = 0.627, Recall: 0.603, Precision: 0.653
********************
********************
# XP = FR: Lemma(70) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 105
# Features = False
Deep model(Non compositional)
# Parameters = 1040103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 70)        1028510     input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 35)        8225        input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 280)          0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 140)          0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 420)          0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 8)            3368        concatenate_13[0][0]             
==================================================================================================
Total params: 1,040,103
Trainable params: 1,040,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0737 - acc: 0.9854 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0637 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0632 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0637 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0640 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0634 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0638 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0640 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0641 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0650 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0646 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0644 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0646 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0653 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0658 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0652 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0657 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0655 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0664 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0666 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0668 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0676 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0676 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0677 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0677 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0678 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0677 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0695 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0685 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0702 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0693 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0698 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0693 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0701 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0706 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0716 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0719 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0713 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0710 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0710 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:35.003446
# F-Score(Ordinary) = 0.572, Recall: 0.556, Precision: 0.588
# F-Score(lvc) = 0.396, Recall: 0.474, Precision: 0.341
# F-Score(ireflv) = 0.576, Recall: 0.427, Precision: 0.885
# F-Score(id) = 0.686, Recall: 0.872, Precision: 0.565
********************
********************
# XP = FR: Lemma(70) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 105
# Features = False
Deep model(Non compositional)
# Parameters = 1040103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 70)        1028510     input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 35)        8225        input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 280)          0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 140)          0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 420)          0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            3368        concatenate_14[0][0]             
==================================================================================================
Total params: 1,040,103
Trainable params: 1,040,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0734 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0640 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0632 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0627 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0628 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0628 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0637 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0637 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0635 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0637 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0641 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0634 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0641 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0648 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0649 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0649 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0650 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0647 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0662 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0661 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0659 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0659 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0660 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0665 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0666 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0661 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0669 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0675 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0685 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0689 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0691 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0687 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0689 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0696 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0699 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0704 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0698 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0701 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:35.071112
# F-Score(Ordinary) = 0.588, Recall: 0.579, Precision: 0.597
# F-Score(lvc) = 0.43, Recall: 0.473, Precision: 0.394
# F-Score(ireflv) = 0.644, Recall: 0.549, Precision: 0.779
# F-Score(id) = 0.649, Recall: 0.678, Precision: 0.622
********************
********************
# XP = FR: Lemma(70) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 105
# Features = False
Deep model(Non compositional)
# Parameters = 1040103
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 70)        1028510     input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 35)        8225        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 280)          0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 140)          0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 420)          0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 8)            3368        concatenate_15[0][0]             
==================================================================================================
Total params: 1,040,103
Trainable params: 1,040,103
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0735 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0641 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0641 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0633 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0631 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0626 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0628 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0632 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0632 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0636 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0633 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0637 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0641 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0641 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0643 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0651 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0650 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0655 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0656 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0664 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0659 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0666 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0666 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0665 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0666 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0668 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0669 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0673 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0675 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0675 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0688 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0681 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0687 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0695 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0689 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0696 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0695 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0707 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:34.832294
# F-Score(Ordinary) = 0.65, Recall: 0.67, Precision: 0.631
# F-Score(lvc) = 0.56, Recall: 0.773, Precision: 0.439
# F-Score(ireflv) = 0.64, Recall: 0.493, Precision: 0.91
# F-Score(id) = 0.713, Recall: 0.926, Precision: 0.58
********************
********************
# XP = FR: Lemma(70) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 120
# Features = False
Deep model(Non compositional)
# Parameters = 1044108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 70)        1028510     input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 50)        11750       input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 280)          0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 200)          0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 480)          0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            3848        concatenate_16[0][0]             
==================================================================================================
Total params: 1,044,108
Trainable params: 1,044,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0743 - acc: 0.9854 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0649 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0650 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0645 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0650 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0643 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0646 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0646 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0645 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0648 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0646 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0646 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0660 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0656 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0659 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0662 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0667 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0660 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0665 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0678 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0677 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0681 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0681 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0691 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0689 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0686 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0697 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0696 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0699 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0691 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0705 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0700 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0703 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0706 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0711 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0715 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0720 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0722 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0723 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:57.905695
# F-Score(Ordinary) = 0.593, Recall: 0.552, Precision: 0.64
# F-Score(lvc) = 0.481, Recall: 0.492, Precision: 0.47
# F-Score(ireflv) = 0.559, Recall: 0.424, Precision: 0.82
# F-Score(id) = 0.701, Recall: 0.787, Precision: 0.632
********************
********************
# XP = FR: Lemma(70) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 120
# Features = False
Deep model(Non compositional)
# Parameters = 1044108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 70)        1028510     input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 50)        11750       input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 280)          0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 200)          0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 480)          0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 8)            3848        concatenate_17[0][0]             
==================================================================================================
Total params: 1,044,108
Trainable params: 1,044,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0751 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0658 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0651 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0642 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0642 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0637 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0639 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0646 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0646 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0645 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0645 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0650 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0655 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0650 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0656 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0659 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0660 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0661 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0669 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0675 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0672 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0679 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0675 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0683 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0678 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0683 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0679 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0679 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0681 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0686 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0688 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0680 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0685 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0690 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0690 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0691 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0691 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0703 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0697 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0710 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:17.384975
# F-Score(Ordinary) = 0.584, Recall: 0.55, Precision: 0.622
# F-Score(lvc) = 0.473, Recall: 0.533, Precision: 0.424
# F-Score(ireflv) = 0.615, Recall: 0.484, Precision: 0.844
# F-Score(id) = 0.621, Recall: 0.631, Precision: 0.611
********************
********************
# XP = FR: Lemma(70) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 120
# Features = False
Deep model(Non compositional)
# Parameters = 1044108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 70)        1028510     input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 50)        11750       input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 280)          0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 200)          0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 480)          0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            3848        concatenate_18[0][0]             
==================================================================================================
Total params: 1,044,108
Trainable params: 1,044,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0741 - acc: 0.9854 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0638 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0634 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0634 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0641 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0641 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0641 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0646 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0648 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0640 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0648 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0657 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0651 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0654 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0657 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0659 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0662 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0665 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0663 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0672 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0667 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0672 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0673 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0682 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0682 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0691 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0684 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0690 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0684 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0688 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0691 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0691 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0696 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0702 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0705 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0706 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0714 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0717 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0705 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0713 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:26.278735
# F-Score(Ordinary) = 0.637, Recall: 0.699, Precision: 0.586
# F-Score(lvc) = 0.441, Recall: 0.58, Precision: 0.356
# F-Score(ireflv) = 0.66, Recall: 0.564, Precision: 0.795
# F-Score(id) = 0.745, Recall: 0.967, Precision: 0.606
********************
********************
# XP = FR: Lemma(70) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 120
# Features = False
Deep model(Non compositional)
# Parameters = 1044108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 70)        1028510     input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 50)        11750       input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 280)          0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 200)          0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 480)          0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 8)            3848        concatenate_19[0][0]             
==================================================================================================
Total params: 1,044,108
Trainable params: 1,044,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.0740 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0651 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0648 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0643 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0642 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0644 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0648 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0644 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0641 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0641 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0645 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0639 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0645 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0658 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0653 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0659 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0655 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0660 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0668 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0665 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0684 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0672 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0671 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0676 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0672 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0672 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0685 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0693 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0686 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0687 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0689 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0688 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0701 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0713 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0696 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0700 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0710 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0709 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0707 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0706 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:53.670513
# F-Score(Ordinary) = 0.576, Recall: 0.601, Precision: 0.553
# F-Score(lvc) = 0.364, Recall: 0.455, Precision: 0.303
# F-Score(ireflv) = 0.676, Recall: 0.603, Precision: 0.77
# F-Score(id) = 0.63, Recall: 0.681, Precision: 0.585
********************
********************
# XP = FR: Lemma(70) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 120
# Features = False
Deep model(Non compositional)
# Parameters = 1044108
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_39 (Embedding)        (None, 4, 70)        1028510     input_39[0][0]                   
__________________________________________________________________________________________________
embedding_40 (Embedding)        (None, 4, 50)        11750       input_40[0][0]                   
__________________________________________________________________________________________________
flatten_39 (Flatten)            (None, 280)          0           embedding_39[0][0]               
__________________________________________________________________________________________________
flatten_40 (Flatten)            (None, 200)          0           embedding_40[0][0]               
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 480)          0           flatten_39[0][0]                 
                                                                 flatten_40[0][0]                 
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            3848        concatenate_20[0][0]             
==================================================================================================
Total params: 1,044,108
Trainable params: 1,044,108
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0745 - acc: 0.9853 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0652 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0647 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0638 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0639 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0645 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0634 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0638 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0639 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0645 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0640 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0644 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0652 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0653 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0654 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0668 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0661 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0665 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0666 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0663 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0670 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0675 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0674 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0676 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0685 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0683 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0685 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0687 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0693 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0690 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0694 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0702 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0691 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0700 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0696 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0712 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0699 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0705 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0714 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0714 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:17:39.537897
# F-Score(Ordinary) = 0.593, Recall: 0.651, Precision: 0.544
# F-Score(lvc) = 0.531, Recall: 0.733, Precision: 0.417
# F-Score(ireflv) = 0.609, Recall: 0.488, Precision: 0.811
# F-Score(id) = 0.615, Recall: 0.946, Precision: 0.456
********************
********************
# XP = FR: Lemma(100) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 115
# Features = False
Deep model(Non compositional)
# Parameters = 1476513
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_41 (Embedding)        (None, 4, 100)       1469300     input_41[0][0]                   
__________________________________________________________________________________________________
embedding_42 (Embedding)        (None, 4, 15)        3525        input_42[0][0]                   
__________________________________________________________________________________________________
flatten_41 (Flatten)            (None, 400)          0           embedding_41[0][0]               
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 60)           0           embedding_42[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 460)          0           flatten_41[0][0]                 
                                                                 flatten_42[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 8)            3688        concatenate_21[0][0]             
==================================================================================================
Total params: 1,476,513
Trainable params: 1,476,513
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0760 - acc: 0.9848 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0685 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0687 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0683 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0690 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0690 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0694 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0700 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0704 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0705 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0708 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0718 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0726 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0717 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0726 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0725 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0745 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0741 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0744 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0745 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0749 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0748 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0744 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0756 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0755 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0758 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0765 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0761 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0776 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0772 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0769 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0788 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0776 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0778 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0791 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0781 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0783 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0789 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0788 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0788 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:52.354569
# F-Score(Ordinary) = 0.572, Recall: 0.518, Precision: 0.638
# F-Score(lvc) = 0.442, Recall: 0.47, Precision: 0.417
# F-Score(ireflv) = 0.496, Recall: 0.35, Precision: 0.852
# F-Score(id) = 0.76, Recall: 0.919, Precision: 0.648
********************
********************
# XP = FR: Lemma(100) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 115
# Features = False
Deep model(Non compositional)
# Parameters = 1476513
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 4, 100)       1469300     input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 4, 15)        3525        input_44[0][0]                   
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 400)          0           embedding_43[0][0]               
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 60)           0           embedding_44[0][0]               
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 460)          0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            3688        concatenate_22[0][0]             
==================================================================================================
Total params: 1,476,513
Trainable params: 1,476,513
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0762 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0690 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0695 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0695 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0699 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0704 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0705 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0708 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0714 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0728 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0722 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0729 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0732 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0728 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0735 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0738 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0741 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0740 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0749 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0750 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0763 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0749 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0755 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0751 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0766 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0783 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0773 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0772 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0785 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0778 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0779 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0780 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0776 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0786 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0793 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0787 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0785 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0805 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0797 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0799 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:50.910987
# F-Score(Ordinary) = 0.589, Recall: 0.577, Precision: 0.602
# F-Score(lvc) = 0.478, Recall: 0.574, Precision: 0.409
# F-Score(ireflv) = 0.504, Recall: 0.383, Precision: 0.738
# F-Score(id) = 0.76, Recall: 0.919, Precision: 0.648
********************
********************
# XP = FR: Lemma(100) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 115
# Features = False
Deep model(Non compositional)
# Parameters = 1476513
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_45 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 4, 100)       1469300     input_45[0][0]                   
__________________________________________________________________________________________________
embedding_46 (Embedding)        (None, 4, 15)        3525        input_46[0][0]                   
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 400)          0           embedding_45[0][0]               
__________________________________________________________________________________________________
flatten_46 (Flatten)            (None, 60)           0           embedding_46[0][0]               
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 460)          0           flatten_45[0][0]                 
                                                                 flatten_46[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 8)            3688        concatenate_23[0][0]             
==================================================================================================
Total params: 1,476,513
Trainable params: 1,476,513
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0763 - acc: 0.9848 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0692 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0703 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0706 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0708 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0707 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0715 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0715 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0716 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0718 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0729 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0723 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0723 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0732 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0739 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0737 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0733 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0744 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0748 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0753 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0760 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0751 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0753 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0758 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0766 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0759 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0757 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0775 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0767 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0779 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0783 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0780 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0778 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0787 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0777 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0781 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0790 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0793 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0794 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0797 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:15.481016
# F-Score(Ordinary) = 0.577, Recall: 0.593, Precision: 0.562
# F-Score(lvc) = 0.452, Recall: 0.562, Precision: 0.379
# F-Score(ireflv) = 0.522, Recall: 0.409, Precision: 0.721
# F-Score(id) = 0.718, Recall: 0.941, Precision: 0.58
********************
********************
# XP = FR: Lemma(100) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 115
# Features = False
Deep model(Non compositional)
# Parameters = 1476513
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_47 (Embedding)        (None, 4, 100)       1469300     input_47[0][0]                   
__________________________________________________________________________________________________
embedding_48 (Embedding)        (None, 4, 15)        3525        input_48[0][0]                   
__________________________________________________________________________________________________
flatten_47 (Flatten)            (None, 400)          0           embedding_47[0][0]               
__________________________________________________________________________________________________
flatten_48 (Flatten)            (None, 60)           0           embedding_48[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 460)          0           flatten_47[0][0]                 
                                                                 flatten_48[0][0]                 
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            3688        concatenate_24[0][0]             
==================================================================================================
Total params: 1,476,513
Trainable params: 1,476,513
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0758 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0693 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0692 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0697 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0706 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0705 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0718 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0723 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0720 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0727 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0725 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0726 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0731 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0734 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0740 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0738 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0747 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0752 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0762 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0744 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0757 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0766 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0766 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0758 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0766 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0767 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0773 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0778 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0768 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0775 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0782 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0775 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0784 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0795 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0790 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0788 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0791 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0791 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0792 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0801 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:31.172206
# F-Score(Ordinary) = 0.575, Recall: 0.611, Precision: 0.544
# F-Score(lvc) = 0.437, Recall: 0.608, Precision: 0.341
# F-Score(ireflv) = 0.657, Recall: 0.573, Precision: 0.77
# F-Score(id) = 0.584, Recall: 0.644, Precision: 0.534
********************
********************
# XP = FR: Lemma(100) POS(15) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 115
# Features = False
Deep model(Non compositional)
# Parameters = 1476513
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_49 (Embedding)        (None, 4, 100)       1469300     input_49[0][0]                   
__________________________________________________________________________________________________
embedding_50 (Embedding)        (None, 4, 15)        3525        input_50[0][0]                   
__________________________________________________________________________________________________
flatten_49 (Flatten)            (None, 400)          0           embedding_49[0][0]               
__________________________________________________________________________________________________
flatten_50 (Flatten)            (None, 60)           0           embedding_50[0][0]               
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 460)          0           flatten_49[0][0]                 
                                                                 flatten_50[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 8)            3688        concatenate_25[0][0]             
==================================================================================================
Total params: 1,476,513
Trainable params: 1,476,513
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0760 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0687 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0691 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0695 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0702 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0701 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0708 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0705 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0710 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0724 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0727 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0721 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0736 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0736 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0736 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0746 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0750 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0752 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0742 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0750 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0755 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0755 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0769 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0774 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0768 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0782 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0773 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0781 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0784 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0795 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0795 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0797 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 26s - loss: 0.0792 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.0815 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.0814 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.0808 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.0803 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.0823 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0824 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0830 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:13.214494
# F-Score(Ordinary) = 0.528, Recall: 0.447, Precision: 0.644
# F-Score(lvc) = 0.455, Recall: 0.422, Precision: 0.492
# F-Score(ireflv) = 0.61, Recall: 0.459, Precision: 0.91
# F-Score(id) = 0.482, Recall: 0.429, Precision: 0.549
********************
********************
# XP = FR: Lemma(100) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 125
# Features = False
Deep model(Non compositional)
# Parameters = 1479183
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_51 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_51 (Embedding)        (None, 4, 100)       1469300     input_51[0][0]                   
__________________________________________________________________________________________________
embedding_52 (Embedding)        (None, 4, 25)        5875        input_52[0][0]                   
__________________________________________________________________________________________________
flatten_51 (Flatten)            (None, 400)          0           embedding_51[0][0]               
__________________________________________________________________________________________________
flatten_52 (Flatten)            (None, 100)          0           embedding_52[0][0]               
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 500)          0           flatten_51[0][0]                 
                                                                 flatten_52[0][0]                 
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            4008        concatenate_26[0][0]             
==================================================================================================
Total params: 1,479,183
Trainable params: 1,479,183
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 30s - loss: 0.0775 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0701 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.0702 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.0706 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.0716 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0707 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0718 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0717 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0719 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0726 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0721 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0724 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0739 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0733 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0736 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0745 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0753 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0750 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0751 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0753 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0748 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0760 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0758 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0769 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0766 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0762 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0768 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0778 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0786 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0783 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0785 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0777 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0784 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0787 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0791 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0788 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0789 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0806 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0797 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0803 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:11.494565
# F-Score(Ordinary) = 0.612, Recall: 0.579, Precision: 0.649
# F-Score(lvc) = 0.485, Recall: 0.492, Precision: 0.477
# F-Score(ireflv) = 0.584, Recall: 0.44, Precision: 0.869
# F-Score(id) = 0.738, Recall: 0.909, Precision: 0.622
********************
********************
# XP = FR: Lemma(100) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 125
# Features = False
Deep model(Non compositional)
# Parameters = 1479183
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_53 (Embedding)        (None, 4, 100)       1469300     input_53[0][0]                   
__________________________________________________________________________________________________
embedding_54 (Embedding)        (None, 4, 25)        5875        input_54[0][0]                   
__________________________________________________________________________________________________
flatten_53 (Flatten)            (None, 400)          0           embedding_53[0][0]               
__________________________________________________________________________________________________
flatten_54 (Flatten)            (None, 100)          0           embedding_54[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 500)          0           flatten_53[0][0]                 
                                                                 flatten_54[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 8)            4008        concatenate_27[0][0]             
==================================================================================================
Total params: 1,479,183
Trainable params: 1,479,183
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0773 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0701 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0708 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0706 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0710 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0709 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0707 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0726 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0724 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0724 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0732 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0734 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0736 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0738 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0739 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0747 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0749 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0746 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0749 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0756 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0767 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0759 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0773 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0757 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0769 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0775 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0770 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0773 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0782 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0774 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0781 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0782 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0791 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0783 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0784 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0783 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0788 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0803 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0805 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0801 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:37.701644
# F-Score(Ordinary) = 0.542, Recall: 0.468, Precision: 0.644
# F-Score(lvc) = 0.404, Recall: 0.38, Precision: 0.432
# F-Score(ireflv) = 0.509, Recall: 0.366, Precision: 0.836
# F-Score(id) = 0.683, Recall: 0.703, Precision: 0.663
********************
********************
# XP = FR: Lemma(100) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 125
# Features = False
Deep model(Non compositional)
# Parameters = 1479183
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_55 (Embedding)        (None, 4, 100)       1469300     input_55[0][0]                   
__________________________________________________________________________________________________
embedding_56 (Embedding)        (None, 4, 25)        5875        input_56[0][0]                   
__________________________________________________________________________________________________
flatten_55 (Flatten)            (None, 400)          0           embedding_55[0][0]               
__________________________________________________________________________________________________
flatten_56 (Flatten)            (None, 100)          0           embedding_56[0][0]               
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 500)          0           flatten_55[0][0]                 
                                                                 flatten_56[0][0]                 
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            4008        concatenate_28[0][0]             
==================================================================================================
Total params: 1,479,183
Trainable params: 1,479,183
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0774 - acc: 0.9848 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0701 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0710 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0714 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0721 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0728 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0726 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0730 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0735 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0733 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0726 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0738 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0742 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0738 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0746 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0743 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.0746 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.0745 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.0755 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.0755 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 26s - loss: 0.0758 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 26s - loss: 0.0765 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.0767 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.0764 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.0769 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.0766 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.0766 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.0780 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.0785 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.0782 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 26s - loss: 0.0780 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 26s - loss: 0.0776 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 26s - loss: 0.0787 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.0801 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.0801 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.0797 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.0806 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.0804 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.0800 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.0804 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:45.200464
# F-Score(Ordinary) = 0.601, Recall: 0.654, Precision: 0.557
# F-Score(lvc) = 0.38, Recall: 0.673, Precision: 0.265
# F-Score(ireflv) = 0.632, Recall: 0.491, Precision: 0.885
# F-Score(id) = 0.695, Recall: 0.963, Precision: 0.544
********************
********************
# XP = FR: Lemma(100) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 125
# Features = False
Deep model(Non compositional)
# Parameters = 1479183
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_57 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_58 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_57 (Embedding)        (None, 4, 100)       1469300     input_57[0][0]                   
__________________________________________________________________________________________________
embedding_58 (Embedding)        (None, 4, 25)        5875        input_58[0][0]                   
__________________________________________________________________________________________________
flatten_57 (Flatten)            (None, 400)          0           embedding_57[0][0]               
__________________________________________________________________________________________________
flatten_58 (Flatten)            (None, 100)          0           embedding_58[0][0]               
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 500)          0           flatten_57[0][0]                 
                                                                 flatten_58[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 8)            4008        concatenate_29[0][0]             
==================================================================================================
Total params: 1,479,183
Trainable params: 1,479,183
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0772 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0700 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0697 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0695 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0700 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0703 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0706 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0719 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0723 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0724 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0722 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0723 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0745 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0741 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0742 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0750 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0755 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0753 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0762 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0758 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0766 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0762 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0769 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0757 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0779 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0779 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0791 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0783 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0778 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0785 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0789 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0788 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0809 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0798 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0792 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0787 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0794 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0796 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0804 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0802 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:05.615198
# F-Score(Ordinary) = 0.619, Recall: 0.65, Precision: 0.591
# F-Score(lvc) = 0.505, Recall: 0.758, Precision: 0.379
# F-Score(ireflv) = 0.667, Recall: 0.562, Precision: 0.82
# F-Score(id) = 0.631, Recall: 0.691, Precision: 0.58
********************
********************
# XP = FR: Lemma(100) POS(25) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 125
# Features = False
Deep model(Non compositional)
# Parameters = 1479183
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_59 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_60 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_59 (Embedding)        (None, 4, 100)       1469300     input_59[0][0]                   
__________________________________________________________________________________________________
embedding_60 (Embedding)        (None, 4, 25)        5875        input_60[0][0]                   
__________________________________________________________________________________________________
flatten_59 (Flatten)            (None, 400)          0           embedding_59[0][0]               
__________________________________________________________________________________________________
flatten_60 (Flatten)            (None, 100)          0           embedding_60[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 500)          0           flatten_59[0][0]                 
                                                                 flatten_60[0][0]                 
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            4008        concatenate_30[0][0]             
==================================================================================================
Total params: 1,479,183
Trainable params: 1,479,183
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0771 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0702 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0704 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0706 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0712 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0714 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0708 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0712 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0721 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0728 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0724 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0724 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0734 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0726 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0735 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0739 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0740 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0744 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0745 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0749 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0757 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0761 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0771 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0766 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0762 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0768 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0768 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0773 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0786 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0786 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0785 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0786 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0779 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0790 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0805 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0802 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0795 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0804 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0804 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0815 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:10.761084
# F-Score(Ordinary) = 0.587, Recall: 0.608, Precision: 0.568
# F-Score(lvc) = 0.487, Recall: 0.559, Precision: 0.432
# F-Score(ireflv) = 0.634, Recall: 0.489, Precision: 0.902
# F-Score(id) = 0.606, Recall: 0.945, Precision: 0.446
********************
********************
# XP = FR: Lemma(100) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 135
# Features = False
Deep model(Non compositional)
# Parameters = 1481853
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_61 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_62 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_61 (Embedding)        (None, 4, 100)       1469300     input_61[0][0]                   
__________________________________________________________________________________________________
embedding_62 (Embedding)        (None, 4, 35)        8225        input_62[0][0]                   
__________________________________________________________________________________________________
flatten_61 (Flatten)            (None, 400)          0           embedding_61[0][0]               
__________________________________________________________________________________________________
flatten_62 (Flatten)            (None, 140)          0           embedding_62[0][0]               
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 540)          0           flatten_61[0][0]                 
                                                                 flatten_62[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 8)            4328        concatenate_31[0][0]             
==================================================================================================
Total params: 1,481,853
Trainable params: 1,481,853
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0787 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0710 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0715 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0716 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0725 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0730 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0734 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0738 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0734 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0750 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0753 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0750 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0762 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0759 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0768 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0747 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0765 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0765 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0761 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0768 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0766 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0772 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0771 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0776 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0791 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0791 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0784 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0792 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0794 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0804 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0800 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0791 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0801 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0802 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0802 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0800 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0815 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0823 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0818 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0818 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:59.219211
# F-Score(Ordinary) = 0.558, Recall: 0.494, Precision: 0.642
# F-Score(lvc) = 0.4, Recall: 0.353, Precision: 0.462
# F-Score(ireflv) = 0.529, Recall: 0.38, Precision: 0.869
# F-Score(id) = 0.733, Recall: 0.915, Precision: 0.611
********************
********************
# XP = FR: Lemma(100) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 135
# Features = False
Deep model(Non compositional)
# Parameters = 1481853
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_63 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_64 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_63 (Embedding)        (None, 4, 100)       1469300     input_63[0][0]                   
__________________________________________________________________________________________________
embedding_64 (Embedding)        (None, 4, 35)        8225        input_64[0][0]                   
__________________________________________________________________________________________________
flatten_63 (Flatten)            (None, 400)          0           embedding_63[0][0]               
__________________________________________________________________________________________________
flatten_64 (Flatten)            (None, 140)          0           embedding_64[0][0]               
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 540)          0           flatten_63[0][0]                 
                                                                 flatten_64[0][0]                 
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            4328        concatenate_32[0][0]             
==================================================================================================
Total params: 1,481,853
Trainable params: 1,481,853
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0779 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0699 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0702 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0705 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0708 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0712 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0714 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0720 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0719 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0725 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0724 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0729 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0733 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0745 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0740 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0752 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0753 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0752 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0763 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0763 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0772 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0769 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0781 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0768 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0778 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0791 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0785 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0788 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0787 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0791 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0795 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0790 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0806 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0815 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0815 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0815 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0808 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0817 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0814 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0819 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:58.201835
# F-Score(Ordinary) = 0.567, Recall: 0.596, Precision: 0.541
# F-Score(lvc) = 0.441, Recall: 0.544, Precision: 0.371
# F-Score(ireflv) = 0.471, Recall: 0.388, Precision: 0.598
# F-Score(id) = 0.741, Recall: 0.93, Precision: 0.617
********************
********************
# XP = FR: Lemma(100) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 135
# Features = False
Deep model(Non compositional)
# Parameters = 1481853
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_65 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_66 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_65 (Embedding)        (None, 4, 100)       1469300     input_65[0][0]                   
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 4, 35)        8225        input_66[0][0]                   
__________________________________________________________________________________________________
flatten_65 (Flatten)            (None, 400)          0           embedding_65[0][0]               
__________________________________________________________________________________________________
flatten_66 (Flatten)            (None, 140)          0           embedding_66[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 540)          0           flatten_65[0][0]                 
                                                                 flatten_66[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 8)            4328        concatenate_33[0][0]             
==================================================================================================
Total params: 1,481,853
Trainable params: 1,481,853
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0785 - acc: 0.9847 - val_loss: 8.9407e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0703 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0702 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0704 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0714 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0712 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0715 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0722 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0730 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0734 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0736 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0737 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0740 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0735 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0738 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0748 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0758 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0756 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0749 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0762 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0771 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0767 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0767 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0769 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0770 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0765 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0775 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0781 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0778 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0789 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0799 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0786 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0788 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0786 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0799 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0793 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0801 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0807 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0804 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0813 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:58.067697
# F-Score(Ordinary) = 0.558, Recall: 0.524, Precision: 0.597
# F-Score(lvc) = 0.391, Recall: 0.459, Precision: 0.341
# F-Score(ireflv) = 0.523, Recall: 0.373, Precision: 0.877
# F-Score(id) = 0.711, Recall: 0.904, Precision: 0.585
********************
********************
# XP = FR: Lemma(100) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 135
# Features = False
Deep model(Non compositional)
# Parameters = 1481853
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_67 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_68 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 4, 100)       1469300     input_67[0][0]                   
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 4, 35)        8225        input_68[0][0]                   
__________________________________________________________________________________________________
flatten_67 (Flatten)            (None, 400)          0           embedding_67[0][0]               
__________________________________________________________________________________________________
flatten_68 (Flatten)            (None, 140)          0           embedding_68[0][0]               
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 540)          0           flatten_67[0][0]                 
                                                                 flatten_68[0][0]                 
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            4328        concatenate_34[0][0]             
==================================================================================================
Total params: 1,481,853
Trainable params: 1,481,853
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0774 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0708 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0706 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0718 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0717 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0715 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0724 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0733 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0727 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0736 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0738 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0730 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0748 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0741 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0743 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0748 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0748 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0753 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0765 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0759 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0762 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0765 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0766 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0771 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0775 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0773 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0770 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0786 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0779 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0793 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0799 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0793 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0806 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0809 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0795 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0805 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0825 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0817 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0825 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0810 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:54.990282
# F-Score(Ordinary) = 0.572, Recall: 0.578, Precision: 0.566
# F-Score(lvc) = 0.338, Recall: 0.493, Precision: 0.258
# F-Score(ireflv) = 0.641, Recall: 0.529, Precision: 0.811
# F-Score(id) = 0.613, Recall: 0.632, Precision: 0.596
********************
********************
# XP = FR: Lemma(100) POS(35) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 135
# Features = False
Deep model(Non compositional)
# Parameters = 1481853
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_69 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_70 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_69 (Embedding)        (None, 4, 100)       1469300     input_69[0][0]                   
__________________________________________________________________________________________________
embedding_70 (Embedding)        (None, 4, 35)        8225        input_70[0][0]                   
__________________________________________________________________________________________________
flatten_69 (Flatten)            (None, 400)          0           embedding_69[0][0]               
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 140)          0           embedding_70[0][0]               
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 540)          0           flatten_69[0][0]                 
                                                                 flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 8)            4328        concatenate_35[0][0]             
==================================================================================================
Total params: 1,481,853
Trainable params: 1,481,853
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0777 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0707 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.0706 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.0715 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.0717 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 26s - loss: 0.0716 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0719 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0727 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0730 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0733 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0731 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0736 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0745 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0749 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0752 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0755 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0756 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0769 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0768 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0768 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0767 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0764 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0773 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0769 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0785 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0777 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0787 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0771 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0793 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0788 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0789 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0800 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0785 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0800 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0790 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0804 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0803 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0819 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0817 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0820 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:45.284654
# F-Score(Ordinary) = 0.638, Recall: 0.679, Precision: 0.602
# F-Score(lvc) = 0.554, Recall: 0.674, Precision: 0.47
# F-Score(ireflv) = 0.685, Recall: 0.553, Precision: 0.902
# F-Score(id) = 0.638, Recall: 0.905, Precision: 0.492
********************
********************
# XP = FR: Lemma(100) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1485858
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_71 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_72 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_71 (Embedding)        (None, 4, 100)       1469300     input_71[0][0]                   
__________________________________________________________________________________________________
embedding_72 (Embedding)        (None, 4, 50)        11750       input_72[0][0]                   
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 400)          0           embedding_71[0][0]               
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 200)          0           embedding_72[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 600)          0           flatten_71[0][0]                 
                                                                 flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            4808        concatenate_36[0][0]             
==================================================================================================
Total params: 1,485,858
Trainable params: 1,485,858
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0790 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0715 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.0721 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.0719 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.0721 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 26s - loss: 0.0731 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 26s - loss: 0.0727 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 26s - loss: 0.0732 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 26s - loss: 0.0739 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 26s - loss: 0.0734 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 26s - loss: 0.0742 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.0749 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.0753 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.0754 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0759 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0753 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0766 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0769 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0766 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0782 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0771 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0781 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0777 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0776 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0785 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0794 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0796 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0800 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0799 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0805 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0811 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0802 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0795 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0812 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0810 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0811 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0827 - acc: 0.9881 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0821 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0826 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0829 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:31.551759
# F-Score(Ordinary) = 0.572, Recall: 0.505, Precision: 0.66
# F-Score(lvc) = 0.492, Recall: 0.526, Precision: 0.462
# F-Score(ireflv) = 0.481, Recall: 0.33, Precision: 0.885
# F-Score(id) = 0.749, Recall: 0.887, Precision: 0.648
********************
********************
# XP = FR: Lemma(100) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1485858
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_73 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_74 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_73 (Embedding)        (None, 4, 100)       1469300     input_73[0][0]                   
__________________________________________________________________________________________________
embedding_74 (Embedding)        (None, 4, 50)        11750       input_74[0][0]                   
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 400)          0           embedding_73[0][0]               
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 200)          0           embedding_74[0][0]               
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 600)          0           flatten_73[0][0]                 
                                                                 flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 8)            4808        concatenate_37[0][0]             
==================================================================================================
Total params: 1,485,858
Trainable params: 1,485,858
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0784 - acc: 0.9845 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0708 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0704 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0708 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0715 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0722 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0725 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0728 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0735 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0737 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0738 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0743 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0748 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0749 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0754 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0762 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0756 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0759 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0777 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0775 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0790 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0781 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0797 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0793 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0793 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0795 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0796 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0794 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0813 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0819 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0816 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0814 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0812 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0816 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0823 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0826 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0826 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0836 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0834 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0831 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:16.157621
# F-Score(Ordinary) = 0.527, Recall: 0.473, Precision: 0.595
# F-Score(lvc) = 0.334, Recall: 0.286, Precision: 0.402
# F-Score(ireflv) = 0.633, Recall: 0.564, Precision: 0.721
# F-Score(id) = 0.585, Recall: 0.548, Precision: 0.627
********************
********************
# XP = FR: Lemma(100) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1485858
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_75 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_76 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_75 (Embedding)        (None, 4, 100)       1469300     input_75[0][0]                   
__________________________________________________________________________________________________
embedding_76 (Embedding)        (None, 4, 50)        11750       input_76[0][0]                   
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 400)          0           embedding_75[0][0]               
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 200)          0           embedding_76[0][0]               
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 600)          0           flatten_75[0][0]                 
                                                                 flatten_76[0][0]                 
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            4808        concatenate_38[0][0]             
==================================================================================================
Total params: 1,485,858
Trainable params: 1,485,858
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0794 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0710 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0708 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0717 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0722 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0725 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0732 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0733 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0737 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0744 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0747 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.0750 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.0756 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.0755 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 26s - loss: 0.0763 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 26s - loss: 0.0755 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.0766 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.0765 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.0769 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.0774 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0788 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0779 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0779 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0795 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0786 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0798 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0789 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0794 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0796 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0806 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0801 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0800 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0794 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0799 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0810 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0816 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0825 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0829 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0828 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0836 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:23.945976
# F-Score(Ordinary) = 0.607, Recall: 0.587, Precision: 0.629
# F-Score(lvc) = 0.449, Recall: 0.467, Precision: 0.432
# F-Score(ireflv) = 0.597, Recall: 0.455, Precision: 0.869
# F-Score(id) = 0.741, Recall: 0.951, Precision: 0.606
********************
********************
# XP = FR: Lemma(100) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1485858
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_77 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_78 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_77 (Embedding)        (None, 4, 100)       1469300     input_77[0][0]                   
__________________________________________________________________________________________________
embedding_78 (Embedding)        (None, 4, 50)        11750       input_78[0][0]                   
__________________________________________________________________________________________________
flatten_77 (Flatten)            (None, 400)          0           embedding_77[0][0]               
__________________________________________________________________________________________________
flatten_78 (Flatten)            (None, 200)          0           embedding_78[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 600)          0           flatten_77[0][0]                 
                                                                 flatten_78[0][0]                 
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 8)            4808        concatenate_39[0][0]             
==================================================================================================
Total params: 1,485,858
Trainable params: 1,485,858
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0788 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0722 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0723 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0727 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0736 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0739 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0735 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0744 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0739 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0750 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0749 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0752 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0755 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0767 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0768 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0762 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0772 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0780 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0791 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0779 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0782 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0779 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0777 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0788 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0795 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0799 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0795 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0803 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0801 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0807 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0798 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0809 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0815 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0820 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0834 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0815 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0826 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0819 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0824 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0836 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:43.862618
# F-Score(Ordinary) = 0.543, Recall: 0.494, Precision: 0.604
# F-Score(lvc) = 0.386, Recall: 0.458, Precision: 0.333
# F-Score(ireflv) = 0.644, Recall: 0.515, Precision: 0.861
# F-Score(id) = 0.532, Recall: 0.474, Precision: 0.606
********************
********************
# XP = FR: Lemma(100) POS(50) 
********************
********************
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
Training started# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = {'active': True, 's0Padding': 5, 'bPadding': 2, 's1Padding': 5}
# Embedding = True
# Initialisation = False
# Concatenation = True
# Emb = 150
# Features = False
Deep model(Non compositional)
# Parameters = 1485858
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_79 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_80 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_79 (Embedding)        (None, 4, 100)       1469300     input_79[0][0]                   
__________________________________________________________________________________________________
embedding_80 (Embedding)        (None, 4, 50)        11750       input_80[0][0]                   
__________________________________________________________________________________________________
flatten_79 (Flatten)            (None, 400)          0           embedding_79[0][0]               
__________________________________________________________________________________________________
flatten_80 (Flatten)            (None, 200)          0           embedding_80[0][0]               
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 600)          0           flatten_79[0][0]                 
                                                                 flatten_80[0][0]                 
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 8)            4808        concatenate_40[0][0]             
==================================================================================================
Total params: 1,485,858
Trainable params: 1,485,858
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0791 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0711 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0713 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0718 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0729 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0724 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0729 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0733 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0739 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0737 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0748 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0747 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0756 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0761 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0757 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0765 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0761 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0775 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0760 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0765 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0773 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0774 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0786 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0785 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0793 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0793 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0797 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0802 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0797 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0797 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0803 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0801 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0806 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0806 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0814 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0811 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0804 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0814 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0821 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0826 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:18:52.772861
# F-Score(Ordinary) = 0.62, Recall: 0.683, Precision: 0.568
# F-Score(lvc) = 0.528, Recall: 0.679, Precision: 0.432
# F-Score(ireflv) = 0.66, Recall: 0.542, Precision: 0.844
# F-Score(id) = 0.639, Recall: 0.949, Precision: 0.482
********************
