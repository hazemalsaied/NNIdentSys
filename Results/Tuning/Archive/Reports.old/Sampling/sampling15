INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
Debug enabled
********************
# XP = FR: referencial
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 48)        23568       input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 24)        936         input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 192)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 96)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 288)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 24)           6936        concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            200         dropout_1[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.3349883177570093, 1.0: 0.33961705487564153, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0}
Train on 9290 samples, validate on 1033 samples
Epoch 1/40
 - 0s - loss: 0.6565 - acc: 0.8825 - val_loss: 0.1723 - val_acc: 0.9574
Epoch 2/40
 - 0s - loss: 0.0851 - acc: 0.9807 - val_loss: 0.1427 - val_acc: 0.9584
Epoch 3/40
 - 0s - loss: 0.0624 - acc: 0.9845 - val_loss: 0.1375 - val_acc: 0.9622
Epoch 4/40
 - 0s - loss: 0.0478 - acc: 0.9864 - val_loss: 0.1372 - val_acc: 0.9642
Epoch 5/40
 - 0s - loss: 0.0400 - acc: 0.9883 - val_loss: 0.1307 - val_acc: 0.9642
Epoch 6/40
 - 0s - loss: 0.0337 - acc: 0.9890 - val_loss: 0.1318 - val_acc: 0.9652
Epoch 7/40
 - 0s - loss: 0.0316 - acc: 0.9904 - val_loss: 0.1278 - val_acc: 0.9632
Epoch 8/40
 - 0s - loss: 0.0284 - acc: 0.9902 - val_loss: 0.1285 - val_acc: 0.9652
Epoch 9/40
 - 0s - loss: 0.0272 - acc: 0.9903 - val_loss: 0.1318 - val_acc: 0.9661
Epoch 10/40
 - 0s - loss: 0.0225 - acc: 0.9918 - val_loss: 0.1346 - val_acc: 0.9632
Epoch 11/40
 - 0s - loss: 0.0231 - acc: 0.9916 - val_loss: 0.1390 - val_acc: 0.9671
Epoch 12/40
 - 0s - loss: 0.0222 - acc: 0.9904 - val_loss: 0.1405 - val_acc: 0.9681
Epoch 13/40
 - 0s - loss: 0.0220 - acc: 0.9916 - val_loss: 0.1356 - val_acc: 0.9681
Epoch 14/40
 - 0s - loss: 0.0178 - acc: 0.9916 - val_loss: 0.1510 - val_acc: 0.9681
Epoch 15/40
 - 0s - loss: 0.0180 - acc: 0.9913 - val_loss: 0.1426 - val_acc: 0.9690
Epoch 16/40
 - 0s - loss: 0.0180 - acc: 0.9921 - val_loss: 0.1564 - val_acc: 0.9681
Epoch 17/40
 - 0s - loss: 0.0168 - acc: 0.9925 - val_loss: 0.1584 - val_acc: 0.9710
Epoch 18/40
 - 0s - loss: 0.0166 - acc: 0.9926 - val_loss: 0.1591 - val_acc: 0.9681
Epoch 19/40
 - 0s - loss: 0.0154 - acc: 0.9930 - val_loss: 0.1574 - val_acc: 0.9671
Epoch 20/40
 - 0s - loss: 0.0134 - acc: 0.9929 - val_loss: 0.1627 - val_acc: 0.9690
Epoch 21/40
 - 0s - loss: 0.0140 - acc: 0.9934 - val_loss: 0.1559 - val_acc: 0.9661
Epoch 22/40
 - 0s - loss: 0.0145 - acc: 0.9933 - val_loss: 0.1643 - val_acc: 0.9681
Epoch 23/40
 - 0s - loss: 0.0127 - acc: 0.9938 - val_loss: 0.1725 - val_acc: 0.9700
Epoch 24/40
 - 0s - loss: 0.0144 - acc: 0.9930 - val_loss: 0.1697 - val_acc: 0.9681
Epoch 25/40
 - 0s - loss: 0.0126 - acc: 0.9940 - val_loss: 0.1700 - val_acc: 0.9671
Epoch 26/40
 - 0s - loss: 0.0130 - acc: 0.9934 - val_loss: 0.1707 - val_acc: 0.9652
Epoch 27/40
 - 0s - loss: 0.0124 - acc: 0.9932 - val_loss: 0.1838 - val_acc: 0.9700
Epoch 28/40
 - 0s - loss: 0.0118 - acc: 0.9939 - val_loss: 0.1849 - val_acc: 0.9710
Epoch 29/40
 - 0s - loss: 0.0113 - acc: 0.9939 - val_loss: 0.1801 - val_acc: 0.9710
Epoch 30/40
 - 0s - loss: 0.0119 - acc: 0.9929 - val_loss: 0.1889 - val_acc: 0.9700
Epoch 31/40
 - 0s - loss: 0.0112 - acc: 0.9940 - val_loss: 0.1874 - val_acc: 0.9700
Epoch 32/40
 - 0s - loss: 0.0117 - acc: 0.9941 - val_loss: 0.1919 - val_acc: 0.9690
Epoch 33/40
 - 0s - loss: 0.0117 - acc: 0.9933 - val_loss: 0.1950 - val_acc: 0.9690
Epoch 34/40
 - 0s - loss: 0.0112 - acc: 0.9925 - val_loss: 0.1986 - val_acc: 0.9690
Epoch 35/40
 - 0s - loss: 0.0111 - acc: 0.9944 - val_loss: 0.2001 - val_acc: 0.9652
Epoch 36/40
 - 0s - loss: 0.0123 - acc: 0.9936 - val_loss: 0.1897 - val_acc: 0.9690
Epoch 37/40
 - 0s - loss: 0.0112 - acc: 0.9932 - val_loss: 0.1917 - val_acc: 0.9690
Epoch 38/40
 - 0s - loss: 0.0102 - acc: 0.9943 - val_loss: 0.1916 - val_acc: 0.9700
Epoch 39/40
 - 0s - loss: 0.0102 - acc: 0.9944 - val_loss: 0.1894 - val_acc: 0.9710
Epoch 40/40
 - 0s - loss: 0.0104 - acc: 0.9939 - val_loss: 0.1933 - val_acc: 0.9700
Epoch 1/40
 - 0s - loss: 0.1713 - acc: 0.9681
Epoch 2/40
 - 0s - loss: 0.0635 - acc: 0.9787
Epoch 3/40
 - 0s - loss: 0.0434 - acc: 0.9816
Epoch 4/40
 - 0s - loss: 0.0315 - acc: 0.9874
Epoch 5/40
 - 0s - loss: 0.0275 - acc: 0.9894
Epoch 6/40
 - 0s - loss: 0.0248 - acc: 0.9923
Epoch 7/40
 - 0s - loss: 0.0216 - acc: 0.9903
Epoch 8/40
 - 0s - loss: 0.0213 - acc: 0.9894
Epoch 9/40
 - 0s - loss: 0.0220 - acc: 0.9913
Epoch 10/40
 - 0s - loss: 0.0191 - acc: 0.9894
Epoch 11/40
 - 0s - loss: 0.0211 - acc: 0.9894
Epoch 12/40
 - 0s - loss: 0.0195 - acc: 0.9903
Epoch 13/40
 - 0s - loss: 0.0174 - acc: 0.9913
Epoch 14/40
 - 0s - loss: 0.0174 - acc: 0.9913
Epoch 15/40
 - 0s - loss: 0.0163 - acc: 0.9903
Epoch 16/40
 - 0s - loss: 0.0183 - acc: 0.9923
Epoch 17/40
 - 0s - loss: 0.0148 - acc: 0.9923
Epoch 18/40
 - 0s - loss: 0.0154 - acc: 0.9923
Epoch 19/40
 - 0s - loss: 0.0166 - acc: 0.9874
Epoch 20/40
 - 0s - loss: 0.0158 - acc: 0.9913
Epoch 21/40
 - 0s - loss: 0.0158 - acc: 0.9884
Epoch 22/40
 - 0s - loss: 0.0169 - acc: 0.9894
Epoch 23/40
 - 0s - loss: 0.0165 - acc: 0.9903
Epoch 24/40
 - 0s - loss: 0.0126 - acc: 0.9913
Epoch 25/40
 - 0s - loss: 0.0155 - acc: 0.9913
Epoch 26/40
 - 0s - loss: 0.0142 - acc: 0.9894
Epoch 27/40
 - 0s - loss: 0.0156 - acc: 0.9884
Epoch 28/40
 - 0s - loss: 0.0149 - acc: 0.9884
Epoch 29/40
 - 0s - loss: 0.0147 - acc: 0.9923
Epoch 30/40
 - 0s - loss: 0.0194 - acc: 0.9874
Epoch 31/40
 - 0s - loss: 0.0149 - acc: 0.9903
Epoch 32/40
 - 0s - loss: 0.0160 - acc: 0.9932
Epoch 33/40
 - 0s - loss: 0.0167 - acc: 0.9894
Epoch 34/40
 - 0s - loss: 0.0130 - acc: 0.9923
Epoch 35/40
 - 0s - loss: 0.0146 - acc: 0.9913
Epoch 36/40
 - 0s - loss: 0.0133 - acc: 0.9913
Epoch 37/40
 - 0s - loss: 0.0156 - acc: 0.9884
Epoch 38/40
 - 0s - loss: 0.0161 - acc: 0.9874
Epoch 39/40
 - 0s - loss: 0.0154 - acc: 0.9874
Epoch 40/40
 - 0s - loss: 0.0141 - acc: 0.9932
# Training time = 0:00:18.181115
# F-Score(Ordinary) = 0.066, Recall: 0.5, Precision: 0.035
# F-Score(ireflv) = 0.2, Recall: 1.0, Precision: 0.111
********************
Debug enabled
********************
# XP = FR: impTrans 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 48)        23568       input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 24)        936         input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 192)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 96)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 288)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 24)           6936        concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 24)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            200         dropout_2[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.36519607843137253, 1.0: 0.610655737704918, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0}
Train on 402 samples, validate on 45 samples
Epoch 1/40
 - 0s - loss: 2.0474 - acc: 0.3035 - val_loss: 1.9910 - val_acc: 0.5333
Epoch 2/40
 - 0s - loss: 1.9631 - acc: 0.5274 - val_loss: 1.9042 - val_acc: 0.5333
Epoch 3/40
 - 0s - loss: 1.8665 - acc: 0.5199 - val_loss: 1.7931 - val_acc: 0.5333
Epoch 4/40
 - 0s - loss: 1.7422 - acc: 0.5000 - val_loss: 1.6617 - val_acc: 0.5111
Epoch 5/40
 - 0s - loss: 1.6071 - acc: 0.4925 - val_loss: 1.5178 - val_acc: 0.4889
Epoch 6/40
 - 0s - loss: 1.4598 - acc: 0.4925 - val_loss: 1.3760 - val_acc: 0.4889
Epoch 7/40
 - 0s - loss: 1.3284 - acc: 0.4975 - val_loss: 1.2574 - val_acc: 0.5111
Epoch 8/40
 - 0s - loss: 1.2202 - acc: 0.5498 - val_loss: 1.1626 - val_acc: 0.5333
Epoch 9/40
 - 0s - loss: 1.1112 - acc: 0.5846 - val_loss: 1.0820 - val_acc: 0.5556
Epoch 10/40
 - 0s - loss: 1.0094 - acc: 0.7040 - val_loss: 1.0074 - val_acc: 0.6889
Epoch 11/40
 - 0s - loss: 0.9256 - acc: 0.7662 - val_loss: 0.9353 - val_acc: 0.8000
Epoch 12/40
 - 0s - loss: 0.8435 - acc: 0.7861 - val_loss: 0.8688 - val_acc: 0.8000
Epoch 13/40
 - 0s - loss: 0.7731 - acc: 0.8308 - val_loss: 0.8077 - val_acc: 0.8000
Epoch 14/40
 - 0s - loss: 0.6936 - acc: 0.8358 - val_loss: 0.7532 - val_acc: 0.8222
Epoch 15/40
 - 0s - loss: 0.6587 - acc: 0.8433 - val_loss: 0.7050 - val_acc: 0.8444
Epoch 16/40
 - 0s - loss: 0.5908 - acc: 0.8507 - val_loss: 0.6640 - val_acc: 0.8444
Epoch 17/40
 - 0s - loss: 0.5390 - acc: 0.8632 - val_loss: 0.6290 - val_acc: 0.8444
Epoch 18/40
 - 0s - loss: 0.5271 - acc: 0.8532 - val_loss: 0.5985 - val_acc: 0.8444
Epoch 19/40
 - 0s - loss: 0.4783 - acc: 0.8557 - val_loss: 0.5751 - val_acc: 0.8444
Epoch 20/40
 - 0s - loss: 0.4661 - acc: 0.8458 - val_loss: 0.5565 - val_acc: 0.8444
Epoch 21/40
 - 0s - loss: 0.4502 - acc: 0.8507 - val_loss: 0.5396 - val_acc: 0.8444
Epoch 22/40
 - 0s - loss: 0.4056 - acc: 0.8682 - val_loss: 0.5227 - val_acc: 0.8667
Epoch 23/40
 - 0s - loss: 0.3864 - acc: 0.8657 - val_loss: 0.5068 - val_acc: 0.8667
Epoch 24/40
 - 0s - loss: 0.3789 - acc: 0.8731 - val_loss: 0.4934 - val_acc: 0.8667
Epoch 25/40
 - 0s - loss: 0.3745 - acc: 0.8657 - val_loss: 0.4818 - val_acc: 0.8667
Epoch 26/40
 - 0s - loss: 0.3519 - acc: 0.8731 - val_loss: 0.4712 - val_acc: 0.8667
Epoch 27/40
 - 0s - loss: 0.3321 - acc: 0.8731 - val_loss: 0.4632 - val_acc: 0.8667
Epoch 28/40
 - 0s - loss: 0.3225 - acc: 0.8756 - val_loss: 0.4545 - val_acc: 0.8667
Epoch 29/40
 - 0s - loss: 0.3150 - acc: 0.8831 - val_loss: 0.4471 - val_acc: 0.8667
Epoch 30/40
 - 0s - loss: 0.3063 - acc: 0.8731 - val_loss: 0.4415 - val_acc: 0.8667
Epoch 31/40
 - 0s - loss: 0.2998 - acc: 0.8756 - val_loss: 0.4354 - val_acc: 0.8667
Epoch 32/40
 - 0s - loss: 0.2864 - acc: 0.8881 - val_loss: 0.4288 - val_acc: 0.8667
Epoch 33/40
 - 0s - loss: 0.2828 - acc: 0.8632 - val_loss: 0.4193 - val_acc: 0.8667
Epoch 34/40
 - 0s - loss: 0.2783 - acc: 0.8756 - val_loss: 0.4140 - val_acc: 0.8667
Epoch 35/40
 - 0s - loss: 0.2746 - acc: 0.8781 - val_loss: 0.4072 - val_acc: 0.8667
Epoch 36/40
 - 0s - loss: 0.2686 - acc: 0.8756 - val_loss: 0.4043 - val_acc: 0.8667
Epoch 37/40
 - 0s - loss: 0.2527 - acc: 0.8930 - val_loss: 0.4026 - val_acc: 0.8667
Epoch 38/40
 - 0s - loss: 0.2494 - acc: 0.8781 - val_loss: 0.4022 - val_acc: 0.8667
Epoch 39/40
 - 0s - loss: 0.2687 - acc: 0.8607 - val_loss: 0.3979 - val_acc: 0.8667
Epoch 40/40
 - 0s - loss: 0.2437 - acc: 0.8706 - val_loss: 0.3978 - val_acc: 0.8444
Epoch 1/40
 - 0s - loss: 0.4672 - acc: 0.8444
Epoch 2/40
 - 0s - loss: 0.4009 - acc: 0.8667
Epoch 3/40
 - 0s - loss: 0.4285 - acc: 0.8667
Epoch 4/40
 - 0s - loss: 0.3676 - acc: 0.8667
Epoch 5/40
 - 0s - loss: 0.3833 - acc: 0.8667
Epoch 6/40
 - 0s - loss: 0.3763 - acc: 0.8667
Epoch 7/40
 - 0s - loss: 0.3420 - acc: 0.8667
Epoch 8/40
 - 0s - loss: 0.3156 - acc: 0.8667
Epoch 9/40
 - 0s - loss: 0.3727 - acc: 0.8667
Epoch 10/40
 - 0s - loss: 0.2884 - acc: 0.8667
Epoch 11/40
 - 0s - loss: 0.2880 - acc: 0.8889
Epoch 12/40
 - 0s - loss: 0.2932 - acc: 0.8667
Epoch 13/40
 - 0s - loss: 0.2956 - acc: 0.8444
Epoch 14/40
 - 0s - loss: 0.2466 - acc: 0.9111
Epoch 15/40
 - 0s - loss: 0.3334 - acc: 0.8444
Epoch 16/40
 - 0s - loss: 0.2859 - acc: 0.8889
Epoch 17/40
 - 0s - loss: 0.2204 - acc: 0.9111
Epoch 18/40
 - 0s - loss: 0.2681 - acc: 0.8667
Epoch 19/40
 - 0s - loss: 0.2555 - acc: 0.8889
Epoch 20/40
 - 0s - loss: 0.2334 - acc: 0.9333
Epoch 21/40
 - 0s - loss: 0.2247 - acc: 0.9556
Epoch 22/40
 - 0s - loss: 0.2437 - acc: 0.8667
Epoch 23/40
 - 0s - loss: 0.2441 - acc: 0.8889
Epoch 24/40
 - 0s - loss: 0.2330 - acc: 0.8889
Epoch 25/40
 - 0s - loss: 0.2109 - acc: 0.9111
Epoch 26/40
 - 0s - loss: 0.2057 - acc: 0.9333
Epoch 27/40
 - 0s - loss: 0.2303 - acc: 0.8667
Epoch 28/40
 - 0s - loss: 0.2218 - acc: 0.8667
Epoch 29/40
 - 0s - loss: 0.2010 - acc: 0.9111
Epoch 30/40
 - 0s - loss: 0.1820 - acc: 0.9333
Epoch 31/40
 - 0s - loss: 0.1845 - acc: 0.9333
Epoch 32/40
 - 0s - loss: 0.1963 - acc: 0.8667
Epoch 33/40
 - 0s - loss: 0.1955 - acc: 0.9111
Epoch 34/40
 - 0s - loss: 0.2050 - acc: 0.8889
Epoch 35/40
 - 0s - loss: 0.2732 - acc: 0.8444
Epoch 36/40
 - 0s - loss: 0.2522 - acc: 0.8444
Epoch 37/40
 - 0s - loss: 0.1806 - acc: 0.9556
Epoch 38/40
 - 0s - loss: 0.2125 - acc: 0.8889
Epoch 39/40
 - 0s - loss: 0.2279 - acc: 0.8667
Epoch 40/40
 - 0s - loss: 0.2357 - acc: 0.8667
# Training time = 0:00:02.234498
# F-Score(Ordinary) = 0.034, Recall: 1.0, Precision: 0.018
# F-Score(id) = 0.095, Recall: 1.0, Precision: 0.05
********************
Debug enabled
********************
# XP = FR: impTrans favorisationCoeff = 1
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 48)        23568       input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 24)        936         input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 192)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 96)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 288)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 24)           6936        concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 24)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            200         dropout_3[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.36519607843137253, 1.0: 0.610655737704918, 2.0: 1.0642857142857143, 4.0: 4.138888888888889, 5.0: 3.725, 6.0: 5.730769230769231}
Train on 402 samples, validate on 45 samples
Epoch 1/40
 - 0s - loss: 2.0847 - acc: 0.2587 - val_loss: 2.0054 - val_acc: 0.4222
Epoch 2/40
 - 0s - loss: 2.0478 - acc: 0.4502 - val_loss: 1.9833 - val_acc: 0.5556
Epoch 3/40
 - 0s - loss: 2.0086 - acc: 0.5274 - val_loss: 1.9518 - val_acc: 0.5778
Epoch 4/40
 - 0s - loss: 1.9623 - acc: 0.5896 - val_loss: 1.9156 - val_acc: 0.6222
Epoch 5/40
 - 0s - loss: 1.9150 - acc: 0.6343 - val_loss: 1.8728 - val_acc: 0.6667
Epoch 6/40
 - 0s - loss: 1.8478 - acc: 0.6692 - val_loss: 1.8233 - val_acc: 0.7333
Epoch 7/40
 - 0s - loss: 1.7921 - acc: 0.7363 - val_loss: 1.7641 - val_acc: 0.7333
Epoch 8/40
 - 0s - loss: 1.6971 - acc: 0.7114 - val_loss: 1.6989 - val_acc: 0.7556
Epoch 9/40
 - 0s - loss: 1.6081 - acc: 0.7413 - val_loss: 1.6275 - val_acc: 0.8000
Epoch 10/40
 - 0s - loss: 1.5020 - acc: 0.7836 - val_loss: 1.5489 - val_acc: 0.8000
Epoch 11/40
 - 0s - loss: 1.3647 - acc: 0.7711 - val_loss: 1.4594 - val_acc: 0.8000
Epoch 12/40
 - 0s - loss: 1.2606 - acc: 0.7886 - val_loss: 1.3717 - val_acc: 0.8000
Epoch 13/40
 - 0s - loss: 1.1648 - acc: 0.7910 - val_loss: 1.2849 - val_acc: 0.8000
Epoch 14/40
 - 0s - loss: 1.0348 - acc: 0.7736 - val_loss: 1.2100 - val_acc: 0.8444
Epoch 15/40
 - 0s - loss: 0.9428 - acc: 0.7836 - val_loss: 1.1359 - val_acc: 0.8444
Epoch 16/40
 - 0s - loss: 0.8722 - acc: 0.7861 - val_loss: 1.0668 - val_acc: 0.8444
Epoch 17/40
 - 0s - loss: 0.7638 - acc: 0.7910 - val_loss: 1.0141 - val_acc: 0.8444
Epoch 18/40
 - 0s - loss: 0.7480 - acc: 0.8035 - val_loss: 0.9594 - val_acc: 0.8444
Epoch 19/40
 - 0s - loss: 0.6512 - acc: 0.7985 - val_loss: 0.9152 - val_acc: 0.8444
Epoch 20/40
 - 0s - loss: 0.6061 - acc: 0.8035 - val_loss: 0.8875 - val_acc: 0.8444
Epoch 21/40
 - 0s - loss: 0.5611 - acc: 0.8159 - val_loss: 0.8643 - val_acc: 0.8444
Epoch 22/40
 - 0s - loss: 0.5192 - acc: 0.8159 - val_loss: 0.8441 - val_acc: 0.8444
Epoch 23/40
 - 0s - loss: 0.4785 - acc: 0.8333 - val_loss: 0.8340 - val_acc: 0.8222
Epoch 24/40
 - 0s - loss: 0.4597 - acc: 0.8483 - val_loss: 0.8178 - val_acc: 0.8222
Epoch 25/40
 - 0s - loss: 0.4502 - acc: 0.8507 - val_loss: 0.8108 - val_acc: 0.8444
Epoch 26/40
 - 0s - loss: 0.4309 - acc: 0.8458 - val_loss: 0.8176 - val_acc: 0.8444
Epoch 27/40
 - 0s - loss: 0.3978 - acc: 0.8532 - val_loss: 0.8217 - val_acc: 0.8444
Epoch 28/40
 - 0s - loss: 0.3605 - acc: 0.8657 - val_loss: 0.8168 - val_acc: 0.8444
Epoch 29/40
 - 0s - loss: 0.3875 - acc: 0.8607 - val_loss: 0.8212 - val_acc: 0.8444
Epoch 30/40
 - 0s - loss: 0.3620 - acc: 0.8607 - val_loss: 0.8063 - val_acc: 0.8444
Epoch 31/40
 - 0s - loss: 0.3592 - acc: 0.8657 - val_loss: 0.8096 - val_acc: 0.8444
Epoch 32/40
 - 0s - loss: 0.3497 - acc: 0.8756 - val_loss: 0.8097 - val_acc: 0.8444
Epoch 33/40
 - 0s - loss: 0.3411 - acc: 0.8657 - val_loss: 0.7881 - val_acc: 0.8444
Epoch 34/40
 - 0s - loss: 0.3176 - acc: 0.8731 - val_loss: 0.7840 - val_acc: 0.8444
Epoch 35/40
 - 0s - loss: 0.3309 - acc: 0.8706 - val_loss: 0.7645 - val_acc: 0.8444
Epoch 36/40
 - 0s - loss: 0.3270 - acc: 0.8731 - val_loss: 0.7762 - val_acc: 0.8444
Epoch 37/40
 - 0s - loss: 0.3048 - acc: 0.8706 - val_loss: 0.7906 - val_acc: 0.8444
Epoch 38/40
 - 0s - loss: 0.3078 - acc: 0.8706 - val_loss: 0.8132 - val_acc: 0.8444
Epoch 39/40
 - 0s - loss: 0.3153 - acc: 0.8607 - val_loss: 0.8044 - val_acc: 0.8444
Epoch 40/40
 - 0s - loss: 0.3144 - acc: 0.8756 - val_loss: 0.8138 - val_acc: 0.8444
Epoch 1/40
 - 0s - loss: 0.8776 - acc: 0.8000
Epoch 2/40
 - 0s - loss: 0.8494 - acc: 0.8222
Epoch 3/40
 - 0s - loss: 0.7636 - acc: 0.8222
Epoch 4/40
 - 0s - loss: 0.5360 - acc: 0.8889
Epoch 5/40
 - 0s - loss: 0.5911 - acc: 0.8667
Epoch 6/40
 - 0s - loss: 0.5808 - acc: 0.8222
Epoch 7/40
 - 0s - loss: 0.4735 - acc: 0.8889
Epoch 8/40
 - 0s - loss: 0.3859 - acc: 0.8222
Epoch 9/40
 - 0s - loss: 0.4944 - acc: 0.8222
Epoch 10/40
 - 0s - loss: 0.3675 - acc: 0.8667
Epoch 11/40
 - 0s - loss: 0.4348 - acc: 0.8222
Epoch 12/40
 - 0s - loss: 0.3222 - acc: 0.8222
Epoch 13/40
 - 0s - loss: 0.3768 - acc: 0.8889
Epoch 14/40
 - 0s - loss: 0.3425 - acc: 0.8667
Epoch 15/40
 - 0s - loss: 0.4386 - acc: 0.8667
Epoch 16/40
 - 0s - loss: 0.3826 - acc: 0.8667
Epoch 17/40
 - 0s - loss: 0.2939 - acc: 0.8667
Epoch 18/40
 - 0s - loss: 0.3664 - acc: 0.8667
Epoch 19/40
 - 0s - loss: 0.4214 - acc: 0.8444
Epoch 20/40
 - 0s - loss: 0.3322 - acc: 0.8667
Epoch 21/40
 - 0s - loss: 0.2801 - acc: 0.8444
Epoch 22/40
 - 0s - loss: 0.3524 - acc: 0.8889
Epoch 23/40
 - 0s - loss: 0.3261 - acc: 0.8889
Epoch 24/40
 - 0s - loss: 0.2992 - acc: 0.8444
Epoch 25/40
 - 0s - loss: 0.2899 - acc: 0.8667
Epoch 26/40
 - 0s - loss: 0.3009 - acc: 0.8889
Epoch 27/40
 - 0s - loss: 0.2846 - acc: 0.8889
Epoch 28/40
 - 0s - loss: 0.3067 - acc: 0.8889
Epoch 29/40
 - 0s - loss: 0.2853 - acc: 0.8889
Epoch 30/40
 - 0s - loss: 0.2566 - acc: 0.8889
Epoch 31/40
 - 0s - loss: 0.2457 - acc: 0.8889
Epoch 32/40
 - 0s - loss: 0.2830 - acc: 0.8889
Epoch 33/40
 - 0s - loss: 0.3142 - acc: 0.8889
Epoch 34/40
 - 0s - loss: 0.2820 - acc: 0.8667
Epoch 35/40
 - 0s - loss: 0.3207 - acc: 0.8889
Epoch 36/40
 - 0s - loss: 0.3310 - acc: 0.8889
Epoch 37/40
 - 0s - loss: 0.2467 - acc: 0.8889
Epoch 38/40
 - 0s - loss: 0.3279 - acc: 0.8667
Epoch 39/40
 - 0s - loss: 0.3008 - acc: 0.8889
Epoch 40/40
 - 0s - loss: 0.2706 - acc: 0.8889
# Training time = 0:00:02.245106
# F-Score(Ordinary) = 0.146, Recall: 0.117, Precision: 0.193
# F-Score(ireflv) = 0.205, Recall: 0.133, Precision: 0.444
# F-Score(id) = 0.12, Recall: 0.1, Precision: 0.15
********************
Debug enabled
********************
# XP = FR: impTrans overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 48)        23568       input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 24)        936         input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 192)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 96)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 288)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 24)           6936        concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            200         dropout_4[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 447
data size after sampling = 1224
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0}
Train on 1101 samples, validate on 123 samples
Epoch 1/40
 - 0s - loss: 2.0291 - acc: 0.2661 - val_loss: 2.1516 - val_acc: 0.0000e+00
Epoch 2/40
 - 0s - loss: 1.8746 - acc: 0.4233 - val_loss: 2.2824 - val_acc: 0.0000e+00
Epoch 3/40
 - 0s - loss: 1.6426 - acc: 0.5332 - val_loss: 2.3959 - val_acc: 0.0000e+00
Epoch 4/40
 - 0s - loss: 1.3324 - acc: 0.6712 - val_loss: 2.2856 - val_acc: 0.0000e+00
Epoch 5/40
 - 0s - loss: 1.0075 - acc: 0.7738 - val_loss: 1.7892 - val_acc: 0.3496
Epoch 6/40
 - 0s - loss: 0.7676 - acc: 0.8084 - val_loss: 1.2637 - val_acc: 0.6667
Epoch 7/40
 - 0s - loss: 0.5849 - acc: 0.8619 - val_loss: 0.8038 - val_acc: 0.8374
Epoch 8/40
 - 0s - loss: 0.4816 - acc: 0.8710 - val_loss: 0.6365 - val_acc: 0.8374
Epoch 9/40
 - 0s - loss: 0.4098 - acc: 0.8874 - val_loss: 0.4709 - val_acc: 0.9512
Epoch 10/40
 - 0s - loss: 0.3718 - acc: 0.8955 - val_loss: 0.3831 - val_acc: 0.9512
Epoch 11/40
 - 0s - loss: 0.3505 - acc: 0.8955 - val_loss: 0.3488 - val_acc: 0.9512
Epoch 12/40
 - 0s - loss: 0.3272 - acc: 0.9001 - val_loss: 0.3398 - val_acc: 0.9512
Epoch 13/40
 - 0s - loss: 0.3071 - acc: 0.9046 - val_loss: 0.4024 - val_acc: 0.7967
Epoch 14/40
 - 0s - loss: 0.2995 - acc: 0.9055 - val_loss: 0.3047 - val_acc: 0.9512
Epoch 15/40
 - 0s - loss: 0.3013 - acc: 0.9019 - val_loss: 0.3076 - val_acc: 0.9512
Epoch 16/40
 - 0s - loss: 0.3003 - acc: 0.9064 - val_loss: 0.2841 - val_acc: 1.0000
Epoch 17/40
 - 0s - loss: 0.2835 - acc: 0.9037 - val_loss: 0.2516 - val_acc: 1.0000
Epoch 18/40
 - 0s - loss: 0.2760 - acc: 0.9074 - val_loss: 0.3418 - val_acc: 0.9106
Epoch 19/40
 - 0s - loss: 0.2736 - acc: 0.9019 - val_loss: 0.2750 - val_acc: 1.0000
Epoch 20/40
 - 0s - loss: 0.2705 - acc: 0.9092 - val_loss: 0.2969 - val_acc: 1.0000
Epoch 21/40
 - 0s - loss: 0.2667 - acc: 0.9110 - val_loss: 0.2808 - val_acc: 1.0000
Epoch 22/40
 - 0s - loss: 0.2702 - acc: 0.9074 - val_loss: 0.3090 - val_acc: 0.9593
Epoch 23/40
 - 0s - loss: 0.2654 - acc: 0.9092 - val_loss: 0.2321 - val_acc: 1.0000
Epoch 24/40
 - 0s - loss: 0.2710 - acc: 0.9064 - val_loss: 0.3261 - val_acc: 0.9593
Epoch 25/40
 - 0s - loss: 0.2538 - acc: 0.9146 - val_loss: 0.2470 - val_acc: 1.0000
Epoch 26/40
 - 0s - loss: 0.2552 - acc: 0.9137 - val_loss: 0.2941 - val_acc: 1.0000
Epoch 27/40
 - 0s - loss: 0.2546 - acc: 0.9146 - val_loss: 0.2981 - val_acc: 1.0000
Epoch 28/40
 - 0s - loss: 0.2514 - acc: 0.9128 - val_loss: 0.2898 - val_acc: 1.0000
Epoch 29/40
 - 0s - loss: 0.2528 - acc: 0.9110 - val_loss: 0.2363 - val_acc: 1.0000
Epoch 30/40
 - 0s - loss: 0.2485 - acc: 0.9137 - val_loss: 0.2120 - val_acc: 1.0000
Epoch 31/40
 - 0s - loss: 0.2545 - acc: 0.9137 - val_loss: 0.1997 - val_acc: 1.0000
Epoch 32/40
 - 0s - loss: 0.2412 - acc: 0.9164 - val_loss: 0.1946 - val_acc: 1.0000
Epoch 33/40
 - 0s - loss: 0.2457 - acc: 0.9173 - val_loss: 0.2127 - val_acc: 1.0000
Epoch 34/40
 - 0s - loss: 0.2486 - acc: 0.9155 - val_loss: 0.2349 - val_acc: 1.0000
Epoch 35/40
 - 0s - loss: 0.2410 - acc: 0.9183 - val_loss: 0.2379 - val_acc: 1.0000
Epoch 36/40
 - 0s - loss: 0.2446 - acc: 0.9155 - val_loss: 0.2201 - val_acc: 1.0000
Epoch 37/40
 - 0s - loss: 0.2477 - acc: 0.9146 - val_loss: 0.1814 - val_acc: 1.0000
Epoch 38/40
 - 0s - loss: 0.2443 - acc: 0.9155 - val_loss: 0.2234 - val_acc: 1.0000
Epoch 39/40
 - 0s - loss: 0.2391 - acc: 0.9137 - val_loss: 0.2625 - val_acc: 1.0000
Epoch 40/40
 - 0s - loss: 0.2463 - acc: 0.9173 - val_loss: 0.2005 - val_acc: 1.0000
Epoch 1/40
 - 0s - loss: 0.2259 - acc: 1.0000
Epoch 2/40
 - 0s - loss: 0.2130 - acc: 0.9919
Epoch 3/40
 - 0s - loss: 0.1411 - acc: 1.0000
Epoch 4/40
 - 0s - loss: 0.1009 - acc: 1.0000
Epoch 5/40
 - 0s - loss: 0.0576 - acc: 1.0000
Epoch 6/40
 - 0s - loss: 0.0407 - acc: 1.0000
Epoch 7/40
 - 0s - loss: 0.0305 - acc: 1.0000
Epoch 8/40
 - 0s - loss: 0.0312 - acc: 1.0000
Epoch 9/40
 - 0s - loss: 0.0244 - acc: 1.0000
Epoch 10/40
 - 0s - loss: 0.0224 - acc: 1.0000
Epoch 11/40
 - 0s - loss: 0.0180 - acc: 1.0000
Epoch 12/40
 - 0s - loss: 0.0122 - acc: 1.0000
Epoch 13/40
 - 0s - loss: 0.0122 - acc: 1.0000
Epoch 14/40
 - 0s - loss: 0.0105 - acc: 1.0000
Epoch 15/40
 - 0s - loss: 0.0092 - acc: 1.0000
Epoch 16/40
 - 0s - loss: 0.0119 - acc: 1.0000
Epoch 17/40
 - 0s - loss: 0.0082 - acc: 1.0000
Epoch 18/40
 - 0s - loss: 0.0054 - acc: 1.0000
Epoch 19/40
 - 0s - loss: 0.0095 - acc: 1.0000
Epoch 20/40
 - 0s - loss: 0.0062 - acc: 1.0000
Epoch 21/40
 - 0s - loss: 0.0084 - acc: 1.0000
Epoch 22/40
 - 0s - loss: 0.0069 - acc: 1.0000
Epoch 23/40
 - 0s - loss: 0.0080 - acc: 1.0000
Epoch 24/40
 - 0s - loss: 0.0064 - acc: 1.0000
Epoch 25/40
 - 0s - loss: 0.0047 - acc: 1.0000
Epoch 26/40
 - 0s - loss: 0.0051 - acc: 1.0000
Epoch 27/40
 - 0s - loss: 0.0032 - acc: 1.0000
Epoch 28/40
 - 0s - loss: 0.0058 - acc: 1.0000
Epoch 29/40
 - 0s - loss: 0.0036 - acc: 1.0000
Epoch 30/40
 - 0s - loss: 0.0067 - acc: 1.0000
Epoch 31/40
 - 0s - loss: 0.0031 - acc: 1.0000
Epoch 32/40
 - 0s - loss: 0.0056 - acc: 1.0000
Epoch 33/40
 - 0s - loss: 0.0048 - acc: 1.0000
Epoch 34/40
 - 0s - loss: 0.0048 - acc: 1.0000
Epoch 35/40
 - 0s - loss: 0.0066 - acc: 1.0000
Epoch 36/40
 - 0s - loss: 0.0043 - acc: 1.0000
Epoch 37/40
 - 0s - loss: 0.0028 - acc: 1.0000
Epoch 38/40
 - 0s - loss: 0.0049 - acc: 1.0000
Epoch 39/40
 - 0s - loss: 0.0057 - acc: 1.0000
Epoch 40/40
 - 0s - loss: 0.0060 - acc: 1.0000
# Training time = 0:00:04.031946
# F-Score(Ordinary) = 0.059, Recall: 0.034, Precision: 0.228
# F-Score(lvc) = 0.031, Recall: 0.016, Precision: 0.316
# F-Score(ireflv) = 0.25, Recall: 0.5, Precision: 0.167
# F-Score(id) = 0.174, Recall: 0.667, Precision: 0.1
********************
Debug enabled
********************
# XP = FR: impTrans overSampling favorisationCoeff = 1
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 48)        23568       input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 24)        936         input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 192)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 96)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 288)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 24)           6936        concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 24)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            200         dropout_5[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 447
data size after sampling = 1224
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 1.0, 4.0: 1.0, 5.0: 1.0, 6.0: 1.0}
Train on 1101 samples, validate on 123 samples
Epoch 1/40
 - 0s - loss: 2.0291 - acc: 0.2661 - val_loss: 2.1516 - val_acc: 0.0000e+00
Epoch 2/40
 - 0s - loss: 1.8746 - acc: 0.4233 - val_loss: 2.2824 - val_acc: 0.0000e+00
Epoch 3/40
 - 0s - loss: 1.6426 - acc: 0.5332 - val_loss: 2.3959 - val_acc: 0.0000e+00
Epoch 4/40
 - 0s - loss: 1.3324 - acc: 0.6712 - val_loss: 2.2856 - val_acc: 0.0000e+00
Epoch 5/40
 - 0s - loss: 1.0075 - acc: 0.7738 - val_loss: 1.7892 - val_acc: 0.3496
Epoch 6/40
 - 0s - loss: 0.7676 - acc: 0.8084 - val_loss: 1.2637 - val_acc: 0.6667
Epoch 7/40
 - 0s - loss: 0.5849 - acc: 0.8619 - val_loss: 0.8038 - val_acc: 0.8374
Epoch 8/40
 - 0s - loss: 0.4816 - acc: 0.8710 - val_loss: 0.6365 - val_acc: 0.8374
Epoch 9/40
 - 0s - loss: 0.4098 - acc: 0.8874 - val_loss: 0.4709 - val_acc: 0.9512
Epoch 10/40
 - 0s - loss: 0.3718 - acc: 0.8955 - val_loss: 0.3831 - val_acc: 0.9512
Epoch 11/40
 - 0s - loss: 0.3505 - acc: 0.8955 - val_loss: 0.3488 - val_acc: 0.9512
Epoch 12/40
 - 0s - loss: 0.3272 - acc: 0.9001 - val_loss: 0.3398 - val_acc: 0.9512
Epoch 13/40
 - 0s - loss: 0.3071 - acc: 0.9046 - val_loss: 0.4024 - val_acc: 0.7967
Epoch 14/40
 - 0s - loss: 0.2995 - acc: 0.9055 - val_loss: 0.3047 - val_acc: 0.9512
Epoch 15/40
 - 0s - loss: 0.3013 - acc: 0.9019 - val_loss: 0.3076 - val_acc: 0.9512
Epoch 16/40
 - 0s - loss: 0.3003 - acc: 0.9064 - val_loss: 0.2841 - val_acc: 1.0000
Epoch 17/40
 - 0s - loss: 0.2835 - acc: 0.9037 - val_loss: 0.2516 - val_acc: 1.0000
Epoch 18/40
 - 0s - loss: 0.2760 - acc: 0.9074 - val_loss: 0.3418 - val_acc: 0.9106
Epoch 19/40
 - 0s - loss: 0.2736 - acc: 0.9019 - val_loss: 0.2750 - val_acc: 1.0000
Epoch 20/40
 - 0s - loss: 0.2705 - acc: 0.9092 - val_loss: 0.2969 - val_acc: 1.0000
Epoch 21/40
 - 0s - loss: 0.2667 - acc: 0.9110 - val_loss: 0.2808 - val_acc: 1.0000
Epoch 22/40
 - 0s - loss: 0.2702 - acc: 0.9074 - val_loss: 0.3090 - val_acc: 0.9593
Epoch 23/40
 - 0s - loss: 0.2654 - acc: 0.9092 - val_loss: 0.2321 - val_acc: 1.0000
Epoch 24/40
 - 0s - loss: 0.2710 - acc: 0.9064 - val_loss: 0.3261 - val_acc: 0.9593
Epoch 25/40
 - 0s - loss: 0.2538 - acc: 0.9146 - val_loss: 0.2470 - val_acc: 1.0000
Epoch 26/40
 - 0s - loss: 0.2552 - acc: 0.9137 - val_loss: 0.2941 - val_acc: 1.0000
Epoch 27/40
 - 0s - loss: 0.2546 - acc: 0.9146 - val_loss: 0.2981 - val_acc: 1.0000
Epoch 28/40
 - 0s - loss: 0.2514 - acc: 0.9128 - val_loss: 0.2898 - val_acc: 1.0000
Epoch 29/40
 - 0s - loss: 0.2528 - acc: 0.9110 - val_loss: 0.2363 - val_acc: 1.0000
Epoch 30/40
 - 0s - loss: 0.2485 - acc: 0.9137 - val_loss: 0.2120 - val_acc: 1.0000
Epoch 31/40
 - 0s - loss: 0.2545 - acc: 0.9137 - val_loss: 0.1997 - val_acc: 1.0000
Epoch 32/40
 - 0s - loss: 0.2412 - acc: 0.9164 - val_loss: 0.1946 - val_acc: 1.0000
Epoch 33/40
 - 0s - loss: 0.2457 - acc: 0.9173 - val_loss: 0.2127 - val_acc: 1.0000
Epoch 34/40
 - 0s - loss: 0.2486 - acc: 0.9155 - val_loss: 0.2349 - val_acc: 1.0000
Epoch 35/40
 - 0s - loss: 0.2410 - acc: 0.9183 - val_loss: 0.2379 - val_acc: 1.0000
Epoch 36/40
 - 0s - loss: 0.2446 - acc: 0.9155 - val_loss: 0.2201 - val_acc: 1.0000
Epoch 37/40
 - 0s - loss: 0.2477 - acc: 0.9146 - val_loss: 0.1814 - val_acc: 1.0000
Epoch 38/40
 - 0s - loss: 0.2443 - acc: 0.9155 - val_loss: 0.2234 - val_acc: 1.0000
Epoch 39/40
 - 0s - loss: 0.2391 - acc: 0.9137 - val_loss: 0.2625 - val_acc: 1.0000
Epoch 40/40
 - 0s - loss: 0.2463 - acc: 0.9173 - val_loss: 0.2005 - val_acc: 1.0000
Epoch 1/40
 - 0s - loss: 0.2259 - acc: 1.0000
Epoch 2/40
 - 0s - loss: 0.2130 - acc: 0.9919
Epoch 3/40
 - 0s - loss: 0.1411 - acc: 1.0000
Epoch 4/40
 - 0s - loss: 0.1009 - acc: 1.0000
Epoch 5/40
 - 0s - loss: 0.0576 - acc: 1.0000
Epoch 6/40
 - 0s - loss: 0.0407 - acc: 1.0000
Epoch 7/40
 - 0s - loss: 0.0305 - acc: 1.0000
Epoch 8/40
 - 0s - loss: 0.0312 - acc: 1.0000
Epoch 9/40
 - 0s - loss: 0.0244 - acc: 1.0000
Epoch 10/40
 - 0s - loss: 0.0224 - acc: 1.0000
Epoch 11/40
 - 0s - loss: 0.0180 - acc: 1.0000
Epoch 12/40
 - 0s - loss: 0.0122 - acc: 1.0000
Epoch 13/40
 - 0s - loss: 0.0122 - acc: 1.0000
Epoch 14/40
 - 0s - loss: 0.0105 - acc: 1.0000
Epoch 15/40
 - 0s - loss: 0.0092 - acc: 1.0000
Epoch 16/40
 - 0s - loss: 0.0119 - acc: 1.0000
Epoch 17/40
 - 0s - loss: 0.0082 - acc: 1.0000
Epoch 18/40
 - 0s - loss: 0.0054 - acc: 1.0000
Epoch 19/40
 - 0s - loss: 0.0095 - acc: 1.0000
Epoch 20/40
 - 0s - loss: 0.0062 - acc: 1.0000
Epoch 21/40
 - 0s - loss: 0.0084 - acc: 1.0000
Epoch 22/40
 - 0s - loss: 0.0069 - acc: 1.0000
Epoch 23/40
 - 0s - loss: 0.0080 - acc: 1.0000
Epoch 24/40
 - 0s - loss: 0.0064 - acc: 1.0000
Epoch 25/40
 - 0s - loss: 0.0047 - acc: 1.0000
Epoch 26/40
 - 0s - loss: 0.0051 - acc: 1.0000
Epoch 27/40
 - 0s - loss: 0.0032 - acc: 1.0000
Epoch 28/40
 - 0s - loss: 0.0058 - acc: 1.0000
Epoch 29/40
 - 0s - loss: 0.0036 - acc: 1.0000
Epoch 30/40
 - 0s - loss: 0.0067 - acc: 1.0000
Epoch 31/40
 - 0s - loss: 0.0031 - acc: 1.0000
Epoch 32/40
 - 0s - loss: 0.0056 - acc: 1.0000
Epoch 33/40
 - 0s - loss: 0.0048 - acc: 1.0000
Epoch 34/40
 - 0s - loss: 0.0048 - acc: 1.0000
Epoch 35/40
 - 0s - loss: 0.0066 - acc: 1.0000
Epoch 36/40
 - 0s - loss: 0.0043 - acc: 1.0000
Epoch 37/40
 - 0s - loss: 0.0028 - acc: 1.0000
Epoch 38/40
 - 0s - loss: 0.0049 - acc: 1.0000
Epoch 39/40
 - 0s - loss: 0.0057 - acc: 1.0000
Epoch 40/40
 - 0s - loss: 0.0060 - acc: 1.0000
# Training time = 0:00:02.941699
# F-Score(Ordinary) = 0.059, Recall: 0.034, Precision: 0.228
# F-Score(lvc) = 0.031, Recall: 0.016, Precision: 0.316
# F-Score(ireflv) = 0.25, Recall: 0.5, Precision: 0.167
# F-Score(id) = 0.174, Recall: 0.667, Precision: 0.1
********************
Debug enabled
********************
# XP = FR: impSent 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 48)        23568       input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 24)        936         input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 192)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 96)           0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 288)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 24)           6936        concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 24)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            200         dropout_6[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.3388349514563107, 1.0: 0.35491525423728815, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0}
Train on 2826 samples, validate on 315 samples
Epoch 1/40
 - 0s - loss: 1.6157 - acc: 0.5902 - val_loss: 0.9334 - val_acc: 0.8921
Epoch 2/40
 - 0s - loss: 0.5082 - acc: 0.9179 - val_loss: 0.4109 - val_acc: 0.8857
Epoch 3/40
 - 0s - loss: 0.2687 - acc: 0.9360 - val_loss: 0.3570 - val_acc: 0.8889
Epoch 4/40
 - 0s - loss: 0.2057 - acc: 0.9466 - val_loss: 0.3377 - val_acc: 0.8921
Epoch 5/40
 - 0s - loss: 0.1725 - acc: 0.9526 - val_loss: 0.3224 - val_acc: 0.8952
Epoch 6/40
 - 0s - loss: 0.1471 - acc: 0.9611 - val_loss: 0.3195 - val_acc: 0.9048
Epoch 7/40
 - 0s - loss: 0.1278 - acc: 0.9653 - val_loss: 0.3173 - val_acc: 0.9079
Epoch 8/40
 - 0s - loss: 0.1111 - acc: 0.9657 - val_loss: 0.3138 - val_acc: 0.9048
Epoch 9/40
 - 0s - loss: 0.1014 - acc: 0.9671 - val_loss: 0.3167 - val_acc: 0.9079
Epoch 10/40
 - 0s - loss: 0.0919 - acc: 0.9696 - val_loss: 0.3133 - val_acc: 0.9048
Epoch 11/40
 - 0s - loss: 0.0855 - acc: 0.9699 - val_loss: 0.3200 - val_acc: 0.9111
Epoch 12/40
 - 0s - loss: 0.0774 - acc: 0.9763 - val_loss: 0.3173 - val_acc: 0.9111
Epoch 13/40
 - 0s - loss: 0.0678 - acc: 0.9777 - val_loss: 0.3333 - val_acc: 0.9111
Epoch 14/40
 - 0s - loss: 0.0709 - acc: 0.9717 - val_loss: 0.3227 - val_acc: 0.9143
Epoch 15/40
 - 0s - loss: 0.0702 - acc: 0.9728 - val_loss: 0.3258 - val_acc: 0.9238
Epoch 16/40
 - 0s - loss: 0.0631 - acc: 0.9752 - val_loss: 0.3385 - val_acc: 0.9206
Epoch 17/40
 - 0s - loss: 0.0612 - acc: 0.9745 - val_loss: 0.3429 - val_acc: 0.9175
Epoch 18/40
 - 0s - loss: 0.0549 - acc: 0.9781 - val_loss: 0.3476 - val_acc: 0.9206
Epoch 19/40
 - 0s - loss: 0.0554 - acc: 0.9745 - val_loss: 0.3417 - val_acc: 0.9079
Epoch 20/40
 - 0s - loss: 0.0522 - acc: 0.9788 - val_loss: 0.3619 - val_acc: 0.9238
Epoch 21/40
 - 0s - loss: 0.0499 - acc: 0.9791 - val_loss: 0.3602 - val_acc: 0.9238
Epoch 22/40
 - 0s - loss: 0.0521 - acc: 0.9777 - val_loss: 0.3530 - val_acc: 0.9238
Epoch 23/40
 - 0s - loss: 0.0491 - acc: 0.9756 - val_loss: 0.3546 - val_acc: 0.9238
Epoch 24/40
 - 0s - loss: 0.0504 - acc: 0.9759 - val_loss: 0.3696 - val_acc: 0.9238
Epoch 25/40
 - 0s - loss: 0.0462 - acc: 0.9774 - val_loss: 0.3696 - val_acc: 0.9238
Epoch 26/40
 - 0s - loss: 0.0455 - acc: 0.9763 - val_loss: 0.3850 - val_acc: 0.9238
Epoch 27/40
 - 0s - loss: 0.0414 - acc: 0.9798 - val_loss: 0.3795 - val_acc: 0.9175
Epoch 28/40
 - 0s - loss: 0.0418 - acc: 0.9770 - val_loss: 0.3981 - val_acc: 0.9238
Epoch 29/40
 - 0s - loss: 0.0408 - acc: 0.9809 - val_loss: 0.3765 - val_acc: 0.9238
Epoch 30/40
 - 0s - loss: 0.0417 - acc: 0.9766 - val_loss: 0.3900 - val_acc: 0.9206
Epoch 31/40
 - 0s - loss: 0.0391 - acc: 0.9788 - val_loss: 0.3979 - val_acc: 0.9206
Epoch 32/40
 - 0s - loss: 0.0371 - acc: 0.9809 - val_loss: 0.4043 - val_acc: 0.9206
Epoch 33/40
 - 0s - loss: 0.0375 - acc: 0.9781 - val_loss: 0.4014 - val_acc: 0.9270
Epoch 34/40
 - 0s - loss: 0.0383 - acc: 0.9784 - val_loss: 0.4100 - val_acc: 0.9238
Epoch 35/40
 - 0s - loss: 0.0366 - acc: 0.9809 - val_loss: 0.4037 - val_acc: 0.9238
Epoch 36/40
 - 0s - loss: 0.0362 - acc: 0.9823 - val_loss: 0.4127 - val_acc: 0.9238
Epoch 37/40
 - 0s - loss: 0.0319 - acc: 0.9830 - val_loss: 0.4225 - val_acc: 0.9238
Epoch 38/40
 - 0s - loss: 0.0344 - acc: 0.9809 - val_loss: 0.4454 - val_acc: 0.9238
Epoch 39/40
 - 0s - loss: 0.0361 - acc: 0.9802 - val_loss: 0.4283 - val_acc: 0.9238
Epoch 40/40
 - 0s - loss: 0.0332 - acc: 0.9809 - val_loss: 0.4378 - val_acc: 0.9238
Epoch 1/40
 - 0s - loss: 0.4580 - acc: 0.9143
Epoch 2/40
 - 0s - loss: 0.2842 - acc: 0.9175
Epoch 3/40
 - 0s - loss: 0.2112 - acc: 0.9397
Epoch 4/40
 - 0s - loss: 0.1168 - acc: 0.9556
Epoch 5/40
 - 0s - loss: 0.0890 - acc: 0.9714
Epoch 6/40
 - 0s - loss: 0.0756 - acc: 0.9651
Epoch 7/40
 - 0s - loss: 0.0610 - acc: 0.9810
Epoch 8/40
 - 0s - loss: 0.0583 - acc: 0.9651
Epoch 9/40
 - 0s - loss: 0.0628 - acc: 0.9746
Epoch 10/40
 - 0s - loss: 0.0574 - acc: 0.9651
Epoch 11/40
 - 0s - loss: 0.0475 - acc: 0.9810
Epoch 12/40
 - 0s - loss: 0.0486 - acc: 0.9778
Epoch 13/40
 - 0s - loss: 0.0505 - acc: 0.9683
Epoch 14/40
 - 0s - loss: 0.0529 - acc: 0.9683
Epoch 15/40
 - 0s - loss: 0.0410 - acc: 0.9778
Epoch 16/40
 - 0s - loss: 0.0433 - acc: 0.9810
Epoch 17/40
 - 0s - loss: 0.0441 - acc: 0.9778
Epoch 18/40
 - 0s - loss: 0.0429 - acc: 0.9714
Epoch 19/40
 - 0s - loss: 0.0402 - acc: 0.9778
Epoch 20/40
 - 0s - loss: 0.0412 - acc: 0.9810
Epoch 21/40
 - 0s - loss: 0.0368 - acc: 0.9905
Epoch 22/40
 - 0s - loss: 0.0406 - acc: 0.9778
Epoch 23/40
 - 0s - loss: 0.0497 - acc: 0.9714
Epoch 24/40
 - 0s - loss: 0.0357 - acc: 0.9841
Epoch 25/40
 - 0s - loss: 0.0397 - acc: 0.9778
Epoch 26/40
 - 0s - loss: 0.0425 - acc: 0.9746
Epoch 27/40
 - 0s - loss: 0.0346 - acc: 0.9778
Epoch 28/40
 - 0s - loss: 0.0371 - acc: 0.9810
Epoch 29/40
 - 0s - loss: 0.0435 - acc: 0.9746
Epoch 30/40
 - 0s - loss: 0.0345 - acc: 0.9746
Epoch 31/40
 - 0s - loss: 0.0479 - acc: 0.9683
Epoch 32/40
 - 0s - loss: 0.0327 - acc: 0.9841
Epoch 33/40
 - 0s - loss: 0.0372 - acc: 0.9714
Epoch 34/40
 - 0s - loss: 0.0420 - acc: 0.9746
Epoch 35/40
 - 0s - loss: 0.0432 - acc: 0.9746
Epoch 36/40
 - 0s - loss: 0.0372 - acc: 0.9778
Epoch 37/40
 - 0s - loss: 0.0413 - acc: 0.9746
Epoch 38/40
 - 0s - loss: 0.0396 - acc: 0.9746
Epoch 39/40
 - 0s - loss: 0.0337 - acc: 0.9841
Epoch 40/40
 - 0s - loss: 0.0302 - acc: 0.9873
# Training time = 0:00:04.729019
# F-Score(Ordinary) = 0.034, Recall: 0.5, Precision: 0.018
# F-Score(id) = 0.091, Recall: 0.5, Precision: 0.05
********************
Debug enabled
********************
# XP = FR: impSent favorisationCoeff = 1
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 48)        23568       input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 24)        936         input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 192)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 96)           0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 288)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 24)           6936        concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 24)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            200         dropout_7[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
class_weight{0.0: 0.3388349514563107, 1.0: 0.35491525423728815, 2.0: 7.478571428571429, 4.0: 29.083333333333332, 5.0: 26.175, 6.0: 40.26923076923077}
Train on 2826 samples, validate on 315 samples
Epoch 1/40
 - 0s - loss: 1.9753 - acc: 0.5467 - val_loss: 2.5113 - val_acc: 0.8508
Epoch 2/40
 - 0s - loss: 1.7868 - acc: 0.8447 - val_loss: 2.2918 - val_acc: 0.8984
Epoch 3/40
 - 0s - loss: 1.4799 - acc: 0.9268 - val_loss: 1.9284 - val_acc: 0.8984
Epoch 4/40
 - 0s - loss: 1.0805 - acc: 0.9303 - val_loss: 1.4771 - val_acc: 0.8984
Epoch 5/40
 - 0s - loss: 0.7331 - acc: 0.9271 - val_loss: 1.2044 - val_acc: 0.8952
Epoch 6/40
 - 0s - loss: 0.5173 - acc: 0.9253 - val_loss: 1.0199 - val_acc: 0.8921
Epoch 7/40
 - 0s - loss: 0.3472 - acc: 0.9310 - val_loss: 0.8534 - val_acc: 0.8921
Epoch 8/40
 - 0s - loss: 0.2813 - acc: 0.9391 - val_loss: 0.7985 - val_acc: 0.8952
Epoch 9/40
 - 0s - loss: 0.2196 - acc: 0.9483 - val_loss: 0.7715 - val_acc: 0.8952
Epoch 10/40
 - 0s - loss: 0.1781 - acc: 0.9551 - val_loss: 0.7820 - val_acc: 0.8952
Epoch 11/40
 - 0s - loss: 0.1496 - acc: 0.9558 - val_loss: 0.9020 - val_acc: 0.8952
Epoch 12/40
 - 0s - loss: 0.1400 - acc: 0.9561 - val_loss: 0.6827 - val_acc: 0.8952
Epoch 13/40
 - 0s - loss: 0.1076 - acc: 0.9586 - val_loss: 0.7968 - val_acc: 0.8952
Epoch 14/40
 - 0s - loss: 0.1160 - acc: 0.9575 - val_loss: 0.8003 - val_acc: 0.8952
Epoch 15/40
 - 0s - loss: 0.0893 - acc: 0.9597 - val_loss: 0.8211 - val_acc: 0.8921
Epoch 16/40
 - 0s - loss: 0.1095 - acc: 0.9614 - val_loss: 0.7929 - val_acc: 0.8921
Epoch 17/40
 - 0s - loss: 0.0922 - acc: 0.9604 - val_loss: 0.8005 - val_acc: 0.8921
Epoch 18/40
 - 0s - loss: 0.0793 - acc: 0.9625 - val_loss: 0.8768 - val_acc: 0.8952
Epoch 19/40
 - 0s - loss: 0.0878 - acc: 0.9653 - val_loss: 0.8363 - val_acc: 0.8952
Epoch 20/40
 - 0s - loss: 0.0690 - acc: 0.9636 - val_loss: 0.8674 - val_acc: 0.8984
Epoch 21/40
 - 0s - loss: 0.0714 - acc: 0.9653 - val_loss: 0.8990 - val_acc: 0.9016
Epoch 22/40
 - 0s - loss: 0.0710 - acc: 0.9636 - val_loss: 0.8158 - val_acc: 0.9016
Epoch 23/40
 - 0s - loss: 0.0670 - acc: 0.9678 - val_loss: 0.8534 - val_acc: 0.9016
Epoch 24/40
 - 0s - loss: 0.0695 - acc: 0.9678 - val_loss: 0.8494 - val_acc: 0.9016
Epoch 25/40
 - 0s - loss: 0.0604 - acc: 0.9678 - val_loss: 0.9125 - val_acc: 0.9016
Epoch 26/40
 - 0s - loss: 0.0596 - acc: 0.9713 - val_loss: 0.9260 - val_acc: 0.9048
Epoch 27/40
 - 0s - loss: 0.0577 - acc: 0.9720 - val_loss: 0.9729 - val_acc: 0.9048
Epoch 28/40
 - 0s - loss: 0.0575 - acc: 0.9710 - val_loss: 0.9891 - val_acc: 0.9048
Epoch 29/40
 - 0s - loss: 0.0605 - acc: 0.9696 - val_loss: 0.9839 - val_acc: 0.9048
Epoch 30/40
 - 0s - loss: 0.0591 - acc: 0.9717 - val_loss: 1.0224 - val_acc: 0.9048
Epoch 31/40
 - 0s - loss: 0.0517 - acc: 0.9728 - val_loss: 1.0503 - val_acc: 0.9048
Epoch 32/40
 - 0s - loss: 0.0518 - acc: 0.9735 - val_loss: 1.0621 - val_acc: 0.9048
Epoch 33/40
 - 0s - loss: 0.0527 - acc: 0.9731 - val_loss: 1.0967 - val_acc: 0.9048
Epoch 34/40
 - 0s - loss: 0.0593 - acc: 0.9738 - val_loss: 1.0742 - val_acc: 0.9079
Epoch 35/40
 - 0s - loss: 0.0509 - acc: 0.9749 - val_loss: 1.1625 - val_acc: 0.9048
Epoch 36/40
 - 0s - loss: 0.0504 - acc: 0.9752 - val_loss: 1.2145 - val_acc: 0.9079
Epoch 37/40
 - 0s - loss: 0.0456 - acc: 0.9759 - val_loss: 1.2723 - val_acc: 0.9048
Epoch 38/40
 - 0s - loss: 0.0564 - acc: 0.9738 - val_loss: 1.1755 - val_acc: 0.9079
Epoch 39/40
 - 0s - loss: 0.0556 - acc: 0.9738 - val_loss: 1.1757 - val_acc: 0.9175
Epoch 40/40
 - 0s - loss: 0.0537 - acc: 0.9749 - val_loss: 1.0741 - val_acc: 0.9143
Epoch 1/40
 - 0s - loss: 1.2461 - acc: 0.9048
Epoch 2/40
 - 0s - loss: 0.6300 - acc: 0.8921
Epoch 3/40
 - 0s - loss: 0.2775 - acc: 0.8889
Epoch 4/40
 - 0s - loss: 0.1735 - acc: 0.8984
Epoch 5/40
 - 0s - loss: 0.1406 - acc: 0.8952
Epoch 6/40
 - 0s - loss: 0.1345 - acc: 0.9048
Epoch 7/40
 - 0s - loss: 0.1301 - acc: 0.9333
Epoch 8/40
 - 0s - loss: 0.1473 - acc: 0.9365
Epoch 9/40
 - 0s - loss: 0.1004 - acc: 0.9460
Epoch 10/40
 - 0s - loss: 0.0975 - acc: 0.9492
Epoch 11/40
 - 0s - loss: 0.0876 - acc: 0.9492
Epoch 12/40
 - 0s - loss: 0.0964 - acc: 0.9619
Epoch 13/40
 - 0s - loss: 0.0926 - acc: 0.9556
Epoch 14/40
 - 0s - loss: 0.0998 - acc: 0.9587
Epoch 15/40
 - 0s - loss: 0.0727 - acc: 0.9587
Epoch 16/40
 - 0s - loss: 0.0679 - acc: 0.9683
Epoch 17/40
 - 0s - loss: 0.0717 - acc: 0.9619
Epoch 18/40
 - 0s - loss: 0.0854 - acc: 0.9683
Epoch 19/40
 - 0s - loss: 0.0781 - acc: 0.9683
Epoch 20/40
 - 0s - loss: 0.0727 - acc: 0.9714
Epoch 21/40
 - 0s - loss: 0.0738 - acc: 0.9683
Epoch 22/40
 - 0s - loss: 0.1022 - acc: 0.9683
Epoch 23/40
 - 0s - loss: 0.0987 - acc: 0.9714
Epoch 24/40
 - 0s - loss: 0.0565 - acc: 0.9714
Epoch 25/40
 - 0s - loss: 0.0619 - acc: 0.9714
Epoch 26/40
 - 0s - loss: 0.0685 - acc: 0.9714
Epoch 27/40
 - 0s - loss: 0.0645 - acc: 0.9714
Epoch 28/40
 - 0s - loss: 0.0647 - acc: 0.9746
Epoch 29/40
 - 0s - loss: 0.0687 - acc: 0.9714
Epoch 30/40
 - 0s - loss: 0.0555 - acc: 0.9746
Epoch 31/40
 - 0s - loss: 0.1030 - acc: 0.9714
Epoch 32/40
 - 0s - loss: 0.0727 - acc: 0.9714
Epoch 33/40
 - 0s - loss: 0.0540 - acc: 0.9746
Epoch 34/40
 - 0s - loss: 0.0704 - acc: 0.9714
Epoch 35/40
 - 0s - loss: 0.0925 - acc: 0.9651
Epoch 36/40
 - 0s - loss: 0.0543 - acc: 0.9714
Epoch 37/40
 - 0s - loss: 0.0597 - acc: 0.9746
Epoch 38/40
 - 0s - loss: 0.0570 - acc: 0.9714
Epoch 39/40
 - 0s - loss: 0.0595 - acc: 0.9778
Epoch 40/40
 - 0s - loss: 0.0563 - acc: 0.9778
# Training time = 0:00:04.712157
# F-Score(Ordinary) = 0.382, Recall: 0.531, Precision: 0.298
# F-Score(ireflv) = 0.591, Recall: 0.5, Precision: 0.722
# F-Score(id) = 0.308, Recall: 0.667, Precision: 0.2
********************
Debug enabled
********************
# XP = FR: impSent overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 48)        23568       input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 24)        936         input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 192)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 96)           0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 288)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 24)           6936        concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 24)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            200         dropout_8[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 3141
data size after sampling = 9270
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0}
Train on 8343 samples, validate on 927 samples
Epoch 1/40
 - 0s - loss: 1.1803 - acc: 0.7078 - val_loss: 0.4853 - val_acc: 1.0000
Epoch 2/40
 - 0s - loss: 0.1627 - acc: 0.9743 - val_loss: 0.0412 - val_acc: 1.0000
Epoch 3/40
 - 0s - loss: 0.0981 - acc: 0.9806 - val_loss: 0.0227 - val_acc: 1.0000
Epoch 4/40
 - 0s - loss: 0.0809 - acc: 0.9842 - val_loss: 0.0184 - val_acc: 1.0000
Epoch 5/40
 - 0s - loss: 0.0730 - acc: 0.9853 - val_loss: 0.0212 - val_acc: 1.0000
Epoch 6/40
 - 0s - loss: 0.0642 - acc: 0.9871 - val_loss: 0.0204 - val_acc: 1.0000
Epoch 7/40
 - 0s - loss: 0.0591 - acc: 0.9879 - val_loss: 0.0197 - val_acc: 1.0000
Epoch 8/40
 - 0s - loss: 0.0583 - acc: 0.9896 - val_loss: 0.0136 - val_acc: 1.0000
Epoch 9/40
 - 0s - loss: 0.0541 - acc: 0.9892 - val_loss: 0.0223 - val_acc: 1.0000
Epoch 10/40
 - 0s - loss: 0.0521 - acc: 0.9908 - val_loss: 0.0175 - val_acc: 1.0000
Epoch 11/40
 - 0s - loss: 0.0497 - acc: 0.9911 - val_loss: 0.0128 - val_acc: 1.0000
Epoch 12/40
 - 0s - loss: 0.0478 - acc: 0.9903 - val_loss: 0.0197 - val_acc: 1.0000
Epoch 13/40
 - 0s - loss: 0.0461 - acc: 0.9905 - val_loss: 0.0163 - val_acc: 1.0000
Epoch 14/40
 - 0s - loss: 0.0467 - acc: 0.9915 - val_loss: 0.0275 - val_acc: 1.0000
Epoch 15/40
 - 0s - loss: 0.0454 - acc: 0.9914 - val_loss: 0.0163 - val_acc: 1.0000
Epoch 16/40
 - 0s - loss: 0.0451 - acc: 0.9908 - val_loss: 0.0258 - val_acc: 1.0000
Epoch 17/40
 - 0s - loss: 0.0452 - acc: 0.9910 - val_loss: 0.0176 - val_acc: 1.0000
Epoch 18/40
 - 0s - loss: 0.0443 - acc: 0.9910 - val_loss: 0.0186 - val_acc: 1.0000
Epoch 19/40
 - 0s - loss: 0.0435 - acc: 0.9914 - val_loss: 0.0246 - val_acc: 1.0000
Epoch 20/40
 - 0s - loss: 0.0446 - acc: 0.9915 - val_loss: 0.0176 - val_acc: 1.0000
Epoch 21/40
 - 0s - loss: 0.0421 - acc: 0.9917 - val_loss: 0.0312 - val_acc: 1.0000
Epoch 22/40
 - 0s - loss: 0.0408 - acc: 0.9916 - val_loss: 0.0201 - val_acc: 1.0000
Epoch 23/40
 - 0s - loss: 0.0419 - acc: 0.9914 - val_loss: 0.0188 - val_acc: 1.0000
Epoch 24/40
 - 0s - loss: 0.0418 - acc: 0.9916 - val_loss: 0.0137 - val_acc: 1.0000
Epoch 25/40
 - 0s - loss: 0.0422 - acc: 0.9921 - val_loss: 0.0159 - val_acc: 1.0000
Epoch 26/40
 - 0s - loss: 0.0413 - acc: 0.9916 - val_loss: 0.0146 - val_acc: 1.0000
Epoch 27/40
 - 0s - loss: 0.0408 - acc: 0.9921 - val_loss: 0.0178 - val_acc: 1.0000
Epoch 28/40
 - 0s - loss: 0.0418 - acc: 0.9917 - val_loss: 0.0209 - val_acc: 1.0000
Epoch 29/40
 - 0s - loss: 0.0418 - acc: 0.9920 - val_loss: 0.0243 - val_acc: 1.0000
Epoch 30/40
 - 0s - loss: 0.0399 - acc: 0.9923 - val_loss: 0.0154 - val_acc: 1.0000
Epoch 31/40
 - 0s - loss: 0.0397 - acc: 0.9924 - val_loss: 0.0128 - val_acc: 1.0000
Epoch 32/40
 - 0s - loss: 0.0421 - acc: 0.9920 - val_loss: 0.0217 - val_acc: 1.0000
Epoch 33/40
 - 0s - loss: 0.0392 - acc: 0.9923 - val_loss: 0.0248 - val_acc: 1.0000
Epoch 34/40
 - 0s - loss: 0.0397 - acc: 0.9926 - val_loss: 0.0312 - val_acc: 1.0000
Epoch 35/40
 - 0s - loss: 0.0411 - acc: 0.9922 - val_loss: 0.0194 - val_acc: 1.0000
Epoch 36/40
 - 0s - loss: 0.0405 - acc: 0.9921 - val_loss: 0.0319 - val_acc: 1.0000
Epoch 37/40
 - 0s - loss: 0.0403 - acc: 0.9922 - val_loss: 0.0230 - val_acc: 1.0000
Epoch 38/40
 - 0s - loss: 0.0403 - acc: 0.9923 - val_loss: 0.0199 - val_acc: 1.0000
Epoch 39/40
 - 0s - loss: 0.0394 - acc: 0.9918 - val_loss: 0.0220 - val_acc: 1.0000
Epoch 40/40
 - 0s - loss: 0.0398 - acc: 0.9923 - val_loss: 0.0234 - val_acc: 1.0000
Epoch 1/40
 - 0s - loss: 0.0171 - acc: 1.0000
Epoch 2/40
 - 0s - loss: 0.0045 - acc: 1.0000
Epoch 3/40
 - 0s - loss: 0.0015 - acc: 1.0000
Epoch 4/40
 - 0s - loss: 0.0015 - acc: 1.0000
Epoch 5/40
 - 0s - loss: 6.7119e-04 - acc: 1.0000
Epoch 6/40
 - 0s - loss: 8.2675e-04 - acc: 1.0000
Epoch 7/40
 - 0s - loss: 5.7613e-04 - acc: 1.0000
Epoch 8/40
 - 0s - loss: 5.1254e-04 - acc: 1.0000
Epoch 9/40
 - 0s - loss: 5.2686e-04 - acc: 1.0000
Epoch 10/40
 - 0s - loss: 5.1612e-04 - acc: 1.0000
Epoch 11/40
 - 0s - loss: 2.9932e-04 - acc: 1.0000
Epoch 12/40
 - 0s - loss: 3.2047e-04 - acc: 1.0000
Epoch 13/40
 - 0s - loss: 3.9871e-04 - acc: 1.0000
Epoch 14/40
 - 0s - loss: 3.9291e-04 - acc: 1.0000
Epoch 15/40
 - 0s - loss: 2.5998e-04 - acc: 1.0000
Epoch 16/40
 - 0s - loss: 2.6784e-04 - acc: 1.0000
Epoch 17/40
 - 0s - loss: 2.2868e-04 - acc: 1.0000
Epoch 18/40
 - 0s - loss: 1.8889e-04 - acc: 1.0000
Epoch 19/40
 - 0s - loss: 1.5693e-04 - acc: 1.0000
Epoch 20/40
 - 0s - loss: 2.5098e-04 - acc: 1.0000
Epoch 21/40
 - 0s - loss: 1.6638e-04 - acc: 1.0000
Epoch 22/40
 - 0s - loss: 3.7146e-04 - acc: 1.0000
Epoch 23/40
 - 0s - loss: 3.1718e-04 - acc: 1.0000
Epoch 24/40
 - 0s - loss: 1.9253e-04 - acc: 1.0000
Epoch 25/40
 - 0s - loss: 2.3919e-04 - acc: 1.0000
Epoch 26/40
 - 0s - loss: 1.4346e-04 - acc: 1.0000
Epoch 27/40
 - 0s - loss: 2.9076e-04 - acc: 1.0000
Epoch 28/40
 - 0s - loss: 1.4400e-04 - acc: 1.0000
Epoch 29/40
 - 0s - loss: 1.3499e-04 - acc: 1.0000
Epoch 30/40
 - 0s - loss: 1.2142e-04 - acc: 1.0000
Epoch 31/40
 - 0s - loss: 3.3723e-04 - acc: 1.0000
Epoch 32/40
 - 0s - loss: 1.6539e-04 - acc: 1.0000
Epoch 33/40
 - 0s - loss: 1.5932e-04 - acc: 1.0000
Epoch 34/40
 - 0s - loss: 1.3944e-04 - acc: 1.0000
Epoch 35/40
 - 0s - loss: 4.1457e-04 - acc: 1.0000
Epoch 36/40
 - 0s - loss: 9.1887e-05 - acc: 1.0000
Epoch 37/40
 - 0s - loss: 9.6059e-05 - acc: 1.0000
Epoch 38/40
 - 0s - loss: 1.3303e-04 - acc: 1.0000
Epoch 39/40
 - 0s - loss: 1.1267e-04 - acc: 1.0000
Epoch 40/40
 - 0s - loss: 5.8587e-05 - acc: 1.0000
# Training time = 0:00:10.295485
# F-Score(Ordinary) = 0.278, Recall: 0.667, Precision: 0.175
# F-Score(lvc) = 0.167, Recall: 0.4, Precision: 0.105
# F-Score(ireflv) = 0.364, Recall: 1.0, Precision: 0.222
# F-Score(id) = 0.308, Recall: 0.667, Precision: 0.2
********************
Debug enabled
********************
# XP = FR: impSent overSampling favorisationCoeff = 1
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 48)        23568       input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 24)        936         input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 192)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 96)           0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 288)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 24)           6936        concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 24)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            200         dropout_9[0][0]                  
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 3141
data size after sampling = 9270
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 1.0, 4.0: 1.0, 5.0: 1.0, 6.0: 1.0}
Train on 8343 samples, validate on 927 samples
Epoch 1/40
 - 0s - loss: 1.1803 - acc: 0.7078 - val_loss: 0.4853 - val_acc: 1.0000
Epoch 2/40
 - 0s - loss: 0.1627 - acc: 0.9743 - val_loss: 0.0412 - val_acc: 1.0000
Epoch 3/40
 - 0s - loss: 0.0981 - acc: 0.9806 - val_loss: 0.0227 - val_acc: 1.0000
Epoch 4/40
 - 0s - loss: 0.0809 - acc: 0.9842 - val_loss: 0.0184 - val_acc: 1.0000
Epoch 5/40
 - 0s - loss: 0.0730 - acc: 0.9853 - val_loss: 0.0212 - val_acc: 1.0000
Epoch 6/40
 - 0s - loss: 0.0642 - acc: 0.9871 - val_loss: 0.0204 - val_acc: 1.0000
Epoch 7/40
 - 0s - loss: 0.0591 - acc: 0.9879 - val_loss: 0.0197 - val_acc: 1.0000
Epoch 8/40
 - 0s - loss: 0.0583 - acc: 0.9896 - val_loss: 0.0137 - val_acc: 1.0000
Epoch 9/40
 - 0s - loss: 0.0541 - acc: 0.9892 - val_loss: 0.0223 - val_acc: 1.0000
Epoch 10/40
 - 0s - loss: 0.0521 - acc: 0.9908 - val_loss: 0.0175 - val_acc: 1.0000
Epoch 11/40
 - 0s - loss: 0.0496 - acc: 0.9910 - val_loss: 0.0129 - val_acc: 1.0000
Epoch 12/40
 - 0s - loss: 0.0478 - acc: 0.9904 - val_loss: 0.0197 - val_acc: 1.0000
Epoch 13/40
 - 0s - loss: 0.0461 - acc: 0.9905 - val_loss: 0.0161 - val_acc: 1.0000
Epoch 14/40
 - 0s - loss: 0.0467 - acc: 0.9914 - val_loss: 0.0306 - val_acc: 1.0000
Epoch 15/40
 - 0s - loss: 0.0454 - acc: 0.9914 - val_loss: 0.0153 - val_acc: 1.0000
Epoch 16/40
 - 0s - loss: 0.0450 - acc: 0.9908 - val_loss: 0.0260 - val_acc: 1.0000
Epoch 17/40
 - 0s - loss: 0.0452 - acc: 0.9911 - val_loss: 0.0167 - val_acc: 1.0000
Epoch 18/40
 - 0s - loss: 0.0444 - acc: 0.9910 - val_loss: 0.0186 - val_acc: 1.0000
Epoch 19/40
 - 0s - loss: 0.0436 - acc: 0.9914 - val_loss: 0.0241 - val_acc: 1.0000
Epoch 20/40
 - 0s - loss: 0.0447 - acc: 0.9915 - val_loss: 0.0176 - val_acc: 1.0000
Epoch 21/40
 - 0s - loss: 0.0421 - acc: 0.9917 - val_loss: 0.0314 - val_acc: 1.0000
Epoch 22/40
 - 0s - loss: 0.0408 - acc: 0.9916 - val_loss: 0.0200 - val_acc: 1.0000
Epoch 23/40
 - 0s - loss: 0.0419 - acc: 0.9914 - val_loss: 0.0211 - val_acc: 1.0000
Epoch 24/40
 - 0s - loss: 0.0416 - acc: 0.9917 - val_loss: 0.0147 - val_acc: 1.0000
Epoch 25/40
 - 0s - loss: 0.0421 - acc: 0.9920 - val_loss: 0.0158 - val_acc: 1.0000
Epoch 26/40
 - 0s - loss: 0.0413 - acc: 0.9916 - val_loss: 0.0146 - val_acc: 1.0000
Epoch 27/40
 - 0s - loss: 0.0408 - acc: 0.9922 - val_loss: 0.0175 - val_acc: 1.0000
Epoch 28/40
 - 0s - loss: 0.0419 - acc: 0.9918 - val_loss: 0.0235 - val_acc: 1.0000
Epoch 29/40
 - 0s - loss: 0.0419 - acc: 0.9922 - val_loss: 0.0250 - val_acc: 1.0000
Epoch 30/40
 - 0s - loss: 0.0401 - acc: 0.9921 - val_loss: 0.0162 - val_acc: 1.0000
Epoch 31/40
 - 0s - loss: 0.0400 - acc: 0.9924 - val_loss: 0.0125 - val_acc: 1.0000
Epoch 32/40
 - 0s - loss: 0.0419 - acc: 0.9921 - val_loss: 0.0225 - val_acc: 1.0000
Epoch 33/40
 - 0s - loss: 0.0392 - acc: 0.9923 - val_loss: 0.0244 - val_acc: 1.0000
Epoch 34/40
 - 0s - loss: 0.0398 - acc: 0.9924 - val_loss: 0.0276 - val_acc: 1.0000
Epoch 35/40
 - 0s - loss: 0.0410 - acc: 0.9921 - val_loss: 0.0204 - val_acc: 1.0000
Epoch 36/40
 - 0s - loss: 0.0404 - acc: 0.9922 - val_loss: 0.0335 - val_acc: 1.0000
Epoch 37/40
 - 0s - loss: 0.0403 - acc: 0.9922 - val_loss: 0.0238 - val_acc: 1.0000
Epoch 38/40
 - 0s - loss: 0.0402 - acc: 0.9924 - val_loss: 0.0194 - val_acc: 1.0000
Epoch 39/40
 - 0s - loss: 0.0396 - acc: 0.9918 - val_loss: 0.0217 - val_acc: 1.0000
Epoch 40/40
 - 0s - loss: 0.0401 - acc: 0.9922 - val_loss: 0.0234 - val_acc: 1.0000
Epoch 1/40
 - 0s - loss: 0.0169 - acc: 1.0000
Epoch 2/40
 - 0s - loss: 0.0044 - acc: 1.0000
Epoch 3/40
 - 0s - loss: 0.0015 - acc: 1.0000
Epoch 4/40
 - 0s - loss: 0.0015 - acc: 1.0000
Epoch 5/40
 - 0s - loss: 6.5242e-04 - acc: 1.0000
Epoch 6/40
 - 0s - loss: 8.1153e-04 - acc: 1.0000
Epoch 7/40
 - 0s - loss: 5.8017e-04 - acc: 1.0000
Epoch 8/40
 - 0s - loss: 5.0064e-04 - acc: 1.0000
Epoch 9/40
 - 0s - loss: 5.2696e-04 - acc: 1.0000
Epoch 10/40
 - 0s - loss: 5.0778e-04 - acc: 1.0000
Epoch 11/40
 - 0s - loss: 2.9839e-04 - acc: 1.0000
Epoch 12/40
 - 0s - loss: 3.1230e-04 - acc: 1.0000
Epoch 13/40
 - 0s - loss: 4.0386e-04 - acc: 1.0000
Epoch 14/40
 - 0s - loss: 3.9511e-04 - acc: 1.0000
Epoch 15/40
 - 0s - loss: 2.5707e-04 - acc: 1.0000
Epoch 16/40
 - 0s - loss: 2.6738e-04 - acc: 1.0000
Epoch 17/40
 - 0s - loss: 2.2273e-04 - acc: 1.0000
Epoch 18/40
 - 0s - loss: 1.8462e-04 - acc: 1.0000
Epoch 19/40
 - 0s - loss: 1.5563e-04 - acc: 1.0000
Epoch 20/40
 - 0s - loss: 2.5080e-04 - acc: 1.0000
Epoch 21/40
 - 0s - loss: 1.6411e-04 - acc: 1.0000
Epoch 22/40
 - 0s - loss: 3.6895e-04 - acc: 1.0000
Epoch 23/40
 - 0s - loss: 3.2109e-04 - acc: 1.0000
Epoch 24/40
 - 0s - loss: 1.9490e-04 - acc: 1.0000
Epoch 25/40
 - 0s - loss: 2.4453e-04 - acc: 1.0000
Epoch 26/40
 - 0s - loss: 1.4218e-04 - acc: 1.0000
Epoch 27/40
 - 0s - loss: 2.7806e-04 - acc: 1.0000
Epoch 28/40
 - 0s - loss: 1.4724e-04 - acc: 1.0000
Epoch 29/40
 - 0s - loss: 1.3722e-04 - acc: 1.0000
Epoch 30/40
 - 0s - loss: 1.1907e-04 - acc: 1.0000
Epoch 31/40
 - 0s - loss: 3.3002e-04 - acc: 1.0000
Epoch 32/40
 - 0s - loss: 1.5766e-04 - acc: 1.0000
Epoch 33/40
 - 0s - loss: 1.5971e-04 - acc: 1.0000
Epoch 34/40
 - 0s - loss: 1.3850e-04 - acc: 1.0000
Epoch 35/40
 - 0s - loss: 4.1854e-04 - acc: 1.0000
Epoch 36/40
 - 0s - loss: 8.9246e-05 - acc: 1.0000
Epoch 37/40
 - 0s - loss: 9.3669e-05 - acc: 1.0000
Epoch 38/40
 - 0s - loss: 1.3482e-04 - acc: 1.0000
Epoch 39/40
 - 0s - loss: 1.0976e-04 - acc: 1.0000
Epoch 40/40
 - 0s - loss: 5.8165e-05 - acc: 1.0000
# Training time = 0:00:10.396479
# F-Score(Ordinary) = 0.27, Recall: 0.588, Precision: 0.175
# F-Score(lvc) = 0.154, Recall: 0.286, Precision: 0.105
# F-Score(ireflv) = 0.364, Recall: 1.0, Precision: 0.222
# F-Score(id) = 0.308, Recall: 0.667, Precision: 0.2
********************
Debug enabled
********************
# XP = FR: overSampling 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 48)        23568       input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 24)        936         input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 192)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 96)           0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 288)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 24)           6936        concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 24)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            200         dropout_10[0][0]                 
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 10323
data size after sampling = 30816
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 0.0, 4.0: 0.0, 5.0: 0.0, 6.0: 0.0}
Train on 27734 samples, validate on 3082 samples
Epoch 1/40
 - 1s - loss: 0.4026 - acc: 0.9118 - val_loss: 0.0084 - val_acc: 1.0000
Epoch 2/40
 - 1s - loss: 0.0381 - acc: 0.9936 - val_loss: 0.0048 - val_acc: 1.0000
Epoch 3/40
 - 1s - loss: 0.0281 - acc: 0.9946 - val_loss: 0.0022 - val_acc: 1.0000
Epoch 4/40
 - 1s - loss: 0.0249 - acc: 0.9956 - val_loss: 0.0023 - val_acc: 1.0000
Epoch 5/40
 - 1s - loss: 0.0218 - acc: 0.9963 - val_loss: 0.0020 - val_acc: 1.0000
Epoch 6/40
 - 1s - loss: 0.0203 - acc: 0.9965 - val_loss: 0.0035 - val_acc: 1.0000
Epoch 7/40
 - 1s - loss: 0.0183 - acc: 0.9969 - val_loss: 0.0024 - val_acc: 1.0000
Epoch 8/40
 - 1s - loss: 0.0176 - acc: 0.9972 - val_loss: 0.0021 - val_acc: 1.0000
Epoch 9/40
 - 1s - loss: 0.0173 - acc: 0.9971 - val_loss: 0.0040 - val_acc: 1.0000
Epoch 10/40
 - 1s - loss: 0.0171 - acc: 0.9972 - val_loss: 0.0023 - val_acc: 1.0000
Epoch 11/40
 - 1s - loss: 0.0173 - acc: 0.9972 - val_loss: 0.0043 - val_acc: 1.0000
Epoch 12/40
 - 1s - loss: 0.0164 - acc: 0.9972 - val_loss: 0.0045 - val_acc: 1.0000
Epoch 13/40
 - 1s - loss: 0.0159 - acc: 0.9973 - val_loss: 0.0019 - val_acc: 1.0000
Epoch 14/40
 - 1s - loss: 0.0167 - acc: 0.9973 - val_loss: 0.0045 - val_acc: 1.0000
Epoch 15/40
 - 1s - loss: 0.0157 - acc: 0.9972 - val_loss: 0.0029 - val_acc: 1.0000
Epoch 16/40
 - 1s - loss: 0.0153 - acc: 0.9975 - val_loss: 0.0030 - val_acc: 1.0000
Epoch 17/40
 - 1s - loss: 0.0160 - acc: 0.9973 - val_loss: 0.0050 - val_acc: 1.0000
Epoch 18/40
 - 1s - loss: 0.0154 - acc: 0.9974 - val_loss: 0.0076 - val_acc: 1.0000
Epoch 19/40
 - 1s - loss: 0.0154 - acc: 0.9974 - val_loss: 0.0047 - val_acc: 1.0000
Epoch 20/40
 - 1s - loss: 0.0148 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000
Epoch 21/40
 - 1s - loss: 0.0153 - acc: 0.9975 - val_loss: 0.0092 - val_acc: 1.0000
Epoch 22/40
 - 1s - loss: 0.0150 - acc: 0.9977 - val_loss: 0.0050 - val_acc: 1.0000
Epoch 23/40
 - 1s - loss: 0.0148 - acc: 0.9977 - val_loss: 0.0041 - val_acc: 1.0000
Epoch 24/40
 - 1s - loss: 0.0148 - acc: 0.9977 - val_loss: 0.0103 - val_acc: 1.0000
Epoch 25/40
 - 1s - loss: 0.0151 - acc: 0.9977 - val_loss: 0.0043 - val_acc: 1.0000
Epoch 26/40
 - 1s - loss: 0.0156 - acc: 0.9975 - val_loss: 0.0035 - val_acc: 1.0000
Epoch 27/40
 - 1s - loss: 0.0147 - acc: 0.9975 - val_loss: 0.0079 - val_acc: 1.0000
Epoch 28/40
 - 1s - loss: 0.0146 - acc: 0.9976 - val_loss: 0.0041 - val_acc: 1.0000
Epoch 29/40
 - 1s - loss: 0.0150 - acc: 0.9976 - val_loss: 0.0033 - val_acc: 1.0000
Epoch 30/40
 - 1s - loss: 0.0150 - acc: 0.9977 - val_loss: 0.0059 - val_acc: 1.0000
Epoch 31/40
 - 1s - loss: 0.0148 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 1.0000
Epoch 32/40
 - 1s - loss: 0.0144 - acc: 0.9977 - val_loss: 0.0027 - val_acc: 1.0000
Epoch 33/40
 - 1s - loss: 0.0147 - acc: 0.9978 - val_loss: 0.0048 - val_acc: 1.0000
Epoch 34/40
 - 1s - loss: 0.0144 - acc: 0.9978 - val_loss: 0.0065 - val_acc: 1.0000
Epoch 35/40
 - 1s - loss: 0.0144 - acc: 0.9976 - val_loss: 0.0070 - val_acc: 1.0000
Epoch 36/40
 - 1s - loss: 0.0145 - acc: 0.9976 - val_loss: 0.0050 - val_acc: 1.0000
Epoch 37/40
 - 1s - loss: 0.0144 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000
Epoch 38/40
 - 1s - loss: 0.0145 - acc: 0.9977 - val_loss: 0.0059 - val_acc: 1.0000
Epoch 39/40
 - 1s - loss: 0.0142 - acc: 0.9978 - val_loss: 0.0049 - val_acc: 1.0000
Epoch 40/40
 - 1s - loss: 0.0141 - acc: 0.9976 - val_loss: 0.0078 - val_acc: 1.0000
Epoch 1/40
 - 0s - loss: 0.0030 - acc: 1.0000
Epoch 2/40
 - 0s - loss: 6.9918e-04 - acc: 1.0000
Epoch 3/40
 - 0s - loss: 1.9454e-04 - acc: 1.0000
Epoch 4/40
 - 0s - loss: 2.9810e-04 - acc: 1.0000
Epoch 5/40
 - 0s - loss: 8.2356e-04 - acc: 0.9997
Epoch 6/40
 - 0s - loss: 1.9396e-04 - acc: 1.0000
Epoch 7/40
 - 0s - loss: 1.4204e-04 - acc: 1.0000
Epoch 8/40
 - 0s - loss: 2.4147e-04 - acc: 1.0000
Epoch 9/40
 - 0s - loss: 1.5453e-04 - acc: 1.0000
Epoch 10/40
 - 0s - loss: 1.3030e-04 - acc: 1.0000
Epoch 11/40
 - 0s - loss: 5.6426e-04 - acc: 0.9997
Epoch 12/40
 - 0s - loss: 8.0978e-05 - acc: 1.0000
Epoch 13/40
 - 0s - loss: 5.8628e-05 - acc: 1.0000
Epoch 14/40
 - 0s - loss: 5.8911e-04 - acc: 0.9997
Epoch 15/40
 - 0s - loss: 3.4642e-04 - acc: 0.9997
Epoch 16/40
 - 0s - loss: 3.9021e-05 - acc: 1.0000
Epoch 17/40
 - 0s - loss: 1.1079e-04 - acc: 1.0000
Epoch 18/40
 - 0s - loss: 4.1725e-05 - acc: 1.0000
Epoch 19/40
 - 0s - loss: 2.1701e-05 - acc: 1.0000
Epoch 20/40
 - 0s - loss: 2.9043e-05 - acc: 1.0000
Epoch 21/40
 - 0s - loss: 7.6216e-05 - acc: 1.0000
Epoch 22/40
 - 0s - loss: 8.8568e-05 - acc: 1.0000
Epoch 23/40
 - 0s - loss: 5.7259e-05 - acc: 1.0000
Epoch 24/40
 - 0s - loss: 4.3853e-04 - acc: 0.9997
Epoch 25/40
 - 0s - loss: 2.7280e-05 - acc: 1.0000
Epoch 26/40
 - 0s - loss: 6.0686e-05 - acc: 1.0000
Epoch 27/40
 - 0s - loss: 6.7106e-05 - acc: 1.0000
Epoch 28/40
 - 0s - loss: 3.2735e-05 - acc: 1.0000
Epoch 29/40
 - 0s - loss: 1.1422e-05 - acc: 1.0000
Epoch 30/40
 - 0s - loss: 1.9735e-04 - acc: 1.0000
Epoch 31/40
 - 0s - loss: 3.0176e-05 - acc: 1.0000
Epoch 32/40
 - 0s - loss: 3.1373e-05 - acc: 1.0000
Epoch 33/40
 - 0s - loss: 1.1443e-05 - acc: 1.0000
Epoch 34/40
 - 0s - loss: 3.6225e-05 - acc: 1.0000
Epoch 35/40
 - 0s - loss: 4.2540e-05 - acc: 1.0000
Epoch 36/40
 - 0s - loss: 3.4188e-05 - acc: 1.0000
Epoch 37/40
 - 0s - loss: 1.4964e-05 - acc: 1.0000
Epoch 38/40
 - 0s - loss: 1.9620e-04 - acc: 1.0000
Epoch 39/40
 - 0s - loss: 1.7653e-05 - acc: 1.0000
Epoch 40/40
 - 0s - loss: 1.3579e-05 - acc: 1.0000
# Training time = 0:00:30.368436
# F-Score(Ordinary) = 0.159, Recall: 0.833, Precision: 0.088
# F-Score(lvc) = 0.095, Recall: 0.5, Precision: 0.053
# F-Score(ireflv) = 0.105, Recall: 1.0, Precision: 0.056
# F-Score(id) = 0.174, Recall: 0.667, Precision: 0.1
********************
Debug enabled
********************
# XP = FR: overSampling favorisationCoeff = 1
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 200
# Test = 200
# Tokens vocabulary = 491
# POSs vocabulary = 39
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 491
# POS vocabulary = 39
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 31640
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 48)        23568       input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 24)        936         input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 192)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 96)           0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 288)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 24)           6936        concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 24)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            200         dropout_11[0][0]                 
==================================================================================================
Total params: 31,640
Trainable params: 31,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 10323
data size after sampling = 30816
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 1.0, 4.0: 1.0, 5.0: 1.0, 6.0: 1.0}
Train on 27734 samples, validate on 3082 samples
Epoch 1/40
 - 1s - loss: 0.4026 - acc: 0.9118 - val_loss: 0.0084 - val_acc: 1.0000
Epoch 2/40
 - 1s - loss: 0.0381 - acc: 0.9936 - val_loss: 0.0048 - val_acc: 1.0000
Epoch 3/40
 - 1s - loss: 0.0281 - acc: 0.9946 - val_loss: 0.0022 - val_acc: 1.0000
Epoch 4/40
 - 1s - loss: 0.0249 - acc: 0.9956 - val_loss: 0.0023 - val_acc: 1.0000
Epoch 5/40
 - 1s - loss: 0.0218 - acc: 0.9963 - val_loss: 0.0020 - val_acc: 1.0000
Epoch 6/40
 - 1s - loss: 0.0203 - acc: 0.9965 - val_loss: 0.0035 - val_acc: 1.0000
Epoch 7/40
 - 1s - loss: 0.0183 - acc: 0.9969 - val_loss: 0.0024 - val_acc: 1.0000
Epoch 8/40
 - 1s - loss: 0.0176 - acc: 0.9972 - val_loss: 0.0021 - val_acc: 1.0000
Epoch 9/40
 - 1s - loss: 0.0173 - acc: 0.9971 - val_loss: 0.0040 - val_acc: 1.0000
Epoch 10/40
 - 1s - loss: 0.0171 - acc: 0.9972 - val_loss: 0.0023 - val_acc: 1.0000
Epoch 11/40
 - 1s - loss: 0.0173 - acc: 0.9972 - val_loss: 0.0043 - val_acc: 1.0000
Epoch 12/40
 - 1s - loss: 0.0164 - acc: 0.9972 - val_loss: 0.0045 - val_acc: 1.0000
Epoch 13/40
 - 1s - loss: 0.0159 - acc: 0.9973 - val_loss: 0.0019 - val_acc: 1.0000
Epoch 14/40
 - 1s - loss: 0.0167 - acc: 0.9973 - val_loss: 0.0045 - val_acc: 1.0000
Epoch 15/40
 - 1s - loss: 0.0157 - acc: 0.9972 - val_loss: 0.0029 - val_acc: 1.0000
Epoch 16/40
 - 1s - loss: 0.0153 - acc: 0.9975 - val_loss: 0.0030 - val_acc: 1.0000
Epoch 17/40
 - 1s - loss: 0.0160 - acc: 0.9973 - val_loss: 0.0050 - val_acc: 1.0000
Epoch 18/40
 - 1s - loss: 0.0154 - acc: 0.9974 - val_loss: 0.0076 - val_acc: 1.0000
Epoch 19/40
 - 1s - loss: 0.0154 - acc: 0.9974 - val_loss: 0.0047 - val_acc: 1.0000
Epoch 20/40
 - 1s - loss: 0.0148 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000
Epoch 21/40
 - 1s - loss: 0.0153 - acc: 0.9975 - val_loss: 0.0092 - val_acc: 1.0000
Epoch 22/40
 - 1s - loss: 0.0150 - acc: 0.9977 - val_loss: 0.0050 - val_acc: 1.0000
Epoch 23/40
 - 1s - loss: 0.0148 - acc: 0.9977 - val_loss: 0.0041 - val_acc: 1.0000
Epoch 24/40
 - 1s - loss: 0.0148 - acc: 0.9977 - val_loss: 0.0103 - val_acc: 1.0000
Epoch 25/40
 - 1s - loss: 0.0151 - acc: 0.9977 - val_loss: 0.0043 - val_acc: 1.0000
Epoch 26/40
 - 1s - loss: 0.0156 - acc: 0.9975 - val_loss: 0.0035 - val_acc: 1.0000
Epoch 27/40
 - 1s - loss: 0.0147 - acc: 0.9975 - val_loss: 0.0079 - val_acc: 1.0000
Epoch 28/40
 - 1s - loss: 0.0146 - acc: 0.9976 - val_loss: 0.0041 - val_acc: 1.0000
Epoch 29/40
 - 1s - loss: 0.0150 - acc: 0.9976 - val_loss: 0.0033 - val_acc: 1.0000
Epoch 30/40
 - 1s - loss: 0.0150 - acc: 0.9977 - val_loss: 0.0059 - val_acc: 1.0000
Epoch 31/40
 - 1s - loss: 0.0148 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 1.0000
Epoch 32/40
 - 1s - loss: 0.0144 - acc: 0.9977 - val_loss: 0.0027 - val_acc: 1.0000
Epoch 33/40
 - 1s - loss: 0.0147 - acc: 0.9978 - val_loss: 0.0048 - val_acc: 1.0000
Epoch 34/40
 - 1s - loss: 0.0144 - acc: 0.9978 - val_loss: 0.0065 - val_acc: 1.0000
Epoch 35/40
 - 1s - loss: 0.0144 - acc: 0.9976 - val_loss: 0.0070 - val_acc: 1.0000
Epoch 36/40
 - 1s - loss: 0.0145 - acc: 0.9976 - val_loss: 0.0050 - val_acc: 1.0000
Epoch 37/40
 - 1s - loss: 0.0144 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000
Epoch 38/40
 - 1s - loss: 0.0145 - acc: 0.9977 - val_loss: 0.0059 - val_acc: 1.0000
Epoch 39/40
 - 1s - loss: 0.0142 - acc: 0.9978 - val_loss: 0.0049 - val_acc: 1.0000
Epoch 40/40
 - 1s - loss: 0.0141 - acc: 0.9976 - val_loss: 0.0078 - val_acc: 1.0000
Epoch 1/40
 - 0s - loss: 0.0030 - acc: 1.0000
Epoch 2/40
 - 0s - loss: 6.9918e-04 - acc: 1.0000
Epoch 3/40
 - 0s - loss: 1.9454e-04 - acc: 1.0000
Epoch 4/40
 - 0s - loss: 2.9810e-04 - acc: 1.0000
Epoch 5/40
 - 0s - loss: 8.2356e-04 - acc: 0.9997
Epoch 6/40
 - 0s - loss: 1.9396e-04 - acc: 1.0000
Epoch 7/40
 - 0s - loss: 1.4204e-04 - acc: 1.0000
Epoch 8/40
 - 0s - loss: 2.4147e-04 - acc: 1.0000
Epoch 9/40
 - 0s - loss: 1.5453e-04 - acc: 1.0000
Epoch 10/40
 - 0s - loss: 1.3030e-04 - acc: 1.0000
Epoch 11/40
 - 0s - loss: 5.6426e-04 - acc: 0.9997
Epoch 12/40
 - 0s - loss: 8.0978e-05 - acc: 1.0000
Epoch 13/40
 - 0s - loss: 5.8628e-05 - acc: 1.0000
Epoch 14/40
 - 0s - loss: 5.8911e-04 - acc: 0.9997
Epoch 15/40
 - 0s - loss: 3.4642e-04 - acc: 0.9997
Epoch 16/40
 - 0s - loss: 3.9021e-05 - acc: 1.0000
Epoch 17/40
 - 0s - loss: 1.1079e-04 - acc: 1.0000
Epoch 18/40
 - 0s - loss: 4.1725e-05 - acc: 1.0000
Epoch 19/40
 - 0s - loss: 2.1701e-05 - acc: 1.0000
Epoch 20/40
 - 0s - loss: 2.9043e-05 - acc: 1.0000
Epoch 21/40
 - 0s - loss: 7.6216e-05 - acc: 1.0000
Epoch 22/40
 - 0s - loss: 8.8568e-05 - acc: 1.0000
Epoch 23/40
 - 0s - loss: 5.7259e-05 - acc: 1.0000
Epoch 24/40
 - 0s - loss: 4.3853e-04 - acc: 0.9997
Epoch 25/40
 - 0s - loss: 2.7280e-05 - acc: 1.0000
Epoch 26/40
 - 0s - loss: 6.0686e-05 - acc: 1.0000
Epoch 27/40
 - 0s - loss: 6.7106e-05 - acc: 1.0000
Epoch 28/40
 - 0s - loss: 3.2735e-05 - acc: 1.0000
Epoch 29/40
 - 0s - loss: 1.1422e-05 - acc: 1.0000
Epoch 30/40
 - 0s - loss: 1.9735e-04 - acc: 1.0000
Epoch 31/40
 - 0s - loss: 3.0176e-05 - acc: 1.0000
Epoch 32/40
 - 0s - loss: 3.1373e-05 - acc: 1.0000
Epoch 33/40
 - 0s - loss: 1.1443e-05 - acc: 1.0000
Epoch 34/40
 - 0s - loss: 3.6225e-05 - acc: 1.0000
Epoch 35/40
 - 0s - loss: 4.2540e-05 - acc: 1.0000
Epoch 36/40
 - 0s - loss: 3.4188e-05 - acc: 1.0000
Epoch 37/40
 - 0s - loss: 1.4964e-05 - acc: 1.0000
Epoch 38/40
 - 0s - loss: 1.9620e-04 - acc: 1.0000
Epoch 39/40
 - 0s - loss: 1.7653e-05 - acc: 1.0000
Epoch 40/40
 - 0s - loss: 1.3579e-05 - acc: 1.0000
# Training time = 0:00:30.349398
# F-Score(Ordinary) = 0.159, Recall: 0.833, Precision: 0.088
# F-Score(lvc) = 0.095, Recall: 0.5, Precision: 0.053
# F-Score(ireflv) = 0.105, Recall: 1.0, Precision: 0.056
# F-Score(id) = 0.174, Recall: 0.667, Precision: 0.1
********************
