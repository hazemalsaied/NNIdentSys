INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 200)       2938600     input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 56)        13160       input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 800)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 224)          0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 1024)         0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 256)          262400      concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            2056        dropout_1[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 88s - loss: 0.0188 - acc: 0.9964 - val_loss: 0.0140 - val_acc: 0.9976
Epoch 2/30
 - 88s - loss: 0.0159 - acc: 0.9974 - val_loss: 0.0167 - val_acc: 0.9974
Epoch 3/30
 - 88s - loss: 0.0163 - acc: 0.9974 - val_loss: 0.0161 - val_acc: 0.9975
Epoch 4/30
 - 88s - loss: 0.0171 - acc: 0.9974 - val_loss: 0.0150 - val_acc: 0.9975
Epoch 5/30
 - 88s - loss: 0.0170 - acc: 0.9974 - val_loss: 0.0175 - val_acc: 0.9975
Epoch 6/30
 - 88s - loss: 0.0168 - acc: 0.9975 - val_loss: 0.0206 - val_acc: 0.9975
Epoch 7/30
 - 88s - loss: 0.0188 - acc: 0.9974 - val_loss: 0.0196 - val_acc: 0.9974
Epoch 8/30
 - 88s - loss: 0.0229 - acc: 0.9975 - val_loss: 0.0250 - val_acc: 0.9974
Epoch 9/30
 - 88s - loss: 0.0241 - acc: 0.9976 - val_loss: 0.0253 - val_acc: 0.9974
Epoch 10/30
 - 88s - loss: 0.0248 - acc: 0.9976 - val_loss: 0.0254 - val_acc: 0.9975
Epoch 11/30
 - 88s - loss: 0.0251 - acc: 0.9976 - val_loss: 0.0265 - val_acc: 0.9973
Epoch 12/30
 - 88s - loss: 0.0273 - acc: 0.9976 - val_loss: 0.0297 - val_acc: 0.9975
Epoch 13/30
 - 88s - loss: 0.0289 - acc: 0.9976 - val_loss: 0.0299 - val_acc: 0.9975
Epoch 14/30
 - 88s - loss: 0.0290 - acc: 0.9977 - val_loss: 0.0293 - val_acc: 0.9975
Epoch 15/30
 - 88s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0305 - val_acc: 0.9975
Epoch 16/30
 - 88s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0297 - val_acc: 0.9974
Epoch 17/30
 - 88s - loss: 0.0286 - acc: 0.9977 - val_loss: 0.0301 - val_acc: 0.9975
Epoch 18/30
 - 88s - loss: 0.0289 - acc: 0.9977 - val_loss: 0.0296 - val_acc: 0.9976
Epoch 19/30
 - 88s - loss: 0.0286 - acc: 0.9978 - val_loss: 0.0307 - val_acc: 0.9974
Epoch 20/30
 - 88s - loss: 0.0289 - acc: 0.9978 - val_loss: 0.0305 - val_acc: 0.9974
Epoch 21/30
 - 88s - loss: 0.0297 - acc: 0.9977 - val_loss: 0.0306 - val_acc: 0.9975
Epoch 22/30
 - 88s - loss: 0.0301 - acc: 0.9977 - val_loss: 0.0322 - val_acc: 0.9975
Epoch 23/30
 - 88s - loss: 0.0292 - acc: 0.9977 - val_loss: 0.0307 - val_acc: 0.9975
Epoch 24/30
 - 88s - loss: 0.0296 - acc: 0.9977 - val_loss: 0.0312 - val_acc: 0.9975
Epoch 25/30
 - 88s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.0303 - val_acc: 0.9975
Epoch 26/30
 - 88s - loss: 0.0294 - acc: 0.9977 - val_loss: 0.0332 - val_acc: 0.9972
Epoch 27/30
 - 88s - loss: 0.0293 - acc: 0.9978 - val_loss: 0.0305 - val_acc: 0.9975
Epoch 28/30
 - 88s - loss: 0.0296 - acc: 0.9977 - val_loss: 0.0315 - val_acc: 0.9975
Epoch 29/30
 - 88s - loss: 0.0290 - acc: 0.9978 - val_loss: 0.0312 - val_acc: 0.9975
Epoch 30/30
 - 88s - loss: 0.0293 - acc: 0.9978 - val_loss: 0.0312 - val_acc: 0.9975
Epoch 1/30
 - 9s - loss: 0.0316 - acc: 0.9973
Epoch 2/30
 - 9s - loss: 0.0315 - acc: 0.9974
Epoch 3/30
 - 9s - loss: 0.0296 - acc: 0.9976
Epoch 4/30
 - 9s - loss: 0.0293 - acc: 0.9977
Epoch 5/30
 - 9s - loss: 0.0303 - acc: 0.9976
Epoch 6/30
 - 9s - loss: 0.0294 - acc: 0.9977
Epoch 7/30
 - 9s - loss: 0.0297 - acc: 0.9977
Epoch 8/30
 - 9s - loss: 0.0293 - acc: 0.9977
Epoch 9/30
 - 9s - loss: 0.0291 - acc: 0.9977
Epoch 10/30
 - 9s - loss: 0.0286 - acc: 0.9978
Epoch 11/30
 - 9s - loss: 0.0290 - acc: 0.9977
Epoch 12/30
 - 9s - loss: 0.0287 - acc: 0.9978
Epoch 13/30
 - 9s - loss: 0.0284 - acc: 0.9978
Epoch 14/30
 - 9s - loss: 0.0281 - acc: 0.9978
Epoch 15/30
 - 9s - loss: 0.0284 - acc: 0.9978
Epoch 16/30
 - 9s - loss: 0.0285 - acc: 0.9978
Epoch 17/30
 - 9s - loss: 0.0288 - acc: 0.9978
Epoch 18/30
 - 9s - loss: 0.0288 - acc: 0.9978
Epoch 19/30
 - 9s - loss: 0.0288 - acc: 0.9978
Epoch 20/30
 - 9s - loss: 0.0291 - acc: 0.9977
Epoch 21/30
 - 9s - loss: 0.0298 - acc: 0.9977
Epoch 22/30
 - 9s - loss: 0.0296 - acc: 0.9977
Epoch 23/30
 - 9s - loss: 0.0291 - acc: 0.9978
Epoch 24/30
 - 9s - loss: 0.0287 - acc: 0.9978
Epoch 25/30
 - 9s - loss: 0.0281 - acc: 0.9979
Epoch 26/30
 - 9s - loss: 0.0285 - acc: 0.9978
Epoch 27/30
 - 9s - loss: 0.0291 - acc: 0.9978
Epoch 28/30
 - 9s - loss: 0.0277 - acc: 0.9979
Epoch 29/30
 - 9s - loss: 0.0272 - acc: 0.9979
Epoch 30/30
 - 9s - loss: 0.0280 - acc: 0.9979
# Training time = 0:50:12.425094
# F-Score(Ordinary) = 0.684, Recall: 0.875, Precision: 0.562
# F-Score(lvc) = 0.535, Recall: 0.909, Precision: 0.379
# F-Score(ireflv) = 0.836, Recall: 0.913, Precision: 0.77
# F-Score(id) = 0.667, Recall: 0.836, Precision: 0.554
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 200)       2938600     input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 56)        13160       input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 800)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 224)          0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 1024)         0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 256)          262400      concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 256)          0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            2056        dropout_2[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 89s - loss: 0.0187 - acc: 0.9964 - val_loss: 0.0156 - val_acc: 0.9975
Epoch 2/30
 - 89s - loss: 0.0160 - acc: 0.9974 - val_loss: 0.0179 - val_acc: 0.9973
Epoch 3/30
 - 89s - loss: 0.0174 - acc: 0.9974 - val_loss: 0.0192 - val_acc: 0.9975
Epoch 4/30
 - 89s - loss: 0.0175 - acc: 0.9974 - val_loss: 0.0183 - val_acc: 0.9974
Epoch 5/30
 - 89s - loss: 0.0181 - acc: 0.9974 - val_loss: 0.0190 - val_acc: 0.9975
Epoch 6/30
 - 89s - loss: 0.0188 - acc: 0.9974 - val_loss: 0.0210 - val_acc: 0.9975
Epoch 7/30
 - 89s - loss: 0.0238 - acc: 0.9975 - val_loss: 0.0246 - val_acc: 0.9975
Epoch 8/30
 - 89s - loss: 0.0248 - acc: 0.9976 - val_loss: 0.0265 - val_acc: 0.9973
Epoch 9/30
 - 89s - loss: 0.0251 - acc: 0.9976 - val_loss: 0.0299 - val_acc: 0.9975
Epoch 10/30
 - 89s - loss: 0.0283 - acc: 0.9976 - val_loss: 0.0312 - val_acc: 0.9974
Epoch 11/30
 - 89s - loss: 0.0289 - acc: 0.9976 - val_loss: 0.0314 - val_acc: 0.9975
Epoch 12/30
 - 89s - loss: 0.0288 - acc: 0.9977 - val_loss: 0.0307 - val_acc: 0.9975
Epoch 13/30
 - 89s - loss: 0.0289 - acc: 0.9977 - val_loss: 0.0316 - val_acc: 0.9974
Epoch 14/30
 - 89s - loss: 0.0287 - acc: 0.9977 - val_loss: 0.0313 - val_acc: 0.9973
Epoch 15/30
 - 89s - loss: 0.0286 - acc: 0.9977 - val_loss: 0.0322 - val_acc: 0.9975
Epoch 16/30
 - 89s - loss: 0.0289 - acc: 0.9977 - val_loss: 0.0319 - val_acc: 0.9973
Epoch 17/30
 - 89s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0316 - val_acc: 0.9974
Epoch 18/30
 - 89s - loss: 0.0292 - acc: 0.9978 - val_loss: 0.0328 - val_acc: 0.9973
Epoch 19/30
 - 89s - loss: 0.0291 - acc: 0.9978 - val_loss: 0.0335 - val_acc: 0.9974
Epoch 20/30
 - 89s - loss: 0.0290 - acc: 0.9978 - val_loss: 0.0323 - val_acc: 0.9974
Epoch 21/30
 - 88s - loss: 0.0291 - acc: 0.9978 - val_loss: 0.0332 - val_acc: 0.9973
Epoch 22/30
 - 89s - loss: 0.0289 - acc: 0.9978 - val_loss: 0.0316 - val_acc: 0.9974
Epoch 23/30
 - 89s - loss: 0.0294 - acc: 0.9978 - val_loss: 0.0321 - val_acc: 0.9975
Epoch 24/30
 - 89s - loss: 0.0292 - acc: 0.9978 - val_loss: 0.0320 - val_acc: 0.9974
Epoch 25/30
 - 89s - loss: 0.0295 - acc: 0.9978 - val_loss: 0.0347 - val_acc: 0.9972
Epoch 26/30
 - 89s - loss: 0.0293 - acc: 0.9978 - val_loss: 0.0327 - val_acc: 0.9975
Epoch 27/30
 - 89s - loss: 0.0294 - acc: 0.9978 - val_loss: 0.0325 - val_acc: 0.9975
Epoch 28/30
 - 89s - loss: 0.0295 - acc: 0.9978 - val_loss: 0.0323 - val_acc: 0.9974
Epoch 29/30
 - 89s - loss: 0.0298 - acc: 0.9978 - val_loss: 0.0362 - val_acc: 0.9972
Epoch 30/30
 - 89s - loss: 0.0295 - acc: 0.9978 - val_loss: 0.0341 - val_acc: 0.9974
Epoch 1/30
 - 10s - loss: 0.0343 - acc: 0.9972
Epoch 2/30
 - 10s - loss: 0.0321 - acc: 0.9974
Epoch 3/30
 - 10s - loss: 0.0327 - acc: 0.9974
Epoch 4/30
 - 10s - loss: 0.0337 - acc: 0.9974
Epoch 5/30
 - 10s - loss: 0.0313 - acc: 0.9975
Epoch 6/30
 - 10s - loss: 0.0306 - acc: 0.9976
Epoch 7/30
 - 10s - loss: 0.0313 - acc: 0.9976
Epoch 8/30
 - 10s - loss: 0.0321 - acc: 0.9976
Epoch 9/30
 - 10s - loss: 0.0305 - acc: 0.9977
Epoch 10/30
 - 10s - loss: 0.0309 - acc: 0.9977
Epoch 11/30
 - 10s - loss: 0.0317 - acc: 0.9976
Epoch 12/30
 - 10s - loss: 0.0315 - acc: 0.9976
Epoch 13/30
 - 10s - loss: 0.0313 - acc: 0.9976
Epoch 14/30
 - 10s - loss: 0.0315 - acc: 0.9977
Epoch 15/30
 - 10s - loss: 0.0307 - acc: 0.9977
Epoch 16/30
 - 10s - loss: 0.0310 - acc: 0.9977
Epoch 17/30
 - 10s - loss: 0.0306 - acc: 0.9977
Epoch 18/30
 - 10s - loss: 0.0313 - acc: 0.9977
Epoch 19/30
 - 10s - loss: 0.0331 - acc: 0.9975
Epoch 20/30
 - 10s - loss: 0.0316 - acc: 0.9976
Epoch 21/30
 - 10s - loss: 0.0309 - acc: 0.9977
Epoch 22/30
 - 10s - loss: 0.0312 - acc: 0.9977
Epoch 23/30
 - 10s - loss: 0.0311 - acc: 0.9977
Epoch 24/30
 - 10s - loss: 0.0316 - acc: 0.9977
Epoch 25/30
 - 10s - loss: 0.0317 - acc: 0.9977
Epoch 26/30
 - 10s - loss: 0.0331 - acc: 0.9976
Epoch 27/30
 - 10s - loss: 0.0306 - acc: 0.9977
Epoch 28/30
 - 10s - loss: 0.0304 - acc: 0.9977
Epoch 29/30
 - 10s - loss: 0.0302 - acc: 0.9978
Epoch 30/30
 - 10s - loss: 0.0300 - acc: 0.9978
# Training time = 0:50:30.808041
# F-Score(Ordinary) = 0.676, Recall: 0.906, Precision: 0.539
# F-Score(lvc) = 0.548, Recall: 0.944, Precision: 0.386
# F-Score(ireflv) = 0.821, Recall: 0.857, Precision: 0.787
# F-Score(id) = 0.635, Recall: 0.93, Precision: 0.482
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 200)       2938600     input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 56)        13160       input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 800)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 224)          0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 1024)         0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 256)          262400      concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 256)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            2056        dropout_3[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 88s - loss: 0.0186 - acc: 0.9964 - val_loss: 0.0147 - val_acc: 0.9974
Epoch 2/30
 - 88s - loss: 0.0158 - acc: 0.9974 - val_loss: 0.0154 - val_acc: 0.9974
Epoch 3/30
 - 88s - loss: 0.0171 - acc: 0.9974 - val_loss: 0.0173 - val_acc: 0.9972
Epoch 4/30
 - 88s - loss: 0.0172 - acc: 0.9974 - val_loss: 0.0175 - val_acc: 0.9973
Epoch 5/30
 - 88s - loss: 0.0171 - acc: 0.9975 - val_loss: 0.0193 - val_acc: 0.9973
Epoch 6/30
 - 88s - loss: 0.0217 - acc: 0.9976 - val_loss: 0.0249 - val_acc: 0.9974
Epoch 7/30
 - 88s - loss: 0.0225 - acc: 0.9976 - val_loss: 0.0259 - val_acc: 0.9973
Epoch 8/30
 - 88s - loss: 0.0244 - acc: 0.9976 - val_loss: 0.0272 - val_acc: 0.9972
Epoch 9/30
 - 88s - loss: 0.0246 - acc: 0.9976 - val_loss: 0.0277 - val_acc: 0.9973
Epoch 10/30
 - 88s - loss: 0.0249 - acc: 0.9977 - val_loss: 0.0291 - val_acc: 0.9971
Epoch 11/30
 - 88s - loss: 0.0264 - acc: 0.9977 - val_loss: 0.0316 - val_acc: 0.9974
Epoch 12/30
 - 88s - loss: 0.0280 - acc: 0.9977 - val_loss: 0.0316 - val_acc: 0.9973
Epoch 13/30
 - 88s - loss: 0.0285 - acc: 0.9977 - val_loss: 0.0328 - val_acc: 0.9973
Epoch 14/30
 - 88s - loss: 0.0284 - acc: 0.9978 - val_loss: 0.0333 - val_acc: 0.9973
Epoch 15/30
 - 88s - loss: 0.0282 - acc: 0.9978 - val_loss: 0.0340 - val_acc: 0.9969
Epoch 16/30
 - 88s - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0332 - val_acc: 0.9973
Epoch 17/30
 - 88s - loss: 0.0285 - acc: 0.9977 - val_loss: 0.0320 - val_acc: 0.9973
Epoch 18/30
 - 88s - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0339 - val_acc: 0.9971
Epoch 19/30
 - 88s - loss: 0.0285 - acc: 0.9977 - val_loss: 0.0325 - val_acc: 0.9974
Epoch 20/30
 - 88s - loss: 0.0286 - acc: 0.9978 - val_loss: 0.0343 - val_acc: 0.9973
Epoch 21/30
 - 88s - loss: 0.0289 - acc: 0.9978 - val_loss: 0.0339 - val_acc: 0.9973
Epoch 22/30
 - 88s - loss: 0.0295 - acc: 0.9977 - val_loss: 0.0376 - val_acc: 0.9972
Epoch 23/30
 - 88s - loss: 0.0293 - acc: 0.9978 - val_loss: 0.0344 - val_acc: 0.9972
Epoch 24/30
 - 88s - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0334 - val_acc: 0.9973
Epoch 25/30
 - 88s - loss: 0.0292 - acc: 0.9978 - val_loss: 0.0345 - val_acc: 0.9972
Epoch 26/30
 - 88s - loss: 0.0289 - acc: 0.9978 - val_loss: 0.0338 - val_acc: 0.9973
Epoch 27/30
 - 88s - loss: 0.0291 - acc: 0.9978 - val_loss: 0.0345 - val_acc: 0.9973
Epoch 28/30
 - 88s - loss: 0.0293 - acc: 0.9978 - val_loss: 0.0339 - val_acc: 0.9974
Epoch 29/30
 - 88s - loss: 0.0288 - acc: 0.9979 - val_loss: 0.0334 - val_acc: 0.9974
Epoch 30/30
 - 88s - loss: 0.0287 - acc: 0.9979 - val_loss: 0.0339 - val_acc: 0.9973
Epoch 1/30
 - 10s - loss: 0.0343 - acc: 0.9972
Epoch 2/30
 - 10s - loss: 0.0343 - acc: 0.9972
Epoch 3/30
 - 10s - loss: 0.0349 - acc: 0.9973
Epoch 4/30
 - 10s - loss: 0.0333 - acc: 0.9974
Epoch 5/30
 - 10s - loss: 0.0332 - acc: 0.9974
Epoch 6/30
 - 10s - loss: 0.0321 - acc: 0.9975
Epoch 7/30
 - 10s - loss: 0.0331 - acc: 0.9974
Epoch 8/30
 - 10s - loss: 0.0323 - acc: 0.9975
Epoch 9/30
 - 9s - loss: 0.0319 - acc: 0.9976
Epoch 10/30
 - 9s - loss: 0.0318 - acc: 0.9976
Epoch 11/30
 - 9s - loss: 0.0318 - acc: 0.9976
Epoch 12/30
 - 9s - loss: 0.0320 - acc: 0.9976
Epoch 13/30
 - 9s - loss: 0.0324 - acc: 0.9975
Epoch 14/30
 - 10s - loss: 0.0331 - acc: 0.9975
Epoch 15/30
 - 10s - loss: 0.0318 - acc: 0.9976
Epoch 16/30
 - 9s - loss: 0.0313 - acc: 0.9976
Epoch 17/30
 - 10s - loss: 0.0323 - acc: 0.9976
Epoch 18/30
 - 10s - loss: 0.0319 - acc: 0.9976
Epoch 19/30
 - 9s - loss: 0.0317 - acc: 0.9976
Epoch 20/30
 - 10s - loss: 0.0317 - acc: 0.9976
Epoch 21/30
 - 9s - loss: 0.0330 - acc: 0.9975
Epoch 22/30
 - 9s - loss: 0.0307 - acc: 0.9977
Epoch 23/30
 - 9s - loss: 0.0310 - acc: 0.9977
Epoch 24/30
 - 9s - loss: 0.0308 - acc: 0.9977
Epoch 25/30
 - 10s - loss: 0.0316 - acc: 0.9977
Epoch 26/30
 - 9s - loss: 0.0310 - acc: 0.9977
Epoch 27/30
 - 9s - loss: 0.0311 - acc: 0.9977
Epoch 28/30
 - 9s - loss: 0.0304 - acc: 0.9977
Epoch 29/30
 - 10s - loss: 0.0310 - acc: 0.9977
Epoch 30/30
 - 9s - loss: 0.0307 - acc: 0.9977
# Training time = 0:50:06.316492
# F-Score(Ordinary) = 0.686, Recall: 0.851, Precision: 0.575
# F-Score(lvc) = 0.565, Recall: 0.915, Precision: 0.409
# F-Score(ireflv) = 0.836, Recall: 0.882, Precision: 0.795
# F-Score(id) = 0.634, Recall: 0.78, Precision: 0.534
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 200)       2938600     input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 56)        13160       input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 800)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 224)          0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 1024)         0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 256)          262400      concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 256)          0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            2056        dropout_4[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 87s - loss: 0.0189 - acc: 0.9963 - val_loss: 0.0139 - val_acc: 0.9976
Epoch 2/30
 - 87s - loss: 0.0158 - acc: 0.9974 - val_loss: 0.0153 - val_acc: 0.9975
Epoch 3/30
 - 87s - loss: 0.0168 - acc: 0.9974 - val_loss: 0.0176 - val_acc: 0.9975
Epoch 4/30
 - 87s - loss: 0.0187 - acc: 0.9974 - val_loss: 0.0202 - val_acc: 0.9973
Epoch 5/30
 - 87s - loss: 0.0197 - acc: 0.9975 - val_loss: 0.0204 - val_acc: 0.9975
Epoch 6/30
 - 87s - loss: 0.0227 - acc: 0.9975 - val_loss: 0.0231 - val_acc: 0.9975
Epoch 7/30
 - 87s - loss: 0.0242 - acc: 0.9975 - val_loss: 0.0236 - val_acc: 0.9975
Epoch 8/30
 - 87s - loss: 0.0241 - acc: 0.9976 - val_loss: 0.0243 - val_acc: 0.9975
Epoch 9/30
 - 87s - loss: 0.0242 - acc: 0.9976 - val_loss: 0.0256 - val_acc: 0.9974
Epoch 10/30
 - 87s - loss: 0.0243 - acc: 0.9976 - val_loss: 0.0333 - val_acc: 0.9972
Epoch 11/30
 - 87s - loss: 0.0258 - acc: 0.9976 - val_loss: 0.0259 - val_acc: 0.9973
Epoch 12/30
 - 87s - loss: 0.0272 - acc: 0.9976 - val_loss: 0.0296 - val_acc: 0.9973
Epoch 13/30
 - 87s - loss: 0.0294 - acc: 0.9976 - val_loss: 0.0290 - val_acc: 0.9975
Epoch 14/30
 - 87s - loss: 0.0297 - acc: 0.9976 - val_loss: 0.0308 - val_acc: 0.9974
Epoch 15/30
 - 87s - loss: 0.0293 - acc: 0.9976 - val_loss: 0.0309 - val_acc: 0.9973
Epoch 16/30
 - 87s - loss: 0.0295 - acc: 0.9976 - val_loss: 0.0315 - val_acc: 0.9974
Epoch 17/30
 - 87s - loss: 0.0292 - acc: 0.9977 - val_loss: 0.0297 - val_acc: 0.9975
Epoch 18/30
 - 87s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.0301 - val_acc: 0.9976
Epoch 19/30
 - 87s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0301 - val_acc: 0.9975
Epoch 20/30
 - 87s - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0299 - val_acc: 0.9975
Epoch 21/30
 - 87s - loss: 0.0292 - acc: 0.9977 - val_loss: 0.0302 - val_acc: 0.9975
Epoch 22/30
 - 87s - loss: 0.0292 - acc: 0.9977 - val_loss: 0.0325 - val_acc: 0.9974
Epoch 23/30
 - 87s - loss: 0.0294 - acc: 0.9977 - val_loss: 0.0329 - val_acc: 0.9973
Epoch 24/30
 - 87s - loss: 0.0295 - acc: 0.9977 - val_loss: 0.0313 - val_acc: 0.9974
Epoch 25/30
 - 87s - loss: 0.0300 - acc: 0.9977 - val_loss: 0.0304 - val_acc: 0.9975
Epoch 26/30
 - 87s - loss: 0.0294 - acc: 0.9978 - val_loss: 0.0321 - val_acc: 0.9974
Epoch 27/30
 - 87s - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0317 - val_acc: 0.9974
Epoch 28/30
 - 87s - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0308 - val_acc: 0.9974
Epoch 29/30
 - 87s - loss: 0.0297 - acc: 0.9978 - val_loss: 0.0324 - val_acc: 0.9974
Epoch 30/30
 - 87s - loss: 0.0294 - acc: 0.9978 - val_loss: 0.0305 - val_acc: 0.9974
Epoch 1/30
 - 9s - loss: 0.0335 - acc: 0.9971
Epoch 2/30
 - 9s - loss: 0.0330 - acc: 0.9972
Epoch 3/30
 - 9s - loss: 0.0316 - acc: 0.9973
Epoch 4/30
 - 9s - loss: 0.0308 - acc: 0.9974
Epoch 5/30
 - 9s - loss: 0.0315 - acc: 0.9974
Epoch 6/30
 - 9s - loss: 0.0313 - acc: 0.9976
Epoch 7/30
 - 9s - loss: 0.0307 - acc: 0.9975
Epoch 8/30
 - 9s - loss: 0.0303 - acc: 0.9976
Epoch 9/30
 - 9s - loss: 0.0294 - acc: 0.9976
Epoch 10/30
 - 9s - loss: 0.0293 - acc: 0.9976
Epoch 11/30
 - 9s - loss: 0.0294 - acc: 0.9976
Epoch 12/30
 - 9s - loss: 0.0290 - acc: 0.9977
Epoch 13/30
 - 9s - loss: 0.0287 - acc: 0.9977
Epoch 14/30
 - 9s - loss: 0.0285 - acc: 0.9978
Epoch 15/30
 - 9s - loss: 0.0289 - acc: 0.9977
Epoch 16/30
 - 9s - loss: 0.0295 - acc: 0.9977
Epoch 17/30
 - 9s - loss: 0.0320 - acc: 0.9975
Epoch 18/30
 - 9s - loss: 0.0291 - acc: 0.9977
Epoch 19/30
 - 9s - loss: 0.0292 - acc: 0.9977
Epoch 20/30
 - 9s - loss: 0.0293 - acc: 0.9977
Epoch 21/30
 - 9s - loss: 0.0284 - acc: 0.9978
Epoch 22/30
 - 9s - loss: 0.0290 - acc: 0.9977
Epoch 23/30
 - 9s - loss: 0.0290 - acc: 0.9977
Epoch 24/30
 - 9s - loss: 0.0294 - acc: 0.9977
Epoch 25/30
 - 9s - loss: 0.0289 - acc: 0.9977
Epoch 26/30
 - 9s - loss: 0.0287 - acc: 0.9977
Epoch 27/30
 - 9s - loss: 0.0283 - acc: 0.9977
Epoch 28/30
 - 9s - loss: 0.0295 - acc: 0.9977
Epoch 29/30
 - 9s - loss: 0.0299 - acc: 0.9976
Epoch 30/30
 - 9s - loss: 0.0290 - acc: 0.9977
# Training time = 0:49:34.318763
# F-Score(Ordinary) = 0.668, Recall: 0.908, Precision: 0.528
# F-Score(lvc) = 0.532, Recall: 0.893, Precision: 0.379
# F-Score(ireflv) = 0.814, Recall: 0.885, Precision: 0.754
# F-Score(id) = 0.63, Recall: 0.929, Precision: 0.477
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 200)       2938600     input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 56)        13160       input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 800)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 224)          0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 1024)         0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 256)          262400      concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 256)          0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            2056        dropout_5[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 87s - loss: 0.0188 - acc: 0.9964 - val_loss: 0.0145 - val_acc: 0.9974
Epoch 2/30
 - 87s - loss: 0.0158 - acc: 0.9974 - val_loss: 0.0153 - val_acc: 0.9975
Epoch 3/30
 - 87s - loss: 0.0156 - acc: 0.9974 - val_loss: 0.0195 - val_acc: 0.9973
Epoch 4/30
 - 87s - loss: 0.0159 - acc: 0.9974 - val_loss: 0.0175 - val_acc: 0.9974
Epoch 5/30
 - 87s - loss: 0.0164 - acc: 0.9975 - val_loss: 0.0161 - val_acc: 0.9974
Epoch 6/30
 - 87s - loss: 0.0170 - acc: 0.9974 - val_loss: 0.0194 - val_acc: 0.9974
Epoch 7/30
 - 87s - loss: 0.0193 - acc: 0.9974 - val_loss: 0.0209 - val_acc: 0.9974
Epoch 8/30
 - 87s - loss: 0.0197 - acc: 0.9974 - val_loss: 0.0220 - val_acc: 0.9973
Epoch 9/30
 - 87s - loss: 0.0228 - acc: 0.9975 - val_loss: 0.0259 - val_acc: 0.9973
Epoch 10/30
 - 87s - loss: 0.0233 - acc: 0.9976 - val_loss: 0.0249 - val_acc: 0.9974
Epoch 11/30
 - 87s - loss: 0.0232 - acc: 0.9976 - val_loss: 0.0242 - val_acc: 0.9975
Epoch 12/30
 - 87s - loss: 0.0240 - acc: 0.9976 - val_loss: 0.0270 - val_acc: 0.9973
Epoch 13/30
 - 87s - loss: 0.0247 - acc: 0.9977 - val_loss: 0.0270 - val_acc: 0.9974
Epoch 14/30
 - 87s - loss: 0.0256 - acc: 0.9977 - val_loss: 0.0294 - val_acc: 0.9971
Epoch 15/30
 - 87s - loss: 0.0258 - acc: 0.9977 - val_loss: 0.0270 - val_acc: 0.9975
Epoch 16/30
 - 87s - loss: 0.0259 - acc: 0.9977 - val_loss: 0.0267 - val_acc: 0.9974
Epoch 17/30
 - 87s - loss: 0.0272 - acc: 0.9977 - val_loss: 0.0273 - val_acc: 0.9975
Epoch 18/30
 - 87s - loss: 0.0271 - acc: 0.9977 - val_loss: 0.0302 - val_acc: 0.9973
Epoch 19/30
 - 87s - loss: 0.0295 - acc: 0.9977 - val_loss: 0.0318 - val_acc: 0.9973
Epoch 20/30
 - 87s - loss: 0.0297 - acc: 0.9977 - val_loss: 0.0317 - val_acc: 0.9973
Epoch 21/30
 - 87s - loss: 0.0297 - acc: 0.9976 - val_loss: 0.0304 - val_acc: 0.9975
Epoch 22/30
 - 87s - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0314 - val_acc: 0.9974
Epoch 23/30
 - 87s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0317 - val_acc: 0.9973
Epoch 24/30
 - 87s - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0327 - val_acc: 0.9973
Epoch 25/30
 - 87s - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0304 - val_acc: 0.9974
Epoch 26/30
 - 87s - loss: 0.0285 - acc: 0.9978 - val_loss: 0.0327 - val_acc: 0.9974
Epoch 27/30
 - 87s - loss: 0.0290 - acc: 0.9978 - val_loss: 0.0318 - val_acc: 0.9973
Epoch 28/30
 - 87s - loss: 0.0292 - acc: 0.9978 - val_loss: 0.0317 - val_acc: 0.9974
Epoch 29/30
 - 87s - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0308 - val_acc: 0.9975
Epoch 30/30
 - 87s - loss: 0.0292 - acc: 0.9978 - val_loss: 0.0356 - val_acc: 0.9972
Epoch 1/30
 - 9s - loss: 0.0329 - acc: 0.9971
Epoch 2/30
 - 9s - loss: 0.0328 - acc: 0.9972
Epoch 3/30
 - 9s - loss: 0.0314 - acc: 0.9973
Epoch 4/30
 - 9s - loss: 0.0305 - acc: 0.9974
Epoch 5/30
 - 9s - loss: 0.0307 - acc: 0.9974
Epoch 6/30
 - 9s - loss: 0.0310 - acc: 0.9974
Epoch 7/30
 - 9s - loss: 0.0296 - acc: 0.9976
Epoch 8/30
 - 9s - loss: 0.0298 - acc: 0.9976
Epoch 9/30
 - 9s - loss: 0.0294 - acc: 0.9976
Epoch 10/30
 - 9s - loss: 0.0295 - acc: 0.9976
Epoch 11/30
 - 9s - loss: 0.0316 - acc: 0.9975
Epoch 12/30
 - 9s - loss: 0.0307 - acc: 0.9975
Epoch 13/30
 - 9s - loss: 0.0303 - acc: 0.9975
Epoch 14/30
 - 9s - loss: 0.0287 - acc: 0.9977
Epoch 15/30
 - 9s - loss: 0.0293 - acc: 0.9976
Epoch 16/30
 - 9s - loss: 0.0300 - acc: 0.9976
Epoch 17/30
 - 9s - loss: 0.0298 - acc: 0.9976
Epoch 18/30
 - 9s - loss: 0.0299 - acc: 0.9976
Epoch 19/30
 - 9s - loss: 0.0298 - acc: 0.9976
Epoch 20/30
 - 9s - loss: 0.0303 - acc: 0.9976
Epoch 21/30
 - 9s - loss: 0.0288 - acc: 0.9977
Epoch 22/30
 - 9s - loss: 0.0289 - acc: 0.9977
Epoch 23/30
 - 9s - loss: 0.0289 - acc: 0.9977
Epoch 24/30
 - 9s - loss: 0.0295 - acc: 0.9976
Epoch 25/30
 - 9s - loss: 0.0288 - acc: 0.9977
Epoch 26/30
 - 9s - loss: 0.0295 - acc: 0.9976
Epoch 27/30
 - 9s - loss: 0.0293 - acc: 0.9977
Epoch 28/30
 - 9s - loss: 0.0295 - acc: 0.9977
Epoch 29/30
 - 9s - loss: 0.0294 - acc: 0.9977
Epoch 30/30
 - 9s - loss: 0.0293 - acc: 0.9976
# Training time = 0:49:32.362688
# F-Score(Ordinary) = 0.645, Recall: 0.758, Precision: 0.562
# F-Score(lvc) = 0.521, Recall: 0.875, Precision: 0.371
# F-Score(ireflv) = 0.826, Recall: 0.858, Precision: 0.795
# F-Score(id) = 0.569, Recall: 0.623, Precision: 0.523
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 200)       2938600     input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 56)        13160       input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 800)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 224)          0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 1024)         0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 256)          262400      concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 256)          0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            2056        dropout_6[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 87s - loss: 0.0186 - acc: 0.9964 - val_loss: 0.0158 - val_acc: 0.9973
Epoch 2/30
 - 87s - loss: 0.0158 - acc: 0.9974 - val_loss: 0.0156 - val_acc: 0.9972
Epoch 3/30
 - 88s - loss: 0.0159 - acc: 0.9974 - val_loss: 0.0173 - val_acc: 0.9973
Epoch 4/30
 - 87s - loss: 0.0163 - acc: 0.9974 - val_loss: 0.0179 - val_acc: 0.9973
Epoch 5/30
 - 87s - loss: 0.0163 - acc: 0.9974 - val_loss: 0.0204 - val_acc: 0.9973
Epoch 6/30
 - 87s - loss: 0.0169 - acc: 0.9975 - val_loss: 0.0209 - val_acc: 0.9974
Epoch 7/30
 - 87s - loss: 0.0178 - acc: 0.9975 - val_loss: 0.0224 - val_acc: 0.9972
Epoch 8/30
 - 87s - loss: 0.0198 - acc: 0.9974 - val_loss: 0.0230 - val_acc: 0.9972
Epoch 9/30
 - 87s - loss: 0.0217 - acc: 0.9974 - val_loss: 0.0244 - val_acc: 0.9973
Epoch 10/30
 - 87s - loss: 0.0231 - acc: 0.9975 - val_loss: 0.0291 - val_acc: 0.9973
Epoch 11/30
 - 87s - loss: 0.0262 - acc: 0.9976 - val_loss: 0.0334 - val_acc: 0.9971
Epoch 12/30
 - 87s - loss: 0.0271 - acc: 0.9976 - val_loss: 0.0327 - val_acc: 0.9973
Epoch 13/30
 - 87s - loss: 0.0294 - acc: 0.9976 - val_loss: 0.0332 - val_acc: 0.9973
Epoch 14/30
 - 87s - loss: 0.0290 - acc: 0.9977 - val_loss: 0.0326 - val_acc: 0.9974
Epoch 15/30
 - 87s - loss: 0.0292 - acc: 0.9976 - val_loss: 0.0327 - val_acc: 0.9974
Epoch 16/30
 - 87s - loss: 0.0295 - acc: 0.9977 - val_loss: 0.0337 - val_acc: 0.9973
Epoch 17/30
 - 87s - loss: 0.0287 - acc: 0.9977 - val_loss: 0.0336 - val_acc: 0.9973
Epoch 18/30
 - 87s - loss: 0.0290 - acc: 0.9977 - val_loss: 0.0341 - val_acc: 0.9973
Epoch 19/30
 - 87s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.0339 - val_acc: 0.9973
Epoch 20/30
 - 87s - loss: 0.0289 - acc: 0.9977 - val_loss: 0.0351 - val_acc: 0.9971
Epoch 21/30
 - 87s - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0358 - val_acc: 0.9973
Epoch 22/30
 - 87s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0348 - val_acc: 0.9973
Epoch 23/30
 - 87s - loss: 0.0295 - acc: 0.9977 - val_loss: 0.0342 - val_acc: 0.9973
Epoch 24/30
 - 87s - loss: 0.0294 - acc: 0.9977 - val_loss: 0.0349 - val_acc: 0.9973
Epoch 25/30
 - 87s - loss: 0.0298 - acc: 0.9977 - val_loss: 0.0339 - val_acc: 0.9973
Epoch 26/30
 - 87s - loss: 0.0301 - acc: 0.9977 - val_loss: 0.0347 - val_acc: 0.9974
Epoch 27/30
 - 87s - loss: 0.0297 - acc: 0.9977 - val_loss: 0.0354 - val_acc: 0.9973
Epoch 28/30
 - 87s - loss: 0.0295 - acc: 0.9977 - val_loss: 0.0348 - val_acc: 0.9973
Epoch 29/30
 - 87s - loss: 0.0290 - acc: 0.9978 - val_loss: 0.0346 - val_acc: 0.9973
Epoch 30/30
 - 88s - loss: 0.0292 - acc: 0.9978 - val_loss: 0.0362 - val_acc: 0.9973
Epoch 1/30
 - 9s - loss: 0.0375 - acc: 0.9971
Epoch 2/30
 - 9s - loss: 0.0343 - acc: 0.9972
Epoch 3/30
 - 9s - loss: 0.0353 - acc: 0.9972
Epoch 4/30
 - 9s - loss: 0.0356 - acc: 0.9973
Epoch 5/30
 - 9s - loss: 0.0367 - acc: 0.9972
Epoch 6/30
 - 9s - loss: 0.0353 - acc: 0.9973
Epoch 7/30
 - 9s - loss: 0.0331 - acc: 0.9975
Epoch 8/30
 - 9s - loss: 0.0327 - acc: 0.9975
Epoch 9/30
 - 9s - loss: 0.0346 - acc: 0.9974
Epoch 10/30
 - 9s - loss: 0.0339 - acc: 0.9974
Epoch 11/30
 - 9s - loss: 0.0347 - acc: 0.9973
Epoch 12/30
 - 9s - loss: 0.0330 - acc: 0.9975
Epoch 13/30
 - 9s - loss: 0.0333 - acc: 0.9975
Epoch 14/30
 - 9s - loss: 0.0330 - acc: 0.9975
Epoch 15/30
 - 9s - loss: 0.0322 - acc: 0.9976
Epoch 16/30
 - 9s - loss: 0.0338 - acc: 0.9975
Epoch 17/30
 - 9s - loss: 0.0313 - acc: 0.9976
Epoch 18/30
 - 9s - loss: 0.0317 - acc: 0.9976
Epoch 19/30
 - 9s - loss: 0.0320 - acc: 0.9976
Epoch 20/30
 - 9s - loss: 0.0329 - acc: 0.9975
Epoch 21/30
 - 9s - loss: 0.0326 - acc: 0.9976
Epoch 22/30
 - 9s - loss: 0.0335 - acc: 0.9975
Epoch 23/30
 - 9s - loss: 0.0316 - acc: 0.9976
Epoch 24/30
 - 9s - loss: 0.0311 - acc: 0.9977
Epoch 25/30
 - 9s - loss: 0.0321 - acc: 0.9976
Epoch 26/30
 - 9s - loss: 0.0327 - acc: 0.9975
Epoch 27/30
 - 9s - loss: 0.0328 - acc: 0.9976
Epoch 28/30
 - 9s - loss: 0.0333 - acc: 0.9975
Epoch 29/30
 - 9s - loss: 0.0334 - acc: 0.9975
Epoch 30/30
 - 9s - loss: 0.0333 - acc: 0.9976
# Training time = 0:49:42.158108
# F-Score(Ordinary) = 0.66, Recall: 0.841, Precision: 0.544
# F-Score(lvc) = 0.493, Recall: 0.68, Precision: 0.386
# F-Score(ireflv) = 0.809, Recall: 0.861, Precision: 0.762
# F-Score(id) = 0.642, Recall: 0.906, Precision: 0.497
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 200)       2938600     input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 56)        13160       input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 800)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 224)          0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 1024)         0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 256)          262400      concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 256)          0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            2056        dropout_7[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 88s - loss: 0.0186 - acc: 0.9964 - val_loss: 0.0150 - val_acc: 0.9975
Epoch 2/30
 - 88s - loss: 0.0160 - acc: 0.9974 - val_loss: 0.0159 - val_acc: 0.9973
Epoch 3/30
 - 91s - loss: 0.0163 - acc: 0.9974 - val_loss: 0.0235 - val_acc: 0.9971
Epoch 4/30
 - 88s - loss: 0.0168 - acc: 0.9974 - val_loss: 0.0171 - val_acc: 0.9975
Epoch 5/30
 - 88s - loss: 0.0167 - acc: 0.9974 - val_loss: 0.0188 - val_acc: 0.9975
Epoch 6/30
 - 88s - loss: 0.0184 - acc: 0.9975 - val_loss: 0.0184 - val_acc: 0.9972
Epoch 7/30
 - 88s - loss: 0.0176 - acc: 0.9974 - val_loss: 0.0187 - val_acc: 0.9975
Epoch 8/30
 - 88s - loss: 0.0194 - acc: 0.9974 - val_loss: 0.0218 - val_acc: 0.9973
Epoch 9/30
 - 88s - loss: 0.0227 - acc: 0.9974 - val_loss: 0.0253 - val_acc: 0.9974
Epoch 10/30
 - 88s - loss: 0.0241 - acc: 0.9975 - val_loss: 0.0266 - val_acc: 0.9975
Epoch 11/30
 - 88s - loss: 0.0252 - acc: 0.9976 - val_loss: 0.0271 - val_acc: 0.9974
Epoch 12/30
 - 88s - loss: 0.0251 - acc: 0.9976 - val_loss: 0.0300 - val_acc: 0.9967
Epoch 13/30
 - 88s - loss: 0.0250 - acc: 0.9977 - val_loss: 0.0279 - val_acc: 0.9974
Epoch 14/30
 - 88s - loss: 0.0254 - acc: 0.9977 - val_loss: 0.0271 - val_acc: 0.9974
Epoch 15/30
 - 88s - loss: 0.0261 - acc: 0.9977 - val_loss: 0.0315 - val_acc: 0.9970
Epoch 16/30
 - 88s - loss: 0.0277 - acc: 0.9977 - val_loss: 0.0304 - val_acc: 0.9973
Epoch 17/30
 - 88s - loss: 0.0286 - acc: 0.9977 - val_loss: 0.0313 - val_acc: 0.9973
Epoch 18/30
 - 88s - loss: 0.0285 - acc: 0.9977 - val_loss: 0.0320 - val_acc: 0.9973
Epoch 19/30
 - 88s - loss: 0.0292 - acc: 0.9977 - val_loss: 0.0328 - val_acc: 0.9973
Epoch 20/30
 - 88s - loss: 0.0289 - acc: 0.9977 - val_loss: 0.0326 - val_acc: 0.9974
Epoch 21/30
 - 88s - loss: 0.0289 - acc: 0.9977 - val_loss: 0.0326 - val_acc: 0.9973
Epoch 22/30
 - 88s - loss: 0.0289 - acc: 0.9977 - val_loss: 0.0327 - val_acc: 0.9973
Epoch 23/30
 - 88s - loss: 0.0286 - acc: 0.9977 - val_loss: 0.0327 - val_acc: 0.9973
Epoch 24/30
 - 88s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0321 - val_acc: 0.9973
Epoch 25/30
 - 88s - loss: 0.0289 - acc: 0.9978 - val_loss: 0.0313 - val_acc: 0.9975
Epoch 26/30
 - 88s - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0313 - val_acc: 0.9974
Epoch 27/30
 - 88s - loss: 0.0289 - acc: 0.9978 - val_loss: 0.0351 - val_acc: 0.9973
Epoch 28/30
 - 88s - loss: 0.0291 - acc: 0.9978 - val_loss: 0.0323 - val_acc: 0.9975
Epoch 29/30
 - 88s - loss: 0.0293 - acc: 0.9978 - val_loss: 0.0334 - val_acc: 0.9973
Epoch 30/30
 - 88s - loss: 0.0294 - acc: 0.9978 - val_loss: 0.0372 - val_acc: 0.9971
Epoch 1/30
 - 9s - loss: 0.0339 - acc: 0.9972
Epoch 2/30
 - 9s - loss: 0.0321 - acc: 0.9973
Epoch 3/30
 - 9s - loss: 0.0326 - acc: 0.9973
Epoch 4/30
 - 9s - loss: 0.0323 - acc: 0.9974
Epoch 5/30
 - 9s - loss: 0.0326 - acc: 0.9974
Epoch 6/30
 - 9s - loss: 0.0322 - acc: 0.9974
Epoch 7/30
 - 9s - loss: 0.0307 - acc: 0.9976
Epoch 8/30
 - 9s - loss: 0.0320 - acc: 0.9975
Epoch 9/30
 - 9s - loss: 0.0306 - acc: 0.9976
Epoch 10/30
 - 9s - loss: 0.0304 - acc: 0.9976
Epoch 11/30
 - 9s - loss: 0.0300 - acc: 0.9977
Epoch 12/30
 - 9s - loss: 0.0297 - acc: 0.9977
Epoch 13/30
 - 10s - loss: 0.0299 - acc: 0.9977
Epoch 14/30
 - 9s - loss: 0.0297 - acc: 0.9977
Epoch 15/30
 - 9s - loss: 0.0304 - acc: 0.9976
Epoch 16/30
 - 9s - loss: 0.0303 - acc: 0.9977
Epoch 17/30
 - 9s - loss: 0.0301 - acc: 0.9977
Epoch 18/30
 - 9s - loss: 0.0293 - acc: 0.9977
Epoch 19/30
 - 9s - loss: 0.0303 - acc: 0.9977
Epoch 20/30
 - 9s - loss: 0.0302 - acc: 0.9977
Epoch 21/30
 - 9s - loss: 0.0295 - acc: 0.9978
Epoch 22/30
 - 9s - loss: 0.0306 - acc: 0.9977
Epoch 23/30
 - 9s - loss: 0.0302 - acc: 0.9977
Epoch 24/30
 - 9s - loss: 0.0301 - acc: 0.9978
Epoch 25/30
 - 9s - loss: 0.0288 - acc: 0.9978
Epoch 26/30
 - 9s - loss: 0.0289 - acc: 0.9979
Epoch 27/30
 - 9s - loss: 0.0293 - acc: 0.9978
Epoch 28/30
 - 9s - loss: 0.0312 - acc: 0.9977
Epoch 29/30
 - 9s - loss: 0.0303 - acc: 0.9978
Epoch 30/30
 - 9s - loss: 0.0294 - acc: 0.9978
# Training time = 0:50:11.381732
# F-Score(Ordinary) = 0.702, Recall: 0.911, Precision: 0.57
# F-Score(lvc) = 0.516, Recall: 0.94, Precision: 0.356
# F-Score(ireflv) = 0.817, Recall: 0.85, Precision: 0.787
# F-Score(id) = 0.703, Recall: 0.932, Precision: 0.565
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 200)       2938600     input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 56)        13160       input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 800)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 224)          0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 1024)         0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 256)          262400      concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 256)          0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            2056        dropout_8[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 86s - loss: 0.0187 - acc: 0.9964 - val_loss: 0.0158 - val_acc: 0.9974
Epoch 2/30
 - 87s - loss: 0.0168 - acc: 0.9974 - val_loss: 0.0201 - val_acc: 0.9973
Epoch 3/30
 - 87s - loss: 0.0175 - acc: 0.9974 - val_loss: 0.0170 - val_acc: 0.9974
Epoch 4/30
 - 87s - loss: 0.0168 - acc: 0.9974 - val_loss: 0.0169 - val_acc: 0.9974
Epoch 5/30
 - 87s - loss: 0.0178 - acc: 0.9975 - val_loss: 0.0217 - val_acc: 0.9974
Epoch 6/30
 - 87s - loss: 0.0204 - acc: 0.9975 - val_loss: 0.0214 - val_acc: 0.9974
Epoch 7/30
 - 87s - loss: 0.0217 - acc: 0.9975 - val_loss: 0.0259 - val_acc: 0.9972
Epoch 8/30
 - 87s - loss: 0.0240 - acc: 0.9976 - val_loss: 0.0310 - val_acc: 0.9973
Epoch 9/30
 - 87s - loss: 0.0233 - acc: 0.9976 - val_loss: 0.0252 - val_acc: 0.9975
Epoch 10/30
 - 87s - loss: 0.0242 - acc: 0.9976 - val_loss: 0.0297 - val_acc: 0.9975
Epoch 11/30
 - 87s - loss: 0.0257 - acc: 0.9977 - val_loss: 0.0280 - val_acc: 0.9974
Epoch 12/30
 - 87s - loss: 0.0267 - acc: 0.9976 - val_loss: 0.0305 - val_acc: 0.9974
Epoch 13/30
 - 87s - loss: 0.0285 - acc: 0.9976 - val_loss: 0.0400 - val_acc: 0.9968
Epoch 14/30
 - 87s - loss: 0.0294 - acc: 0.9977 - val_loss: 0.0317 - val_acc: 0.9973
Epoch 15/30
 - 87s - loss: 0.0295 - acc: 0.9976 - val_loss: 0.0326 - val_acc: 0.9973
Epoch 16/30
 - 87s - loss: 0.0292 - acc: 0.9976 - val_loss: 0.0320 - val_acc: 0.9973
Epoch 17/30
 - 87s - loss: 0.0292 - acc: 0.9977 - val_loss: 0.0357 - val_acc: 0.9970
Epoch 18/30
 - 87s - loss: 0.0294 - acc: 0.9976 - val_loss: 0.0322 - val_acc: 0.9971
Epoch 19/30
 - 87s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.0327 - val_acc: 0.9970
Epoch 20/30
 - 87s - loss: 0.0294 - acc: 0.9977 - val_loss: 0.0344 - val_acc: 0.9973
Epoch 21/30
 - 87s - loss: 0.0296 - acc: 0.9977 - val_loss: 0.0320 - val_acc: 0.9974
Epoch 22/30
 - 87s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0337 - val_acc: 0.9972
Epoch 23/30
 - 87s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.0316 - val_acc: 0.9974
Epoch 24/30
 - 87s - loss: 0.0291 - acc: 0.9978 - val_loss: 0.0325 - val_acc: 0.9973
Epoch 25/30
 - 87s - loss: 0.0292 - acc: 0.9978 - val_loss: 0.0330 - val_acc: 0.9974
Epoch 26/30
 - 87s - loss: 0.0301 - acc: 0.9977 - val_loss: 0.0349 - val_acc: 0.9971
Epoch 27/30
 - 87s - loss: 0.0295 - acc: 0.9978 - val_loss: 0.0336 - val_acc: 0.9972
Epoch 28/30
 - 87s - loss: 0.0301 - acc: 0.9977 - val_loss: 0.0332 - val_acc: 0.9973
Epoch 29/30
 - 87s - loss: 0.0296 - acc: 0.9978 - val_loss: 0.0341 - val_acc: 0.9972
Epoch 30/30
 - 87s - loss: 0.0298 - acc: 0.9977 - val_loss: 0.0343 - val_acc: 0.9973
Epoch 1/30
 - 9s - loss: 0.0343 - acc: 0.9971
Epoch 2/30
 - 9s - loss: 0.0331 - acc: 0.9972
Epoch 3/30
 - 9s - loss: 0.0331 - acc: 0.9972
Epoch 4/30
 - 9s - loss: 0.0312 - acc: 0.9974
Epoch 5/30
 - 9s - loss: 0.0320 - acc: 0.9973
Epoch 6/30
 - 9s - loss: 0.0352 - acc: 0.9972
Epoch 7/30
 - 9s - loss: 0.0314 - acc: 0.9975
Epoch 8/30
 - 9s - loss: 0.0313 - acc: 0.9975
Epoch 9/30
 - 9s - loss: 0.0318 - acc: 0.9975
Epoch 10/30
 - 9s - loss: 0.0306 - acc: 0.9976
Epoch 11/30
 - 9s - loss: 0.0310 - acc: 0.9975
Epoch 12/30
 - 9s - loss: 0.0302 - acc: 0.9976
Epoch 13/30
 - 9s - loss: 0.0308 - acc: 0.9976
Epoch 14/30
 - 9s - loss: 0.0315 - acc: 0.9976
Epoch 15/30
 - 9s - loss: 0.0305 - acc: 0.9976
Epoch 16/30
 - 9s - loss: 0.0316 - acc: 0.9976
Epoch 17/30
 - 9s - loss: 0.0316 - acc: 0.9975
Epoch 18/30
 - 9s - loss: 0.0333 - acc: 0.9975
Epoch 19/30
 - 9s - loss: 0.0315 - acc: 0.9976
Epoch 20/30
 - 9s - loss: 0.0306 - acc: 0.9976
Epoch 21/30
 - 9s - loss: 0.0322 - acc: 0.9975
Epoch 22/30
 - 9s - loss: 0.0301 - acc: 0.9977
Epoch 23/30
 - 9s - loss: 0.0305 - acc: 0.9976
Epoch 24/30
 - 9s - loss: 0.0309 - acc: 0.9977
Epoch 25/30
 - 9s - loss: 0.0303 - acc: 0.9977
Epoch 26/30
 - 9s - loss: 0.0303 - acc: 0.9977
Epoch 27/30
 - 9s - loss: 0.0310 - acc: 0.9976
Epoch 28/30
 - 9s - loss: 0.0314 - acc: 0.9976
Epoch 29/30
 - 9s - loss: 0.0310 - acc: 0.9976
Epoch 30/30
 - 9s - loss: 0.0301 - acc: 0.9976
# Training time = 0:49:22.021420
# F-Score(Ordinary) = 0.628, Recall: 0.765, Precision: 0.532
# F-Score(lvc) = 0.497, Recall: 0.918, Precision: 0.341
# F-Score(ireflv) = 0.827, Recall: 0.903, Precision: 0.762
# F-Score(id) = 0.557, Recall: 0.616, Precision: 0.508
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 200)       2938600     input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 56)        13160       input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 800)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 224)          0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 1024)         0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 256)          262400      concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 256)          0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            2056        dropout_9[0][0]                  
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 88s - loss: 0.0187 - acc: 0.9963 - val_loss: 0.0164 - val_acc: 0.9973
Epoch 2/30
 - 88s - loss: 0.0160 - acc: 0.9974 - val_loss: 0.0158 - val_acc: 0.9975
Epoch 3/30
 - 88s - loss: 0.0167 - acc: 0.9974 - val_loss: 0.0174 - val_acc: 0.9974
Epoch 4/30
 - 88s - loss: 0.0169 - acc: 0.9974 - val_loss: 0.0181 - val_acc: 0.9974
Epoch 5/30
 - 88s - loss: 0.0169 - acc: 0.9974 - val_loss: 0.0211 - val_acc: 0.9975
Epoch 6/30
 - 88s - loss: 0.0182 - acc: 0.9974 - val_loss: 0.0235 - val_acc: 0.9971
Epoch 7/30
 - 88s - loss: 0.0195 - acc: 0.9974 - val_loss: 0.0206 - val_acc: 0.9974
Epoch 8/30
 - 88s - loss: 0.0215 - acc: 0.9974 - val_loss: 0.0238 - val_acc: 0.9974
Epoch 9/30
 - 88s - loss: 0.0233 - acc: 0.9975 - val_loss: 0.0268 - val_acc: 0.9973
Epoch 10/30
 - 88s - loss: 0.0241 - acc: 0.9976 - val_loss: 0.0276 - val_acc: 0.9974
Epoch 11/30
 - 88s - loss: 0.0246 - acc: 0.9976 - val_loss: 0.0266 - val_acc: 0.9973
Epoch 12/30
 - 88s - loss: 0.0246 - acc: 0.9976 - val_loss: 0.0285 - val_acc: 0.9973
Epoch 13/30
 - 88s - loss: 0.0249 - acc: 0.9976 - val_loss: 0.0277 - val_acc: 0.9974
Epoch 14/30
 - 88s - loss: 0.0257 - acc: 0.9976 - val_loss: 0.0302 - val_acc: 0.9972
Epoch 15/30
 - 88s - loss: 0.0260 - acc: 0.9977 - val_loss: 0.0329 - val_acc: 0.9974
Epoch 16/30
 - 88s - loss: 0.0280 - acc: 0.9977 - val_loss: 0.0327 - val_acc: 0.9973
Epoch 17/30
 - 88s - loss: 0.0295 - acc: 0.9976 - val_loss: 0.0324 - val_acc: 0.9974
Epoch 18/30
 - 88s - loss: 0.0299 - acc: 0.9976 - val_loss: 0.0328 - val_acc: 0.9974
Epoch 19/30
 - 88s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.0324 - val_acc: 0.9974
Epoch 20/30
 - 88s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.0341 - val_acc: 0.9973
Epoch 21/30
 - 88s - loss: 0.0295 - acc: 0.9976 - val_loss: 0.0339 - val_acc: 0.9973
Epoch 22/30
 - 88s - loss: 0.0298 - acc: 0.9976 - val_loss: 0.0364 - val_acc: 0.9968
Epoch 23/30
 - 88s - loss: 0.0293 - acc: 0.9977 - val_loss: 0.0336 - val_acc: 0.9972
Epoch 24/30
 - 88s - loss: 0.0299 - acc: 0.9976 - val_loss: 0.0328 - val_acc: 0.9974
Epoch 25/30
 - 88s - loss: 0.0299 - acc: 0.9977 - val_loss: 0.0354 - val_acc: 0.9972
Epoch 26/30
 - 88s - loss: 0.0299 - acc: 0.9977 - val_loss: 0.0345 - val_acc: 0.9974
Epoch 27/30
 - 88s - loss: 0.0295 - acc: 0.9978 - val_loss: 0.0353 - val_acc: 0.9973
Epoch 28/30
 - 88s - loss: 0.0301 - acc: 0.9977 - val_loss: 0.0335 - val_acc: 0.9973
Epoch 29/30
 - 88s - loss: 0.0294 - acc: 0.9978 - val_loss: 0.0383 - val_acc: 0.9969
Epoch 30/30
 - 88s - loss: 0.0300 - acc: 0.9977 - val_loss: 0.0339 - val_acc: 0.9974
Epoch 1/30
 - 9s - loss: 0.0346 - acc: 0.9972
Epoch 2/30
 - 9s - loss: 0.0355 - acc: 0.9971
Epoch 3/30
 - 9s - loss: 0.0331 - acc: 0.9973
Epoch 4/30
 - 9s - loss: 0.0351 - acc: 0.9973
Epoch 5/30
 - 9s - loss: 0.0332 - acc: 0.9974
Epoch 6/30
 - 9s - loss: 0.0344 - acc: 0.9973
Epoch 7/30
 - 9s - loss: 0.0327 - acc: 0.9975
Epoch 8/30
 - 9s - loss: 0.0339 - acc: 0.9974
Epoch 9/30
 - 9s - loss: 0.0353 - acc: 0.9974
Epoch 10/30
 - 9s - loss: 0.0328 - acc: 0.9975
Epoch 11/30
 - 9s - loss: 0.0333 - acc: 0.9975
Epoch 12/30
 - 9s - loss: 0.0325 - acc: 0.9976
Epoch 13/30
 - 9s - loss: 0.0329 - acc: 0.9975
Epoch 14/30
 - 9s - loss: 0.0332 - acc: 0.9975
Epoch 15/30
 - 9s - loss: 0.0327 - acc: 0.9976
Epoch 16/30
 - 9s - loss: 0.0323 - acc: 0.9976
Epoch 17/30
 - 9s - loss: 0.0317 - acc: 0.9976
Epoch 18/30
 - 9s - loss: 0.0324 - acc: 0.9976
Epoch 19/30
 - 9s - loss: 0.0317 - acc: 0.9977
Epoch 20/30
 - 9s - loss: 0.0325 - acc: 0.9976
Epoch 21/30
 - 9s - loss: 0.0316 - acc: 0.9977
Epoch 22/30
 - 9s - loss: 0.0324 - acc: 0.9976
Epoch 23/30
 - 9s - loss: 0.0325 - acc: 0.9976
Epoch 24/30
 - 9s - loss: 0.0320 - acc: 0.9976
Epoch 25/30
 - 9s - loss: 0.0318 - acc: 0.9977
Epoch 26/30
 - 9s - loss: 0.0323 - acc: 0.9976
Epoch 27/30
 - 9s - loss: 0.0309 - acc: 0.9977
Epoch 28/30
 - 9s - loss: 0.0314 - acc: 0.9977
Epoch 29/30
 - 9s - loss: 0.0316 - acc: 0.9977
Epoch 30/30
 - 9s - loss: 0.0316 - acc: 0.9977
# Training time = 0:50:05.829914
# F-Score(Ordinary) = 0.651, Recall: 0.796, Precision: 0.55
# F-Score(lvc) = 0.525, Recall: 0.757, Precision: 0.402
# F-Score(ireflv) = 0.813, Recall: 0.892, Precision: 0.746
# F-Score(id) = 0.6, Recall: 0.723, Precision: 0.513
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091
# Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = True
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 200
# POS = True
# POS emb = 56
# Features = False
# Token weight matrix used# POS weight matrix used# Parameters = 3216216
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 200)       2938600     input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 56)        13160       input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 800)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 224)          0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 1024)         0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 256)          262400      concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 256)          0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            2056        dropout_10[0][0]                 
==================================================================================================
Total params: 3,216,216
Trainable params: 3,216,216
Non-trainable params: 0
__________________________________________________________________________________________________
None
[(0.0, 393870), (1.0, 388780), (2.0, 5090), (4.0, 1191), (5.0, 1593), (6.0, 1196), (7.0, 1)]
[(0.0, 393870), (1.0, 393870), (2.0, 393870), (4.0, 393870), (5.0, 393870), (6.0, 393870), (7.0, 393870)]
(2757090,) 2757090 (2, 2757090, 4) 22056720
Train on 2481381 samples, validate on 275709 samples
Epoch 1/30
 - 88s - loss: 0.0187 - acc: 0.9964 - val_loss: 0.0151 - val_acc: 0.9971
Epoch 2/30
 - 88s - loss: 0.0160 - acc: 0.9974 - val_loss: 0.0149 - val_acc: 0.9974
Epoch 3/30
 - 89s - loss: 0.0170 - acc: 0.9974 - val_loss: 0.0169 - val_acc: 0.9975
Epoch 4/30
 - 89s - loss: 0.0172 - acc: 0.9974 - val_loss: 0.0209 - val_acc: 0.9974
Epoch 5/30
 - 89s - loss: 0.0192 - acc: 0.9975 - val_loss: 0.0244 - val_acc: 0.9975
Epoch 6/30
 - 88s - loss: 0.0236 - acc: 0.9975 - val_loss: 0.0211 - val_acc: 0.9973
Epoch 7/30
 - 88s - loss: 0.0237 - acc: 0.9975 - val_loss: 0.0257 - val_acc: 0.9975
Epoch 8/30
 - 88s - loss: 0.0252 - acc: 0.9975 - val_loss: 0.0270 - val_acc: 0.9972
Epoch 9/30
 - 88s - loss: 0.0245 - acc: 0.9976 - val_loss: 0.0253 - val_acc: 0.9974
Epoch 10/30
 - 88s - loss: 0.0260 - acc: 0.9976 - val_loss: 0.0283 - val_acc: 0.9975
Epoch 11/30
 - 88s - loss: 0.0266 - acc: 0.9976 - val_loss: 0.0263 - val_acc: 0.9975
Epoch 12/30
 - 88s - loss: 0.0265 - acc: 0.9976 - val_loss: 0.0306 - val_acc: 0.9974
Epoch 13/30
 - 88s - loss: 0.0273 - acc: 0.9977 - val_loss: 0.0314 - val_acc: 0.9973
Epoch 14/30
 - 88s - loss: 0.0294 - acc: 0.9976 - val_loss: 0.0310 - val_acc: 0.9974
Epoch 15/30
 - 88s - loss: 0.0287 - acc: 0.9977 - val_loss: 0.0315 - val_acc: 0.9973
Epoch 16/30
 - 88s - loss: 0.0286 - acc: 0.9977 - val_loss: 0.0311 - val_acc: 0.9974
Epoch 17/30
 - 88s - loss: 0.0285 - acc: 0.9977 - val_loss: 0.0330 - val_acc: 0.9973
Epoch 18/30
 - 88s - loss: 0.0291 - acc: 0.9977 - val_loss: 0.0320 - val_acc: 0.9974
Epoch 19/30
 - 88s - loss: 0.0289 - acc: 0.9977 - val_loss: 0.0307 - val_acc: 0.9975
Epoch 20/30
 - 88s - loss: 0.0289 - acc: 0.9978 - val_loss: 0.0313 - val_acc: 0.9974
Epoch 21/30
 - 88s - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0313 - val_acc: 0.9975
Epoch 22/30
 - 88s - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0309 - val_acc: 0.9974
Epoch 23/30
 - 88s - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0322 - val_acc: 0.9974
Epoch 24/30
 - 88s - loss: 0.0287 - acc: 0.9978 - val_loss: 0.0332 - val_acc: 0.9973
Epoch 25/30
 - 88s - loss: 0.0292 - acc: 0.9978 - val_loss: 0.0330 - val_acc: 0.9971
Epoch 26/30
 - 88s - loss: 0.0297 - acc: 0.9977 - val_loss: 0.0336 - val_acc: 0.9974
Epoch 27/30
 - 88s - loss: 0.0296 - acc: 0.9978 - val_loss: 0.0338 - val_acc: 0.9972
Epoch 28/30
 - 88s - loss: 0.0294 - acc: 0.9978 - val_loss: 0.0338 - val_acc: 0.9973
Epoch 29/30
 - 88s - loss: 0.0306 - acc: 0.9977 - val_loss: 0.0318 - val_acc: 0.9975
Epoch 30/30
 - 88s - loss: 0.0302 - acc: 0.9977 - val_loss: 0.0323 - val_acc: 0.9974
Epoch 1/30
 - 10s - loss: 0.0331 - acc: 0.9972
Epoch 2/30
 - 10s - loss: 0.0331 - acc: 0.9973
Epoch 3/30
 - 10s - loss: 0.0341 - acc: 0.9973
Epoch 4/30
 - 10s - loss: 0.0323 - acc: 0.9974
Epoch 5/30
 - 10s - loss: 0.0321 - acc: 0.9974
Epoch 6/30
 - 10s - loss: 0.0322 - acc: 0.9974
Epoch 7/30
 - 10s - loss: 0.0306 - acc: 0.9976
Epoch 8/30
 - 10s - loss: 0.0307 - acc: 0.9976
Epoch 9/30
 - 10s - loss: 0.0312 - acc: 0.9976
Epoch 10/30
 - 10s - loss: 0.0314 - acc: 0.9975
Epoch 11/30
 - 10s - loss: 0.0312 - acc: 0.9975
Epoch 12/30
 - 10s - loss: 0.0316 - acc: 0.9975
Epoch 13/30
 - 10s - loss: 0.0310 - acc: 0.9976
Epoch 14/30
 - 10s - loss: 0.0311 - acc: 0.9976
Epoch 15/30
 - 10s - loss: 0.0310 - acc: 0.9976
Epoch 16/30
 - 10s - loss: 0.0305 - acc: 0.9977
Epoch 17/30
 - 10s - loss: 0.0318 - acc: 0.9976
Epoch 18/30
 - 10s - loss: 0.0305 - acc: 0.9976
Epoch 19/30
 - 10s - loss: 0.0305 - acc: 0.9977
Epoch 20/30
 - 10s - loss: 0.0298 - acc: 0.9977
Epoch 21/30
 - 10s - loss: 0.0309 - acc: 0.9976
Epoch 22/30
 - 10s - loss: 0.0301 - acc: 0.9977
Epoch 23/30
 - 10s - loss: 0.0315 - acc: 0.9976
Epoch 24/30
 - 10s - loss: 0.0302 - acc: 0.9977
Epoch 25/30
 - 10s - loss: 0.0296 - acc: 0.9977
Epoch 26/30
 - 10s - loss: 0.0293 - acc: 0.9977
Epoch 27/30
 - 10s - loss: 0.0295 - acc: 0.9978
Epoch 28/30
 - 10s - loss: 0.0297 - acc: 0.9977
Epoch 29/30
 - 10s - loss: 0.0289 - acc: 0.9978
Epoch 30/30
 - 10s - loss: 0.0302 - acc: 0.9977
# Training time = 0:50:15.309343
# F-Score(Ordinary) = 0.678, Recall: 0.846, Precision: 0.566
# F-Score(lvc) = 0.543, Recall: 0.962, Precision: 0.379
# F-Score(ireflv) = 0.742, Recall: 0.683, Precision: 0.811
# F-Score(id) = 0.639, Recall: 0.949, Precision: 0.482
********************
